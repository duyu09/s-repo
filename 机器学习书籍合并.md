# MACHINE LEARNING

Technical Strategy for Al Engineers, In the Era of Deep Learning

ANDREW NG

![](images/3ce3a648a1d71fa0f28fc88980505063fcbff8d8f73743771bbba756f572bf74.jpg)

deeplearning.ai

机器学习训练秘籍

属于 deeplearning.ai 项目.

=======中文PDF相关信息=======

项目地址: 点击此处

文件版本: 0.5.0 draft

昀后更新: 2018/10/31

译者水平有限，如有翻译不当之处，

恳请读者指正，联系邮箱:

acdoge.cao@gmail.com

=========================

© 2018 Andrew Ng. All Rights Reserved.

# 目录

1 机器学习为什么需要策略?  
2 如何使用此书来帮助你的团队  
3 先修知识与符号说明  
4 规模驱动机器学习发展  
5 开发集和测试集的定义  
6 开发集和测试集应该服从同一分布  
7 开发集和测试集应该有多大？?  
8 使用单值评估指标进行优化  
9 优化指标和满意度指标   
10 通过开发集和度量指标加速迭代  
11 何时修改开发集、测试集和指标  
12 小结：建立开发集和测试集  
13 快速构建并迭代你的第一个系统  
14 误差分析：根据开发集样本评估想法  
15 在误差分析时并行评估多个想法  
16 清洗误标注的开发集和测试集样本  
17 将大型开发集拆分为两个子集，专注其一  
18 Eyeball 和 Blackbox 开发集该设置多大？  
19 小结：基础误差分析  
20 偏差和方差：误差的两大来源  
21 偏差和方差举例  
22 与昀优错误率比较   
23 处理偏差和方差  
24 偏差和方差间的权衡   
25 减少可避免偏差的技术

26 训练集误差分析  
27 减少方差的技术  
28 诊断偏差与方差：学习曲线  
29 绘制训练误差曲线  
30 解读学习曲线：高偏差  
31 解读学习曲线：其它情况  
32 绘制学习曲线  
33 为何与人类表现水平进行对比  
34 如何定义人类表现水平  
35 超越人类表现水平  
36 何时在不同的分布上训练与测试  
37 如何决定是否使用你所有的数据  
38 如何决定是否添加不一致的数据  
39 给数据添加权重  
40 从训练集泛化到开发集  
41 辨别偏差、方差和数据不匹配误差   
42 解决数据不匹配问题  
43 人工合成数据  
44 优化验证测试   
45 优化验证测试的一般形式  
46 强化学习举例  
47 端到端学习的兴起   
48 端到端学习的更多例子  
49 端到端学习的优缺点  
50 流水线组件的选择：数据可用性  
51 流水线组件的选择：任务简单性

52 直接学习更为丰富的输出  
53 根据组件进行误差分析  
54 误差归因至某个组件  
55 误差归因的一般情况  
56 组件误差分析与人类水平对比  
57 发现有缺陷的机器学习流水线  
58 建立超级英雄团队 - 让你的队友阅读这本书吧！

# 1 机器学习为什么需要策略?

机器学习（machine learning）已然成为无数重要应用的基石——如今，在网络搜索、垃圾邮件检测、语音识别以及产品推荐等领域，你都能够发现它的身影。如果你或你的团队正在研发一款机器学习相关应用，并期待取得较快进展，那么这本书将会是你的得力助手。

![](images/bcf063b818007c83310adc1cb669a6ee1367c176ddda6788d602df777cd11961.jpg)

# 案例：建立猫咪图片初创公司

想象一下，你正在建立一家初创公司，这家公司的产品目标是为猫咪爱好者们提供数不尽的猫咪图片，你打算应用神经网络（neural network）技术来构建一套计算机视觉系统，通过该系统来识别图片中的猫。

但悲剧的是，目前你的学习算法准确度还不够好。为了改进这个猫咪检测器，你正面临着巨大的压力。可你该怎么做呢？

你的团队给出了许多建议，例如：

获取更多的数据，即收集更多的猫咪图片  
● 收集更加多样化的训练数据集，图片中猫的位置可能不常见，也有可能颜色奇异，或者拍摄时使用不同的相机参数  
● 通过增加梯度下降的迭代次数，使算法训练得久一些  
● 尝试一个拥有更多层/更多隐藏元/更多参数的，规模更大的神经网络  
尝试一个更小的神经网络  
● 尝试加入正则化（例如 L2 正则化）

改变神经网络的架构（激活函数，隐藏元数量等等）

在上面众多的方向中，如果你做出了正确的选择，就将建立起一个效果领先的猫咪图片识别平台，并带领你的公司取得成功。但如果你选择了一个糟糕的方向，则可能因此浪费掉几个月的时间。那么你该如何做出决定呢？

这本书将告诉你应该怎么做。众多的机器学习问题会留下一些线索，告诉你什么样的尝试有用，什么样的没用。而学会解读这些线索将会节省你几个月甚至几年的开发时间。

# 2 如何使用此书来帮助你的团队

完成本书的阅读后，你将进一步理解如何为一个机器学习项目设定技术方向，但团队成员可能不理解你为何要推荐某个特定的方向。有时你希望你的团队定义一个单值评估指标，但他们并不认可你的想法，此时你将如何说服他们？

这正是我决定缩短章节篇幅的原因——这样你就能够将它们打印出来，并且让你的成员仅阅读其中他们需要了解的那几页。

优先级的稍加改变会对团队的生产力产生巨大的影响，我希望你能帮助团队做出一些这样的改变，从而成为团队里的超级英雄！

![](images/e095f5d61ef67dc399ef09e27ba7631207a5f7d27cdb0b939270a0a4a84d3983.jpg)

# 3 先修知识与符号说明

如果你有学习过机器学习相关课程（例如我在 Coursera 开设的机器学习 MOOC），或者有过监督学习的应用经验，这本书的内容对你而言则不难理解。

本书假设你熟悉监督学习（supervised learning）概念，即使用标注（labeled）的训练样本(x,y) 来学习一个从 x 映射到 y 的函数。监督学习算法主要包括线性回归（linear regression）、对数几率回归（logistic regression，又译作逻辑回归、逻辑斯蒂回归）和神经网络（neural network）。虽然机器学习的形式有许多种，但当前具备实用价值的大部分机器学习算法都来自于监督学习。

我将经常提及神经网络（也被人们称为“深度学习” ），但你只需对这个概念有基础的了解便可以阅读本书后面的内容。

如果对上文提到的一些概念你还不是很熟悉，可以在 Coursera 观看《机器学习》前三周的课程内容。（课程地址：http://ml-class.org）

![](images/85719fbd5301d316bbfad291b194b5beef408f384094e842c2c8075a1cb24962.jpg)

# 4 规模驱动机器学习发展

关于深度学习（神经网络）的一些想法在几十年前就有了，那为什么它们到现在才流行起来了呢？

推动其近期发展的主要因素有两个：

- 数据可用性（data availability）：如今人们在数字设备（笔记本电脑、移动设备等）上花费的时间越来越多，对应的数字化行为与活动产生了海量的数据，而这些数据都可以提供给我们的学习算法用来训练。

- 计算规模（computational scale）：在近几年前，我们才开始有能力训练出规模足够大的神经网络来使用现有的海量数据集。

具体来说，即使你积累了更多的数据，但应用在类似于对数几率回归（logistic regression）这样较传统的学习算法上，其性能表现（performance）也将趋于 “平稳” 。这意味着算法的学习曲线将 “变得平缓” ，就算提供更多的数据，算法的性能也将不再提升。

![](images/7628a80fc4c122644242a28058e7d4d695feba2356fb7fe0e21c18b26b5af7b7.jpg)

传统学习算法似乎并不知道要如何来处理现今这般规模量级的数据。

在同样的监督学习任务下，选择训练一个小型的神经网络（neutral network, NN），你可能会获得较好的性能表现：

![](images/97018b23f444634578ee0bad4b026b81a1e81a9e305e47afda451f724accb326.jpg)

这里的 “小型神经网络” 指的是只含有少量的隐藏元/层/参数的神经网络。但如果你训练的神经网络规模越来越大，昀终很有可能会获得更好的表现：1

![](images/37bf398b3aa21a98f77952da396257a06ccf5c7e7a1610cdfd35e0057b1ca1ab.jpg)

因此，为了获得昀佳的性能表现，你可以这样做：

训练大型的神经网络，效果如同上图的绿色曲线；

拥有海量的数据。

在算法训练时，许多其它的细节也同等重要，例如神经网络的架构。但目前来说，提升算法性能的更加可靠的方法仍然是训练更大的网络以及获取更多的数据。完成 1 和 2 的过程异常复

杂，本书将对其中的细节作进一步的讨论。我们将从传统学习算法与神经网络中都起作用的通用策略入手，循序渐进地讲解至昀前沿的构建深度学习系统的策略。

# 设置开发集

# 与测试集

# 5 开发集和测试集的定义

继续分析我们之前提到的猫咪图片的案例：现在你负责运营一个移动端 app，用户会向这个app 上传许多不同内容的图片。而你希望这个 app 能够从图片中自动地找到含有猫的图片。

你的团队已经在不同的网站下载了含有猫的图片（正样本，又译作正例），以及不含猫的图片（负样本，又译作反例），从而得到了一个巨型的数据集。他们将数据集按照 $7 0 \% / 3 0 \%$ 的比例划分为训练集（training set）和测试集（test set），并且使用这些数据构建出了一个在训练集和测试集上均表现良好的猫咪检测器。

可当你将这个分类器（classifier）部署到移动应用中时，却发现它的性能相当之差！

这究竟是什么原因导致的呢？

你会发现，从网站上下载下来作为训练集的图片与用户上传的图片有较大的区别——用户上传的图片大部分是使用手机拍摄的，此类图片往往分辨率较低，且模糊不清，采光也不够理想。但由于用来进行训练和测试的数据集图片均取自网站，这就导致了算法不能够很好地泛化（generalize）到我们所关心的手机图片的实际分布（actual distribution）情况上。

在大数据时代来临前，机器学习中的普遍做法是使用 $7 0 \% / 3 0 \%$ 的比例来随机划分出训练集和测试集。这种做法的确可行，但在越来越多的实际应用中，训练数据集的分布（例如上述案例中的网站图片）与人们昀终所关心的分布情况（例如上述案例中的手机图片）往往不同，此时执意采取这样的划分其实是一个坏主意。

我们通常认为:

训练集（training set）用于运行你的学习算法。  
• 开发集（development set）用于调整参数，选择特征，以及对学习算法作出其它决定。有时也称为​留出交叉验证集（hold-out cross validation set）。  
• 测试集（test set）用于评估算法的性能，但不会据此改变学习算法或参数。

在定义了开发集（development set）和测试集（test set）后，你的团队将可以尝试许多的想法，比如调整学习算法的参数来探索哪些参数的使用效果昀好。开发集和测试集能够帮助你的团队快速检测算法性能。

换而言之，开发集和测试集的使命就是引导你的团队对机器学习系统做出昀重要的改变​。

所以你应当这样处理：

合理地选择开发集和测试集，使之能够代表将来实际数据的情况，并期望算法能够运行良好。

也就是说你的测试集不应该仅是简单地将可用的数据划分出 $30 \%$ ，尤其是将来获取的数据（移动端图片）在性质上可能会与训练集（网站图片）有所不同时。

如果你尚未推出移动端 app，那么可能还没有任何的用户，因此也无法获取一些准确的反馈数据来为后续的行动提供依据。但你仍然能够尝试去模拟出这种情况，例如邀请你的朋友用手机拍下照片并发送给你。当你的 app 上线后，就能够使用实际的用户数据对开发集和测试集进行更新。

如果你实在没有途径获取近似未来实际情况的数据，也可以尝试使用已有的网站图片。但你应该意识到这其中的风险，它将导致系统不能够很好地泛化（generalize）。

选择一个理想的开发集和测试集是需要一定投入的，投入多少由你来决定。但请不要武断地认为测试集分布和训练集分布是一致的。尽可能地选择你昀终期望算法能够正确处理的样本作为测试集，而不是随便选择一个你恰好拥有的训练集样本。

# 6 开发集和测试集应该服从同一分布

根据公司的核心市场分布情况，你将猫咪 app 的图像数据划分为 “美国” 、 “中国” 、 “印度” 和“其它地区” 四个区域。在设立开发集和测试集时，可以尝试将 “美国” 和 “印度” 的数据归于开发集，而 “中国” 和 “其它地区” 的数据归于测试集。也就是说我们可以随机地将其中两个区域的数据分配给开发集，另外两个区域的数据分配给测试集。这样做对吗？

当然不对！

一旦定义好了开发集和测试集，你的团队将专注于提升开发集的性能表现，这就要求开发集能够体现核心任务：使算法在四个地区都表现优异，而不仅仅是其中的两个。

开发集和测试集的分布不同还将导致第二个问题：你的团队所开发的系统可能在开发集上表现良好，却在测试集上表现不佳。我曾目睹过这样的事件，这令人十分沮丧并且还会浪费大量的时间，因此希望你不要重蹈他们的覆辙。

举个例子，假设你的团队开发了一套能在开发集上运行性能良好，却在测试集上效果不佳的系统。如果此时开发集和测试集的分布相同，那么你就能清楚地明白问题所在：算法在开发集上过拟合了（overfit）。解决方案显然就是去获取更多的开发集数据。

但是如果开发集和测试集服从不同的分布，解决方案就不那么明确了。此时可能存在以下一种或者多种情况：

1. 算法在开发集上过拟合了。  
2. 测试集比开发集更难进行预测，尽管算法做得足够好了，却很难有进一步的提升空间。  
3. 测试集不一定更难预测，但它与开发集性质并不相同（分布不同）。因此在开发集上表现良好的算法不一定在测试集上也能够取得出色表现。如果是这种情况，大量针对开发集性能的改进工作将会是徒劳的。

构建机器学习应用已并非易事，而开发集和测试集分布的不匹配又会引入额外的不确定性—即提高算法在开发集上的性能表现，是否也能提升其在测试集的性能表现？在这种情况下很难去弄清楚哪些工作是有效的，哪些工作又是在浪费时间，从而会影响到工作的优先级安排。

在处理第三方基准测试（benchmark）问题时，样本提供方很可能已经指定了服从不同分布的开发集和测试集数据。与数据分布一致的情况相比，此时运气带来的性能影响将超过你使用的技术所带来的影响。因此，寻找能够在某个分布上进行训练，同时也能够很好地泛化到另一个

分布上的学习算法，同样是一个重要的研究课题。但是如果你想要在特定的机器学习应用上取得进展，而不是搞研究，我建议你尽可能地选择服从相同分布的开发集和测试集数据，这会让你的团队更有效率。

# 7 开发集和测试集应该有多大？

开发集的规模应该尽可能的大，至少要能够区分出你所尝试的不同算法之间的性能差异。例如，如果分类器 A 的准确率为 $90 . 0 \%$ ，而分类器 B 的准确率为 $9 0 . 1 \%$ ，那么使用仅含有 100个样本的开发集将无法检测出这 $0 . 1 \%$ 的差异。与我所遇到的机器学习问题相比，一个样本容量仅为 100 的开发集，规模太小了。通常来说，开发集的规模应该在 1,000 到 10,000 个样本数据之间，而当开发集样本容量为 10,000 时，你将很有可能检测到这 $0 . 1 \%$ 的性能提升。2

在类似广告服务、网络搜索和产品推荐等较为成熟且关键的应用领域，我曾见过一些团队非常积极地去改进算法性能，哪怕只有 $0 . 0 1 \%$ 的提升，因为这将直接影响到公司的利润。在这种情况下，开发集规模可能远超过 10,000 个样本，从而有利于检测到那些不易察觉的效果提升。

那么测试集的大小又该如何确定呢？它的规模应该大到使你能够对整体系统的性能进行一个高度可信的评估。一种常见的启发式策略是将整体 $30 \%$ 的数据用作测试集，这适用于总体数据量规模一般的情况（比如 100 至 10,000 个样本）。但是在大数据时代，有时我们所面临的机器学习问题的样本数量将超过 10 个亿，即使开发集和测试集中样本的绝对数量一直在增长，可总体上分配给开发集和测试集的数据比例正在不断降低。可以看出，我们并不需要将开发集和测试集的规模提升到远远超过评估算法性能所需的程度，也就是说，开发集和测试集的规模并不是越大越好。

# 8 使用单值评估指标进行优化

所谓的单值评估指标（single-number evaluation metric）有很多，分类准确率就是其中的一种：待你在开发集（或测试集）上运行分类器之后，它将返回单个数值，代表着样本被正确分类的比例。根据这个指标，如果分类器 A 的准确率为 $9 7 \%$ ，而分类器 B 的准确率为 $90 \%$ ，那么我们可以认为分类器 A 更优秀。

相比之下，查准率（Precision，又译作精度）和查全率（Recall，又译作召回率）的组合并不能作为单值评估指标，因为它给出了两个值来对你的分类器进行评估。 多3 值评估指标提高了在算法之间进行优劣比较的难度，假设你的算法表现如下：

<table><tr><td>Classifier</td><td>Precision</td><td>Recall</td><td></td></tr><tr><td>A</td><td colspan="2">95%</td><td>90%</td></tr><tr><td>B</td><td colspan="2">98%</td><td>85%</td></tr></table>

若根据上方表格中的数值对两个分类器进行比较，显然二者都没有较为明显的优势，因此也无法指导你立即做出选择。

<table><tr><td>Classifier</td><td>Precision</td><td>Recall</td><td>F1 score</td></tr><tr><td>A</td><td>95%</td><td>90%</td><td>92.4%</td></tr></table>

当你的团队在进行开发时，往往会尝试多种多样的算法架构、模型参数、特征选择，或是一些其它的想法。你可以通过使用单值评估指标（如准确率），根据所有的模型在此指标上的表现，进行排序，从而能够快速确定哪一个模型的性能表现昀好。

如果你认为查准率和查全率指标很关键，可以参照其他人的做法，将这两个值合并为一个值来表示。例如取二者的平均值，或者你可以计算 “F1分数（F1 score）” ，这是一种经过修正的平均值计算方法，比起直接取平均值的效果会好一些。4

<table><tr><td>Classifier</td><td>Precision</td><td>Recall</td><td>F1 score</td></tr><tr><td>A</td><td>95%</td><td>90%</td><td>92.4%</td></tr><tr><td>B</td><td>98%</td><td>85%</td><td>91.0%</td></tr></table>

综上可知，当你需要在多个分类器之间进行选择时，使用单值评估指标将帮助你更快速地作出决定。它能给出一个清晰明了的分类器性能排名，从而帮助团队明确后续的改进方向。

昀后补充一个例子，假设你在 “美国” 、 “印度” 、 “中国” 和 “其它地区” 这四个关键市场追踪你的猫分类器准确率，并且获得了四个指标。通过对这四个指标取平均值或进行加权平均，你将得到一个单值指标。取平均值或者加权平均值是将多个指标合并为一个指标的昀常用方法之一。

# 9 优化指标和满意度指标

下面我们来了解一下组合多个评估指标的另一种方法。

假设你既关心学习算法的准确率（accuracy），又在意其运行时间（running time），请从下面的三个分类器中做出选择：

<table><tr><td>Classifier</td><td>Accuracy</td><td>Running time</td></tr><tr><td>A</td><td>90%</td><td>80ms</td></tr><tr><td>B</td><td>92%</td><td>95ms</td></tr><tr><td>C</td><td>95%</td><td>1,500ms</td></tr></table>

将准确率和与运行时间放入单个公式计算后可以导出单个的指标，这似乎不太符合常理：

Accuracy - 0.5*RunningTime

有一种替代方案可供选择：首先定义一个 “可接受的” 运行时间，一般低于 100ms 。接着，在限定的运行时间范围内，尽可能地将分类器的准确率昀大化。此时，运行时间代表着 “满意度指标” —— 你的分类器必须在这个指标上表现得 “足够好” ，这里指的是运行时间约束上限为100ms；而准确度则代表着 “优化指标”。

如果要考虑 N 项不同的标准，比如模型的二进制文件大小（这对移动端 app 尤为重要，因为用户不想下载体积很大的 app）、运行时间和准确率，你或许需要设置 N-1 个 “满意度” 指标，即先要求它们满足一定的值或范围，下一步才是定义一个 “优化” 指标。例如分别为二进制文件的大小和运行时间设定可接受的阈值，并尝试根据这些限制来优化准确率指标。

昀后再举一个例子，假设你正在设计一个硬件设备，该设备可以根据用户设置的特殊 “唤醒词”来唤醒系统，类似于 Amazon Echo 的监听词为 “Alexa”，苹果（Apple） Siri 的监听词为“Hey Siri”，安卓（Android） 的监听词为 “Okay Google”，以及百度（Baidu）应用的监听词“Hello Baidu.” 我们关心的指标是假正例率（false positive rate）—— 用户没有说出唤醒词，系统却被唤醒了，以及假反例率（false negative rate）——用户说出了唤醒词，系统却没能正确被唤醒。这个系统的一个较为合理的优化对象是尝试去昀小化假反例率（优化指标），减少用户说出唤醒词而系统却没能正确唤醒的发生率，同时设置约束为每 24 小时不超过一次误报（满意度指标）。

一旦你的团队在优化评估指标上保持一致，他们将能够取得更快的进展。

# 10 通过开发集和度量指标加速迭代

对于当前面临的新问题，我们很难提前知道使用哪种方法会是昀合适的，即使是一个经验丰富的机器学习研究员，通常也需要在尝试多种多样的方法之后才能发现令人满意的方案。当我要建立一个机器学习系统时，往往会这么做：

1. 尝试一些关于系统构建的想法（idea）。  
2. 使用代码（code）​实现想法。  
3. 根据实验（experiment）​结果判断想法是否行得通。（第一个想到的点子一般都行不通！）在此基础上学习总结，从而产生新的想法，并保持这一迭代过程。迭代过程如下图所示：

![](images/878c033ee144c0db4acf3e495f8ddbb80ba572be3784bfcfa30fde4c0d388f00.jpg)

迭代过程循环得越快，你也将进展得越快。此时，拥有开发集、测试集和度量指标的重要性便得以体现了：每当你有了一个新想法，在开发集上评估其性能就可以帮助你判断当前的方向是否正确。

假如你没有一个特定的开发集和度量指标，则需要在每次开发新的分类器时把它整合到 app中，并通过几个小时的体验来了解分类器的性能是否有所改进——这会浪费大量的时间！另外，如果你的团队将分类器的准确率从 95.0％ 提高到 95.1％，这 0.1% 的提升可能很难被检测出来。但是积少成多，通过不断积累这 0.1％ 的改进，你的系统将取得巨大的提升。拥有开发集和度量指标，可以使你更快地检测出哪些想法给系统带来了小（或大）的提升 ，从而快速确定下一步要研究或者是要放弃的方向。

# 11 何时修改开发集、测试集和指标

每当开展一个新项目时，我会尽快选好开发集和测试集，因为这可以帮团队制定一个明确的目标。

我通常会要求我的团队在不到一周（一般不会更长）的时间内给出一个初始的开发集、测试集和指标，提出一个不太完美的方案并迅速执行 ，这比起花过多的时间去思考要好很多。但是一周的时间要求并不适用于成熟的应用程序，譬如垃圾邮件过滤。我也见到过一些团队在已经成熟的系统上花费数月的时间来获得更好的开发集和测试集。

如果你渐渐发现初始的开发集、测试集和指标设置与期望目标有一定差距，那就尽快想办法去改进它们。例如你的开发集与指标在排序时将分类器 A 排在 B 的前面，然而你的团队认为分类器 B 在实际产品上的表现更加优异，这个时候就需要考虑修改开发集和测试集，或者是你的评估指标了。

在上述例子中，有三个主要原因可能导致开发集/评估指标错误地将分类器 A 排在 B 前面：

1. 你需要处理的实际数据的分布和开发集/测试集数据的分布情况不同。

假设你的初始开发集和测试集中主要是成年猫的图片，然而你在 app 上发现用户上传的更多是小猫的图片，这就导致了开发集和测试集的分布与你需要处理数据的实际分布情况不同。在这种情况下，需要更新你的开发集和测试集，使之更具代表性。

![](images/6472881986c6877d91ded3178a97db930c11933ae3bc1da84ca7ffd73e3e6ba2.jpg)

2. 算法在开发集上过拟合了。

在开发集上反复评估某个想法会导致算法在开发集上 “过拟合” 。当你完成开发后，应该在测试集上评估你的系统。如果你发现算法在开发集上的性能比测试集好得多，则表明你很有可能在开发集上过拟合了。在这种情况下，你需要获取一个新的开发集。

如果需要跟踪团队的进度，你可以每周或者每月在测试集上对系统进行一次定期评估。但不要根据测试集指标对算法做出任何决策，包括是否将系统回滚到前一周的状态。坚持这样做会导致算法在测试集上开始过拟合，并且不要再指望通过测试集对你的系统性能进行完全无偏估计（这对发表研究论文以及需要做出商业决策的人来说影响很大）。

# 3. 该指标不是项目应当优化的目标。

假设你的猫咪 app 当前的指标为分类准确率，而该指标认为分类器 A 优于分类器 B。然而在尝试了两种算法后，你发现分类器 A 竟然允许出现一些色情图片，这实在是难以容忍。应该怎么办呢？

以上这种情况表明，此时的指标并不能辨别出算法 B 在实际产品中的表现是否比 A 更好，因此根据该指标来选择算法并不可靠，也说明此时应该改变现有的评估指标。你可以选择修改指标，使之对出现色情图片的情况执行严重惩罚。此外，强烈建议你选择一个新的指标并为你的团队制定一个新的研究目标，而不是在不可信的指标上耗费太多的时间，昀终导致不得不回过头对分类器进行人工选择。

在项目中改变开发集、测试集或者指标是很常见的。一个初始的开发集、测试集和指标能够帮助团队进行快速迭代，当你发现它们对团队的导向不正确时，不要担心，你只需要对其进行修改并确保团队能够了解接下来的新方向。

# 12 小结：建立开发集和测试集

• 被选择作为开发集和测试集的数据，应当与你未来计划获取并对其进行良好处理的数据有着相同的分布，而不一定和训练集的数据分布一致。  
• 开发集和测试集的分布应当尽可能一致。  
• 为你的团队选择一个单值评估指标进行优化。  
• 当需要考虑多项目标时，不妨将它们整合到一个表达式里（比如对多个误差指标取平均），或者设定满意度指标和优化指标。  
• 机器学习是一个高度迭代的过程：在出现昀终令人满意的方案之前，你可能要尝试很多想法。  
• 拥有开发集、测试集和单值评估指标可以帮助你快速评估一个算法，从而加速迭代进程。  
• 当你要探索一个全新的应用时，尽可能在一周内建立你的开发集、测试集和评估指标；而在已经相对成熟的应用上，可以考虑花费更长的时间来执行这些工作。  
• 传统的 $7 0 \% / 3 0 \%$ 训练集/测试集划分对于大规模数据并不适用，实际上，开发集和测试集的比例会远低于 $30 \%$ .  
• 开发集的规模应当大到能够检测出算法精度的细微改变，但也不需要太大；测试集的规模应该大到能够使你能对系统的昀终性作出一个充分的估计。  
当开发集和评估指标对于团队已经不能提供一个正确的导向时，尽快修改它们：(i) 如果算法在开发集上过拟合，则需要获取更多的开发集数据。(ii) 如果开发集与测试集的数据分布和实际数据分布不同，则需要获取新的开发集和测试集。 (iii) 如果评估指标无法对昀重要的任务目标进行度量，则需要修改评估指标。

# 基础误差分析

# 13 快速构建并迭代你的第一个系统

当你想要构建一个新的垃圾邮件过滤系统时，团队可能会有各式各样不同的想法：

• 收集海量的垃圾邮件作为训练集。例如，设置一个 “蜜罐（honeypot，计算机安全领域又称为诱饵）”，故意地向垃圾邮件系统发送一些虚假邮件地址，以便于能够自动收集到垃圾邮件系统发送到这些地址的垃圾邮件内容。  
• 开发用于理解邮件文本内容的功能。  
• 开发用于理解邮件信封或邮件头部特征的功能，据此了解邮件经由了哪些网络服务器进行中转。  
更多…

虽然我在反垃圾邮件领域已经有了一定的经验，却依然难以保证每次都能顺利在上面的方向中进行抉择。如果你在此领域并不是一个专家，难度会变得更大。

如果在一开始就试图设计和构建出完美的系统会显得有些困难，不妨先花几天时间构建并训练一个昀基础的系统。5

或许这个昀基础的系统与我们所能构建的 “昀佳” 系统相去甚远，但研究其中的基础功能也是很有价值的：你能快速找到一些线索来帮助你决定在什么方向上投入时间。后面的一些章节会向你剖析该如何解读这些线索。

![](images/310cf29fd8ecc80f792a2a5399455cb22bf3dfc29a1ab627ab054c1ade577a39.jpg)

# 14 误差分析：根据开发集样本评估想法

![](images/d0f700b9a7eb504c3cc67715a37ccfa429b702131808234c606088490df0db51.jpg)

在使用猫咪 app 时，你注意到它将一些狗的图片误分类为猫了，因为有些狗的样子的确很像猫。

团队中有人建议，加入第三方软件来帮助系统更好地处理狗的样本，但这需要一个月的时间去完成。面对团队成员高涨的热情，你会允许他们这样做吗？

在执行这项计划前，我建议你先预估一下该任务能提升多少系统精度。这样你就能够更加理性地判断是否值得花一个月的时间做这件事，还是将这段时间用于其它任务。

具体而言，你可以这样：

1. 收集 100 个开发集中被误分类的样本，即造成系统误差的样本。  
2. 人为查看这些样本，并计算其中狗的比例。

查看误分类样本的这一过程称为误差分析。在上面的例子中，如果只有 $5 \%$ 误分类的图像是狗，那么无论你在狗的问题上做多少的算法改进，昀终都不会消除超过原有的 $5 \%$ 误差 . 也即是说 $5 \%$ 是该计划项目所能起到帮助的“上限”（昀大可能值）。所以如果整个系统当前的精度为 $90 \%$ （对应误差为 $10 \%$ ），那么这种改进昀多能将精度提升到 $90 . 5 \%$ （对应误差下降到$9 . 5 \%$ ， 改进了原有 $10 \%$ 误差其中的 $5 \%$ ）。

相反，如果你发现 $50 \%$ 的误分类图像是狗，那就可以自信地说这个项目将效果明显，它可以将精度从 $90 \%$ 提升到 $9 5 \%$ （相对误差减少 $50 \%$ ，整体误差由 $10 \%$ 下降到 $5 \%$ ）。

这种简单的误差分析计算过程可以协助你快速评估，从而了解是否需要将处理狗样本的第三方软件进行整合。它为是否值得进行这项投入给出了一个量化的基准。

误差分析也可以帮助你在不同的想法中发现哪些想法将更有前景。我见过许多工程师不愿意进行误差分析，他们更倾向于直接提出一个并实现一个想法，而不考虑该想法是否值得花时间去投入。这是一个普遍的误解：后果可能是你的团队在耗费一个月的时间后却发现收益甚少。

人为检查 100 个样本并不会占用太多的时间。即使你每分钟只检查一张图片，也能够在两小时内完成，而这两个小时可能可以帮你节省一个月的工作量。

误差分析（Error Analysis） 指的是检查被算法误分类的开发集样本的过程，以便帮助你找到造成这些误差的原因。这将协助你确定各个项目的优先级（就像上面的例子所提到的那样）并且获得探索新方向的灵感，我们将会在之后再讨论这些内容。接下来的几章会给出一些误差分析的昀佳实践。

# 15 在误差分析时并行评估多个想法

对于改进猫检测器，你的团队有一些想法

• 修正算法将狗误分类为猫的问题。  
• 修正算法将大型猫科动物（比如狮子、黑豹等等,下面用大猫代指）误分类为家猫的问题。  
• 改善系统在模糊图像上的表现。

上述的想法都可以以并行的形式进行评估。通常我会创建一个电子表格，一边查看被误分类的100 个开发集样本一边完善表格内容，同时我也会留下一些能够帮助我记住特定样本的备注。为了说明这一过程，如下所示是一个仅含四个样本的小型开发集的电子表格：

<table><tr><td>图像</td><td>狗</td><td>大猫</td><td>模糊</td><td>备注</td></tr><tr><td>1</td><td>✓</td><td></td><td></td><td>罕见美国比特犬</td></tr><tr><td>2</td><td></td><td></td><td>✓</td><td></td></tr><tr><td>3</td><td></td><td>✓</td><td>✓</td><td>狮子；雨天在动物园拍摄的图片</td></tr><tr><td>4</td><td></td><td>✓</td><td></td><td>树木后的美洲豹</td></tr><tr><td>占全体比例</td><td>25%</td><td>50%</td><td>50%</td><td></td></tr></table>

你会发现表格中图片 3 的”大猫”与“模糊”列都被勾选了，另外由于一个样本可能与多个类别都有关联，所以表格底部的百分比加起来可能不是 $100 \%$ .

虽然你可能事先规定了一些类别（狗，大猫，模糊）并进行了手动的分类，但在浏览图像时你可能会受到启发并发现一些新的类别。比如你在浏览一些图片时发现，经过 Instagram 滤镜处理过的图片在识别时对结果造成了误差，你就可以在电子表格中加入一个新的 “Instagram”列。手动查看误分类样本，并思考人类如何/是否能正确地分类这些样本，这将帮助你发现新的类别以及解决方案。

探寻方法去改进对应误差的类别是很有帮助的。举个例子，如果你能“撤销” Instagram 的滤镜效果并恢复到原始图片，那么 Instagram 分类就能发挥作用。这不代表你只能局限于那些已有的改进途径的类别；这个过程主要是为了帮助你对一些潜在的，有改进效果的方向培养一定的敏感度。

假设你已检查了 100 个开发集的误分类样本，并得到了下面的表格：

<table><tr><td>图像</td><td>狗</td><td>大猫</td><td>模糊</td><td>备注</td></tr><tr><td>1</td><td>✓</td><td></td><td></td><td>罕见美国比特犬</td></tr><tr><td>2</td><td></td><td></td><td>✓</td><td></td></tr><tr><td>3</td><td></td><td>✓</td><td>✓</td><td>狮子；雨天在动物园拍摄的图片</td></tr><tr><td>4</td><td></td><td>✓</td><td></td><td>树木后的美洲豹</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>占全体比例</td><td>8%</td><td>43%</td><td>61%</td><td></td></tr></table>

现在你应该可以判断出，对于解决狗的误分类问题项目昀多可以改进 $8 \%$ 的误差，处理大猫和模糊类则可以改进更多。因此你将更有可能挑选后两者之一进行处理。如果你的团队有足够的人力并行处理多个方向，则可以要求一部分成员处理大猫类别，另一部分成员处理模糊类别。

误差分析并不会产生一个明确的数学公式来告诉你什么任务的优先级昀高。你还需要考虑在不同类别上的预期进展以及解决每个类别所需的工作量。

# 16 清洗误标注的开发集和测试集样本

在进行误差分析时，你可能会注意到一些开发集的样本被误标注（mislabeled ）了。此处的“误标注”指的是图像在使用算法处理前，已经被负责标注的人员进行了错误的标注，也就是说，某个样本 $( \mathsf { x } , \mathsf { y } )$ 的分类标签（label） y 的值并不正确。例如，一些不是猫的图片被误标注为猫，反之亦然。如果你不确定这些被误标注的图片是否起着关键作用，可以添加一个类别来跟踪记录误标注样本的比例：

<table><tr><td>图像</td><td>狗</td><td>大猫</td><td>模糊</td><td>误标注</td><td>备注</td></tr><tr><td>…</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>98</td><td></td><td></td><td></td><td>✓</td><td>标注者忽略了背景中的猫</td></tr><tr><td>99</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>100</td><td></td><td></td><td></td><td>✓</td><td>猫的画像；非真猫</td></tr><tr><td>占全体比例</td><td>8%</td><td>43%</td><td>61%</td><td>6%</td><td></td></tr></table>

那么这个时候，需要修正开发集中的标签吗？回忆一下当初设立开发集的目标——帮助你快速评估算法性能，从而判断算法 A 和 B 哪一个更好。如果开发集中误标注的部分的确影响了你的判断，那么花时间去修正这些标签就是值得的。

举个例子，假设你的分类器表现如下：

• 开发集整体精度… …….. $90 \%$ （ $10 \%$ 整体误差）  
误标注样本造成的误差…… $0 . 6 \%$ （ $6 \%$ 开发集误差）  
• 其它原因造成的误差………. $9 . 4 \%$ （ $94 \%$ 开发集误差）

相对于你正在改进的 $9 . 4 \%$ 误差，误标记的 $0 . 6 \%$ 误差看起来就不那么重要了。在开发集中手动修正误标注样本不会带来不好的影响，但这样的做法并不是必要的：当你不知道整个系统的误差是 $10 \%$ 还是 $9 . 4 \%$ 时，这样处理会看起来不错。

假设你不断地改进猫分类器，并达到了以下性能：

• 开发集整体精度… ….. $9 8 . 0 \%$ （ $2 . 0 \%$ 整体误差）  
• 误标注样本造成的误差…… $0 . 6 \%$ （ $30 \%$ 开发集误差）

其它原因造成的误差………. $1 . 4 \%$ （ $70 \%$ 开发集误差）

此时 $30 \%$ 的开发集误差是由误标注样本造成的，这对精度估计造成了显著的影响。此时就应该考虑改进开发集样本中的标签质量。处理这些误标注的样本将帮助你找出分类器的误差是接近 $1 . 4 \%$ 还是 $2 . 0 \%$ ，差异显著。

在项目初始阶段容许一些误标注的开发集/测试集样本并不罕见，你可以选择在系统改进到一定程度时再来考虑被误标注的样本，因为这些误差在整体误差中的占比会逐渐增大。

上一章中解释了如何通过算法改进类似于狗、大猫和模糊图片的误差类别，而在本章你学习了如何通过修正数据的标签来处理误标注的样本。

不论你使用什么方式修正开发集标签，请记住，要将同样的方式应用于你的测试集，这可以保持二者服从相同的分布。我们在第 6 章中已经讨论过了这样操作的意义，在那个问题中，团队成员花费了很多时间优化开发集，直到后来才意识到他们正在根据不同的测试集以不同的标准进行判断。

如果你决定改进标签质量，请仔细检查系统中被误分类和正确分类的样本的标签。对于某个样本，可能原始标注和算法预测的标签都是错误的。如果你仅修复系统误分类的样本标签，则有可能在评估中引入偏差。假设你有 1000 个开发集样本，同时分类准确率为 $98 \%$ ，那么检查20 个误分类的样本会比检查 980 个正确分类的样本要容易得多。由于在实际操作中，检查误分类样本较为容易，这就导致了开发集会衍生出一定的偏差。如果你只对开发产品和应用感兴趣，如此程度的偏差是可以接受的。但如果你计划在学术论文中引用此结果，可能就需要完全无偏地度量测试集的精度，此时这样做就会出现问题。

# 17 将大型开发集拆分为两个子集，专注其一

假设你有一个含有 5000 个样本的大型开发集，该开发集的误差为 $20 \%$ ，这意味着算法将误分类 1000 张开发集图片。人工手动检查这 1000 张图片会花费很长时间，所以我们在误差分析时没必要使用所有的图片。

在这种情况下，我会将开发集明确地分成两个子集，但只人工检查其中的一个。你将使得那些经人工查看的图片更快地过拟合，而另一部分没有被人工查看的图片可以在调参时使用。

![](images/5840f2c8172a0119113e9c40dc9f323f5eccfee9a18b15c60c1e207bf0013933.jpg)

继续上面的例子：在该例子中，算法将 5000 个开发集样本中 的1000 个样本误分类。假设我们想要对经人工检查的大约 100 个错误样本（整体的 $10 \%$ ）进行误差分析，你应该随机选择$10 \%$ 的开发集，并将其放入 Eyeball 开发集中（译者注：直译为眼球不妥，保留原文），以提醒我们自己，我们正在用眼睛看着它。（对于语音识别项目，你可能需要听一些音频的剪辑，此时则将数据集称为 Ear 开发集）。因此 Eyeball 开发集将有 500 个样本，其中，预计被算法误分类样本约 100 个。

![](images/a165e14d9b5d6a016105a496019ff82739a52bc483adda8bcfe18218842e5840.jpg)

第二个子集叫做 Blackbox 开发集（直译为黑箱，保留原文），它由剩余的 4500 个样本组成。你可以使用 Blackbox 开发集，通过测量错误率来自动评估分类器，也可以使用它来选择算法或调整超参数。但是，你应该避免将目光局限于此。我们使用术语“ Blackbox ”是因为我们只使用该子集来获得分类器的“ Blackbox ”评价。

为什么我们要把开发集明确分为 Eyeball 开发集和 Blackbox 开发集呢？因为当你在 Eyeball开发集中建立了对样本的直观认识之后，可使得Eyeball开发集更快地过拟合。如果你发现

Eyeball 开发集的性能比 Blackbox 开发集提升得更快，说明已经过拟合到 Eyeball 开发集了。此时可能需要丢弃它并寻找一个新的 Eyeball 开发集，比如可以将更多 Blackbox 开发集中的样本移到 Eyeball 开发集中，也可以考虑去获取新的标注数据。

将开发集明确地分为 Eyeball 和 Blackbox 开发两个子集将很有帮助，它使你了解在人为的误差分析过程中 Eyeball 开发集何时开始发生过拟合。

# 18 Eyeball 和 Blackbox 开发集该设置多大？

Eyeball 开发集应该大到能够让你对算法的主要错误类别有所察觉。如果你正在处理一项实际情况中人类表现良好的任务（例如识别图像中的猫），下面是一些粗略的指导方案：

• 如果分类器在 Eyeball 开发集上只出现 10 次误判，这个开发集就有点小了。若只有 10 个错误样本，很难准确估计不同错误类别的影响。但如果数据非常少且无法提供更多 Eyeball开发集样本时，聊胜于无，这将有助于确立项目的优先级。  
• 如果分类器在 Eyeball 开发集样本上出现了约 20 次误判，你将可以开始大致了解主要的误差来源。  
• 如果有约 50 个错误样本，你将能够比较好地了解主要的误差来源。  
• 如果有约 100 个错误样本，你将会非常清楚主要的误差来源。我见过有人手动分析更多的错误样本——有时候多达500个。只要你有足够多的数据，这将是无害的。

假设你的分类器有 $5 \%$ 的错误率。为了确保在 Eyeball 开发集中有约 100 个误分类的样本，样本开发集应该有约 2000 个样本（因为 $0 . 0 5 \times 2 0 0 0 = 1 0 0 )$ ）。分类器的错误率越低，为了获得足够多的错误样本进行误差分析，需要的 Eyeball 开发集就越大。

如果你正在处理一项，实际情况中人类也无法很好完成的任务，那么检查 Eyeball 开发集将不会有大的帮助，因为很难找出算法不能正确分类一个样本的原因。此时你可能也不需要建立Eyeball 开发集，我们会在后面的章节讨论诸如此类问题的指导方案。

Blackbox 开发集该有多大呢？我们先前提到，开发集有约 1000-10000 个样本是正常的 。完善一下该陈述，一个有 1000-10000 个样本的 Blackbox 开发集通常会为你提供足够的数据去调超参和选择模型，即使数据再多一些也无妨。而含有 100 个样本的 Blackbox 开发集虽然比较小，但仍然是有用的。

如果开发集较小，那么你可能没有足够的数据将其分成足够大的 Eyeball 开发集和 Blackbox开发集来满足目的。相反，你的整个开发集可能需要用作 Eyeball 开发集——即，你将手动检查所有的开发集数据。

在 Eyeball 和 Blackbox 开发集之间，我认为 Eyeball 开发集更加重要（假设你正在研究一个人类能够很好解决的问题，检查这些样本能使得你更有洞悉力）。如果你只有一个 Eyeball 开发集，你可以在这个开发集上进行误差分析、模型选择和超参数调整，缺点是过拟合开发集的风险更大。

如果你有充足的数据，那么 Eyeball 开发集的大小将主要取决于你能够手动分析样本的时间。实际上，我很少看到有人手动分析超过 1000 个错误样本。

# 19 小结：基础误差分析

• 当你开始一个新项目，尤其是在一个你不擅长的领域开展项目时，很难正确预判出昀有前景的方向。  
• 所以，不要在一开始就试图设计和构建一个完美的系统。相反，应尽可能快（例如在短短几天内）地构建和训练一个系统雏形。然后使用误差分析法去帮助你识别出昀有前景的方向，并据此不断迭代改进你的算法。  
• 通过手动检查约 100 个被算法错误分类的开发集样本来执行误差分析，并计算主要的错误类别。使用这些信息来确定优先修正哪种类型的错误。  
• 考虑将开发集分为人为检查的 Eyeball 开发集和非人为检查的 Blackbox 开发集。如果在Eyeball 开发集上的性能比在 Blackbox 开发集上好很多，说明你已过拟合 Eyeball 开发集，下一步应该考虑为其获取更多数据。  
Eyeball 开发集应该足够大，以便于算法有足够多的错误分类样本供你分析。对大多数应用来说，含有1000-10000个样本的 Blackbox 开发集已足够。  
• 如果你的开发集不够大，无法按照这种方式进行拆分，那么就使用 Eyeball 开发集来执行人工误差分析、模型选择和调超参。

# 偏差与方差

# 20 偏差和方差：误差的两大来源

假设你的训练集、开发集和测试集都来自相同的分布，那么每次你都应该试图去获取更多的训练数据，因为这样能单独提高性能，对吗？

拥有更多的数据是无害的，然而它并不总是如我们期望的那样有帮助。有时获取更多的数据可能是在浪费时间。那么，何时才应该添加数据呢？

机器学习中有两个主要的误差来源：偏差和方差。理解它们将协助你决定是否该添加数据，并依此合理安排时间去执行其它的策略来提升性能。

假设你希望构建一个误差为 $5 \%$ 的猫识别器。而目前的训练集错误率为 $1 5 \%$ ，开发集错误率为 $16 \%$ 。在这种情况下，添加数据对结果可能不会有太多帮助。你应该关注其他改进策略。实际上，在你的训练集上添加过多样本只会让你的算法难以在训练集上做的更好。（我们在后面章节中解释了原因）

如果你在训练集上的错误率是 $1 5 \%$ （即 $85 \%$ 的精度），但你的目标是 $5 \%$ 错误率（ $9 5 \%$ 精度），那么首先要解决的问题是提高算法在训练集上的性能。算法在开发/测试集上的性能通常比在训练集上要差。所以，如果算法在已知样本上达到了 $85 \%$ 的精度，那么是不可能在未知样本上达到 $9 5 \%$ 精度的。

如上所述，假设你的算法在开发集上有 $16 \%$ 的错误率（ $84 \%$ 精度），我们将这 $16 \%$ 的错误率分为两部分：

• 第一部分是算法在训练集上的错误率。在本例中，它是 $1 5 \%$ 。我们非正式地将它作为算法的偏差（bias）。  
• 第二部分指的是算法在开发集（或测试集）上的表现比训练集上差多少。在本例中，开发集表现比训练集差 $1 \%$ 。我们非正式地将它作为算法的方差（variance）。 6

一些学习算法的优化能解决误差来源的第一个部分——偏差，并且提高算法在训练集上的性能；而另一些优化能解决第二个部分——方差，并帮助算法从训练集到开发/测试集上更好地泛化。为了选择昀有成效的改变方向，了解二者哪一方更需解决是很有用的。

建立对偏差和方差的良好直觉将帮助你为算法选择出有效的改进策略。

# 21 偏差和方差举例

思考一下，我们的“猫分类”任务目标：一个“理想的”分类器（比如人类）在这个任务中能够取得近乎完美的表现。

假设你的算法表现如下：

训练错误率 $= 1 \%$   
● 开发错误率 $= 1 1 \%$

这其中存在什么问题呢？根据前一章的定义，我们估计偏差为 1%，方差为 10%（=11%-1%）。因此，它有一个很高的方差（high variance）。虽然分类器的训练误差非常低，但是并没有成功泛化到开发集上。这也被叫做过拟合（overfitting）

接下来，考虑如下情况：

训练错误率 $= 1 5 \%$   
● 开发错误率 $= 1 6 \%$

我们估计偏差为 $1 5 \%$ ，方差为 1%。该分类器的错误率为 15%，没有很好地拟合训练集，但它在开发集上的误差不比在训练集上的误差高多少。因此，该分类器具有较高的偏差（highbias），而方差较低。我们称该算法是欠拟合（underfitting）的。

下面，考虑如下情况：

训练错误率 $= 1 5 \%$   
● 开发错误率 $= 3 0 \%$

我们估计偏差为 $1 5 \%$ ，方差为 $1 5 \%$ 。该分类器有高偏差和高方差（high bias and highvariance）：它在训练集上表现得很差，因此有较高的偏差，而它在开发集上表现更差，因此方差同样较高。由于该分类器同时过拟合和欠拟合，过拟合/欠拟合术语很难准确应用于此。

昀后，考虑如下情况：

训练错误率 $= 0 . 5 \%$   
● 开发错误率 $= 1 \%$

该分类器效果很好，它具有低偏差和低方差。恭喜获得这么好的表现！

# 22 与昀优错误率比较

在我们的“猫咪识别”案例中，“理想”错误率——即一个“昀优”分类器应该达到的值——接近 $0 \%$ 。在几乎所有情况下，人类总是可以识别出图片中的猫。因此，我们希望机器也能够有这样优秀的表现。

若换作其他问题，难度则更大：假设你正在构建一个语音识别系统，并发现 $14 \%$ 的音频片段背景噪声太多，或者十分难以理解，导致即使是人类也无法识别出所说的内容。在这种情况下，即使是“昀优”的语音识别系统也可能约有 $14 \%$ 的误差。

假设在这个语音识别问题上，你的算法达到：

训练错误率 $= 1 5 \%$   
● 开发错误率 $= 3 0 \%$

算法在训练集上的表现已经接近昀优错误率 $14 \%$ ，因此在偏差上或者说在训练集表现上没有太大的提升空间。然而，算法没有很好地泛化到开发集上，在方差造成的误差上还有很大的提升空间。

这个例子和前一章的第三个例子类似，都有 $1 5 \%$ 的训练错误率和 $30 \%$ 的开发错误率。如果昀优错误率接近 $0 \%$ ，那么 $1 5 \%$ 的训练错误率则留下了很大的提升空间，这表明降低偏差可能有益。但如果昀优错误率是 $14 \%$ ，那么 $1 5 \%$ 的训练错误率表现告诉我们，在分类器的偏差方面几乎没有改进的余地。

对于昀佳错误率远超零的状况，有一个对算法误差更详细的分解。继续使用上述语音识别的例子，可以将 $30 \%$ 的总开发集误差分解如下（类似的分析可以应用于测试集误差）：

昀优错误率（“不可避免偏差”）： $14 \%$ 。假设我们决定，即使是世界上昀好的语音系统，仍会有 $1 4 \%$ 的误差。我们可以将其认为是学习算法的偏差“不可避免”的部分。  
● 可避免偏差： $1 \%$ 。即训练错误率和昀优误差率之间的差值。 8  
● 方差： $1 5 \%$ 。即开发错误和训练错误之间的差值。

为了将这与我们之前的定义联系起来，偏差和可避免偏差关系如下：9

偏差 $=$ 昀佳误差率（“不可避免偏差”） $^ +$ 可避免的偏差

这个“可避免偏差”反映了算法在训练集上的表现比起“昀优分类器”差多少。

方差的概念和之前保持一致。理论上来说，我们可以通过训练一个大规模训练集将方差减少到接近零。因此只要拥有足够大的数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不可避免方差”。

再考虑一个例子，该例子中昀优错误率是 $14 \%$ ，我们有：

训练误差 $= 1 5 \%$   
● 开发误差 $= 1 6 \%$

我们在前一章称之为高偏差分类器，现在可避免的偏差误差是 $1 \%$ ，方差误差约为 $1 \%$ 。因此，算法已经做的很好了，几乎没有提升的空间。它只比昀佳错误率低 $2 \%$ 。

从这些例子中我们可以看出，了解昀优错误率有利于指导我们的后续工作。在统计学上，昀优错误率也被称为​贝叶斯错误率（Bayes error rate），或贝叶斯率。

如何才能知道昀优错误率是多少呢？对于人类擅长的任务，例如图片识别或音频剪辑转录，你可以让普通人提供标签，然后测评这些人为标签相对于训练集标签的精度，这将给出昀优错误率的估计。如果你正在解决一项人类也很难解决的问题（例如预测推荐什么电影，或向用户展示什么广告），这将很难去估计昀优错误率。

在“与人类表现比较”（第33~35章）这一节中，我将更详细地讨论学习算法的表现和人类表现相比较的过程。

在前面几个章节中，你学习了如何通过查看训练集和开发集的错误率来估计可避免/不可避免的偏差和方差。下一章将讨论如何据此来判断该优先减少偏差还是方差。项目当前的问题是高偏差（可避免偏差）还是高方差，将导致你采用截然不同的方法。请继续阅读。

# 23 处理偏差和方差

下面是处理偏差和方差问题昀简单的形式：

如果具有较高的可避免偏差，那么加大模型的规模（例如通过添加层/神经元数量来增加神经网络的大小）。  
● 如果具有较高的方差，那么增加训练集的数据量。

如果你可以加大神经网络的规模且无限制地增加训练集数据，那么许多机器学习问题都可以取得很好的效果。

实际上，不断加大网络的规模使你终将遇到算力问题，因为训练一个大型模型需要很多时间。另外，你也可能会耗尽获取更多训练数据的能力。（即使在网上，猫图片的数量也是有限的）

不同的模型架构（例如不同的神经网络架构）对于你的问题将有不同的偏差/方差值。近期，不少深度学习研究已经开发出很多新的模型架构。所以，如果你在使用神经网络，学术文献可能会是一个很好的灵感来源，在 Github 上也有许多不错的开源实现。但尝试新架构的结果要比简单地加大模型规模或添加数据的形式更难以预测。

加大模型的规模通常可以减少偏差，但也可能会增加方差和过拟合的风险。然而，这种过拟合风险通常只在你不使用正则化技术的时候出现。如果你的算法含有一个精心设计的正则化方法，通常可以安全地加大模型的规模，而不用担心增加过拟合风险。

假设你正在应用深度学习方法，使用了 L2 正则化和 dropout 技术，并且设置了在开发集上表现昀好的正则化参数。此时你加大模型规模，算法的表现往往会保持不变或提升；它不太可能明显地变差。这种情况下，不使用更大模型的唯一原因就是这将使得计算代价变大。

# 24 偏差和方差间的权衡

你可能听过“偏差和方差间的权衡”。目前，在大部分针对学习算法的改进中，有一些能够减少偏差，但代价是增大方差，反之亦然。于是在偏差和方差之间就产生了“权衡”。

例如，加大模型的规模（在神经网络中增加神经元/层，或增加输入特征），通常可以减少偏差，但可能会增加方差。另外，加入正则化一般会增加偏差，但能减少方差。

在现代，我们往往能够获取充足的数据，并且可以使用非常大的神经网络（深度学习）。因此，这种权衡的情况比较少，并且现在有更多的选择可以在不损害方差的情况下减少偏差，反之亦然。

例如，一般情况下，你可以通过增加神经网络的规模大小，并调整正则化方法去减少偏差，而不会明显的增加方差。通过增加训练数据，你也可以在不影响偏差的情况下减少方差。

如果你选择了一个非常契合任务的模型架构，那么你也可以同时减少偏差和方差。只是选择这样的架构可能有点难度。

在接下来的几个章节中，我们将讨论处理偏差和方差的其它特定技术。

# 25 减少可避免偏差的技术

如果你的学习算法存在着很高的可避免偏差，你可能会尝试以下方法：

● 加大模型规模（例如神经元/层的数量）：这项技术能够使算法更好地拟合训练集，从而减少偏差。当你发现这样做会增大方差时，通过加入正则化可以抵消方差的增加。  
● 根据误差分析结果修改输入特征：假设误差分析结果鼓励你增加额外的特征，从而帮助算法消除某个特定类别的误差。（我们会在接下来的章节深入讨论这个话题。）这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；当这种情况发生时，你可以加入正则化来抵消方差的增加。  
● 减少或者去除正则化（L2 正则化，L1 正则化，dropout）：这将减少可避免偏差，但会增大方差。  
● 修改模型架构（比如神经网络架构）使之更适用于你的问题：这将同时影响偏差和方差。

有一种方法并不能奏效：

● 添加更多的训练数据：这项技术可以帮助解决方差问题，但它对于偏差通常没有明显的影响。

# 26 训练集误差分析

你的算法必须先在训练集上表现得很好，才能期望它在开发集和测试集上也有良好的表现。

除了先前提到的用于处理高偏差的技术外，我通常也会在训练数据上进行误差分析，处理方式类似于在开发集上设置一个 Eyeball 开发集。当你的算法有着高偏差时（例如算法没能很好的拟合训练集）这将有所帮助。

举个例子，假设你正在为一个应用程序构建一个语音识别系统，并收集了一组志愿者提供的音频片段。如果系统在训练集上表现不佳，你可能会考虑选取那些算法处理得很差的样本，以100 个左右作为一组并人为去听它们，从而知道训练集误差的主要种类。类似于开发集上的误差分析，你可以计算不同类别的错误样本数量：

<table><tr><td>音频片段</td><td>背景噪音很大</td><td>语速太快</td><td>距离麦克风太远</td><td>备注</td></tr><tr><td>1</td><td>✓</td><td></td><td></td><td>汽车噪音</td></tr><tr><td>2</td><td>✓</td><td></td><td>✓</td><td>餐馆噪音</td></tr><tr><td>3</td><td></td><td>✓</td><td>✓</td><td>在起居室里喊叫?</td></tr><tr><td>4</td><td>✓</td><td></td><td></td><td>咖啡厅</td></tr><tr><td>占全体比例</td><td>75%</td><td>25%</td><td>50%</td><td></td></tr></table>

在本例中，你可能会发现算法在处理具有大量背景噪音的训练样本时遇到了困难。因此你可能会关注一些技术，使其能够更好地适应包含背景噪音的训练样本。

你也可以仔细检查正常人是否能转录这些音频片段，这些音频应该与你的学习算法的输入音频相同。如果背景噪音过于嘈杂，导致任何人都无法理解音频里说了什么，那么期望算法正确地识别这样的话语就不太合理。我们会在后面的章节中讨论，将算法的性能与人类水平进行比较的好处。

# 27 减少方差的技术

如果你的学习算法存在着高方差问题，可以考虑尝试下面的技术：

添加更多的训练数据：这是昀简单昀可靠的一种处理方差的策略，只要你有大量的数据和对应的计算能力来处理他们。  
● 加入正则化（L2 正则化，L1 正则化，dropout）：这项技术可以降低方差，但却增大了偏差。  
● 加入提前终止（例如根据开发集误差提前终止梯度下降）：这项技术可以降低方差但却增大了偏差。提前终止（Early stopping）有点像正则化理论，一些学者认为它是正则化技术之一。  
● 通过特征选择减少输入特征的数量和种类​：这种技术或许有助于解决方差问题，但也可能增加偏差。稍微减少特征的数量（比如从 1000 个特征减少到 900 个）也许不会对偏差产生很大的影响，但显著地减少它们（比如从 1000 个特征减少到 100 个，10 倍地降低）则很有可能产生很大的影响，你也许排除了太多有用的特征。在现代深度学习研究过程中，当数据充足时，特征选择的比重需要做些调整，现在我们更可能将拥有的所有特征提供给算法，并让算法根据数据来确定哪些特征可以使用。而当你的训练集很小的时候，特征选择是非常有用的。  
● 减小模型规模（比如神经元/层的数量）：谨慎使用 这种技术可以减少方差，同时可能增加偏差。然而我不推荐这种处理方差的方法，添加正则化通常能更好的提升分类性能。 减少模型规模的好处是降低了计算成本，从而加快了你训练模型的速度。如果加速模型训练是有用的，那么无论如何都要考虑减少模型的规模。但如果你的目标是减少方差，且不关心计算成本，那么考虑添加正则化会更好。

下面是两种额外的策略，和解决偏差问题章节所提到的方法重复：

● 根据误差分析结果修改输入特征：假设误差分析的结果鼓励你创建额外的特征，从而帮助算法消除某个特定类别的误差。这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；当这种情况发生时，加入正则化，这可以消除方差的增加。  
● 修改模型架构（比如神经网络架构）使之更适用于你的问题：这项策略将同时影响偏差和方差。

# 学习曲线

# 28 诊断偏差与方差：学习曲线

我们现在已经知道了一些方法，来估计可避免多少由偏差与方差所导致的误差——人们通常会估计昀优错误率，以及算法在训练集和测试集上的误差。下面让我们来讨论一项更有帮助的技术：绘制学习曲线。

学习曲线可以将开发集的误差与训练集样本的数量进行关联比较。想要绘制出它，你需要设置不同大小的训练集运行算法。假设有 1000 个样本，你可以选择在规模为 100、200、300 ...1000 的样本集中分别运行算法，接着便能得到开发集误差随训练集大小变化的曲线。下图是一个例子：

![](images/f30de0034dcfde1b8c6026181cf31d491a6465c7606ede71eb362df2e6234a5b.jpg)

随着训练集大小的增加，开发集误差应该降低。

我们通常会设置一些“期望错误率”，并希望学习算法昀终能够达到该值。例如：

● 如果希望算法能达到人类水平的表现，那么人类错误率可能就是“期望错误率”。  
如果学习算法为某些产品提供服务（如提供猫咪图片），我们或许将主观感受到需什么样的水平才能给用户提供出色的体验。  
● 如果你已经从事一项应用很长时间，那么你可能会有一种直觉，预判在下一个季度里你会有多大的进步。

将期望的表现水平添加到你的学习曲线中：

![](images/16e680778943c98847b0ddefd096d703e13acf43988c67a4521accb36dc9935c.jpg)

显而易见，你可以根据红色的“开发误差”曲线的走势来推测，在添加一定量的数据后，曲线距离期望的性能接近了多少。在上面的例子中，将训练集的大小增加一倍可能会让你达到期望的性能，这看起来是合理的。

![](images/68b984c76ec7a5048e6cb1df1a8c3a523cdd98bdbe3dc343567ef0010db796b4.jpg)

观察学习曲线或许能帮到你，避免花费几个月的时间去收集两倍的训练数据，到头来却发现这并不管用的情况。

该过程的一个缺点是，如果你只关注了开发错误曲线，当数据量变得越来越多时，将很难预测后续红色曲线的走向。因此我们会选择另外一条曲线来协助评估添加数据所带来的影响：即训练误差曲线。

# 29 绘制训练误差曲线

随着训练集大小的增加，开发集（和测试集）误差应该会降低，但你的训练集误差往往会随之增加。

让我们来举例说明一下。假设你的训练集只有两个样本：一张猫图和一张非猫图。学习算法很轻易地就可以“记住”训练集中这两个样本，并且训练集错误率为 $0 \%$ . 即使有一张或两张的样本图片被误标注了，算法也能够轻松地记住它们。

现在假设你的训练集包含 100 个样本，其中有一些样本可能被误标记，或者是模棱两可的（图像非常模糊），所以即使是人类也无法分辨图中是否有一只猫。此时，或许学习算法仍然可以“记住”大部分或全部的训练集，但很难获得 $100 \%$ 的准确率。通过将训练集样本数量从 2个增加到 100 个，你会发现训练集的准确率会略有下降。

下面我们将训练误差曲线添加到原有的学习曲线中：

可以看到，蓝色的“训练误差”曲线随着训练集大小的增加而上升，而且算法在训练集上通常比在开发集上表现得更好；因此，红色的开发误差曲线通常严格位于蓝色训练错误曲线之上。

让我们来讨论一下如何解释这些曲线的含义。

# 30 解读学习曲线：高偏差

假设你的开发误差曲线如下图所示：

![](images/61f72728b5d13f1b3bc44a2c2993df288405c42a93098ec6226db2968b0013f1.jpg)

我们之前提到，如果开发误差曲线趋于平稳，则不太可能通过添加数据来达到预期的性能，但也很难确切地知道红色的开发错误曲线将趋于何值。如果开发集很小，或许会更加不确定，因为曲线中可能含有一些噪音干扰。

假设我们把训练误差曲线加到上图中，可以得到如下结果：

![](images/985fc1f80a7ac6b327478244d0329cac3197242bbcd7b238294bb156c2e9e779.jpg)

现在可以绝对肯定地说，添加更多的数据并不奏效。为什么呢？记住我们的两个观察结果：

● 随着我们添加更多的训练数据，训练误差变得更糟。因此蓝色的训练误差曲线只会保持不动或上升，这表明它正远离期望的性能水平（绿色的线）。

● 红色的开发误差曲线通常要高于蓝色的训练误差曲线。因此只要训练误差高于期望性能，通过添加更多数据来让红色开发误差曲线下降到期望性能水平之下也基本没有可能。在同一张图中检查开发误差曲线和训练误差曲线可以让我们对推测开发误差曲线的走势更有信心。

为了便于讨论，假设期望性能是我们对昀优错误率的估计。那么上面的图片就是一个标准的“教科书”式的例子（具有高可避免偏差的学习曲线是什么样的）：在训练集大小的昀大处（大致对应使用我们所有的训练数据），训练误差和期望性能之间有较大的间隙，这代表可避免偏差较大。此外，如果训练曲线和开发曲线之间的间隙小，则表明方差小。

之前，我们只在曲线昀右端的点去衡量训练集误差和开发集误差，这与使用所有的可训练数据训练算法相对应。绘制完整的学习曲线将为我们呈现更全面的结果图片，显示算法在不同训练集大小上的表现。

# 31 解读学习曲线：其它情况

考虑下面的学习曲线：

![](images/604e8dc23f58fe4a59c963e9f76a29ba1cafa3f3fd9a20d07206492b5b1f4c49.jpg)

这张图表示的是高偏差？高方差？还是两者都有？

蓝色训练误差曲线相对较低，红色的开发误差曲线比蓝色训练误差高得多。因此，偏差很小，但方差很大。添加更多的训练数据可能有助于缩小开发误差和训练误差之间的差距。

现在考虑下面的情况：

![](images/7d9373f85ec945f7b6e2a445e970aab03834f939646443537e12bb2fc6e5d0ca.jpg)

这种情况下，训练误差很大，它比期望的性能水平要高得多，开发误差也比训练误差大得多。因此，学习算法有着明显的偏差和方差。此时你必须找到一种方法来减少算法中的偏差和方差。

# 32 绘制学习曲线

假设你有一个非常小的训练集，仅有 100 个样本 。那么你可以从中随机选择 10 个样本来训练你的算法，然后是 20 个，30 个，100 个，每次增加 10 个样本。然后使用 10 个数据点来绘制你的学习曲线。你可能会发现，在较小规模的训练集上，曲线看起来带有点噪声（这意味着这些值比预期的要高/低）。

当只使用 10 个随机选择的样本进行训练时，你可能会不幸碰到特别“糟糕”的训练集，比如含有很模糊的或者误标记的样本。你当然也有可能会幸运地碰到特别“棒”的训练集。训练集的规模较小意味着开发和训练误差将随机波动。

如果你的机器学习应用程序很倾向于某一个类（如猫分类任务的负面样本比例远远大于正面样本），或者说有大量的类（如识别 100 种不同的动物物种），那么选择一个“非代表性”或糟糕的特殊训练集的几率也将更大 。例如，假设你的整个样本中有 $80 \%$ 是负样本（ $\scriptstyle \cdot y = 0$ ），只有$20 \%$ 是正样本（ $\cdot y = 1$ ），那么一个含有 10 个样本的训练集就有可能只包含负样本，因而算法很难从中学到有意义的东西。

存在训练集噪声致使难以正确理解曲线的变化时，有两种解决方案：

● 与其只使用 10 个样本训练单个模型，不如从你原来的 100 个样本中进行随机有放回抽样10，选择几批（比如 3-10 ）不同的 10 个样本进行组合。在这些数据上训练不同的模型，并计算每个模型的训练和开发错误。昀终，计算和绘制平均训练集误差和平均开发集误差。  
如果你的训练集偏向于一个类，或者它有许多类，那么选择一个“平衡”子集，而不是从100 个样本中随机抽取 10 个训练样本。例如，你可以确保这些样本中的 2/10是正样本，8/10 是负样本。更常见的做法是，确保每个类的样本比例尽可能地接近原始训练集的总体比例。

除非你已经尝试过绘制学习曲线，并得出了曲线太过嘈杂且无法看到潜在趋势的结论，否则我将不会考虑使用这两种技术。因为当你的训练集规模很大——比如超过 10000 个样本——而且类分布不是很倾斜时，你可能就不需要这些技巧了。

昀后提一点，绘制一个学习曲线的成本可能非常高：例如，你可能需要训练 10 个模型，其中样本规模可以是 1000 个，然后是 2000 个，一直到 10000 个。使用小数据集训练模型比使用大型数据集要快得多。因此，你可以用 1000、2000、4000、6000 和 10000 个样本来训练模

型，而不是像上面那样将训练集的大小均匀地间隔在一个线性的范围内。这仍然可以让你对学习曲线的变化趋势有一个清晰的认识。当然，这种技术只有在训练所有额外模型所需的计算成本很重要时才有意义。

# 与人类表现水平对比

# 33 为何与人类表现水平进行对比

许多机器学习系统的设计目的是想要自动化一些人类可以处理得很好的事情，可举的例子有图像识别、语音识别以及垃圾邮件分类等等。学习算法进步如此之快，有许多类似任务的处理已经超过了人类的表现水平。

此外，有许多理由表明在处理人类擅长的任务时，构建一个机器学习系统会更加简单：

1. 易于从人为标签中获取数据。​举个例子，由于人类可以很好地识别图片中的猫，因此让人们为你的学习算法提供高精度的带标签数据也很方便。  
2. 基于人类直觉进行误差分析。假设某个语音识别系统的表现要低于人类的表现水平。比如错误地将音频片段 “This recipe calls for a pear of apples” 中的 “pair” 认为是 “pear”.此时你可以利用人类的直觉来尝试理解，普通人会利用何种信息来获取正确的转录内容，并且试着修改你的学习算法，使它在相同的知识点上有着更好的表现。  
3. 使用人类表现水平来估计昀优错误率，并设置可达到的“期望错误率”。 假设你的算法在某个任务上达到了 $10 \%$ 的误差，但普通人所能达到的误差是 $2 \%$ . 由此我们就可以知道昀优错误率是 $2 \%$ 或更低，这也表明可避免偏差至少是 $8 \%$ . 所以你应当尝试一下降低偏差的技术。更一般地说，有一个合理可实现的“期望错误率”可以帮助你去估计学习算法的可避免偏差。这反过来也帮你决定是否使用误差降低技术。

尽管第三点可能听起来不太重要，但我发现有一个合理且可实现的目标错误率有助于加快团队的进度。知道你的算法有很高的可避免偏差是非常有价值的，它将展开一个具有多个选项的菜单供你去尝试。

连人类都不擅长的任务也是存在的。比如向你推荐一本书，或者在某个网站上选择向用户展示的广告，或者对股票市场进行预测。如今计算机在此类任务上的表现已经远超过人类。对于这些应用，我们主要面临着下面的问题：

● 获取标签数据很难。 比如很难去获取用户数据库，并要求人工标记者使用“昀优”的书籍标签对数据库进行注释，从而向用户推荐书籍。如果你正在负责一个书籍销售网站或者是 APP 的运营，你可以通过向用户展示书籍并查看他们的购买记录来获取数据。可当你没有这样一个网站时，就需要去找到一些更具创意的方法来获取数据了。  
人类的直觉难以依靠。例如，几乎没有人能准确地预测股票市场。因此当我们的股票预测算法比随机猜测的表现还要差时，很难弄清楚要如何去改进它。  
昀优错误率和合理的期望错误率难以确定。假设你已经有了一个很好的图书推荐系统。如果没有人类水平作为参考，你怎么知道它还能改善多少呢？

# 34 如何定义人类表现水平

假设你正在做一个医学成像应用程序，它可以自动依据X射线图像进行诊断。 除了一些基础的训练外，一个没有任何医学背景的人在该任务上的错误率为 $1 5 \%$ . 一名新手医生的错误率为$10 \%$ ，而经验丰富的医生可以达到 $5 \%$ . 如果由小型的医生团队对每一幅图像进行单独的讨论，错误率将降低至 $2 \%$ . 上述的哪一种错误率可以定义为“人类表现水平”呢？

在该情景下，我将使用 $2 \%$ 作为人类表现水平的代表来获得昀优错误率。 你还可以将 $2 \%$ 设置为期望的性能水平，因为与人类表现水平相比，前一章的所有三个理由都适用：

易于从人为标签中获取数据。你可以让一组医生为你提供错误率为 $2 \%$ 的标签。  
● 基于人类直觉进行误差分析。通过与医生讨论图像内容，你可以利用他们的直觉。  
● 使用人类表现水平来估计昀优错误率，并设置可达到的“期望错误率”。 使用 $2 \%$ 的误差作为我们对昀优错误率的估计是合理的。昀优错误率甚至可能低于 $2 \%$ ，但它不可能更高，因为一个医生团队就已经有可能达到 $2 \%$ 的误差水平。相比之下，使用 $5 \%$ 或$10 \%$ 作为昀优错误率的估计就显得不合理了，因为我们知道这些估算值必然过高了。

当需要获得标签数据时，你可能不希望与整个团队讨论每一张图片，因为他们的时间很宝贵。或许你可以让新手医生给绝大多数的病例贴上标签，而把那些较难分析的病例交给更有经验的医生或医生团队。

如果你的系统目前的误差为 $40 \%$ ，那么不论是让初级医生（ $10 \%$ 误差）还是有经验的医生（$5 \%$ 误差误）来给你的数据贴上标签，那都没有关系。是如果你的系统误差已经是 $10 \%$ ，那么将人类表现水平定义为 $2 \%$ 将为你提供更好的途径来改进你的系统。

# 35 超越人类表现水平

现在你在做一个语音识别项目，并且有一个音频片段数据集。假设数据集里有许多的噪声，导致即使是人类来识别也会有 $10 \%$ 的误差。同时假设你的算法已经达到了 $8 \%$ 的误差，你能够使用第 33 章中提到的三种技术来继续取得快速的进展吗？

如果你能找到人类表现水平远超现有系统的数据子集，使用那些技术来驱动进则仍然可行。举个例子，假设你的系统在处理识别含有噪音的音频任务时表现已经优于人类，然而在转录语速很快的语音时人类仍然占有优势。

对于语速很快的语音数据子集：

1. 你仍可以从输出质量比你的算法高的人那儿获取转录数据。  
2. 你可以利用人类的直觉来理解，为什么你的系统没能够识别这些数据，而人类做到了。  
3. 你可以使用该子集上的人类表现水平作为期望表现目标。

更常见的做法是，只要在开发集上存在着一些人类能正确处理而算法不能的样本，前面提到的技术就能够被应用。即使你的算法在整个开发集或是测试集上的表现已经超过了人类，这样做也是正确的。

在许多重要的机器学习应用程序中，机器已经超越了人类的水平。例如，机器可以更好地预测电影分级，一辆送货车到某个地方需要多长时间，或者是否批准贷款申请。只有当人类很难识别出算法明显出错的样本时，一小部分技术才可以被应用。因此在机器已经超越人类水平的问题上，进展通常比较慢，而当机器仍在试图赶上人类水平时，进展速度反而更快。

# 在不同的分布上训练与测试

# 36 何时在不同的分布上训练与测试

假设用户已经向你的猫咪图片程序上传了 10000 张图片，且图片已被人为标记为含有猫与不含猫两类。同时你也从互联网上下载了规模更大的 200000 张图片集，此时训练集、测试集与开发集应该如何定义呢？

由于用户的 10000 张图片密切地反映了你想要处理的数据的实际概率分布，因此你可以将它们作为开发集与测试集。如果你正在训练一个数据量饥渴的深度学习算法，则可能需要使用额外的 200000 张网络图片来进行训练。这样的话，你的训练集与开发集/测试集将服从不同的概率分布。这对你的工作会有什么影响呢？

除了将数据集直接划分为训练集、开发集和测试集这一做法外，我们还能将所有的 210000 张已有图片先进行整合，接着随机打乱它们，再进行划分。经过这样的处理，所有的数据都将服从相同的分布。但我建议你不要使用这种方法，因为这样大约 $9 7 . 6 \%$ （205,000/210,000）的开发/测试数据将来自于互联网图像，这并不能反映出你想要处理数据的实际分布。请记住我们关于选择开发/测试集的建议：

选择开发集和测试集以反映你在将来想要正确处理的数据。

大多数关于机器学习的学术文献都假定训练集、开发集和测试集都来自于相同的分布。在机器学习的早期，数据是稀缺的。我们通常只有一个服从某些概率分布的数据集 。因此，我11 们会随机地将这些数据分割成训练/开发/测试集，并且假设所有的数据来源相同且满足要求。

但在大数据时代，我们现在可以使用大型的训练集，比如猫的网络图像。即使训练集的分布不同，我们仍然希望使用它来学习，因为它可以提供大量的信息。

对于猫咪检测器的示例，我们不会将用户上传的所有 10000 个图像放到开发/测试集合中，而是将其中 5000 张放入。 这样的话，训练集中的 205000 个样本的分布将来自现有的开发/测试集，以及 200000 张网络图片。我们将在后面的章节中讨论为什么这个方法是有帮助的。

让我们考虑第二个例子。假设你正在建立一个语音识别系统，将某个街道地址转换为一个语音控制的移动地图/导航应用程序。现在你有 20000 个“用户说出街道地址”的样本，但是你也有500000 个其他音频片段的样本，内容是“用户谈论其他话题”。你可能会为开发/测试集合选取10000 个街道地址样本，并使用剩下的 10000 个样本，再加上 50 万个其它音频内容的样本进行训练。

我们将继续假设你的开发数据和测试数据来自相同的分布。但重要的是你要明白，不同的训练和开发/测试集分布将带来一些特殊的挑战。

# 37 如何决定是否使用你所有的数据

假设你的猫咪检测器的训练集包括 10000 张用户上传的图片，这些数据来自相同的数据分布且将作为单独的开发/测试集，同时也代表着你关心的将要处理的数据分布。你还从互联网下载了额外的 20000 张图片。此时你是否应该为你的学习算法提供所有的 $2 0 0 0 0 + 1 0 0 0 0$ 张图片作为它的训练集，或者丢弃这 20000 张网络图片，以免它会影响你的学习算法呢？

在使用早期的学习算法（比如人为设计的计算机视觉特征，然后使用一个简单的线性分类器）时，真正的风险在于：合并这两种类型的数据会导致算法的表现更差。因此，一些工程师会警告你不要加入 20000 张互联网图片。

但是有了现代强大而灵活的学习算法——比如大型的神经网络——这种风险已经大大降低了。如果你能够构建一个有足够多的隐藏单元/层的神经网络，你可以安全地将 20000 张图片添加到你的训练集。此时添加图片则更有可能提升算法的性能。

这种观察依赖于这样一个事实，即有一些 x-y 映射对于这两种类型的数据都很有效。换而言之，有这么些系统可以输入互联网图像或移动应用上的图像，并可靠地预测标签，即使它不知道图像的来源。

添加额外的 20000 张图片会产生以下影响：

1. 它给你的神经网络提供了更多关于猫咪外貌的样本。这是很有帮助的，因为互联网图片和用户上传的移动应用图片确实有一些相似之处。你的神经网络可以将从互联网图像中获得的一些知识应用到移动应用图像中。  
2. 它迫使神经网络花费部分容量来学习网络图像的特定属性（比如更高的分辨率，不同画面结构图像的分布等等）。如果这些属性与移动应用图像有很大的不同，那么它将“耗尽”神经网络的一些表征能力，导致从移动应用图像的分布识别数据的能力就会降低，而这正是你真正关心的东西。从理论上讲，这可能会损害算法的性能。

换一种不同的术语来描述第二个影响，我们可以求助于小说中的人物夏洛克福尔摩斯，他解释道大脑就像一个阁楼；它只有有限的空间。他说，“每增加一个知识，你就会忘记你以前知道的东西。”因此，昀重要的是，不要让无用的事实把有用的真相排挤出去。” 12

幸运的是，如果你有足够的计算能力来构建一个足够大的神经网络——也就是一个足够大的阁楼——那么这就不是一个严重的问题了。你有足够的能力从互联网和移动应用图像中学习，而不会存在两种类型的数据在容量上的竞争。也即是说，你的算法的“大脑”足够大，不必担心会

耗尽阁楼的空间。

但是，如果你没有足够大的神经网络（或者另一个高度灵活的学习算法），那么你应该更加关注训练数据，需要与开发集/测试集的分布相匹配。

如果你认为有些数据没有任何帮助，那么应该将这些数据排除在计算原因之外。例如，假设你的开发/测试集主要包含一些内容是人员、地点、地标、动物的任意图片。同时假设里面有大量的历史文档扫描图片:

![](images/e959a5549e3c6d7debd356cf40447ca31dca23d0236c9b1114138ad6caae6e9b.jpg)

这些文件不包含任何类似猫的东西。它们看起来和开发/测试集的分布完全不同。没有必要将这些数据作为负样本，因为上述第一个影响带来的好处在这种情况下几乎忽略不计——你的神经网络几乎没有任何东西可以从这些数据中学习，但它们可以应用到开发/测试集中，加入它们将会浪费计算资源和神经网络的表征能力。

# 38 如何决定是否添加不一致的数据

假设你想要学习预测纽约市的房价。考虑房子的大小（输入特征 $\mathsf { x }$ ），你需要预测价格（目的标签 y）。纽约市的房价非常高。假设你在密歇根州的底特律有第二个住房价格数据集，就会发现那里的房价要低得多。应该把这些数据包含在你的训练集里吗？

房子的大小 $\pmb { \times }$ 相同，而价格 y 明显不同，这取决于它是在纽约还是在底特律。如果你只关心预测纽约市的房价，把这两个数据集放在一起会影响算法的表现。在这种情况下，昀好忽略不一致的底特律数据 。13

纽约和底特律的样本与移动应用和互联网猫图片的样本有什么不同？

猫咪图像的样本和这有点不一样，因为给定一个输入图片 x ，你能可靠地预测出标签 y （是否有猫），即使不知道图像是网络图像还是移动应用图像。即有一个函数 f（x）可以从输入 x映射到目标输出 y ，即使不知道 $\pmb { \times }$ 的来源。 因此，从互联网图像中识别的任务与移动应用图像识别的任务是“一致的”。这意味着，将所有的数据包括在内，几乎没有什么负面影响（除了计算成本），甚至还可能有一些积极的作用。相比之下，纽约和底特律的数据则不一致。考虑相同的 $\pmb { \times }$ （房子的大小），价格会根据房子的位置而不同。

# 39 给数据添加权重

假设你有 20 万张来自互联网的图片，还有来自移动应用用户的 5000 张照片。数据集的大小之间有一个 40:1 的比率。从理论上讲，只要你建立了一个庞大的神经网络，并在所有205000 张图片上进行足够长的时间训练，那么在网络图像和移动图像上将算法都训练得很好是没有害处的。

但在实际操作中，拥有 40 倍的网络图像可能意味着，相比只使用 5000 张图片，你需要花费40 倍（或更多）的计算资源来对两者进行建模。

如果你没有巨大的计算资源，你可以给互联网图片一个较低的权重作为妥协。

例如，假设优化目标是平方误差（对于分类任务来说这不是一个好的选择，但它将简化解释过程）。因此，我们的学习算法试图优化：

$$
\min  _ {\theta} \sum_ {(x, y) \in \text {M o b i l e I m g}} \left(h _ {\theta} (x) - y\right) ^ {2} + \sum_ {(x, y) \in \text {I n t e r n e t I m g}} \left(h _ {\theta} (x) - y\right) ^ {2}
$$

上面的第一个项是对 5000 个移动应用图像误差求和，第二项对 20 万个互联网图像误差求和。你可以使用一个额外的参数 $\beta$ 进行优化：

$$
\min  _ {\theta} \sum_ {(x, y) \in \text {M o b i l e I m g}} (h _ {\theta} (x) - y) ^ {2} + \beta \sum_ {(x, y) \in \text {I n t e r n e t I m g}} (h _ {\theta} (x) - y) ^ {2}
$$

如果你设置 $\beta = 1 / 4 0$ ，这个算法会对 5000 个移动图像和 20 万个互联网图像给予同等的权重。你还可以将参数 $\beta$ 设置为其他值，也可以类似地对开发集进行调优。

通过对额外的网络图像赋予更少的权重，你不需要构建一个庞大的神经网络来确保算法在这两种类型的任务上都能很好地完成。只有当你怀疑这些额外的数据（网络图像）与开发/测试集分布不一致，或者额外的数据规模比与相同分布的开发/测试集（手机图像）数据规模大得多时，这种类型的权重加权才需要。

# 40 从训练集泛化到开发集

假设你正在将机器学习应用于不同分布的训练集和开发/测试集上。例如，训练集包含了互联网图像 $+$ 移动应用图像，而开发/测试集只包含移动应用图像。然而，该算法运行得不太好：它的开发/测试集误差比想要的要高得多。以下是一些可能出现问题的情况：

1. 它在训练集上表现不佳，这属于训练集分布上的高（可避免）偏差的问题。  
2. 它在训练集上做得很好，但是不能很好地泛化到与训练集分布相同的未知数据，这是高方差问题。  
3. 它能够很好地泛化到与训练集相同分布的未知数据，但不能很好地泛化到与开发/测试集相同分布的未知数据。我们将这种情况称之为​数据不匹配，因为训练集的数据与开发/测试集的数据匹配得相当地糟糕。

例如，假设人类在猫识别任务上取得近乎完美的表现。你的算法实现了：

. ● $1 \%$ 的训练集误差   
● $1 . 5 \%$ 的与训练集分布相同的未知数据上的误差  
● $10 \%$ 的开发集误差

在这种情况下，显然存在着数据不匹配问题。为了解决这个问题，你可能会尝试使训练数据更类似于开发/测试数据。我们稍后将讨论一些相关技术。

为了诊断一个算法在上面 1 到 3 个问题受到了多大程度的影响，存在另一个数据集将是很有用的。具体地说，与其给算法提供所有可用的训练数据，你还可以把它分成两个子集：算法将进行训练的实际训练集，以及一个单独的集合，我们称之为“训练开发”集，我们将不会对它进行训练。

你现在有四个数据子集：

● 训练集：这是算法将学习的数据（例如，互联网图像 $^ +$ 移动应用图像）。这并不需要我们从与真正关心的相同分布（开发/测试集分布）的数据中提取。  
训练开发集：这些数据来自与训练集相同的分布（例如，互联网图像 $^ +$ 移动应用图像）。它通常比训练集要小；它只需要足够大到来评估和跟踪我们的学习算法的进展。  
● 开发集：这是从与测试集相同分布的数据中抽取出来的，它反映了我们昀终关心的数据的分布（例如，移动应用图像） 。  
● 测试集：这是从与开发集相同分布的数据中抽取出来的（例如，移动应用图像）。

有了这四个独立的数据集，你现在可以评估：

训练误差，对训练集进行评估。  
● 该算法能够泛化到与训练集相同分布数据的能力，并对训练开发集进行评估。

● 算法在你实际关心的任务上的性能，通过对开发集 和/或 测试集评估。在第 5-7 章中，用于选择开发集大小的大多数指导原则也适用于训练开发集。

# 41 辨别偏差、方差和数据不匹配误差

假设在猫咪检测任务中，人类获得了近乎完美的性能（0%误差），因此昀优错误率大约为$0 \%$ 。假设你有：

- $1 \%$ 的训练集误差   
- $5 \%$ 的训练开发集误差  
- $5 \%$ 的开发集误差

这表明了什么？你知道你有很高的方差。先前章节描述的减少方差的技术应该能使你取得进展。

现在，假设你的算法达到了：

- $10 \%$ 的训练集误差   
- $1 1 \%$ 的训练开发集误差  
- $12 \%$ 的开发集误差

这表明你在训练集上有很高的可避免偏差。该算法在训练集上做得很差，偏差降低技术应该能有所帮助。

在上面的两个例子中，该算法只存在高可避免偏差或高方差。一个算法有可能同时受到高可避免偏差、高方差和数据不匹配的子集的影响。例如：

- $10 \%$ 的训练集误差   
- $1 1 \%$ 的训练开发集误差  
- $20 \%$ 的开发集误差

该算法存在高可避免偏差和数据不匹配问题。然而，它在训练集的分布上并没有很大的差异。通过将不同类型的误差理解为表中的条目，可能将更容易理解不同类型的误差是如何相互关联的：

![](images/b6a7a75bf3908c4bcf909ae9c71d84355c5777b8fa6d595aa3c156ab5f410a71.jpg)

继续以猫咪图像检测器为例，你可以看到在 $\pmb { \times }$ 轴上有两种不同的数据分布。在 y 轴上，我们有三种类型的误差：人为误差，算法上误差，以及算法未经过训练的样本误差。我们可以用我们在前一章中发现的不同类型的误差来填写表格。

如果你愿意，你也可以在这个表格中填入剩下的两个空格：你可以通过让一些人给你的手机图片数据贴上标签，并测量他们的误差，你可以填写右上角的空格（移动应用图像上的人类水平表现）。你也可以通过移动应用猫的图像（分布 B ）来填充下一个空格，并将一小部分放入训练集，这样神经网络也可以学习它。 然后在数据的子集上测量学习模型的误差。填充这两个额外的条目可能会让我们对算法在两个不同的分布（分布 A 和 B ）上做的事情有更多的了解。

通过了解算法昀容易产生哪些类型的误差，你将能够更好地决定是否聚焦于减少偏差、减少方差或减少数据不匹配的技术。

# 42 解决数据不匹配问题

假设你已经开发了一个语音识别系统，它在训练集和训练开发集上都做得很好。但是，它在你的开发集上做得很差：这表明有一个数据不匹配的问题。你会怎么做呢?

我建议你：（I）尝试理解数据属性在训练集和开发集分布之间的差异。（ii）尝试找到更多的训练数据，以便更好地匹配你的算法碰到的开发集样本。14

例如，假设你在语音识别的开发集中进行误差分析：手动地遍历 100 个样本，并尝试理解算法错出在哪。你会发现你的系统做得的确很差，因为在开发集中，大部分的音频剪辑都是在一辆车里录制的，而大多数的训练样本都是在一个安静的环境下录制的。引擎和道路噪音极大地恶化了你的语音系统的性能。在这种情况下，你可能会尝试获得更多的训练数据，包括在汽车里拍摄的音频片段。误差分析的目的是了解训练集和开发集之间的显著差异，这正是导致数据不匹配的原因。

不幸的是，这个过程没有任何保证。例如，如果你没有任何方法获得更多的训练数据，来更好地匹配开发集数据，那么你可能没有一条明确的路径来提高性能。

# 43 人工合成数据

你的语音系统需要更多的数据，它们听起来就像是从车里录制得到的。与其在开车的时候收集大量的数据，不如通过人工合成数据来获取这些数据。

假设你获得了大量的汽车/道路噪音的音频剪辑。你可以从几个网站下载这些数据。假设你也有一群在安静的房间里说话的人。如果你把一个人的音频片段“添加”到一个汽车/道路噪音的音频片段，你会得到一个音频剪辑，听起来就好像那个人在嘈杂的汽车里说话一样。使用这个过程，你可以“合成”大量的数据，听起来就像是在汽车里收集的。

更一般的情况是，在一些情况下，人工合成数据允许你创建一个与开发集相当匹配的巨大数据集，让我们使用猫咪图像检测器作为第二个例子。你注意到，开发集的图像有更多的动态模糊，因为它们往往来自手机用户，他们在拍照时会微微地移动手机。你可以从网络图像的训练集中获取非模糊的图像，并将模拟的动态模糊添加到它们中，从而使它们更类似于开发集。

请记住，人工数据合成存在一定的挑战：有时候创建一个对人而言真实的合成数据比创建对计算机而言真实的数据要容易得多。例如，假设你有 1000 小时的语音训练数据，但只有 1 小时的汽车噪音。如果你反复使用相同的 1 小时的汽车噪音，从昀初的 1000 小时的训练数据中，你将会得到一个合成的数据集，然而同样的汽车噪音会不断重复。听这段音频的人可能无法分辨——所有的汽车噪音对我们大多数人来说都是一样的——但是某种学习算法可能会“过拟合”一小时的汽车噪音。因此，它可能无法很好地泛化到一个新的音频剪辑片段，里面汽车的噪音听起来是不同的。

另一种情况，假设你有 1000 个小时的汽车噪音片段，但所有的噪音都是从 10 辆不同的车上提取的。在这种情况下，一种算法可能会“过拟合”这 10 辆车，如果在不同的汽车上进行音频测试，性能则会很差。不幸的是，这些问题很难被发现。

再举一个例子，假设你正在建立一个计算机视觉系统来识别汽车：你正与一家电脑游戏公司合作，该公司拥有几辆汽车的计算机图形模型。为了训练你的算法，你可以使用这些模型来生成汽车的合成图像。即使合成的图像看起来非常真实，但这种方法（已经被许多人独立提出）可能不会很好地工作。在整个电脑游戏中，可能有 20 种汽车设计。制造一辆汽车的 3D 模型价格是非常昂贵的；如果你在玩这个游戏，你可能不会注意到你正在一遍又一遍地看到同样的车，也许只是换了一种颜色。即这些数据对你来说很真实。但是，与所有在道路上行驶的汽车相比——也就是你可能在开发/测试集里看到的——这组根据 20 辆汽车模型合成的汽车只捕获了世界上销售的汽车的极小一部分。因此，如果你的 10 万个训练样本都来自这 20 辆车，你的系统将会“过拟合”这 20 款特定的汽车设计，而且它将无法很好地泛化到包含其他汽车设计在内的开发/测试集。

当你在合成数据时，请考虑一下你是否真的在合成一组具有代表性的样本。尽量避免给出合成数据的属性，这将使学习算法有可能将合成和非合成的样本区分开来——例如，所有的合成数据是否来自 20 个汽车设计中的某一个，或者所有的合成音频是否都来自于某个小时的汽车噪音。这个建议很容易被忽视。

在处理数据合成过程时，我的团队有时会花上几周的时间来生成带有细节的数据，这些数据与实际的数据分布非常接近，从而产生显著的效果。但如果你能够正确地获取这些细节，你可以突然获得比以前更大的训练集。

# 调试推理算法

# 44 优化验证测试

假设你正在构建一个语音识别系统，该系统通过输入一个音频片段 A ，并为每一个可能的输出句子 S 计算得分 Score (S) 。例如，你可以试着估计 $\mathsf { S c o r e } _ { \mathsf { A } } ( S ) = \mathsf { P } ( S | A )$ ，表示句子 S 是正确输出的转录的概率，其中 A 是给定的输入音频。

给定某种方法能够计算 ​ ​ Score (S) 后，你仍然需要找到一个英文句子 S 来使之昀大化：

$$
\mathrm {O u t p u t} = \arg \max _ {S} \operatorname {S c o r e} _ {A} (S)
$$

要如何去计算上面的 ​ ​“arg max” 呢？假设在英文中共有 5000 个词汇，对于长度为 $N$ 的句子，则有 ​ ​ (50,000)N 种搭配，多到根本无法一一列举。因此，你需要使用一种近似搜索算法，努力去找到能够优化（昀大化）Score (S) 的那个 S . 有一种叫做 “定向搜索” 的搜索算法，在搜索过程中仅保留昀优的 K​ ​个候选项（在本章中你并不需要了解该算法的细节）。类似这样的算法并不足以保证能够找到满足条件的 ​ ​ S 来昀大化 ​ ​ ScoreA(S).

假设有一个音频片段 A 记录着某人说的：“我爱机器学习。”但你的系统输出的却是不正确的“我爱机器人。”，它没能够输出正确的转录。造成该误差的可能原因有两种：

1. 搜索算法存在问题。 近似搜索算法没能够找到昀大化 ​ ​ ScoreA(S) 的那个 ​ ​ S .  
2. 目标（得分函数）存在问题。 我们对 $\mathsf { S c o r e } _ { \mathsf { A } } ( S ) = \mathsf { P } ( S | A )$ 的估计并不准确。尤其在此例中，我们的得分函数 Score (S) 没能辨认出 “我爱机器学习” 是正确的转录。

对应不同的失败原因，你需要优先考虑的工作方向也将很不一样。如果原因 1 导致了问题，你应该改进搜索算法。如果原因 2 导致了问题，你应该在评估学习算法的函 Score (S) 上面多花些心思。

面对这种情况，一些研究人员将决定研究搜索算法；其他人则努力去找到更好的 Score (S) .但是，除非你知道其中哪一个是造成误差的潜在原因，否则你的努力可能会被浪费掉。怎么样才能更系统地决定要做什么呢？

让我们用 $\mathsf { S } _ { \mathsf { o u t } }$ 表示实际的输出 “我爱机器人”，用 $\mathsf { S } ^ { \star }$ 表示正确的输出 “我爱机器学习” 。为了搞清楚上面的 1 或 2 是否存在问题，你可以执行 优化验证测试（Optimization Verificationtest）： 首先计算 $\mathsf { S c o r e } _ { \mathsf { A } } ( S ^ { \star } )$ 和 $\mathsf { S c o r e } _ { \mathsf { A } } ( S _ { \mathsf { o u t } } )$ ，接着比较他们的大小。有两种可能：

情况 $1 \colon \mathsf { S c o r e } _ { \mathsf { A } } ( \mathsf { S } ^ { \star } ) > \mathsf { S c o r e } _ { \mathsf { A } } ( \mathsf { S } _ { \mathsf { o u t } } )$

在这种情况下，你的学习算法正确地给了 $\mathsf { S } ^ { \star }$ 一个比 $\mathsf { S } _ { \mathsf { o u t } }$ 更高的分数。尽管如此，我们的近似搜索算法选择了 $\mathsf { S } _ { \mathsf { o u t } }$ 而不是 $\mathsf { S } ^ { \star }$ . 则表示你的近似搜索算法没能够找到昀大化 Score (S) 的那

个 ​ ​S. 此时优化验证测试告诉你，搜索算法存在着问题，应该花时间研究。例如，你可以尝试增加定向搜索的搜索宽度。

情况 2: ScoreA(S*) ≤ ScoreA(Sout)

在这种情况下，计算 ScoreA(.) 的方式是错误的：它没有给正确的输出 $S ^ { \star }$ 比实际输出 $s _ { \mathrm { { o u t } } } -$ 个相同或更高的分数。优化验证测试告诉你，目标（得分函数）存在问题。因此，你应该专注于改进你的算法对不同的句子S 学习或近似出得分 Score (S) 的方式。

我们上面的讨论集中于某个单一的样本上，想要在实践中运用优化验证测试，你需要在开发集中检测这些误差样本。对于每一个误差样本，你都需要测试是否有 $\mathsf { S c o r e } _ { \mathsf { A } } ( \mathsf { S } ^ { \star } ) > \mathsf { S c o r e } _ { \mathsf { A } } ( \mathsf { S } _ { \mathsf { o u t } } )$ .开发集中所有满足该不等式的样本都将被标记为优化算法自身所造成的误差，而满足不等式$\mathtt { S c o r e } _ { \mathtt { A } } ( \mathbb { S } ^ { \star } ) \le \mathtt { S c o r e } _ { \mathtt { A } } ( \mathbb { S } _ { \mathrm { o u t } } )$ 的样本将被记为是计算得分 ${ \mathsf { S c o r e } } _ { \mathsf { A } } ( . )$ 造成的误差。

假设你昀终发现 $9 5 \%$ 的误差是得分函数 Score (.) 造成的，而仅有 $5 \%$ 的误差是由优化算法造成的。现在你应该知道了，无论你如何改进你的优化程序， 实际上也只会消除误差中的$5 \%$ 左右。因此，你应该专注于改进你的得分函数 ScoreA(.) .

# 45 优化验证测试的一般形式

你可以在如下情况运用优化验证测试，给定输入 $x$ ，且知道如何计算 Score (y) 来表示 $y$ 对输入 $x$ 的响应好坏。此外，你正在使用一种近似算法来尽可能地找到 arg max Score (y) ，但却怀疑该搜索算法有时候并不能找到昀大值。在我们先前提到的语音识别的例子中， $x { = } A$ 代表某个音频片段， $y = s$ 代表输出的转录。

假设 $\mathsf { y } ^ { \star }$ 是 “正确的” 输出，可算法输出了 $\mathsf { y } _ { \mathsf { o u t } }$ . 此时的关键在于测量是否有 Scorex(y*) >Score $( \mathsf { y } _ { \mathsf { o u t } } )$ . 如果该不等式成立，我们便可以将误差归咎于优化算法。（请参考前一章的内容，以确保你理解这背后的逻辑。）否则，我们将误差归咎于 Score (y) 的计算方式。

让我们再看一个例子：假设你正在构建一个中译英的机器翻译系统，输入一个中文句子 C，并计算出每一个可能的翻译句子 $\boldsymbol { E }$ 的得分 ${ \mathsf { S c o r e } } _ { \mathrm { c } } ( E )$ ，例如，你可以使用 $\mathsf { S c o r e } _ { \mathsf { c } } ( E ) = \mathsf { P } ( E | C ) ,$ ，表示给定输入句子 C，对应翻译句子为 E 的概率。

你的算法将通过计算下面的公式来进行句子的翻译：

$$
\operatorname {O u t p u t} = \arg \max _ {E} \operatorname {S c o r e} _ {C} (E)
$$

然而所有可能的英语句子构成的集合 $\boldsymbol { E }$ 太大了，所以你将依赖于启发式搜索算法。

假设你的算法输出了一个错误的翻译 $E _ { \mathrm { { o u t } } }$ ，而不是正确的翻译 $E ^ { \star }$ . 优化验证测试会要求你计算是否有 $\mathsf { S c o r e } _ { \mathrm { c } } ( E ^ { \star } ) > \mathsf { S c o r e } _ { \mathrm { c } } ( E _ { \mathrm { o u t } } )$ . 如果这个不等式成立，表明 ${ \mathsf { S c o r e } } _ { \mathsf { c } } ( . )$ 正确地辨认 E* 是一个更好的输出；因此你可以将把这个误差归咎于近似搜索算法。否则，你可以将这个误差归咎于 ScoreC(.) 的计算方式。

在人工智能领域，这是一种非常常见的 “设计模式”，首先要学习一个近似的得分函数Score (.)，然后使用近似昀大化算法。如果你能够发现这种模式，就能够使用优化验证测试来理解造成误差的来源。

# 46 强化学习举例

假设你正在用机器学习来教直升机复杂的飞行动作。下面是一张延时照片，照片上是一台电脑控制器的直升机正在引擎关闭的情况下执行着陆。

![](images/7ba090315869ed2e5c413840f882e922de16b01358b3ef57e52eb7951abbb5e2.jpg)

这被称为“自旋”策略，即使引擎意外故障了，它也允许直升机着陆。这也是人类飞行员经常进行的训练。而你的目标是使用一种学习算法，让直升机通过一个轨迹 $\tau$ 安全地着陆。

要应用强化学习策略，你必须设计一个 “奖励函数” R(.)，它给出一个分数来衡量每一个可能轨迹 T 的好坏。例如，如果 导致直升机坠毁，那么奖励也许是 $R ( T ) = - 1 { , } 0 0 0$ ，这是一个巨大的负反馈；而一个导致安全着陆的轨迹T 可能会产生一个正的 $R ( T )$ 值，它的精确值取决于着陆过程的平稳程度。奖励函数 R(.) 通常是人为选择的，以量化不同轨迹 $\tau$ 的理想程度。它必须权衡考虑着陆的颠簸程度，直升机是否降落在理想的位置，乘客的降落体验等因素。设计一个好的奖励函数并非易事。

给定一个奖励函数 $R ( T )$ ，强化学习算法的工作是控制直升机，使其达到 $\mathtt { m a x } _ { \tau } R ( T )$ . 然而，强化学习算法原理内部有许多近似操作，可能无法成功实现这种昀大化需求。

假设你已经选择了某些奖励函数 R(.) 作为反馈，并运行了学习算法。然而它的表现似乎比人类飞行员要糟糕得多——它更加颠簸，而且似乎不那么安全。你如何判断错误是否由强化学习

算法造成——它试图找到一个轨迹 T，满足 max R(T) ——或者错误来自于你的奖励函数—它尝试衡量并且指定一种在颠簸程度和着陆精度之间权衡的理想结果。

为了应用优化验证测试，让 $T _ { \mathsf { h u m a n } }$ 表示人类飞行员所选择的轨迹，并让 $T _ { \mathrm { o u t } }$ 代表算法所选择的轨迹。根据我们上面的描述， $T _ { \mathsf { h u m a n } }$ 是优于 $T _ { \mathrm { o u t } }$ 的发展轨迹。因此，关键的测试点在于：不等式 $R ( T _ { \mathrm { h u m a n } } ) > R ( T _ { \mathrm { o u t } } )$ 是否成立？

情况1：如果不等式成立，奖励函数 R(.) 正确地使 $T _ { \mathsf { h u m a n } }$ 优于 $T _ { \mathrm { o u t } }$ ，但这表明我们的强化学习算法找到的 $T _ { \mathrm { o u t } }$ 仍不够好，花时间去改进算法是很值得的。

情况2：如果上面不等式不成立，而是 $R ( T _ { \mathrm { \mathfrak { n u m a n } } } ) \leq R ( T _ { \mathrm { \mathfrak { o u t } } } )$ . 这表明 $R ( . )$ 的设计使得理应是更优策略的 $T _ { \mathsf { h u m a n } }$ 得到了一个更糟的评分。你应当致力于改进 $R ( . )$ ，以更好地获得与良好着陆情况相对应的权衡。

许多机器学习应用程序使用这种优化某个近似的 “模式” 来确定得分函数 . 有时没有特定的输入 $\pmb { \chi } ,$ ，形式简化为 Score (.) 。在上面的例子中，得分函数即是奖励函数 $\mathsf { S c o r e } ( T ) { = } \mathsf { R } ( T )$ ，而采用的优化算法是强化学习算法，目的是找到好的轨迹 T.

这和前面的例子有一个区别，那就是，与其比较 “昀优” 输出，不如将其与人类水平的表现进行比较。我们认为，即使 $T _ { \mathsf { h u m a n } }$ 不是昀优的，它也是相当不错的。一般而言，只要有一个比当前学习算法性能更好的输出 y*（在这个例子中即是指 $T _ { \mathsf { h u m a n } } )$ ），即使它不是 “昀优” 的，优化验证测试也能够反映改进学习算法与改进得分函数之间哪一个更具前途。

# 端到端深度学习

# 47 端到端学习的兴起

假设你想要构建一个系统来对产品的线上评论进行检查，并且要能够自动地告诉你给出评论的人是否喜欢这个产品。比如说，你希望系统将下面的句子识别为十分正面的评论：

这个拖把非常好用！

而下面的评论应当是十分负面的：

拖把的质量好差，我后悔买它了。

这种识别正面与负面评论的问题被称为 “情感分类”（sentiment classification） 。想要构建一个这样的系统，你的流水线模块需要有以下两个组件：

1. 解析器（parser）：一种通过识别关键词汇来对文本进行注释的系统。例如，你可以使用解析器对所有的形容词和名词做标记15，因此你可能会得到类似下面的句子：

这个拖把（名词）非常好用（形容词）！

2. 情感分类器（sentiment classifier）：一种学习算法，它可以输入带注释的文本，并预测整体的情感。解析器的注释将对这个算法起到极大的帮助：通过给形容词一个较高的权重，你的算法将能很快地找到像 “非常” 这样的重要词汇，并忽视像 “这个” 这样的非关键词。

我们可以将 “流水线” 的两个组件可视化为下面的图片：

![](images/9e16ff7270aa4d986c7df43c2fe893cc8c4afa413b38516d74365f41b2b00e24.jpg)

昀近的趋势更倾向于用一种单一的学习算法取代此类流水线。该任务的端到端学习算法只需输入一个原始的文本 “这个拖把非常好用！” ，接着尝试直接识别其中的情感：

![](images/7908f78560dc72deaa2ff5236f5472a5d631a710a8e62b67ba03e87ccaa2f176.jpg)

神经网络通常用于端到端学习系统，“端到端” 这个术语指的是我们要求学习算法直接从输入得到期望的输出，即学习算法将系统的 “输入端” 连接到 “输出端” 。

在数据量十分丰富的问题上，端到端系统往往很奏效，但它并不总是一个很好的选择。接下来的几个章节将给出更多的端到端系统的例子，并提供一些关于何时使用它们的建议。

# 48 端到端学习的更多例子

假设你正在构建一个语音识别系统，你的系统可能需要三个组件：

![](images/9bc750be05013c0cd701123a647d9be0255713afa025e851832f46949240dd17.jpg)

它们的工作形式如下：

1. 计算特征（compute features）：提取人工设计的特征，如 MFCC （Mel-frequencycepstrum coefficients，频谱系数）特征，以此来试图捕捉对话的内容，而忽略不太相关的属性，比如说话者的音高。  
2. 音素识别器（phoneme recognizer）：一些语言学家人为，有一些基本的声音单元叫做 “音素” 。 例如， “keep” 中的 “k” 和 “cake” 中的 “c” 是相同的音素，而这个系统试图识别音频片段中的音素。  
3. 昀终识别器（final recognizer）：以已识别音素的序列为序，并试着将它们串在一起，形成转录输出。

与此相反，端到端系统可能会输入一个音频片段，并尝试直接输出文字记录：

![](images/24a081579828733b19f94c3bdf4c70a13ff51430609ecb886f57756caebcdc0f.jpg)

到目前为止，我们只描述了纯线性的机器学习 “流水线”：输出顺序地从一个阶段传递到下一个阶段。实际上流水线可能会更复杂。例如，这是一个自动驾驶汽车的简单流水线架构：

![](images/0dd26dffb4d8634b6e29d6f3640d2db704ebd2d9521126b05e6461863e4ad451.jpg)

它拥有三个组件：一个使用相机图片检测车辆，一个检测行人，昀后一个组件则为我们自己的车规划路径，从而避让车辆和行人。

并不是流水线中的每一个组件都需要进行学习。例如在文献 “robot motion planning” 中对汽车的昀终路径规划提出了许多算法，而其中的一些算法并不涉及到学习。

相反，端到端的方法可能会尝试从传感器获取输入并直接输出转向方向:

![](images/9f1fefb2914d752d418b971a1971bed9422f02da09eec49c8feaa2309026bd97.jpg)

尽管端到端学习已经在许多领域取得了成功，但它并不总是昀佳方案。端到端的语音识别功能很不错，但我对自动驾驶的端到端学习持怀疑态度。在接下来的几章将会解释原因。

# 49 端到端学习的优缺点

考虑先前提到的语音识别系统的流水线：

![](images/c4abf1c63f2faf8a57c7a145e3ac4ddc25a46d81d28e2520e3479b49da902e1f.jpg)

该流水线中的许多部分都是 “人工设计” 的：

● 频谱系数即是一套经过人工设计的特征。尽管它们提供了对音频输入的一种合理归纳，但也通过抛出一些信息简化了输入信号。  
● 音素是语言学家的发明，作为说话声音的一种不完备的表示。在某种意义上，音素是对现实语音很差的近似，因此迫使算法使用音素进行表示将限制语音系统的性能。

这些人工设计的成分限制了语音系统的潜在性能，然而这样做也有一些优点：

● 频谱系数的特性对于处理一些不影响内容的音频属性是很有效的，比如说话者的音高。因此它们有助于讲话学习算法面临的问题。  
● 在一定程度上，音素是一种合理的语音表示方法，它们也可以帮助学习算法理解基本的声音成分，从而提高其性能。

拥有更多的人工设计成分通常可以让语音系统学习更少的数据，由频谱系数和音素所捕获的人工设计的知识对算法从数据中获取的知识进行了补充。当我们的数据量不是很多时，这些知识是非常有用的。

现在，考虑端到端系统：

![](images/aec65bed171cc4c28bf3ca2cf7aeddd37c907ae8266b00b99de9f2e52e617d16.jpg)

这个系统缺乏人工设计知识，因此当训练集很小的时候，它的表现可能比人工设计的流水线更加糟糕。

然而当训练集很大时，它不会收到频谱系数或者语音表示方法的限制。如果学习算法是一个足够大的神经网络，且喂进去许多的训练数据，就有可能做得更好，甚至达到昀优错误率。

端到端学习系统在 “两端” —— 输入端和输出端拥有大量标记数据时，往往做得更好。在这个例子中，我们需要一个大数据集（包含<音频,文本>对）。当这种类型的数据不可用时，使用端到端学习则需非常谨慎。

如果你正在研究一个训练集非常小的机器学习问题，那么算法的大部分知识都将来自于你的人类洞察力，即你的 “人工设计” 成分。

如果不使用端到端系统，你将不得不决定在流水线中需要有哪些组件，以及它们应该如何连接在一起。在接下来的几章中，我们将对这类流水线的设计提出一些建议。

# 50 流水线组件的选择：数据可用性

在构建非端到端的流水线系统时，什么样的流水线组件可作为合适的选项呢？对流水线的设计将极大地影响整个系统的性能，其中一个重要因素是，是否能够轻松地收集到数据来训练每个组件。

例如，考虑下面这个自动驾驶架构：

![](images/cb6e67191d5ae0c438ebb76ffd1f2048ce28f0066b8c302e24cee1876be867b5.jpg)

你可以使用机器学习算法进行车辆和行人的检测，另外要获取这些数据也并非难事——如今有许多含有大量带车辆和行人标记的计算机视觉数据集。你也可以通过众包市场（比如 AmazonMechanical Turk）获得甚至更大规模的数据集。因此，获取训练数据来构建车辆检测器与行人检测器相对而言是比较容易的。

![](images/02cd84cb257392c67038d5d0aea3c5e9888db941f3d72d485401a3d3cc3e7b6a.jpg)

相反，考虑一种纯端到端的方式：

为了训练这样一个系统，我们需要一个包含 <图像，操纵方向 $>$ 数据对的大型数据集。然而让人们在驾驶汽车时收集汽车的操纵方向的数据是非常费时费力的，你需要一辆特殊配置的汽车，且需要巨大的驾驶量来涵盖各种可能的场景。这就使得端到端系统难以进行训练，获得大量带标记的行人或者是汽车图像反而要容易得多。

更常见的情况是，如果有大量的数据可以被用来训练流水线的 “中间模块” （例如汽车检测器或行人检测器），你便可以考虑使用多段的流水线架构。因为可以使用所有可用数据进行训练，所以这种结构可能是更优的。

在更多端到端数据变得可用之前，我相信非端到端的方法对于自动驾驶而言是更有希望的—它的体系架构更匹配于数据的可用性。

# 51 流水线组件的选择：任务简单性

除了数据可用性之外，你还应该考虑流水线组件选择的第二个因素：独立的组件使得任务简单了多少？你应该尽可能地选择那些易于构建或学习的独立流水线组件，但对一个组件而言，“易于” 学习指的是什么呢？

![](images/e0ba3f56c344c3ca6ece0605e678d91fdaaa8d1c5440e50e70b31e8a484844d7.jpg)

考虑下面列出的机器学习任务，它们的难度逐级递增：

1. 分类判断某张图片是否过度曝光（就像上面这张图片）；  
2. 分类判断某张图片拍摄于室内还是室外；  
3. 分类判断某张图片中是否有猫；  
4. 分类判断某张图片中是否有黑白两色皮毛的猫；  
5. 分类判断某张图片中是否有暹罗猫（一种特殊的猫）

这些图像分类任务都属于二分类任务——你需要输入一张图片，输出 0 或者 1. 但列出的前面几个任务看起来 “更易于” 使神经网络进行学习。你将能够通过较少的训练样本来学习更加简单的任务。

对于使问题变得简单或困难的原因，机器学习领域尚未给出一个较好的正式定义。 随着深度16学习和多层神经网络的兴起，我们有时认为，如果一个任务可以用更少的计算步骤（对应于一个浅的神经网络）来实现，那么相对于需要更多的计算步骤（对应于一个深的神经网络），这个任务就 “更简单” 了。但这些都是非正式的定义。

如果你能够完成一个复杂的任务，并将其分解为更简单的子任务，然后显式编写子任务步骤代码，那么你就会给算法一些先验知识，从而帮助它更有效地学习任务。

![](images/0911db5ab1087226cdb1a7e676c1f761e1fe5d3658cbb0e9b188b4cbfc16956c.jpg)

假设你正在构建一个暹罗猫检测器。下面是一个纯粹的端到端架构：

![](images/ddfaafa6c132910a18625c7eba783726e18bd4ba3b086c82d095f900734add8d.jpg)

与此相反，你可以分成两个步骤来形成流水线：

![](images/18d5d13ab317466f362f06a4c474519a3c5afd3c9b644baaa7fed1e0adcbbdd3.jpg)

第一步：（猫检测器）检测图像中所有的猫。

![](images/e257bbad5c285da07ead6b7edfbdbd78dbea306cb7687af9eb2972bad0019e73.jpg)

第二步：将每一块被检测到的猫的图像传送给猫种类分类器（每次一张），如果其中任何一只猫是暹罗猫，则在昀后输出 1.

![](images/24374b239255a4b1b6244e3cc03be8f8519ad79958eae6d82e8c2b3f41fa7693.jpg)  
猫种类分类器

暹罗猫？ (0/1)

![](images/0dfbeb0913d4ce5e68f7e8bdaf831cdc97b7cdf17093595b806d7c3846687e09.jpg)  
猫种类分类器

暹罗猫？ (0/1)

与仅仅使用标签 0/1 来训练一个纯粹的端到端分类器相比，流水线中的两个组件——猫咪检测器和猫种类分类器——似乎更容易进行学习，而且需要更少的数据。17

昀后一个例子，让我们回顾一下自动驾驶流水线。

通过使用该流水线架构，你可以告诉算法总共有三个关键的步骤：（1）检测其他车辆，（2）检测行人，（3）为你的车规划一条道路。此外，每一个步骤都是相对简单的功能——因此可以用更少的数据来学习——而不是纯粹的端到端方法。

总而言之，当决定流水线组件的内容组成时，试着构建这样的流水线，其中每个组件都是一个相对 “简单” 的功能，因此只需要从少量的数据中学习。

# 52 直接学习更为丰富的输出

一个图像分类算法可以输入一个图像 x ，并输出一个表示对象类别的整数。那么一个算法可以输出一个完整的句子来对图像进行描述吗？

举个例子—

$$
x =
$$

$y = { ^ { \circ } \mathsf { A } }$ yellow bus driving down a road with green trees and green grass in the background.”

输入 $\pmb { \times }$ 为，输出 y 为：“一辆黄色的公共汽车在路上开着，背景是绿色的树和绿色的草。 ”

传统的监督学习应用学得一个函数 $h { \cdot } X { \to } Y ,$ 其中输出 y 通常是一个整数或者实数。例如：

<table><tr><td>问题</td><td>X</td><td>Y</td></tr><tr><td>垃圾邮件分类</td><td>邮件</td><td>垃圾邮件/非垃圾邮件 (0/1)</td></tr><tr><td>图像识别</td><td>图像</td><td>整数值标签</td></tr><tr><td>房价预测</td><td>房屋特征</td><td>美元价格</td></tr><tr><td>产品推荐</td><td>产品与用户特征</td><td>购买几率</td></tr></table>

端到端深度学习中昀令人兴奋的进展之一是，它让我们直接学习比数字复杂得多的 y . 在上面的图像配字示例中，你可以使用一个神经网络输入一个图像（x），并直接输出一个标题（y）.

下面有更多的例子：

<table><tr><td>问题</td><td>X</td><td>Y</td><td>样例引用</td></tr><tr><td>图像配字</td><td>图像</td><td>文本</td><td>Mao et al., 2014</td></tr><tr><td>机器翻译</td><td>英语文本</td><td>法语文本</td><td>Suskever et al., 2014</td></tr><tr><td>问题回答</td><td>（文本，问题）对</td><td>回答文本</td><td>Bordes et al., 2015</td></tr><tr><td>语音识别</td><td>音频</td><td>转录</td><td>Hannun et al., 2015</td></tr><tr><td>文本转换语音</td><td>文本特征</td><td>音频</td><td>van der Oord et al., 2016</td></tr></table>

这体现了深度学习的高速变化趋势：当你有正确的（输入，输出）标签对的时候，有时可以进行端到端学习，即使输出是一个句子、图像、音频，或者其它的比一个数字更丰富的输出形式。

# 根据组件进行误差分析

# 53 根据组件进行误差分析

假设你的系统由复杂的机器学习流水线所构建，并且你希望提高该系统的性能，那应该从流水线的哪一部分开始改进呢？你可以通过将误差归因于流水线的特定组件，来决定工作的优先级。

我们使用暹罗猫分类器的例子来进行说明：

![](images/57ade0f154463df0fdbdfa851e46cd8a84f56a235b07b12a7ea9824fbe7df476.jpg)

在上图的流水线中，第一部分是猫检测器，它能够检测出猫，并将它们从图像裁剪出来；第二部分是猫的品种分类器，决定它是否是暹罗猫。 改进两个组件中的任何一个都有可能花费数年的时间。 你该决定关注哪个（些）组件呢？

通过​按组件进行误差分析，你可以尝试将每一个算法造成的误差归因于流水线的某个（有时是两个）组件。例如，即使某张图片样本的正确标签为含有暹罗猫（ $\scriptstyle \mathsf { y } = 1 \ )$ ） ，算法依旧将其标签误分类为不含有暹罗猫（ $\scriptstyle { \mathsf { y } } = 0$ ）.

![](images/1f5c5b38e6a1663ff4c641c2acd3ea90e822bd95c1d032d3f5c342921fca0f34.jpg)

让我们人为地检查一下算法两个步骤的执行过程。假设暹罗猫检测器从下图中检测出一只猫：

![](images/35a7b0862b875eb0dc2a04a94cc0b49ab8fea62ffd0e8d0572bbc736767a0357.jpg)

这表示猫检测器给出了这样的图片：

![](images/81b9cd8468bdc706d9a9d8f7ef485b4c330080d34721fae65fa46a7e9659de23.jpg)

接下来，猫品种分类器将此图像正确分类为不包含暹罗猫。 因此，猫品种分类器是正常工作的：给它一堆岩石并输出一个非常合理的标签（ $\scriptstyle \cdot y = 0 .$ ）. 实际上，对上面裁剪的图像进行分类的人也会预测（ $\scriptstyle \cdot y = 0$ ）. 因此，你可以清楚地将此误差归因于猫探测器组件。

另一方面，如果猫探测器输出了以下边界框：

![](images/20b47da4920f33c22ca5eca66e78f32e411113acd0cfcee0c4d2d7a4060477cc.jpg)

你会得出如下结论，猫探测器已经完成了它的工作，并且它是有缺陷的猫品种分类器。

如果你对 100 个误分类的开发集图像遍历检查，并发现 90 个误差可归因于猫探测器，并只有10 个误差可归因于猫品种分类器。 那么你可以有把握地得出结论，应该更加专注于改进猫探测器。

此外，你现在还可以方便地找到 90 个样本，其中猫探测器将输出不正确的边界框。 你可以使用这 90 个样本对猫探测器进行更深层次的误差分析，以了解该如何改进。

到目前为止，我们关于如何将误差归因于流水线的特定组件的描述是非正式的：查看每个组件的输出，并看看是否可以决定哪个部分出错了。 这种非正式的方法可能就是你所需要的。但在下一章中，你还将看到一种更正式的误差归因方式。

# 54 误差归因至某个组件

让我们继续使用这个例子：

![](images/73d3c73b558b3a3555426308a054ed19019f33130784c989aa6946a480201972.jpg)

假设猫检测器输出了这样的边界框：

![](images/7f814971b1beb2d52d037d552421596da941e86c16bd40ed656bd4b50505fab1.jpg)

被裁剪的图片将会送给猫品种分类器，因此它给出了错误的分类结果 $\scriptstyle { \mathsf { y } } = 0$ , 即图片中没有猫。

![](images/eb11926061d56e6a5aa9a59cb382934ef2d83ffe5033075f6c54e73e50f77ac7.jpg)

如果像这样模糊情况的数量很少，你可以做出你想要的任何决定并获得类似的结果。 但这里有一个更正式的测试方法，可以让你更明确地将误差归因于某一个组件：

1. 用手动标记的边界框替换猫检测器的输出。  
2. 通过猫品种分类器处理相应的裁剪图像。 如果猫品种分类器仍将其错误地分类，则将误差归因于猫品种分类器。 否则，将此误差归因于猫检测器。

换而言之，进行一个实验，在其中为猫品种分类器提供 “完美” 输入。 有两种情况：

情况1：即使给出了一个 “完美” 的边界框，猫品种分类器仍然错误地输出 . 在这种情况下，猫品种分类器很明显存在着问题。  
情况2：给定一个 “完美” 的边界框，品种分类器现在正确输出 . 这表明只要猫检测器给出了一个更完美的边界框，那么整个系统的输出就是正确的。 因此，将误差归因于猫检测器。

通过对开发集误分类的图像执行此分析，你现在可以将每个误差明确地归因于一个组件。 这允许你对流水线不同组件所造成的误差程度进行估计，从而决定将注意力集中在哪儿。

# 55 误差归因的一般情况

以下是误差归因的一般步骤。 假设在流水线中有三个步骤 A，B 和 C，其中 A 直接输出到 B，B直接输出到 C.

![](images/036afafbca6da940fc5d765023d9c5d31204af32315a1ee44746ae735486efe1.jpg)

对于系统在开发集上存在的每个错误样本：

1. 尝试人为修改 A 的输出为 “完美” 输出（例如，猫的“完美”边界框），并在此输出上运行流水线其余的 B，C 部分。 如果算法现在给出了正确的输出，那么这表明，只要 A 给出了更好的输出，那么整个算法的输出就是正确的；因此，你可以将此误差归因于组件 A. 否则，请继续执行步骤 2.  
2. 尝试人为修改 B 的输出为 “完美” 输出。如果算法现在给出正确的输出，则将误差归因于组件 B. 否则，继续执行步骤 3.  
3. 将误差归因于组件 B.

![](images/4654d759563099558924fbacffc602a06c171127ce8d953dfe69a580be7a761d.jpg)

让我们来看一个复杂一点的例子：

你的自动驾驶汽车将使用上面的流水线技术。如何根据组件进行误差分析来决定专注于哪个（些）组件呢？

你可以将三个组件映射到 A, B, C，如下所示：

A：检测汽车  
B：检测行人  
C：规划汽车路径

按照上述程序，假设你在封闭的轨道上对你的汽车进行测试，发现汽车选择了一个比熟练司机更刺耳的转向方向。在自动驾驶领域，这种情况通常被称为​场景（scenario）。接着你需要：

1. 尝试人为修改 A （检测汽车）的输出，使之成为 “完美” 输出（例如，手动进入并告诉它其他汽车在哪里)。像之前一样运行流水线其余的 B，C 部分，但是允许 C （规划路径）使用 A 现在的完美输出。如果算法现在为汽车规划出一条更好的路径，那么这表明，如果 A 给出更好的输出，整个算法的输出会更好；因此，你可以将此误差归因于组件 A. 否则，继续执行步骤 2.  
2. 尝试人为修改 B （检测行人）的输出，使之成为 “完美” 输出。如果算法现在给出了正确的输出，那么将误差归因为组件 B.  
3. 将误差归因于组件 C.

ML 流水线的组件应该按照有向无环图（DAG）排序，这意味着你应该能够以某种固定的从左到右的顺序来计算它们，并且后面的组件应该只依赖于早期组件的输出。只要组件到$A - > B - > C$ 顺序的映射遵循 DAG 顺序，那么误差分析就没问题。但如果你交换 A 和 B，可能会得到略微不同的结果：

A：检测行人（以前是检测汽车）  
B：检测汽车（以前是检测行人）  
C：规划汽车路径

但是这个分析的结果仍然是有效的，并且可以很好地指导你把注意力集中在哪里。

# 56 组件误差分析与人类水平对比

对学习算法进行误差分析就像使用数据科学来分析 ML 系统的错误，以获得有关下一步该做什么的建议。从基本角度来看，组件误差分析告诉我们：哪些组件的性能是昀值得尽力去改进的。

假设你有一个客户在网站上购物的数据集，数据科学家可能有许多不同的方法来分析数据。她可能会得出许多不同的结论——关于网站是否应该提高价格，关于通过不同营销活动获得的客户的终身价值等等。并不存在一种 “正确” 的方法来对数据集进行分析并得出许多可能有用的见解。同样，也没有一种 “正确” 的方法来进行误差分析。通过这些章节，你已经学习了许多昀常见的设计模式，用于得到有关 ML 系统的有用见解，但你也可以自由尝试其他误差分析的方法。

让我们回到自动驾驶应用程序的讨论中，其中汽车检测算法输出附近汽车的位置（也可能是速

![](images/f5826b00b8654a084496682bd8dabcca9d7e9fc563ef760f973857ea2505ff76.jpg)

度），行人检测算法输出附近行人的位置，这两个输出昀终用于为当前车辆进行路径规划。

如果想要调试该流水线，却不希望严格遵循上一章中提到的过程，你可以非正式地询问：

1. 在检测汽车时，汽车检测组件与人类水平表现相差多少？  
2. 在检测行人时，行人检测组件与人类水平表现相差多少？  
3. 整个系统的性能与人类表现相差多少？在这里，人类水平的表现假定：人类必须仅根据前两个流水线组件的输出（而不是访问摄像机图像）来规划汽车的路径。换句话说，当人类只得到相同的输入时，路径规划组件的性能与人类的性能相较如何？

如果你发现其中一个组件远低于人类水平的表现，那么你现在可以专注于提高该组件的性能。

当我们尝试自动化人类可以做的事情时，许多误差分析过程将表现得昀好，因此可以对人类水平的表现进行基准测试。 我们前面的大多数例子都有这个隐含的假设。 如果你正在构建 ML

系统，其中昀终输出或某些中间组件正在做甚至连人类都无法做好的事情，那么这些过程中的一些步骤将不起作用。

这是解决人类可解决的问题的另一个优势 - 你拥有更强大的误差分析工具，因此你可以更有效地优先处理团队的工作。

# 57 发现有缺陷的机器学习流水线

如果你的机器学习流水线的每个单独组件在人类水平性能或接近人类水平性能上执行，但总体流水线性能却远远低于人类水平会怎么样？这通常意味着流水线存在缺陷，需要重新设计。误差分析还可以帮助你了解是否需要重新设计流水线。

![](images/68c73db35d05c31f12918bc2288da036bc3e97b4ae35df720c0f3b62572de8d3.jpg)

在前一章中，我们提出了这三个部分组件的表现是否达到人类水平的问题。假设所有三个问题的答案都是肯定的，也即是说：

1. 汽车检测部件（大概）是人类级别的性能，用于从摄像机图像中检测汽车。  
2. 行人检测组件（大概）是人类级别的性能，用于从摄像机图像中检测行人。  
3. 与仅根据前两个流水线组件的输出（而不是访问摄像机图像）规划汽车路径的人相比，路径规划组件的性能处于类似水平。

然而，你的自动驾驶汽车的整体性能远远低于人类水平。即，能够访问摄像机图像的人可以为汽车规划出明显更好的路径。据此你能得出什么结论？

唯一可能的结论是， ML 流水线存在着缺陷。在这种情况下，路径规划组件在给定输出的情况下可以做得很好，但输入没能包含足够的信息。你应该询问自己，除了两个早期流水线组件的输出之外，还需要哪些其他信息来为汽车的驾驶辅助规划路径。换句话说，熟练的人类驾驶员还需要什么其他信息？

例如，假设你意识到人类驾驶员还需要知道车道标记的位置。 这表明你应该按如下方式重新设计流水线：18

![](images/28b7d8b026831ce57f9696e42dc85b1909b3590efa125f8746e435aaf08d7fc4.jpg)

昀后，如果你认为，即使每个组件都具有人类级别的性能（请记住，你要与被给予与组件相同输入的人类进行比较），流水线整体上也不会达到人类水平的性能，则表明流水线有缺陷，应该重新设计。

# 结论

# 58 建立超级英雄团队 - 让你的队友阅读这本书吧！

恭喜你完成这本书的阅读！

在第2章中，我们讨论了本书如何帮助你成为团队的超级英雄。

![](images/bab0972008529d089af3225fc731db2bf739df4ce008f240e79bc1ac83e64f47.jpg)

唯一比成为超级英雄更好的事情就是成为超级英雄团队的一员。 我希望你能把这本书的副本交给你的朋友和队友，并帮助创造出其他的超级英雄！

# Hands-on Machine Learning with Scikit-Learn, Keras &TensorFlow

Concepts,Tools,and Techniques

to Build Intelligent Systems

powered by

![](images/0c64ae436d0acb4a529ee0f61ed7ab25696f94060cbc97135c7b8ea00a6860ff.jpg)

Early Release

RAW& UNEDITED

AurelienGeron

# Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow

Concepts, Tools, and Techniques to Build Intelligent Systems

Aurélien Géron

# Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron

Copyright $^ ©$ 2019 Aurélien Géron. All rights reserved.

Printed in the United States of America.

Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.

Editor: Nicole Tache

Interior Designer: David Futato

Cover Designer: Karen Montgomery

Illustrator: Rebecca Demarest

June 2019:

Second Edition

# Revision History for the Early Release

2018-11-05: First Release

2019-01-24: Second Release

2019-03-07: Third Release

2019-03-29: Fourth Release

2019-04-22: Fifth Release

See http://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release details.

The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.

While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.

# Table of Contents

Preface. . xi

# Part I. The Fundamentals of Machine Learning

# 1. The Machine Learning Landscape. . . .

What Is Machine Learning? 4

Why Use Machine Learning? 4

Types of Machine Learning Systems 8

Supervised/Unsupervised Learning 8

Batch and Online Learning 15

Instance-Based Versus Model-Based Learning 18

Main Challenges of Machine Learning 24

Insufficient Quantity of Training Data 24

Nonrepresentative Training Data 26

Poor-Quality Data 27

Irrelevant Features 27

Overfitting the Training Data 28

Underfitting the Training Data 30

Stepping Back 30

Testing and Validating 31

Hyperparameter Tuning and Model Selection 32

Data Mismatch 33

Exercises 34

# 2. End-to-End Machine Learning Project. . . . . 37

Working with Real Data 38

Look at the Big Picture 39

Frame the Problem 39

Select a Performance Measure 42

Check the Assumptions 45

Get the Data 45

Create the Workspace 45

Download the Data 49

Take a Quick Look at the Data Structure 50

Create a Test Set 54

Discover and Visualize the Data to Gain Insights 58

Visualizing Geographical Data 59

Looking for Correlations 62

Experimenting with Attribute Combinations 65

Prepare the Data for Machine Learning Algorithms 66

Data Cleaning 67

Handling Text and Categorical Attributes 69

Custom Transformers 71

Feature Scaling 72

Transformation Pipelines 73

Select and Train a Model 75

Training and Evaluating on the Training Set 75

Better Evaluation Using Cross-Validation 76

Fine-Tune Your Model 79

Grid Search 79

Randomized Search 81

Ensemble Methods 82

Analyze the Best Models and Their Errors 82

Evaluate Your System on the Test Set 83

Launch, Monitor, and Maintain Your System 84

Try It Out! 85

Exercises 85

# 3. Classication. . . 87

MNIST 87

Training a Binary Classifier 90

Performance Measures 90

Measuring Accuracy Using Cross-Validation 91

Confusion Matrix 92

Precision and Recall 94

Precision/Recall Tradeoff 95

The ROC Curve 99

Multiclass Classification 102

Error Analysis 104

Multilabel Classification 108

Multioutput Classification 109

Exercises 110

# 4. Training Models. . . . 113

Linear Regression 114

The Normal Equation 116

Computational Complexity 119

Gradient Descent 119

Batch Gradient Descent 123

Stochastic Gradient Descent 126

Mini-batch Gradient Descent 129

Polynomial Regression 130

Learning Curves 132

Regularized Linear Models 136

Ridge Regression 137

Lasso Regression 139

Elastic Net 142

Early Stopping 142

Logistic Regression 144

Estimating Probabilities 144

Training and Cost Function 145

Decision Boundaries 146

Softmax Regression 149

Exercises 153

# 5. Support Vector Machines. . . . 155

Linear SVM Classification 155

Soft Margin Classification 156

Nonlinear SVM Classification 159

Polynomial Kernel 160

Adding Similarity Features 161

Gaussian RBF Kernel 162

Computational Complexity 163

SVM Regression 164

Under the Hood 166

Decision Function and Predictions 166

Training Objective 167

Quadratic Programming 169

The Dual Problem 170

Kernelized SVM 171

Online SVMs 174

Exercises 175

# 6. Decision Trees. . . 177

Training and Visualizing a Decision Tree 177

Making Predictions 179

Estimating Class Probabilities 181

The CART Training Algorithm 182

Computational Complexity 183

Gini Impurity or Entropy? 183

Regularization Hyperparameters 184

Regression 185

Instability 188

Exercises 189

# 7. Ensemble Learning and Random Forests. . . . 191

Voting Classifiers 192

Bagging and Pasting 195

Bagging and Pasting in Scikit-Learn 196

Out-of-Bag Evaluation 197

Random Patches and Random Subspaces 198

Random Forests 199

Extra-Trees 200

Feature Importance 200

Boosting 201

AdaBoost 202

Gradient Boosting 205

Stacking 210

Exercises 213

# 8. Dimensionality Reduction. . . . . 215

The Curse of Dimensionality 216

Main Approaches for Dimensionality Reduction 218

Projection 218

Manifold Learning 220

PCA 222

Preserving the Variance 222

Principal Components 223

Projecting Down to $d$ Dimensions 224

Using Scikit-Learn 224

Explained Variance Ratio 225

Choosing the Right Number of Dimensions 225

PCA for Compression 226

Randomized PCA 227

Incremental PCA 227

Kernel PCA 228

Selecting a Kernel and Tuning Hyperparameters 229

LLE 232

Other Dimensionality Reduction Techniques 234

Exercises 235

# 9. Unsupervised Learning Techniques. . . . 237

Clustering 238

K-Means

Limits of K-Means 250

Using clustering for image segmentation 251

Using Clustering for Preprocessing 252

Using Clustering for Semi-Supervised Learning 254

DBSCAN 256

Other Clustering Algorithms 259

Gaussian Mixtures 260

Anomaly Detection using Gaussian Mixtures 266

Selecting the Number of Clusters 267

Bayesian Gaussian Mixture Models 270

Other Anomaly Detection and Novelty Detection Algorithms 274

# Part II. Neural Networks and Deep Learning

# 10. Introduction to Articial Neural Networks with Keras. . . . 277

From Biological to Artificial Neurons 278

Biological Neurons 279

Logical Computations with Neurons 281

The Perceptron 281

Multi-Layer Perceptron and Backpropagation 286

Regression MLPs 289

Classification MLPs 290

Implementing MLPs with Keras 292

Installing TensorFlow 2 293

Building an Image Classifier Using the Sequential API 294

Building a Regression MLP Using the Sequential API 303

Building Complex Models Using the Functional API 304

Building Dynamic Models Using the Subclassing API 309

Saving and Restoring a Model 311

Using Callbacks 311

Visualization Using TensorBoard 313 Fine-Tuning Neural Network Hyperparameters 315 Number of Hidden Layers 319 Number of Neurons per Hidden Layer 320 Learning Rate, Batch Size and Other Hyperparameters 320 Exercises 322

11. Training Deep Neural Networks. . 325 Vanishing/Exploding Gradients Problems 326 Glorot and He Initialization 327 Nonsaturating Activation Functions 329 Batch Normalization 333 Gradient Clipping 338 Reusing Pretrained Layers 339 Transfer Learning With Keras 341 Unsupervised Pretraining 343 Pretraining on an Auxiliary Task 344 Faster Optimizers 344 Momentum Optimization 345 Nesterov Accelerated Gradient 346 AdaGrad 347 RMSProp 349 Adam and Nadam Optimization 349 Learning Rate Scheduling 352 Avoiding Overfitting Through Regularization 356 $\ell _ { 1 }$ and $\ell _ { 2 }$ Regularization 356 Dropout 357 Monte-Carlo (MC) Dropout 360 Max-Norm Regularization 362 Summary and Practical Guidelines 363 Exercises 364   
12. Custom Models and Training with TensorFlow. . . 367 A Quick Tour of TensorFlow 368 Using TensorFlow like NumPy 371 Tensors and Operations 371 Tensors and NumPy 373 Type Conversions 374 Variables 374 Other Data Structures 375 Customizing Models and Training Algorithms 376 Custom Loss Functions 376

Saving and Loading Models That Contain Custom Components 377

Custom Activation Functions, Initializers, Regularizers, and Constraints 379

Custom Metrics 380

Custom Layers 383

Custom Models 386

Losses and Metrics Based on Model Internals 388

Computing Gradients Using Autodiff 389

Custom Training Loops 393

TensorFlow Functions and Graphs 396

Autograph and Tracing 398

TF Function Rules 400

# 13. Loading and Preprocessing Data with TensorFlow. . . . . . 403

The Data API 404

Chaining Transformations 405

Shuffling the Data 406

Preprocessing the Data 409

Putting Everything Together 410

Prefetching 411

Using the Dataset With tf.keras 413

The TFRecord Format 414

Compressed TFRecord Files 415

A Brief Introduction to Protocol Buffers 415

TensorFlow Protobufs 416

Loading and Parsing Examples 418

Handling Lists of Lists Using the SequenceExample Protobuf 419

The Features API 420

Categorical Features 421

Crossed Categorical Features 421

Encoding Categorical Features Using One-Hot Vectors 422

Encoding Categorical Features Using Embeddings 423

Using Feature Columns for Parsing 426

Using Feature Columns in Your Models 426

TF Transform 428

The TensorFlow Datasets (TFDS) Project 429

# 14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . 431

The Architecture of the Visual Cortex 432

Convolutional Layer 434

Filters 436

Stacking Multiple Feature Maps 437

TensorFlow Implementation 439

<table><tr><td>Memory Requirements</td><td>441</td></tr><tr><td>Pooling Layer</td><td>442</td></tr><tr><td>TensorFlow Implementation</td><td>444</td></tr><tr><td>CNN Architectures</td><td>446</td></tr><tr><td>LeNet-5</td><td>449</td></tr><tr><td>AlexNet</td><td>450</td></tr><tr><td>GoogLeNet</td><td>452</td></tr><tr><td>VGGNet</td><td>456</td></tr><tr><td>ResNet</td><td>457</td></tr><tr><td>Xception</td><td>459</td></tr><tr><td>SENet</td><td>461</td></tr><tr><td>Implementing a ResNet-34 CNN Using Keras</td><td>464</td></tr><tr><td>Using Pretrained Models From Keras</td><td>465</td></tr><tr><td>Pretrained Models for Transfer Learning</td><td>467</td></tr><tr><td>Classification and Localization</td><td>469</td></tr><tr><td>Object Detection</td><td>471</td></tr><tr><td>Fully Convolutional Networks (FCNs)</td><td>473</td></tr><tr><td>You Only Look Once (YOLO)</td><td>475</td></tr><tr><td>Semantic Segmentation</td><td>478</td></tr><tr><td>Exercises</td><td>482</td></tr></table>

# The Machine Learning Tsunami

In 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural network capable of recognizing handwritten digits with state-of-the-art precision $( > 9 8 \% )$ . They branded this technique “Deep Learning.” Training a deep neural net was widely considered impossible at the time,2 and most researchers had abandoned the idea since the 1990s. This paper revived the interest of the scientific community and before long many new papers demonstrated that Deep Learning was not only possible, but capable of mind-blowing achievements that no other Machine Learning (ML) technique could hope to match (with the help of tremendous computing power and great amounts of data). This enthusiasm soon extended to many other areas of Machine Learning.

Fast-forward 10 years and Machine Learning has conquered the industry: it is now at the heart of much of the magic in today’s high-tech products, ranking your web search results, powering your smartphone’s speech recognition, recommending vid‐ eos, and beating the world champion at the game of Go. Before you know it, it will be driving your car.

# Machine Learning in Your Projects

So naturally you are excited about Machine Learning and you would love to join the party!

Perhaps you would like to give your homemade robot a brain of its own? Make it rec‐ ognize faces? Or learn to walk around?

Or maybe your company has tons of data (user logs, financial data, production data, machine sensor data, hotline stats, HR reports, etc.), and more than likely you could unearth some hidden gems if you just knew where to look; for example:

• Segment customers and find the best marketing strategy for each group   
• Recommend products for each client based on what similar clients bought   
• Detect which transactions are likely to be fraudulent   
• Forecast next year’s revenue   
• And more

Whatever the reason, you have decided to learn Machine Learning and implement it in your projects. Great idea!

# Objective and Approach

This book assumes that you know close to nothing about Machine Learning. Its goal is to give you the concepts, the intuitions, and the tools you need to actually imple‐ ment programs capable of learning from data.

We will cover a large number of techniques, from the simplest and most commonly used (such as linear regression) to some of the Deep Learning techniques that regu‐ larly win competitions.

Rather than implementing our own toy versions of each algorithm, we will be using actual production-ready Python frameworks:

• Scikit-Learn is very easy to use, yet it implements many Machine Learning algo‐ rithms efficiently, so it makes for a great entry point to learn Machine Learning.   
• TensorFlow is a more complex library for distributed numerical computation. It makes it possible to train and run very large neural networks efficiently by dis‐ tributing the computations across potentially hundreds of multi-GPU servers. TensorFlow was created at Google and supports many of their large-scale Machine Learning applications. It was open sourced in November 2015.   
Keras is a high level Deep Learning API that makes it very simple to train and run neural networks. It can run on top of either TensorFlow, Theano or Micro‐ soft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its own implementation of this API, called tf.keras, which provides support for some advanced TensorFlow features (e.g., to efficiently load data).

The book favors a hands-on approach, growing an intuitive understanding of Machine Learning through concrete working examples and just a little bit of theory. While you can read this book without picking up your laptop, we highly recommend

you experiment with the code examples available online as Jupyter notebooks at https://github.com/ageron/handson-ml2.

# Prerequisites

This book assumes that you have some Python programming experience and that you are familiar with Python’s main scientific libraries, in particular NumPy, Pandas, and Matplotlib.

Also, if you care about what’s under the hood you should have a reasonable under‐ standing of college-level math as well (calculus, linear algebra, probabilities, and sta‐ tistics).

If you don’t know Python yet, http://learnpython.org/ is a great place to start. The offi‐ cial tutorial on python.org is also quite good.

If you have never used Jupyter, Chapter 2 will guide you through installation and the basics: it is a great tool to have in your toolbox.

If you are not familiar with Python’s scientific libraries, the provided Jupyter note‐ books include a few tutorials. There is also a quick math tutorial for linear algebra.

# Roadmap

This book is organized in two parts. Part I, e Fundamentals of Machine Learning, covers the following topics:

• What is Machine Learning? What problems does it try to solve? What are the main categories and fundamental concepts of Machine Learning systems?   
• The main steps in a typical Machine Learning project.   
• Learning by fitting a model to data.   
• Optimizing a cost function.   
• Handling, cleaning, and preparing data.   
• Selecting and engineering features.   
• Selecting a model and tuning hyperparameters using cross-validation.   
• The main challenges of Machine Learning, in particular underfitting and overfit‐ ting (the bias/variance tradeoff).   
• Reducing the dimensionality of the training data to fight the curse of dimension‐ ality.   
• Other unsupervised learning techniques, including clustering, density estimation and anomaly detection.

• The most common learning algorithms: Linear and Polynomial Regression, Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision Trees, Random Forests, and Ensemble methods.

Part II, Neural Networks and Deep Learning, covers the following topics:

• What are neural nets? What are they good for?   
• Building and training neural nets using TensorFlow and Keras.   
• The most important neural net architectures: feedforward neural nets, convolu‐ tional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders and generative adversarial networks (GANs).   
• Techniques for training deep neural nets.   
• Scaling neural networks for large datasets.   
• Learning strategies with Reinforcement Learning.   
• Handling uncertainty with Bayesian Deep Learning.

The first part is based mostly on Scikit-Learn while the second part uses TensorFlow and Keras.

![](images/c2f8aea61349b717051d39b79d9dabf20d15941493fd8166748ca85e341ec3a6.jpg)

Don’t jump into deep waters too hastily: while Deep Learning is no doubt one of the most exciting areas in Machine Learning, you should master the fundamentals first. Moreover, most problems can be solved quite well using simpler techniques such as Random Forests and Ensemble methods (discussed in Part I). Deep Learn‐ ing is best suited for complex problems such as image recognition, speech recognition, or natural language processing, provided you have enough data, computing power, and patience.

# Other Resources

Many resources are available to learn about Machine Learning. Andrew Ng’s ML course on Coursera and Geoffrey Hinton’s course on neural networks and Deep Learning are amazing, although they both require a significant time investment (think months).

There are also many interesting websites about Machine Learning, including of course Scikit-Learn’s exceptional User Guide. You may also enjoy Dataquest, which provides very nice interactive tutorials, and ML blogs such as those listed on Quora. Finally, the Deep Learning website has a good list of resources to learn more.

Of course there are also many other introductory books about Machine Learning, in particular:

• Joel Grus, Data Science from Scratch (O’Reilly). This book presents the funda‐ mentals of Machine Learning, and implements some of the main algorithms in pure Python (from scratch, as the name suggests).

• Stephen Marsland, Machine Learning: An Algorithmic Perspective (Chapman and Hall). This book is a great introduction to Machine Learning, covering a wide range of topics in depth, with code examples in Python (also from scratch, but using NumPy).   
• Sebastian Raschka, Python Machine Learning (Packt Publishing). Also a great introduction to Machine Learning, this book leverages Python open source libra‐ ries (Pylearn 2 and Theano).   
• François Chollet, Deep Learning with Python (Manning). A very practical book that covers a large range of topics in a clear and concise way, as you might expect from the author of the excellent Keras library. It favors code examples over math‐ ematical theory.   
• Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from Data (AMLBook). A rather theoretical approach to ML, this book provides deep insights, in particular on the bias/variance tradeoff (see Chapter 4).   
• Stuart Russell and Peter Norvig, Articial Intelligence: A Modern Approach, 3rd Edition (Pearson). This is a great (and huge) book covering an incredible amount of topics, including Machine Learning. It helps put ML into perspective.

Finally, a great way to learn is to join ML competition websites such as Kaggle.com this will allow you to practice your skills on real-world problems, with help and insights from some of the best ML professionals out there.

# Conventions Used in This Book

The following typographical conventions are used in this book:

# Italic

Indicates new terms, URLs, email addresses, filenames, and file extensions.

# Constant width

Used for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements and keywords.

# Constant width bold

Shows commands or other text that should be typed literally by the user.

# Constant width italic

Shows text that should be replaced with user-supplied values or by values deter‐ mined by context.

![](images/ab1eae3baec266fa9d901d01af40c304c856ba5b4338094b84d4e615da0e9dea.jpg)

This element signifies a tip or suggestion.

![](images/72fccf39a0d7ef4a9e1e2f3f49d8096ebfdd7c8e9894577dfb94def7a6c27dfa.jpg)

This element signifies a general note.

![](images/6f76f750a76a51b82e185fd7adb39193f91c891d59cd3ce23b0b6527c48d9b9d.jpg)

This element indicates a warning or caution.

# Code Examples

Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/ageron/handson-ml2. It is mostly composed of Jupyter notebooks.

Some of the code examples in the book leave out some repetitive sections, or details that are obvious or unrelated to Machine Learning. This keeps the focus on the important parts of the code, and it saves space to cover more topics. However, if you want the full code examples, they are all available in the Jupyter notebooks.

Note that when the code examples display some outputs, then these code examples are shown with Python prompts $> > >$ and ...), as in a Python shell, to clearly distin‐ guish the code from the outputs. For example, this code defines the square() func‐ tion then it computes and displays the square of 3:

>>> def square(x): return x \*\* 2   
>>> result $=$ square(3)   
>>> result   
9

When code does not display anything, prompts are not used. However, the result may sometimes be shown as a comment like this:

```python
def square(x):
    return x ** 2
result = square(3) # result is 9 
```

# Using Code Examples

This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a signifi‐ cant amount of example code from this book into your product’s documentation does require permission.

We appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow by Aurélien Géron (O’Reilly). Copyright 2019 Aurélien Géron, 978-1-492-03264-9.” If you feel your use of code examples falls out‐ side fair use or the permission given above, feel free to contact us at permis‐ sions@oreilly.com.

# O’Reilly Safari

![](images/62ba70acaf4126ebfcd85405201a3443a51305ad5423f9e8cf7947a657aba56d.jpg)

#

Safari (formerly Safari Books Online) is a membership-based training and reference platform for enterprise, government, educators, and individuals.

Members have access to thousands of books, training videos, Learning Paths, interac‐ tive tutorials, and curated playlists from over 250 publishers, including O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐ sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and Course Technology, among others.

For more information, please visit http://oreilly.com/safari.

# How to Contact Us

Please address comments and questions concerning this book to the publisher:

O’Reilly Media, Inc.

1005 Gravenstein Highway North

Sebastopol, CA 95472

800-998-9938 (in the United States or Canada)

707-829-0515 (international or local)

707-829-0104 (fax)

We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at http://bit.ly/hands-on-machine-learningwith-scikit-learn-and-tensorow or https://homl.info/oreilly.

To comment or ask technical questions about this book, send email to bookques‐ tions@oreilly.com.

For more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com.

Find us on Facebook: http://facebook.com/oreilly

Follow us on Twitter: http://twitter.com/oreillymedia

Watch us on YouTube: http://www.youtube.com/oreillymedia

# Changes in the Second Edition

This second edition has five main objectives:

1. Cover additional topics: additional unsupervised learning techniques (including clustering, anomaly detection, density estimation and mixture models), addi‐ tional techniques for training deep nets (including self-normalized networks), additional computer vision techniques (including the Xception, SENet, object detection with YOLO, and semantic segmentation using R-CNN), handling sequences using CNNs (including WaveNet), natural language processing using RNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐ sorFlow models, and more.   
2. Update the book to mention some of the latest results from Deep Learning research.   
3. Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐ mentation of the Keras API (called tf.keras) whenever possible, to simplify the code examples.   
4. Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐ das, Matplotlib and other libraries.   
5. Clarify some sections and fix some errors, thanks to plenty of great feedback from readers.

Some chapters were added, others were rewritten and a few were reordered. Table P-1 shows the mapping between the $1 ^ { \mathrm { s t } }$ edition chapters and the $2 ^ { \mathrm { n d } }$ edition chapters:

Table P-1. Chapter mapping between $I ^ { s t }$ and $2 ^ { n d }$ edition   

<table><tr><td>1stEd. chapter</td><td>2ndEd. Chapter</td><td>% Changes</td><td>2ndEd. Title</td></tr><tr><td>1</td><td>1</td><td>&lt;10%</td><td>The Machine Learning Landscape</td></tr><tr><td>2</td><td>2</td><td>&lt;10%</td><td>End-to-End Machine Learning Project</td></tr><tr><td>3</td><td>3</td><td>&lt;10%</td><td>Classification</td></tr><tr><td>4</td><td>4</td><td>&lt;10%</td><td>Training Models</td></tr><tr><td>5</td><td>5</td><td>&lt;10%</td><td>Support Vector Machines</td></tr><tr><td>6</td><td>6</td><td>&lt;10%</td><td>Decision Trees</td></tr><tr><td>7</td><td>7</td><td>&lt;10%</td><td>Ensemble Learning and Random Forests</td></tr><tr><td>8</td><td>8</td><td>&lt;10%</td><td>Dimensionality Reduction</td></tr><tr><td>N/A</td><td>9</td><td>100% new</td><td>Unsupervised Learning Techniques</td></tr><tr><td>10</td><td>10</td><td>~75%</td><td>Introduction to Artificial Neural Networks with Keras</td></tr><tr><td>11</td><td>11</td><td>~50%</td><td>Training Deep Neural Networks</td></tr><tr><td>9</td><td>12</td><td>100% rewritten</td><td>Custom Models and Training with TensorFlow</td></tr><tr><td>Part of 12</td><td>13</td><td>100% rewritten</td><td>Loading and Preprocessing Data with TensorFlow</td></tr><tr><td>13</td><td>14</td><td>~50%</td><td>Deep Computer Vision Using Convolutional Neural Networks</td></tr><tr><td>Part of 14</td><td>15</td><td>~75%</td><td>Processing Sequences Using RNNs and CNNs</td></tr><tr><td>Part of 14</td><td>16</td><td>~90%</td><td>Natural Language Processing with RNNs and Attention</td></tr><tr><td>15</td><td>17</td><td>~75%</td><td>Autoencoders and GANs</td></tr><tr><td>16</td><td>18</td><td>~75%</td><td>Reinforcement Learning</td></tr><tr><td>Part of 12</td><td>19</td><td>100% rewritten</td><td>Deploying your TensorFlow Models</td></tr></table>

More specifically, here are the main changes for each $2 ^ { \mathrm { n d } }$ edition chapter (other than clarifications, corrections and code updates):

• Chapter 1

— Added a section on handling mismatch between the training set and the vali‐ dation & test sets.

• Chapter 2

— Added how to compute a confidence interval.

— Improved the installation instructions (e.g., for Windows).

— Introduced the upgraded OneHotEncoder and the new ColumnTransformer.

• Chapter 4

— Explained the need for training instances to be Independent and Identically Distributed (IID).

• Chapter 7

— Added a short section about XGBoost.

• Chapter 9 – new chapter including:

— Clustering with K-Means, how to choose the number of clusters, how to use it for dimensionality reduction, semi-supervised learning, image segmentation, and more.   
— The DBSCAN clustering algorithm and an overview of other clustering algo‐ rithms available in Scikit-Learn.   
— Gaussian mixture models, the Expectation-Maximization (EM) algorithm, Bayesian variational inference, and how mixture models can be used for clus‐ tering, density estimation, anomaly detection and novelty detection.   
— Overview of other anomaly detection and novelty detection algorithms.

• Chapter 10 (mostly new)

— Added an introduction to the Keras API, including all its APIs (Sequential, Functional and Subclassing), persistence and callbacks (including the Tensor Board callback).

• Chapter 11 (many changes)

— Introduced self-normalizing nets, the SELU activation function and Alpha Dropout.   
— Introduced self-supervised learning.   
— Added Nadam optimization.   
— Added Monte-Carlo Dropout.   
— Added a note about the risks of adaptive optimization methods.   
— Updated the practical guidelines.

• Chapter 12 – completely rewritten chapter, including:

— A tour of TensorFlow 2   
— TensorFlow’s lower-level Python API   
— Writing custom loss functions, metrics, layers, models   
— Using auto-differentiation and creating custom training algorithms.   
— TensorFlow Functions and graphs (including tracing and autograph).

• Chapter 13 – new chapter, including:

— The Data API   
— Loading/Storing data efficiently using TFRecords   
— The Features API (including an introduction to embeddings).   
— An overview of TF Transform and TF Datasets   
— Moved the low-level implementation of the neural network to the exercises.

— Removed details about queues and readers that are now superseded by the Data API.

• Chapter 14

— Added Xception and SENet architectures.   
— Added a Keras implementation of ResNet-34.   
— Showed how to use pretrained models using Keras.   
— Added an end-to-end transfer learning example.   
— Added classification and localization.   
— Introduced Fully Convolutional Networks (FCNs).   
— Introduced object detection using the YOLO architecture.   
— Introduced semantic segmentation using R-CNN.

• Chapter 15

— Added an introduction to Wavenet.   
— Moved the Encoder–Decoder architecture and Bidirectional RNNs to Chapter 16.

• Chapter 16

— Explained how to use the Data API to handle sequential data.   
— Showed an end-to-end example of text generation using a Character RNN, using both a stateless and a stateful RNN.   
— Showed an end-to-end example of sentiment analysis using an LSTM.   
— Explained masking in Keras.   
— Showed how to reuse pretrained embeddings using TF Hub.   
— Showed how to build an Encoder–Decoder for Neural Machine Translation using TensorFlow Addons/seq2seq.   
— Introduced beam search.   
— Explained attention mechanisms.   
— Added a short overview of visual attention and a note on explainability.   
— Introduced the fully attention-based Transformer architecture, including posi‐ tional embeddings and multi-head attention.   
— Added an overview of recent language models (2018).

• Chapters 17, 18 and 19: coming soon.

# Acknowledgments

Never in my wildest dreams did I imagine that the first edition of this book would get such a large audience. I received so many messages from readers, many asking ques‐ tions, some kindly pointing out errata, and most sending me encouraging words. I cannot express how grateful I am to all these readers for their tremendous support. Thank you all so very much! Please do not hesitate to file issues on github if you find errors in the code examples (or just to ask questions), or to submit errata if you find errors in the text. Some readers also shared how this book helped them get their first job, or how it helped them solve a concrete problem they were working on: I find such feedback incredibly motivating. If you find this book helpful, I would love it if you could share your story with me, either privately (e.g., via LinkedIn) or publicly (e.g., in an Amazon review).

I am also incredibly thankful to all the amazing people who took time out of their busy lives to review my book with such care. In particular, I would like to thank Fran‐ çois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving me some great, in-depth feedback. Since Keras is one of the main additions to this $2 ^ { \mathrm { n d } }$ edition, having its author review the book was invaluable. I highly recommend Fran‐ çois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and depth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed every chapter of this $2 ^ { \mathrm { n d } }$ edition and gave me excellent feedback.

This book also benefited from plenty of help from members of the TensorFlow team, in particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐ patched the rest to the right people, including Alexandre Passos, Allen Lavoie, André Susano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐ van, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐ mel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the $1 ^ { \mathrm { s t } }$ edition), Ryan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William Chargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank you to all of you, and to all other members of the TensorFlow team. Not just for your help, but also for making such a great library.

Big thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐ eral errors while he was writing the Korean translation of the $1 ^ { \mathrm { s t } }$ edition of this book. He also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s documentation. I do not speak Korean, but judging by the quality of his feedback, all his translations must be truly excellent! Moreover, he kindly contributed some of the solutions to the exercises in this book.

Many thanks as well to O’Reilly’s fantastic staff, in particular Nicole Tache, who gave me insightful feedback, always cheerful, encouraging, and helpful: I could not dream of a better editor. Big thanks to Michele Cronin as well, who was very helpful (and patient) at the start of this $2 ^ { \mathrm { n d } }$ edition. Thanks to Marie Beaugureau, Ben Lorica, Mike Loukides, and Laurel Ruma for believing in this project and helping me define its scope. Thanks to Matt Hacker and all of the Atlas team for answering all my technical questions regarding formatting, asciidoc, and LaTeX, and thanks to Rachel Mona‐ ghan, Nick Adams, and all of the production team for their final review and their hundreds of corrections.

I would also like to thank my former Google colleagues, in particular the YouTube video classification team, for teaching me so much about Machine Learning. I could never have started the first edition without them. Special thanks to my personal ML gurus: Clément Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James Pack, Alexander Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn, Rich Washington, and everyone I worked with at YouTube and in the amazing Goo‐ gle research teams in Mountain View. All these people are just as nice and helpful as they are bright, and that’s saying a lot.

I will never forget the kind people who reviewed the $1 ^ { \mathrm { s t } }$ edition of this book, including David Andrzejewski, Eddy Hung, Grégoire Mesnil, Iain Smears, Ingrid von Glehn, Justin Francis, Karim Matrah, Lukas Biewald, Michel Tessier, Salim Sémaoune, Vin‐ cent Guilbeau and of course my dear brother Sylvain.

Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our three wonderful children, Alexandre, Rémi, and Gabrielle, for encouraging me to work hard on this book, as well as for their insatiable curiosity: explaining some of the most difficult concepts in this book to my wife and children helped me clarify my thoughts and directly improved many parts of this book. Plus, they keep bringing me cookies and coffee! What more can one dream of?

# The Fundamentals of Machine Learning

# The Machine Learning Landscape

![](images/12b505e2bab9bba5a8391439bb87a4c1d64d650b67903c0bdd2395e3f0bdd64c.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 1 in the final release of the book.

When most people hear “Machine Learning,” they picture a robot: a dependable but‐ ler or a deadly Terminator depending on who you ask. But Machine Learning is not just a futuristic fantasy, it’s already here. In fact, it has been around for decades in some specialized applications, such as Optical Character Recognition (OCR). But the first ML application that really became mainstream, improving the lives of hundreds of millions of people, took over the world back in the 1990s: it was the spam lter. Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning (it has actually learned so well that you seldom need to flag an email as spam any‐ more). It was followed by hundreds of ML applications that now quietly power hun‐ dreds of products and features that you use regularly, from better recommendations to voice search.

Where does Machine Learning start and where does it end? What exactly does it mean for a machine to learn something? If I download a copy of Wikipedia, has my computer really “learned” something? Is it suddenly smarter? In this chapter we will start by clarifying what Machine Learning is and why you may want to use it.

Then, before we set out to explore the Machine Learning continent, we will take a look at the map and learn about the main regions and the most notable landmarks: supervised versus unsupervised learning, online versus batch learning, instancebased versus model-based learning. Then we will look at the workflow of a typical ML project, discuss the main challenges you may face, and cover how to evaluate and fine-tune a Machine Learning system.

This chapter introduces a lot of fundamental concepts (and jargon) that every data scientist should know by heart. It will be a high-level overview (the only chapter without much code), all rather simple, but you should make sure everything is crystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s get started!

![](images/3490719fbacef1687f746052fadcf9ac45d4bffdc116729a315c54e4f154eaa0.jpg)

If you already know all the Machine Learning basics, you may want to skip directly to Chapter 2. If you are not sure, try to answer all the questions listed at the end of the chapter before moving on.

# What Is Machine Learning?

Machine Learning is the science (and art) of programming computers so they can learn from data.

Here is a slightly more general definition:

[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.

—Arthur Samuel, 1959

And a more engineering-oriented one:

A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.

—Tom Mitchell, 1997

For example, your spam filter is a Machine Learning program that can learn to flag spam given examples of spam emails (e.g., flagged by users) and examples of regular (nonspam, also called “ham”) emails. The examples that the system uses to learn are called the training set. Each training example is called a training instance (or sample). In this case, the task T is to flag spam for new emails, the experience E is the training data, and the performance measure P needs to be defined; for example, you can use the ratio of correctly classified emails. This particular performance measure is called accuracy and it is often used in classification tasks.

If you just download a copy of Wikipedia, your computer has a lot more data, but it is not suddenly better at any task. Thus, it is not Machine Learning.

# Why Use Machine Learning?

Consider how you would write a spam filter using traditional programming techni‐ ques (Figure 1-1):

1. First you would look at what spam typically looks like. You might notice that some words or phrases (such as “4U,” “credit card,” “free,” and “amazing”) tend to come up a lot in the subject. Perhaps you would also notice a few other patterns in the sender’s name, the email’s body, and so on.   
2. You would write a detection algorithm for each of the patterns that you noticed, and your program would flag emails as spam if a number of these patterns are detected.   
3. You would test your program, and repeat steps 1 and 2 until it is good enough.

![](images/711f7346eb974182a5b3e62d60d58863b111da0c11ac5ea6ed27377e5debe17a.jpg)  
Figure 1-1. e traditional approach

Since the problem is not trivial, your program will likely become a long list of com‐ plex rules—pretty hard to maintain.

In contrast, a spam filter based on Machine Learning techniques automatically learns which words and phrases are good predictors of spam by detecting unusually fre‐ quent patterns of words in the spam examples compared to the ham examples (Figure 1-2). The program is much shorter, easier to maintain, and most likely more accurate.

![](images/87ac65a6274a28c6336abbb1c510158f13307962b1555428b93ff0aa65619e91.jpg)  
Figure 1-2. Machine Learning approach

Moreover, if spammers notice that all their emails containing $^ { \mathfrak { a } } 4 \mathrm { U } ^ { \mathfrak { p } }$ are blocked, they might start writing “For $\boldsymbol { \mathrm { U } } ^ { \flat }$ instead. A spam filter using traditional programming techniques would need to be updated to flag “For U” emails. If spammers keep work‐ ing around your spam filter, you will need to keep writing new rules forever.

In contrast, a spam filter based on Machine Learning techniques automatically noti‐ ces that “For $\boldsymbol { \mathrm { U } } ^ { \flat }$ has become unusually frequent in spam flagged by users, and it starts flagging them without your intervention (Figure 1-3).

![](images/01eb60306f3ed3342b2be1faa6d1bfb7d0b0e74eb8615dc5a663e1e51e1022d6.jpg)  
Figure 1-3. Automatically adapting to change

Another area where Machine Learning shines is for problems that either are too com‐ plex for traditional approaches or have no known algorithm. For example, consider speech recognition: say you want to start simple and write a program capable of dis‐ tinguishing the words “one” and “two.” You might notice that the word “two” starts with a high-pitch sound (“T”), so you could hardcode an algorithm that measures high-pitch sound intensity and use that to distinguish ones and twos. Obviously this technique will not scale to thousands of words spoken by millions of very different

people in noisy environments and in dozens of languages. The best solution (at least today) is to write an algorithm that learns by itself, given many example recordings for each word.

Finally, Machine Learning can help humans learn (Figure 1-4): ML algorithms can be inspected to see what they have learned (although for some algorithms this can be tricky). For instance, once the spam filter has been trained on enough spam, it can easily be inspected to reveal the list of words and combinations of words that it believes are the best predictors of spam. Sometimes this will reveal unsuspected cor‐ relations or new trends, and thereby lead to a better understanding of the problem.

Applying ML techniques to dig into large amounts of data can help discover patterns that were not immediately apparent. This is called data mining.

![](images/d65ec3d39c45787ed33382abd6eb6e5fcc35e0e269b898448d69e5b34be56824.jpg)  
Figure 1-4. Machine Learning can help humans learn

To summarize, Machine Learning is great for:

• Problems for which existing solutions require a lot of hand-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform bet‐ ter.   
• Complex problems for which there is no good solution at all using a traditional approach: the best Machine Learning techniques can find a solution.   
• Fluctuating environments: a Machine Learning system can adapt to new data.   
• Getting insights about complex problems and large amounts of data.

# Types of Machine Learning Systems

There are so many different types of Machine Learning systems that it is useful to classify them in broad categories based on:

• Whether or not they are trained with human supervision (supervised, unsuper‐ vised, semisupervised, and Reinforcement Learning)   
• Whether or not they can learn incrementally on the fly (online versus batch learning)   
• Whether they work by simply comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model, much like scientists do (instance-based versus model-based learning)

These criteria are not exclusive; you can combine them in any way you like. For example, a state-of-the-art spam filter may learn on the fly using a deep neural net‐ work model trained using examples of spam and ham; this makes it an online, modelbased, supervised learning system.

Let’s look at each of these criteria a bit more closely.

# Supervised/Unsupervised Learning

Machine Learning systems can be classified according to the amount and type of supervision they get during training. There are four major categories: supervised learning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐ ing.

# Supervised learning

In supervised learning, the training data you feed to the algorithm includes the desired solutions, called labels (Figure 1-5).

![](images/63a7a535c9ba56a38b9d88a1bd31b1374f6c9290f24861699dcdd6a6ba666eb1.jpg)  
Figure 1-5. A labeled training set for supervised learning (e.g., spam classication)

A typical supervised learning task is classication. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails.

Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is called regression (Figure 1-6).1 To train the system, you need to give it many examples of cars, including both their predictors and their labels (i.e., their prices).

![](images/20c7e28434033a586ecccc4b4c7df62814c79dcdaf9c3bde4202a654f85a020b.jpg)

In Machine Learning an attribute is a data type (e.g., “Mileage”), while a feature has several meanings depending on the context, but generally means an attribute plus its value (e.g., “Mileage $=$ $1 5 { , } 0 0 0 ^ { \ ' } )$ . Many people use the words attribute and feature inter‐ changeably, though.

![](images/c7876f6ecb4c6b6d1b1841410810a4af5a7f0db19ecab55d29324c008652fa63.jpg)  
Figure 1-6. Regression

Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., $2 0 \%$ chance of being spam).

Here are some of the most important supervised learning algorithms (covered in this book):

• k-Nearest Neighbors   
• Linear Regression   
• Logistic Regression   
• Support Vector Machines (SVMs)   
• Decision Trees and Random Forests   
• Neural networks2

# Unsupervised learning

In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher.

![](images/5a1581214ac584ec4c8a8eba7fa6a21736a4099c711ae617e6952005dac52328.jpg)  
Figure 1-7. An unlabeled training set for unsupervised learning

Here are some of the most important unsupervised learning algorithms (most of these are covered in Chapter 8 and Chapter 9):

• Clustering

— K-Means   
— DBSCAN   
— Hierarchical Cluster Analysis (HCA)

• Anomaly detection and novelty detection

— One-class SVM   
— Isolation Forest

• Visualization and dimensionality reduction

— Principal Component Analysis (PCA)   
— Kernel PCA   
— Locally-Linear Embedding (LLE)   
— t-distributed Stochastic Neighbor Embedding (t-SNE)

• Association rule learning

— Apriori   
— Eclat

For example, say you have a lot of data about your blog’s visitors. You may want to run a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At no point do you tell the algorithm which group a visitor belongs to: it finds those connections without your help. For example, it might notice that $4 0 \%$ of your visitors are males who love comic books and generally read your blog in the evening, while $2 0 \%$ are young sci-fi lovers who visit during the weekends, and so on. If you use a hierarchical clustering algorithm, it may also subdivide each group into smaller groups. This may help you target your posts for each group.

![](images/7a2890be9bc8e0a332f121843fc7533a16d3b55e628d84475786fd488442fe66.jpg)  
Figure 1-8. Clustering

Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep‐ resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns.

![](images/09a8187cec5ea1adf2e01014c70470ee765a772935384ec593c470f1ac108ec5.jpg)

![](images/ddb1eeb1c1c07bce67b1719fff4d35dd0260bd12c59caa97479294c834374731.jpg)  
Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3

A related task is dimensionality reduction, in which the goal is to simplify the data without losing too much information. One way to do this is to merge several correla‐ ted features into one. For example, a car’s mileage may be very correlated with its age, so the dimensionality reduction algorithm will merge them into one feature that rep‐ resents the car’s wear and tear. This is called feature extraction.

![](images/1dfa43921add2d0a715a8aa80eb06c73a6afbbc9ca12cfd8ad2d5d8572296370.jpg)

It is often a good idea to try to reduce the dimension of your train‐ ing data using a dimensionality reduction algorithm before you feed it to another Machine Learning algorithm (such as a super‐ vised learning algorithm). It will run much faster, the data will take up less disk and memory space, and in some cases it may also per‐ form better.

Yet another important unsupervised task is anomaly detection—for example, detect‐ ing unusual credit card transactions to prevent fraud, catching manufacturing defects, or automatically removing outliers from a dataset before feeding it to another learn‐ ing algorithm. The system is shown mostly normal instances during training, so it learns to recognize them and when it sees a new instance it can tell whether it looks

like a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar task is novelty detection: the difference is that novelty detection algorithms expect to see only normal data during training, while anomaly detection algorithms are usually more tolerant, they can often perform well even with a small percentage of outliers in the training set.

![](images/b3ed269359bf36055091dbd13369c2e96f63a0237e3734eebc52cc72be9fb9ac.jpg)  
Figure 1-10. Anomaly detection

Finally, another common unsupervised task is association rule learning, in which the goal is to dig into large amounts of data and discover interesting relations between attributes. For example, suppose you own a supermarket. Running an association rule on your sales logs may reveal that people who purchase barbecue sauce and potato chips also tend to buy steak. Thus, you may want to place these items close to each other.

# Semisupervised learning

Some algorithms can deal with partially labeled training data, usually a lot of unla‐ beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11).

Some photo-hosting services, such as Google Photos, are good examples of this. Once you upload all your family photos to the service, it automatically recognizes that the same person A shows up in photos 1, 5, and 11, while another person B shows up in photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all the system needs is for you to tell it who these people are. Just one label per person,4 and it is able to name everyone in every photo, which is useful for searching photos.

![](images/d6ff9a103311a77fd42a3604c7597a131dd441a33809f9906ab0a8222adeeda5.jpg)  
Figure 1-11. Semisupervised learning

Most semisupervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, deep belief networks (DBNs) are based on unsu‐ pervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the whole system is fine-tuned using supervised learning techniques.

# Reinforcement Learning

Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation.

![](images/a12741b2fc9ad6b142bca37b19d654b21b7f368381b48de258c676d57d44fe29.jpg)  
Figure 1-12. Reinforcement Learning

For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the games against the champion; AlphaGo was just applying the policy it had learned.

# Batch and Online Learning

Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data.

# Batch learning

In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing resources, so it is typically done offline. First the system is trained, and then it is launched into production and runs without learning anymore; it just applies what it has learned. This is called oine learning.

If you want a batch learning system to know about new data (such as a new type of spam), you need to train a new version of the system from scratch on the full dataset (not just the new data, but also the old data), then stop the old system and replace it with the new one.

Fortunately, the whole process of training, evaluating, and launching a Machine Learning system can be automated fairly easily (as shown in Figure 1-3), so even a

batch learning system can adapt to change. Simply update the data and train a new version of the system from scratch as often as needed.

This solution is simple and often works fine, but training using the full set of data can take many hours, so you would typically train a new system only every 24 hours or even just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐ dict stock prices), then you need a more reactive solution.

Also, training on the full set of data requires a lot of computing resources (CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and you automate your system to train from scratch every day, it will end up costing you a lot of money. If the amount of data is huge, it may even be impossible to use a batch learning algorithm.

Finally, if your system needs to be able to learn autonomously and it has limited resources (e.g., a smartphone application or a rover on Mars), then carrying around large amounts of training data and taking up a lot of resources to train for hours every day is a showstopper.

Fortunately, a better option in all these cases is to use algorithms that are capable of learning incrementally.

# Online learning

In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives (see Figure 1-13).

![](images/a543766d58f5e291f2c566c698d4da91ec709720725ec769bde2a324c23cf858.jpg)  
Figure 1-13. Online learning

Online learning is great for systems that receive data as a continuous flow (e.g., stock prices) and need to adapt to change rapidly or autonomously. It is also a good option

if you have limited computing resources: once an online learning system has learned about new data instances, it does not need them anymore, so you can discard them (unless you want to be able to roll back to a previous state and “replay” the data). This can save a huge amount of space.

Online learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine’s main memory (this is called out-of-core learning). The algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14).

![](images/48805b04db45a44005f1dd65b7c9e37eb2cf13ef7a5e24307f0c8621b0fa0939.jpg)

Out-of-core learning is usually done offline (i.e., not on the live system), so online learning can be a confusing name. Think of it as incremental learning.

![](images/f9774b3a8b1514cee7dd470e88239865b8aaaa252951097fd5b21c853c66f97f.jpg)  
Figure 1-14. Using online learning to handle huge datasets

One important parameter of online learning systems is how fast they should adapt to changing data: this is called the learning rate. If you set a high learning rate, then your system will rapidly adapt to new data, but it will also tend to quickly forget the old data (you don’t want a spam filter to flag only the latest kinds of spam it was shown). Conversely, if you set a low learning rate, the system will have more inertia; that is, it will learn more slowly, but it will also be less sensitive to noise in the new data or to sequences of nonrepresentative data points (outliers).

A big challenge with online learning is that if bad data is fed to the system, the sys‐ tem’s performance will gradually decline. If we are talking about a live system, your clients will notice. For example, bad data could come from a malfunctioning sensor on a robot, or from someone spamming a search engine to try to rank high in search

results. To reduce this risk, you need to monitor your system closely and promptly switch learning off (and possibly revert to a previously working state) if you detect a drop in performance. You may also want to monitor the input data and react to abnormal data (e.g., using an anomaly detection algorithm).

# Instance-Based Versus Model-Based Learning

One more way to categorize Machine Learning systems is by how they generalize. Most Machine Learning tasks are about making predictions. This means that given a number of training examples, the system needs to be able to generalize to examples it has never seen before. Having a good performance measure on the training data is good, but insufficient; the true goal is to perform well on new instances.

There are two main approaches to generalization: instance-based learning and model-based learning.

# Instance-based learning

Possibly the most trivial form of learning is simply to learn by heart. If you were to create a spam filter this way, it would just flag all emails that are identical to emails that have already been flagged by users—not the worst solution, but certainly not the best.

Instead of just flagging emails that are identical to known spam emails, your spam filter could be programmed to also flag emails that are very similar to known spam emails. This requires a measure of similarity between two emails. A (very basic) simi‐ larity measure between two emails could be to count the number of words they have in common. The system would flag an email as spam if it has many words in com‐ mon with a known spam email.

This is called instance-based learning: the system learns the examples by heart, then generalizes to new cases by comparing them to the learned examples (or a subset of them), using a similarity measure. For example, in Figure 1-15 the new instance would be classified as a triangle because the majority of the most similar instances belong to that class.

![](images/73fcb7cf1b388f571e38aae86fad7b540f0d16128597793c0ce66aeca19de06d.jpg)  
Figure 1-15. Instance-based learning

# Model-based learning

Another way to generalize from a set of examples is to build a model of these exam‐ ples, then use that model to make predictions. This is called model-based learning (Figure 1-16).

![](images/6045c229c0a2fd4fab5118e41f4cead89cc09e37258d0bae1f0d9ce2ffca09bd.jpg)  
Figure 1-16. Model-based learning

For example, suppose you want to know if money makes people happy, so you down‐ load the Better Life Index data from the OECD’s website as well as stats about GDP per capita from the IMF’s website. Then you join the tables and sort by GDP per cap‐ ita. Table 1-1 shows an excerpt of what you get.

Table 1-1. Does money make people happier?   

<table><tr><td>Country</td><td>GDP per capita (USD)</td><td>Life satisfaction</td></tr><tr><td>Hungary</td><td>12,240</td><td>4.9</td></tr><tr><td>Korea</td><td>27,195</td><td>5.8</td></tr><tr><td>France</td><td>37,675</td><td>6.5</td></tr><tr><td>Australia</td><td>50,962</td><td>7.3</td></tr><tr><td>United States</td><td>55,805</td><td>7.2</td></tr></table>

Let’s plot the data for a few random countries (Figure 1-17).

![](images/4c978a70258582223dbec478857d30cc4aee6d9f0c260b132da0dfb7539ae704.jpg)  
Figure 1-17. Do you see a trend here?

There does seem to be a trend here! Although the data is noisy (i.e., partly random), it looks like life satisfaction goes up more or less linearly as the country’s GDP per cap‐ ita increases. So you decide to model life satisfaction as a linear function of GDP per capita. This step is called model selection: you selected a linear model of life satisfac‐ tion with just one attribute, GDP per capita (Equation 1-1).

Equation 1-1. A simple linear model

life_satisfaction $\mathbf { \sigma } = \theta _ { 0 } + \theta _ { 1 } \times \mathrm { G D P }$ _per_capita

This model has two model parameters, $\theta _ { 0 }$ and $\theta _ { 1 }$ .5 By tweaking these parameters, you can make your model represent any linear function, as shown in Figure 1-18.

![](images/dacf4e1283a68b1660fd98f6f6156588ae66250f231556fb44a09a76964f1d80.jpg)  
Figure 1-18. A few possible linear models

Before you can use your model, you need to define the parameter values $\theta _ { 0 }$ and $\theta _ { 1 }$ . How can you know which values will make your model perform best? To answer this question, you need to specify a performance measure. You can either define a utility function (or tness function) that measures how good your model is, or you can define a cost function that measures how bad it is. For linear regression problems, people typically use a cost function that measures the distance between the linear model’s predictions and the training examples; the objective is to minimize this distance.

This is where the Linear Regression algorithm comes in: you feed it your training examples and it finds the parameters that make the linear model fit best to your data. This is called training the model. In our case the algorithm finds that the optimal parameter values are $\theta _ { 0 } = 4 . 8 5$ and $\theta _ { \mathrm { { 1 } } } = 4 . 9 1 \times 1 0 ^ { - 5 }$ .

Now the model fits the training data as closely as possible (for a linear model), as you can see in Figure 1-19.

![](images/b6fb70c15b2fb5e508e3a0eee6122d6f9a84132e8cd19899a992c030dc42a415.jpg)  
Figure 1-19. e linear model that ts the training data best

You are finally ready to run the model to make predictions. For example, say you want to know how happy Cypriots are, and the OECD data does not have the answer. Fortunately, you can use your model to make a good prediction: you look up Cyprus’s GDP per capita, find $\$ 22,587$ , and then apply your model and find that life satisfac‐ tion is likely to be somewhere around $4 . 8 5 + 2 2 , 5 8 7 \times 4 . 9 1 \times 1 0 ^ { - 5 } = 5 . 9 6$ .

To whet your appetite, Example 1-1 shows the Python code that loads the data, pre‐ pares it,6 creates a scatterplot for visualization, and then trains a linear model and makes a prediction.7

Example 1-1. Training and running a linear model using Scikit-Learn   
```python
import matplotlib.pyplot as plt  
import numpy as np  
import pandas as pd  
import sklearn.linear_model  
# Load the data  
oecd_bli = pd.read_csv("oecd_bli_2015.csv", thousands=['',])  
gdp_per_capita = pd.read_csv("gdp_per_capita.csv", thousands=['', 'delimiter='%t', encoding='latin1', na_values="n/a")  
# Prepare the data  
country.stats = prepare_country/stats(oecd_bli, gdp_per_capita)  
X = np.c_[country.stats["GDP per capita"]]  
y = np.c_[country.stats["Life satisfaction"]]  
# Visualize the data  
country.stats.plot(kind='scatter', x="GDP per capita", y='Life satisfaction')  
plt.show()  
# Select a linear model  
model = sklearn.linear_model.LinearRegression()  
# Train the model  
model.fit(X, y)  
# Make a prediction for Cyprus  
X_new = [[22587]] # Cyprus' GDP per capita  
print(model.predict(X_new)) # outputs [[ 5.96242338]] 
```

![](images/0be70754245a0dd82f073feac67a214609a766c0a26e4edb4686ff66f74fdaf2.jpg)

If you had used an instance-based learning algorithm instead, you would have found that Slovenia has the closest GDP per capita to that of Cyprus $( \$ 20,732)$ , and since the OECD data tells us that Slovenians’ life satisfaction is 5.7, you would have predicted a life satisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the two next closest countries, you will find Portugal and Spain with life satisfactions of 5.1 and 6.5, respectively. Averaging these three values, you get 5.77, which is pretty close to your model-based pre‐ diction. This simple algorithm is called $k$ -Nearest Neighbors regres‐ sion (in this example, $k = 3$ ).

Replacing the Linear Regression model with k-Nearest Neighbors regression in the previous code is as simple as replacing these two lines:

import sklearn.linear_model

model $=$ sklearn.linear_model.LinearRegression()

with these two:

import sklearn.neighbors

model $=$ sklearn.neighbors.KNeighborsRegressor(n_neighbors $\scriptstyle \mathbf { \alpha = { \begin{array} { l } { \mathbf { \alpha } } \\ { \mathbf { \beta = { \frac { } { } } } } \end{array} } }$ )

If all went well, your model will make good predictions. If not, you may need to use more attributes (employment rate, health, air pollution, etc.), get more or better qual‐ ity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres‐ sion model).

# In summary:

• You studied the data.   
• You selected a model.   
• You trained it on the training data (i.e., the learning algorithm searched for the model parameter values that minimize a cost function).   
• Finally, you applied the model to make predictions on new cases (this is called inference), hoping that this model will generalize well.

This is what a typical Machine Learning project looks like. In Chapter 2 you will experience this first-hand by going through an end-to-end project.

We have covered a lot of ground so far: you now know what Machine Learning is really about, why it is useful, what some of the most common categories of ML sys‐ tems are, and what a typical project workflow looks like. Now let’s look at what can go wrong in learning and prevent you from making accurate predictions.

# Main Challenges of Machine Learning

In short, since your main task is to select a learning algorithm and train it on some data, the two things that can go wrong are “bad algorithm” and “bad data.” Let’s start with examples of bad data.

# Insucient Quantity of Training Data

For a toddler to learn what an apple is, all it takes is for you to point to an apple and say “apple” (possibly repeating this procedure a few times). Now the child is able to recognize apples in all sorts of colors and shapes. Genius.

Machine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐ ing algorithms to work properly. Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recogni‐ tion you may need millions of examples (unless you can reuse parts of an existing model).

# The Unreasonable Eectiveness of Data

In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric Brill showed that very different Machine Learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation8 once they were given enough data (as you can see in Figure 1-20).

![](images/b9058e79600095c12ff312bd111bba21f7763a8191d9e517f42be12a70b9796e.jpg)  
Figure 1-20. e importance of data versus algorithms9

As the authors put it: “these results suggest that we may want to reconsider the tradeoff between spending time and money on algorithm development versus spending it on corpus development.”

The idea that data matters more than algorithms for complex problems was further popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness of Data” published in 2009.10 It should be noted, however, that small- and mediumsized datasets are still very common, and it is not always easy or cheap to get extra training data, so don’t abandon algorithms just yet.

# Nonrepresentative Training Data

In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is true whether you use instance-based learning or model-based learning.

For example, the set of countries we used earlier for training the linear model was not perfectly representative; a few countries were missing. Figure 1-21 shows what the data looks like when you add the missing countries.

![](images/8fd70dba537ff5f9d8b489f8fc65366da9c3ec6f803ab9dd32ac4c9908c393cd.jpg)  
Figure 1-21. A more representative training sample

If you train a linear model on this data, you get the solid line, while the old model is represented by the dotted line. As you can see, not only does adding a few missing countries significantly alter the model, but it makes it clear that such a simple linear model is probably never going to work well. It seems that very rich countries are not happier than moderately rich countries (in fact they seem unhappier), and conversely some poor countries seem happier than many rich countries.

By using a nonrepresentative training set, we trained a model that is unlikely to make accurate predictions, especially for very poor and very rich countries.

It is crucial to use a training set that is representative of the cases you want to general‐ ize to. This is often harder than it sounds: if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.

# A Famous Example of Sampling Bias

Perhaps the most famous example of sampling bias happened during the US presi‐ dential election in 1936, which pitted Landon against Roosevelt: the Literary Digest conducted a very large poll, sending mail to about 10 million people. It got 2.4 million answers, and predicted with high confidence that Landon would get $5 7 \%$ of the votes.

Instead, Roosevelt won with $6 2 \%$ of the votes. The flaw was in the Literary Digest’s sampling method:

• First, to obtain the addresses to send the polls to, the Literary Digest used tele‐ phone directories, lists of magazine subscribers, club membership lists, and the like. All of these lists tend to favor wealthier people, who are more likely to vote Republican (hence Landon).   
• Second, less than $2 5 \%$ of the people who received the poll answered. Again, this introduces a sampling bias, by ruling out people who don’t care much about poli‐ tics, people who don’t like the Literary Digest, and other key groups. This is a spe‐ cial type of sampling bias called nonresponse bias.

Here is another example: say you want to build a system to recognize funk music vid‐ eos. One way to build your training set is to search “funk music” on YouTube and use the resulting videos. But this assumes that YouTube’s search engine returns a set of videos that are representative of all the funk music videos on YouTube. In reality, the search results are likely to be biased toward popular artists (and if you live in Brazil you will get a lot of “funk carioca” videos, which sound nothing like James Brown). On the other hand, how else can you get a large training set?

# Poor-Quality Data

Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poorquality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. It is often well worth the effort to spend time cleaning up your training data. The truth is, most data scientists spend a significant part of their time doing just that. For example:

• If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.   
• If some instances are missing a few features (e.g., $5 \%$ of your customers did not specify their age), you must decide whether you want to ignore this attribute alto‐ gether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it, and so on.

# Irrelevant Features

As the saying goes: garbage in, garbage out. Your system will only be capable of learn‐ ing if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves:

• Feature selection: selecting the most useful features to train on among existing features.   
• Feature extraction: combining existing features to produce a more useful one (as we saw earlier, dimensionality reduction algorithms can help).   
• Creating new features by gathering new data.

Now that we have looked at many examples of bad data, let’s look at a couple of exam‐ ples of bad algorithms.

# Overtting the Training Data

Say you are visiting a foreign country and the taxi driver rips you off. You might be tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is something that we humans do all too often, and unfortunately machines can fall into the same trap if we are not careful. In Machine Learning this is called overtting: it means that the model performs well on the training data, but it does not generalize well.

![](images/79c1501cb0110738fd5d2ab1ea2823a0cf5642164359e1aec62c662fbae443fa.jpg)  
Figure 1-22 shows an example of a high-degree polynomial life satisfaction model that strongly overfits the training data. Even though it performs much better on the training data than the simple linear model, would you really trust its predictions?   
Figure 1-22. Overtting the training data

Complex models such as deep neural networks can detect subtle patterns in the data, but if the training set is noisy, or if it is too small (which introduces sampling noise), then the model is likely to detect patterns in the noise itself. Obviously these patterns will not generalize to new instances. For example, say you feed your life satisfaction model many more attributes, including uninformative ones such as the country’s name. In that case, a complex model may detect patterns like the fact that all coun‐ tries in the training data with a w in their name have a life satisfaction greater than 7: New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident

are you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously this pattern occurred in the training data by pure chance, but the model has no way to tell whether a pattern is real or simply the result of noise in the data.

![](images/97903b8a846d8557cb66487dc438cbea5652c3983ad67cefd372b407f1186f6f.jpg)

Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. The possible solutions are:

• To simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model   
• To gather more training data   
• To reduce the noise in the training data (e.g., fix data errors and remove outliers)

Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, $\theta _ { 0 }$ and $\theta _ { 1 }$ . This gives the learning algorithm two degrees of freedom to adapt the model to the training data: it can tweak both the height $( \theta _ { 0 } )$ and the slope $( \theta _ { 1 } )$ of the line. If we forced $\theta _ { \mathrm { 1 } } = 0$ , the algorithm would have only one degree of freedom and would have a much harder time fitting the data properly: all it could do is move the line up or down to get as close as possible to the training instances, so it would end up around the mean. A very simple model indeed! If we allow the algorithm to modify $\theta _ { 1 }$ but we force it to keep it small, then the learning algorithm will effectively have some‐ where in between one and two degrees of freedom. It will produce a simpler model than with two degrees of freedom, but more complex than with just one. You want to find the right balance between fitting the training data perfectly and keeping the model simple enough to ensure that it will generalize well.

Figure 1-23 shows three models: the dotted line represents the original model that was trained with a few countries missing, the dashed line is our second model trained with all countries, and the solid line is a linear model trained with the same data as the first model but with a regularization constraint. You can see that regularization forced the model to have a smaller slope, which fits a bit less the training data that the model was trained on, but actually allows it to generalize better to new examples.

![](images/d9b7274c8282c1e9bf2e049eafe5c45d53bb56706a46476d4ae9c834e7a2d885.jpg)  
Figure 1-23. Regularization reduces the risk of overtting

The amount of regularization to apply during learning can be controlled by a hyper‐ parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper‐ parameter to a very large value, you will get an almost flat model (a slope close to zero); the learning algorithm will almost certainly not overfit the training data, but it will be less likely to find a good solution. Tuning hyperparameters is an important part of building a Machine Learning system (you will see a detailed example in the next chapter).

# Undertting the Training Data

As you might guess, undertting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a lin‐ ear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training exam‐ ples.

The main options to fix this problem are:

• Selecting a more powerful model, with more parameters   
• Feeding better features to the learning algorithm (feature engineering)   
• Reducing the constraints on the model (e.g., reducing the regularization hyper‐ parameter)

# Stepping Back

By now you already know a lot about Machine Learning. However, we went through so many concepts that you may be feeling a little lost, so let’s step back and look at the big picture:

• Machine Learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules.   
• There are many different types of ML systems: supervised or not, batch or online, instance-based or model-based, and so on.   
• In a ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based it tunes some parameters to fit the model to the training set (i.e., to make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and generalizes to new instances by comparing them to the learned instances using a similarity measure.   
• The system will not perform well if your training set is too small, or if the data is not representative, noisy, or polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit).

There’s just one last important topic to cover: once you have trained a model, you don’t want to just “hope” it generalizes to new cases. You want to evaluate it, and finetune it if necessary. Let’s see how.

# Testing and Validating

The only way to know how well a model will generalize to new cases is to actually try it out on new cases. One way to do that is to put your model in production and moni‐ tor how well it performs. This works well, but if your model is horribly bad, your users will complain—not the best idea.

A better option is to split your data into two sets: the training set and the test set. As these names imply, you train your model using the training set, and you test it using the test set. The error rate on new cases is called the generalization error (or out-ofsample error), and by evaluating your model on the test set, you get an estimate of this error. This value tells you how well your model will perform on instances it has never seen before.

If the training error is low (i.e., your model makes few mistakes on the training set) but the generalization error is high, it means that your model is overfitting the train‐ ing data.

![](images/0f1c60a25fd9b81846d6892c3eca85a8858cca4d581a9bc5692aaa7c6d333079.jpg)

It is common to use $8 0 \%$ of the data for training and hold out $2 0 \%$ for testing. However, this depends on the size of the dataset: if it contains 10 million instances, then holding out $1 \%$ means your test set will contain 100,000 instances: that’s probably more than enough to get a good estimate of the generalization error.

# Hyperparameter Tuning and Model Selection

So evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐ tating between two models (say a linear model and a polynomial model): how can you decide? One option is to train both and compare how well they generalize using the test set.

Now suppose that the linear model generalizes better, but you want to apply some regularization to avoid overfitting. The question is: how do you choose the value of the regularization hyperparameter? One option is to train 100 different models using 100 different values for this hyperparameter. Suppose you find the best hyperparame‐ ter value that produces a model with the lowest generalization error, say just $5 \%$ error.

So you launch this model into production, but unfortunately it does not perform as well as expected and produces $1 5 \%$ errors. What just happened?

The problem is that you measured the generalization error multiple times on the test set, and you adapted the model and hyperparameters to produce the best model for that particular set. This means that the model is unlikely to perform as well on new data.

A common solution to this problem is called holdout validation: you simply hold out part of the training set to evaluate several candidate models and select the best one. The new heldout set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout vali‐ dation process, you train the best model on the full training set (including the valida‐ tion set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error.

This solution usually works quite well. However, if the validation set is too small, then model evaluations will be imprecise: you may end up selecting a suboptimal model by mistake. Conversely, if the validation set is too large, then the remaining training set will be much smaller than the full training set. Why is this bad? Well, since the final model will be trained on the full training set, it is not ideal to compare candidate models trained on a much smaller training set. It would be like selecting the fastest sprinter to participate in a marathon. One way to solve this problem is to perform repeated cross-validation, using many small validation sets. Each model is evaluated once per validation set, after it is trained on the rest of the data. By averaging out all the evaluations of a model, we get a much more accurate measure of its performance. However, there is a drawback: the training time is multiplied by the number of valida‐ tion sets.

# Data Mismatch

In some cases, it is easy to get a large amount of data for training, but it is not per‐ fectly representative of the data that will be used in production. For example, suppose you want to create a mobile app to take pictures of flowers and automatically deter‐ mine their species. You can easily download millions of pictures of flowers on the web, but they won’t be perfectly representative of the pictures that will actually be taken using the app on a mobile device. Perhaps you only have 10,000 representative pictures (i.e., actually taken with the app). In this case, the most important rule to remember is that the validation set and the test must be as representative as possible of the data you expect to use in production, so they should be composed exclusively of representative pictures: you can shuffle them and put half in the validation set, and half in the test set (making sure that no duplicates or near-duplicates end up in both sets). After training your model on the web pictures, if you observe that the perfor‐ mance of your model on the validation set is disappointing, you will not know whether this is because your model has overfit the training set, or whether this is just due to the mismatch between the web pictures and the mobile app pictures. One sol‐ ution is to hold out part of the training pictures (from the web) in yet another set that Andrew Ng calls the train-dev set. After the model is trained (on the training set, not on the train-dev set), you can evaluate it on the train-dev set: if it performs well, then the model is not overfitting the training set, so if performs poorly on the validation set, the problem must come from the data mismatch. You can try to tackle this prob‐ lem by preprocessing the web images to make them look more like the pictures that will be taken by the mobile app, and then retraining the model. Conversely, if the model performs poorly on the train-dev set, then the model must have overfit the training set, so you should try to simplify or regularize the model, get more training data and clean up the training data, as discussed earlier.

# No Free Lunch Theorem

A model is a simplified version of the observations. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances. How‐ ever, to decide what data to discard and what data to keep, you must make assump‐ tions. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored.

In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best

model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and you evaluate only a few reasonable models. For example, for simple tasks you may evalu‐ ate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks.

# Exercises

In this chapter we have covered some of the most important concepts in Machine Learning. In the next chapters we will dive deeper and write more code, but before we do, make sure you know how to answer the following questions:

1. How would you define Machine Learning?   
2. Can you name four types of problems where it shines?   
3. What is a labeled training set?   
4. What are the two most common supervised tasks?   
5. Can you name four common unsupervised tasks?   
6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains?   
7. What type of algorithm would you use to segment your customers into multiple groups?   
8. Would you frame the problem of spam detection as a supervised learning prob‐ lem or an unsupervised learning problem?   
9. What is an online learning system?   
10. What is out-of-core learning?   
11. What type of learning algorithm relies on a similarity measure to make predic‐ tions?   
12. What is the difference between a model parameter and a learning algorithm’s hyperparameter?   
13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?   
14. Can you name four of the main challenges in Machine Learning?   
15. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?   
16. What is a test set and why would you want to use it?

17. What is the purpose of a validation set?   
18. What can go wrong if you tune hyperparameters using the test set?   
19. What is repeated cross-validation and why would you prefer it to using a single validation set?

Solutions to these exercises are available in ???.

# End-to-End Machine Learning Project

![](images/c6625588cf620d7ab8cc14a6784ae43ee87f2d55419f0ad4cee1b5b20a42bee7.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 2 in the final release of the book.

In this chapter, you will go through an example project end to end, pretending to be a recently hired data scientist in a real estate company.1 Here are the main steps you will go through:

1. Look at the big picture.   
2. Get the data.   
3. Discover and visualize the data to gain insights.   
4. Prepare the data for Machine Learning algorithms.   
5. Select a model and train it.   
6. Fine-tune your model.   
7. Present your solution.   
8. Launch, monitor, and maintain your system.

# Working with Real Data

When you are learning about Machine Learning it is best to actually experiment with real-world data, not just artificial datasets. Fortunately, there are thousands of open datasets to choose from, ranging across all sorts of domains. Here are a few places you can look to get data:

• Popular open data repositories:

— UC Irvine Machine Learning Repository   
— Kaggle datasets   
— Amazon’s AWS datasets

• Meta portals (they list open data repositories):

— http://dataportals.org/   
— http://opendatamonitor.eu/   
— http://quandl.com/

• Other pages listing many popular open data repositories:

— Wikipedia’s list of Machine Learning datasets   
Quora.com question   
— Datasets subreddit

In this chapter we chose the California Housing Prices dataset from the StatLib repos‐ itory2 (see Figure 2-1). This dataset was based on data from the 1990 California cen‐ sus. It is not exactly recent (you could still afford a nice house in the Bay Area at the time), but it has many qualities for learning, so we will pretend it is recent data. We also added a categorical attribute and removed a few features for teaching purposes.

![](images/1711c09ac33fcf30dc9e90d45715743a9446a5de76bc5d6a46bf76fa9217c651.jpg)  
Figure 2-1. California housing prices

# Look at the Big Picture

Welcome to Machine Learning Housing Corporation! The first task you are asked to perform is to build a model of housing prices in California using the California cen‐ sus data. This data has metrics such as the population, median income, median hous‐ ing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “dis‐ tricts” for short.

Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.

![](images/f03be31d29148e7c44275eec68f1468e05d994072d6068eceddd57c8623d68e0.jpg)

Since you are a well-organized data scientist, the first thing you do is to pull out your Machine Learning project checklist. You can start with the one in ???; it should work reasonably well for most Machine Learning projects but make sure to adapt it to your needs. In this chapter we will go through many checklist items, but we will also skip a few, either because they are self-explanatory or because they will be discussed in later chapters.

# Frame the Problem

The first question to ask your boss is what exactly is the business objective; building a model is probably not the end goal. How does the company expect to use and benefit

from this model? This is important because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.

Your boss answers that your model’s output (a prediction of a district’s median hous‐ ing price) will be fed to another Machine Learning system (see Figure 2-2), along with many other signals.3 This downstream system will determine whether it is worth investing in a given area or not. Getting this right is critical, as it directly affects reve‐ nue.

![](images/c56804c940064bbe5c5bce38fc8a87ca07ac5800db142ad0e5b76dd441457b02.jpg)  
Figure 2-2. A Machine Learning pipeline for real estate investments

# Pipelines

A sequence of data processing components is called a data pipeline. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply.

Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store, and then some time later the next component in the pipeline pulls this data and spits out its own output, and so on. Each component is fairly self-contained: the interface between components is simply the data store. This makes the system quite simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken compo‐ nent. This makes the architecture quite robust.

On the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system’s perfor‐ mance drops.

The next question to ask is what the current solution looks like (if any). It will often give you a reference performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually by experts: a team gathers up-to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.

This is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than $2 0 \%$ . This is why the company thinks that it would be useful to train a model to predict a district’s median housing price given other data about that district. The census data looks like a great dataset to exploit for this pur‐ pose, since it includes the median housing prices of thousands of districts, as well as other data.

Okay, with all this information you are now ready to start designing your system. First, you need to frame the problem: is it supervised, unsupervised, or Reinforce‐ ment Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.

Have you found the answers? Let’s see: it is clearly a typical supervised learning task since you are given labeled training examples (each instance comes with the expected output, i.e., the district’s median housing price). Moreover, it is also a typical regres‐ sion task, since you are asked to predict a value. More specifically, this is a multiple regression problem since the system will use multiple features to make a prediction (it will use the district’s population, the median income, etc.). It is also a univariate regression problem since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. Finally, there is no continuous flow of data coming in the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine.

![](images/8b12b23e014187395e90d2e70ef7460cf7206dc026f355db35936809d415e328.jpg)

If the data was huge, you could either split your batch learning work across multiple servers (using the MapReduce technique), or you could use an online learning technique instead.

# Select a Performance Measure

Your next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. Equation 2-1 shows the mathematical formula to compute the RMSE.

Equation 2-1. Root Mean Square Error (RMSE)

$$
\operatorname {R M S E} (\mathbf {X}, h) = \sqrt {\frac {1}{m} \sum_ {i = 1} ^ {m} \left(h \left(\mathbf {x} ^ {(i)}\right) - y ^ {(i)}\right) ^ {2}}
$$

# Notations

This equation introduces several very common Machine Learning notations that we will use throughout this book:

• m is the number of instances in the dataset you are measuring the RMSE on.

— For example, if you are evaluating the RMSE on a validation set of 2,000 dis‐ tricts, then $m = 2 , 0 0 0$ .

• $\mathbf { x } ^ { ( i ) }$ is a vector of all the feature values (excluding the label) of the $i ^ { t h }$ instance in the dataset, and $y ^ { ( i ) }$ is its label (the desired output value for that instance).

— For example, if the first district in the dataset is located at longitude $- 1 1 8 . 2 9 ^ { \circ }$ , latitude $3 3 . 9 1 ^ { \circ } \mathrm { . }$ , and it has 1,416 inhabitants with a median income of $\$ 38,372$ , and the median house value is $\$ 156,400$ (ignoring the other features for now), then:

$$
\mathbf {x} ^ {(1)} = \left( \begin{array}{c} - 1 1 8. 2 9 \\ 3 3. 9 1 \\ 1, 4 1 6 \\ 3 8, 3 7 2 \end{array} \right)
$$

and:

$$
y ^ {(1)} = 1 5 6, 4 0 0
$$

• X is a matrix containing all the feature values (excluding labels) of all instances in the dataset. There is one row per instance and the $i ^ { t h }$ row is equal to the transpose of $\mathbf { x } ^ { ( i ) }$ , noted $( \mathbf { x } ^ { ( i ) } ) ^ { T }$ .4

— For example, if the first district is as just described, then the matrix X looks like this:

$$
\mathbf {X} = \left( \begin{array}{c} \left(\mathbf {x} ^ {(1)}\right) ^ {T} \\ \left(\mathbf {x} ^ {(2)}\right) ^ {T} \\ \vdots \\ \left(\mathbf {x} ^ {(1 9 9 9)}\right) ^ {T} \\ \left(\mathbf {x} ^ {(2 0 0 0)}\right) ^ {T} \end{array} \right) = \left( \begin{array}{c c c c c} - 1 1 8. 2 9 & 3 3. 9 1 & 1, 4 1 6 & 3 8, 3 7 2 \\ \vdots & \vdots & \vdots & \vdots \end{array} \right)
$$

• $h$ is your system’s prediction function, also called a hypothesis. When your system is given an instance’s feature vector $\mathbf { x } ^ { ( i ) }$ , it outputs a predicted value $\hat { y } ^ { ( i ) } = h ( \mathbf { x } ^ { ( i ) } )$ for that instance $\hat { y }$ is pronounced “y-hat”).

— For example, if your system predicts that the median housing price in the first district is $\$ 158,400$ , then $\hat { y } ^ { ( 1 ) } = h ( \mathbf { x } ^ { ( 1 ) } ) = 1 5 8 , 4 0 0$ . The prediction error for this district is $\hat { y } ^ { ( 1 ) } - y ^ { ( 1 ) } = 2 , 0 0 0$ .

• RMSE(X,h) is the cost function measured on the set of examples using your hypothesis $h$ .

We use lowercase italic font for scalar values (such as m or $\gamma ^ { ( i ) } .$ ) and function names (such as $h$ ), lowercase bold font for vectors (such as $\mathbf { x } ^ { ( i ) }$ ), and uppercase bold font for matrices (such as X).

Even though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, suppose that there are many outlier districts. In that case, you may consider using the Mean Absolute Error (also called the Average Absolute Deviation; see Equation 2-2):

Equation 2-2. Mean Absolute Error

$$
\operatorname {M A E} (\mathbf {X}, h) = \frac {1}{m} \sum_ {i = 1} ^ {m} \left| h \left(\mathbf {x} ^ {(i)}\right) - y ^ {(i)} \right|
$$

Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:

• Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: it is the notion of distance you are familiar with. It is also called the $\ell _ { 2 }$ norm, noted $\Vert \cdot \Vert _ { 2 }$ (or just $\| \cdot \| ,$ ).   
• Computing the sum of absolutes (MAE) corresponds to the $\ell _ { 1 }$ norm, noted $\Vert \cdot \Vert _ { 1 }$ It is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.   
• More generally, the $\ell _ { k }$ norm of a vector v containing $n$ elements is defined as 1 $\left\| \textbf { v } \right\| _ { k } = \left( \left| \nu _ { 0 } \right| ^ { k } + \left| \nu _ { 1 } \right| ^ { k } + \cdots + \left| \nu _ { n } \right| ^ { k } \right) ^ { \overline { { k } } }$ . $\ell _ { 0 }$ just gives the number of non-zero ele‐ ments in the vector, and $\ell _ { \infty }$ gives the maximum absolute value in the vector.   
• The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when

outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.

# Check the Assumptions

Lastly, it is good practice to list and verify the assumptions that were made so far (by you or others); this can catch serious issues early on. For example, the district prices that your system outputs are going to be fed into a downstream Machine Learning system, and we assume that these prices are going to be used as such. But what if the downstream system actually converts the prices into categories (e.g., “cheap,” “medium,” or “expensive”) and then uses those categories instead of the prices them‐ selves? In this case, getting the price perfectly right is not important at all; your sys‐ tem just needs to get the category right. If that’s so, then the problem should have been framed as a classification task, not a regression task. You don’t want to find this out after working on a regression system for months.

Fortunately, after talking with the team in charge of the downstream system, you are confident that they do indeed need the actual prices, not just categories. Great! You’re all set, the lights are green, and you can start coding now!

# Get the Data

It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk through the following code examples in a Jupyter notebook. The full Jupyter note‐ book is available at https://github.com/ageron/handson-ml2.

# Create the Workspace

First you will need to have Python installed. It is probably already installed on your system. If not, you can get it at https://www.python.org/.5

Next you need to create a workspace directory for your Machine Learning code and datasets. Open a terminal and type the following commands (after the $\$ 5$ prompts):

```powershell
$ export ML_PATH="$HOME/ml" # You can change the path if you prefer $ mkdir -p $ML_PATH 
```

You will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and Scikit-Learn. If you already have Jupyter running with all these modules installed, you can safely skip to “Download the Data” on page 49. If you don’t have them yet, there are many ways to install them (and their dependencies). You can use your sys‐

tem’s packaging system (e.g., apt-get on Ubuntu, or MacPorts or HomeBrew on MacOS), install a Scientific Python distribution such as Anaconda and use its packag‐ ing system, or just use Python’s own packaging system, pip, which is included by default with the Python binary installers (since Python 2.7.9).6 You can check to see if pip is installed by typing the following command:

```txt
$ python3 -m pip --version
pip 19.0.2 from [...]/lib/python3.6/site-packages (python 3.6) 
```

You should make sure you have a recent version of pip installed. To upgrade the pip module, type:7

```shell
$ python3 -m pip install --user -U pip
Collecting pip
[...] 
```

# Creating an Isolated Environment

If you would like to work in an isolated environment (which is strongly recom‐ mended so you can work on different projects without having conflicting library ver‐ sions), install virtualenv8 by running the following pip command (again, if you want virtualenv to be installed for all users on your machine, remove --user and run this command with administrator rights):

```txt
$ python3 -m pip install --user -U virtualenv
Collecting virtualenv
[...] 
```

Now you can create an isolated Python environment by typing:

```txt
$ cd $ML_PATH
$ virtualenv env
Using base prefix '[...]'
New python executable in [...]/ml/env/bin/python3.6
Also creating executable in [...]/ml/env/bin/python
Installing setuptools, pip, wheel...done. 
```

Now every time you want to activate this environment, just open a terminal and type:

```powershell
$ cd $ML_PATH
$ source env/bin/activate # on Linux or MacOSX
$ .env\Scripts\activate # on Windows 
```

To deactivate this environment, just type deactivate. While the environment is active, any package you install using pip will be installed in this isolated environment, and Python will only have access to these packages (if you also want access to the sys‐ tem’s packages, you should create the environment using virtualenv’s --system-sitepackages option). Check out virtualenv’s documentation for more information.

Now you can install all the required modules and their dependencies using this sim‐ ple pip command (if you are not using a virtualenv, you will need the --user option or administrator rights):

```powershell
$ python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn
Collecting jupyter
Downloading jupyter-1.0.0-py2.py3-none-any.whl
Collecting matplotlib
[...] 
```

To check your installation, try to import every module like this:

```txt
$ python3 -c "import jupyter, matplotlib, numpy, pandas, scipy, sklearn" 
```

There should be no output and no error. Now you can fire up Jupyter by typing:

```powershell
$ jupyter notebook
[I 15:24 NotebookApp] Serving notebooks from local directory: [...]/ml
[I 15:24 NotebookApp] θ active kernels
[I 15:24 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/
[I 15:24 NotebookApp] Use Control-C to stop this server and shut down all
kernels (twice to skip confirmation). 
```

A Jupyter server is now running in your terminal, listening to port 8888. You can visit this server by opening your web browser to http://localhost:8888/ (this usually hap‐ pens automatically when the server starts). You should see your empty workspace directory (containing only the env directory if you followed the preceding virtualenv instructions).

Now create a new Python notebook by clicking on the New button and selecting the appropriate Python version9 (see Figure 2-3).

This does three things: first, it creates a new notebook file called Untitled.ipynb in your workspace; second, it starts a Jupyter Python kernel to run this notebook; and

third, it opens this notebook in a new tab. You should start by renaming this note‐ book to “Housing” (this will automatically rename the file to Housing.ipynb) by click‐ ing Untitled and typing the new name.

![](images/cb4fb7422b63a99cb45c2c6a8705db357ab4a64705248c6e7f54e53fa5c18c61.jpg)  
Figure 2-3. Your workspace in Jupyter

A notebook contains a list of cells. Each cell can contain executable code or formatted text. Right now the notebook contains only one empty code cell, labeled “In [1]:”. Try typing print("Hello world!") in the cell, and click on the play button (see Figure 2-4) or press Shift-Enter. This sends the current cell to this notebook’s Python kernel, which runs it and returns the output. The result is displayed below the cell, and since we reached the end of the notebook, a new cell is automatically created. Go through the User Interface Tour from Jupyter’s Help menu to learn the basics.

![](images/75d5a9fe0fb9725cf74bd8ea60133bf665b0e90faa370f54037add61eca29820.jpg)  
Figure 2-4. Hello world Python notebook

# Download the Data

In typical environments your data would be available in a relational database (or some other common datastore) and spread across multiple tables/documents/files. To access it, you would first need to get your credentials and access authorizations,10 and familiarize yourself with the data schema. In this project, however, things are much simpler: you will just download a single compressed file, housing.tgz, which contains a comma-separated value (CSV) file called housing.csv with all the data.

You could use your web browser to download it, and run tar xzf housing.tgz to decompress the file and extract the CSV file, but it is preferable to create a small func‐ tion to do that. It is useful in particular if data changes regularly, as it allows you to write a small script that you can run whenever you need to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals). Auto‐ mating the process of fetching the data is also useful if you need to install the dataset on multiple machines.

Here is the function to fetch the data:11

```python
import os   
import tarfile   
from six.moves import urllib   
DAILYLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"   
HOUSING_PATH = os.path.join("datasets","housing")   
HOUSING_URL = DAILYLOAD_ROOT + "datasets/housing/housing.tgz"   
def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tzg_path = os.path.join(housing_path, "housing.tgz") urllib.request.urlretrieve(housing_url, tzg_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close() 
```

Now when you call fetch_housing_data(), it creates a datasets/housing directory in your workspace, downloads the housing.tgz file, and extracts the housing.csv from it in this directory.

Now let’s load the data using Pandas. Once again you should write a small function to load the data:

import pandas as pd   
def load_housing_data(housing_path $\equiv$ HOUSING_PATH): csv_path $=$ os.path.join(housing_path, "housing.csv") return pd.read_csv(csv_path)

This function returns a Pandas DataFrame object containing all the data.

# Take a Quick Look at the Data Structure

Let’s take a look at the top five rows using the DataFrame’s head() method (see Figure 2-5).

![](images/5f8b0d934da138365eb145ba69a555aaebc4a6d039c2ee4efa405bdde34a90a7.jpg)  
Figure 2-5. Top ve rows in the dataset

Each row represents one district. There are 10 attributes (you can see the first 6 in the screenshot): longitude, latitude, housing_median_age, total_rooms, total_bed rooms, population, households, median_income, median_house_value, and ocean_proximity.

The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values (see Figure 2-6).

In [6]: housing.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total Rooms 20640 non-null float64 total Bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: $1.6+$ MB

Figure 2-6. Housing info

There are 20,640 instances in the dataset, which means that it is fairly small by Machine Learning standards, but it’s perfect to get started. Notice that the total_bed rooms attribute has only 20,433 non-null values, meaning that 207 districts are miss‐ ing this feature. We will need to take care of this later.

All attributes are numerical, except the ocean_proximity field. Its type is object, so it could hold any kind of Python object, but since you loaded this data from a CSV file you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute. You can find out what cate‐ gories exist and how many districts belong to each category by using the value_counts() method:

$>>$ housing["ocean_proximity"].value_counts() $<1H$ OCEAN 9136  
INLAND 6551  
NEAR OCEAN 2658  
NEAR BAY 2290  
ISLAND 5  
Name: ocean_proximity, dtype: int64

Let’s look at the other fields. The describe() method shows a summary of the numerical attributes (Figure 2-7).

![](images/ef63dd10ca60288c1140f98e1bc7d76d5575fab40d5882cdc53f390b85178624.jpg)  
Figure 2-7. Summary of each numerical attribute

The count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (so, for example, count of total_bedrooms is 20,433, not 20,640). The std row shows the standard deviation, which measures how dispersed the values are.12 The $2 5 \%$ , $5 0 \%$ , and $7 5 \%$ rows show the corresponding percentiles: a percentile indi‐ cates the value below which a given percentage of observations in a group of observa‐ tions falls. For example, $2 5 \%$ of the districts have a housing_median_age lower than 18, while $5 0 \%$ are lower than 29 and $7 5 \%$ are lower than 37. These are often called the $2 5 ^ { \mathrm { t h } }$ percentile (or $1 ^ { \mathrm { s t } }$ quartile), the median, and the $7 5 ^ { \mathrm { t h } }$ percentile (or $3 ^ { \mathrm { r d } }$ quartile).

Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the hist() method on the whole dataset, and it will plot a histogram for each numerical attribute (see Figure 2-8). For example, you can see that slightly over 800 districts have a median_house_value equal to about $\$ 100,000$ .

```txt
%matplotlib inline # only in a Jupyter notebook  
import matplotlib.pyplot as plt  
housing.hist(bins=50, figsize=(20,15))  
plt.show() 
```

![](images/ca07f20073af06707793043c6e22fc5d061336a1355d14c5924676576b73f8a6.jpg)

The hist() method relies on Matplotlib, which in turn relies on a user-specified graphical backend to draw on your screen. So before you can plot anything, you need to specify which backend Matplot‐ lib should use. The simplest option is to use Jupyter’s magic com‐ mand %matplotlib inline. This tells Jupyter to set up Matplotlib so it uses Jupyter’s own backend. Plots are then rendered within the notebook itself. Note that calling show() is optional in a Jupyter notebook, as Jupyter will automatically display plots when a cell is executed.

![](images/683fa75d0164f2ff99bfa982fd501fc48d916b98a7bbde18396989642ae60444.jpg)  
Figure 2-8. A histogram for each numerical attribute

Notice a few things in these histograms:

1. First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $\$ 30,000)$ . Working with preprocessed attributes is common in Machine Learning,

and it is not necessarily a problem, but you should try to understand how the data was computed.

2. The housing median age and the median house value were also capped. The lat‐ ter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s out‐ put) to see if this is a problem or not. If they tell you that they need precise pre‐ dictions even beyond $\$ 500,000$ , then you have mainly two options:

a. Collect proper labels for the districts whose labels were capped.   
b. Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $\$ 500,000$ .   
3. These attributes have very different scales. We will discuss this later in this chap‐ ter when we explore feature scaling.   
4. Finally, many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.

Hopefully you now have a better understanding of the kind of data you are dealing with.

![](images/fcb7cd56fb4f9f64d41c799a421f8e13f5b14cf42d21e9b4ba4b413b544f4c1b.jpg)

Wait! Before you look at the data any further, you need to create a test set, put it aside, and never look at it.

# Create a Test Set

It may sound strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic and you will launch a system that will not perform as well as expected. This is called data snooping bias.

Creating a test set is theoretically quite simple: just pick some instances randomly, typically $2 0 \%$ of the dataset (or less if your dataset is very large), and set them aside:

import numpy as np   
def split_train_test(data, test_ratio): shuffled Indices $=$ np.random.permutation(len(data)) test_set_size $=$ int(len(data)\*test_ratio) testIndices $=$ shuffledIndices[:test_set_size] train Indices $=$ shuffledIndices[test_set_size:] return data.iloc[train Indices],data.iloc[test Indices]

You can then use this function like this:13

```erlang
>>> train_set, test_set = split_train_test(housing, 0.2)
>>> len(train_set)
16512
>>> len(test_set)
4128 
```

Well, this works, but it is not perfect: if you run the program again, it will generate a different test set! Over time, you (or your Machine Learning algorithms) will get to see the whole dataset, which is what you want to avoid.

One solution is to save the test set on the first run and then load it in subsequent runs. Another option is to set the random number generator’s seed (e.g., np.ran dom.seed(42))14 before calling np.random.permutation(), so that it always generates the same shuffled indices.

But both these solutions will break next time you fetch an updated dataset. A com‐ mon solution is to use each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, you could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower or equal to $2 0 \%$ of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain $2 0 \%$ of the new instances, but it will not contain any instance that was previously in the training set. Here is a possible implementation:

from zlib import crc32   
def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) & 0xFFFFFF $<$ test_ratio * 2**32   
def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column]

```python
in_test_set = ids.apply(lambda id: test_set_check(id, test_ratio))  
return data.loc[~in_test_set], data.loc[in_test_set] 
```

Unfortunately, the housing dataset does not have an identifier column. The simplest solution is to use the row index as the ID:

```txt
housing_with_id = housing.reset_index() # adds an `index` column  
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index") 
```

If you use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset, and no row ever gets deleted. If this is not possible, then you can try to use the most stable features to build a unique identifier. For example, a district’s latitude and longitude are guaranteed to be stable for a few million years, so you could combine them into an ID like so:15

```python
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]  
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id") 
```

Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split, which does pretty much the same thing as the function split_train_test defined earlier, with a couple of additional features. First there is a random_state parameter that allows you to set the random generator seed as explained previously, and second you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels):

from sklearn.model_selection import train_test_split

```txt
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) 
```

So far we have considered purely random sampling methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population. For example, the US population is com‐ posed of $5 1 . 3 \%$ female and $4 8 . 7 \%$ male, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called strati‐ ed sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population. If they used purely random sam‐ pling, there would be about $1 2 \%$ chance of sampling a skewed test set with either less than $4 9 \%$ female or more than $5 4 \%$ female. Either way, the survey results would be significantly biased.

Suppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely (back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e., $\$ 15,000-960,000$ , but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the pd.cut() function to create an income category attribute with 5 categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $\$ 15,000$ ), category 2 from 1.5 to 3, and so on:

```python
housing["income_cat"] = pd.cut(housing["median_income"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) 
```

These income categories are represented in Figure 2-9:

```javascript
housing["income_cat"].hist() 
```

![](images/8983989a2f5d83d08f0be70261045ad5c8bb220bec7e4ccdc94f272d1d983f65.jpg)  
Figure 2-9. Histogram of income categories

Now you are ready to do stratified sampling based on the income category. For this you can use Scikit-Learn’s StratifiedShuffleSplit class:

```python
from sklearn.model_selection import StratifiedShuffleSplit 
```

```python
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)  
for train_index, test_index in split.split(housing, housing["income_cat']):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index] 
```

Let’s see if this worked as expected. You can start by looking at the income category proportions in the test set:

```txt
>>> strat_test_set["income_cat"].value_counts() / len(strat_test_set)
3 0.350533
2 0.318798
4 0.176357
5 0.114583
1 0.039729
Name: income_cat, dtype: float64 
```

With similar code you can measure the income category proportions in the full data‐ set. Figure 2-10 compares the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is quite skewed.

![](images/32c73777080e7c4da5a04449734942d4a9abe47df39b30b65f89a12e5ae86ab2.jpg)  
Figure 2-10. Sampling bias comparison of stratied versus purely random sampling

Now you should remove the income_cat attribute so the data is back to its original state:

```python
for set_in(strat_train_set, strat_test_set): set_.drop("income_cat", axis=1, inplace=True) 
```

We spent quite a bit of time on test set generation for a good reason: this is an often neglected but critical part of a Machine Learning project. Moreover, many of these ideas will be useful later when we discuss cross-validation. Now it’s time to move on to the next stage: exploring the data.

# Discover and Visualize the Data to Gain Insights

So far you have only taken a quick glance at the data to get a general understanding of the kind of data you are manipulating. Now the goal is to go a little bit more in depth.

First, make sure you have put the test set aside and you are only exploring the train‐ ing set. Also, if the training set is very large, you may want to sample an exploration

set, to make manipulations easy and fast. In our case, the set is quite small so you can just work directly on the full set. Let’s create a copy so you can play with it without harming the training set:

housing $=$ strat_train_set.copy()

# Visualizing Geographical Data

Since there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data (Figure 2-11):

housing.plot(kind="scatter", $\mathbf { x } \mathbf { = } ^ { \mathrm { { \scriptscriptstyle ~ \mathrm { ~  ~ } } } }$ longitude", y="latitude")

![](images/6c08a3a230b525739958af0fc6640656846c84774192f1e6582449adac5bcdc6.jpg)  
Figure 2-11. A geographical scatterplot of the data

This looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points (Figure 2-12):

housing.plot(kind="scatter", $\mathbf { \boldsymbol { x } } \mathbf { = } ^ { \mathsf { \Pi } }$ longitude", y="latitude", alpha=0.1)

![](images/f33298d92904b08e4294f6821b8022f3e54a0c0358a74d76bba86c44673e83f8.jpg)  
Figure 2-12. A better visualization highlighting high-density areas

Now that’s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high density in the Central Valley, in particular around Sacramento and Fresno.

More generally, our brains are very good at spotting patterns on pictures, but you may need to play around with visualization parameters to make the patterns stand out.

Now let’s look at the housing prices (Figure 2-13). The radius of each circle represents the district’s population (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices):16

```txt
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population", figsize=(10,7), c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,) plt.legend() 
```

![](images/b7ceb38ebbaae411363c2cf7ec1d28825cf60210ae63d1442b3d66475ecdf9c6.jpg)  
Figure 2-13. California housing prices

This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already. It will probably be useful to use a clustering algorithm to detect the main clusters, and add new features that measure the proximity to the cluster centers. The ocean prox‐ imity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.

# Looking for Correlations

Since the dataset is not too large, you can easily compute the standard correlation coecient (also called Pearson’s r) between every pair of attributes using the corr() method:

```python
corr_matrix = housing.corr() 
```

Now let’s look at how much each attribute correlates with the median house value:

```txt
>>> corr_matrix["median_house_value"].sort_values(ascending=False)  
median_house_value 1.000000  
median_income 0.687170  
total Rooms 0.135231  
housing_median_age 0.114220  
households 0.064702  
total_bedrooms 0.047865  
population -0.026699  
longitude -0.047279  
latitude -0.142826  
Name: median_house_value, dtype: float64 
```

The correlation coefficient ranges from $^ { - 1 }$ to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up. When the coefficient is close to $^ { - 1 }$ , it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). Finally, coefficients close to zero mean that there is no linear correlation. Figure 2-14 shows various plots along with the correlation coeffi‐ cient between their horizontal and vertical axes.

![](images/46164549872e383347f80e8b98e02df4951f14b627bd41088b7caa573940a215.jpg)  
Figure 2-14. Standard correlation coecient of various datasets (source: Wikipedia; public domain image)

![](images/e15fbd607bd70035cbd058a6a3538d714aaf464c5c6b0ae87462c9dabfb8ce9e.jpg)

The correlation coefficient only measures linear correlations (“if $x$ goes up, then y generally goes up/down”). It may completely miss out on nonlinear relationships (e.g., “if $x$ is close to zero then y gen‐ erally goes up”). Note how all the plots of the bottom row have a correlation coefficient equal to zero despite the fact that their axes are clearly not independent: these are examples of nonlinear rela‐ tionships. Also, the second row shows examples where the correla‐ tion coefficient is equal to 1 or –1; notice that this has nothing to do with the slope. For example, your height in inches has a correla‐ tion coefficient of 1 with your height in feet or in nanometers.

Another way to check for correlation between attributes is to use Pandas’ scatter_matrix function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get $1 1 ^ { 2 } =$ 121 plots, which would not fit on a page, so let’s just focus on a few promising attributes that seem most correlated with the median housing value (Figure 2-15):

from pandas.plotting import scatter_matrix   
```txt
attributes = ["median_house_value", "median_income", "total Rooms", "housing_median_age"]  
scatter_matrix(housing[attributes], figsize=(12, 8)) 
```

![](images/2bbb4b4e1912ddf7e9200a18e5a4aa6ea65db8020c385c2a70997dbfdb5e9183.jpg)  
Figure 2-15. Scatter matrix

The main diagonal (top left to bottom right) would be full of straight lines if Pandas plotted each variable against itself, which would not be very useful. So instead Pandas displays a histogram of each attribute (other options are available; see Pandas’ docu‐ mentation for more details).

The most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot (Figure 2-16):

```javascript
housing.plot(kind="scatter", x="median_income", y="median_house_value", alpha=0.1) 
```

This plot reveals a few things. First, the correlation is indeed very strong; you can clearly see the upward trend and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at $\$ 500,000$ . But this plot reveals other less obvious straight lines: a horizontal line around $\$ 450,000$ , another around $\$ 350,000$ , perhaps one around $\$ 280,000$ , and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.

![](images/42141cf8046bf5f5b3ef68aaaab1018ec3cc3342cfeadbf6a01749cb277fc789.jpg)  
Figure 2-16. Median income versus median house value

# Experimenting with Attribute Combinations

Hopefully the previous sections gave you an idea of a few ways you can explore the data and gain insights. You identified a few data quirks that you may want to clean up before feeding the data to a Machine Learning algorithm, and you found interesting correlations between attributes, in particular with the target attribute. You also noticed that some attributes have a tail-heavy distribution, so you may want to trans‐ form them (e.g., by computing their logarithm). Of course, your mileage will vary considerably with each project, but the general ideas are similar.

One last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes:

```toml
housing["rooms_per_household"] = housing["total Rooms"]/housing["households"]
housing["bedrooms_per_room"] = housing["total.bedrooms"]/housing["totalRooms"]
housing["population_per_household"]=housing["population"]/housing["households"] 
```

And now let’s look at the correlation matrix again:

```txt
>>> corr_matrix = housing.corr()
>>> corr_matrix["median_house_value"].sort_values(ascending=False)
median_house_value 1.000000 
```

median_income 0.687160  
rooms_per_household 0.146285  
total Rooms 0.135097  
housing_median_age 0.114110  
households 0.064506  
total_bedrooms 0.047689  
population_per_household -0.021985  
population -0.026920  
longitude -0.047432  
latitude -0.142724  
bedrooms_per_room -0.259984  
Name: median_house_value, dtype: float64

Hey, not bad! The new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.

This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first rea‐ sonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.

# Prepare the Data for Machine Learning Algorithms

It’s time to prepare the data for your Machine Learning algorithms. Instead of just doing this manually, you should write functions to do that, for several good reasons:

• This will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).   
• You will gradually build a library of transformation functions that you can reuse in future projects.   
• You can use these functions in your live system to transform the new data before feeding it to your algorithms.   
• This will make it possible for you to easily try various transformations and see which combination of transformations works best.

But first let’s revert to a clean training set (by copying strat_train_set once again), and let’s separate the predictors and the labels since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that drop() creates a copy of the data and does not affect strat_train_set):

housing $=$ strat_train_set.drop("median_house_value", axis=1) housing_labels $=$ strat_train_set["median_house_value"].copy()

# Data Cleaning

Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:

• Get rid of the corresponding districts.   
• Get rid of the whole attribute.   
• Set the values to some value (zero, the mean, the median, etc.).

You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna() methods:

```txt
housing.dropna(subset=['total Bedrooms']) # option 1  
housing.drop("total Bedrooms", axis=1) # option 2  
median = housing["total Bedrooms"].median() # option 3  
housing["total Bedrooms"].fillna(median, inplace=True) 
```

If you choose option 3, you should compute the median value on the training set, and use it to fill the missing values in the training set, but also don’t forget to save the median value that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.

Scikit-Learn provides a handy class to take care of missing values: SimpleImputer. Here is how to use it. First, you need to create a SimpleImputer instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:

from sklearn.impute import SimpleImputer

```python
imputer = SimpleImputer(strategy="median") 
```

Since the median can only be computed on numerical attributes, we need to create a copy of the data without the text attribute ocean_proximity:

```python
housing_num = housing.drop("ocean_proximity", axis=1) 
```

Now you can fit the imputer instance to the training data using the fit() method:

imputer.fit(housing_num)

The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:

```javascript
>>imputer.statistics_array([-118.51,34.26,29.,2119.5,433.,1164.,408.,3.5409]) 
```

```txt
>>> housing_num_median().values  
array([-118.51, 34.26, 29., 2119.5, 433., 1164., 408., 3.5409]) 
```

Now you can use this “trained” imputer to transform the training set by replacing missing values by the learned medians:

```txt
X = imputer.transform(housing_num) 
```

The result is a plain NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple:

```python
housing_tr = pd.DataFrame(X, columns=housing_num.columns) 
```

# Scikit-Learn Design

Scikit-Learn’s API is remarkably well designed. The main design principles are:17

• Consistency. All objects share a consistent and simple interface:

— Estimators. Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator). The estimation itself is performed by the fit() method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is con‐ sidered a hyperparameter (such as an imputer’s strategy), and it must be set as an instance variable (generally via a constructor parameter).   
— Transformers. Some estimators (such as an imputer) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the transform() method with the dataset to transform as a parameter. It returns the transformed dataset. This transforma‐ tion generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called fit_transform() that is equivalent to calling fit() and then transform() (but sometimes fit_transform() is optimized and runs much faster).   
— Predictors. Finally, some estimators are capable of making predictions given a dataset; they are called predictors. For example, the LinearRegression model in the previous chapter was a predictor: it predicted life satisfaction given a country’s GDP per capita. A predictor has a predict() method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a score() method that measures the quality of the predictions given

a test set (and the corresponding labels in the case of supervised learning algorithms).18

• Inspection. All the estimator’s hyperparameters are accessible directly via public instance variables (e.g., imputer.strategy), and all the estimator’s learned parameters are also accessible via public instance variables with an underscore suffix (e.g., imputer.statistics_).   
• Nonproliferation of classes. Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers.   
• Composition. Existing building blocks are reused as much as possible. For example, it is easy to create a Pipeline estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see.   
• Sensible defaults. Scikit-Learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly.

# Handling Text and Categorical Attributes

Earlier we left out the categorical attribute ocean_proximity because it is a text attribute so we cannot compute its median:

```txt
>>> housing_cat = housing [["ocean_proximity"]
>>> housing_cat.head(10)
ocean_proximity
17606 <1H OCEAN
18632 <1H OCEAN
14650 NEAR OCEAN
3230 INLAND
3555 <1H OCEAN
19480 INLAND
8879 <1H OCEAN
13685 INLAND
4937 <1H OCEAN
4861 <1H OCEAN 
```

Most Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐ vert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina lEncoder class19:

```python
>>> from sklearn preprocess import OrdinalEncoder
>>> ordinal Encoder = OrdinalEncoder() 
```

$\ggg$ housing_cat_encoding $\equiv$ ordinalEncoder.fit_transform(housing_cat) $\ggg$ housing_cat_encoding[:10]   
array([[0.], [0.], [4.], [1.], [0.], [1.], [0.], [1.], [0.], [1.], [0.], [0.]]

You can get the list of categories using the categories_ instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):

>>> ordinal Encodercategories_   
[array(['<1H OCEAN'，'INLAND'，'ISLAND'，'NEAR BAY'，'NEAR OCEAN']， dtype $\equiv$ object)]

One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is $^ { \infty } { < } 1 \mathrm { H }$ OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEn coder class to convert categorical values into one-hot vectors20:

```python
>>> from sklearn preprocess import OneHotEncoder
>>> cat Encoder = OneHotEncoder()
>>> housing_cat_1hot = cat Encoder.fit_transform(housing_cat)
>>> housing_cat_1hot
<16512x5 sparse matrix of type <class 'numpy.float64'>' with 16512 stored elements in Compressed Sparse Row format> 
```

Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After onehot encoding we get a matrix with thousands of columns, and the matrix is full of zeros except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the non‐

zero elements. You can use it mostly like a normal 2D array,21 but if you really want to convert it to a (dense) NumPy array, just call the toarray() method:

```txt
>>>housing_cat_1hot.toarray()   
array([[1.，0.，0.，0.，0.], [1.，0.，0.，0.，0.], [0.，0.，0.，0.，1.], ...， [0.，1.，0.，0.，0.], [1.，0.，0.，0.，0.], [0.，0.，0.，1.，0.]）
```

Once again, you can get the list of categories using the encoder’s categories_ instance variable:

>>>catEncoder categories_   
[array(['<1H OCEAN'，'INLAND'，'ISLAND'，'NEAR BAY'，'NEAR OCEAN']， dtype $\equiv$ object)]

![](images/e78d44c742e9d8b485b0f4b8b6893141728c729bf719719f13a37545ba5c3e14.jpg)

If a categorical attribute has a large number of possible categories (e.g., country code, profession, species, etc.), then one-hot encod‐ ing will result in a large number of input features. This may slow down training and degrade performance. If this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, you could replace each category with a learnable low dimensional vector called an embedding. Each category’s representation would be learned during training: this is an example of representation learning (see Chapter 13 and ??? for more details).

# Custom Transformers

Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn func‐ tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐ itance), all you need is to create a class and implement three methods: fit() (returning self), transform(), and fit_transform(). You can get the last one for free by simply adding TransformerMixin as a base class. Also, if you add BaseEstima tor as a base class (and avoid *args and **kargs in your constructor) you will get two extra methods (get_params() and set_params()) that will be useful for auto‐

matic hyperparameter tuning. For example, here is a small transformer class that adds the combined attributes we discussed earlier:

```python
from sklearn.base import BaseEstimator, TransformerMixin  
rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6  
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):  
    def __init__(self, add Bedrooms_per_room = True): # no *args or **kargs  
        self.add Bedrooms_per_room = add Bedrooms_per_room  
        def fit(self, X, y=None):  
            return self # nothing else to do  
    def transform(self, X, y=None):  
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]  
        population_per_household = X[:, population_ix] / X[:, households_ix]  
        if self.add Bedrooms_per_room:  
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]  
            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]  
        else:  
            return np.c_[X, rooms_per_household, population_per_household]  
    attr_adder = CombinedAttributesAdder(add Bedrooms_per_room=False)  
    housingextra_attributes = attr_adder.transform(housing.values) 
```

In this example the transformer has one hyperparameter, add_bedrooms_per_room, set to True by default (it is often helpful to provide sensible defaults). This hyperpara‐ meter will allow you to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, you can add a hyperparameter to gate any data preparation step that you are not $1 0 0 \%$ sure about. The more you automate these data preparation steps, the more combinations you can automatically try out, making it much more likely that you will find a great combination (and sav‐ ing you a lot of time).

# Feature Scaling

One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the hous‐ ing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required.

There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.

Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐ ing the min value and dividing by the max minus the min. Scikit-Learn provides a

transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.

Standardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algo‐ rithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐ ever, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called StandardScaler for stand‐ ardization.

![](images/88036a01f70bb372bc2fdf06309d9572ae002d6c359684345f74ed7ed2e896bd.jpg)

As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data).

# Transformation Pipelines

As you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for the numerical attributes:

from sklearnpipeline import Pipeline   
from sklearn.preprocessing import StandardScaler   
numpipeline $\equiv$ Pipeline([ ('imputer',SimpleImputer(strategy $=$ "median"）), ('attributes_adder',CombinedAttributesAdder()）， ('std_scheduler',StandardScaler()， 1)   
housing_num_tr $\equiv$ numpipeline.fit_transform(housing_num)

The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a fit_transform() method). The names can be anything you like (as long as they are unique and don’t contain double underscores “__”): they will come in handy later for hyperparameter tuning.

When you call the pipeline’s fit() method, it calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call, until it reaches the final estimator, for which it just calls the fit() method.

The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a StandardScaler, which is a transformer, so the pipeline has a trans form() method that applies all the transforms to the data in sequence (and of course also a fit_transform() method, which is the one we used).

So far, we have handled the categorical columns and the numerical columns sepa‐ rately. It would be more convenient to have a single transformer able to handle all col‐ umns, applying the appropriate transformations to each column. In version 0.20, Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with Pandas DataFrames. Let’s use it to apply all the transforma‐ tions to the housing data:

from sklearn.metrics import ColumnTransformer  
num_attributes = list(housing_num)  
cat_attributes = ["ocean_proximity"]  
full=Pipeline $=$ ColumnTransformer([ ("num",num+pipeline，num_attributes), ("cat",OneHotEncoder(),cat_attributes), ])

```txt
housing_prepared = fullpipeline.fit_transform(housing) 
```

Here is how this works: first we import the ColumnTransformer class, next we get the list of numerical column names and the list of categorical column names, and we construct a ColumnTransformer. The constructor requires a list of tuples, where each tuple contains a name22, a transformer and a list of names (or indices) of columns that the transformer should be applied to. In this example, we specify that the numer‐ ical columns should be transformed using the num_pipeline that we defined earlier, and the categorical columns should be transformed using a OneHotEncoder. Finally, we apply this ColumnTransformer to the housing data: it applies each transformer to the appropriate columns and concatenates the outputs along the second axis (the transformers must return the same number of rows).

Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns a dense matrix. When there is such a mix of sparse and dense matrices, the Colum nTransformer estimates the density of the final matrix (i.e., the ratio of non-zero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). In this example, it returns a dense matrix. And that’s it! We have a preprocessing pipeline that takes the full housing data and applies the appropriate transformations to each column.

![](images/63662e58fd5b29fa387f22529a856e87e8ea7c696f8609108eb8f4eee30aee8d.jpg)

Instead of a transformer, you can specify the string "drop" if you want the columns to be dropped. Or you can specify "pass through" if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to "passthrough") if you want these columns to be handled differently.

If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as sklearn-pandas, or roll out your own custom transformer to get the same function‐ ality as the ColumnTransformer. Alternatively, you can use the FeatureUnion class which can also apply different transformers and concatenate their outputs, but you cannot specify different columns for each transformer, they all apply to the whole data. It is possible to work around this limitation using a custom transformer for col‐ umn selection (see the Jupyter notebook for an example).

# Select and Train a Model

At last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote transformation pipelines to clean up and prepare your data for Machine Learning algorithms automatically. You are now ready to select and train a Machine Learning model.

# Training and Evaluating on the Training Set

The good news is that thanks to all these previous steps, things are now going to be much simpler than you might think. Let’s first train a Linear Regression model, like we did in the previous chapter:

from sklearn.linear_model import LinearRegression   
```python
lin_reg = LinearRegression()  
lin_reg.fit(housing_prepared, housing_labels) 
```

Done! You now have a working Linear Regression model. Let’s try it out on a few instances from the training set:

```python
>>> some_data = housing-iloc[:5]
>>> some_labels = housing_labels.iloc[:5]
>>> some_data_prepared = fullpipeline.transform(some_data)
>>> print("Predictions:", lin_reg.predict(some_data_prepared))
Predictions: [210644.6045 317768.8069 210956.4333 59218.9888 189747.5584]
>>> print("Labels:", list(some_labels))
Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] 
```

It works, although the predictions are not exactly accurate (e.g., the first prediction is off by close to $4 0 \% !$ ). Let’s measure this regression model’s RMSE on the whole train‐ ing set using Scikit-Learn’s mean_squared_error function:

```python
>>> from sklearn.metrics import mean_squared_error
>>> housingpredictions = lin_reg.predict(housing_prepared)
>>> lin_mse = mean_squared_error(housing_labels, housingpredictions)
>>> lin_rmse = np.sqrt(lin_mse)
>>> lin_rmse
68628.19819848922 
```

Okay, this is better than nothing but clearly not a great score: most districts’ median_housing_values range between $\$ 120,000$ and $\$ 265,000$ , so a typical predic‐ tion error of $\$ 68,628$ is not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, so this rules out the last option. You could try to add more features (e.g., the log of the popula‐ tion), but first let’s try a more complex model to see how it does.

Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data (Decision Trees are presented in more detail in Chapter 6). The code should look familiar by now:

```python
from sklearn.tree import DecisionTreeRegressor 
```

```python
tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels) 
```

Now that the model is trained, let’s evaluate it on the training set:

```txt
>>> housingpredictions = tree_reg.predict(housing_prepared)  
>>> tree_mse = mean_squared_error(housing_labels, housing_predictedancies)  
>>> tree_rmse = np.sqrt-tree_mse)  
>>> tree_rmse  
0.0 
```

Wait, what!? No error at all? Could this model really be absolutely perfect? Of course, it is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for train‐ ing, and part for model validation.

# Better Evaluation Using Cross-Validation

One way to evaluate the Decision Tree model would be to use the train_test_split function to split the training set into a smaller training set and a validation set, then

train your models against the smaller training set and evaluate them against the vali‐ dation set. It’s a bit of work, but nothing too difficult and it would work fairly well.

A great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The follow‐ ing code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array con‐ taining the 10 evaluation scores:

```python
from sklearn.model_selection import cross_val_score  
scores = cross_val_score-tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)  
tree_rmse Scores = np.sqrt(-scores) 
```

![](images/649ad630f5a2b9a5a95a458d0dc2933a06f0a29baa54a346bfa7085653c230cd.jpg)

Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a neg‐ ative value), which is why the preceding code computes -scores before calculating the square root.

Let’s look at the results:

```python
>>> def display Scores(scores):
...
    print("Scores:", scores)
...
    print("Mean:", scores.mean())
...
    print("Standard deviation:", scores.std())
...
>>> display Scores.tree Rimse Scores
Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782
71115.88230639 75585.14172901 70262.86139133 70273.6325285
75366.87952553 71231.65726027]
Mean: 71407.68766037929
Standard deviation: 2439.4345041191004 
```

Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to per‐ form worse than the Linear Regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 71,407, generally $^ { \pm 2 , 4 3 9 }$ . You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always possible.

Let’s compute the same scores for the Linear Regression model just to be sure:

```python
>>> lin Scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                         **scoring="neg_mean_squared_error", cv=10)
>>> lin_rmse Scores = np.sqrt(-lin Scores)
>>> displayScores(lin_rmse Scores) 
```

```txt
Scores: [66782.73843989 66960.118071 70347.95244419 74739.57052552  
68031.13388938 71193.84183426 64969.63056405 68281.61137997  
71552.91566558 67665.10082067]  
Mean: 69052.46136345083  
Standard deviation: 2731.674001798348 
```

That’s right: the Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model.

Let’s try one last model now: the RandomForestRegressor. As we will see in Chap‐ ter 7, Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algo‐ rithms even further. We will skip most of the code since it is essentially the same as for the other models:

```txt
>>> from sklearnensemble import RandomForestRegressor
>>> forest_reg = RandomForestRegressor()
>>> forest_reg.fit(housing_prepared, housing_labels)
>>> [...]
>>> forest_rmse
18603.515021376355
>>> display Scores(forest_rmse Scores)
Scores: [49519.80364233 47461.9115823 50029.02762854 52325.28068953
49308.39426421 53446.37892622 48634.8036574 47585.73832311
53490.10699751 50021.5852922 ]
Mean: 50182.303100336096
Standard deviation: 2097.0810550985693 
```

Wow, this is much better: Random Forests look very promising. However, note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions for overfitting are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. However, before you dive much deeper in Random Forests, you should try out many other models from various categories of Machine Learning algorithms (several Sup‐ port Vector Machines with different kernels, possibly a neural network, etc.), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models.

![](images/90a033692d18bda47dea3c3c6b9456620a646c97973f8b8bd22d1378a3cb59d5.jpg)

You should save every model you experiment with, so you can come back easily to any model you want. Make sure you save both the hyperparameters and the trained parameters, as well as the cross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types, and compare the types of errors they make. You can easily save Scikit-Learn models by using Python’s pickle module, or using sklearn.externals.joblib, which is more efficient at serializing large NumPy arrays:

```python
from sklearn内外ns import joblib  
joblib.dump(my_model, "my_model.pkl")  
# and later...  
my_model_loadd = joblib.load("my_model.pkl") 
```

# Fine-Tune Your Model

Let’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.

# Grid Search

One way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.

Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. For example, the following code searches for the best combi‐ nation of hyperparameter values for the RandomForestRegressor:

from sklearn.model_selection import GridSearchCV   
param_grid $=$ [ {n_estimators':[3,10,30],max_features':[2,4,6,8]}, {'bootstrap':[False],n_estimators':[3,10],max_features':[2,3,4]}, ]   
forest_reg $=$ RandomForestRegressor()   
grid_search $=$ GridSearchCV(forest_reg，param_grid，cv=5, scoring $\equiv$ 'neg_mean_squared_error'， return_train_score=True)   
grid_search.fit(housing_prepared,housing_labels)

This param_grid tells Scikit-Learn to first evaluate all $3 \times 4 = 1 2$ combinations of n_estimators and max_features hyperparameter values specified in the first dict (don’t worry about what these hyperparameters mean for now; they will be explained in Chapter 7), then try all $2 \times 3 = 6$ combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True (which is the default value for this hyperparameter).

All in all, the grid search will explore $1 2 + 6 = 1 8$ combinations of RandomForestRe gressor hyperparameter values, and it will train each model five times (since we are using five-fold cross validation). In other words, all in all, there will be $1 8 \times 5 = 9 0$ rounds of training! It may take quite a long time, but when it is done you can get the best combination of parameters like this:

```txt
>>>grid_search.best.params_   
{'max_features':8,'n_estimators':30} 
```

![](images/c820a364488ab404fea9796eab2b55f527988311fde78721ea861735406d7e75.jpg)

Since 8 and 30 are the maximum values that were evaluated, you should probably try searching again with higher values, since the score may continue to improve.

You can also get the best estimator directly:

```txt
>>> grid_search.best_estimator_
RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,
min_impurity_split=None, min_samples_leaf=1,
min_samples_split=2, min_weight_fraction_leaf=0.0,
n_estimators=30, n_jobs=None, oob_score=False, random_state=None,
verbose=0, warm_start=False) 
```

![](images/629a265c65da89b8ea04985b04b84b50775fa24e64ca4e1ca16c6abeba3e0fea.jpg)

If GridSearchCV is initialized with refit=True (which is the default), then once it finds the best estimator using crossvalidation, it retrains it on the whole training set. This is usually a good idea since feeding it more data will likely improve its perfor‐ mance.

And of course the evaluation scores are also available:

```erlang
>>> cvres = grid_search(cv_results_  
>>> for mean_score, params in zip(cvres["mean_test_score"], cvres["params]): 
```

```txt
... print(np.sqrt(-mean_score), params)  
63669.05791727153 {'max_features': 2, 'n_estimators': 3}  
55627.16171305252 {'max_features': 2, 'n_estimators': 10}  
53384.57867637289 {'max_features': 2, 'n_estimators': 30}  
60965.99185930139 {'max_features': 4, 'n_estimators': 3}  
52740.98248528835 {'max_features': 4, 'n_estimators': 10}  
50377.344409590376 {'max_features': 4, 'n_estimators': 30}  
58663.84733372485 {'max_features': 6, 'n_estimators': 3}  
52006.15355973719 {'max_features': 6, 'n_estimators': 10}  
50146.465964159885 {'max_features': 6, 'n_estimators': 30}  
57869.25504027614 {'max_features': 8, 'n_estimators': 3}  
51711.09443660957 {'max_features': 8, 'n_estimators': 10}  
49682.25345942335 {'max_features': 8, 'n_estimators': 30}  
62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}  
54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}  
59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}  
52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}  
57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}  
51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10} 
```

In this example, we obtain the best solution by setting the max_features hyperpara‐ meter to 8, and the n_estimators hyperparameter to 30. The RMSE score for this combination is 49,682, which is slightly better than the score you got earlier using the default hyperparameter values (which was 50,182). Congratulations, you have suc‐ cessfully fine-tuned your best model!

![](images/c0258f6a493cfeeac93bcda0a091fc5c4ad426024e94c472cd73c89b006eb51e.jpg)

Don’t forget that you can treat some of the data preparation steps as hyperparameters. For example, the grid search will automatically find out whether or not to add a feature you were not sure about (e.g., using the add_bedrooms_per_room hyperparameter of your CombinedAttributesAdder transformer). It may similarly be used to automatically find the best way to handle outliers, missing fea‐ tures, feature selection, and more.

# Randomized Search

The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combi‐ nations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main bene‐ fits:

• If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few val‐ ues per hyperparameter with the grid search approach).   
• You have more control over the computing budget you want to allocate to hyper‐ parameter search, simply by setting the number of iterations.

# Ensemble Methods

Another way to fine-tune your system is to try to combine the models that perform best. The group (or “ensemble”) will often perform better than the best individual model (just like Random Forests perform better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors. We will cover this topic in more detail in Chapter 7.

# Analyze the Best Models and Their Errors

You will often gain good insights on the problem by inspecting the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions:

```python
>>> feature-importantances = grid_search.best_estimator_.feature-importantances_  
>>> feature-importantances  
array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]) 
```

Let’s display these importance scores next to their corresponding attribute names:

```python
>>> extra_attributes = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
>>> catEncoder = fullpipeline.named_transformers["cat"]
>>> cat_one-hot_attributes = list(catEncoder.categories_[0])
>>> attributes = num_attributes + extra_attributes + cat_one-hot_attributes
>>> sorted(zip feature-importantances, attributes), reverse=True)
[(0.3661589806181342, 'median_income'),
(0.1647809935615905, 'INLAND'),
(0.10879295677551573, 'pop_per_hhold'),
(0.07334423551601242, 'longitude'),
(0.0629090704826203, 'latitude'),
(0.05641917918195401, 'rooms_per_hhold'],
(0.05335107734767581, 'bedrooms_per_room'),
(0.041143798478729635, 'housing_median_age'),
(0.014874280890402767, 'population'),
(0.014672685420543237, 'total Rooms'),
(0.014257599323407807, 'households'),
(0.014106483453584102, 'total Bedrooms'],
(0.010311488326303787, "<1H OCEAN"),
(0.002856474637320158, 'NEAR OCEAN"), 
```

(0.00196041559947807, 'NEAR BAY'),

(6.028038672736599e-05, 'ISLAND')]

With this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others).

You should also look at the specific errors that your system makes, then try to under‐ stand why it makes them and what could fix the problem (adding extra features or, on the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).

# Evaluate Your System on the Test Set

After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your full_pipeline to transform the data (call transform(), not fit_transform(), you do not want to fit the test set!), and evaluate the final model on the test set:

```python
final_model = grid_search.best_estimator_  
X_test = strat_test_set.drop("median_house_value", axis=1)  
y_test = strat_test_set["median_house_value"].copy()  
X_test_prepared = fullpipeline.transform(X_test)  
finalpredictions = final_model.predict(X_test_prepared)  
final_mse = mean_squared_error(y_test, finalpredictions)  
final_rmse = np.sqrt(final_mse) # => evaluates to 47,730.2 
```

In some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just $0 . 1 \%$ better than the model cur‐ rently in production? You might want to have an idea of how precise this estimate is. For this, you can compute a $9 5 \%$ condence interval for the generalization error using scipy.stats.t.interval():

```python
>>> from scipy import stats
>>> confidence = 0.95
>>> squared Errors = (finalpredictions - y_test) ** 2
>>> np.sqrt/stats.t.interval(confidence, len(squared Errors) - 1,
...
...
...
...
array([45685.10470776, 49691.25001878]) 
```

The performance will usually be slightly worse than what you measured using crossvalidation if you did a lot of hyperparameter tuning (because your system ends up fine-tuned to perform well on the validation data, and will likely not perform as well

on unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data.

Now comes the project prelaunch phase: you need to present your solution (high‐ lighting what you have learned, what worked and what did not, what assumptions were made, and what your system’s limitations are), document everything, and create nice presentations with clear visualizations and easy-to-remember statements (e.g., “the median income is the number one predictor of housing prices”). In this Califor‐ nia housing example, the final performance of the system is not better than the experts’, but it may still be a good idea to launch it, especially if this frees up some time for the experts so they can work on more interesting and productive tasks.

# Launch, Monitor, and Maintain Your System

Perfect, you got approval to launch! You need to get your solution ready for produc‐ tion, in particular by plugging the production input data sources into your system and writing tests.

You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. This is important to catch not only sudden breakage, but also performance degradation. This is quite common because models tend to “rot” as data evolves over time, unless the models are regularly trained on fresh data.

Evaluating your system’s performance will require sampling the system’s predictions and evaluating them. This will generally require a human analysis. These analysts may be field experts, or workers on a crowdsourcing platform (such as Amazon Mechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐ tion pipeline into your system.

You should also make sure you evaluate the system’s input data quality. Sometimes performance will degrade slightly because of a poor quality signal (e.g., a malfunc‐ tioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the inputs is particularly important for online learning systems.

Finally, you will generally want to train your models on a regular basis using fresh data. You should automate this process as much as possible. If you don’t, you are very likely to refresh your model only every six months (at best), and your system’s perfor‐ mance may fluctuate severely over time. If your system is an online learning system, you should make sure you save snapshots of its state at regular intervals so you can easily roll back to a previously working state.

# Try It Out!

Hopefully this chapter gave you a good idea of what a Machine Learning project looks like, and showed you some of the tools you can use to train a great system. As you can see, much of the work is in the data preparation step, building monitoring tools, setting up human evaluation pipelines, and automating regular model training. The Machine Learning algorithms are also important, of course, but it is probably preferable to be comfortable with the overall process and know three or four algo‐ rithms well rather than to spend all your time exploring advanced algorithms and not enough time on the overall process.

So, if you have not already done so, now is a good time to pick up a laptop, select a dataset that you are interested in, and try to go through the whole process from A to Z. A good place to start is on a competition website such as http://kaggle.com/: you will have a dataset to play with, a clear goal, and people to share the experience with.

# Exercises

Using this chapter’s housing dataset:

1. Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyper‐ parameters such as kernel $\bar { . }$ "linear" (with various values for the C hyperpara‐ meter) or kernel $\displaystyle { \overline { { \cdot } } }$ "rbf" (with various values for the C and gamma hyperparameters). Don’t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?   
2. Try replacing GridSearchCV with RandomizedSearchCV.   
3. Try adding a transformer in the preparation pipeline to select only the most important attributes.   
4. Try creating a single pipeline that does the full data preparation plus the final prediction.   
5. Automatically explore some preparation options using GridSearchCV.

Solutions to these exercises are available in the online Jupyter notebooks at https:// github.com/ageron/handson-ml2.

![](images/93e8bcac44417b202e0dd08dcebfddd11f9e71f0c8cc5bc0a0cd3415c08fde36.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 3 in the final release of the book.

In Chapter 1 we mentioned that the most common supervised learning tasks are regression (predicting values) and classification (predicting classes). In Chapter 2 we explored a regression task, predicting housing values, using various algorithms such as Linear Regression, Decision Trees, and Random Forests (which will be explained in further detail in later chapters). Now we will turn our attention to classification systems.

# MNIST

In this chapter, we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Cen‐ sus Bureau. Each image is labeled with the digit it represents. This set has been stud‐ ied so much that it is often called the “Hello World” of Machine Learning: whenever people come up with a new classification algorithm, they are curious to see how it will perform on MNIST. Whenever someone learns Machine Learning, sooner or later they tackle MNIST.

Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset:1

```python
>>> from sklearndatasets import fetch_openml
>>> mnist = fetch_openml('mnist_784', version=1)
>>> mnist.keys()
dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details', 'categories', 'url']) 
```

Datasets loaded by Scikit-Learn generally have a similar dictionary structure includ‐ ing:

• A DESCR key describing the dataset   
• A data key containing an array with one row per instance and one column per feature   
• A target key containing an array with the labels

Let’s look at these arrays:

>>X，y $\equiv$ mnist["data"],mnist["target"]   
>>X.shape   
(70000,784)   
>>y.shape   
(70000,)

There are 70,000 images, and each image has 784 features. This is because each image is $2 8 \times 2 8$ pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to do is grab an instance’s feature vector, reshape it to a $2 8 \times 2 8$ array, and display it using Matplotlib’s imshow() function:

import matplotlib as plt   
import matplotlib.pyplot as plt   
somedigit $= X[\theta ]$ somedigit_image $\equiv$ somedigit.reshape(28,28)   
plt.imshow(somedigit_image，cmap $\equiv$ mpl.cm.binary，interpolation $\equiv$ "nearest")   
plt.axis("off")   
plt.show()

![](images/06ec59d0a797f8dc656f688cbbe4a74fd31887a2f29ec0086c5217ec2a703772.jpg)

This looks like a 5, and indeed that’s what the label tells us:

```txt
>> y[0] '5' 
```

Note that the label is a string. We prefer numbers, so let’s cast y to integers:

```txt
>>> y = y.astype(np.uint8) 
```

![](images/71f818b6500d4d90e2d970a1dc80e0da1a13028c1e6370f1b4920fc9057ac128.jpg)  
Figure 3-1 shows a few more images from the MNIST dataset to give you a feel for the complexity of the classification task.   
Figure 3-1. A few digits from the MNIST dataset

But wait! You should always create a test set and set it aside before inspecting the data closely. The MNIST dataset is actually already split into a training set (the first 60,000 images) and a test set (the last 10,000 images):

```txt
X_train, X_test, y_train, y_test = X[:60000], X[60000]:, y[:60000], y[60000:] 
```

The training set is already shuffled for us, which is good as this guarantees that all cross-validation folds will be similar (you don’t want one fold to be missing some dig‐ its). Moreover, some learning algorithms are sensitive to the order of the training

instances, and they perform poorly if they get many similar instances in a row. Shuf‐ fling the dataset ensures that this won’t happen.2

# Training a Binary Classier

Let’s simplify the problem for now and only try to identify one digit—for example, the number 5. This $^ { \mathfrak { a } } 5$ -detector” will be an example of a binary classier, capable of distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for this classification task:

```python
y_train_5 = (y_train == 5) # True for all 5s, False for all other digits.  
y_test_5 = (y_test == 5) 
```

Okay, now let’s pick a classifier and train it. A good place to start is with a Stochastic Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This clas‐ sifier has the advantage of being capable of handling very large datasets efficiently. This is in part because SGD deals with training instances independently, one at a time (which also makes SGD well suited for online learning), as we will see later. Let’s create an SGDClassifier and train it on the whole training set:

```python
from sklearn.linear_model import SGDClassifier 
```

```txt
sgd_clf = SGDClassifier(random_state=42)  
sgd_clf.fit(X_train, y_train_5) 
```

![](images/49381412cb434388faaca45177305e7afce3d8a32875948b4115365810ef3113.jpg)

The SGDClassifier relies on randomness during training (hence the name “stochastic”). If you want reproducible results, you should set the random_state parameter.

Now you can use it to detect images of the number 5:

```txt
>>sgd_clf.predict([some digit]) array([ True]) 
```

The classifier guesses that this image represents a 5 (True). Looks like it guessed right in this particular case! Now, let’s evaluate this model’s performance.

# Performance Measures

Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this chapter on this topic. There are many performance

measures available, so grab another coffee and get ready to learn many new concepts and acronyms!

# Measuring Accuracy Using Cross-Validation

A good way to evaluate a model is to use cross-validation, just as you did in Chap‐ ter 2.

# Implementing Cross-Validation

Occasionally you will need more control over the cross-validation process than what Scikit-Learn provides off-the-shelf. In these cases, you can implement crossvalidation yourself; it is actually fairly straightforward. The following code does roughly the same thing as Scikit-Learn’s cross_val_score() function, and prints the same result:

from sklearn.model_selection import StratifiedKFold   
from sklearn.base import clone   
skfolds $=$ StratifiedKFold(n_splits $\coloneqq$ 3, random_state $\coloneqq$ 42)   
for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf $=$ clone(sgd_clf) X_train_folds $=$ X_train[train_index] y_train_folds $=$ y_train_5[train_index] X_test_fold $=$ X_train[test_index] y_test_fold $=$ y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred $=$ clone_clf.predict(X_test_fold) n.correct $=$ sum(y_pred $= =$ y_test_fold) print(n(correct / len(y_pred)) # prints 0.9502, 0.96565 and 0.96495

The StratifiedKFold class performs stratified sampling (as explained in Chapter 2) to produce folds that contain a representative ratio of each class. At each iteration the code creates a clone of the classifier, trains that clone on the training folds, and makes predictions on the test fold. Then it counts the number of correct predictions and outputs the ratio of correct predictions.

Let’s use the cross_val_score() function to evaluate your SGDClassifier model using K-fold cross-validation, with three folds. Remember that K-fold crossvalidation means splitting the training set into K-folds (in this case, three), then mak‐ ing predictions and evaluating them on each fold using a model trained on the remaining folds (see Chapter 2):

```python
>>> from sklearn.model_selection import cross_val_score
>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
array([0.96355, 0.93795, 0.95615]) 
```

Wow! Above $9 3 \%$ accuracy (ratio of correct predictions) on all cross-validation folds? This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very dumb classifier that just classifies every single image in the “not- $. 5 ^ { \mathrm { \scriptsize ~ , } }$ class:

from sklearn.base import BaseEstimator   
```python
class Never5Classifier(BaseEstimator): def fit(self，X，y=None): pass def predict(self,X): return np.zeros((len(X)，1)，dtype=bool)
```

Can you guess this model’s accuracy? Let’s find out:

```python
>>> never_5 clf = Never5Classifier()
>>> cross_val_score(never_5 clf, X_train, y_train_5, cv=3, scoring="accuracy")
array([0.91125, 0.90855, 0.90915]) 
```

That’s right, it has over $9 0 \%$ accuracy! This is simply because only about $1 0 \%$ of the images are 5s, so if you always guess that an image is not a 5, you will be right about $9 0 \%$ of the time. Beats Nostradamus.

This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).

# Confusion Matrix

A much better way to evaluate the performance of a classifier is to look at the confu‐ sion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the $5 ^ { \mathrm { t h } }$ row and $3 ^ { \mathrm { r d } }$ column of the confusion matrix.

To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let’s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function:

from sklearn.model_selection import cross_val_predict

```python
y_train_pred = cross_val_prediction(sgd_clf, X_train, y_train_5, cv=3) 
```

Just like the cross_val_score() function, cross_val_predict() performs K-fold cross-validation, but instead of returning the evaluation scores, it returns the predic‐

tions made on each test fold. This means that you get a clean prediction for each instance in the training set (“clean” meaning that the prediction is made by a model that never saw the data during training).

Now you are ready to get the confusion matrix using the confusion_matrix() func‐ tion. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred):

```python
>>> from sklearn.metrics import confusion_matrix
>>> confusion_matrix(y_train_5, y_train_pred)
array([[53057, 1522],
[1325, 4096]]) 
```

Each row in a confusion matrix represents an actual class, while each column repre‐ sents a predicted class. The first row of this matrix considers non-5 images (the nega‐ tive class): 53,057 of them were correctly classified as non-5s (they are called true negatives), while the remaining 1,522 were wrongly classified as 5s (false positives). The second row considers the images of 5s (the positive class): 1,325 were wrongly classified as non-5s (false negatives), while the remaining 4,096 were correctly classi‐ fied as 5s (true positives). A perfect classifier would have only true positives and true negatives, so its confusion matrix would have nonzero values only on its main diago‐ nal (top left to bottom right):

```txt
>>> y_train/perfectpredictions = y_train_5 # pretend we reached perfection  
>>> confusion_matrix(y_train_5, y_train_perfectpredictions)  
array([[54579, 0], [0, 5421]]) 
```

The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre‐ dictions; this is called the precision of the classifier (Equation 3-1).

Equation 3-1. Precision

$$
\mathrm {p r e c i s i o n} = \frac {T P}{T P + F P}
$$

TP is the number of true positives, and FP is the number of false positives.

A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision $= 1 / 1 = 1 0 0 \%$ . This would not be very useful since the classifier would ignore all but one positive instance. So precision is typically used along with another metric named recall, also called sensitivity or true positive rate

(TPR): this is the ratio of positive instances that are correctly detected by the classifier (Equation 3-2).

Equation 3-2. Recall

$$
\text {r e c a l l} = \frac {T P}{T P + F N}
$$

FN is of course the number of false negatives.

If you are confused about the confusion matrix, Figure 3-2 may help.

![](images/cc7f1bc297b74d1688e89c76e3aad8a31f3b4dc67ae17eece187570d88af53bb.jpg)  
Figure 3-2. An illustrated confusion matrix

# Precision and Recall

Scikit-Learn provides several functions to compute classifier metrics, including preci‐ sion and recall:

```python
>>> from sklearn.metrics import precision_score, recall_score
>>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)
0.7290850836596654
>>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)
0.7555801512636044 
```

Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only $7 2 . 9 \%$ of the time. More‐ over, it only detects $7 5 . 6 \%$ of the 5s.

It is often convenient to combine precision and recall into a single metric called the $F _ { I }$ score, in particular if you need a simple way to compare two classifiers. The $\mathrm { F } _ { 1 }$ score is the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean

treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high $\mathrm { F } _ { 1 }$ score if both recall and precision are high.

Equation 3-3. $F _ { I }$

$$
F _ {1} = \frac {2}{\frac {1}{\text {p r e c i s i o n}} + \frac {1}{\text {r e c a l l}}} = 2 \times \frac {\text {p r e c i s i o n} \times \text {r e c a l l}}{\text {p r e c i s i o n} + \text {r e c a l l}} = \frac {T P}{T P + \frac {F N + F P}{2}}
$$

To compute the $\mathrm { F } _ { 1 }$ score, simply call the f1_score() function:

```txt
>>> from sklearn.metrics import f1_score
>>> f1_score(y_train_5, y_train_pred)
0.7420962043663375 
```

The $\mathrm { F } _ { 1 }$ score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other con‐ texts you really care about recall. For example, if you trained a classifier to detect vid‐ eos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a clas‐ sifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the clas‐ sifier’s video selection). On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only $3 0 \%$ precision as long as it has $9 9 \%$ recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught).

Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the precision/recall tradeo.

# Precision/Recall Tradeo

To understand this tradeoff, let’s look at how the SGDClassifier makes its classifica‐ tion decisions. For each instance, it computes a score based on a decision function, and if that score is greater than a threshold, it assigns the instance to the positive class, or else it assigns it to the negative class. Figure 3-3 shows a few digits positioned from the lowest score on the left to the highest score on the right. Suppose the deci‐ sion threshold is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold, and one false positive (actually a 6). Therefore, with that threshold, the precision is $8 0 \%$ (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is $6 7 \%$ (4 out of 6). Now if you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing precision (up to $1 0 0 \%$ in this case), but one true positive becomes a false negative, decreasing recall down to $5 0 \%$ . Conversely, lowering the threshold increases recall and reduces precision.

![](images/e677e66aa7a165fa119e81ec3d2e6e102df3c4af5ee4ea0437c722b95c1a6143.jpg)  
Figure 3-3. Decision threshold and precision/recall tradeo

Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier’s predict() method, you can call its decision_function() method, which returns a score for each instance, and then make predictions based on those scores using any threshold you want:

```python
>>> y Scores = sgd_clf.decision_function([some_digit])
>>> y Scores
array([2412.53175101])
>>> threshold = 0
>>> y SOME_digit_pred = (y Scores > threshold)
array([ True]) 
```

The SGDClassifier uses a threshold equal to 0, so the previous code returns the same result as the predict() method (i.e., True). Let’s raise the threshold:

```txt
>>> threshold = 8000
>>> y(Somedigit_pred = (y Scores > threshold)
>>> y(Somedigit_pred
array([False]) 
```

This confirms that raising the threshold decreases recall. The image actually repre‐ sents a 5, and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 8,000.

Now how do you decide which threshold to use? For this you will first need to get the scores of all instances in the training set using the cross_val_predict() function again, but this time specifying that you want it to return decision scores instead of predictions:

```python
y Scores = cross_val.predict(sgd_clf, X_train, y_train_5, cv=3, method="decision_function") 
```

Now with these scores you can compute precision and recall for all possible thresh‐ olds using the precision_recall_curve() function:

from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds $=$ precision_recall_curve(y_train_5, y_scores)

Finally, you can plot precision and recall as functions of the threshold value using Matplotlib (Figure 3-4):

```python
def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:1], "b--", label="Precision") plt.plot(thresholds, recalls[:1], "g--", label="Recall") [...] # highlight the threshold, add the legend, axis label and grid 
```

```txt
plot_precision_recall_vs_threshold(precisions, recalls, thresholds)  
plt.show() 
```

![](images/6ccc3e2203f80e592b1dbc3e8e7cd7eb66340034ff31ab5a5e673ea81cc6b4ed.jpg)  
Figure 3-4. Precision and recall versus the decision threshold

![](images/a0aea51bc09b0a5221c1d38ef2d9f63c144cd8d696299e15fbe29a898c517608.jpg)

You may wonder why the precision curve is bumpier than the recall curve in Figure 3-4. The reason is that precision may sometimes go down when you raise the threshold (although in general it will go up). To understand why, look back at Figure 3-3 and notice what happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 $( 8 0 \% )$ down to 3/4 $( 7 5 \% )$ . On the other hand, recall can only go down when the thres‐ hold is increased, which explains why its curve looks smooth.

Another way to select a good precision/recall tradeoff is to plot precision directly against recall, as shown in Figure 3-5 (the same threshold as earlier is highlighed).

![](images/3238d13c748fcb1d97f9ba39b18fc674cebcaccf422fb19d44a808d17bec2595.jpg)  
Figure 3-5. Precision versus recall

You can see that precision really starts to fall sharply around $8 0 \%$ recall. You will probably want to select a precision/recall tradeoff just before that drop—for example, at around $6 0 \%$ recall. But of course the choice depends on your project.

So let’s suppose you decide to aim for $9 0 \%$ precision. You look up the first plot and find that you need to use a threshold of about 8,000. To be more precise you can search for the lowest threshold that gives you at least $9 0 \%$ precision (np.argmax() will give us the first index of the maximum value, which in this case means the first True value):

```python
threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816 
```

To make predictions (on the training set for now), instead of calling the classifier’s predict() method, you can just run this code:

```txt
y_train_pred_90 = (y Scores >= threshold_90_precis) 
```

Let’s check these predictions’ precision and recall:

```txt
>>> precision_score(y_train_5, y_train_pred_90)  
0.9000380083618396  
>>> recall_score(y_train_5, y_train_pred_90)  
0.4368197749492714 
```

Great, you have a $9 0 \%$ precision classifier ! As you can see, it is fairly easy to create a classifier with virtually any precision you want: just set a high enough threshold, and you’re done. Hmm, not so fast. A high-precision classifier is not very useful if its recall is too low!

# The ROC Curve

The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot‐ ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, which is the ratio of negative instances that are correctly classified as negative. The TNR is also called specicity. Hence the ROC curve plots sensitivity (recall) versus 1 – specicity.

To plot the ROC curve, you first need to compute the TPR and FPR for various thres‐ hold values, using the roc_curve() function:

from sklearn.metrics import roc_curve

fpr, tpr, thresholds $=$ roc_curve(y_train_5, y_scores)

Then you can plot the FPR against the TPR using Matplotlib. This code produces the plot in Figure 3-6:

```python
def plot_roc_curve(fpr, tpr, label=None):  
    plt.plot(fpr, tpr, linewidth=2, label=label)  
    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal  
    [...] # Add axis labels and grid  
plot_roc Curve(fpr, tpr)  
plt.show() 
```

![](images/d4aea4dc2d35c52c4a52fe9dd72a5c584f6d43614500c2eb84d9aaf10f691b7e.jpg)  
Figure 3-6. ROC curve

Once again there is a tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).

One way to compare classifiers is to measure the area under the curve (AUC). A per‐ fect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC:

```txt
>>> from sklearn.metrics import roc_auc_score
>>> roc_auc_score(y_train_5, y Scores)
0.9611778893101814 
```

![](images/8bc748eb0c9cafc584769d6346f8d88d873ecc6392001ec32896243bd5f43187.jpg)

Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise. For example, looking at the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear that the classifier has room for improvement (the curve could be closer to the topright corner).

Let’s train a RandomForestClassifier and compare its ROC curve and ROC AUC score to the SGDClassifier. First, you need to get scores for each instance in the training set. But due to the way it works (see Chapter 7), the RandomForestClassi fier class does not have a decision_function() method. Instead it has a pre dict_proba() method. Scikit-Learn classifiers generally have one or the other. The predict_proba() method returns an array containing a row per instance and a col‐ umn per class, each containing the probability that the given instance belongs to the given class (e.g., $7 0 \%$ chance that the image represents a 5):

from sklearn.ensemble import RandomForestClassifier   
y_scores_forest $=$ y_probas_forest[:, 1] # score $=$ proba of positive class fpr_forest, tpr_forest, thresholds_forest $=$ roc_curve(y_train_5,y_scores_forest)   
plt.plot(fpr, tpr, "b:", label="SGD") plot_roc_curve(fpr_forest, tpr_forest, "Random Forest") plt.legend(loc="lower right") plt.show()   
```python
forest_clf = RandomForestClassifier(random_state=42)
y_probabilitiesForest = cross_val.predict(forest_clf, X_train, y_train_5, cv=3, method="predict(prob") 
```

But to plot a ROC curve, you need scores, not probabilities. A simple solution is to use the positive class’s probability as the score:

Now you are ready to plot the ROC curve. It is useful to plot the first ROC curve as well to see how they compare (Figure 3-7):

![](images/07c8bbb528a55d1ec43016a067cf6b195b23bae31717523ee59b7f979357e4e6.jpg)  
Figure 3-7. Comparing ROC curves

As you can see in Figure 3-7, the RandomForestClassifier’s ROC curve looks much better than the SGDClassifier’s: it comes much closer to the top-left corner. As a result, its ROC AUC score is also significantly better:

```txt
>>roc_auc_score(y_train_5，y Scores_forest) 0.9983436731328145 
```

Try measuring the precision and recall scores: you should find $9 9 . 0 \%$ precision and $8 6 . 6 \%$ recall. Not too bad!

Hopefully you now know how to train binary classifiers, choose the appropriate met‐ ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves and ROC AUC scores. Now let’s try to detect more than just the 5s.

# Multiclass Classication

Whereas binary classifiers distinguish between two classes, multiclass classiers (also called multinomial classiers) can distinguish between more than two classes.

Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐ ous strategies that you can use to perform multiclass classification using multiple binary classifiers.

For example, one way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score. This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest).

Another strategy is to train a binary classifier for every pair of digits: one to distin‐ guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy. If there are $N$ classes, you need to train $N \times \left( N - 1 \right) /$ 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels. The main advan‐ tage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.

Some algorithms (such as Support Vector Machine classifiers) scale poorly with the size of the training set, so for these algorithms OvO is preferred since it is faster to train many classifiers on small training sets than training few classifiers on large training sets. For most binary classification algorithms, however, OvA is preferred.

Scikit-Learn detects when you try to use a binary classification algorithm for a multi‐ class classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO). Let’s try this with the SGDClassifier:

```txt
>>> sgd_clf.fit(X_train, y_train)  # y_train, not y_train_5
>>> sgd_clf.predict([[some_digit])
array([5], dtype=uint8) 
```

That was easy! This code trains the SGDClassifier on the training set using the origi‐ nal target classes from 0 to 9 (y_train), instead of the 5-versus-all target classes (y_train_5). Then it makes a prediction (a correct one in this case). Under the hood, Scikit-Learn actually trained 10 binary classifiers, got their decision scores for the image, and selected the class with the highest score.

To see that this is indeed the case, you can call the decision_function() method. Instead of returning just one score per instance, it now returns 10 scores, one per class:

```txt
>>> some_digit Scores = sgd_clf.decision_function([some_digit])
>>> some Digit Scores
array([-15955.22627845, -38080.96296175, -13326.66694897, 573.52692379, -17680.6846644, 2412.53175101, -25526.86498156, -12290.15704709, -7946.05205023, -10631.35888549]) 
```

The highest score is indeed the one corresponding to class 5:

```txt
>>> np.argmax(some_digit_scores)
5
>>> sgd clf/classes_
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)
>>> sgd clf/classes_[5]
5 
```

![](images/597ab7cd8e9aa2527bd57e5cb274f2ee485df7b21b41a0162a5e8e6f73d7deaa.jpg)

When a classifier is trained, it stores the list of target classes in its classes_ attribute, ordered by value. In this case, the index of each class in the classes_ array conveniently matches the class itself (e.g., the class at index 5 happens to be class 5), but in general you won’t be so lucky.

If you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use the OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance and pass a binary classifier to its constructor. For example, this code creates a multi‐ class classifier using the OvO strategy, based on a SGDClassifier:

```python
>>> from sklearn四级 import OneVsOneClassifier
>>> ovo_clf = OneVsOneClassifier(SGClassifier(random_state=42))
>>> ovo_clf.fit(X_train, y_train)
>>> ovo_clf.predict([some_digit]) 
```

```txt
array([5], dtype=uint8)  
>>> len(ovo_clf.estimators_)  
45 
```

Training a RandomForestClassifier is just as easy:

```python
>>> forest clf.fit(X_train, y_train)
>>> forest clf.predict([some_digit])
array([5], dtype=uint8) 
```

This time Scikit-Learn did not have to run OvA or OvO because Random Forest classifiers can directly classify instances into multiple classes. You can call predict_proba() to get the list of probabilities that the classifier assigned to each instance for each class:

```python
>>> forest clf.predict(probability[some_digit])
array([[0. , 0. , 0.01, 0.08, 0. , 0.9, 0. , 0. , 0.01]]) 
```

You can see that the classifier is fairly confident about its prediction: the 0.9 at the $5 ^ { \mathrm { t h } }$ index in the array means that the model estimates a $9 0 \%$ probability that the image represents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐ tively with $1 \%$ , $8 \%$ and $1 \%$ probability.

Now of course you want to evaluate these classifiers. As usual, you want to use crossvalidation. Let’s evaluate the SGDClassifier’s accuracy using the cross_val_score() function:

```erlang
>>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring="accuracy")  
array([0.8489802, 0.87129356, 0.86988048]) 
```

It gets over $8 4 \%$ on all test folds. If you used a random classifier, you would get $1 0 \%$ accuracy, so this is not such a bad score, but you can still do much better. For exam‐ ple, simply scaling the inputs (as discussed in Chapter 2) increases accuracy above $8 9 \%$ :

```python
>>> from sklearn preprocess import StandardScaler
>>>Scaler = StandardScaler()
>>>X_trainScaled =Scaler.fit_transform(X_train.A genotype(np.float64))
>>>cross_val_score(sgd_clf, X_trainScaled, y_train, cv=3, scoring="accuracy")
array([0.89707059, 0.8960948, 0.90693604]) 
```

# Error Analysis

Of course, if this were a real project, you would follow the steps in your Machine Learning project checklist (see ???): exploring data preparation options, trying out multiple models, shortlisting the best ones and fine-tuning their hyperparameters using GridSearchCV, and automating as much as possible, as you did in the previous chapter. Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.

First, you can look at the confusion matrix. You need to make predictions using the cross_val_predict() function, then call the confusion_matrix() function, just like you did earlier:

```txt
>> y_train_pred = cross_val_prediction(sgd_clf, X_trainScaled, y_train, cv=3)  
>> conf_mx = confusion_matrix(y_train, y_train_pred)  
>> conf_mx  
array([[5578, 0, 22, 7, 8, 45, 35, 5, 222, 1],  
[0, 6410, 35, 26, 4, 44, 4, 8, 198, 13],  
[28, 27, 5232, 100, 74, 27, 68, 37, 354, 11],  
[23, 18, 115, 5254, 2, 209, 26, 38, 373, 73],  
[11, 14, 45, 12, 5219, 11, 33, 26, 299, 172],  
[26, 16, 31, 173, 54, 4484, 76, 14, 482, 65],  
[31, 17, 45, 2, 42, 98, 5556, 3, 123, 1],  
[20, 10, 53, 27, 50, 13, 3, 5696, 173, 220],  
[17, 64, 47, 91, 3, 125, 24, 11, 5421, 48],  
[24, 18, 29, 67, 116, 39, 1, 174, 329, 5152]) 
```

That’s a lot of numbers. It’s often more convenient to look at an image representation of the confusion matrix, using Matplotlib’s matshow() function:

```txt
plt MATshow(conf_mx, cmap=plt.cm(gray)  
plt.show() 
```

![](images/96ab4769a817a586b4fdae80b1bb97f659de0afd53ddacd981d93a100eaf08b8.jpg)

This confusion matrix looks fairly good, since most images are on the main diagonal, which means that they were classified correctly. The 5s look slightly darker than the other digits, which could mean that there are fewer images of 5s in the dataset or that the classifier does not perform as well on 5s as on other digits. In fact, you can verify that both are the case.

Let’s focus the plot on the errors. First, you need to divide each value in the confusion matrix by the number of images in the corresponding class, so you can compare error

rates instead of absolute number of errors (which would make abundant classes look unfairly bad):

```python
row-sums = conf_mx.sum(axis=1, keepdims=True)  
norm_conf_mx = conf_mx / row-sums 
```

Now let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:

```txt
np fillsdiagonal(norm_conf_mx, 0)  
plt.subplot(norm_conf_mx, cmap=plt.cm(gray)  
plt.show() 
```

![](images/96985e5a1bd282eb5810b0828d726dc9ec93921d18486807ef80876c1b285081.jpg)

Now you can clearly see the kinds of errors the classifier makes. Remember that rows represent actual classes, while columns represent predicted classes. The column for class 8 is quite bright, which tells you that many images get misclassified as 8s. How‐ ever, the row for class 8 is not that bad, telling you that actual 8s in general get prop‐ erly classified as 8s. As you can see, the confusion matrix is not necessarily symmetrical. You can also see that 3s and 5s often get confused (in both directions).

Analyzing the confusion matrix can often give you insights on ways to improve your classifier. Looking at this plot, it seems that your efforts should be spent on reducing the false 8s. For example, you could try to gather more training data for digits that look like 8s (but are not) so the classifier can learn to distinguish them from real 8s. Or you could engineer new features that would help the classifier—for example, writ‐ ing an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5 has none). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make some patterns stand out more, such as closed loops.

Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and why it is failing, but it is more difficult and time-consuming.

For example, let’s plot examples of 3s and 5s (the plot_digits() function just uses Matplotlib’s imshow() function; see this chapter’s Jupyter notebook for details):

```python
cl_a, cl_b = 3, 5  
X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]  
X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]  
X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]  
X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]  
plt.figure(figsize=(8,8))  
plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)  
plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)  
plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)  
plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)  
plt.show() 
```

![](images/90e815db19ad0b1c2c904e0560fbd02fae241308f59c35cc3cfb0fa5c98202fe.jpg)

The two $5 { \times } 5$ blocks on the left show digits classified as 3s, and the two $5 { \times } 5$ blocks on the right show images classified as 5s. Some of the digits that the classifier gets wrong (i.e., in the bottom-left and top-right blocks) are so badly written that even a human would have trouble classifying them (e.g., the 5 on the $1 ^ { \mathrm { s t } }$ row and $2 ^ { \mathrm { n d } }$ column truly looks like a badly written 3). However, most misclassified images seem like obvious errors to us, and it’s hard to understand why the classifier made the mistakes it did.3 The reason is that we used a simple SGDClassifier, which is a linear model. All it does is assign a weight per class to each pixel, and when it sees a new image it just sums up the weighted pixel intensities to get a score for each class. So since 3s and 5s differ only by a few pixels, this model will easily confuse them.

The main difference between 3s and 5s is the position of the small line that joins the top line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left, the classifier might classify it as a 5, and vice versa. In other words, this classifier is quite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion would be to preprocess the images to ensure that they are well centered and not too rotated. This will probably help reduce other errors as well.

# Multilabel Classication

Until now each instance has always been assigned to just one class. In some cases you may want your classifier to output multiple classes for each instance. For example, consider a face-recognition classifier: what should it do if it recognizes several people on the same picture? Of course it should attach one tag per person it recognizes. Say the classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then when it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning “Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple binary tags is called a multilabel classication system.

We won’t go into face recognition just yet, but let’s look at a simpler example, just for illustration purposes:

from sklearn.neighbors import KNeighborsClassifier

```txt
y_train_large = (y_train >= 7)  
y_trainOdd = (y_train % 2 == 1)  
yMULTIlabel = np.c_[y_train_large, y_train Odd] 
```

```python
knn_clf = KNeighborsClassifier()  
knn_clf.fit(X_train, y_multilabel) 
```

This code creates a y_multilabel array containing two target labels for each digit image: the first indicates whether or not the digit is large (7, 8, or 9) and the second indicates whether or not it is odd. The next lines create a KNeighborsClassifier instance (which supports multilabel classification, but not all classifiers do) and we train it using the multiple targets array. Now you can make a prediction, and notice that it outputs two labels:

```clojure
>>> knn_clf.predict([somedigit]) array([[False, True]]) 
```

And it gets it right! The digit 5 is indeed not large (False) and odd (True).

There are many ways to evaluate a multilabel classifier, and selecting the right metric really depends on your project. For example, one approach is to measure the $\mathrm { F } _ { 1 }$ score for each individual label (or any other binary classifier metric discussed earlier), then simply compute the average score. This code computes the average $\mathrm { F } _ { 1 }$ score across all labels:

```txt
>>> y_train_knn_pred = cross_val_prediction(knn_clf, X_train, yMULTilabel, cv=3)  
>>> f1_score(yMULTilabel, y_train_knn_pred, average="macro")  
0.976410265560605 
```

This assumes that all labels are equally important, which may not be the case. In par‐ ticular, if you have many more pictures of Alice than of Bob or Charlie, you may want to give more weight to the classifier’s score on pictures of Alice. One simple option is to give each label a weight equal to its support (i.e., the number of instances with that target label). To do this, simply set average $^ { 1 } =$ "weighted" in the preceding code.4

# Multioutput Classication

The last type of classification task we are going to discuss here is called multioutputmulticlass classication (or simply multioutput classication). It is simply a generaliza‐ tion of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).

To illustrate this, let’s build a system that removes noise from images. It will take as input a noisy digit image, and it will (hopefully) output a clean digit image, repre‐ sented as an array of pixel intensities, just like the MNIST images. Notice that the classifier’s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput classification system.

![](images/c9a43f5cbfb9455aa8a776540cdecf0b778e74b6e7dd1e77969e7c25fd11550f.jpg)

The line between classification and regression is sometimes blurry, such as in this example. Arguably, predicting pixel intensity is more akin to regression than to classification. Moreover, multioutput systems are not limited to classification tasks; you could even have a system that outputs multiple labels per instance, including both class labels and value labels.

Let’s start by creating the training and test sets by taking the MNIST images and adding noise to their pixel intensities using NumPy’s randint() function. The target images will be the original images:

noise $=$ np.random.randint(0,100，(len(X_train)，784))   
X_train_mod $=$ X_train $^+$ noise   
noise $=$ np.random.randint(0，100，(len(X_test)，784))   
X_test_mod $=$ X_test $^+$ noise   
y_train_mod $=$ X_train   
y_test_mod $=$ X_test

Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so you should be frowning right now):

![](images/46c4d56296ed9d3b8c1e2643bb111a9a4579677e7cf76f9ab6d52a96a571c072.jpg)

On the left is the noisy input image, and on the right is the clean target image. Now let’s train the classifier and make it clean this image:

```python
knn_clf.fit(X_train_mod, y_train_mod)  
clean_digit = knn_clf.predict([X_test_mod[some_index]])  
plot_digit(clean_digit) 
```

![](images/475760bb590f68db0d8fd00a8d772912a3435f52e4ed6b75032436427166e7d1.jpg)

Looks close enough to the target! This concludes our tour of classification. Hopefully you should now know how to select good metrics for classification tasks, pick the appropriate precision/recall tradeoff, compare classifiers, and more generally build good classification systems for a variety of tasks.

# Exercises

1. Try to build a classifier for the MNIST dataset that achieves over $9 7 \%$ accuracy on the test set. Hint: the KNeighborsClassifier works quite well for this task; you just need to find good hyperparameter values (try a grid search on the weights and n_neighbors hyperparameters).   
2. Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel.5 Then, for each image in the training set, create four shif‐

ted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called data augmentation or training set expansion.

3. Tackle the Titanic dataset. A great place to start is on Kaggle.

4. Build a spam classifier (a more challenging exercise):

• Download examples of spam and ham from Apache SpamAssassin’s public datasets.   
• Unzip the datasets and familiarize yourself with the data format.   
• Split the datasets into a training set and a test set.   
• Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector indicating the presence or absence of each possible word. For example, if all emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email “Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1] (meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word.   
• You may want to add hyperparameters to your preparation pipeline to control whether or not to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with “URL,” replace all numbers with “NUMBER,” or even perform stemming (i.e., trim off word endings; there are Python libraries available to do this).   
• Then try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision.

Solutions to these exercises are available in the online Jupyter notebooks at https:// github.com/ageron/handson-ml2.

# Training Models

![](images/2ec9fb6e01a6acc06fd76fdb4a1fe185e7ba8106d558a6c224e9041074c1d5b7.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 4 in the final release of the book.

So far we have treated Machine Learning models and their training algorithms mostly like black boxes. If you went through some of the exercises in the previous chapters, you may have been surprised by how much you can get done without knowing any‐ thing about what’s under the hood: you optimized a regression system, you improved a digit image classifier, and you even built a spam classifier from scratch—all this without knowing how they actually work. Indeed, in many situations you don’t really need to know the implementation details.

However, having a good understanding of how things work can help you quickly home in on the appropriate model, the right training algorithm to use, and a good set of hyperparameters for your task. Understanding what’s under the hood will also help you debug issues and perform error analysis more efficiently. Lastly, most of the top‐ ics discussed in this chapter will be essential in understanding, building, and training neural networks (discussed in Part II of this book).

In this chapter, we will start by looking at the Linear Regression model, one of the simplest models there is. We will discuss two very different ways to train it:

• Using a direct “closed-form” equation that directly computes the model parame‐ ters that best fit the model to the training set (i.e., the model parameters that minimize the cost function over the training set).

• Using an iterative optimization approach, called Gradient Descent (GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method. We will look at a few variants of Gradient Descent that we will use again and again when we study neural networks in Part II: Batch GD, Mini-batch GD, and Stochastic GD.

Next we will look at Polynomial Regression, a more complex model that can fit non‐ linear datasets. Since this model has more parameters than Linear Regression, it is more prone to overfitting the training data, so we will look at how to detect whether or not this is the case, using learning curves, and then we will look at several regulari‐ zation techniques that can reduce the risk of overfitting the training set.

Finally, we will look at two more models that are commonly used for classification tasks: Logistic Regression and Softmax Regression.

![](images/6479226a4b6d40fb7ecf212117b6f945ce8054735c4a363c5adab16f1c03ac29.jpg)

There will be quite a few math equations in this chapter, using basic notions of linear algebra and calculus. To understand these equa‐ tions, you will need to know what vectors and matrices are, how to transpose them, multiply them, and inverse them, and what partial derivatives are. If you are unfamiliar with these concepts, please go through the linear algebra and calculus introductory tutorials avail‐ able as Jupyter notebooks in the online supplemental material. For those who are truly allergic to mathematics, you should still go through this chapter and simply skip the equations; hopefully, the text will be sufficient to help you understand most of the concepts.

# Linear Regression

In Chapter 1, we looked at a simple regression model of life satisfaction: life_satisfac‐ $t i o n = { \theta } _ { \scriptscriptstyle 0 } + { \theta } _ { \scriptscriptstyle 1 } \times \cal { G D P } .$ _per_capita.

This model is just a linear function of the input feature GDP_per_capita. $\theta _ { 0 }$ and $\theta _ { 1 }$ are the model’s parameters.

More generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term), as shown in Equation 4-1.

Equation 4-1. Linear Regression model prediction

$$
\hat {y} = \theta_ {0} + \theta_ {1} x _ {1} + \theta_ {2} x _ {2} + \dots + \theta_ {n} x _ {n}
$$

• ŷ is the predicted value.

• $n$ is the number of features.   
• $x _ { \mathrm { i } }$ is the ith feature value.   
• $\theta _ { j }$ is the jth model parameter (including the bias term $\theta _ { 0 }$ and the feature weights $\theta _ { 1 } , \theta _ { 2 } , \cdots , \theta _ { n } )$ .

This can be written much more concisely using a vectorized form, as shown in Equa‐ tion 4-2.

Equation 4-2. Linear Regression model prediction (vectorized form)

$$
\hat {y} = h _ {\boldsymbol {\theta}} (\mathbf {x}) = \boldsymbol {\theta} \cdot \mathbf {x}
$$

• θ is the model’s parameter vector, containing the bias term $\theta _ { 0 }$ and the feature weights $\theta _ { 1 }$ to $\theta _ { \mathrm { n } }$ .   
• $\mathbf { x }$ is the instance’s feature vector, containing $x _ { 0 }$ to $x _ { n } ,$ with $x _ { 0 }$ always equal to 1.   
• $\textbf { \theta } \cdot \textbf { x }$ is the dot product of the vectors θ and $\mathbf { x } .$ , which is of course equal to $\theta _ { 0 } x _ { 0 } + \theta _ { 1 } x _ { 1 } + \theta _ { 2 } x _ { 2 } + \cdots + \theta _ { n } x _ { n } .$   
• $h _ { \mathfrak { \theta } }$ is the hypothesis function, using the model parameters θ.

![](images/e8ee5dccfc7a7a300430d2e1209c83cdb0f6fac9f405452ef26b29cca024278b.jpg)

In Machine Learning, vectors are often represented as column vec‐ tors, which are 2D arrays with a single column. If θ and $\mathbf { x }$ are col‐ umn vectors, then the prediction is: $\widehat { \boldsymbol { y } } = \boldsymbol { \Theta } ^ { \mathbf { T } } \mathbf { x } ,$ where ${ \boldsymbol { \theta } } ^ { \mathrm { T } }$ is the transpose of θ (a row vector instead of a column vector) and θTx is ${ \boldsymbol { \theta } } ^ { \mathrm { T } } \mathbf { x }$ the matrix multiplication of ${ \boldsymbol { \theta } } ^ { \mathrm { T } }$ and x. It is of course the same pre‐ diction, except it is now represented as a single cell matrix rather than a scalar value. In this book we will use this notation to avoid switching between dot products and matrix multiplications.

Okay, that’s the Linear Regression model, so now how do we train it? Well, recall that training a model means setting its parameters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data. In Chapter 2 we saw that the most common performance measure of a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There‐ fore, to train a Linear Regression model, you need to find the value of θ that minimi‐ zes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE)

than the RMSE, and it leads to the same result (because the value that minimizes a function also minimizes its square root).1

The MSE of a Linear Regression hypothesis $h _ { \mathfrak { \theta } }$ on a training set X is calculated using Equation 4-3.

Equation 4-3. MSE cost function for a Linear Regression model

$$
\operatorname {M S E} \left(\mathbf {X}, h _ {\boldsymbol {\theta}}\right) = \frac {1}{m} \sum_ {i = 1} ^ {m} \left(\boldsymbol {\theta} ^ {T} \mathbf {x} ^ {(i)} - y ^ {(i)}\right) ^ {2}
$$

Most of these notations were presented in Chapter 2 (see “Notations” on page 43). The only difference is that we write $h _ { \mathfrak { \theta } }$ instead of just $h$ in order to make it clear that the model is parametrized by the vector θ. To simplify notations, we will just write MSE(θ) instead of MSE $( \mathbf { X } , h _ { \mathbf { \theta } } )$ .

# The Normal Equation

To find the value of θ that minimizes the cost function, there is a closed-form solution —in other words, a mathematical equation that gives the result directly. This is called the Normal Equation (Equation 4-4).2

Equation 4-4. Normal Equation

$$
\widehat {\boldsymbol {\theta}} = \left(\mathbf {X} ^ {T} \mathbf {X}\right) ^ {- 1} \quad \mathbf {X} ^ {T} \quad \mathbf {y}
$$

• θ is the value of θ that minimizes the cost function.   
• y is the vector of target values containing $y ^ { ( 1 ) }$ to $y ^ { ( m ) }$

Let’s generate some linear-looking data to test this equation on (Figure 4-1):

import numpy as np

$$
\begin{array}{l} X = 2 * \text {n p . r a n d o m . r a n d} (1 0 0, 1) \\ y = 4 + 3 * X + n p. r a n d o m. r a n d n (1 0 0, 1) \\ \end{array}
$$

![](images/0a18157a077da57709f8934106a89815bc311c7799b6099dea14eaf0e3b03fb8.jpg)  
Figure 4-1. Randomly generated linear dataset

Now let’s compute θ using the Normal Equation. We will use the inv() function from NumPy’s Linear Algebra module (np.linalg) to compute the inverse of a matrix, and the dot() method for matrix multiplication:

```python
X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) 
```

The actual function that we used to generate the data is $y = 4 + 3 x _ { \mathrm { { 1 } } } + \mathrm { { G } }$ aussian noise. Let’s see what the equation found:

```txt
>>> theta_best array([[4.21509616], [2.77011339]]) 
```

We would have hoped for $\theta _ { 0 } = 4$ and $\theta _ { 1 } = 3$ instead of $\theta _ { 0 } = 4 . 2 1 5$ and $\theta _ { \mathrm { 1 } } = 2 . 7 7 0$ . Close enough, but the noise made it impossible to recover the exact parameters of the origi‐ nal function.

Now you can make predictions using θ:

```python
>>> X_new = np.array([[0], [2]])
>>> X_new_b = np_c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance
>>> y_prediction = X_new_b.dot(theta_best)
>>> y_prediction
array([[4.21509616],
[9.75532293]]) 
```

Let’s plot this model’s predictions (Figure 4-2):

```txt
plt.plot(X_new, y_prediction, "r-")  
plt.plot(X, y, "b.") 
```

plt.axis([0, 2, 0, 15])

plt.show()

![](images/9cdb038b0edd07491626755f8890576b4ae5a4aee360cb29664bbbf7051e748d.jpg)  
Figure 4-2. Linear Regression model predictions

Performing linear regression using Scikit-Learn is quite simple:3

```python
>>> from sklearn.linear_model import LinearRegression
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X, y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([[4.21509616]], array([[2.77011339]])
>>> lin_reg.predict(X_new)
array([[4.21509616],
[9.75532293]]) 
```

The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:

```txt
>>> theta_best_svd, residuals, rank, s = np.linalg lstsq(X_b, y, rcond=1e-6)
>>> theta_best_svd
array([[4.21509616],
[2.77011339]]) 
```

This function computes $\widehat { \mathbf { \theta } } = \mathbf { X } ^ { + } \mathbf { Y } ;$ where $\mathbf { X } ^ { + }$ is the pseudoinverse of X (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoin‐ verse directly:

```txt
>>> np.linalg.pinv(X_b).dot(y)
array([[4.21509616],
[2.77011339]])
```

The pseudoinverse itself is computed using a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix X into the matrix multiplication of three matrices U Σ VT (see numpy.linalg.svd()). The pseudoinverse is computed as $\mathbf { X } ^ { + } = \mathbf { V } \mathbf { \Sigma } \mathbf { Z } ^ { + } \mathbf { U } ^ { \mathrm { T } }$ . To compute the matrix ${ \boldsymbol { \Sigma } } ^ { + }$ , the algorithm takes $\pmb { \Sigma }$ and sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse, and finally it transposes the resulting matrix. This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix $\mathbf { X } ^ { \mathrm { { T } } } \mathbf { X }$ is not invertible (i.e., singular), such as if $m < n$ or if some features are redundant, but the pseudoinverse is always defined.

# Computational Complexity

The Normal Equation computes the inverse of $\mathbf { X } ^ { T } \mathbf { X } _ { }$ , which is an $( n + 1 ) \times ( n + 1 )$ matrix (where $n$ is the number of features). The computational complexity of inverting such a matrix is typically about $O ( n ^ { 2 . 4 } )$ to $O ( n ^ { 3 } )$ (depending on the implementation). In other words, if you double the number of features, you multiply the computation time by roughly $2 ^ { 2 . 4 } = 5 . 3$ to $2 ^ { 3 } = 8$ .

The SVD approach used by Scikit-Learn’s LinearRegression class is about $O ( n ^ { 2 } )$ . If you double the number of features, you multiply the computation time by roughly 4.

![](images/693f01c063ad44c31cdbdf13b86deeefe5d70cb8ea8d866a3d152ab4461b0a21.jpg)

Both the Normal Equation and the SVD approach get very slow when the number of features grows large (e.g., 100,000). On the positive side, both are linear with regards to the number of instan‐ ces in the training set (they are $O ( m ) $ ), so they handle large training sets efficiently, provided they can fit in memory.

Also, once you have trained your Linear Regression model (using the Normal Equa‐ tion or any other algorithm), predictions are very fast: the computational complexity is linear with regards to both the number of instances you want to make predictions on and the number of features. In other words, making predictions on twice as many instances (or twice as many features) will just take roughly twice as much time.

Now we will look at very different ways to train a Linear Regression model, better suited for cases where there are a large number of features, or too many training instances to fit in memory.

# Gradient Descent

Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.

Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope. This is exactly what Gradient Descent does: it measures the local gradient of the error function with regards to the parameter vector θ, and it goes in the direction of descending gradient. Once the gra‐ dient is zero, you have reached a minimum!

Concretely, you start by filling θ with random values (this is called random initializa‐ tion), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum (see Figure 4-3).

![](images/7cdffc18f85567c2b773779bf9a625477f768e4eae6e1c019874df0b88da4cca.jpg)  
Figure 4-3. Gradient Descent

An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time (see Figure 4-4).

![](images/cfbd423f31c31d0dc6f957c065931bc7683cab4e79ab2fd4932d55234259d2c0.jpg)  
Figure 4-4. Learning rate too small

On the other hand, if the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution (see Figure 4-5).

![](images/e6a31daa5f7e9159351658479fcd2a020d6785512ebe0dfc9b17df2707ec8573.jpg)  
Figure 4-5. Learning rate too large

Finally, not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum very difficult. Figure 4-6 shows the two main challenges with Gradient Descent: if the ran‐ dom initialization starts the algorithm on the left, then it will converge to a local mini‐ mum, which is not as good as the global minimum. If it starts on the right, then it will take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.

![](images/a10d94143f451a09991db324ec4e09b857c1a192dd50ba6d6e4b2bd6897782a1.jpg)  
Cost   
Figure 4-6. Gradient Descent pitfalls

Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly.4 These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high).

In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales. Figure 4-7 shows Gradient Descent on a train‐ ing set where features 1 and 2 have the same scale (on the left), and on a training set where feature 1 has much smaller values than feature 2 (on the right).5

![](images/1e0d5ae679ccfd04d0a14ea98e22c3be71fdecf4170e035eafbfb09d0b47e13a.jpg)  
Figure 4-7. Gradient Descent with and without feature scaling

As you can see, on the left the Gradient Descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right it first goes in a direction almost orthogonal to the direction of the global minimum, and it ends with a long march down an almost flat valley. It will eventually reach the minimum, but it will take a long time.

![](images/3527136299e4cc9c4485a6a94b43f58b8720d66677be1a26d97675a702aef169.jpg)

When using Gradient Descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.

This diagram also illustrates the fact that training a model means searching for a combination of model parameters that minimizes a cost function (over the training set). It is a search in the model’s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is: searching for a nee‐ dle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐ nately, since the cost function is convex in the case of Linear Regression, the needle is simply at the bottom of the bowl.

# Batch Gradient Descent

To implement Gradient Descent, you need to compute the gradient of the cost func‐ tion with regards to each model parameter $\theta _ { j }$ . In other words, you need to calculate how much the cost function will change if you change $\theta _ { j }$ just a little bit. This is called a partial derivative. It is like asking “what is the slope of the mountain under my feet if I face east?” and then asking the same question facing north (and so on for all other dimensions, if you can imagine a universe with more than three dimensions). Equa‐ tion 4-5 computes the partial derivative of the cost function with regards to parame‐ ter $\theta _ { j } ,$ noted $\frac { \partial } { \partial \theta _ { j } } \operatorname { M S E } ( \pmb { \theta } )$ .

Equation 4-5. Partial derivatives of the cost function

$$
\frac {\partial}{\partial \theta_ {j}} \mathrm {M S E} (\pmb {\theta}) = \frac {2}{m} \sum_ {i = 1} ^ {m} \left(\pmb {\theta} ^ {T} \mathbf {x} ^ {(i)} - y ^ {(i)}\right) x _ {j} ^ {(i)}
$$

Instead of computing these partial derivatives individually, you can use Equation 4-6 to compute them all in one go. The gradient vector, noted $\nabla _ { \boldsymbol { \Theta } } \mathbf { M S E } ( \boldsymbol { \Theta } )$ , contains all the partial derivatives of the cost function (one for each model parameter).

Equation 4-6. Gradient vector of the cost function

$$
\nabla_ {\boldsymbol {\theta}} \operatorname {M S E} (\boldsymbol {\theta}) = \left( \begin{array}{c} \frac {\partial}{\partial \theta_ {0}} \operatorname {M S E} (\boldsymbol {\theta}) \\ \frac {\partial}{\partial \theta_ {1}} \operatorname {M S E} (\boldsymbol {\theta}) \\ \vdots \\ \frac {\partial}{\partial \theta_ {n}} \operatorname {M S E} (\boldsymbol {\theta}) \end{array} \right) = \frac {2}{m} \mathbf {X} ^ {T} (\mathbf {X} \boldsymbol {\theta} - \mathbf {y})
$$

![](images/0ffa19acc9e8989d8c6f4e8f2c3e02708ffec9299533d2c525e544df9872db73.jpg)

Notice that this formula involves calculations over the full training set X, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step (actually, Full Gradient Descent would probably be a better name). As a result it is terribly slow on very large train‐ ing sets (but we will see much faster Gradient Descent algorithms shortly). However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hun‐ dreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition.

Once you have the gradient vector, which points uphill, just go in the opposite direc‐ tion to go downhill. This means subtracting $\mathrm { V _ { \pmb { \Theta } } M S E } ( \pmb { \Theta } )$ from θ. This is where the learning rate $\eta$ comes into play:6 multiply the gradient vector by $\eta$ to determine the size of the downhill step (Equation 4-7).

Equation 4-7. Gradient Descent step

$$
\boldsymbol {\theta} ^ {(\text {n e x t s t e p})} = \boldsymbol {\theta} - \eta \nabla_ {\boldsymbol {\theta}} \operatorname {M S E} (\boldsymbol {\theta})
$$

Let’s look at a quick implementation of this algorithm:

eta $= 0.1$ # learning rate   
n_iterations $= 1000$ m $= 100$ theta $=$ np.random.randint(2,1) # random initialization   
for iteration in range(n_iterations): gradients $= 2 / m^{*}$ X_b.T.dot(X_b.dot(theta)-y) theta $=$ theta - eta \* gradients

That wasn’t too hard! Let’s look at the resulting theta:

```txt
>>> theta  
array([[4.21509616], [2.77011339]]) 
```

Hey, that’s exactly what the Normal Equation found! Gradient Descent worked per‐ fectly. But what if you had used a different learning rate eta? Figure 4-8 shows the first 10 steps of Gradient Descent using three different learning rates (the dashed line represents the starting point).

![](images/6639004ce8955519bb4c48f0b891edee197f89ab84cee6a7a5e341e2ae8fb246.jpg)  
Figure 4-8. Gradient Descent with various learning rates

On the left, the learning rate is too low: the algorithm will eventually reach the solu‐ tion, but it will take a long time. In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution. On the right, the learn‐ ing rate is too high: the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step.

To find a good learning rate, you can use grid search (see Chapter 2). However, you may want to limit the number of iterations so that grid search can eliminate models that take too long to converge.

You may wonder how to set the number of iterations. If it is too low, you will still be far away from the optimal solution when the algorithm stops, but if it is too high, you will waste time while the model parameters do not change anymore. A simple solu‐ tion is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny number $\epsilon$ (called the tolerance)—because this happens when Gradient Descent has (almost) reached the minimum.

# Convergence Rate

When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but you may have to wait a while: it can take $\mathrm { O } ( 1 / \epsilon )$ iterations to reach the optimum within a range of $\epsilon$ depending on the shape of the cost function. If you divide the tolerance by 10 to have a more precise solution, then the algorithm may have to run about 10 times longer.

# Stochastic Gradient Descent

The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (SGD can be implemented as an out-of-core algorithm.7)

On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on aver‐ age. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down (see Figure 4-9). So once the algo‐ rithm stops, the final parameter values are good, but not optimal.

![](images/888ce50b441b05eb6a0bbef3af2e095fcc1af98f32e574dba5b69b069ed5c8d7.jpg)  
Figure 4-9. Stochastic Gradient Descent

When the cost function is very irregular (as in Figure 4-6), this can actually help the algorithm jump out of local minima, so Stochastic Gradient Descent has a better chance of finding the global minimum than Batch Gradient Descent does.

Therefore randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. This process is akin to simulated anneal‐ ing, an algorithm inspired from the process of annealing in metallurgy where molten metal is slowly cooled down. The function that determines the learning rate at each iteration is called the learning schedule. If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early.

This code implements Stochastic Gradient Descent using a simple learning schedule:

```python
n_epochs = 50  
t0, t1 = 5, 50 # learning schedule hyperparameters  
def learning_schedule(t):  
    return t0 / (t + t1)  
theta = np.random.randint(2,1) # random initialization  
for epoch in range(n_epochs):  
    for i in range(m):  
        random_index = np.random.randint(m)  
        xi = X_b[random_index:random_index+1]  
        yi = y[random_index:random_index+1]  
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)  
        eta = learning_schedule(epoch * m + i)  
        theta = theta - eta * gradients 
```

By convention we iterate by rounds of m iterations; each round is called an epoch. While the Batch Gradient Descent code iterated 1,000 times through the whole train‐ ing set, this code goes through the training set only 50 times and reaches a fairly good solution:

```txt
>>> theta  
array([[4.21076011], [2.74856079]]) 
```

Figure 4-10 shows the first 20 steps of training (notice how irregular the steps are).

![](images/916561a79cdde258ead7c3c1baa38093921a3dc2208a2c07d79990ed3eaaa8ec.jpg)  
Figure 4-10. Stochastic Gradient Descent rst 20 steps

Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set (making sure to shuffle the input features and the labels jointly), then go through it instance by instance, then shuffle it again, and so on. However, this gen‐ erally converges more slowly.

![](images/216341fe7a10adfe3793d730ff1b9e369f45eb8438f9a8b84bec0d6b6dde6ce5.jpg)

When using Stochastic Gradient Descent, the training instances must be independent and identically distributed (IID), to ensure that the parameters get pulled towards the global optimum, on average. A simple way to ensure this is to shuffle the instances dur‐ ing training (e.g., pick each instance randomly, or shuffle the train‐ ing set at the beginning of each epoch). If you do not do this, for example if the instances are sorted by label, then SGD will start by optimizing for one label, then the next, and so on, and it will not settle close to the global minimum.

To perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe gressor class, which defaults to optimizing the squared error cost function. The fol‐ lowing code runs for maximum 1000 epochs (max_iter=1000) or until the loss drops by less than 1e-3 during one epoch $( \mathsf { t o l } = 1 \mathsf { e } - 3 )$ ), starting with a learning rate of 0.1 $( { \tt e t a } { \tt e } = { \tt 0 } . 1 )$ ), using the default learning schedule (different from the preceding one), and it does not use any regularization (penalty=None; more details on this shortly):

from sklearn.linear_model import SGDRegressor sgd_reg $=$ SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1) sgd_reg.fit(X, y.ravel())

Once again, you find a solution quite close to the one returned by the Normal Equa‐ tion:

>>> sgd_reg.intercept_, sgd_reg.coef_ (array([4.24365286]), array([2.8250878]))

# Mini-batch Gradient Descent

The last Gradient Descent algorithm we will look at is called Mini-batch Gradient Descent. It is quite simple to understand once you know Batch and Stochastic Gradi‐ ent Descent: at each step, instead of computing the gradients based on the full train‐ ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Minibatch GD computes the gradients on small random sets of instances called minibatches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.

The algorithm’s progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression as we saw earlier). Figure 4-11 shows the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐ tic GD and Mini-batch GD would also reach the minimum if you used a good learn‐ ing schedule.

![](images/c4641901538859c528e598f2180641a16182b084cfc15596236194de99fbcf81.jpg)  
Figure 4-11. Gradient Descent paths in parameter space

Let’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that m is the number of training instances and $n$ is the number of features); see Table 4-1.

Table 4-1. Comparison of algorithms for Linear Regression   

<table><tr><td>Algorithm</td><td>Large m</td><td>Out-of-core support</td><td>Large n</td><td>Hyperparams</td><td>Scaling required</td><td>Scikit-Learn</td></tr><tr><td>Normal Equation</td><td>Fast</td><td>No</td><td>Slow</td><td>0</td><td>No</td><td>n/a</td></tr><tr><td>SVD</td><td>Fast</td><td>No</td><td>Slow</td><td>0</td><td>No</td><td>LinearRegression</td></tr><tr><td>Batch GD</td><td>Slow</td><td>No</td><td>Fast</td><td>2</td><td>Yes</td><td>SGDRegressor</td></tr><tr><td>Stochastic GD</td><td>Fast</td><td>Yes</td><td>Fast</td><td>≥2</td><td>Yes</td><td>SGDRegressor</td></tr><tr><td>Mini-batch GD</td><td>Fast</td><td>Yes</td><td>Fast</td><td>≥2</td><td>Yes</td><td>SGDRegressor</td></tr></table>

![](images/c7e89b533102fd5c39ab26acda54f15db12f33748ba015f1a6807a4f7aefbb82.jpg)

There is almost no difference after training: all these algorithms end up with very similar models and make predictions in exactly the same way.

# Polynomial Regression

What if your data is actually more complex than a simple straight line? Surprisingly, you can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.

Let’s look at an example. First, let’s generate some nonlinear data, based on a simple quadratic equation9 (plus some noise; see Figure 4-12):

$\mathsf{m} = 100$ $X = 6^{*}$ np.random.randint(m,1)-3   
y $= 0.5^{*}$ X\*\*2+X+2+np.random.randint(m，1)

![](images/a259449347dcff818c92f4916bae8dd94d4ebea963fd41bb00f6fde4cfc8ade1.jpg)  
Figure 4-12. Generated nonlinear and noisy dataset

Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s Poly nomialFeatures class to transform our training data, adding the square ( $2 ^ { \mathrm { n d } }$ -degree polynomial) of each feature in the training set as new features (in this case there is just one feature):

```python
>>> from sklearn preprocess import PolynomialFeatures
>>> poly_features = PolynomialFeatures(degree=2, include.bias=False)
>>> Xpoly = poly_features.fit_transform(X)
>>> X[0]
array([-0.75275929])
>>> X_poly[0]
array([-0.75275929, 0.56664654]) 
```

X_poly now contains the original feature of X plus the square of this feature. Now you can fit a LinearRegression model to this extended training data (Figure 4-13):

```txt
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X.poly, y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([1.78134581]), array([[0.93366893, 0.56456263]])) 
```

![](images/5cf9cfafdfd9e15d7d9d3f642348f83fad092e34a5ccef7af420b7bc0f9eb7e9.jpg)  
Figure 4-13. Polynomial Regression model predictions

Not bad: the model estimates $\widehat { y } = 0 . 5 6 x _ { 1 } ^ { 2 } + 0 . 9 3 x _ { 1 } + 1 . 7 8$ when in fact the original function was $y = 0 . 5 x _ { 1 } ^ { 2 } + 1 . 0 x _ { 1 } + 2 . 0 + \mathrm { G }$ aussian noise.

Note that when there are multiple features, Polynomial Regression is capable of find‐ ing relationships between features (which is something a plain Linear Regression model cannot do). This is made possible by the fact that PolynomialFeatures also adds all combinations of features up to the given degree. For example, if there were two features $^ a$ and $b$ , PolynomialFeatures with degree $^ { - 3 }$ would not only add the features $a ^ { 2 } , a ^ { 3 } , b ^ { 2 }$ , and $b ^ { 3 }$ , but also the combinations ab, $a ^ { 2 } b$ , and $a b ^ { 2 }$ .

![](images/518f82861bea5d3638e4de10998ae8900800fa4e61cc5e32bbb2cf80aabd4ba2.jpg)

PolynomialFeatures(degree=d) transforms an array containing n features into an array containing $\frac { ( n + d ) ! } { d ! n ! }$ features, where $n !$ is the factorial of $n$ , equal to $1 \times 2 \times 3 \times \cdots \times n$ . Beware of the combinato‐ rial explosion of the number of features!

# Learning Curves

If you perform high-degree Polynomial Regression, you will likely fit the training data much better than with plain Linear Regression. For example, Figure 4-14 applies a 300-degree polynomial model to the preceding training data, and compares the result with a pure linear model and a quadratic model ( $2 ^ { \mathrm { n d } }$ -degree polynomial). Notice how the 300-degree polynomial model wiggles around to get as close as possi‐ ble to the training instances.

![](images/7161db6a509385f5b402a5e244b1cc56db1603871ea8ce0555b6cc700ef25c08.jpg)  
Figure 4-14. High-degree Polynomial Regression

Of course, this high-degree Polynomial Regression model is severely overfitting the training data, while the linear model is underfitting it. The model that will generalize best in this case is the quadratic model. It makes sense since the data was generated using a quadratic model, but in general you won’t know what function generated the data, so how can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data?

In Chapter 2 you used cross-validation to get an estimate of a model’s generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it per‐ forms poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.

Another way is to look at the learning curves: these are plots of the model’s perfor‐ mance on the training set and the validation set as a function of the training set size (or the training iteration). To generate the plots, simply train the model several times on different sized subsets of the training set. The following code defines a function that plots the learning curves of a model given some training data:

from sklearn.metrics import mean_squared_error   
from sklearn.model_selection import train_test_split   
def plot_learning-curves(model, X, y): X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) train Errors, val errors $=$ [], [] for m in range(1, len(X_train)): model.fit(X_train[:m], y_train[:m]) y_train_prediction $=$ model.predict(X_train[:m])

```python
y_val_prediction = model.predict(X_val)  
train Errors.append(mean_squared_error(y_train[:m], y_train_prediction))  
val errors.append(mean_squared_error(y_val, y_val_prediction))  
plt.plot(np.sqrt(trainErrors), "r-+", linewidth=2, label="train")  
plt.plot(np.sqrt(valErrors), "b-", linewidth=3, label="val") 
```

Let’s look at the learning curves of the plain Linear Regression model (a straight line; Figure 4-15):

```python
lin_reg = LinearRegression() plot_learning_curves(lin_reg, X, y) 
```

![](images/b4c7f201a5bfb59505ed03d2850b6b135cc0ce29cd42102137696aef68af0edb.jpg)  
Figure 4-15. Learning curves

This deserves a bit of explanation. First, let’s look at the performance on the training data: when there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the training data per‐ fectly, both because the data is noisy and because it is not linear at all. So the error on the training data goes up until it reaches a plateau, at which point adding new instan‐ ces to the training set doesn’t make the average error much better or worse. Now let’s look at the performance of the model on the validation data. When the model is trained on very few training instances, it is incapable of generalizing properly, which is why the validation error is initially quite big. Then as the model is shown more training examples, it learns and thus the validation error slowly goes down. However, once again a straight line cannot do a good job modeling the data, so the error ends up at a plateau, very close to the other curve.

These learning curves are typical of an underfitting model. Both curves have reached a plateau; they are close and fairly high.

Now let’s look at the learning curves of a $1 0 ^ { \mathrm { t h } }$ -degree polynomial model on the same data (Figure 4-16):

from sklearnpipeline import Pipeline   
polynomial_regression $=$ Pipeline([ ("poly_features", PolynomialFeatures(degree $\coloneqq$ 10, include.bias $\coloneqq$ False)), ("lin_reg", LinearRegression(), ])   
plot_learning_curves(polynomial_regression, X, y)

These learning curves look a bit like the previous ones, but there are two very impor‐ tant differences:

• The error on the training data is much lower than with the Linear Regression model.   
• There is a gap between the curves. This means that the model performs signifi‐ cantly better on the training data than on the validation data, which is the hall‐ mark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer.

![](images/d6f45accfa513ce4e718cdd37dc23ccabae90cde114419bd03a19e871588f7d3.jpg)  
Figure 4-16. Learning curves for the polynomial model

# The Bias/Variance Tradeo

An important theoretical result of statistics and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of three very different errors:

# Bias

This part of the generalization error is due to wrong assumptions, such as assum‐ ing that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.10

# Variance

This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree pol‐ ynomial model) is likely to have high variance, and thus to overfit the training data.

# Irreducible error

This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).

Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a tradeoff.

# Regularized Linear Models

As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees.

For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights.

# Ridge Regression

Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin‐ ear Regression: a regularization term equal to $\alpha { \Sigma } _ { i = 1 } ^ { n } { \theta } _ { i } ^ { 2 }$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.

![](images/0c39db444da8799c5959022451cd729faac1df6173e358b46ac118fe12de3778.jpg)

It is quite common for the cost function used during training to be different from the performance measure used for testing. Apart from regularization, another reason why they might be different is that a good training cost function should have optimizationfriendly derivatives, while the performance measure used for test‐ ing should be as close as possible to the final objective. A good example of this is a classifier trained using a cost function such as the log loss (discussed in a moment) but evaluated using precision/ recall.

The hyperparameter α controls how much you want to regularize the model. If $\alpha = 0$ then Ridge Regression is just Linear Regression. If $\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean. Equa‐ tion 4-8 presents the Ridge Regression cost function.11

Equation 4-8. Ridge Regression cost function

$$
J (\pmb {\theta}) = \mathrm {M S E} (\pmb {\theta}) + \alpha \frac {1}{2} \sum_ {i = 1} ^ {n} \theta_ {i} ^ {2}
$$

Note that the bias term $\theta _ { 0 }$ is not regularized (the sum starts at $i = 1$ , not 0). If we define w as the vector of feature weights ( $\left( \theta _ { \mathrm { { 1 } } } \right.$ to $\theta _ { n } )$ , then the regularization term is simply equal to $\sqrt [ 1 ] { 2 } ( \Vert \textbf { w } \Vert _ { 2 } ) ^ { 2 }$ , where ${ \left\| \textbf { w } \right\| } _ { 2 }$ represents the $\ell _ { 2 }$ norm of the weight vector.12 For Gradient Descent, just add αw to the MSE gradient vector (Equation 4-6).

![](images/2f9eb17ad01823e29fcbb993bcf600e0ceed82913114695f677e487f7fb4e07e.jpg)

It is important to scale the data (e.g., using a StandardScaler) before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.

Figure 4-17 shows several Ridge models trained on some linear data using different $\alpha$ value. On the left, plain Ridge models are used, leading to linear predictions. On the right, the data is first expanded using PolynomialFeatures(degree=10), then it is scaled using a StandardScaler, and finally the Ridge models are applied to the result‐ ing features: this is Polynomial Regression with Ridge regularization. Note how increasing $\alpha$ leads to flatter (i.e., less extreme, more reasonable) predictions; this reduces the model’s variance but increases its bias.

As with Linear Regression, we can perform Ridge Regression either by computing a closed-form equation or by performing Gradient Descent. The pros and cons are the same. Equation 4-9 shows the closed-form solution (where A is the $( n + 1 ) \times ( n + 1 )$ identity matrix13 except with a 0 in the top-left cell, corresponding to the bias term).

![](images/6d46f50d155dea08314c6654733f5b49fbae7d5a050a9ea8ff414d298dad1e12.jpg)  
Figure 4-17. Ridge Regression

Equation 4-9. Ridge Regression closed-form solution

$$
\widehat {\boldsymbol {\theta}} = \left(\mathbf {X} ^ {T} \mathbf {X} + \alpha \mathbf {A}\right) ^ {- 1} \quad \mathbf {X} ^ {T} \quad \mathbf {y}
$$

Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐ tion (a variant of Equation 4-9 using a matrix factorization technique by André-Louis Cholesky):

```python
>>> from sklearn.linear_model import Ridge
>>> ridge_reg = Ridge(alpha=1, solver="cholesky")
>>> ridge_reg.fit(X, y) 
```

```txt
>>> ridge_reg.predict([[1.5]])
array([[1.55071465]]) 
```

And using Stochastic Gradient Descent:14

```python
>>> sgd_reg = SGDRegressor(penalty='l2')
>>> sgd_reg.fit(X, y.ravel())
>>> sgd_reg.predict([[1.5]])
array([1.47012588]) 
```

The penalty hyperparameter sets the type of regularization term to use. Specifying $" 1 2 "$ indicates that you want SGD to add a regularization term to the cost function equal to half the square of the $\ell _ { 2 }$ norm of the weight vector: this is simply Ridge Regression.

# Lasso Regression

Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the $\ell _ { 1 }$ norm of the weight vector instead of half the square of the $\ell _ { 2 }$ norm (see Equation 4-10).

Equation 4-10. Lasso Regression cost function

$$
J (\boldsymbol {\theta}) = \operatorname {M S E} (\boldsymbol {\theta}) + \alpha \Sigma_ {i = 1} ^ {n} \left| \theta_ {i} \right|
$$

Figure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models with Lasso models and uses smaller $\alpha$ values.

![](images/f493e15b8e295fc356303f89559fb536c6c43551f4191e5f61904244fb9d5bbf.jpg)  
Figure 4-18. Lasso Regression

An important characteristic of Lasso Regression is that it tends to completely elimi‐ nate the weights of the least important features (i.e., set them to zero). For example, the dashed line in the right plot on Figure 4-18 (with $\alpha = 1 0 ^ { - 7 }$ ) looks quadratic, almost linear: all the weights for the high-degree polynomial features are equal to zero. In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).

You can get a sense of why this is the case by looking at Figure 4-19: on the top-left plot, the background contours (ellipses) represent an unregularized MSE cost func‐ tion $( \alpha = 0 )$ , and the white circles show the Batch Gradient Descent path with that cost function. The foreground contours (diamonds) represent the $\ell _ { 1 }$ penalty, and the triangles show the BGD path for this penalty only $( \alpha \to \infty )$ ). Notice how the path first reaches $\theta _ { \mathrm { 1 } } = 0$ , then rolls down a gutter until it reaches $\theta _ { 2 } = 0$ . On the top-right plot, the contours represent the same cost function plus an $\ell _ { 1 }$ penalty with $\alpha = 0 . 5$ . The global minimum is on the $\theta _ { 2 } = 0$ axis. BGD first reaches $\theta _ { 2 } = 0$ , then rolls down the gutter until it reaches the global minimum. The two bottom plots show the same thing but uses an $\ell _ { 2 }$ penalty instead. The regularized minimum is closer to $\mathbf { \boldsymbol { \mathsf { \mathbf { \theta } } } } = \mathbf { \boldsymbol { \mathsf { \mathbf { 0 } } } }$ than the unregularized minimum, but the weights do not get fully eliminated.

![](images/b758d3010cf6d1ff81976798a46fcdccb20bccca4977e843f50f63dc88eb7131.jpg)

![](images/a7a6bb45ee7a88b4b7f84d9a0d8721f4650896dc3e6694b9e8c4e113f42a0a26.jpg)

![](images/402bae4a69bf112584bd0c6258dd90db8b49dababbaea3b3bd54cd181363beb8.jpg)

![](images/7ff8bb9f8ad657a3d6169a926c0ffe781d1a2f8cb11bcb7266d69af5e1744ab0.jpg)

![](images/2c8961f485b0ed711bdb8106a5125865cc9100f7cda0a4595a1ba36bfdfeb571.jpg)  
Figure 4-19. Lasso versus Ridge regularization

On the Lasso cost function, the BGD path tends to bounce across the gutter toward the end. This is because the slope changes abruptly at $\theta _ { 2 } = 0$ . You need to gradually reduce the learning rate in order to actually converge to the global minimum.

The Lasso cost function is not differentiable at $\theta _ { i } = 0$ (for $i = 1 , 2 , \cdots , n )$ , but Gradient Descent still works fine if you use a subgradient vector $\mathbf { g } ^ { 1 5 }$ instead when any $\theta _ { i } = 0$ . Equation 4-11 shows a subgradient vector equation you can use for Gradient Descent with the Lasso cost function.

Equation 4-11. Lasso Regression subgradient vector

$$
g (\boldsymbol {\theta}, J) = \nabla_ {\boldsymbol {\theta}} \mathrm {M S E} (\boldsymbol {\theta}) + \alpha \left( \begin{array}{c} \mathrm {s i g n} \left(\theta_ {1}\right) \\ \mathrm {s i g n} \left(\theta_ {2}\right) \\ \vdots \\ \mathrm {s i g n} \left(\theta_ {n}\right) \end{array} \right) \quad \text {w h e r e} \mathrm {s i g n} \left(\theta_ {i}\right) = \left\{ \begin{array}{l l} - 1 & \text {i f} \theta_ {i} <   0 \\ 0 & \text {i f} \theta_ {i} = 0 \\ + 1 & \text {i f} \theta_ {i} > 0 \end{array} \right.
$$

Here is a small Scikit-Learn example using the Lasso class. Note that you could instead use an SGDRegressor(penalty="l1").

```python
>>> from sklearn.linear_model import Lasso
>>> lasso_reg = Lasso(alpha=0.1)
>>> lasso_reg.fit(X, y)
>>> lasso_reg.predict([[1.5]])
array([1.53788174]) 
```

# Elastic Net

Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When $r = 0$ , Elastic Net is equivalent to Ridge Regression, and when $r = 1$ , it is equivalent to Lasso Regression (see Equation 4-12).

Equation 4-12. Elastic Net cost function

$$
J (\boldsymbol {\theta}) = \operatorname {M S E} (\boldsymbol {\theta}) + r \alpha \Sigma_ {i = 1} ^ {n} \left| \theta_ {i} \right| + \frac {1 - r}{2} \alpha \Sigma_ {i = 1} ^ {n} \theta_ {i} ^ {2}
$$

So when should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should pre‐ fer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.

Here is a short example using Scikit-Learn’s ElasticNet (l1_ratio corresponds to the mix ratio r):

```python
>>> from sklearn.linear_model import ElasticNet
>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
>>> elastic_net.fit(X, y)
>>> elastic_net.predict([[1.5]])
array([1.54333232])
```

# Early Stopping

A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Figure 4-20 shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does its prediction error on the validation set. However,

after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stop‐ ping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch.”

![](images/caa238f645c1c93e22d76254b6478b0983e6b5dcd9a6b732db9ab6fc615eda24.jpg)  
Figure 4-20. Early stopping regularization

![](images/3655d6a8c8934d6117ef462aa8e4402d08bf7e530210d0424c9b732f16b8015d.jpg)

With Stochastic and Mini-batch Gradient Descent, the curves are not so smooth, and it may be hard to know whether you have reached the minimum or not. One solution is to stop only after the validation error has been above the minimum for some time (when you are confident that the model will not do any better), then roll back the model parameters to the point where the validation error was at a minimum.

Here is a basic implementation of early stopping:

from sklearn.base import clone   
# prepare the data   
polyScaler $=$ Pipeline([ ("poly_features", PolynomialFeatures(degree $= 90$ ,include.bias $\equiv$ False)), ("stdScaler",StandardScaler()) ])   
X_train(polyScaled $=$ polyScaler.fit_transform(X_train)   
X_val(polyScaled $=$ polyScaler.transform(X_val)   
sgd_reg $=$ SGDRegressor(max_iter $= 1$ ,tol=-np.infty,warm_start $\equiv$ True, penalty $\equiv$ None, learning_rate $\equiv$ "constant", eta0=0.0005)

```python
minimum_val_error = float("inf")
best_epoch = None
best_model = None
for epoch in range(1000):
    sgd_reg.fit(X_train(polyScaled, y_train) # continues where it left off
    y_val_prediction = sgd_reg.predict(X_val(polyScaled))
    val_error = mean_squared_error(y_val, y_val_prediction)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch = epoch
        best_model = clone(sgd_reg) 
```

Note that with warm_start=True, when the fit() method is called, it just continues training where it left off instead of restarting from scratch.

# Logistic Regression

As we discussed in Chapter 1, some regression algorithms can be used for classifica‐ tion as well (and vice versa). Logistic Regression (also called Logit Regression) is com‐ monly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than $5 0 \%$ , then the model predicts that the instance belongs to that class (called the positive class, labeled $^ { \mathfrak { c } _ { 1 } \mathfrak { n } }$ ), or else it predicts that it does not (i.e., it belongs to the negative class, labeled $^ { \mathfrak { c } } 0 ^ { \mathfrak { n } }$ ). This makes it a binary classifier.

# Estimating Probabilities

So how does it work? Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the logistic of this result (see Equation 4-13).

Equation 4-13. Logistic Regression model estimated probability (vectorized form)

$$
\hat {\boldsymbol {p}} = h _ {\boldsymbol {\theta}} (\mathbf {x}) = \sigma \left(\mathbf {x} ^ {T} \boldsymbol {\theta}\right)
$$

The logistic—noted $\sigma ( \cdot )$ —is a sigmoid function (i.e., S-shaped) that outputs a number between 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21.

Equation 4-14. Logistic function

$$
\sigma (t) = \frac {1}{1 + \exp (- t)}
$$

![](images/1a1eda903d09f54aa722c514e8dcfda88f7fa6be772436fc8f08e85db74c0692.jpg)  
Figure 4-21. Logistic function

Once the Logistic Regression model has estimated the probability $\hat { p } = h _ { \boldsymbol { \theta } } ( \mathbf { x } )$ that an instance x belongs to the positive class, it can make its prediction $\hat { y }$ easily (see Equa‐ tion 4-15).

Equation 4-15. Logistic Regression model prediction

$$
\hat {y} = \left\{ \begin{array}{l l} 0 & \text {i f} \hat {p} <   0. 5 \\ 1 & \text {i f} \hat {p} \geq 0. 5 \end{array} \right.
$$

Notice that $\sigma ( t ) < 0 . 5$ when $t < 0$ , and $\sigma ( t ) \geq 0 . 5$ when $t \geq 0$ , so a Logistic Regression model predicts 1 if $\mathbf { x } ^ { T } \mathbf { \boldsymbol { \theta } }$ is positive, and 0 if it is negative.

![](images/ff5ca1664613c11891311c21fad1b48faa9a23743e9d51a6137af9c4a8d6723c.jpg)

The score $t$ is often called the logit: this name comes from the fact that the logit function, defined as $\mathrm { l o g i t } ( p ) = \log ( p / ( 1 - p ) )$ , is the inverse of the logistic function. Indeed, if you compute the logit of the estimated probability $\boldsymbol { p }$ , you will find that the result is t. The logit is also called the log-odds, since it is the log of the ratio between the estimated probability for the positive class and the estimated probability for the negative class.

# Training and Cost Function

Good, now you know how a Logistic Regression model estimates probabilities and makes predictions. But how is it trained? The objective of training is to set the param‐ eter vector θ so that the model estimates high probabilities for positive instances $( \gamma =$ 1) and low probabilities for negative instances $( \boldsymbol { y } = \boldsymbol { 0 } )$ ). This idea is captured by the cost function shown in Equation 4-16 for a single training instance x.

Equation 4-16. Cost function of a single training instance

$$
c (\boldsymbol {\theta}) = \left\{ \begin{array}{l l} - \log (\hat {p}) & \text {i f} y = 1 \\ - \log (1 - \hat {p}) & \text {i f} y = 0 \end{array} \right.
$$

This cost function makes sense because – $\log ( t )$ grows very large when $t$ approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, and it will also be very large if the model estimates a probability close to 1 for a negative instance. On the other hand, $- \log ( t )$ is close to 0 when $t$ is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.

The cost function over the whole training set is simply the average cost over all train‐ ing instances. It can be written in a single expression (as you can verify easily), called the log loss, shown in Equation 4-17.

Equation 4-17. Logistic Regression cost function (log loss)

$$
J (\pmb {\theta}) = - \frac {1}{m} \Sigma_ {i = 1} ^ {m} \left[ y ^ {(i)} l o g \Bigl (\hat {p} ^ {(i)} \Bigr) + \Bigl (1 - y ^ {(i)} \Bigr) l o g \Bigl (1 - \hat {p} ^ {(i)} \Bigr) \right]
$$

The bad news is that there is no known closed-form equation to compute the value of θ that minimizes this cost function (there is no equivalent of the Normal Equation). But the good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learn‐ ing rate is not too large and you wait long enough). The partial derivatives of the cost function with regards to the jth model parameter $\theta _ { j }$ is given by Equation 4-18.

Equation 4-18. Logistic cost function partial derivatives

$$
\frac {\partial}{\partial \theta_ {j}} \mathrm {J} (\pmb {\theta}) = \frac {1}{m} \sum_ {i = 1} ^ {m} \left(\sigma \left(\pmb {\theta} ^ {T} \mathbf {x} ^ {(i)}\right) - y ^ {(i)}\right) x _ {j} ^ {(i)}
$$

This equation looks very much like Equation 4-5: for each instance it computes the prediction error and multiplies it by the jth feature value, and then it computes the average over all training instances. Once you have the gradient vector containing all the partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s it: you now know how to train a Logistic Regression model. For Stochastic GD you would of course just take one instance at a time, and for Mini-batch GD you would use a mini-batch at a time.

# Decision Boundaries

Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22).

![](images/61c0bd43a668229083c3997978b150cd4b36fd173a45f3612324d5d1fe76ab6c.jpg)  
Figure 4-22. Flowers of three iris plant species16

Let’s try to build a classifier to detect the Iris-Virginica type based only on the petal width feature. First let’s load the data:

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> list(iris.keys())
['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']
>>> X = iris["data"][ :, 3:] # petal width
>>> y = (iris["target"] == 2).astype(np.int) # 1 if Iris-Virginica, else 0 
```

Now let’s train a Logistic Regression model:

from sklearn.linear_model import LogisticRegression

```python
log_reg = LogisticRegression()  
log_reg.fit(X, y) 
```

Let’s look at the model’s estimated probabilities for flowers with petal widths varying from 0 to $3 \mathrm { c m }$ (Figure 4-23)17:

```python
X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  
y(probab = log_reg.predict(probab(X_new))  
plt.plot(X_new, y(probab[:, 1], "g-", label="Iris-Virginica") 
```

![](images/b22c63756c3a3160e6534e9defc85b1794d99ad5e85eebe94aff1171c3c3eab7.jpg)  
Figure 4-23. Estimated probabilities and decision boundary

The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4 cm to $2 . 5 ~ \mathrm { c m }$ , while the other iris flowers (represented by squares) generally have a smaller petal width, ranging from $0 . 1 \mathrm { { c m } }$ to $1 . 8 ~ \mathrm { c m }$ . Notice that there is a bit of over‐ lap. Above about $2 \ \mathrm { c m }$ the classifier is highly confident that the flower is an Iris-Virginica (it outputs a high probability to that class), while below $1 \ \mathrm { c m }$ it is highly confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica” class). In between these extremes, the classifier is unsure. However, if you ask it to predict the class (using the predict() method rather than the predict_proba() method), it will return whichever class is the most likely. Therefore, there is a decision boundary at around $1 . 6 ~ \mathrm { c m }$ where both probabilities are equal to $5 0 \%$ : if the petal width is higher than $1 . 6 \ c m$ , the classifier will predict that the flower is an Iris-Virginica, or else it will predict that it is not (even if it is not very confident):

```javascript
>>log_reg.predict([[1.7],[1.5]]) array([1,0]) 
```

Figure 4-24 shows the same dataset but this time displaying two features: petal width and length. Once trained, the Logistic Regression classifier can estimate the probabil‐ ity that a new flower is an Iris-Virginica based on these two features. The dashed line represents the points where the model estimates a $5 0 \%$ probability: this is the model’s decision boundary. Note that it is a linear boundary.18 Each parallel line represents the points where the model outputs a specific probability, from $1 5 \%$ (bottom left) to $9 0 \%$ (top right). All the flowers beyond the top-right line have an over $9 0 \%$ chance of being Iris-Virginica according to the model.

![](images/cf9b2696a561a44efb922f537e396d2ff6568ccb93e8c4609d1d1f755876c792.jpg)  
Figure 4-24. Linear decision boundary

Just like the other linear models, Logistic Regression models can be regularized using $\ell _ { 1 }$ or $\ell _ { 2 }$ penalties. Scitkit-Learn actually adds an $\ell _ { 2 }$ penalty by default.

![](images/8a2550ce543d23bb2bd882182cf02df3549d6de170e0d5fd825f4b0f4386e4a6.jpg)

The hyperparameter controlling the regularization strength of a Scikit-Learn LogisticRegression model is not alpha (as in other linear models), but its inverse: C. The higher the value of C, the less the model is regularized.

# Softmax Regression

The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers (as discussed in Chapter 3). This is called Somax Regression, or Multinomial Logistic Regression.

The idea is quite simple: when given an instance x, the Softmax Regression model first computes a score $s _ { k } ( { \bf x } )$ for each class $k ,$ then estimates the probability of each class by applying the somax function (also called the normalized exponential) to the scores. The equation to compute $s _ { k } ( { \bf x } )$ should look familiar, as it is just like the equa‐ tion for Linear Regression prediction (see Equation 4-19).

Equation 4-19. Somax score for class $k$

$$
s _ {k} (\mathbf {x}) = \mathbf {x} ^ {T} \pmb {\theta} ^ {(k)}
$$

Note that each class has its own dedicated parameter vector $\mathbf { \boldsymbol { \theta } } ^ { ( k ) }$ . All these vectors are typically stored as rows in a parameter matrix Θ.

Once you have computed the score of every class for the instance x, you can estimate the probability $\hat { p } _ { k }$ that the instance belongs to class $k$ by running the scores through the softmax function (Equation 4-20): it computes the exponential of every score,

then normalizes them (dividing by the sum of all the exponentials). The scores are generally called logits or log-odds (although they are actually unnormalized logodds).

Equation 4-20. Somax function

$$
\hat {p} _ {k} = \sigma (\mathbf {s} (\mathbf {x})) _ {k} = \frac {\exp (s _ {k} (\mathbf {x}))}{\sum_ {j = 1} ^ {K} \exp (s _ {j} (\mathbf {x}))}
$$

• $K$ is the number of classes.   
• $\mathbf { \boldsymbol { s } } ( \mathbf { \boldsymbol { x } } )$ is a vector containing the scores of each class for the instance x.   
• $\sigma ( \mathbf { s } ( \mathbf { x } ) ) _ { k }$ is the estimated probability that the instance x belongs to class $k$ given the scores of each class for that instance.

Just like the Logistic Regression classifier, the Softmax Regression classifier predicts the class with the highest estimated probability (which is simply the class with the highest score), as shown in Equation 4-21.

Equation 4-21. Somax Regression classier prediction

$$
\hat {y} = \underset {k} {\operatorname {a r g m a x}} \sigma (\mathbf {s} (\mathbf {x})) _ {k} = \underset {k} {\operatorname {a r g m a x}} s _ {k} (\mathbf {x}) = \underset {k} {\operatorname {a r g m a x}} \left(\left(\boldsymbol {\theta} ^ {(k)}\right) ^ {T} \mathbf {x}\right)
$$

• The argmax operator returns the value of a variable that maximizes a function. In this equation, it returns the value of $k$ that maximizes the estimated probability $\sigma ( \mathbf { s } ( \mathbf { x } ) ) _ { k }$ .

![](images/336533dffd3db5de3f8d7f728dc338a62a715e1a5b03750c8b39696cee83c6be.jpg)

The Softmax Regression classifier predicts only one class at a time (i.e., it is multiclass, not multioutput) so it should be used only with mutually exclusive classes such as different types of plants. You cannot use it to recognize multiple people in one picture.

Now that you know how the model estimates probabilities and makes predictions, let’s take a look at training. The objective is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). Minimizing the cost function shown in Equation 4-22, called the cross entropy, should lead to this objective because it penalizes the model when it estimates a low probability for a target class. Cross entropy is frequently used to measure how

well a set of estimated class probabilities match the target classes (we will use it again several times in the following chapters).

Equation 4-22. Cross entropy cost function

$$
J (\boldsymbol {\Theta}) = - \frac {1}{m} \sum_ {i = 1} ^ {m} \sum_ {k = 1} ^ {K} y _ {k} ^ {(i)} \mathrm {l o g} \Big (\hat {p} _ {k} ^ {(i)} \Big)
$$

• $y _ { k } ^ { ( i ) }$ is the target probability that the $\mathrm { i ^ { t h } }$ instance belongs to class $k$ . In general, it is either equal to 1 or 0, depending on whether the instance belongs to the class or not.

Notice that when there are just two classes $\left( K = 2 \right)$ , this cost function is equivalent to the Logistic Regression’s cost function (log loss; see Equation 4-17).

# Cross Entropy

Cross entropy originated from information theory. Suppose you want to efficiently transmit information about the weather every day. If there are eight options (sunny, rainy, etc.), you could encode each option using 3 bits since $2 ^ { 3 } = 8$ . However, if you think it will be sunny almost every day, it would be much more efficient to code “sunny” on just one bit (0) and the other seven options on 4 bits (starting with a 1). Cross entropy measures the average number of bits you actually send per option. If your assumption about the weather is perfect, cross entropy will just be equal to the entropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump‐ tions are wrong (e.g., if it rains often), cross entropy will be greater by an amount called the Kullback–Leibler divergence.

The cross entropy between two probability distributions $\boldsymbol { p }$ and $q$ is defined as $H ( p , q ) = \ - \ \Sigma _ { x } p ( x ) \log q ( x )$ (at least when the distributions are discrete). For more details, check out this video.

The gradient vector of this cost function with regards to $\mathbf { \boldsymbol { \theta } } ^ { ( k ) }$ is given by Equation 4-23:

Equation 4-23. Cross entropy gradient vector for class k

$$
\nabla_ {\pmb {\theta} ^ {(k)}} J (\pmb {\Theta}) = \frac {1}{m} \sum_ {i = 1} ^ {m} \left(\hat {p} _ {k} ^ {(i)} - y _ {k} ^ {(i)}\right) \mathbf {x} ^ {(i)}
$$

Now you can compute the gradient vector for every class, then use Gradient Descent (or any other optimization algorithm) to find the parameter matrix $\Theta$ that minimizes the cost function.

Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-Learn’s LogisticRegression uses one-versus-all by default when you train it on more than two classes, but you can set the multi_class hyperparameter to "multinomial" to switch it to Softmax Regression instead. You must also specify a solver that sup‐ ports Softmax Regression, such as the "lbfgs" solver (see Scikit-Learn’s documenta‐ tion for more details). It also applies $\ell _ { 2 }$ regularization by default, which you can control using the hyperparameter C.

```python
X = iris["data"]["(:, (2, 3)] # petal length, petal width  
y = iris["target"]  
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10)  
softmax_reg.fit(X, y) 
```

So the next time you find an iris with $5 \ c m$ long and $2 \mathrm { \ c m }$ wide petals, you can ask your model to tell you what type of iris it is, and it will answer Iris-Virginica (class 2) with $9 4 . 2 \%$ probability (or Iris-Versicolor with $5 . 8 \%$ probability):

```python
>>> softmax_reg.predict([[5, 2]])
array([2])
>>> softmax_reg.predict_prob([[5, 2]])
array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])
```

Figure 4-25 shows the resulting decision boundaries, represented by the background colors. Notice that the decision boundaries between any two classes are linear. The figure also shows the probabilities for the Iris-Versicolor class, represented by the curved lines (e.g., the line labeled with 0.450 represents the $4 5 \%$ probability bound‐ ary). Notice that the model can predict a class that has an estimated probability below $5 0 \%$ . For example, at the point where all decision boundaries meet, all classes have an equal estimated probability of $3 3 \%$ .

![](images/3826fa44dd8a987d54b58e4229e7adf2fe9568fc558f42066f5ed17989a23672.jpg)  
Figure 4-25. Somax Regression decision boundaries

# Exercises

1. What Linear Regression training algorithm can you use if you have a training set with millions of features?

2. Suppose the features in your training set have very different scales. What algo‐ rithms might suffer from this, and how? What can you do about it?

3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?

4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?

5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?

6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐ dation error goes up?

7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?

8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?

9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regulari‐ zation hyperparameter $\alpha$ or reduce it?

10. Why would you want to use:

• Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐ zation)?   
• Lasso instead of Ridge Regression?   
• Elastic Net instead of Lasso?

11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regres‐ sion classifier?   
12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn).

Solutions to these exercises are available in ???.

# Support Vector Machines

![](images/8e340c034ef087c3db3c1efd98b99acc610e4df84caecff145a3b109f2a6befc.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 5 in the final release of the book.

A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any‐ one interested in Machine Learning should have it in their toolbox. SVMs are partic‐ ularly well suited for classification of complex but small- or medium-sized datasets.

This chapter will explain the core concepts of SVMs, how to use them, and how they work.

# Linear SVM Classication

The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably not perform as well on new instances. In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classi‐ fier; this line not only separates the two classes but also stays as far away from the closest training instances as possible. You can think of an SVM classifier as fitting the

widest possible street (represented by the parallel dashed lines) between the classes. This is called large margin classication.

![](images/4f664c3e425dd10a58a13e3e541cf9ea6fc3e08dc37dfd1276d58035a85ac47f.jpg)  
Figure 5-1. Large margin classication

Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the support vectors (they are circled in Figure 5-1).

![](images/cc03d1e5e6fdcb508f090f0d3408fe9c9c48f381b5f61de47810181acb587beb.jpg)  
SVMs are sensitive to the feature scales, as you can see in Figure 5-2: on the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g., using Scikit-Learn’s StandardScaler), the decision boundary looks much better (on the right plot).

![](images/7f2e1a2d9787848a8e1087b8faf5fd8f269304204763b0c6aca62217970103a7.jpg)  
Figure 5-2. Sensitivity to feature scales

# Soft Margin Classication

If we strictly impose that all instances be off the street and on the right side, this is called hard margin classication. There are two main issues with hard margin classifi‐ cation. First, it only works if the data is linearly separable, and second it is quite sensi‐ tive to outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on the left, it is impossible to find a hard margin, and on the right the decision boundary ends up very different from the one we saw in Figure 5-1 without the outlier, and it will probably not generalize as well.

![](images/d00ad3725d48d7f008e8a7b620d5321f316433008507dfa4e77cc14a9a9ee5e6.jpg)

![](images/6729f869956f0715092a29d8f97b6a96906b6eac833cbd158dd92399a5f1df1f.jpg)  
Figure 5-3. Hard margin sensitivity to outliers

To avoid these issues it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). This is called so margin classication.

In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparame‐ ter: a smaller C value leads to a wider street but more margin violations. Figure 5-4 shows the decision boundaries and margins of two soft margin SVM classifiers on a nonlinearly separable dataset. On the left, using a low C value the margin is quite large, but many instances end up on the street. On the right, using a high C value the classifier makes fewer margin violations but ends up with a smaller margin. However, it seems likely that the first classifier will generalize better: in fact even on this train‐ ing set it makes fewer prediction errors, since most of the margin violations are actually on the correct side of the decision boundary.

![](images/20fe30714b8c93986f1f13546075d1ba005791312667db172bd26aa1873f9193.jpg)

![](images/81eae2f76ca718e527a2a61bfb68f06967bd909ae6d91bc2da31331594716ac6.jpg)

![](images/c97ea2372fd185cf13c8db053bb5c361b7696bdbb83a630ed51ce8514f1ef59c.jpg)  
Figure 5-4. Large margin (le) versus fewer margin violations (right)

If your SVM model is overfitting, you can try regularizing it by reducing C.

The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM model (using the LinearSVC class with $C = 1$ and the hinge loss function, described shortly) to detect Iris-Virginica flowers. The resulting model is represented on the left of Figure 5-4.

import numpy as np   
from sklearn import datasets   
from sklearnpipeline import Pipeline   
from sklearn.preprocessing import StandardScaler   
from sklearn.svm import LinearSVC   
iris $=$ datasets.load_iris()   
X $=$ iris["data"]["(:,2,3)]#petal length,petal width   
y $=$ (iris["target"] $\equiv = 2$ ).astype(np.float64)# Iris-Virginica   
svm_clf $=$ Pipeline([ ("scalar",StandardScaler(), ("linear_svc",LinearSVC(C=1,loss="hinge"))), ])   
svm_clf.fit(X,y)

Then, as usual, you can use the model to make predictions:

```python
>>svm_clf.predict([[5.5，1.7]]) array([1.])
```

![](images/d518f58c59b04260b8f28598195e26420c235942fe9aabe0d635ee7f5c6d3212.jpg)

Unlike Logistic Regression classifiers, SVM classifiers do not out‐ put probabilities for each class.

Alternatively, you could use the SVC class, using SVC(kernel="linear", ${ \mathsf { C } } { = } 1$ ), but it is much slower, especially with large training sets, so it is not recommended. Another option is to use the SGDClassifier class, with SGDClassifier(loss="hinge", alp ${ \mathsf { h a } } { = } 1 / ( { \mathsf { m } } ^ { \star } { \mathsf { C } } ) ;$ . This applies regular Stochastic Gradient Descent (see Chapter 4) to train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it can be useful to handle huge datasets that do not fit in memory (out-of-core train‐ ing), or to handle online classification tasks.

![](images/139ae2a858cce843059ea044c308488aad1313efe9bd346fdebc5dc3f3a530ee.jpg)

The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. This is automatic if you scale the data using the StandardScaler. Moreover, make sure you set the loss hyperparameter to "hinge", as it is not the default value. Finally, for better performance you should set the dual hyperparameter to False, unless there are more features than training instances (we will discuss duality later in the chapter).

# Nonlinear SVM Classication

Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features (as you did in Chapter 4); in some cases this can result in a linearly separable dataset. Consider the left plot in Figure 5-5: it represents a simple dataset with just one feature $x _ { 1 }$ . This dataset is not linearly separable, as you can see. But if you add a second fea‐ ture $x _ { 2 } = ( x _ { 1 } ) ^ { 2 }$ , the resulting 2D dataset is perfectly linearly separable.

![](images/2c2f3eb102340deab7e2cb0f84cc77a8f1f59b27f4ec9253da4ec1eb26e86050.jpg)

![](images/19bb9774b24bacb92a85d83bb3e2f263cd4f5679d4bc7e335111d89be6fa5cb6.jpg)  
Figure 5-5. Adding features to make a dataset linearly separable

To implement this idea using Scikit-Learn, you can create a Pipeline containing a PolynomialFeatures transformer (discussed in “Polynomial Regression” on page 130), followed by a StandardScaler and a LinearSVC. Let’s test this on the moons dataset: this is a toy dataset for binary classification in which the data points are sha‐ ped as two interleaving half circles (see Figure 5-6). You can generate this dataset using the make_moons() function:

from sklearn.datasets import make moons   
from sklearnpipeline import Pipeline   
from sklearn.preprocessing import PolynomialFeatures   
polynomial_svm_clf $=$ Pipeline([ ("poly_features", PolynomialFeatures(degree=3)), ("scalar", StandardScaler(   ), ("svm_clf", LinearSVC(C=10, loss="hinge")) ])   
polynomial_svm_clf.fit(X, y)

![](images/5e202e08b3f1898b297af8b1e90bc83f5de01d90bd0d3522efc937f4934a38a1.jpg)  
Figure 5-6. Linear SVM classier using polynomial features

# Polynomial Kernel

Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow.

Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the kernel trick (it is explained in a moment). It makes it possible to get the same result as if you added many polynomial features, even with very highdegree polynomials, without actually having to add them. So there is no combinato‐ rial explosion of the number of features since you don’t actually add any features. This trick is implemented by the SVC class. Let’s test it on the moons dataset:

from sklearn谣 import SVC   
poly_kernel_svm_clrf $\equiv$ Pipeline([ ("scalar",StandardScaler(   ), ("svm_clrf",SVC(kernel $\equiv$ "poly",degree $= 3$ ,coef0=1,C=5)) ])   
poly_kernel_svm_clf.fit(X,y)

This code trains an SVM classifier using a $3 ^ { \mathrm { r d } }$ -degree polynomial kernel. It is repre‐ sented on the left of Figure 5-7. On the right is another SVM classifier using a $1 0 ^ { \mathrm { t h } }$ - degree polynomial kernel. Obviously, if your model is overfitting, you might want to

reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing it. The hyperparameter coef0 controls how much the model is influenced by highdegree polynomials versus low-degree polynomials.

![](images/5b4bf775503113d575ba992ead7b876fd98838c71f69df1971272f40b70998b2.jpg)  
Figure 5-7. SVM classiers with a polynomial kernel

![](images/a6b13d790d60e70c63b5d3c05d78d9277828b6a9c6d3d724ea40818995a5b57b.jpg)

A common approach to find the right hyperparameter values is to use grid search (see Chapter 2). It is often faster to first do a very coarse grid search, then a finer grid search around the best values found. Having a good sense of what each hyperparameter actually does can also help you search in the right part of the hyperparame‐ ter space.

# Adding Similarity Features

Another technique to tackle nonlinear problems is to add features computed using a similarity function that measures how much each instance resembles a particular landmark. For example, let’s take the one-dimensional dataset discussed earlier and add two landmarks to it at $x _ { 1 } = - 2$ and $x _ { 1 } = 1$ (see the left plot in Figure 5-8). Next, let’s define the similarity function to be the Gaussian Radial Basis Function (RBF) with $\gamma = 0 . 3$ (see Equation 5-1).

Equation 5-1. Gaussian RBF

$$
\phi_ {\gamma} (\mathbf {x}, \ell) = \exp \left(- \gamma | | \mathbf {x} - \ell | | ^ {2}\right)
$$

It is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark). Now we are ready to compute the new features. For example, let’s look at the instance $x _ { 1 } = - 1$ : it is located at a distance of 1 from the first landmark, and 2 from the second landmark. Therefore its new features are x = exp $\left( - 0 . 3 \times 1 ^ { 2 } \right) \approx 0 . 7 4$ and x = exp $\left( - 0 . 3 \times 2 ^ { 2 } \right) \approx 0 . 3 0$ . The plot on the right of Figure 5-8 shows the trans‐ formed dataset (dropping the original features). As you can see, it is now linearly separable.

![](images/537ac73c214639496e8821a230caccd1876010cbb9bfc6bf080aebdc1437122c.jpg)

![](images/ded578c361ec359652d857a5ff18aab10c9feedcfe686119515c1744cb92f154.jpg)  
Figure 5-8. Similarity features using the Gaussian RBF

You may wonder how to select the landmarks. The simplest approach is to create a landmark at the location of each and every instance in the dataset. This creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. The downside is that a training set with m instances and n features gets transformed into a training set with m instances and m features (assuming you drop the original features). If your training set is very large, you end up with an equally large number of features.

# Gaussian RBF Kernel

Just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features, especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them. Let’s try the Gaussian RBF kernel using the SVC class:

```python
rbf_kernel_svm_clf = Pipeline([("scalar", StandardScaler()), ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))])  
rbf_kernel_svm_clf.fit(X, y) 
```

This model is represented on the bottom left of Figure 5-9. The other plots show models trained with different values of hyperparameters gamma (γ) and C. Increasing gamma makes the bell-shape curve narrower (see the left plot of Figure 5-8), and as a result each instance’s range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small gamma value makes the bell-shaped curve wider, so instances have a larger range of influ‐ ence, and the decision boundary ends up smoother. So γ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it, and if it is under‐ fitting, you should increase it (similar to the C hyperparameter).

![](images/16f310d8f1db6cdc3bc176e2d94bbc085df81c26965292c502fe6e72f4b6ecac.jpg)  
Figure 5-9. SVM classiers using an RBF kernel

Other kernels exist but are used much more rarely. For example, some kernels are specialized for specific data structures. String kernels are sometimes used when classi‐ fying text documents or DNA sequences (e.g., using the string subsequence kernel or kernels based on the Levenshtein distance).

![](images/3e694600d91703422f28ebe06a68c5f97b260786dba7c921e9b9f83fefc6a74f.jpg)

With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first (remember that LinearSVC is much faster than SVC(ker nel="linear")), especially if the training set is very large or if it has plenty of features. If the training set is not too large, you should try the Gaussian RBF kernel as well; it works well in most cases. Then if you have spare time and computing power, you can also experiment with a few other kernels using cross-validation and grid search, especially if there are kernels specialized for your training set’s data structure.

# Computational Complexity

The LinearSVC class is based on the liblinear library, which implements an optimized algorithm for linear SVMs.1 It does not support the kernel trick, but it scales almost

linearly with the number of training instances and the number of features: its training time complexity is roughly $O ( m \times n )$ .

The algorithm takes longer if you require a very high precision. This is controlled by the tolerance hyperparameter $\epsilon$ (called tol in Scikit-Learn). In most classification tasks, the default tolerance is fine.

The SVC class is based on the libsvm library, which implements an algorithm that sup‐ ports the kernel trick.2 The training time complexity is usually between $O ( m ^ { 2 } \times n )$ and $O ( m ^ { 3 } \times n )$ . Unfortunately, this means that it gets dreadfully slow when the num‐ ber of training instances gets large (e.g., hundreds of thousands of instances). This algorithm is perfect for complex but small or medium training sets. However, it scales well with the number of features, especially with sparse features (i.e., when each instance has few nonzero features). In this case, the algorithm scales roughly with the average number of nonzero features per instance. Table 5-1 compares Scikit-Learn’s SVM classification classes.

Table 5-1. Comparison of Scikit-Learn classes for SVM classication   

<table><tr><td>Class</td><td>Time complexity</td><td>Out-of-core support</td><td>Scaling required</td><td>Kernel trick</td></tr><tr><td>LinearSVC</td><td>0(m × n)</td><td>No</td><td>Yes</td><td>No</td></tr><tr><td>SGDClassifier</td><td>0(m × n)</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>SVC</td><td>0(m2 × n) to 0(m3 × n)</td><td>No</td><td>Yes</td><td>Yes</td></tr></table>

# SVM Regression

As we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐ port linear and nonlinear classification, but it also supports linear and nonlinear regression. The trick is to reverse the objective: instead of trying to fit the largest pos‐ sible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances o the street). The width of the street is controlled by a hyperparame‐ ter $\epsilon .$ Figure 5-10 shows two linear SVM Regression models trained on some random linear data, one with a large margin $\left( \epsilon = 1 . 5 \right)$ ) and the other with a small margin $\overset { \cdot } { \epsilon } =$ 0.5).

![](images/15987e7920e1928e448bd6496114a77a46aa827c73517abe6d4d55fd7609a0ac.jpg)  
Figure 5-10. SVM Regression

Adding more training instances within the margin does not affect the model’s predic‐ tions; thus, the model is said to be $\epsilon$ -insensitive.

You can use Scikit-Learn’s LinearSVR class to perform linear SVM Regression. The following code produces the model represented on the left of Figure 5-10 (the train‐ ing data should be scaled and centered first):

from sklearn.svm import LinearSVR

```python
svm_reg = LinearSVR(epsilon=1.5)  
svm_reg.fit(X, y) 
```

To tackle nonlinear regression tasks, you can use a kernelized SVM model. For exam‐ ple, Figure 5-11 shows SVM Regression on a random quadratic training set, using a $2 ^ { \mathrm { n d } }$ -degree polynomial kernel. There is little regularization on the left plot (i.e., a large C value), and much more regularization on the right plot (i.e., a small C value).

![](images/266fbeabcd3ffe233fcecf7fabb035682c49c2bf7e0ed14387225ce3eabf2f76.jpg)  
Figure 5-11. SVM regression using a $2 ^ { n d }$ -degree polynomial kernel

The following code produces the model represented on the left of Figure 5-11 using Scikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐ sion equivalent of the SVC class, and the LinearSVR class is the regression equivalent of the LinearSVC class. The LinearSVR class scales linearly with the size of the train‐ ing set (just like the LinearSVC class), while the SVR class gets much too slow when the training set grows large (just like the SVC class).

from sklearn.svm import SVR

svm_poly_reg $=$ SVR(kernel $=$ "poly", degree $\mathbf { \Psi } = \mathbf { \Psi }$ , $\mathsf { C } \mathsf { = } \mathsf { 1 } \Theta \Theta$ , epsilon=0.1) svm_poly_reg.fit(X, y)

![](images/26490b6b85103c4ff586bae71a0702e411c0f6b28d55567c89694fe7dd2fd4e6.jpg)

SVMs can also be used for outlier detection; see Scikit-Learn’s doc‐ umentation for more details.

# Under the Hood

This section explains how SVMs make predictions and how their training algorithms work, starting with linear SVM classifiers. You can safely skip it and go straight to the exercises at the end of this chapter if you are just getting started with Machine Learn‐ ing, and come back later when you want to get a deeper understanding of SVMs.

First, a word about notations: in Chapter 4 we used the convention of putting all the model parameters in one vector θ, including the bias term $\theta _ { 0 }$ and the input feature weights $\theta _ { 1 }$ to $\theta _ { n } ,$ and adding a bias input $x _ { 0 } = 1$ to all instances. In this chapter, we will use a different convention, which is more convenient (and more common) when you are dealing with SVMs: the bias term will be called $^ { b }$ and the feature weights vector will be called w. No bias feature will be added to the input feature vectors.

# Decision Function and Predictions

The linear SVM classifier model predicts the class of a new instance $\mathbf { x }$ by simply com‐ puting the decision function $\mathbf { w } ^ { T } \mathbf { x } + b = w _ { 1 } x _ { 1 } + \cdots + w _ { n } x _ { n } + b ;$ : if the result is positive, the predicted class $\hat { y }$ is the positive class (1), or else it is the negative class (0); see Equation 5-2.

Equation 5-2. Linear SVM classier prediction

$$
\hat {y} = \left\{ \begin{array}{l} 0 \text {i f} \mathbf {w} ^ {T} \mathbf {x} + b <   0, \\ 1 \text {i f} \mathbf {w} ^ {T} \mathbf {x} + b \geq 0 \end{array} \right.
$$

![](images/5716c2dd3ca662dde47d29470bd79685c4afd0681ad4ba6b59ac39489ceb80ee.jpg)  
Figure 5-12 shows the decision function that corresponds to the model on the left of Figure 5-4: it is a two-dimensional plane since this dataset has two features (petal width and petal length). The decision boundary is the set of points where the decision function is equal to 0: it is the intersection of two planes, which is a straight line (rep‐ resented by the thick solid line).3   
Figure 5-12. Decision function for the iris dataset

The dashed lines represent the points where the decision function is equal to 1 or –1: they are parallel and at equal distance to the decision boundary, forming a margin around it. Training a linear SVM classifier means finding the value of w and $^ { b }$ that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin).

# Training Objective

Consider the slope of the decision function: it is equal to the norm of the weight vec‐ tor, $\Vert \textbf { w } \Vert$ . If we divide this slope by 2, the points where the decision function is equal to $\pm 1$ are going to be twice as far away from the decision boundary. In other words, dividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐ ize in 2D in Figure 5-13. The smaller the weight vector w, the larger the margin.

![](images/f673c3fb090abe6ab11187f07fb8b83f8fdf9b1040dc91715bb42678e7577fec.jpg)  
Figure 5-13. A smaller weight vector results in a larger margin

So we want to minimize $\Vert \textbf { w } \Vert$ to get a large margin. However, if we also want to avoid any margin violation (hard margin), then we need the decision function to be greater than 1 for all positive training instances, and lower than $^ { - 1 }$ for negative training instances. If we define $t ^ { ( i ) } = - 1$ for negative instances (if $\boldsymbol { y } ^ { ( i ) } = \boldsymbol { 0 } _ { , }$ ) and $t ^ { ( i ) } = 1$ for positive instances (if $\gamma ^ { ( i ) } = 1 \big )$ ), then we can express this constraint as $t ^ { ( i ) } ( \mathbf { w } ^ { T } \mathbf { x } ^ { ( i ) } + b ) \geq 1$ for all instances.

We can therefore express the hard margin linear SVM classifier objective as the con‐ strained optimization problem in Equation 5-3.

Equation 5-3. Hard margin linear SVM classier objective

minimize 12 wT w 1 w, b

subject to t i wTx i + b ≥ 1 for i = 1, 2, ⋯, m

![](images/41f77f16642eb946999b89d34737be593971726e017ab28351b96b2154ea66f9.jpg)

We are minimizing $\frac { 1 } { 2 } \mathbf { w } ^ { T } \mathbf { w }$ , which is equal to $\frac { 1 } { 2 } \Vert \textbf { w } \Vert ^ { 2 } ;$ , rather than minimizing $\Vert \textbf { w } \Vert$ . Indeed, $\frac { 1 } { 2 } \Vert \textbf { w } \Vert ^ { 2 }$ has a nice and simple derivative (it is just w) while $\parallel \mathbf { w } \parallel$ is not differentiable at $\mathbf { w } = \mathbf { 0 }$ . Optimization algorithms work much better on differentiable functions.

To get the soft margin objective, we need to introduce a slack variable $\zeta ^ { ( i ) } \geq 0$ for each instance:4 $\zeta ^ { ( i ) }$ measures how much the $\mathrm { i ^ { t h } }$ instance is allowed to violate the margin. We now have two conflicting objectives: making the slack variables as small as possible to reduce the margin violations, and making $\frac { 1 } { 2 } \mathbf { w } ^ { T }$ w as small as possible to increase the margin. This is where the C hyperparameter comes in: it allows us to define the trade‐

off between these two objectives. This gives us the constrained optimization problem in Equation 5-4.

Equation 5-4. So margin linear SVM classier objective

$$
\underset {\mathbf {w}, b, \zeta} {\text {m i n i m i z e}} \quad \frac {1}{2} \mathbf {w} ^ {T} \mathbf {w} + C \sum_ {i = 1} ^ {m} \zeta^ {(i)}
$$

$$
\text {s u b j e c t} \quad t ^ {(i)} \left(\mathbf {w} ^ {T} \mathbf {x} ^ {(i)} + b\right) \geq 1 - \zeta^ {(i)} \quad \text {a n d} \quad \zeta^ {(i)} \geq 0 \quad \text {f o r} i = 1, 2, \dots , m
$$

# Quadratic Programming

The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as Quadratic Program‐ ming (QP) problems. Many off-the-shelf solvers are available to solve QP problems using a variety of techniques that are outside the scope of this book.5 The general problem formulation is given by Equation 5-5.

Equation 5-5. Quadratic Programming problem

$$
\underset {\mathbf {p}} {\text {M i n i m i z e}} \quad \frac {1}{2} \mathbf {p} ^ {T} \mathbf {H} \mathbf {p} \quad + \quad \mathbf {f} ^ {T} \mathbf {p}
$$

$$
\text {s u b j e c t} \quad \mathbf {A p} \leq \mathbf {b}
$$

$$
\text {w h e r e} \left\{ \begin{array}{l l} \mathbf {p} & \text {i s a n} n _ {p} \text {- d i m e n s i o n a l v e c t o r (} n _ {p} = \text {n u m b e r o f p a r a m e t e r s) ,} \\ \mathbf {H} & \text {i s a n} n _ {p} \times n _ {p} \text {m a t r i x ,} \\ \mathbf {f} & \text {i s a n} n _ {p} \text {- d i m e n s i o n a l v e c t o r ,} \\ \mathbf {A} & \text {i s a n} n _ {c} \times n _ {p} \text {m a t r i x (} n _ {c} = \text {n u m b e r o f c o n s t r a i n t s) ,} \\ \mathbf {b} & \text {i s a n} n _ {c} \text {- d i m e n s i o n a l v e c t o r .} \end{array} \right.
$$

Note that the expression A $\mathbf { p } \leq \mathbf { b }$ actually defines $n _ { c }$ constraints: $\mathbf { p } ^ { T } \mathbf { \hat { a } } ^ { ( i ) } \leq b ^ { ( i ) }$ for $i = 1$ , $2 , \cdots , n _ { c } ,$ $n _ { c } ,$ where $\mathbf { a } ^ { ( i ) }$ is the vector containing the elements of the $\mathrm { i ^ { \mathrm { t h } } }$ row of A and $b ^ { ( i ) }$ is the $\mathrm { i ^ { t h } }$ element of b.

You can easily verify that if you set the QP parameters in the following way, you get the hard margin linear SVM classifier objective:

• $n _ { p } = n + 1$ , where $n$ is the number of features (the $+ 1$ is for the bias term).

• $n _ { c } = m$ , where $m$ is the number of training instances.   
• $\mathbf { H }$ is the $n _ { p } \times n _ { p }$ identity matrix, except with a zero in the top-left cell (to ignore the bias term).   
• $\mathbf { f } = \mathbf { 0 }$ , an $n _ { p }$ -dimensional vector full of 0s.   
• $\mathbf b = - \mathbf 1$ , an $n _ { c }$ -dimensional vector full of –1s.   
• $\mathbf { a } ^ { ( i ) } = - t ^ { ( i ) } \dot { \mathbf { X } } ^ { ( i ) }$ , where $\dot { \textbf { x } } ^ { ( i ) }$ is equal to $\mathbf { X } ^ { ( i ) }$ with an extra bias feature $\dot { \mathbf { X } } _ { 0 } = 1$

So one way to train a hard margin linear SVM classifier is just to use an off-the-shelf QP solver by passing it the preceding parameters. The resulting vector p will contain the bias term $b = p _ { 0 }$ and the feature weights ${ \boldsymbol { w } } _ { i } = { \boldsymbol { p } } _ { i }$ for i = 1, 2, ⋯, n. Similarly, you can use a QP solver to solve the soft margin problem (see the exercises at the end of the chapter).

However, to use the kernel trick we are going to look at a different constrained opti‐ mization problem.

# The Dual Problem

Given a constrained optimization problem, known as the primal problem, it is possi‐ ble to express a different but closely related problem, called its dual problem. The sol‐ ution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions it can even have the same solutions as the pri‐ mal problem. Luckily, the SVM problem happens to meet these conditions,6 so you can choose to solve the primal problem or the dual problem; both will have the same solution. Equation 5-6 shows the dual form of the linear SVM objective (if you are interested in knowing how to derive the dual problem from the primal problem, see ???).

Equation 5-6. Dual form of the linear SVM objective

$$
\underset {\alpha} {\text {m i n i m i z e}} \frac {1}{2} \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {m} \alpha^ {(i)} \alpha^ {(j)} t ^ {(i)} t ^ {(j)} \mathbf {x} ^ {(i) ^ {T}} \mathbf {x} ^ {(j)} - \sum_ {i = 1} ^ {m} \alpha^ {(i)}
$$

subject to $\alpha ^ { ( i ) } \geq 0$ for i = 1, 2, ⋯, m

Once you find the vector $\hat { \alpha }$ that minimizes this equation (using a QP solver), you can compute $\widehat { \mathbf { w } }$ and $\hat { b }$ that minimize the primal problem by using Equation 5-7.

Equation 5-7. From the dual solution to the primal solution

$$
\begin{array}{l} \widehat {\mathbf {w}} = \sum_ {i = 1} ^ {m} \widehat {\alpha} ^ {(i)} t ^ {(i)} \mathbf {x} ^ {(i)} \\ \hat{b} = \frac{1}{n_{s}}\sum_{\substack{i = 1\\ \hat{\alpha}^{(i)} > 0}}^{m}\left(t^{(i)} - \widehat{\mathbf{w}}^{T}\mathbf{x}^{(i)}\right) \\ \end{array}
$$

The dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features. More importantly, it makes the ker‐ nel trick possible, while the primal does not. So what is this kernel trick anyway?

# Kernelized SVM

Suppose you want to apply a $2 ^ { \mathrm { n d } }$ -degree polynomial transformation to a twodimensional training set (such as the moons training set), then train a linear SVM classifier on the transformed training set. Equation 5-8 shows the $2 ^ { \mathrm { n d } }$ -degree polyno‐ mial mapping function $\phi$ that you want to apply.

Equation 5-8. Second-degree polynomial mapping

$$
\phi (\mathbf {x}) = \phi \left(\left( \begin{array}{c} x _ {1} \\ x _ {2} \end{array} \right)\right) = \left( \begin{array}{c} x _ {1} ^ {2} \\ \sqrt {2}   x _ {1} x _ {2} \\ x _ {2} ^ {2} \end{array} \right)
$$

Notice that the transformed vector is three-dimensional instead of two-dimensional. Now let’s look at what happens to a couple of two-dimensional vectors, a and b, if we apply this $2 ^ { \mathrm { n d } }$ -degree polynomial mapping and then compute the dot product7 of the transformed vectors (See Equation 5-9).

Equation 5-9. Kernel trick for a $2 ^ { n d }$ -degree polynomial mapping

$$
\begin{array}{l} \phi (\mathbf {a}) ^ {T} \phi (\mathbf {b}) = \left( \begin{array}{c} a _ {1} ^ {2} \\ \sqrt {2}   a _ {1} a _ {2} \\ a _ {2} ^ {2} \end{array} \right) ^ {T} \left( \begin{array}{c} b _ {1} ^ {2} \\ \sqrt {2}   b _ {1} b _ {2} \\ b _ {2} ^ {2} \end{array} \right) = a _ {1} ^ {2} b _ {1} ^ {2} + 2 a _ {1} b _ {1} a _ {2} b _ {2} + a _ {2} ^ {2} b _ {2} ^ {2} \\ = \left(a _ {1} b _ {1} + a _ {2} b _ {2}\right) ^ {2} = \left(\left( \begin{array}{c} a _ {1} \\ a _ {2} \end{array} \right) ^ {T} \left( \begin{array}{c} b _ {1} \\ b _ {2} \end{array} \right)\right) ^ {2} = \left(\mathbf {a} ^ {T} \mathbf {b}\right) ^ {2} \\ \end{array}
$$

How about that? The dot product of the transformed vectors is equal to the square of the dot product of the original vectors: $\phi ( \mathbf { a } ) ^ { T } \phi ( \mathbf { b } ) = ( \mathbf { a } ^ { T } \mathbf { b } ) ^ { 2 }$ .

Now here is the key insight: if you apply the transformation $\phi$ to all training instan‐ ces, then the dual problem (see Equation 5-6) will contain the dot product $\phi ( \mathbf { x } ^ { ( i ) } ) ^ { T }$ $\phi ( \mathbf { x } ^ { ( j ) } )$ . But if $\phi$ is the $2 ^ { \mathrm { n d } }$ -degree polynomial transformation defined in Equation 5-8, then you can replace this dot product of transformed vectors simply by $\left( { { \bf { x } } ^ { ( i ) } } ^ { T } { \bf { x } } ^ { ( j ) } \right) ^ { 2 }$ . So you don’t actually need to transform the training instances at all: just replace the dot product by its square in Equation 5-6. The result will be strictly the same as if you went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick.

The function $K ( \mathbf { a } , \mathbf { b } ) = ( \mathbf { a } ^ { T } \mathbf { b } ) ^ { 2 }$ is called a $2 ^ { \mathrm { n d } }$ -degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product $\phi ( \mathbf { a } ) ^ { T } ~ \phi ( \mathbf { b } )$ based only on the original vectors a and b, without having to compute (or even to know about) the transformation $\phi$ . Equation 5-10 lists some of the most commonly used kernels.

Equation 5-10. Common kernels

Linear:

Polynomial:

Gaussian RBF:

Sigmoid:

# Mercer’s Theorem

According to Mercer’s theorem, if a function $K ( \mathbf { a } , \mathbf { b } )$ respects a few mathematical con‐ ditions called Mercer’s conditions (K must be continuous, symmetric in its arguments so $K ( { \bf a } , { \bf b } ) = K ( { \bf b } , { \bf a } )$ , etc.), then there exists a function $\phi$ that maps a and $\mathbf { b }$ into another space (possibly with much higher dimensions) such that $K ( \mathbf { a } , \mathbf { b } ) = \phi ( \mathbf { a } ) ^ { T } \phi ( \mathbf { b } )$ . So you can use $K$ as a kernel since you know $\phi$ exists, even if you don’t know what $\phi$ is. In the case of the Gaussian RBF kernel, it can be shown that $\phi$ actually maps each training instance to an infinite-dimensional space, so it’s a good thing you don’t need to actually perform the mapping!

Note that some frequently used kernels (such as the Sigmoid kernel) don’t respect all of Mercer’s conditions, yet they generally work well in practice.

There is still one loose end we must tie. Equation 5-7 shows how to go from the dual solution to the primal solution in the case of a linear SVM classifier, but if you apply the kernel trick you end up with equations that include $\phi ( \boldsymbol { x } ^ { ( i ) } )$ . In fact, w must have the same number of dimensions as $\phi ( \boldsymbol { x } ^ { ( i ) } )$ , which may be huge or even infinite, so you can’t compute it. But how can you make predictions without knowing w? Well, the good news is that you can plug in the formula for w from Equation 5-7 into the deci‐ sion function for a new instance $\mathbf { X } ^ { ( n ) }$ , and you get an equation with only dot products between input vectors. This makes it possible to use the kernel trick, once again (Equation 5-11).

Equation 5-11. Making predictions with a kernelized SVM

$$
\begin{array}{l} h _ {\widehat {\mathbf {w}}, \widehat {b}} \left(\phi \left(\mathbf {x} ^ {(n)}\right)\right) = \widehat {\mathbf {w}} ^ {T} \phi \left(\mathbf {x} ^ {(n)}\right) + \widehat {b} = \left(\sum_ {i = 1} ^ {m} \widehat {\alpha} ^ {(i)} t ^ {(i)} \phi \left(\mathbf {x} ^ {(i)}\right)\right) ^ {T} \phi \left(\mathbf {x} ^ {(n)}\right) + \widehat {b} \\ = \sum_ {i = 1} ^ {m} \hat {\alpha} ^ {(i)} t ^ {(i)} \left(\phi \left(\mathbf {x} ^ {(i)}\right) ^ {T} \phi \left(\mathbf {x} ^ {(n)}\right)\right) + \hat {b} \\ = \sum_ {i = 1} ^ {m} \hat {\alpha} ^ {(i)} t ^ {(i)} K \left(\mathbf {x} ^ {(i)}, \mathbf {x} ^ {(n)}\right) + \hat {b} \\ \hat {\alpha} ^ {(i)} > 0 \\ \end{array}
$$

Note that since $\alpha ^ { ( i ) } \neq 0$ only for support vectors, making predictions involves comput‐ ing the dot product of the new input vector $\mathbf { x } ^ { ( n ) }$ with only the support vectors, not all the training instances. Of course, you also need to compute the bias term $\hat { b }$ , using the same trick (Equation 5-12).

Equation 5-12. Computing the bias term using the kernel trick

$$
\begin{array}{l} \hat{b}  =   \frac{1}{n_{s}}\sum_{\substack{i = 1\\ \hat{\alpha}^{(i)} > 0}}^{m}\left(t^{(i)} - \widehat{\mathbf{w}}^{T}\phi \bigl(\mathbf{x}^{(i)}\bigr)\right) = \frac{1}{n_{s}}\sum_{\substack{i = 1\\ \hat{\alpha}^{(i)} > 0}}^{m}\left(t^{(i)} - \left(\sum_{j = 1}^{m}\hat{\alpha}^{(j)}t^{(j)}\phi \bigl(\mathbf{x}^{(j)}\bigr)\right)^{T}\phi \bigl(\mathbf{x}^{(i)}\bigr)\right) \\ = \frac{1}{n_{s}}\sum_{\substack{i = 1\\ \hat{\alpha}^{(i)} > 0}}^{m}\left(t^{(i)} - \sum_{\substack{j = 1\\ \hat{\alpha}^{(j)} > 0}}^{m}\hat{\alpha}^{(j)}t^{(j)}K\big(\mathbf{x}^{(i)},\mathbf{x}^{(j)}\big)\right) \\ \end{array}
$$

If you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side effect of the kernel trick.

# Online SVMs

Before concluding this chapter, let’s take a quick look at online SVM classifiers (recall that online learning means learning incrementally, typically as new instances arrive).

For linear SVM classifiers, one method is to use Gradient Descent (e.g., using SGDClassifier) to minimize the cost function in Equation 5-13, which is derived from the primal problem. Unfortunately it converges much more slowly than the methods based on QP.

Equation 5-13. Linear SVM classier cost function

$$
J (\mathbf {w}, b) = \frac {1}{2} \mathbf {w} ^ {T} \mathbf {w} + C \sum_ {i = 1} ^ {m} \max  \left(0, 1 - t ^ {(i)} \left(\mathbf {w} ^ {T} \mathbf {x} ^ {(i)} + b\right)\right)
$$

The first sum in the cost function will push the model to have a small weight vector w, leading to a larger margin. The second sum computes the total of all margin viola‐ tions. An instance’s margin violation is equal to 0 if it is located off the street and on the correct side, or else it is proportional to the distance to the correct side of the street. Minimizing this term ensures that the model makes the margin violations as small and as few as possible

# Hinge Loss

The function $m a x ( 0 , 1 - t )$ is called the hinge loss function (represented below). It is equal to 0 when $t \geq 1$ . Its derivative (slope) is equal to $^ { - 1 }$ if $t < 1$ and 0 if $t > 1$ . It is not differentiable at $t = 1$ , but just like for Lasso Regression (see “Lasso Regression” on page 139) you can still use Gradient Descent using any subderivative at $t = 1$ (i.e., any value between $^ { - 1 }$ and 0).

![](images/0b27f9e72c5b9f827fea0c0cc86b8d34f8434388bbece035cfcd993ab1312789.jpg)

It is also possible to implement online kernelized SVMs—for example, using “Incre‐ mental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and Active Learning.”9 However, these are implemented in Matlab and $\mathrm { C } { + } { + }$ . For largescale nonlinear problems, you may want to consider using neural networks instead (see Part II).

# Exercises

1. What is the fundamental idea behind Support Vector Machines?   
2. What is a support vector?   
3. Why is it important to scale the inputs when using SVMs?   
4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?   
5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?   
6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease γ (gamma)? What about C?   
7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?   
8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.   
9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 10 digits. You may

want to tune the hyperparameters using small validation sets to speed up the pro‐ cess. What accuracy can you reach?

10. Train an SVM regressor on the California housing dataset.

Solutions to these exercises are available in ???.

![](images/757ff93f2cbcdedc4d73d9a0f904dc2032803845a81439c26001afad4579f9e6.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 6 in the final release of the book.

Like SVMs, Decision Trees are versatile Machine Learning algorithms that can per‐ form both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets. For example, in Chap‐ ter 2 you trained a DecisionTreeRegressor model on the California housing dataset, fitting it perfectly (actually overfitting it).

Decision Trees are also the fundamental components of Random Forests (see Chap‐ ter 7), which are among the most powerful Machine Learning algorithms available today.

In this chapter we will start by discussing how to train, visualize, and make predic‐ tions with Decision Trees. Then we will go through the CART training algorithm used by Scikit-Learn, and we will discuss how to regularize trees and use them for regression tasks. Finally, we will discuss some of the limitations of Decision Trees.

# Training and Visualizing a Decision Tree

To understand Decision Trees, let’s just build one and take a look at how it makes pre‐ dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4):

from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier

```python
iris = load_iris()
X = iris.data[:, 2:] # petal length and width
y = iris.target
tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X, y) 
```

You can visualize the trained Decision Tree by first using the export_graphviz() method to output a graph definition file called iris_tree.dot:

from sklearn.tree import export_graphviz   
export_graphviz( tree_clf, out_file $\equiv$ image_path("iris_tree.dot"), feature_names $\equiv$ iris.feature_names[2:], class_names $\equiv$ iris.target_names, rounded $\equiv$ True, filled $\equiv$ True

Then you can convert this .dot file to a variety of formats such as PDF or PNG using the dot command-line tool from the graphviz package.1 This command line converts the .dot file to a .png image file:

```batch
$ dot -Tpng iris_tree.dot -o iris_tree.png 
```

Your first decision tree looks like Figure 6-1.

![](images/5d42e7cc665c035a22746e5329b0364d2484e675cc01988a118ce04ca3343fb4.jpg)  
Figure 6-1. Iris Decision Tree

# Making Predictions

Let’s see how the tree represented in Figure 6-1 makes predictions. Suppose you find an iris flower and you want to classify it. You start at the root node (depth 0, at the top): this node asks whether the flower’s petal length is smaller than $2 . 4 5 ~ \mathrm { c m }$ . If it is, then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf node (i.e., it does not have any children nodes), so it does not ask any questions: you can simply look at the predicted class for that node and the Decision Tree predicts that your flower is an Iris-Setosa (class=setosa).

Now suppose you find another flower, but this time the petal length is greater than $2 . 4 5 ~ \mathrm { c m }$ . You must move down to the root’s right child node (depth 1, right), which is not a leaf node, so it asks another question: is the petal width smaller than $1 . 7 5 \mathrm { c m } ?$ If it is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely an Iris-Virginica (depth 2, right). It’s really that simple.

![](images/523f69f37e0d3e0e639325bee7c0fcd0502196c9b895f168d18ceb75e45b556f.jpg)

One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don’t require feature scaling or centering at all.

A node’s samples attribute counts how many training instances it applies to. For example, 100 training instances have a petal length greater than $2 . 4 5 ~ \mathrm { c m }$ (depth 1, right), among which 54 have a petal width smaller than $1 . 7 5 ~ \mathrm { c m }$ (depth 2, left). A node’s value attribute tells you how many training instances of each class this node applies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-Versicolor, and 45 Iris-Virginica. Finally, a node’s gini attribute measures its impur‐ ity: a node is “pure” (gini=0) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to Iris-Setosa training instances, it is pure and its gini score is 0. Equation 6-1 shows how the training algo‐ rithm computes the gini score $G _ { i }$ of the $\mathrm { i ^ { t h } }$ node. For example, the depth-2 left node has a gini score equal to $1 - ( 0 / 5 4 ) ^ { 2 } - ( 4 9 / 5 4 ) ^ { 2 } - ( 5 / 5 4 ) ^ { 2 } \approx 0 . 1 6 8 .$ Another impurity measure is discussed shortly.

Equation 6-1. Gini impurity

$$
G _ {i} = 1 - \sum_ {k = 1} ^ {n} p _ {i, k} ^ {2}
$$

• $\mathcal { P } _ { i , k }$ is the ratio of class $k$ instances among the training instances in the $i ^ { \mathrm { t h } }$ node.

![](images/05ec95933ae189f557b73bcda4588ae2c0b9e83b4681833afe662845af2a6582.jpg)

Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children (i.e., questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two chil‐ dren.

Figure 6-2 shows this Decision Tree’s decision boundaries. The thick vertical line rep‐ resents the decision boundary of the root node (depth 0): petal length $= 2 . 4 5 \ \mathrm { c m }$ . Since the left area is pure (only Iris-Setosa), it cannot be split any further. However, the right area is impure, so the depth-1 right node splits it at petal width $= 1 . 7 5 ~ \mathrm { c m }$ (represented by the dashed line). Since max_depth was set to 2, the Decision Tree stops right there. However, if you set max_depth to 3, then the two depth-2 nodes would each add another decision boundary (represented by the dotted lines).

![](images/82975d8530e3753e94d60d7c77d6631888f92fc2c3d8751104d0787dc98636a4.jpg)  
Figure 6-2. Decision Tree decision boundaries

# Model Interpretation: White Box Versus Black Box

As you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐ pret. Such models are often called white box models. In contrast, as we will see, Ran‐ dom Forests or neural networks are generally considered black box models. They make great predictions, and you can easily check the calculations that they performed to make these predictions; nevertheless, it is usually hard to explain in simple terms why the predictions were made. For example, if a neural network says that a particu‐ lar person appears on a picture, it is hard to know what actually contributed to this prediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her shoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide nice and simple classification rules that can even be applied manually if need be (e.g., for flower classification).

# Estimating Class Probabilities

A Decision Tree can also estimate the probability that an instance belongs to a partic‐ ular class $k \colon$ first it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class $k$ in this node. For example, suppose you have found a flower whose petals are $5 \ c m$ long and $1 . 5 ~ \mathrm { c m }$ wide. The corre‐ sponding leaf node is the depth-2 left node, so the Decision Tree should output the following probabilities: $0 \%$ for Iris-Setosa (0/54), $9 0 . 7 \%$ for Iris-Versicolor (49/54), and $9 . 3 \%$ for Iris-Virginica (5/54). And of course if you ask it to predict the class, it should output Iris-Versicolor (class 1) since it has the highest probability. Let’s check this:

```python
>>> tree clf.predict(probab([[5, 1.5]])
array([[0, 0.90740741, 0.09259259]]) 
```

>>> tree_clf.predict([[5, 1.5]])

array([1])

Perfect! Notice that the estimated probabilities would be identical anywhere else in the bottom-right rectangle of Figure 6-2—for example, if the petals were $6 ~ \mathrm { c m }$ long and $1 . 5 ~ \mathrm { c m }$ wide (even though it seems obvious that it would most likely be an Iris-Virginica in this case).

# The CART Training Algorithm

Scikit-Learn uses the Classication And Regression Tree (CART) algorithm to train Decision Trees (also called “growing” trees). The idea is really quite simple: the algo‐ rithm first splits the training set in two subsets using a single feature $k$ and a thres‐ hold $t _ { k }$ (e.g., “petal length $\leq 2 . 4 5 ~ \mathrm { c m } ^ { * }$ ). How does it choose $k$ and $t _ { k } ?$ It searches for the pair $( k , t _ { k } )$ that produces the purest subsets (weighted by their size). The cost function that the algorithm tries to minimize is given by Equation 6-2.

Equation 6-2. CART cost function for classication

$$
J (k, t _ {k}) = \frac {m _ {\text {l e f t}}}{m} G _ {\text {l e f t}} + \frac {m _ {\text {r i g h t}}}{m} G _ {\text {r i g h t}}
$$

where $\left\{ { \begin{array} { l } { G _ { \mathrm { l e f t / r i g h t } } } \\ { m _ { \mathrm { l e f t / r i g h t } } } \end{array} } \right.$ measures the impurity of the left/right subset, is the number of instances in the left/right subset.

Once it has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐ ches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity. A few other hyperparameters (described in a moment) control additional stopping conditions (min_samples_split, min_sam ples_leaf, min_weight_fraction_leaf, and max_leaf_nodes).

![](images/1c12afed085ad01cc6b7be5ac9b18c721aa3203820ff19fe922e0e8cb7c10f90.jpg)

As you can see, the CART algorithm is a greedy algorithm: it greed‐ ily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.

Unfortunately, finding the optimal tree is known to be an NP-Complete problem:2 it requires $O ( \exp ( m ) )$ time, making the prob‐ lem intractable even for fairly small training sets. This is why we must settle for a “reasonably good” solution.

# Computational Complexity

Making predictions requires traversing the Decision Tree from the root to a leaf. Decision Trees are generally approximately balanced, so traversing the Decision Tree requires going through roughly $O ( l o g _ { 2 } ( m ) )$ nodes.3 Since each node only requires checking the value of one feature, the overall prediction complexity is just $O ( l o g _ { 2 } ( m ) )$ , independent of the number of features. So predictions are very fast, even when deal‐ ing with large training sets.

However, the training algorithm compares all features (or less if max_features is set) on all samples at each node. This results in a training complexity of $O ( n \times m l o g ( m ) )$ . For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set presort=True), but this slows down training con‐ siderably for larger training sets.

# Gini Impurity or Entropy?

By default, the Gini impurity measure is used, but you can select the entropy impurity measure instead by setting the criterion hyperparameter to "entropy". The concept of entropy originated in thermodynamics as a measure of molecular disorder: entropy approaches zero when molecules are still and well ordered. It later spread to a wide variety of domains, including Shannon’s information theory, where it measures the average information content of a message:4 entropy is zero when all messages are identical. In Machine Learning, it is frequently used as an impurity measure: a set’s

entropy is zero when it contains instances of only one class. Equation 6-3 shows the definition of the entropy of the $\mathrm { i ^ { t h } }$ node. For example, the depth-2 left node in Figure 6-1 has an entropy equal to $\begin{array} { r } { - \frac { 4 9 } { 5 4 } \log _ { 2 } \left( \frac { 4 9 } { 5 4 } \right) - \frac { 5 } { 5 4 } \log _ { 2 } \left( \frac { 5 } { 5 4 } \right) \approx 0 . 4 4 5 } \end{array}$

Equation 6-3. Entropy

$$
H_{i} = -\sum_{\substack{k = 1\\ p_{i,k}\neq 0}}^{n}p_{i,k}\log_{2}\left(p_{i,k}\right)
$$

So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.5

# Regularization Hyperparameters

Decision Trees make very few assumptions about the training data (as opposed to lin‐ ear models, which obviously assume that the data is linear, for example). If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely, and most likely overfitting it. Such a model is often called a nonparametric model, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a parametric model such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).

To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training. As you know by now, this is called regularization. The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the max_depth hyperparameter (the default value is None, which means unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting.

The DecisionTreeClassifier class has a few other parameters that similarly restrict the shape of the Decision Tree: min_samples_split (the minimum number of sam‐ ples a node must have before it can be split), min_samples_leaf (the minimum num‐ ber of samples a leaf node must have), min_weight_fraction_leaf (same as min_samples_leaf but expressed as a fraction of the total number of weighted

instances), max_leaf_nodes (maximum number of leaf nodes), and max_features (maximum number of features that are evaluated for splitting at each node). Increas‐ ing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.

![](images/0b7bc884b200aa49e4a51bb3986eb1b2f0baeda249f95b818cd84c6579d1b1ab.jpg)

Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically signicant. Stan‐ dard statistical tests, such as the $\chi ^ { 2 }$ test, are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the $\pmb { \mathit { p } }$ - value, is higher than a given threshold (typically $5 \%$ , controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.

![](images/975373da11cbfb7d74088af09e1644ebd84ef8a00ee827d9ffb4ffb2bfa74887.jpg)  
Figure 6-3 shows two Decision Trees trained on the moons dataset (introduced in Chapter 5). On the left, the Decision Tree is trained with the default hyperparameters (i.e., no restrictions), and on the right the Decision Tree is trained with min_sam ples_leaf $\mathtt { = } 4$ . It is quite obvious that the model on the left is overfitting, and the model on the right will probably generalize better.   
Figure 6-3. Regularization using min_samples_leaf

# Regression

Decision Trees are also capable of performing regression tasks. Let’s build a regres‐ sion tree using Scikit-Learn’s DecisionTreeRegressor class, training it on a noisy quadratic dataset with max_depth $^ { = 2 }$ :

from sklearn.tree import DecisionTreeRegressor

```txt
tree_reg = DecisionTreeRegressor(max_depth=2)  
tree_reg.fit(X, y) 
```

The resulting tree is represented on Figure 6-4.

![](images/66e34498666c83cd69278d4996308dc256256489b56311786aece9b2de25987f.jpg)  
Figure 6-4. A Decision Tree for regression

This tree looks very similar to the classification tree you built earlier. The main differ‐ ence is that instead of predicting a class in each node, it predicts a value. For example, suppose you want to make a prediction for a new instance with $x _ { 1 } = 0 . 6$ . You traverse the tree starting at the root, and you eventually reach the leaf node that predicts value=0.1106. This prediction is simply the average target value of the 110 training instances associated to this leaf node. This prediction results in a Mean Squared Error (MSE) equal to 0.0151 over these 110 instances.

This model’s predictions are represented on the left of Figure 6-5. If you set max_depth $^ { = 3 }$ , you get the predictions represented on the right. Notice how the pre‐ dicted value for each region is always the average target value of the instances in that region. The algorithm splits each region in a way that makes most training instances as close as possible to that predicted value.

![](images/b7b4cd7e1f6651b93c1eeed541419c4994de3c8da287829cc86eabebcfb9c773.jpg)

![](images/21749b5d2477d46aa391e5bb43fea95f2127863ff2c12687a6127b3f86d438ac.jpg)  
Figure 6-5. Predictions of two Decision Tree regression models

The CART algorithm works mostly the same way as earlier, except that instead of try‐ ing to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE. Equation 6-4 shows the cost function that the algorithm tries to minimize.

Equation 6-4. CART cost function for regression

$$
J (k, t _ {k}) = \frac {m _ {\mathrm {l e f t}}}{m} \mathrm {M S E} _ {\mathrm {l e f t}} + \frac {m _ {\mathrm {r i g h t}}}{m} \mathrm {M S E} _ {\mathrm {r i g h t}} \quad \text {w h e r e} \left\{ \begin{array}{l} \mathrm {M S E} _ {\mathrm {n o d e}} = \sum_ {i \in \mathrm {n o d e}} \left(\hat {y} _ {\mathrm {n o d e}} - y ^ {(i)}\right) ^ {2} \\ \hat {y} _ {\mathrm {n o d e}} = \frac {1}{m _ {\mathrm {n o d e}}} \sum_ {i \in \mathrm {n o d e}} y ^ {(i)} \end{array} \right.
$$

Just like for classification tasks, Decision Trees are prone to overfitting when dealing with regression tasks. Without any regularization (i.e., using the default hyperpara‐ meters), you get the predictions on the left of Figure 6-6. It is obviously overfitting the training set very badly. Just setting min_samples_leaf $\mathtt { \Gamma } = 1 \Theta$ results in a much more reasonable model, represented on the right of Figure 6-6.

![](images/4e97e8282de522147250fa3420274c1ca3da911304d0b28806cebab75672a121.jpg)

![](images/a4b8dda71fc89c84ff9306bbdfca953db1e8fe7a60f4dee5272ea7a26025c658.jpg)  
Figure 6-6. Regularizing a Decision Tree regressor

# Instability

Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a simple linearly separable dataset: on the left, a Decision Tree can split it easily, while on the right, after the dataset is rotated by $4 5 ^ { \circ }$ , the decision boundary looks unneces‐ sarily convoluted. Although both Decision Trees fit the training set perfectly, it is very likely that the model on the right will not generalize well. One way to limit this prob‐ lem is to use PCA (see Chapter 8), which often results in a better orientation of the training data.

![](images/e449e4cc79f42167f1e7d4ce2611dc7d652cb4195105e58b261abbd2c8bbc486.jpg)  
Figure 6-7. Sensitivity to training set rotation

More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data. For example, if you just remove the widest Iris-Versicolor from the iris training set (the one with petals $4 . 8 \mathrm { c m }$ long and $1 . 8 \mathrm { c m }$ wide) and train a new Decision Tree, you may get the model represented in Figure 6-8. As you can see, it looks very different from the previous Decision Tree (Figure 6-2). Actually, since the training algorithm used by Scikit-Learn is stochastic6 you may get very different models even on the same training data (unless you set the random_state hyperparameter).

![](images/9e6e8407eec1044db110c8567d11eb893df31930f8915bcbb32d64680a8578cc.jpg)  
Figure 6-8. Sensitivity to training set details

Random Forests can limit this instability by averaging predictions over many trees, as we will see in the next chapter.

# Exercises

1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances?   
2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐ ally lower/greater, or always lower/greater?   
3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?   
4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?   
5. If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?   
6. If your training set contains 100,000 instances, will setting presort=True speed up training?   
7. Train and fine-tune a Decision Tree for the moons dataset.

a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4).   
b. Split it into a training set and a test set using train_test_split().

c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.

d. Train it on the full training set using these hyperparameters, and measure your model’s performance on the test set. You should get roughly $8 5 \%$ to $8 7 \%$ accuracy.

8. Grow a forest.

a. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s ShuffleSplit class for this.   
b. Train one Decision Tree on each subset, using the best hyperparameter values found above. Evaluate these 1,000 Decision Trees on the test set. Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about $8 0 \%$ accuracy.   
c. Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction (you can use SciPy’s mode() function for this). This gives you majority-vote predictions over the test set.   
d. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to $1 . 5 \%$ higher). Congratulations, you have trained a Random Forest classifier!

Solutions to these exercises are available in ???.

# Ensemble Learning and Random Forests

![](images/fe73bfbad709ff90edee8751bacefadbfbe5e5c55509d14a42a38884e898cd9a.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 7 in the final release of the book.

Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre‐ dictors is called an ensemble; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.

For example, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you just obtain the predic‐ tions of all individual trees, then predict the class that gets the most votes (see the last exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest, and despite its simplicity, this is one of the most powerful Machine Learning algo‐ rithms available today.

Moreover, as we discussed in Chapter 2, you will often use Ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor. In fact, the winning solutions in Machine Learn‐ ing competitions often involve several Ensemble methods (most famously in the Net‐ flix Prize competition).

In this chapter we will discuss the most popular Ensemble methods, including bag‐ ging, boosting, stacking, and a few others. We will also explore Random Forests.

# Voting Classiers

Suppose you have trained a few classifiers, each one achieving about $8 0 \%$ accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1).

![](images/2eff7b8145146209ed660e33fee97d8f5e3a881d5f6e35fd6989fe22315b5668.jpg)  
Figure 7-1. Training diverse classiers

A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classi‐ fier is called a hard voting classifier (see Figure 7-2).

![](images/5b85454c41f9a3b4691d88bae9ae2515da7100f0ed1ee640052d17bcce9be63b.jpg)  
Figure 7-2. Hard voting classier predictions

Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a weak learner (mean‐ ing it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.

How is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased coin that has a $5 1 \%$ chance of coming up heads, and $4 9 \%$ chance of coming up tails. If you toss it 1,000 times, you will generally get more or less 510 heads and 490 tails, and hence a majority of heads. If you do the math, you will find that the probability of obtaining a majority of heads after 1,000 tosses is close to $7 5 \%$ . The more you toss the coin, the higher the probability (e.g., with 10,000 tosses, the probability climbs over $9 7 \%$ ). This is due to the law of large numbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the probability of heads $( 5 1 \% )$ . Figure 7-3 shows 10 series of biased coin tosses. You can see that as the number of tosses increases, the ratio of heads approaches $5 1 \%$ . Eventu‐ ally all 10 series end up so close to $5 1 \%$ that they are consistently above $5 0 \%$ .

![](images/6864e1ab2b5b8b9885c6a5f75fab0f67a184bf09a2f7977b5ea1a33d54a93f46.jpg)  
Figure 7-3. e law of large numbers

Similarly, suppose you build an ensemble containing 1,000 classifiers that are individ‐ ually correct only $5 1 \%$ of the time (barely better than random guessing). If you pre‐ dict the majority voted class, you can hope for up to $7 5 \%$ accuracy! However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case since they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.

![](images/463664a08ce78d2c629c24c95260e076c71a528c6054ca7420fcb5280e66f70a.jpg)

Ensemble methods work best when the predictors are as independ‐ ent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.

The following code creates and trains a voting classifier in Scikit-Learn, composed of three diverse classifiers (the training set is the moons dataset, introduced in Chap‐ ter 5):

from sklearnensemble import RandomForestClassifier   
from sklearnensemble import VotingClassifier   
from sklearn.linear_model import LogisticRegression   
from sklearn.svm import SVC   
log_clf $=$ LogisticRegression()   
rnd_clf $=$ RandomForestClassifier()   
svm_clf $=$ SVC()   
voting_clf $=$ VotingClassifier( estimators $\coloneqq$ ['lr', log_clf), ('rf', rnd_clf), ('svc',svm_clf)], voting $\equiv$ 'hard')   
voting_clf.fit(X_train, y_train)

Let’s look at each classifier’s accuracy on the test set:

```csv
>>> from sklearn.metrics import accuracy_score
>>> for clf in (log_clf, rnd_clf,svm_clf,voting_clf):
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
.
...
....
...
...
...
...
...
...
...
...
...
...
>>> ...
>>> from sklearn.metrics import accuracy_score
>>> for clf in (log_clf, rnd_clf,svm_clf,voting_clf):
...     clf.fit(X_train,y_train)
...     y_pred = clf.predict(X_test)
...     print(clf.__class__.__name__,accuracy_score(y_test,y_pred))
... 
LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.888
VotingClassifier 0.904 
```

There you have it! The voting classifier slightly outperforms all the individual classifi‐ ers.

If all classifiers are able to estimate class probabilities (i.e., they have a pre dict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called so voting. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace voting="hard" with voting $=$ "soft" and ensure that all classifiers can estimate class probabilities. This is not the case of the SVC class by default, so you need to set its probability hyperpara‐ meter to True (this will make the SVC class use cross-validation to estimate class prob‐ abilities, slowing down training, and it will add a predict_proba() method). If you

modify the preceding code to use soft voting, you will find that the voting classifier achieves over $9 1 . 2 \%$ accuracy!

# Bagging and Pasting

One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3

In other words, both bagging and pasting allow training instances to be sampled sev‐ eral times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor. This sampling and training process is represented in Figure 7-4.

![](images/70dbb62925661c1e49deb2e520ed370be91ec97684cfb358e0e0022eaae2b902.jpg)  
Figure 7-4. Pasting/bagging training set sampling and training

Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual

predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.4 Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.

As you can see in Figure 7-4, predictors can all be trained in parallel, via different CPU cores or even different servers. Similarly, predictions can be made in parallel. This is one of the reasons why bagging and pasting are such popular methods: they scale very well.

# Bagging and Pasting in Scikit-Learn

Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas sifier class (or BaggingRegressor for regression). The following code trains an ensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐ domly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting instead, just set bootstrap=False). The n_jobs param‐ eter tells Scikit-Learn the number of CPU cores to use for training and predictions (–1 tells Scikit-Learn to use all available cores):

```python
from sklearnensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier 
```

```python
bag clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)  
bag clf.fit(X_train, y_train)  
y_pred = bag clf.predict(X_test) 
```

![](images/9049cdbdd79e4d9a4dbe412f81b45938e32d4b2775b2f66d30d0ad0dd6d6ba05.jpg)

The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class proba‐ bilities (i.e., if it has a predict_proba() method), which is the case with Decision Trees classifiers.

Figure 7-5 compares the decision boundary of a single Decision Tree with the deci‐ sion boundary of a bagging ensemble of 500 trees (from the preceding code), both trained on the moons dataset. As you can see, the ensemble’s predictions will likely generalize much better than the single Decision Tree’s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular).

![](images/cae975634d5519e95509a1e8e6de601ec3aada152940728d243f005b1f2e5f55.jpg)  
Figure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees

Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but this also means that predictors end up being less correlated so the ensemble’s variance is reduced. Overall, bagging often results in better models, which explains why it is gen‐ erally preferred. However, if you have spare time and CPU power you can use crossvalidation to evaluate both bagging and pasting and select the one that works best.

# Out-of-Bag Evaluation

With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the training set. This means that only about $6 3 \%$ of the training instances are sampled on average for each predictor.6 The remaining $3 7 \%$ of the training instances that are not sampled are called out-of-bag (oob) instances. Note that they are not the same $3 7 \%$ for all predictors.

Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evaluations of each predictor.

In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation score is available through the oob_score_ variable:

```python
>>> bag_clf = BaggingClassifier(  
... DecisionTreeClassifier(), n_estimators=500,  
... bootstrap=True, n_jobs=-1, oob_score=True)  
...  
>>> bag_clf.fit(X_train, y_train) 
```

```txt
>>bag_clf.oob_score_ 0.90133333333333332 
```

According to this oob evaluation, this BaggingClassifier is likely to achieve about $9 0 . 1 \%$ accuracy on the test set. Let’s verify this:

```python
>>> from sklearn.metrics import accuracy_score
>>> y_pred = bag_clf.predict(X_test)
>>> accuracy_score(y_test, y_pred)
0.9120000000000003 
```

We get $9 1 . 2 \%$ accuracy on the test set—close enough!

The oob decision function for each training instance is also available through the oob_decision_function_ variable. In this case (since the base estimator has a pre dict_proba() method) the decision function returns the class probabilities for each training instance. For example, the oob evaluation estimates that the first training instance has a $6 8 . 2 5 \%$ probability of belonging to the positive class (and $3 1 . 7 5 \%$ of belonging to the negative class):

```txt
>>bag_clf.oob_decision_function_   
array([0.31746032，0.68253968]， [0.34117647，0.65882353]， [1. ，0.]， [1. ，0.]， [0.03108808，0.96891192]， [0.57291667，0.42708333]]）
```

# Random Patches and Random Subspaces

The BaggingClassifier class supports sampling the features as well. This is con‐ trolled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.

This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam ples=1.0) but sampling features (i.e., bootstrap_features $\displaystyle { \overline { { \mathbf { \alpha } } } } =$ True and/or max_fea tures smaller than 1.0) is called the Random Subspaces method.8

Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.

# Random Forests

As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass‐ ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is a RandomForestRegressor class for regression tasks). The following code trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:

from sklearn.ensemble import RandomForestClassifier   
bag_clf $=$ BaggingClassifier( DecisionTreeClassifier(splitter="random", max_leaf_nodes $\begin{array} { r l } { : = { } } & { { } } \end{array}$ ), n_estimator $\scriptstyle \mathsf { S } = \quad$ , max_samples=1.0, bootstrap=True, n_jobs $= - 1$ )   
```python
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)  
rnd_clf.fit(X_train, y_train)  
y_pred_rf = rnd_clf.predict(X_test) 
```

With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐ meters of a BaggingClassifier to control the ensemble itself.11

The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:

# Extra-Trees

When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).

A forest of such extremely random trees is simply called an Extremely Randomized Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.

You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API as the RandomForestRegressor class.

![](images/b3d0f08346f776bdf9f5c360d2f3ca7ace6c8e2373608323bedf719c32768183.jpg)

It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. Gen‐ erally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using grid search).

# Feature Importance

Yet another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it (see Chapter 6).

Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can access the result using the feature_importances_ variable. For example, the following code trains a RandomForestClassifier on the iris dataset (introduced in Chapter 4) and outputs each feature’s importance. It seems that the most important features are the petal length $( 4 4 \% )$ and width $( 4 2 \% )$ , while sepal length and width are rather unim‐ portant in comparison ( $1 1 \%$ and $2 \%$ , respectively).

```python
>>> from sklearnDatasets import load_iris
>>> iris = load_iris()
>>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
>>> rnd_clf.fit(iris["data"], iris["target"]
>>> for name, score in zip(iris["feature_names"], rnd_clf feature-importantances():
...     print(name, score)
...
sepal length (cm) 0.112492250999
sepal width (cm) 0.0231192882825
petal length (cm) 0.441030464364
petal width (cm) 0.423357996355 
```

Similarly, if you train a Random Forest classifier on the MNIST dataset (introduced in Chapter 3) and plot each pixel’s importance, you get the image represented in Figure 7-6.

![](images/4c0ebc6734428066768eb9be1ebc50d48444f52e857d3e511afeb8863378e4c7.jpg)  
Figure 7-6. MNIST pixel importance (according to a Random Forest classier)

Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection.

# Boosting

Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede‐ cessor. There are many boosting methods available, but by far the most popular are

AdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let’s start with Ada‐ Boost.

# AdaBoost

One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predic‐ tors focusing more and more on the hard cases. This is the technique used by Ada‐ Boost.

For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on (see Figure 7-7).

![](images/44a94d9cc1f5601a5be682e0f24dec99748deca9da97670110badda894c4d659.jpg)  
Figure 7-7. AdaBoost sequential training with instance weight updates

Figure 7-8 shows the decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor is a highly regularized SVM classifier with an RBF kernel14). The first classifier gets many instances wrong, so their weights

get boosted. The second classifier therefore does a better job on these instances, and so on. The plot on the right represents the same sequence of predictors except that the learning rate is halved (i.e., the misclassified instance weights are boosted half as much at every iteration). As you can see, this sequential learning technique has some similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better.

![](images/edc6e385e69b3252ffb1f0727b076f683ab5383ca7bcc1e07beac8ae84037476.jpg)  
Figure 7-8. Decision boundaries of consecutive predictors

Once all predictors are trained, the ensemble makes predictions very much like bag‐ ging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.

![](images/93254a5a97e56959c35a832c46fca7d45da773217cf9ac4c7acdd94e3105e74d.jpg)

There is one important drawback to this sequential learning techni‐ que: it cannot be parallelized (or only partially), since each predic‐ tor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bag‐ ging or pasting.

Let’s take a closer look at the AdaBoost algorithm. Each instance weight $\boldsymbol { w } ^ { ( i ) }$ is initially set to 1 $\frac { 1 } { m }$ . A first predictor is trained and its weighted error rate $r _ { 1 }$ is computed on the training set; see Equation 7-1.

Equation 7-1. Weighted error rate of the jth predictor

∑m w i i = 1 y i ≠ y i r j = ∑m w i where $\hat { y } _ { j } ^ { ( i ) }$ is the $j ^ { \mathrm { t h } }$ predictor’s prediction for the $i ^ { \mathrm { t h } }$ instance. i = 1

The predictor’s weight $\alpha _ { j }$ is then computed using Equation 7-2, where $\eta$ is the learn‐ ing rate hyperparameter (defaults to 1).15 The more accurate the predictor is, the higher its weight will be. If it is just guessing randomly, then its weight will be close to zero. However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.

Equation 7-2. Predictor weight

$$
\alpha_ {j} = \eta \log \frac {1 - r _ {j}}{r _ {j}}
$$

Next the instance weights are updated using Equation 7-3: the misclassified instances are boosted.

Equation 7-3. Weight update rule

for i = 1, 2, ⋯, m

$$
w ^ {(i)} \leftarrow \left\{ \begin{array}{l l} w ^ {(i)} & \text {i f} \widehat {y} _ {j} ^ {(i)} = y ^ {(i)} \\ w ^ {(i)} \exp \left(\alpha_ {j}\right) & \text {i f} \widehat {y} _ {j} ^ {(i)} \neq y ^ {(i)} \end{array} \right.
$$

Then all the instance weights are normalized (i.e., divided by $\Sigma _ { i = 1 } ^ { m } \boldsymbol { w } ^ { ( i ) } ,$ ).

Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.

To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights $\alpha _ { j } .$ The predicted class is the one that receives the majority of weighted votes (see Equation 7-4).

Equation 7-4. AdaBoost predictions

$\hat { y } ( { \mathbf x } ) = \mathop { \mathrm { \ a r g m a x } } _ { k } \sum _ { \substack { j = 1 } } ^ { N } \alpha _ { j }$ where $N$ is the number of predictors. y j x = k

Scikit-Learn actually uses a multiclass version of AdaBoost called $S A M M E ^ { 1 6 }$ (which stands for Stagewise Additive Modeling using a Multiclass Exponential loss function). When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the predictors can estimate class probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use a variant of SAMME called SAMME.R (the R stands for “Real”), which relies on class probabilities rather than predictions and generally performs better.

The following code trains an AdaBoost classifier based on 200 Decision Stumps using Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an Ada BoostRegressor class). A Decision Stump is a Decision Tree with max_depth $^ { 1 = 1 }$ —in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class:

from sklearn.ensemble import AdaBoostClassifier   
```python
ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm="SAMME.R", learning_rate=0.5)  
ada_clf.fit(X_train, y_train) 
```

![](images/cf41a58720455b0910b56d80b835dddcd9de58402b6419e9086f58c48217c01c.jpg)

If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regulariz‐ ing the base estimator.

# Gradient Boosting

Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.

Let’s go through a simple regression example using Decision Trees as the base predic‐ tors (of course Gradient Boosting also works great with regression tasks). This is called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train‐ ing set):

from sklearn.tree import DecisionTreeRegressor

tree_reg1 $=$ DecisionTreeRegressor(max_depth $\scriptstyle 1 = \atop -$ ) tree_reg1.fit(X, y)

Now train a second DecisionTreeRegressor on the residual errors made by the first predictor:

```txt
y2 = y - tree_reg1.predict(X)  
tree_reg2 = DecisionTreeRegressor(max_depth=2)  
tree_reg2.fit(X, y2) 
```

Then we train a third regressor on the residual errors made by the second predictor:

```txt
y3 = y2 - tree_reg2.predict(X)  
tree_reg3 = DecisionTreeRegressor(max_depth=2)  
tree_reg3.fit(X, y3) 
```

Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:

y_pred $=$ sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))

Figure 7-9 represents the predictions of these three trees in the left column, and the ensemble’s predictions in the right column. In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. In the second row, a new tree is trained on the residual errors of the first tree. On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first two trees. Similarly, in the third row another tree is trained on the residual errors of the second tree. You can see that the ensemble’s predictions gradually get better as trees are added to the ensemble.

A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe gressor class. Much like the RandomForestRegressor class, it has hyperparameters to control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on), as well as hyperparameters to control the ensemble training, such as the number of trees (n_estimators). The following code creates the same ensemble as the previous one:

from sklearn.ensemble import GradientBoostingRegressor

gbrt $=$ GradientBoostingRegressor(max_depth $\scriptstyle 1 = \atop -$ , n_estimator $\scriptstyle \mathsf { S } =$ , learning_rate=1.0) gbrt.fit(X, y)

![](images/d41a19c9595873ff6ba7fdbb2f90e4b4e6a31540472a04935a929611a2d3049f.jpg)  
Figure 7-9. Gradient Boosting

The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐ ing set, but the predictions will usually generalize better. This is a regularization tech‐ nique called shrinkage. Figure 7-10 shows two GBRT ensembles trained with a low learning rate: the one on the left does not have enough trees to fit the training set, while the one on the right has too many trees and overfits the training set.

![](images/84c52be7ad7fc3098da0b1d58e86d742c6e3a0ef5f35b323eac62b6fa198c89d.jpg)  
Figure 7-10. GBRT ensembles with not enough predictors (le) and too many (right)

In order to find the optimal number of trees, you can use early stopping (see Chap‐ ter 4). A simple way to implement this is to use the staged_predict() method: it returns an iterator over the predictions made by the ensemble at each stage of train‐ ing (with one tree, two trees, etc.). The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the opti‐ mal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:

import numpy as np   
from sklearn.model_selection import train_test_split   
from sklearn.metrics import mean_squared_error   
X_train, X_val, y_train, y_val = train_test_split(X, y)   
gbrt $=$ GradientBoostingRegressor(max_depth=2, n_estimators=120)   
gbrt.fit(X_train, y_train)   
errors $=$ [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_prediction(X_val)]   
bst_n_estimators $=$ np.argmax errors)   
gbrt_best $=$ GradientBoostingRegressor(max_depth=2,n_estimators $\equiv$ BST_n_estimators)   
gbrt_best.fit(X_train, y_train)

The validation errors are represented on the left of Figure 7-11, and the best model’s predictions are represented on the right.

![](images/07e66218aadbfdb73f58f90e878f5d8faa8e0ff0bae923a512e6f6738044a517.jpg)

![](images/43457683844a955fbce5c5e9bbdae6bb960d6b73e241ee3e42cf3753799fe75a.jpg)  
Figure 7-11. Tuning the number of trees using early stopping

It is also possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). You can do so by setting warm_start=True, which makes Scikit-Learn keep existing trees when the fit() method is called, allowing incremental training. The following code stops training when the validation error does not improve for five iterations in a row:

gbrt $=$ GradientBoostingRegressor(max_depth $\scriptstyle 1 = \atop -$ , warm_start=True)   
```python
min_val_error = float("inf")  
error_going_up = 0  
for n_estimators in range(1, 120):  
    gbrt.n_estimators = n_estimators  
    gbrt.fit(X_train, y_train)  
    y_pred = gbrt.predict(X_val)  
    val_error = mean_squared_error(y_val, y_pred)  
if val_error < min_val_error:  
    min_val_error = val_error  
    error_going_up = 0  
else:  
    error_going_up += 1  
if error_going_up == 5:  
    break # early stopping 
```

The GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies the fraction of training instances to be used for training each tree. For example, if subsample $= 0 . 2 5$ , then each tree is trained on $2 5 \%$ of the training instan‐ ces, selected randomly. As you can probably guess by now, this trades a higher bias for a lower variance. It also speeds up training considerably. This technique is called Stochastic Gradient Boosting.

![](images/49def8e2bb08162c1b55f317f51940e9c062a067c1281f8a279f6e25e319c6ee.jpg)

It is possible to use Gradient Boosting with other cost functions. This is controlled by the loss hyperparameter (see Scikit-Learn’s documentation for more details).

It is worth noting that an optimized implementation of Gradient Boosting is available in the popular python library XGBoost, which stands for Extreme Gradient Boosting. This package was initially developed by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC), and it aims at being extremely fast, scalable and portable. In fact, XGBoost is often an important component of the winning entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:

import xgboost

```txt
xgb_reg = xgboost.XGBoostRegressor()  
xgb_reg.fit(X_train, y_train)  
y_pred = xgb_reg.predict(X_val) 
```

XGBoost also offers several nice features, such as automatically taking care of early stopping:

```python
xgb_reg.fit(X_train, y_train, eval_set=[[X_val, y_val]), early_stopping_rounds=2)  
y_pred = xgb_reg.predict(X_val) 
```

You should definitely check it out!

# Stacking

The last Ensemble method we will discuss in this chapter is called stacking (short for stacked generalization).18 It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? Figure 7-12 shows such an ensemble performing a regression task on a new instance. Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a blender, or a meta learner) takes these predictions as inputs and makes the final prediction (3.0).

![](images/9d293069ec3847837fe20a3fe83cdde65eb0af21a19bb62a798162560704bc1b.jpg)  
Figure 7-12. Aggregating predictions using a blending predictor

To train the blender, a common approach is to use a hold-out set.19 Let’s see how it works. First, the training set is split in two subsets. The first subset is used to train the predictors in the first layer (see Figure 7-13).

![](images/e094611cd23bc84694716b08fa475c1a87d43e42d7f48e5c0820914164c70c91.jpg)  
Figure 7-13. Training the rst layer

Next, the first layer predictors are used to make predictions on the second (held-out) set (see Figure 7-14). This ensures that the predictions are “clean,” since the predictors never saw these instances during training. Now for each instance in the hold-out set

there are three predicted values. We can create a new training set using these predic‐ ted values as input features (which makes this new training set three-dimensional), and keeping the target values. The blender is trained on this new training set, so it learns to predict the target value given the first layer’s predictions.

![](images/d715a53bdd252524021a286ea504a3c3f29d37b6c2b3afe8f31747be2f8c7410.jpg)  
Figure 7-14. Training the blender

It is actually possible to train several different blenders this way (e.g., one using Lin‐ ear Regression, another using Random Forest Regression, and so on): we get a whole layer of blenders. The trick is to split the training set into three subsets: the first one is used to train the first layer, the second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer), and the third one is used to create the training set to train the third layer (using pre‐ dictions made by the predictors of the second layer). Once this is done, we can make a prediction for a new instance by going through each layer sequentially, as shown in Figure 7-15.

![](images/4bbe99cfc931f8539a9806dea641249f2e2860eb170fa096964bb732dc5346de.jpg)  
Figure 7-15. Predictions in a multilayer stacking ensemble

Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard to roll out your own implementation (see the following exercises). Alternatively, you can use an open source implementation such as brew (available at https://github.com/ viisar/brew).

# Exercises

1. If you have trained five different models on the exact same training data, and they all achieve $9 5 \%$ precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?   
2. What is the difference between hard and soft voting classifiers?   
3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?   
4. What is the benefit of out-of-bag evaluation?   
5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Ran‐ dom Forests?   
6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?

7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?   
8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for val‐ idation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?   
9. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Train a classifier on this new training set. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let’s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s pre‐ dictions. How does it compare to the voting classifier you trained earlier?

Solutions to these exercises are available in ???.

# Dimensionality Reduction

![](images/e70213cea2bd2ee8f315a2274284981bfd26717327142a7c3c00cf33de4f9b97.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 8 in the final release of the book.

Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only does this make training extremely slow, it can also make it much harder to find a good solution, as we will see. This problem is often referred to as the curse of dimensionality.

Fortunately, in real-world problems, it is often possible to reduce the number of fea‐ tures considerably, turning an intractable problem into a tractable one. For example, consider the MNIST images (introduced in Chapter 3): the pixels on the image bor‐ ders are almost always white, so you could completely drop these pixels from the training set without losing much information. Figure 7-6 confirms that these pixels are utterly unimportant for the classification task. Moreover, two neighboring pixels are often highly correlated: if you merge them into a single pixel (e.g., by taking the mean of the two pixel intensities), you will not lose much information.

![](images/4fed46f540838a23a626668f14e4555a3c69591030e6d3dd24157b621bd35d25.jpg)

Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its quality), so even though it will speed up training, it may also make your system per‐ form slightly worse. It also makes your pipelines a bit more com‐ plex and thus harder to maintain. So you should first try to train your system with the original data before considering using dimen‐ sionality reduction if training is too slow. In some cases, however, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher per‐ formance (but in general it won’t; it will just speed up training).

Apart from speeding up training, dimensionality reduction is also extremely useful for data visualization (or DataViz). Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. Moreover, DataViz is essential to communicate your conclusions to people who are not data scientists, in particular decision makers who will use your results.

In this chapter we will discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space. Then, we will present the two main approaches to dimensionality reduction (projection and Manifold Learning), and we will go through three of the most popular dimensionality reduction techniques: PCA, Kernel PCA, and LLE.

# The Curse of Dimensionality

We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.

![](images/c1cd03e4d36bb3b33844c2c2d1a572654a4117544e16af872df281bef98e2496.jpg)  
Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2

It turns out that many things behave very differently in high-dimensional space. For example, if you pick a random point in a unit square (a $1 \times 1$ square), it will have only about a $0 . 4 \%$ chance of being located less than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme” along any dimension). But in a 10,000-dimensional unit hypercube (a $1 \times 1 \times \cdots \times 1$ cube, with ten thousand 1s), this probability is greater than $9 9 . 9 9 9 9 9 9 \%$ . Most points in a high-dimensional hypercube are very close to the border.3

Here is a more troublesome difference: if you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional hypercube? Well, the average distance, believe it or not, will be about 408.25 (roughly $\sqrt { 1 , 0 0 0 , 0 0 0 / 6 } ) !$ This is quite counterintuitive: how can two points be so far apart when they both lie within the same unit hypercube? This fact implies that highdimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less relia‐ ble than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it.

In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features (much less than

in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions.

# Main Approaches for Dimensionality Reduction

Before we dive into specific dimensionality reduction algorithms, let’s take a look at the two main approaches to reducing dimensionality: projection and Manifold Learning.

# Projection

In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. This sounds very abstract, so let’s look at an example. In Figure 8-2 you can see a 3D data‐ set represented by the circles.

![](images/c0844929dbae88c43ea9acc2fdbf01bd231f08e95bfb352c4988635a710aedca.jpg)  
Figure 8-2. A 3D dataset lying close to a 2D subspace

Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the high-dimensional (3D) space. Now if we project every training instance perpendicularly onto this subspace (as represented by the short lines con‐ necting the instances to the plane), we get the new 2D dataset shown in Figure 8-3. Ta-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that the axes correspond to new features $z _ { 1 }$ and $z _ { 2 }$ (the coordinates of the projections on the plane).

![](images/650644051431b4e66c04d494e1ad9e6c677632700ba55a547b7a64bec8bc68ad.jpg)  
Figure 8-3. e new 2D dataset aer projection

However, projection is not always the best approach to dimensionality reduction. In many cases the subspace may twist and turn, such as in the famous Swiss roll toy data‐ set represented in Figure 8-4.

![](images/741379e9c6c4343b3010eed7848e60418967dbd50262171b20e64ab6606f751d.jpg)  
Figure 8-4. Swiss roll dataset

Simply projecting onto a plane (e.g., by dropping $x _ { 3 . }$ ) would squash different layers of the Swiss roll together, as shown on the left of Figure 8-5. However, what you really want is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5.

![](images/1dfb66756604506a3a3774d7970ea221cbd01cd79b58d1096a2219dcaa26d6bb.jpg)  
Figure 8-5. Squashing by projecting onto a plane (le) versus unrolling the Swiss roll (right)

# Manifold Learning

The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a $d$ -dimensional manifold is a part of an n-dimensional space (where $d < n$ ) that locally resembles a $d$ -dimensional hyperplane. In the case of the Swiss roll, $d = 2$ and $n = 3$ : it locally resembles a 2D plane, but it is rolled in the third dimension.

Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed.

Once again, think about the MNIST dataset: all handwritten digit images have some similarities. They are made of connected lines, the borders are white, they are more or less centered, and so on. If you randomly generated images, only a ridiculously tiny fraction of them would look like handwritten digits. In other words, the degrees of freedom available to you if you try to create a digit image are dramatically lower than the degrees of freedom you would have if you were allowed to generate any image you wanted. These constraints tend to squeeze the dataset into a lowerdimensional manifold.

The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. For example, in the top row of Figure 8-6 the Swiss roll is split into two classes: in the 3D space (on the left), the decision

boundary would be fairly complex, but in the 2D unrolled manifold space (on the right), the decision boundary is a simple straight line.

However, this assumption does not always hold. For example, in the bottom row of Figure 8-6, the decision boundary is located at $x _ { 1 } = 5$ . This decision boundary looks very simple in the original 3D space (a vertical plane), but it looks more complex in the unrolled manifold (a collection of four independent line segments).

In short, if you reduce the dimensionality of your training set before training a model, it will usually speed up training, but it may not always lead to a better or sim‐ pler solution; it all depends on the dataset.

Hopefully you now have a good sense of what the curse of dimensionality is and how dimensionality reduction algorithms can fight it, especially when the manifold assumption holds. The rest of this chapter will go through some of the most popular algorithms.

![](images/0ec656383f7532f45c1bd225787d891bb738b25b99ad61a81a330b9807291733.jpg)  
Figure 8-6. e decision boundary may not always be simpler with lower dimensions

# PCA

Principal Component Analysis (PCA) is by far the most popular dimensionality reduc‐ tion algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it, just like in Figure 8-2.

# Preserving the Variance

Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. For example, a simple 2D dataset is repre‐ sented on the left of Figure 8-7, along with three different axes (i.e., one-dimensional hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As you can see, the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves very little variance, and the projection onto the dashed line preserves an intermediate amount of variance.

![](images/e1ebae63bfa9c68f24f16efe9feecd357e35294ba961ecb01e77a968b14f0476.jpg)  
Figure 8-7. Selecting the subspace onto which to project

It seems reasonable to select the axis that preserves the maximum amount of var‐ iance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared dis‐ tance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA.4

# Principal Components

PCA identifies the axis that accounts for the largest amount of variance in the train‐ ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional data‐ set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset.

The unit vector that defines the $\mathrm { i ^ { t h } }$ axis is called the $\mathrm { i ^ { t h } }$ principal component (PC). In Figure 8-7, the 1st PC is $\mathbf { c _ { 1 } }$ and the $2 ^ { \mathrm { n d } }$ PC is $\mathbf { c } _ { 2 }$ . In Figure 8-2 the first two PCs are represented by the orthogonal arrows in the plane, and the third PC would be orthogonal to the plane (pointing up or down).

![](images/f425beb105a53924c0a8bcf43c896c329bcdf6d6388b0cd2599dd31d8cfd4810.jpg)

The direction of the principal components is not stable: if you per‐ turb the training set slightly and run PCA again, some of the new PCs may point in the opposite direction of the original PCs. How‐ ever, they will generally still lie on the same axes. In some cases, a pair of PCs may even rotate or swap, but the plane they define will generally remain the same.

So how can you find the principal components of a training set? Luckily, there is a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix X into the matrix multiplication of three matrices $\begin{array} { r } { \textbf { U } \boldsymbol { \Sigma } \textbf { V } ^ { T } , } \end{array}$ , where V contains all the principal components that we are looking for, as shown in Equation 8-1.

Equation 8-1. Principal components matrix

$$
\mathbf {V} = \left( \begin{array}{c c c c} | & | & & | \\ \mathbf {c _ {1}} & \mathbf {c _ {2}} & \dots & \mathbf {c _ {n}} \\ | & | & & | \end{array} \right)
$$

The following Python code uses NumPy’s svd() function to obtain all the principal components of the training set, then extracts the first two PCs:

```python
X_centered = X - X.mean(axis=0)  
U, s, Vt = np.linalg.svd(X_centered)  
c1 = Vt.T[:, 0]  
c2 = Vt.T[:, 1] 
```

![](images/cb03b011ead1681cfba0502730a68bd05d4ef2efc2fbed354ec79c672eb17ad3.jpg)

PCA assumes that the dataset is centered around the origin. As we will see, Scikit-Learn’s PCA classes take care of centering the data for you. However, if you implement PCA yourself (as in the pre‐ ceding example), or if you use other libraries, don’t forget to center the data first.

# Projecting Down to d Dimensions

Once you have identified all the principal components, you can reduce the dimen‐ sionality of the dataset down to $d$ dimensions by projecting it onto the hyperplane defined by the first $d$ principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible. For example, in Figure 8-2 the 3D dataset is projected down to the 2D plane defined by the first two principal com‐ ponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐ tion looks very much like the original 3D dataset.

To project the training set onto the hyperplane, you can simply compute the matrix multiplication of the training set matrix X by the matrix $\mathbf { W } _ { d } ,$ defined as the matrix containing the first $d$ principal components (i.e., the matrix composed of the first $d$ columns of V), as shown in Equation 8-2.

Equation 8-2. Projecting the training set down to d dimensions

$$
\mathbf {X} _ {d - \mathrm {p r o j}} = \mathbf {X W} _ {d}
$$

The following Python code projects the training set onto the plane defined by the first two principal components:

$$
\begin{array}{l} W 2 = V t. T [ :, : 2 ] \\ X 2 D = X _ {\text {c e n t e r e d . d o t}} (W 2) \\ \end{array}
$$

There you have it! You now know how to reduce the dimensionality of any dataset down to any number of dimensions, while preserving as much variance as possible.

# Using Scikit-Learn

Scikit-Learn’s PCA class implements PCA using SVD decomposition just like we did before. The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data):

from sklearn.decomposition import PCA

$$
\begin{array}{l} p c a = P C A (n _ {\text {c o m p o n e n t s}} = 2) \\ X 2 D = p c a. f i t _ {t} r a n s f o r m (X) \\ \end{array}
$$

After fitting the PCA transformer to the dataset, you can access the principal compo‐ nents using the components_ variable (note that it contains the PCs as horizontal vec‐

tors, so, for example, the first principal component is equal to pca.components_.T[:, 0]).

# Explained Variance Ratio

Another very useful piece of information is the explained variance ratio of each prin‐ cipal component, available via the explained_variance_ratio_ variable. It indicates the proportion of the dataset’s variance that lies along the axis of each principal com‐ ponent. For example, let’s look at the explained variance ratios of the first two compo‐ nents of the 3D dataset represented in Figure 8-2:

```txt
>>> PCA.explained_variance_ratio_array([0.84248607, 0.14631839])
```

This tells you that $8 4 . 2 \%$ of the dataset’s variance lies along the first axis, and $1 4 . 6 \%$ lies along the second axis. This leaves less than $1 . 2 \%$ for the third axis, so it is reason‐ able to assume that it probably carries little information.

# Choosing the Right Number of Dimensions

Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., $9 5 \%$ ). Unless, of course, you are reducing dimen‐ sionality for data visualization—in that case you will generally want to reduce the dimensionality down to 2 or 3.

The following code computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve $9 5 \%$ of the training set’s variance:

$\begin{array}{rl} & {\mathrm{pca} = \mathrm{PCA}()}\\ & {\mathrm{pca.fit(X_train)}}\\ & {\mathrm{cumsum} = \mathrm{np.cumsum(pca.explained\_variance\_ratio\_)}}\\ & {\mathrm{d} = \mathrm{np.argmax(cumsum} >= 0.95) + 1} \end{array}$

You could then set n_components ${ \tt = } { \tt d }$ and run PCA again. However, there is a much better option: instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:

$\mathsf{pca} = \mathsf{PCA}(\mathsf{n\_components} = 0.95)$ X_reduced $=$ PCA.fit_transform(X_train)

Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the curve, where the explained variance stops growing fast. You can think of this as the intrinsic dimensionality of the dataset. In this case, you can see that reducing the

dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐ iance.

![](images/0e571186d7fb54f4d7ad109c82d180d6f5bb3ac2e41cd30e52392fdd2a1619df.jpg)  
Figure 8-8. Explained variance as a function of the number of dimensions

# PCA for Compression

Obviously after dimensionality reduction, the training set takes up much less space. For example, try applying PCA to the MNIST dataset while preserving $9 5 \%$ of its var‐ iance. You should find that each instance will have just over 150 features, instead of the original 784 features. So while most of the variance is preserved, the dataset is now less than $2 0 \%$ of its original size! This is a reasonable compression ratio, and you can see how this can speed up a classification algorithm (such as an SVM classifier) tremendously.

It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. Of course this won’t give you back the original data, since the projection lost a bit of information (within the $5 \%$ variance that was dropped), but it will likely be quite close to the original data. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error. For example, the following code compresses the MNIST dataset down to 154 dimensions, then uses the inverse_transform() method to decompress it back to 784 dimensions. Figure 8-9 shows a few digits from the original training set (on the left), and the cor‐ responding digits after compression and decompression. You can see that there is a slight image quality loss, but the digits are still mostly intact.

$\mathrm{pca} = \mathrm{PCA}(\mathrm{n\_components} = 154)$ X_reduced = pca.fit_transform(X_train)  
X Recovered = pca.inverse_transform(X_reduced)

![](images/e841e695698ed4230116c0fb53a4da97fe6bf64071b290f6bbb72e3f33e29c9b.jpg)

![](images/d399bf32c1699533242aaf02d09b450bf3aa7aad846a9cd3580efd5e4e18b640.jpg)  
Figure 8-9. MNIST compression preserving $9 5 \%$ of the variance

The equation of the inverse transformation is shown in Equation 8-3.

Equation 8-3. PCA inverse transformation, back to the original number of dimensions

$$
\mathbf {X} _ {\text {r e c o v e r e d}} = \mathbf {X} _ {d - \text {p r o j}} \mathbf {W} _ {d} ^ {T}
$$

# Randomized PCA

If you set the svd_solver hyperparameter to "randomized", Scikit-Learn uses a sto‐ chastic algorithm called Randomized PCA that quickly finds an approximation of the first $d$ principal components. Its computational complexity is $O ( m \times d ^ { 2 } ) \ + \ O ( d ^ { 3 } )$ , instead of $O ( m \times n ^ { 2 } ) + O ( n ^ { 3 } )$ for the full SVD approach, so it is dramatically faster than full SVD when $d$ is much smaller than n:

```python
rnd_pca = PCA(n_components=154, svdSolver="randomized")  
X_reduced = rnd_pca.fit_transform(X_train) 
```

By default, svd_solver is actually set to "auto": Scikit-Learn automatically uses the randomized PCA algorithm if m or $n$ is greater than 500 and $d$ is less than $8 0 \%$ of m or $n$ , or else it uses the full SVD approach. If you want to force Scikit-Learn to use full SVD, you can set the svd_solver hyperparameter to "full".

# Incremental PCA

One problem with the preceding implementations of PCA is that they require the whole training set to fit in memory in order for the algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have been developed: you can split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is

useful for large training sets, and also to apply PCA online (i.e., on the fly, as new instances arrive).

The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5 to reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like before). Note that you must call the partial_fit() method with each mini-batch rather than the fit() method with the whole training set:

from sklearn.decomposition import IncrementalPCA   
n_batches $= 100$ inc_pca $\equiv$ IncrementalPCA(n_components $\coloneqq 154$ for X_batch in np.array_split(X_train, n_batches): inc_pca_partial_fit(X_batch)   
X redesired $\equiv$ inc_pca.transform(X_train)

Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it. Since the IncrementalPCA class uses only a small part of the array at any given time, the memory usage remains under control. This makes it possible to call the usual fit() method, as you can see in the following code:

X_mm = np.memmap(filename, dtype $^ { 1 \pm }$ "float32", mode="readonly", shape=(m, n))   
```txt
batch_size = m // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_m) 
```

# Kernel PCA

In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly maps instances into a very high-dimensional space (called the feature space), enabling nonlinear classification and regression with Support Vector Machines. Recall that a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space.

It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called Kernel

PCA (kPCA).6 It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.

For example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an RBF kernel (see Chapter 5 for more details about the RBF kernel and the other kernels):

from sklearn.decomposition import KernelPCA

rbf_pca $=$ KernelPCA(n_components $= 2$ , kernel="rbf", gamma=0.04) X_reduced $=$ rbf_pca.fit_transform(X)

Figure 8-10 shows the Swiss roll, reduced to two dimensions using a linear kernel (equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel (Logistic).

![](images/064224316fd6cdda4c801275a451b56ceed9a3c882ba9f097f602dc4a2a4ee4c.jpg)  
Figure 8-10. Swiss roll reduced to 2D using kPCA with various kernels

# Selecting a Kernel and Tuning Hyperparameters

As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel and hyperparameter values. However, dimensionality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can simply use grid search to select the kernel and hyper‐ parameters that lead to the best performance on that task. For example, the following code creates a two-step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification. Then it uses Grid SearchCV to find the best kernel and gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline:

from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline

```python
clf = Pipeline([ ("kPCA", KernelPCA(n_components=2)), ("log_reg", LogisticRegression()) ])  
param_grid = [["kPCA_gamma": np.linspace(0.03, 0.05, 10), "kPCA_kernel": ["rbf", "sigmoid"]] ]  
grid_search = GridSearchCV(clf, param_grid, cv=3)  
grid_search.fit(X, y) 
```

The best kernel and hyperparameters are then available through the best_params_ variable:

```txt
>>> printgrid_search.best.params_   
{'kpca_gamma':0.04333333333333335,'kpca_kernel': 'rbf'} 
```

Another approach, this time entirely unsupervised, is to select the kernel and hyper‐ parameters that yield the lowest reconstruction error. However, reconstruction is not as easy as with linear PCA. Here’s why. Figure 8-11 shows the original Swiss roll 3D dataset (top left), and the resulting 2D dataset after kPCA is applied using an RBF kernel (top right). Thanks to the kernel trick, this is mathematically equivalent to mapping the training set to an infinite-dimensional feature space (bottom right) using the feature map $\boldsymbol { \Psi } ;$ , then projecting the transformed training set down to 2D using linear PCA. Notice that if we could invert the linear PCA step for a given instance in the reduced space, the reconstructed point would lie in feature space, not in the original space (e.g., like the one represented by an x in the diagram). Since the feature space is infinite-dimensional, we cannot compute the reconstructed point, and therefore we cannot compute the true reconstruction error. Fortunately, it is pos‐ sible to find a point in the original space that would map close to the reconstructed point. This is called the reconstruction pre-image. Once you have this pre-image, you can measure its squared distance to the original instance. You can then select the ker‐ nel and hyperparameters that minimize this reconstruction pre-image error.

![](images/67d8209df1ce79811e2d81442c190f7eb1424aabb2066ee17741e0c9fc6a98f9.jpg)  
Figure 8-11. Kernel PCA and the reconstruction pre-image error

You may be wondering how to perform this reconstruction. One solution is to train a supervised regression model, with the projected instances as the training set and the original instances as the targets. Scikit-Learn will do this automatically if you set fit_inverse_transform=True, as shown in the following code:7

```python
rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433, fitInverse_transform=True)  
X_reduced = rbf_pca.fit_transform(X)  
X_preimage = rbf_pca.inverse_transform(X_reduced) 
```

![](images/cdf386f9d0660ec46ec56e262c21285386e5b00aee3f256f185ea6106e2bab26.jpg)

By default, fit_inverse_transform=False and KernelPCA has no inverse_transform() method. This method only gets created when you set fit_inverse_transform=True.

You can then compute the reconstruction pre-image error:

```python
>>> from sklearn.metrics import mean_squared_error
>>> mean_squared_error(X, X_preimage)
32.786308795766132 
```

Now you can use grid search with cross-validation to find the kernel and hyperpara‐ meters that minimize this pre-image reconstruction error.

# LLE

Locally Linear Embedding (LLE)8 is another very powerful nonlinear dimensionality reduction (NLDR) technique. It is a Manifold Learning technique that does not rely on projections like the previous algorithms. In a nutshell, LLE works by first measur‐ ing how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly). This makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.

For example, the following code uses Scikit-Learn’s LocallyLinearEmbedding class to unroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12. As you can see, the Swiss roll is completely unrolled and the distances between instances are locally well preserved. However, distances are not preserved on a larger scale: the left part of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐ less, LLE did a pretty good job at modeling the manifold.

from sklearn.manifold import LocallyLinearEmbedding

```txt
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_reduced = lle.fit_transform(X) 
```

![](images/bd65b692fdfed18fcd70fa6e54f85fbf10bce488272896b61f51838030fe38cf.jpg)  
Figure 8-12. Unrolled Swiss roll using LLE

Here’s how LLE works: first, for each training instance $\mathbf { X } ^ { ( i ) }$ , the algorithm identifies its $k$ closest neighbors (in the preceding code $k = 1 0 \mathrm { \ : }$ ), then tries to reconstruct $\mathbf { X } ^ { ( i ) }$ as a linear function of these neighbors. More specifically, it finds the weights $w _ { i , j }$ such that the squared distance between $\mathbf { X } ^ { ( i ) }$ and $\boldsymbol { \Sigma _ { j = 1 } ^ { m } } \boldsymbol { w _ { i , j } } \mathbf { x } ^ { ( j ) }$ is as small as possible, assuming $w _ { i , j }$ $= 0$ if $\mathbf { x } ^ { ( j ) }$ is not one of the $k$ closest neighbors of $\mathbf { X } ^ { ( i ) }$ . Thus the first step of LLE is the constrained optimization problem described in Equation 8-4, where W is the weight matrix containing all the weights $w _ { i , j } .$ The second constraint simply normalizes the weights for each training instance $\mathbf { X } ^ { ( i ) }$ .

Equation 8-4. LLE step 1: linearly modeling local relationships

$$
\widehat {\mathbf {W}} = \underset {\mathbf {W}} {\operatorname {a r g m i n}} \sum_ {i = 1} ^ {m} \left(\mathbf {x} ^ {(i)} - \sum_ {j = 1} ^ {m} w _ {i, j} \mathbf {x} ^ {(j)}\right) ^ {2}
$$

$\left\{ \begin{array} { l l } { \boldsymbol { w } _ { i , j } = 0 \qquad { \mathrm { i f ~ } } \mathbf { x } ^ { ( j ) } { \mathrm { ~ i s ~ n o t ~ o n e } } } \\ { \displaystyle \sum _ { j = 1 } ^ { m } \boldsymbol { w } _ { i , j } = 1 { \mathrm { ~ f o r ~ } } i = 1 , 2 , \cdots , m } \end{array} \right.$ of the $k$ c.n. of $\mathbf { x } ^ { ( i ) }$ subject to wi, j

After this step, the weight matrix $\widehat { \bf W }$ (containing the weights $\widehat { w } _ { i , j } )$ encodes the local linear relationships between the training instances. Now the second step is to map the training instances into a $d$ -dimensional space (where $d < n$ ) while preserving these local relationships as much as possible. If $\mathbf { z } ^ { ( i ) }$ is the image of $\mathbf { X } ^ { ( i ) }$ in this $d$ -dimensional space, then we want the squared distance between $\mathbf { z } ^ { ( i ) }$ and $\Sigma _ { j = 1 } ^ { m } \widehat { w } _ { i , j } \mathbf { z } ^ { ( j ) }$ to be as small as possible. This idea leads to the unconstrained optimization problem described in Equation 8-5. It looks very similar to the first step, but instead of keeping the instan‐ ces fixed and finding the optimal weights, we are doing the reverse: keeping the weights fixed and finding the optimal position of the instances’ images in the lowdimensional space. Note that Z is the matrix containing all $\mathbf { z } ^ { ( i ) }$ .

Equation 8-5. LLE step 2: reducing dimensionality while preserving relationships

$$
\widehat {\mathbf {Z}} = \underset {\mathbf {Z}} {\operatorname {a r g m i n}} \sum_ {i = 1} ^ {m} \left(\mathbf {z} ^ {(i)} - \sum_ {j = 1} ^ {m} \widehat {w} _ {i, j} \mathbf {z} ^ {(j)}\right) ^ {2}
$$

Scikit-Learn’s LLE implementation has the following computational complexity: $O ( m \log ( m ) n \log ( k ) )$ for finding the $k$ nearest neighbors, $O ( m n k ^ { 3 } )$ for optimizing the weights, and $O ( d m ^ { 2 } )$ for constructing the low-dimensional representations. Unfortu‐ nately, the $m ^ { 2 }$ in the last term makes this algorithm scale poorly to very large datasets.

# Other Dimensionality Reduction Techniques

There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn. Here are some of the most popular:

• Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve the distances between the instances (see Figure 8-13).

• Isomap creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances9 between the instances.   
• t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).   
• Linear Discriminant Analysis (LDA) is actually a classification algorithm, but dur‐ ing training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier.

![](images/1fc85c339191f92b14fc9e921015fcfa3bac8fd6a2bdeb6dc7991639beaa3672.jpg)  
Figure 8-13. Reducing the Swiss roll to 2D using various techniques

# Exercises

1. What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?   
2. What is the curse of dimensionality?   
3. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?   
4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?   
5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to $9 5 \%$ . How many dimensions will the resulting dataset have?

6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?   
7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?   
8. Does it make any sense to chain two different dimensionality reduction algo‐ rithms?   
9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of $9 5 \%$ Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next evaluate the classifier on the test set: how does it compare to the previous classifier?   
10. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to rep‐ resent each image’s target class. Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.

Solutions to these exercises are available in ???.

# Unsupervised Learning Techniques

![](images/fee8a1bd81d140307b61d93f20add615a4386367a7a7096fa9000e40c2acf2c8.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 9 in the final release of the book.

Although most of the applications of Machine Learning today are based on super‐ vised learning (and as a result, this is where most of the investments go to), the vast majority of the available data is actually unlabeled: we have the input features X, but we do not have the labels y. Yann LeCun famously said that “if intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake”. In other words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth into.

For example, say you want to create a system that will take a few pictures of each item on a manufacturing production line and detect which items are defective. You can fairly easily create a system that will take pictures automatically, and this might give you thousands of pictures every day. You can then build a reasonably large dataset in just a few weeks. But wait, there are no labels! If you want to train a regular binary classifier that will predict whether an item is defective or not, you will need to label every single picture as “defective” or “normal”. This will generally require human experts to sit down and manually go through all the pictures. This is a long, costly and tedious task, so it will usually only be done on a small subset of the available pic‐ tures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐ mance will be disappointing. Moreover, every time the company makes any change to its products, the whole process will need to be started over from scratch. Wouldn’t it

be great if the algorithm could just exploit the unlabeled data without needing humans to label every picture? Enter unsupervised learning.

In Chapter 8, we looked at the most common unsupervised learning task: dimension‐ ality reduction. In this chapter, we will look at a few more unsupervised learning tasks and algorithms:

• Clustering: the goal is to group similar instances together into clusters. This is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.   
• Anomaly detection: the objective is to learn what “normal” data looks like, and use this to detect abnormal instances, such as defective items on a production line or a new trend in a time series.   
• Density estimation: this is the task of estimating the probability density function (PDF) of the random process that generated the dataset. This is commonly used for anomaly detection: instances located in very low-density regions are likely to be anomalies. It is also useful for data analysis and visualization.

Ready for some cake? We will start with clustering, using K-Means and DBSCAN, and then we will discuss Gaussian mixture models and see how they can be used for density estimation, clustering, and anomaly detection.

# Clustering

As you enjoy a hike in the mountains, you stumble upon a plant you have never seen before. You look around and you notice a few more. They are not perfectly identical, yet they are sufficiently similar for you to know that they most likely belong to the same species (or at least the same genus). You may need a botanist to tell you what species that is, but you certainly don’t need an expert to identify groups of similarlooking objects. This is called clustering: it is the task of identifying similar instances and assigning them to clusters, i.e., groups of similar instances.

Just like in classification, each instance gets assigned to a group. However, this is an unsupervised task. Consider Figure 9-1: on the left is the iris dataset (introduced in Chapter 4), where each instance’s species (i.e., its class) is represented with a different marker. It is a labeled dataset, for which classification algorithms such as Logistic Regression, SVMs or Random Forest classifiers are well suited. On the right is the same dataset, but without the labels, so you cannot use a classification algorithm any‐ more. This is where clustering algorithms step in: many of them can easily detect the top left cluster. It is also quite easy to see with our own eyes, but it is not so obvious that the lower right cluster is actually composed of two distinct sub-clusters. That said, the dataset actually has two additional features (sepal length and width), not

represented here, and clustering algorithms can make good use of all features, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mixture model, only 5 instances out of 150 are assigned to the wrong cluster).

![](images/801acad97d4e415826343e70b182800070ed4d7e367f326770125c5718755b98.jpg)  
Figure 9-1. Classication (le) versus clustering (right)

Clustering is used in a wide variety of applications, including:

• For customer segmentation: you can cluster your customers based on their pur‐ chases, their activity on your website, and so on. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, this can be useful in recom‐ mender systems to suggest content that other users in the same cluster enjoyed.   
• For data analysis: when analyzing a new dataset, it is often useful to first discover clusters of similar instances, as it is often easier to analyze clusters separately.   
• As a dimensionality reduction technique: once a dataset has been clustered, it is usually possible to measure each instance’s anity with each cluster (affinity is any measure of how well an instance fits into a cluster). Each instance’s feature vector x can then be replaced with the vector of its cluster affinities. If there are $k$ clusters, then this vector is $k$ dimensional. This is typically much lower dimen‐ sional than the original feature vector, but it can preserve enough information for further processing.   
• For anomaly detection (also called outlier detection): any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clus‐ tered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second, and so on. Anomaly detection is particularly useful in detecting defects in manufacturing, or for fraud detection.   
• For semi-supervised learning: if you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This can greatly increase the amount of labels available for a subsequent supervised learning algorithm, and thus improve its performance.

• For search engines: for example, some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database: similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is to find this image’s cluster using the trained clustering model, and you can then simply return all the images from this cluster.   
• To segment an image: by clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to reduce the number of different colors in the image considerably. This technique is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object.

There is no universal definition of what a cluster is: it really depends on the context, and different algorithms will capture different kinds of clusters. For example, some algorithms look for instances centered around a particular point, called a centroid. Others look for continuous regions of densely packed instances: these clusters can take on any shape. Some algorithms are hierarchical, looking for clusters of clusters. And the list goes on.

In this section, we will look at two popular clustering algorithms: K-Means and DBSCAN, and we will show some of their applications, such as non-linear dimen‐ sionality reduction, semi-supervised learning and anomaly detection.

# K-Means

Consider the unlabeled dataset represented in Figure 9-2: you can clearly see 5 blobs of instances. The K-Means algorithm is a simple algorithm capable of clustering this kind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐ posed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐ tion, but it was only published outside of the company in 1982, in a paper titled “Least square quantization in PCM”.1 By then, in 1965, Edward W. Forgy had pub‐ lished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-Forgy.

![](images/8be5cdd9ff179becc09d457f5049c21c63d4c672137316158e0b96db162b8e13.jpg)  
Figure 9-2. An unlabeled dataset composed of ve blobs of instances

Let’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and assign each instance to the closest blob:

```python
from sklearn.cluster import KMeans  
k = 5  
kmeans = KMeans(n_clusters=k)  
y_pred = kmeans.fit_prediction(X) 
```

Note that you have to specify the number of clusters $k$ that the algorithm must find. In this example, it is pretty obvious from looking at the data that $k$ should be set to 5, but in general it is not that easy. We will discuss this shortly.

Each instance was assigned to one of the 5 clusters. In the context of clustering, an instance’s label is the index of the cluster that this instance gets assigned to by the algorithm: this is not to be confused with the class labels in classification (remember that clustering is an unsupervised learning task). The KMeans instance preserves a copy of the labels of the instances it was trained on, available via the labels_ instance variable:

```erlang
>>> y_pred  
array([4, 0, 1, ..., 2, 1, 0], dtype=int32)  
>>> y_pred is kmeans.labels_  
True 
```

We can also take a look at the 5 centroids that the algorithm found:

```txt
>>> kmeans.clustercenters_   
array([[ -2.80389616， 1.80117999]， [0.20876306， 2.25551336]， [-2.79290307， 2.79641063]， [-1.46679593， 2.28585348]， [-2.80037642， 1.30082566]]）
```

Of course, you can easily assign new instances to the cluster whose centroid is closest:

```python
>>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
>>> kmeans.predict(X_new)
array([1, 1, 2, 2], dtype=int32) 
```

If you plot the cluster’s decision boundaries, you get a Voronoi tessellation (see Figure 9-3, where each centroid is represented with an X):

![](images/882f74ee229381d89d05f6cf825c098d2176c0fe426cb7743da74f0eac42dba2.jpg)  
Figure 9-3. K-Means decision boundaries (Voronoi tessellation)

The vast majority of the instances were clearly assigned to the appropriate cluster, but a few instances were probably mislabeled (especially near the boundary between the top left cluster and the central cluster). Indeed, the K-Means algorithm does not behave very well when the blobs have very different diameters since all it cares about when assigning an instance to a cluster is the distance to the centroid.

Instead of assigning each instance to a single cluster, which is called hard clustering, it can be useful to just give each instance a score per cluster: this is called so clustering. For example, the score can be the distance between the instance and the centroid, or conversely it can be a similarity score (or affinity) such as the Gaussian Radial Basis Function (introduced in Chapter 5). In the KMeans class, the transform() method measures the distance from each instance to every centroid:

```txt
>>> kmeans.transform(X_new)  
array([[2.81093633, 0.32995317, 2.9042344, 1.49439034, 2.88633901], [5.80730058, 2.80290755, 5.84739223, 4.4759332, 5.84236351], [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031], [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]]) 
```

In this example, the first instance in X_new is located at a distance of 2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from the fourth centroid and 2.87 from the fifth centroid. If you have a high-dimensional dataset and you transform it this way, you end up with a $k$ -dimensional dataset: this can be a very efficient non-linear dimensionality reduction technique.

# The K-Means Algorithm

So how does the algorithm work? Well it is really quite simple. Suppose you were given the centroids: you could easily label all the instances in the dataset by assigning each of them to the cluster whose centroid is closest. Conversely, if you were given all the instance labels, you could easily locate all the centroids by computing the mean of the instances for each cluster. But you are given neither the labels nor the centroids, so how can you proceed? Well, just start by placing the centroids randomly (e.g., by picking $k$ instances at random and using their locations as centroids). Then label the instances, update the centroids, label the instances, update the centroids, and so on until the centroids stop moving. The algorithm is guaranteed to converge in a finite number of steps (usually quite small), it will not oscillate forever2. You can see the algorithm in action in Figure 9-4: the centroids are initialized randomly (top left), then the instances are labeled (top right), then the centroids are updated (center left), the instances are relabeled (center right), and so on. As you can see, in just 3 itera‐ tions the algorithm has reached a clustering that seems close to optimal.

![](images/911fea82b0ce38390842512267e7e52d016099315a525a36b8b08414c9ffb1cb.jpg)

![](images/99bc0babb12036eebde1c1894c941a6d1e86da1682f336ece643d7debc35ef29.jpg)

![](images/b4bfe1936937d83257b05aa5e151cd0e70cd1e889ffd303fa3c129857ec058ac.jpg)

![](images/c59ac9f64d31fed870b92b837f9461e602f0bdbaeda449a0f505fdc509608ab1.jpg)

![](images/c0991805772f971136e70d3810f87f244471f5bcf471671309cbdf1e47f5ec26.jpg)

![](images/fe47dc6a87f53b09d8036eedb1550d55a35d8856a169c76381116f10c6115d25.jpg)  
Figure 9-4. e K-Means algorithm

![](images/55842c094a3648067e0f59cd096b4e347cbd21dec757bee0271e4b57a04a0341.jpg)

The computational complexity of the algorithm is generally linear with regards to the number of instances m, the number of clusters $k$ and the number of dimensions n. However, this is only true when the data has a clustering structure. If it does not, then in the worst case scenario the complexity can increase exponentially with the number of instances. In practice, however, this rarely happens, and K-Means is generally one of the fastest clustering algorithms.

Unfortunately, although the algorithm is guaranteed to converge, it may not converge to the right solution (i.e., it may converge to a local optimum): this depends on the centroid initialization. For example, Figure 9-5 shows two sub-optimal solutions that the algorithm can converge to if you are not lucky with the random initialization step:

![](images/7a9120d34b7370ab99b6e3b4f6c18b09fdb38b99a6e9dd632c3cc86400b7a03a.jpg)

![](images/1d3cc29f916f70e85eb9ba31ac51ebe326169507fb11ce3db7a9bbbafc14d50c.jpg)  
Figure 9-5. Sub-optimal solutions due to unlucky centroid initializations

Let’s look at a few ways you can mitigate this risk by improving the centroid initializa‐ tion.

# Centroid Initialization Methods

If you happen to know approximately where the centroids should be (e.g., if you ran another clustering algorithm earlier), then you can set the init hyperparameter to a NumPy array containing the list of centroids, and set n_init to 1:

```python
good_init = np.array([[ -3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1) 
```

Another solution is to run the algorithm multiple times with different random initial‐ izations and keep the best solution. This is controlled by the n_init hyperparameter: by default, it is equal to 10, which means that the whole algorithm described earlier actually runs 10 times when you call fit(), and Scikit-Learn keeps the best solution. But how exactly does it know which solution is the best? Well of course it uses a per‐ formance metric! It is called the model’s inertia: this is the mean squared distance between each instance and its closest centroid. It is roughly equal to 223.3 for the model on the left of Figure 9-5, 237.5 for the model on the right of Figure 9-5, and 211.6 for the model in Figure 9-3. The KMeans class runs the algorithm n_init times and keeps the model with the lowest inertia: in this example, the model in Figure 9-3 will be selected (unless we are very unlucky with n_init consecutive random initiali‐

zations). If you are curious, a model’s inertia is accessible via the inertia_ instance variable:

>>> kmeans.inertia_

211.59853725816856

The score() method returns the negative inertia. Why negative? Well, it is because a predictor’s score() method must always respect the "great is better" rule.

>>> kmeans.score(X)

-211.59853725816856

An important improvement to the K-Means algorithm, called $K { - } M e a n s { + } \backslash { + } .$ , was pro‐ posed in a 2006 paper by David Arthur and Sergei Vassilvitskii:3 they introduced a smarter initialization step that tends to select centroids that are distant from one another, and this makes the K-Means algorithm much less likely to converge to a suboptimal solution. They showed that the additional computation required for the smarter initialization step is well worth it since it makes it possible to drastically reduce the number of times the algorithm needs to be run to find the optimal solu‐ tion. Here is the K-Means $^ { + + }$ initialization algorithm:

• Take one centroid $\mathbf { c } ^ { ( 1 ) }$ , chosen uniformly at random from the dataset.   
• Take a new centroid $\mathbf { c } ^ { ( i ) }$ , choosing an instance $\mathbf { X } ^ { ( i ) }$ with probability: $D \big ( \mathbf { x } ^ { ( i ) } \big ) ^ { 2 }$ $\Sigma _ { j = 1 } ^ { m } D \big ( \mathbf { x } ^ { ( j ) } \big ) ^ { 2 }$ where $\mathrm { D } ( \mathbf { x } ^ { ( i ) } )$ is the distance between the instance $\mathbf { X } ^ { ( i ) }$ and the closest centroid that was already chosen. This probability distribution ensures that instances further away from already chosen centroids are much more likely be selected as centroids.   
• Repeat the previous step until all $k$ centroids have been chosen.

The KMeans class actually uses this initialization method by default. If you want to force it to use the original method (i.e., picking $k$ instances randomly to define the initial centroids), then you can set the init hyperparameter to "random". You will rarely need to do this.

# Accelerated K-Means and Mini-batch K-Means

Another important improvement to the K-Means algorithm was proposed in a 2003 paper by Charles Elkan.4 It considerably accelerates the algorithm by avoiding many unnecessary distance calculations: this is achieved by exploiting the triangle inequal‐

ity (i.e., the straight line is always the shortest5) and by keeping track of lower and upper bounds for distances between instances and centroids. This is the algorithm used by default by the KMeans class (but you can force it to use the original algorithm by setting the algorithm hyperparameter to "full", although you probably will never need to).

Yet another important variant of the K-Means algorithm was proposed in a 2010 paper by David Sculley.6 Instead of using the full dataset at each iteration, the algo‐ rithm is capable of using mini-batches, moving the centroids just slightly at each iter‐ ation. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. You can just use this class like the KMeans class:

from sklearn.cluster import MiniBatchKMeans

minibatch_kmeans $=$ MiniBatchKMeans(n_clusters=5) minibatch_kmeans.fit(X)

If the dataset does not fit in memory, the simplest option is to use the memmap class, as we did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch at a time to the partial_fit() method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself (see the notebook for an example).

Although the Mini-batch K-Means algorithm is much faster than the regular K-Means algorithm, its inertia is generally slightly worse, especially as the number of clusters increases. You can see this in Figure 9-6: the plot on the left compares the inertias of Mini-batch K-Means and regular K-Means models trained on the previous dataset using various numbers of clusters $k$ The difference between the two curves remains fairly constant, but this difference becomes more and more significant as $k$ increases, since the inertia becomes smaller and smaller. However, in the plot on the right, you can see that Mini-batch K-Means is much faster than regular K-Means, and this difference increases with $k$ .

![](images/d2ad7c4094eb055aecacb55805c9f9f6b203355daab9713fc56745f44771e3fa.jpg)  
Figure 9-6. Mini-batch K-Means vs K-Means: worse inertia as k increases (le) but much faster (right)

# Finding the Optimal Number of Clusters

So far, we have set the number of clusters $k$ to 5 because it was obvious by looking at the data that this is the correct number of clusters. But in general, it will not be so easy to know how to set $k ,$ and the result might be quite bad if you set it to the wrong value. For example, as you can see in Figure 9-7, setting $k$ to 3 or 8 results in fairly bad models:

![](images/0006a08061d9ae9027b8d3967c82c11ca6d778ac3d273f061a8408d6b1a02b1e.jpg)  
Figure 9-7. Bad choices for the number of clusters

You might be thinking that we could just pick the model with the lowest inertia, right? Unfortunately, it is not that simple. The inertia for $k { = } 3$ is 653.2, which is much higher than for $k { = } 5$ (which was 211.6), but with $k { = } 8$ , the inertia is just 119.1. The inertia is not a good performance metric when trying to choose $k$ since it keeps get‐ ting lower as we increase $k$ Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. Let’s plot the inertia as a function of $k$ (see Figure 9-8):

![](images/9b84cf0afd65d7519de0be2edd148688268b3179696b8e9dc51b54be2500b430.jpg)  
Figure 9-8. Selecting the number of clusters k using the “elbow rule”

As you can see, the inertia drops very quickly as we increase $k$ up to 4, but then it decreases much more slowly as we keep increasing $k$ . This curve has roughly the shape of an arm, and there is an “elbow” at $k { = } 4$ so if we did not know better, it would be a good choice: any lower value would be dramatic, while any higher value would not help much, and we might just be splitting perfectly good clusters in half for no good reason.

This technique for choosing the best value for the number of clusters is rather coarse. A more precise approach (but also more computationally expensive) is to use the sil‐ houette score, which is the mean silhouette coecient over all the instances. An instan‐ ce’s silhouette coefficient is equal to $( b - a ) / \operatorname* { m a x } ( a , b )$ where $^ a$ is the mean distance to the other instances in the same cluster (it is the mean intra-cluster distance), and $^ { b }$ is the mean nearest-cluster distance, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes $^ { b }$ , excluding the instance’s own cluster). The silhouette coefficient can vary between -1 and $+ 1$ : a coefficient close to $+ 1$ means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster. To compute the silhouette score, you can use Scikit-Learn’s silhou ette_score() function, giving it all the instances in the dataset, and the labels they were assigned:

```python
>>> from sklearn.metrics import silhouette_score
>>> silhouette_score(X, kmeans.labels_)
0.655517642572828 
```

Let’s compare the silhouette scores for different numbers of clusters (see Figure 9-9):

![](images/6fd899fb50c5fc497b711ded0a6d13473c210bf76b97e41bd8c88c1630417867.jpg)  
Figure 9-9. Selecting the number of clusters $k$ using the silhouette score

As you can see, this visualization is much richer than the previous one: in particular, although it confirms that $k { = } 4$ is a very good choice, it also underlines the fact that $k { = } 5$ is quite good as well, and much better than $k { = } 6$ or 7. This was not visible when comparing inertias.

An even more informative visualization is obtained when you plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a silhouette diagram (see Figure 9-10):

![](images/d834e5467b9993b75c825df53cb0bcce2f4fcb50ccadf29dc2600e42b0a88b26.jpg)

![](images/e2f574ab0fcfab549c5f8a4f1c46842e56998b2a88971743c0a251b1fc50c694.jpg)

![](images/7f2823b39214c6046016b007b469cca8559fabca67826fd685e59a51b78f5478.jpg)

![](images/4393a3958df7fe62193f9b4f9b0a755ff267d6d06e7319d8341407174b29cf2a.jpg)  
Figure 9-10. Silouhette analysis: comparing the silhouette diagrams for various values of k

The vertical dashed lines represent the silhouette score for each number of clusters. When most of the instances in a cluster have a lower coefficient than this score (i.e., if many of the instances stop short of the dashed line, ending to the left of it), then the cluster is rather bad since this means its instances are much too close to other clus‐

ters. We can see that when $k { = } 3$ and when $k { = } 6$ , we get bad clusters. But when $k { = } 4$ or $k { = } 5$ , the clusters look pretty good – most instances extend beyond the dashed line, to the right and closer to 1.0. When $k { = } 4$ , the cluster at index 1 (the third from the top), is rather big, while when $k { = } 5$ , all clusters have similar sizes, so even though the over‐ all silhouette score from $k { = } 4$ is slightly greater than for $k { = } 5$ , it seems like a good idea to use $k { = } 5$ to get clusters of similar sizes.

# Limits of K-Means

Despite its many merits, most notably being fast and scalable, K-Means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐ utions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes. For example, Figure 9-11 shows how K-Means clusters a dataset containing three ellipsoidal clusters of different dimensions, densities and orientations:

![](images/7fffc90a64c87ce24177a6075a6f0c827f6b35a4d8386089ef8abdbaf6d8be99.jpg)

![](images/ff59a8109f31c9c3fced15e05b40989326114d99efe0f39de80ac33375612f9a.jpg)  
Figure 9-11. K-Means fails to cluster these ellipsoidal blobs properly

As you can see, neither of these solutions are any good. The solution on the left is better, but it still chops off $2 5 \%$ of the middle cluster and assigns it to the cluster on the right. The solution on the right is just terrible, even though its inertia is lower. So depending on the data, different clustering algorithms may perform better. For exam‐ ple, on these types of elliptical clusters, Gaussian mixture models work great.

![](images/d1e98603cbd10efd742a90f8809a1a7c27c6768c4cb9434e9008d6fedcdfddf9.jpg)

It is important to scale the input features before you run K-Means, or else the clusters may be very stretched, and K-Means will per‐ form poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things.

Now let’s look at a few ways we can benefit from clustering. We will use K-Means, but feel free to experiment with other clustering algorithms.

# Using clustering for image segmentation

Image segmentation is the task of partitioning an image into multiple segments. In semantic segmentation, all pixels that are part of the same object type get assigned to the same segment. For example, in a self-driving car’s vision system, all pixels that are part of a pedestrian’s image might be assigned to the “pedestrian” segment (there would just be one segment containing all the pedestrians). In instance segmentation, all pixels that are part of the same individual object are assigned to the same segment. In this case there would be a different segment for each pedestrian. The state of the art in semantic or instance segmentation today is achieved using complex architec‐ tures based on convolutional neural networks (see Chapter 14). Here, we are going to do something much simpler: color segmentation. We will simply assign pixels to the same segment if they have a similar color. In some applications, this may be sufficient, for example if you want to analyze satellite images to measure how much total forest area there is in a region, color segmentation may be just fine.

First, let’s load the image (see the upper left image in Figure 9-12) using Matplotlib’s imread() function:

>>> from matplotlib.image import imread # you could also use `imageio.imread()`

>>> image $=$ imread(os.path.join("images","clustering","ladybug.png"))

>>> image.shape

(533, 800, 3)

The image is represented as a 3D array: the first dimension’s size is the height, the second is the width, and the third is the number of color channels, in this case red, green and blue (RGB). In other words, for each pixel there is a 3D vector containing the intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255 if you use imageio.imread()). Some images may have less channels, such as gray‐ scale images (one channel), or more channels, such as images with an additional alpha channel for transparency, or satellite images which often contain channels for many light frequencies (e.g., infrared). The following code reshapes the array to get a long list of RGB colors, then it clusters these colors using K-Means. For example, it may identify a color cluster for all shades of green. Next, for each color (e.g., dark green), it looks for the mean color of the pixel’s color cluster. For example, all shades of green may be replaced with the same light green color (assuming the mean color of the green cluster is light green). Finally it reshapes this long list of colors to get the same shape as the original image. And we’re done!

```python
X = image.reshape(-1, 3)  
kmeans = KMeans(n_clusters=8).fit(X)  
segmented_img = kmeans.clustercenters_[kmeans.labels_]  
segmented_img = segmented_img.reshape(image.shape) 
```

This outputs the image shown in the upper right of Figure 9-12. You can experiment with various numbers of clusters, as shown in the figure. When you use less than 8 clusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it

gets merged with colors from the environment. This is due to the fact that the lady‐ bug is quite small, much smaller than the rest of the image, so even though its color is flashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers clusters of similar sizes.

![](images/e23d19980317b1cd6754cae715d4a1ab977718e16f32dc444790f95a5ee229d2.jpg)  
6colors

![](images/81a47756b02abc905a81f5ee4e298c07f841011805a477cf7adf6a471f0df130.jpg)  
4colors

![](images/8f24df8d6b94d0d92920b4ade8621e5aac9357ff1ae573f8e2aed5bb0dcec2fb.jpg)  
2colors

![](images/79badb5ec49528cbfcc3d176639c9d2a068c58e8e0f90ec1f61f0a2299f2c5ab.jpg)

![](images/4b1d32746bfac29fa196f91552e27624e64f42326773cafa5cb17dcc971e8256.jpg)

![](images/592cc781797a703fb57cb7f96acd1d505016129dc635545ee998057208568c2a.jpg)  
Figure 9-12. Image segmentation using K-Means with various numbers of color clusters

That was not too hard, was it? Now let’s look at another application of clustering: pre‐ processing.

# Using Clustering for Preprocessing

Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing step before a supervised learning algorithm. For example, let’s tackle the digits dataset which is a simple MNIST-like dataset containing 1,797 grayscale $8 { \times } 8$ images representing digits 0 to 9. First, let’s load the dataset:

from sklearn.datasets import load_digits

X_digits, y_digits $=$ load_digits(return_X_y=True)

Now, let’s split it into a training set and a test set:

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test $=$ train_test_split(X_digits, y_digits)

Next, let’s fit a Logistic Regression model:

from sklearn.linear_model import LogisticRegression

log_reg $=$ LogisticRegression(random_state $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ ) log_reg.fit(X_train, y_train)

Let’s evaluate its accuracy on the test set:

>>> log_reg.score(X_test, y_test) 0.9666666666666667

Okay, that’s our baseline: $9 6 . 7 \%$ accuracy. Let’s see if we can do better by using K-Means as a preprocessing step. We will create a pipeline that will first cluster the training set into 50 clusters and replace the images with their distances to these 50 clusters, then apply a logistic regression model.

![](images/7bf97ca5c8493953501f578035ca80bd375a8fefc2057caa8718d4364ebcfa00.jpg)

Although it is tempting to define the number of clusters to 10, since there are 10 different digits, it is unlikely to perform well, because there are several different ways to write each digit.

from sklearn.pipeline import Pipeline   
pipeline $=$ Pipeline([ ("kmeans",KMeans(n_clusters=50)), ("log_reg",LogisticRegression(),   
]） pipeline.fit(X_train，y_train)

Now let’s evaluate this classification pipeline:

```txt
>> pipeline.score(X_test, y_test) 0.982222222222222 
```

How about that? We almost divided the error rate by a factor of 2!

But we chose the number of clusters $k$ completely arbitrarily, we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for $k$ is much simpler than earlier: there’s no need to perform silhouette analysis or minimize the inertia, the best value of $k$ is simply the one that results in the best classification performance during cross-validation. Let’s use GridSearchCV to find the optimal number of clusters:

from sklearn.model_selection import GridSearchCV   
param_grid $=$ dict(kmeans_n_clusters=range(2,100))   
grid_clf $=$ GridSearchCV(pipeline，param_grid，cv=3，verbose=2)   
grid_clf.fit(X_train，y_train)

Let’s look at best value for $k ,$ and the performance of the resulting pipeline:

```python
>>> grid_clf.best.params_  
{'kmeans_n_clusters': 90}  
>>> grid_clf.score(X_test, y_test)  
0.984444444444445 
```

With $k { = } 9 0$ clusters, we get a small accuracy boost, reaching $9 8 . 4 \%$ accuracy on the test set. Cool!

# Using Clustering for Semi-Supervised Learning

Another use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances. Let’s train a logistic regression model on a sample of 50 labeled instances from the digits dataset:

n_labeled $= 50$ log_reg $\equiv$ LogisticRegression()   
log_reg.fit(X_train[:n_labeled],y_train[:n_labeled])

What is the performance of this model on the test set?

```txt
>>log_reg.score(X_test，y_test) 0.8266666666666667 
```

The accuracy is just $8 2 . 7 \%$ : it should come as no surprise that this is much lower than earlier, when we trained the model on the full training set. Let’s see how we can do better. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find the image closest to the centroid. We will call these images the representative images:

```python
k = 50  
kmeans = KMeans(n_clusters=k)  
X_digits_dist = kmeans.fit_transform(X_train)  
representative_digit_idx = np.argmax(X_digits_dist, axis=0)  
X REPRESENTATIVE_digits = X_train[representative_digit_idx] 
```

Figure 9-13 shows these 50 representative images:

![](images/815039498521aaa8861e88b0d74c9c4e1721b865f8b83bd8b291e3afca214bda.jpg)  
Figure 9-13. Fiy representative digit images (one per cluster)

Now let’s look at each image and manually label it:

yrepresentativeDigits $=$ np.array([4,8,0,6,8,3,...,7,6,2,3,1,1])

Now we have a dataset with just 50 labeled instances, but instead of being completely random instances, each of them is a representative image of its cluster. Let’s see if the performance is any better:

```txt
>>> log_reg = LogisticRegression()
>>> log_reg.fit(X REPRESENTATIVE_digits, y REPRESENTATIVE_digits)
>>> log_reg.score(X_test, y_test)
0.924444444444444 
```

Wow! We jumped from $8 2 . 7 \%$ accuracy to $9 2 . 4 \%$ , although we are still only training the model on 50 instances. Since it is often costly and painful to label instances, espe‐

cially when it has to be done manually by experts, it is a good idea to label representa‐ tive instances rather than just random instances.

But perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster? This is called label propagation:

y_train_propagated $=$ np.empty(len(X_train)，dtype $\equiv$ np.int32)   
for i in range(k): y_train_propagated[kmeans.labels $= = i] = y_{-}$ representativeDigits[i]

Now let’s train the model again and look at its performance:

```txt
>>> log_reg = LogisticRegression()
>>> log_reg.fit(X_train, y_train_propagated)
>>> log_reg.score(X_test, y_test)
0.9288888888888889 
```

We got a tiny little accuracy boost. Better than nothing, but not astounding. The problem is that we propagated each representative instance’s label to all the instances in the same cluster, including the instances located close to the cluster boundaries, which are more likely to be mislabeled. Let’s see what happens if we only propagate the labels to the $2 0 \%$ of the instances that are closest to the centroids:

percentileclosestet $= 20$ X_cluster_dist $\equiv$ X_digits_dist[np.arange(len(X_train)),kmeans.labels_]   
for i in range(k): in_cluster $\equiv$ (kmeans.labels $\equiv =$ i) cluster_dist $\equiv$ X_cluster_dist[in_cluster] cutoff_distance $\equiv$ np-percentile(cluster_dist,percentileclosestest) above_cutoff $\equiv$ (X_cluster_dist $\rightharpoondown$ cutoff_distance) X_cluster_dist[in_cluster&above_cutoff] $= -1$ partially_propagated $\equiv$ (X_cluster_dist! $\equiv$ -1)   
X_train_partially_propagated $\equiv$ X_train[partially_propagated]   
y_train_partially_propagated $\equiv$ y_train_propagated[partially_propagated]

Now let’s train the model again on this partially propagated dataset:

```erlang
>>> log_reg = LogisticRegression()
>>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
>>> log_reg.score(X_test, y_test)
0.942222222222222 
```

Nice! With just 50 labeled instances (only 5 examples per class on average!), we got $9 4 . 2 \%$ performance, which is pretty close to the performance of logistic regression on the fully labeled digits dataset (which was $9 6 . 7 \%$ ). This is because the propagated labels are actually pretty good, their accuracy is very close to $9 9 \%$ :

# Active Learning

To continue improving your model and your training set, the next step could be to do a few rounds of active learning: this is when a human expert interacts with the learn‐ ing algorithm, providing labels when the algorithm needs them. There are many dif‐ ferent strategies for active learning, but one of the most common ones is called uncertainty sampling:

• The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances.   
• The instances for which the model is most uncertain (i.e., when its estimated probability is lowest) must be labeled by the expert.   
• Then you just iterate this process again and again, until the performance improvement stops being worth the labeling effort.

Other strategies include labeling the instances that would result in the largest model change, or the largest drop in the model’s validation error, or the instances that differ‐ ent models disagree on (e.g., an SVM, a Random Forest, and so on).

Before we move on to Gaussian mixture models, let’s take a look at DBSCAN, another popular clustering algorithm that illustrates a very different approach based on local density estimation. This approach allows the algorithm to identify clusters of arbitrary shapes.

# DBSCAN

This algorithm defines clusters as continuous regions of high density. It is actually quite simple:

• For each instance, the algorithm counts how many instances are located within a small distance ε (epsilon) from it. This region is called the instance’s ε- neighborhood.   
• If an instance has at least min_samples instances in its ε-neighborhood (includ‐ ing itself), then it is considered a core instance. In other words, core instances are those that are located in dense regions.   
• All instances in the neighborhood of a core instance belong to the same cluster. This may include other core instances, therefore a long sequence of neighboring core instances forms a single cluster.

• Any instance that is not a core instance and does not have one in its neighbor‐ hood is considered an anomaly.

This algorithm works well if all the clusters are dense enough, and they are well sepa‐ rated by low-density regions. The DBSCAN class in Scikit-Learn is as simple to use as you might expect. Let’s test it on the moons dataset, introduced in Chapter 5:

```python
from sklearn.cluster import DBSCAN  
from sklearn.datasets import make_moons  
X, y = make_moons(n_samples=1000, noise=0.05)  
dbscan = DBSCAN(eps=0.05, min_samples=5)  
dbscan.fit(X) 
```

The labels of all the instances are now available in the labels_ instance variable:

```txt
>>>dbscan.labels_ array([0,2,-1,-1,1,0,0,0,..，3,2,3,3,4,2,6,3]) 
```

Notice that some instances have a cluster index equal to -1: this means that they are considered as anomalies by the algorithm. The indices of the core instances are avail‐ able in the core_sample_indices_ instance variable, and the core instances them‐ selves are available in the components_ instance variable:

```txt
>>> len(dbscan.core_sample Indices_)
808
>>> dbscan.core_sample Indices_
array([0, 4, 5, 6, 7, 8, 10, 11, ..., 992, 993, 995, 997, 998, 999])
>>> dbscan_components_
array([-0.02137124, 0.40618608], [-0.84192557, 0.53058695], ...
...
[0.94355873, 0.3278936],
[0.79419406, 0.60777171]) 
```

This clustering is represented in the left plot of Figure 9-14. As you can see, it identi‐ fied quite a lot of anomalies, plus 7 different clusters. How disappointing! Fortunately, if we widen each instance’s neighborhood by increasing eps to 0.2, we get the cluster‐ ing on the right, which looks perfect. Let’s continue with this model.

![](images/f9489030cb46404c4a2cb75cda44b921482e03ed2bc60c2ec89be6539fcf26a4.jpg)

![](images/991246b3a4f9c437eac98a061a5df27a7cf488f609b8f55a296a9a421b4e9520.jpg)  
Figure 9-14. DBSCAN clustering using two dierent neighborhood radiuses

Somewhat surprisingly, the DBSCAN class does not have a predict() method, although it has a fit_predict() method. In other words, it cannot predict which cluster a new instance belongs to. The rationale for this decision is that several classi‐ fication algorithms could make sense here, and it is easy enough to train one, for example a KNeighborsClassifier:

```python
from sklearn.neighbors import KNeighborsClassifier  
knn = KNeighborsClassifier(n_neighbors=50)  
knn.fit(dbscan_components_, dbscan.labels_[dbscan.core_sample Indices_]) 
```

Now, given a few new instances, we can predict which cluster they most likely belong to, and even estimate a probability for each cluster. Note that we only trained them on the core instances, but we could also have chosen to train them on all the instances, or all but the anomalies: this choice depends on the final task.

```python
>>> X_new = np.array([[ -0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])
>>> knn.predict(X_new)
array([1, 0, 1, 0])
>>> knn.predictprob(X_new)
array([[0.18, 0.82],
[1., 0.],
[0.12, 0.88],
[1., 0.]]])
```

The decision boundary is represented on Figure 9-15 (the crosses represent the 4 instances in X_new). Notice that since there is no anomaly in the KNN’s training set, the classifier always chooses a cluster, even when that cluster is far away. However, it is fairly straightforward to introduce a maximum distance, in which case the two instances that are far away from both clusters are classified as anomalies. To do this, we can use the kneighbors() method of the KNeighborsClassifier: given a set of instances, it returns the distances and the indices of the $k$ nearest neighbors in the training set (two matrices, each with $k$ columns):

```python
>>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
>>> y_pred = dbscan.labels_[dbscan.core_sample Indices_][y_pred_idx]
>>> y_pred[y_dist > 0.2] = -1
>>> y_pred.ravel()
array([-1, 0, 1, -1]) 
```

![](images/2b7592426dac4700548ddf6b078eb216f2b224ad3a85786150db857fd7f35df4.jpg)  
Figure 9-15. cluster_classication_diagram

In short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any number of clusters, of any shape, it is robust to outliers, and it has just two hyper‐ parameters (eps and min_samples). However, if the density varies significantly across the clusters, it can be impossible for it to capture all the clusters properly. Moreover, its computational complexity is roughly $\mathrm { O } ( m \log m )$ , making it pretty close to linear with regards to the number of instances. However, Scikit-Learn’s implementation can require up to $\mathrm { O } ( m ^ { 2 } )$ memory if eps is large.

# Other Clustering Algorithms

Scikit-Learn implements several more clustering algorithms that you should take a look at. We cannot cover them all in detail here, but here is a brief overview:

• Agglomerative clustering: a hierarchy of clusters is built from the bottom up. Think of many tiny bubbles floating on water and gradually attaching to each other until there’s just one big group of bubbles. Similarly, at each iteration agglomerative clustering connects the nearest pair of clusters (starting with indi‐ vidual instances). If you draw a tree with a branch for every pair of clusters that merged, you get a binary tree of clusters, where the leaves are the individual instances. This approach scales very well to large numbers of instances or clus‐ ters, it can capture clusters of various shapes, it produces a flexible and informa‐ tive cluster tree instead of forcing you to choose a particular cluster scale, and it can be used with any pairwise distance. It can scale nicely to large numbers of instances if you provide a connectivity matrix. This is a sparse m by m matrix that indicates which pairs of instances are neighbors (e.g., returned by sklearn.neighbors.kneighbors_graph()). Without a connectivity matrix, the algorithm does not scale well to large datasets.   
• Birch: this algorithm was designed specifically for very large datasets, and it can be faster than batch K-Means, with similar results, as long as the number of fea‐ tures is not too large $( < 2 0 )$ . It builds a tree structure during training containing

just enough information to quickly assign each new instance to a cluster, without having to store all the instances in the tree: this allows it to use limited memory, while handle huge datasets.

• Mean-shi: this algorithm starts by placing a circle centered on each instance, then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shift step until all the circles stop moving (i.e., until each of them is cen‐ tered on the mean of the instances it contains). This algorithm shifts the circles in the direction of higher density, until each of them has found a local density maximum. Finally, all the instances whose circles have settled in the same place (or close enough) are assigned to the same cluster. This has some of the same fea‐ tures as DBSCAN, in particular it can find any number of clusters of any shape, it has just one hyperparameter (the radius of the circles, called the bandwidth) and it relies on local density estimation. However, it tends to chop clusters into pieces when they have internal density variations. Unfortunately, its computational complexity is $\mathrm { O } ( m ^ { 2 } )$ , so it is not suited for large datasets.

• Anity propagation: this algorithm uses a voting system, where instances vote for similar instances to be their representatives, and once the algorithm converges, each representative and its voters form a cluster. This algorithm can detect any number of clusters of different sizes. Unfortunately, this algorithm has a compu‐ tational complexity of $\mathrm { O } ( m ^ { 2 } )$ , so it is not suited for large datasets.   
• Spectral clustering: this algorithm takes a similarity matrix between the instances and creates a low-dimensional embedding from it (i.e., it reduces its dimension‐ ality), then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture complex cluster structures, and it can also be used to cut graphs (e.g., to identify clusters of friends on a social network), however it does not scale well to large number of instances, and it does not behave well when the clusters have very dif‐ ferent sizes.

Now let’s dive into Gaussian mixture models, which can be used for density estima‐ tion, clustering and anomaly detection.

# Gaussian Mixtures

A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distri‐ bution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐ ferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11. When you observe an instance, you know it was generated from one of the Gaussian distri‐

butions, but you are not told which one, and you do not know what the parameters of these distributions are.

There are several GMM variants: in the simplest variant, implemented in the Gaus sianMixture class, you must know in advance the number $k$ of Gaussian distribu‐ tions. The dataset X is assumed to have been generated through the following probabilistic process:

• For each instance, a cluster is picked randomly among $k$ clusters. The probability of choosing the $j ^ { \mathrm { t h } }$ cluster is defined by the cluster’s weight $\phi ^ { ( j ) }$ .7 The index of the cluster chosen for the $i ^ { \mathrm { t h } }$ instance is noted $z ^ { ( i ) }$ .   
• If $z ^ { ( i ) } { = } j ,$ , meaning the $i ^ { \mathrm { t h } }$ instance has been assigned to the $j ^ { \mathrm { t h } }$ cluster, the location $\mathbf { X } ^ { ( i ) }$ of this instance is sampled randomly from the Gaussian distribution with mean $\mu ^ { ( j ) }$ and covariance matrix $\pmb { \Sigma } ^ { ( j ) }$ . This is noted $\mathbf { x } ^ { ( i ) } \sim \mathcal { N } \big ( \mu ^ { ( j ) } , \Sigma ^ { ( j ) } \big ) ,$

This generative process can be represented as a graphical model (see Figure 9-16). This is a graph which represents the structure of the conditional dependencies between random variables.

![](images/76e4238bad2f2c6fc617296362818de1c9aaf4ccbac55c8ae427f6cfbaa5665e.jpg)  
Figure 9-16. Gaussian mixture model

Here is how to interpret it:8

• The circles represent random variables.   
• The squares represent fixed values (i.e., parameters of the model).

• The large rectangles are called plates: they indicate that their content is repeated several times.   
• The number indicated at the bottom right hand side of each plate indicates how many times its content is repeated, so there are m random variables $\boldsymbol { z } ^ { ( i ) }$ (from $z ^ { ( 1 ) }$ to ${ z } ^ { ( m ) }$ ) and m random variables $\mathbf { X } ^ { ( i ) }$ , and $k$ means $\mu ^ { ( j ) }$ and $k$ covariance matrices $\pmb { \Sigma } ^ { ( j ) }$ , but just one weight vector $\boldsymbol { \Phi }$ (containing all the weights $\phi ^ { ( 1 ) }$ to $\phi ^ { ( k ) } .$ ).   
• Each variable ${ z ^ { ( i ) } }$ is drawn from the categorical distribution with weights $\boldsymbol { \Phi }$ . Each variable $\mathbf { X } ^ { ( i ) }$ is drawn from the normal distribution with the mean and covariance matrix defined by its cluster ${ z ^ { ( i ) } }$ .   
• The solid arrows represent conditional dependencies. For example, the probabil‐ ity distribution for each random variable $z ^ { ( i ) }$ depends on the weight vector $\boldsymbol { \Phi }$ Note that when an arrow crosses a plate boundary, it means that it applies to all the repetitions of that plate, so for example the weight vector $\boldsymbol { \Phi }$ conditions the probability distributions of all the random variables $\mathbf { X } ^ { ( 1 ) }$ to $\mathbf { X } ^ { ( m ) }$ .   
• The squiggly arrow from $z ^ { ( i ) }$ to $\mathbf { X } ^ { ( i ) }$ represents a switch: depending on the value of $z ^ { ( i ) }$ , the instance $\mathbf { X } ^ { ( i ) }$ will be sampled from a different Gaussian distribution. For example, if $z ^ { ( i ) } { = } j ,$ then $\mathbf { x } ^ { ( i ) } \sim \mathcal { N } \big ( \mu ^ { ( j ) } , \Sigma ^ { ( j ) } \big ) .$ .   
• Shaded nodes indicate that the value is known, so in this case only the random variables $\mathbf { X } ^ { ( i ) }$ have known values: they are called observed variables. The unknown random variables $z ^ { ( i ) }$ are called latent variables.

So what can you do with such a model? Well, given the dataset X, you typically want to start by estimating the weights $\boldsymbol { \Phi }$ and all the distribution parameters $\boldsymbol { \mu } ^ { ( 1 ) }$ to ${ \mu } ^ { ( k ) }$ and $\pmb { \Sigma } ^ { ( 1 ) }$ to $\pmb { \Sigma } ^ { ( k ) }$ . Scikit-Learn’s GaussianMixture class makes this trivial:

```python
from sklearn.mixture import GaussianMixture  
gm = GaussianMixture(n_components=3, n_init=10)  
gm.fit(X) 
```

Let’s look at the parameters that the algorithm estimated:

```txt
>>>gmweights_   
array([0.20965228,0.4000662,0.39028152])   
>>gm.means_   
array([[3.39909717, 1.05933727], [-1.40763984, 1.42710194], [0.05135313, 0.07524095]])   
>>gm.covariances_   
array([[1.14807234,-0.03270354], [-0.03270354, 0.95496237]], [[0.63478101, 0.72969804], [0.72969804, 1.1609872]] 
```

[[ 0.68809572, 0.79608475],

[ 0.79608475, 1.21234145]]])

Great, it worked fine! Indeed, the weights that were used to generate the data were 0.2, 0.4 and 0.4, and similarly, the means and covariance matrices were very close to those found by the algorithm. But how? This class relies on the Expectation-Maximization (EM) algorithm, which has many similarities with the K-Means algo‐ rithm: it also initializes the cluster parameters randomly, then it repeats two steps until convergence, first assigning instances to clusters (this is called the expectation step) then updating the clusters (this is called the maximization step). Sounds famil‐ iar? Indeed, in the context of clustering you can think of EM as a generalization of K-Means which not only finds the cluster centers $\mathbf { \rho } _ { \mathbf { \mu } } ^ { \mathbf { \tilde { \mu } } ( 1 ) }$ to ${ \boldsymbol { \mu } } ^ { ( k ) }$ ), but also their size, shape and orientation $\mathbf { \Delta } \cdot \mathbf {  { \Sigma } } ^ { ( 1 ) }$ to $\pmb { \Sigma } ^ { ( k ) }$ ), as well as their relative weights $\boldsymbol { \cdot } \phi ^ { ( 1 ) }$ to $\phi ^ { ( k ) } ,$ ). Unlike K-Means, EM uses soft cluster assignments rather than hard assignments: for each instance during the expectation step, the algorithm estimates the probability that it belongs to each cluster (based on the current cluster parameters). Then, during the maximization step, each cluster is updated using all the instances in the dataset, with each instance weighted by the estimated probability that it belongs to that cluster. These probabilities are called the responsibilities of the clusters for the instances. Dur‐ ing the maximization step, each cluster’s update will mostly be impacted by the instances it is most responsible for.

![](images/79dba9c0f6e9c7729bc44a75d691f739bec1c337f59b1ef0ee5edb9f80c7dbb2.jpg)

Unfortunately, just like K-Means, EM can end up converging to poor solutions, so it needs to be run several times, keeping only the best solution. This is why we set n_init to 10. Be careful: by default n_init is only set to 1.

You can check whether or not the algorithm converged and how many iterations it took:

```txt
>>>gm.converged_ True   
>>>gm.n_iter_ 3 
```

Okay, now that you have an estimate of the location, size, shape, orientation and rela‐ tive weight of each cluster, the model can easily assign each instance to the most likely cluster (hard clustering) or estimate the probability that it belongs to a particular cluster (soft clustering). For this, just use the predict() method for hard clustering, or the predict_proba() method for soft clustering:

```csv
>>gm.predict(X)   
array([2,2,1,...,0,0,0])   
>>gm.predict_proba(X)   
array([[2.32389467e-02,6.77397850e-07,9.76760376e-01], [1.64685609e-02,6.75361303e-04,9.82856078e-01], 
```

```txt
[2.0153533e-06, 9.99923053e-01, 7.49319577e-05], ...  
[9.99999571e-01, 2.13946075e-26, 4.28788333e-07], [1.00000000e+00, 1.46454409e-41, 5.12459171e-16], [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]]) 
```

It is a generative model, meaning you can actually sample new instances from it (note that they are ordered by cluster index):

```erlang
>>> X_new, y_new = gm.sample(6)
>>> X_new
array([[2.95400315, 2.63680992],
[-1.16654575, 1.62792705],
[-1.39477712, -1.48511338],
[0.27221525, 0.690366],
[0.54095936, 0.48591934],
[0.38064009, -0.56240465])
>>> y_new
array([0, 1, 2, 2, 2, 2]) 
```

It is also possible to estimate the density of the model at any given location. This is achieved using the score_samples() method: for each instance it is given, this method estimates the log of the probability density function (PDF) at that location. The greater the score, the higher the density:

```txt
>>gm.score_samples(X) array([-2.60782346，-3.57106041，-3.33003479，...，-3.51352783, -4.39802535，-3.80743859]) 
```

If you compute the exponential of these scores, you get the value of the PDF at the location of the given instances. These are not probabilities, but probability densities: they can take on any positive value, not just between 0 and 1. To estimate the proba‐ bility that an instance will fall within a particular region, you would have to integrate the PDF over that region (if you do so over the entire space of possible instance loca‐ tions, the result will be 1).

Figure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the density contours of this model:

![](images/14d86334963c2678ecfa41c4ec4ffb78c8825e68175e503efc39eb8fa5a25bbc.jpg)  
Figure 9-17. Cluster means, decision boundaries and density contours of a trained Gaus‐ sian mixture model

Nice! The algorithm clearly found an excellent solution. Of course, we made its task easy by actually generating the data using a set of 2D Gaussian distributions (unfortu‐ nately, real life data is not always so Gaussian and low-dimensional), and we also gave the algorithm the correct number of clusters. When there are many dimensions, or many clusters, or few instances, EM can struggle to converge to the optimal solution. You might need to reduce the difficulty of the task by limiting the number of parame‐ ters that the algorithm has to learn: one way to do this is to limit the range of shapes and orientations that the clusters can have. This can be achieved by imposing con‐ straints on the covariance matrices. To do this, just set the covariance_type hyper‐ parameter to one of the following values:

• "spherical": all clusters must be spherical, but they can have different diameters (i.e., different variances).   
• "diag": clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s axes must be parallel to the coordinate axes (i.e., the covariance matrices must be diagonal).   
• "tied": all clusters must have the same ellipsoidal shape, size and orientation (i.e., all clusters share the same covariance matrix).

By default, covariance_type is equal to "full", which means that each cluster can take on any shape, size and orientation (it has its own unconstrained covariance matrix). Figure 9-18 plots the solutions found by the EM algorithm when cova riance_type is set to "tied" or "spherical“.

![](images/85fcb7a4deaa92c0765d36a8b5060f92d5b276e559fa3810f9c2ab2d43235fa9.jpg)

![](images/6f0ab7c85dce5141d57ec6a7a9d03efd73195dc197edc83e098732728520dc2e.jpg)

![](images/9698d3890ab873e5e84467612b1ab6a2d1cd824ece555ea9dbcaeddad52a3b7d.jpg)  
Figure 9-18. covariance_type_diagram

The computational complexity of training a GaussianMixture model depends on the number of instances m, the number of dimensions $n$ , the number of clusters $k ,$ and the constraints on the covariance matrices. If covariance_type is "spherical or "diag", it is $\mathrm { O } ( k m n )$ , assuming the data has a clustering structure. If cova riance_type is "tied" or "full", it is $\mathrm { O } ( k m n ^ { 2 } + k n ^ { 3 } )$ , so it will not scale to large numbers of features.

Gaussian mixture models can also be used for anomaly detection. Let’s see how.

# Anomaly Detection using Gaussian Mixtures

Anomaly detection (also called outlier detection) is the task of detecting instances that deviate strongly from the norm. These instances are of course called anomalies or outliers, while the normal instances are called inliers. Anomaly detection is very use‐ ful in a wide variety of applications, for example in fraud detection, or for detecting defective products in manufacturing, or to remove outliers from a dataset before training another model, which can significantly improve the performance of the resulting model.

Using a Gaussian mixture model for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. You must define what density threshold you want to use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually wellknown. Say it is equal to $4 \%$ , then you can set the density threshold to be the value that results in having $4 \%$ of the instances located in areas below that threshold den‐ sity. If you notice that you get too many false positives (i.e., perfectly good products that are flagged as defective), you can lower the threshold. Conversely, if you have too many false negatives (i.e., defective products that the system does not flag as defec‐ tive), you can increase the threshold. This is the usual precision/recall tradeoff (see Chapter 3). Here is how you would identify the outliers using the 4th percentile low‐

est density as the threshold (i.e., approximately $4 \%$ of the instances will be flagged as anomalies):

```python
densities = gm.score_samples(X)  
density_threshold = np-percentile(densities, 4)  
anomalies = X[densities < density_threshold] 
```

These anomalies are represented as stars on Figure 9-19:

![](images/d72f32fad56082e594af0a7d0d74ac74555888d5006b37a3d6832d52ba3042a7.jpg)  
Figure 9-19. Anomaly detection using a Gaussian mixture model

A closely related task is novelty detection: it differs from anomaly detection in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers, whereas anomaly detection does not make this assumption. Indeed, outlier detection is often precisely used to clean up a dataset.

![](images/58d867eb84b4c1bb6fa54d221a57fb84dd3f00c2df2bde737cb993dd44591182.jpg)

Gaussian mixture models try to fit all the data, including the outli‐ ers, so if you have too many of them, this will bias the model’s view of “normality”: some outliers may wrongly be considered as nor‐ mal. If this happens, you can try to fit the model once, use it to detect and remove the most extreme outliers, then fit the model again on the cleaned up dataset. Another approach is to use robust covariance estimation methods (see the EllipticEnvelope class).

Just like K-Means, the GaussianMixture algorithm requires you to specify the num‐ ber of clusters. So how can you find it?

# Selecting the Number of Clusters

With K-Means, you could use the inertia or the silhouette score to select the appro‐ priate number of clusters, but with Gaussian mixtures, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have dif‐ ferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐

mation criterion such as the Bayesian information criterion (BIC) or the Akaike information criterion (AIC), defined in Equation 9-1.

Equation 9-1. Bayesian information criterion (BIC) and Akaike information criterion (AIC)

$$
B I C = \log (m) p - 2 \log (\hat {L})
$$

$$
A I C = 2 p - 2 \log (\hat {L})
$$

• m is the number of instances, as always.   
• $\boldsymbol { p }$ is the number of parameters learned by the model.   
• $\hat { L }$ is the maximized value of the likelihood function of the model.

Both the BIC and the AIC penalize models that have more parameters to learn (e.g., more clusters), and reward models that fit the data well. They often end up selecting the same model, but when they differ, the model selected by the BIC tends to be sim‐ pler (fewer parameters) than the one selected by the AIC, but it does not fit the data quite as well (this is especially true for larger datasets).

# Likelihood function

The terms “probability” and “likelihood” are often used interchangeably in the English language, but they have very different meanings in statistics: given a statistical model with some parameters θ, the word “probability” is used to describe how plausi‐ ble a future outcome x is (knowing the parameter values θ), while the word “likeli‐ hood” is used to describe how plausible a particular set of parameter values θ are, after the outcome x is known.

Consider a one-dimensional mixture model of two Gaussian distributions centered at -4 and $+ 1$ . For simplicity, this toy model has a single parameter $\theta$ that controls the standard deviations of both distributions. The top left contour plot in Figure 9-20 shows the entire model $f ( x ; \theta )$ as a function of both $x$ and θ. To estimate the probabil‐ ity distribution of a future outcome $x ,$ you need to set the model parameter θ. For example, if you set it to $\scriptstyle \theta = 1 . 3$ (the horizontal line), you get the probability density function $f ( x ; \theta = 1 . 3 )$ shown in the lower left plot. Say you want to estimate the proba‐ bility that $x$ will fall between -2 and $+ 2$ , you must calculate the integral of the PDF on this range (i.e., the surface of the shaded region). On the other hand, if you have observed a single instance $x { = } 2 . 5$ (the vertical line in the upper left plot), you get the likelihood function noted $\mathcal { L } ( \boldsymbol { \theta } | \boldsymbol { x } { = } 2 . 5 ) \mathrm { = f } ( \boldsymbol { x } { = } 2 . 5 ; \boldsymbol { \theta } )$ represented in the upper right plot.

In short, the PDF is a function of $x$ (with $\theta$ fixed) while the likelihood function is a function of $\theta$ (with $x$ fixed). It is important to understand that the likelihood function is not a probability distribution: if you integrate a probability distribution over all

possible values of $x _ { i }$ , you always get 1, but if you integrate the likelihood function over all possible values of $\theta$ , the result can be any positive value.

![](images/8d781c4a1946bb66832c265b3aace8d34a80b33e5b1c4ce552a6f7c1fa9e5267.jpg)

![](images/76be63a987bb6ab01801910fd05fd8b3f9a3f10e7ce647cf5aad1bc35d7ae1f6.jpg)

![](images/25f562569fb12275bd56219515f30f580cab989789efcafc4c0df539f47fd656.jpg)

![](images/5f09949a56d8145d7ecd1a6b97c50d5ca16c4a5b3540dbd238cc92fa169d35a3.jpg)  
Figure 9-20. A model’s parametric function (top le), and some derived functions: a PDF (lower le), a likelihood function (top right) and a log likelihood function (lower right)

Given a dataset X, a common task is to try to estimate the most likely values for the model parameters. To do this, you must find the values that maximize the likelihood function, given X. In this example, if you have observed a single instance $x { = } 2 . 5$ , the maximum likelihood estimate (MLE) of $\theta$ is $\widehat { \theta } { = } 1 . 5$ . If a prior probability distribution $g$ over $\theta$ exists, it is possible to take it into account by maximizing ${ \mathcal { L } } ( \theta | x ) \mathbf { g } ( \theta )$ rather than just maximizing ${ \mathcal { L } } ( \theta | x )$ . This is called maximum a-posteriori (MAP) estimation. Since MAP constrains the parameter values, you can think of it as a regularized ver‐ sion of MLE.

Notice that it is equivalent to maximize the likelihood function or to maximize its logarithm (represented in the lower right hand side of Figure 9-20): indeed, the loga‐ rithm is a strictly increasing function, so if $\theta$ maximizes the log likelihood, it also maximizes the likelihood. It turns out that it is generally easier to maximize the log likelihood. For example, if you observed several independent instances $x ^ { ( 1 ) }$ to $x ^ { ( m ) }$ , you would need to find the value of $\theta$ that maximizes the product of the individual likeli‐ hood functions. But it is equivalent, and much simpler, to maximize the sum (not the product) of the log likelihood functions, thanks to the magic of the logarithm which converts products into sums: $\scriptstyle \log ( a b ) = \log ( a ) + \log ( b )$ .

Once you have estimated $\hat { \theta }$ , the value of $\theta$ that maximizes the likelihood function, then you are ready to compute ${ \hat { L } } = { \mathcal { L } } \left( { \hat { \theta } } , \mathbf { X } \right)$ . This is the value which is used to com‐ pute the AIC and BIC: you can think of it as a measure of how well the model fits the data.

To compute the BIC and AIC, just call the bic() or aic() methods:

```txt
>>> gm.bic(X)  
8189.74345832983  
>>> gm.aic(X)  
8102.518178214792 
```

![](images/a5417b83cf6d652165aac9859db2d25e36647d3fe067f8388198a602883955a2.jpg)  
Figure 9-21 shows the BIC for different numbers of clusters $k$ . As you can see, both the BIC and the AIC are lowest when $k { = } 3$ , so it is most likely the best choice. Note that we could also search for the best value for the covariance_type hyperparameter. For example, if it is "spherical" rather than "full", then the model has much fewer parameters to learn, but it does not fit the data as well.   
Figure 9-21. AIC and BIC for dierent numbers of clusters k

# Bayesian Gaussian Mixture Models

Rather than manually searching for the optimal number of clusters, it is possible to use instead the BayesianGaussianMixture class which is capable of giving weights equal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com ponents to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically. For example, let’s set the number of clusters to 10 and see what happens:

```python
>>> from sklearn.metrics import BayesianGaussianMixture
>>> bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)
>>> bgm.fit(X)
>>> np.round(bgmweights_, 2)
array([0.4, 0.21, 0.4, 0., 0., 0., 0., 0., 0.])
>>> 
```

Perfect: the algorithm automatically detected that only 3 clusters are needed, and the resulting clusters are almost identical to the ones in Figure 9-17.

In this model, the cluster parameters (including the weights, means and covariance matrices) are not treated as fixed model parameters anymore, but as latent random variables, like the cluster assignments (see Figure 9-22). So z now includes both the cluster parameters and the cluster assignments.

![](images/3740fad0456cf59a8197e6953ea5a9f5dbe8df606cfa7e8a088847b852ec7559.jpg)  
Figure 9-22. Bayesian Gaussian mixture model

Prior knowledge about the latent variables z can be encoded in a probability distribu‐ tion $p ( \mathbf { z } )$ called the prior. For example, we may have a prior belief that the clusters are likely to be few (low concentration), or conversely, that they are more likely to be plentiful (high concentration). This can be adjusted using the weight_concentra tion_prior hyperparameter. Setting it to 0.01 or 1000 gives very different clusterings (see Figure 9-23). However, the more data we have, the less the priors matter. In fact, to plot diagrams with such large differences, you must use very strong priors and lit‐ tle data.

![](images/d3b2c158047860b3fd0ea3bfdf0c19d7c15ced5a470ce993b9d204883ed7d0be.jpg)  
Figure 9-23. Using dierent concentration priors

![](images/07eac37ebf85c8b827d94fbb993560fed1e083d8154e399a0ecc60f38b7642e4.jpg)

The fact that you see only 3 regions in the right plot although there are 4 centroids is not a bug: the weight of the top-right cluster is much larger than the weight of the lower-right cluster, so the prob‐ ability that any given point in this region belongs to the top-right cluster is greater than the probability that it belongs to the lowerright cluster, even near the lower-right cluster.

Bayes’ theorem (Equation 9-2) tells us how to update the probability distribution over the latent variables after we observe some data X. It computes the posterior distribu‐ tion $p ( \mathbf { z } | \mathbf { X } )$ , which is the conditional probability of z given X.

Equation 9-2. Bayes’ theorem

$$
p (\mathbf {z} \mid \mathbf {X}) = \text {P o s t e r i o r} = \frac {\text {L i k e l i h o o d} \times \text {P r i o r}}{\text {E v i d e n c e}} = \frac {p (\mathbf {X} \mid \mathbf {z}) p (\mathbf {z})}{p (\mathbf {X})}
$$

Unfortunately, in a Gaussian mixture model (and many other problems), the denomi‐ nator $p ( \mathbf { x } )$ is intractable, as it requires integrating over all the possible values of z (Equation 9-3). This means considering all possible combinations of cluster parame‐ ters and cluster assignments.

Equation 9-3. e evidence $p ( \mathbf { \boldsymbol { X } } )$ is oen intractable

$$
p (\mathbf {X}) = \int p (\mathbf {X} | \mathbf {z}) p (\mathbf {z}) d \mathbf {z}
$$

This is one of the central problems in Bayesian statistics, and there are several approaches to solving it. One of them is variational inference, which picks a family of distributions $q ( { \bf { z } } ; \lambda )$ with its own variational parameters λ (lambda), then it optimizes these parameters to make $q ( \mathbf { z } )$ a good approximation of $p ( \mathbf { z } | \mathbf { X } )$ . This is achieved by finding the value of $\pmb { \lambda }$ that minimizes the KL divergence from $q ( \mathbf { z } )$ to $p ( \mathbf { z } | \mathbf { X } )$ , noted $\mathrm { D } _ { \mathrm { K L } } ( q | | p )$ . The KL divergence equation is shown in (see Equation 9-4), and it can be rewritten as the log of the evidence (log $p ( \mathbf { X } ) )$ minus the evidence lower bound (ELBO). Since the log of the evidence does not depend on $q$ , it is a constant term, so minimizing the KL divergence just requires maximizing the ELBO.

Equation 9-4. KL divergence from $q ( z )$ to $p ( z | X )$

$$
\begin{array}{l} D _ {K L} (q \parallel p) = \mathbb {E} _ {q} \bigg [ \log \frac {q (\mathbf {z})}{p (\mathbf {z} \mid \mathbf {X})} \bigg ] \\ = \mathbb {E} _ {q} [ \log q (\mathbf {z}) - \log p (\mathbf {z} \mid \mathbf {X}) ] \\ = \mathbb {E} _ {q} \left[ \log q (\mathbf {z}) - \log \frac {p (\mathbf {z} , \mathbf {X})}{p (\mathbf {X})} \right] \\ = \mathbb {E} _ {q} [ \log q (\mathbf {z}) - \log p (\mathbf {z}, \mathbf {X}) + \log p (\mathbf {X}) ] \\ = \mathbb {E} _ {q} [ \log q (\mathbf {z}) ] - \mathbb {E} _ {q} [ \log p (\mathbf {z}, \mathbf {X}) ] + \mathbb {E} _ {q} [ \log p (\mathbf {X}) ] \\ = \mathbb {E} _ {q} [ \log p (\mathbf {X}) ] - \left(\mathbb {E} _ {q} [ \log p (\mathbf {z}, \mathbf {X}) ] - \mathbb {E} _ {q} [ \log q (\mathbf {z}) ]\right) \\ = \log p (\mathbf {X}) - \operatorname {E L B O} \\ w h e r e \operatorname {E L B O} = \mathbb {E} _ {q} [ \log p (\mathbf {z}, \mathbf {X}) ] - \mathbb {E} _ {q} [ \log q (\mathbf {z}) ] \\ \end{array}
$$

In practice, there are different techniques to maximize the ELBO. In mean eld varia‐ tional inference, it is necessary to pick the family of distributions $q ( { \bf { z } } ; \lambda )$ and the prior $p ( z )$ very carefully to ensure that the equation for the ELBO simplifies to a form that can actually be computed. Unfortunately, there is no general way to do this, it depends on the task and requires some mathematical skills. For example, the distribu‐ tions and lower bound equations used in Scikit-Learn’s BayesianGaussianMixture class are presented in the documentation. From these equations it is possible to derive update equations for the cluster parameters and assignment variables: these are then used very much like in the Expectation-Maximization algorithm. In fact, the compu‐ tational complexity of the BayesianGaussianMixture class is similar to that of the GaussianMixture class (but generally significantly slower). A simpler approach to maximizing the ELBO is called black box stochastic variational inference (BBSVI): at each iteration, a few samples are drawn from $q$ and they are used to estimate the gra‐ dients of the ELBO with regards to the variational parameters $\pmb { \lambda }$ , which are then used in a gradient ascent step. This approach makes it possible to use Bayesian inference with any kind of model (provided it is differentiable), even deep neural networks: this is called Bayesian deep learning.

![](images/7e015dcf11380f06d7ca8f5ebd02a12f013097d0f1ffcd966923157ab499d94c.jpg)

If you want to dive deeper into Bayesian statistics, check out the Bayesian Data Analysis book by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.

Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try to fit a dataset with different shapes, you may have bad surprises. For example, let’s see what happens if we use a Bayesian Gaussian mixture model to cluster the moons dataset (see Figure 9-24):

![](images/32046d110a56e2839b827eb55df1a5795645f3249026cd06ebddeb9ffabf9252.jpg)

![](images/7ccab2fd18ffa4e4b46a929144440b6376f4bcf517fff39e587835edd0f4ded2.jpg)  
Figure 9-24. moons_vs_bgm_diagram

Oops, the algorithm desperately searched for ellipsoids, so it found 8 different clus‐ ters instead of 2. The density estimation is not too bad, so this model could perhaps be used for anomaly detection, but it failed to identify the two moons. Let’s now look at a few clustering algorithms capable of dealing with arbitrarily shaped clusters.

# Other Anomaly Detection and Novelty Detection Algorithms

Scikit-Learn also implements a few algorithms dedicated to anomaly detection or novelty detection:

• Fast-MCD (minimum covariance determinant), implemented by the EllipticEn velope class: this algorithm is useful for outlier detection, in particular to cleanup a dataset. It assumes that the normal instances (inliers) are generated from a single Gaussian distribution (not a mixture), but it also assumes that the dataset is contaminated with outliers that were not generated from this Gaussian distribution. When it estimates the parameters of the Gaussian distribution (i.e., the shape of the elliptic envelope around the inliers), it is careful to ignore the instances that are most likely outliers. This gives a better estimation of the elliptic envelope, and thus makes it better at identifying the outliers.   
• Isolation forest: this is an efficient algorithm for outlier detection, especially in high-dimensional datasets. The algorithm builds a Random Forest in which each Decision Tree is grown randomly: at each node, it picks a feature randomly, then it picks a random threshold value (between the min and max value) to split the dataset in two. The dataset gradually gets chopped into pieces this way, until all instances end up isolated from the other instances. An anomaly is usually far from other instances, so on average (across all the Decision Trees) it tends to get isolated in less steps than normal instances.   
• Local outlier factor (LOF): this algorithm is also good for outlier detection. It compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its $k$ nearest neighbors.   
• One-class SVM: this algorithm is better suited for novelty detection. Recall that a kernelized SVM classifier separates two classes by first (implicitly) mapping all the instances to a high-dimensional space, then separating the two classes using a linear SVM classifier within this high-dimensional space (see Chapter 5). Since we just have one class of instances, the one-class SVM algorithm instead tries to separate the instances in high-dimensional space from the origin. In the original space, this will correspond to finding a small region that encompasses all the instances. If a new instance does not fall within this region, it is an anomaly. There are a few hyperparameters to tweak: the usual ones for a kernelized SVM, plus a margin hyperparameter that corresponds to the probability of a new instance being mistakenly considered as novel, when it is in fact normal. It works great, especially with high-dimensional datasets, but just like all SVMs, it does not scale to large datasets.

# Neural Networks and Deep Learning

# Introduction to Articial Neural Networks with Keras

![](images/7e05b3bfac587cba04e3fccd5e0008a8190ca52d4ff903667ad6c17c8e1ac743.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 10 in the final release of the book.

Birds inspired us to fly, burdock plants inspired velcro, and countless more inven‐ tions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐ tecture for inspiration on how to build an intelligent machine. This is the key idea that sparked articial neural networks (ANNs). However, although planes were inspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying “units” rather than “neurons”), lest we restrict our creativity to biologically plausible systems.1

ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐ ble, making them ideal to tackle large and highly complex Machine Learning tasks, such as classifying billions of images (e.g., Google Images), powering speech recogni‐ tion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds of millions of users every day (e.g., YouTube), or learning to beat the world champion at the game of Go by playing millions of games against itself (DeepMind’s Alpha‐ Zero).

In the first part of this chapter, we will introduce artificial neural networks, starting with a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐ ceptrons (MLPs) which are heavily used today (other architectures will be explored in the next chapters). In the second part, we will look at how to implement neural net‐ works using the popular Keras API. This is a beautifully designed and simple highlevel API for building, training, evaluating and running neural networks. But don’t be fooled by its simplicity: it is expressive and flexible enough to let you build a wide variety of neural network architectures. In fact, it will probably be sufficient for most of your use cases. Moreover, should you ever need extra flexibility, you can always write custom Keras components using its lower-level API, as we will see in Chap‐ ter 12.

But first, let’s go back in time to see how artificial neural networks came to be!

# From Biological to Articial Neurons

Surprisingly, ANNs have been around for quite a while: they were first introduced back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts. In their landmark paper,2 “A Logical Calculus of Ideas Immanent in Nervous Activity,” McCulloch and Pitts presented a simplified computational model of how biological neurons might work together in animal brains to perform complex computations using propositional logic. This was the first artificial neural network architecture. Since then many other architectures have been invented, as we will see.

The early successes of ANNs until the 1960s led to the widespread belief that we would soon be conversing with truly intelligent machines. When it became clear that this promise would go unfulfilled (at least for quite a while), funding flew elsewhere and ANNs entered a long winter. In the early 1980s there was a revival of interest in connectionism (the study of neural networks), as new architectures were invented and better training techniques were developed. But progress was slow, and by the 1990s other powerful Machine Learning techniques were invented, such as Support Vector Machines (see Chapter 5). These techniques seemed to offer better results and stron‐ ger theoretical foundations than ANNs, so once again the study of neural networks entered a long winter.

Finally, we are now witnessing yet another wave of interest in ANNs. Will this wave die out like the previous ones did? Well, there are a few good reasons to believe that this wave is different and that it will have a much more profound impact on our lives:

• There is now a huge quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.   
• The tremendous increase in computing power since the 1990s now makes it pos‐ sible to train large neural networks in a reasonable amount of time. This is in part due to Moore’s Law, but also thanks to the gaming industry, which has pro‐ duced powerful GPU cards by the millions.   
• The training algorithms have been improved. To be fair they are only slightly dif‐ ferent from the ones used in the 1990s, but these relatively small tweaks have a huge positive impact.   
• Some theoretical limitations of ANNs have turned out to be benign in practice. For example, many people thought that ANN training algorithms were doomed because they were likely to get stuck in local optima, but it turns out that this is rather rare in practice (or when it is the case, they are usually fairly close to the global optimum).   
• ANNs seem to have entered a virtuous circle of funding and progress. Amazing products based on ANNs regularly make the headline news, which pulls more and more attention and funding toward them, resulting in more and more pro‐ gress, and even more amazing products.

# Biological Neurons

Before we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐ resented in Figure 10-1). It is an unusual-looking cell mostly found in animal cerebral cortexes (e.g., your brain), composed of a cell body containing the nucleus and most of the cell’s complex components, and many branching extensions called dendrites, plus one very long extension called the axon. The axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called telodendria, and at the tip of these branches are minuscule structures called synaptic terminals (or simply synap‐ ses), which are connected to the dendrites (or directly to the cell body) of other neu‐ rons. Biological neurons receive short electrical impulses called signals from other neurons via these synapses. When a neuron receives a sufficient number of signals from other neurons within a few milliseconds, it fires its own signals.

![](images/f5270cebb899254bd91645761f2c089862c6113b6db137f40c1a2dfd752731ac.jpg)  
Figure 10-1. Biological neuron3

Thus, individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions of neurons, each neuron typically connec‐ ted to thousands of other neurons. Highly complex computations can be performed by a vast network of fairly simple neurons, much like a complex anthill can emerge from the combined efforts of simple ants. The architecture of biological neural net‐ works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as shown in Figure 10-2.

![](images/abb12aab69e83abdf6e5fc3c083c18fae92e7f3514802d2c12e7af6dd6b89505.jpg)  
Figure 10-2. Multiple layers in a biological neural network (human cortex)5

# Logical Computations with Neurons

Warren McCulloch and Walter Pitts proposed a very simple model of the biological neuron, which later became known as an articial neuron: it has one or more binary (on/off) inputs and one binary output. The artificial neuron simply activates its out‐ put when more than a certain number of its inputs are active. McCulloch and Pitts showed that even with such a simplified model it is possible to build a network of artificial neurons that computes any logical proposition you want. For example, let’s build a few ANNs that perform various logical computations (see Figure 10-3), assuming that a neuron is activated when at least two of its inputs are active.

![](images/d202b91c672940870fce542c412ab5b146fe1dcdc1a8dff42876e9367fe9b51b.jpg)  
Figure 10-3. ANNs performing simple logical computations

• The first network on the left is simply the identity function: if neuron A is activa‐ ted, then neuron C gets activated as well (since it receives two input signals from neuron A), but if neuron A is off, then neuron C is off as well.   
• The second network performs a logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to acti‐ vate neuron C).   
• The third network performs a logical OR: neuron C gets activated if either neu‐ ron A or neuron B is activated (or both).   
• Finally, if we suppose that an input connection can inhibit the neuron’s activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active and if neuron B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, and vice versa.

You can easily imagine how these networks can be combined to compute complex logical expressions (see the exercises at the end of the chapter).

# The Perceptron

The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called

a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU): the inputs and output are now numbers (instead of binary on/off values) and each input con‐ nection is associated with a weight. The TLU computes a weighted sum of its inputs $( z = w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 } + \cdots + w _ { n } x _ { n } = \mathbf { x } ^ { T } \mathbf { w } )$ , then applies a step function to that sum and outputs the result: $h _ { \mathrm { w } } ( \mathbf { x } ) = \mathsf { s t e p } ( z )$ , where $z = \mathbf { x } ^ { T } \mathbf { w }$ .

![](images/4c5e44f690d184a02b9b7696f0a2f129ff9a4627369a1c107feb3d1d65706ef8.jpg)  
Figure 10-4. reshold logic unit

The most common step function used in Perceptrons is the Heaviside step function (see Equation 10-1). Sometimes the sign function is used instead.

Equation 10-1. Common step functions used in Perceptrons

$$
\text {h e a v i s i d e} (z) = \left\{ \begin{array}{l l} 0 & \text {i f} z <   0 \\ 1 & \text {i f} z \geq 0 \end{array} \right. \quad \operatorname {s g n} (z) = \left\{ \begin{array}{l l} - 1 & \text {i f} z <   0 \\ 0 & \text {i f} z = 0 \\ + 1 & \text {i f} z > 0 \end{array} \right.
$$

A single TLU can be used for simple linear binary classification. It computes a linear combination of the inputs and if the result exceeds a threshold, it outputs the positive class or else outputs the negative class (just like a Logistic Regression classifier or a linear SVM). For example, you could use a single TLU to classify iris flowers based on the petal length and width (also adding an extra bias feature $x _ { 0 } = 1$ , just like we did in previous chapters). Training a TLU in this case means finding the right values for $w _ { 0 } \mathrm { . }$ , $w _ { \mathrm { 1 : } }$ , and $w _ { 2 }$ (the training algorithm is discussed shortly).

A Perceptron is simply composed of a single layer of TLUs,6 with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), it is called a fully connected layer or a dense layer. To represent the fact that each input is sent to every TLU, it is common to draw special passthrough neurons called input neurons: they just output whatever input they are fed. All the input neurons form the input layer. Moreover, an extra bias fea‐

ture is generally added $( x _ { 0 } = 1 )$ : it is typically represented using a special type of neu‐ ron called a bias neuron, which just outputs 1 all the time. A Perceptron with two inputs and three outputs is represented in Figure 10-5. This Perceptron can classify instances simultaneously into three different binary classes, which makes it a multi‐ output classifier.

![](images/1bd5ab423d2f1218fac4a8da58fda041132e3d3cc5cdff2f3fa70a9af54ab406.jpg)  
Figure 10-5. Perceptron diagram

Thanks to the magic of linear algebra, it is possible to efficiently compute the outputs of a layer of artificial neurons for several instances at once, by using Equation 10-2:

Equation 10-2. Computing the outputs of a fully connected layer

$$
h _ {\mathbf {W}, \mathbf {b}} (\mathbf {X}) = \phi (\mathbf {X} \mathbf {W} + \mathbf {b})
$$

• As always, X represents the matrix of input features. It has one row per instance, one column per feature.   
• The weight matrix W contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artifi‐ cial neuron in the layer.   
• The bias vector b contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.   
• The function $\phi$ is called the activation function: when the artificial neurons are TLUs, it is a step function (but we will discuss other activation functions shortly).

So how is a Perceptron trained? The Perceptron training algorithm proposed by Frank Rosenblatt was largely inspired by Hebb’s rule. In his book e Organization of Behavior, published in 1949, Donald Hebb suggested that when a biological neuron often triggers another neuron, the connection between these two neurons grows stronger. This idea was later summarized by Siegrid Löwel in this catchy phrase: “Cells that fire together, wire together.” This rule later became known as Hebb’s rule

(or Hebbian learning); that is, the connection weight between two neurons is increased whenever they have the same output. Perceptrons are trained using a var‐ iant of this rule that takes into account the error made by the network; it reinforces connections that help reduce the error. More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The rule is shown in Equation 10-3.

Equation 10-3. Perceptron learning rule (weight update)

$$
w _ {i, j} ^ {\mathrm {(n e x t s t e p)}} = w _ {i, j} + \eta \big (y _ {j} - \hat {y} _ {j} \big) x _ {i}
$$

• $w _ { i , j }$ is the connection weight between the $i ^ { \mathrm { t h } }$ input neuron and the $j ^ { \mathrm { t h } }$ output neu‐ ron.   
• $x _ { i }$ is the $i ^ { \mathrm { t h } }$ input value of the current training instance.   
• $\hat { y } _ { j }$ is the output of the $j ^ { \mathrm { t h } }$ output neuron for the current training instance.   
• yj is the target output of the $j ^ { \mathrm { t h } }$ output neuron for the current training instance.   
• $\eta$ is the learning rate.

The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers). However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution.7 This is called the Perceptron convergence theorem.

Scikit-Learn provides a Perceptron class that implements a single TLU network. It can be used pretty much as you would expect—for example, on the iris dataset (intro‐ duced in Chapter 4):

import numpy as np   
from sklearn.datasets import load_iris   
from sklearn.linear_model import Perceptron   
iris $=$ load_iris()   
X $=$ iris.data(:,2,3)] #petal length,petal width y $=$ (iris.target $\equiv = 0$ .astype(np.int) # Iris Setosa?   
per_clf $=$ Perceptron()   
per_clf.fit(X,y)

$$
y _ {\text {p r e d}} = \text {p e r} _ {\text {c l f . p r e d i c t}} ([ 2, 0. 5 ])
$$

You may have noticed the fact that the Perceptron learning algorithm strongly resem‐ bles Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss="perceptron", learning_rate $^ { 1 } =$ "constant", eta0=1 (the learning rate), and penalty=None (no regu‐ larization).

Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they just make predictions based on a hard threshold. This is one of the good reasons to prefer Logistic Regression over Perceptrons.

In their 1969 monograph titled Perceptrons, Marvin Minsky and Seymour Papert highlighted a number of serious weaknesses of Perceptrons, in particular the fact that they are incapable of solving some trivial problems (e.g., the Exclusive OR (XOR) classification problem; see the left side of Figure 10-6). Of course this is true of any other linear classification model as well (such as Logistic Regression classifiers), but researchers had expected much more from Perceptrons, and their disappointment was great, and many researchers dropped neural networks altogether in favor of higher-level problems such as logic, problem solving, and search.

However, it turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a Multi-Layer Perceptron (MLP). In particular, an MLP can solve the XOR problem, as you can verify by com‐ puting the output of the MLP represented on the right of Figure 10-6: with inputs (0, 0) or (1, 1) the network outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. All connections have a weight equal to 1, except the four connections where the weight is shown. Try verifying that this network indeed solves the XOR problem!

![](images/9188f8360686222f48d4a59eb88bdfdffb9be70f00275cec8e561bd0d85acf44.jpg)

![](images/57fd55319f1f1a97b38eea08967ca644301989c5f8f45b83ab93ff82a1baeb39.jpg)  
Figure 10-6. XOR classication problem and an MLP that solves it

# Multi-Layer Perceptron and Backpropagation

An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layer (see Figure 10-7). The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a bias neuron and is fully connected to the next layer.

![](images/b5050592322d5f35c7d18a6614c073063a502463541d077c134e61c3d646b482.jpg)  
Figure 10-7. Multi-Layer Perceptron

![](images/ff377c5196ac7eee309e11ba2d537e2cb62b3a469e8d89355a521b206bd20603.jpg)

The signal flows only in one direction (from the inputs to the out‐ puts), so this architecture is an example of a feedforward neural net‐ work (FNN).

When an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐ work (DNN). The field of Deep Learning studies DNNs, and more generally models containing deep stacks of computations. However, many people talk about Deep Learning whenever neural networks are involved (even shallow ones).

For many years researchers struggled to find a way to train MLPs, without success. But in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a groundbreaking paper9 introducing the backpropagation training algorithm, which is still used today. In short, it is simply Gradient Descent (introduced in Chapter 4)

using an efficient technique for computing the gradients automatically10: in just two passes through the network (one forward, one backward), the backpropagation algo‐ rithm is able to compute the gradient of the network’s error with regards to every sin‐ gle model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gra‐ dients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.

![](images/49ae8493780eb4b793774f59551c15e723cc74c1d9b154902d0c8515edc2870d.jpg)

Automatically computing gradients is called automatic dierentia‐ tion, or autodi. There are various autodiff techniques, with differ‐ ent pros and cons. The one used by backpropagation is called reverse-mode autodi. It is fast and precise, and is well suited when the function to differentiate has many variables (e.g., connection weights) and few outputs (e.g., one loss). If you want to learn more about autodiff, check out ???.

Let’s run through this algorithm in a bit more detail:

• It handles one mini-batch at a time (for example containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an epoch, as we saw in Chapter 4.   
• Each mini-batch is passed to the network’s input layer, which just sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the forward pass: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.   
• Next, the algorithm measures the network’s output error (i.e., it uses a loss func‐ tion that compares the desired output and the actual output of the network, and returns some measure of the error).   
• Then it computes how much each output connection contributed to the error. This is done analytically by simply applying the chain rule (perhaps the most fun‐ damental rule in calculus), which makes this step fast and precise.   
• The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule—and so on until the algorithm reaches the input layer. As we explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the

network by propagating the error gradient backward through the network (hence the name of the algorithm).

• Finally, the algorithm performs a Gradient Descent step to tweak all the connec‐ tion weights in the network, using the error gradients it just computed.

This algorithm is so important, it’s worth summarizing it again: for each training instance the backpropagation algorithm first makes a prediction (forward pass), measures the error, then goes through each layer in reverse to measure the error con‐ tribution from each connection (reverse pass), and finally slightly tweaks the connec‐ tion weights to reduce the error (Gradient Descent step).

![](images/1b19acc64a697118f27de2fdf74198fc4c387161a8cf988fdd82c684e3ecbb47.jpg)

It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you break the symme‐ try and allow backpropagation to train a diverse team of neurons.

In order for this algorithm to work properly, the authors made a key change to the MLP’s architecture: they replaced the step function with the logistic function, $\sigma ( z ) =$ $1 / \left( 1 + \exp ( - z ) \right)$ . This was essential because the step function contains only flat seg‐ ments, so there is no gradient to work with (Gradient Descent cannot move on a flat surface), while the logistic function has a well-defined nonzero derivative every‐ where, allowing Gradient Descent to make some progress at every step. In fact, the backpropagation algorithm works well with many other activation functions, not just the logistic function. Two other popular activation functions are:

e hyperbolic tangent function tanh ${ \bf \chi } ( z ) = 2 \sigma ( 2 z ) - 1$

Just like the logistic function it is S-shaped, continuous, and differentiable, but its output value ranges from $^ { - 1 }$ to 1 (instead of 0 to 1 in the case of the logistic func‐ tion), which tends to make each layer’s output more or less centered around 0 at the beginning of training. This often helps speed up convergence.

e Rectied Linear Unit function: $R e L U ( z ) = m a x ( 0 , z )$

It is continuous but unfortunately not differentiable at $z = 0$ (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for $z < 0$ . However, in practice it works very well and has the advantage of being

fast to compute11. Most importantly, the fact that it does not have a maximum output value also helps reduce some issues during Gradient Descent (we will come back to this in Chapter 11).

These popular activation functions and their derivatives are represented in Figure 10-8. But wait! Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. For example, say $\operatorname { f } ( x ) = 2 x + 3$ and $\mathrm { g } ( x ) = 5 x - 1$ , then chaining these two linear functions gives you another linear function: $\mathrm { f } ( \mathrm { g } ( \boldsymbol { x } ) ) = 2 ( 5 x - 1 ) + 3 = 1 0 x + 1$ . So if you don’t have some non-linearity between layers, then even a deep stack of layers is equivalent to a single layer: you cannot solve very complex problems with that.

![](images/e8bbfa7bf8d43b0983e86592f0ae327d411720c2b88f9849edc6f64306054ab0.jpg)  
Figure 10-8. Activation functions and their derivatives

Okay! So now you know where neural nets came from, what their architecture is and how to compute their outputs, and you also learned about the backpropagation algo‐ rithm. But what exactly can you do with them?

# Regression MLPs

First, MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object on an image, you need to predict 2D coordi‐ nates, so you need two output neurons. If you also want to place a bounding box around the object, then you need two more numbers: the width and the height of the object. So you end up with 4 output neurons.

In general, when building an MLP for regression, you do not want to use any activa‐ tion function for the output neurons, so they are free to output any range of values. However, if you want to guarantee that the output will always be positive, then you can use the ReLU activation function, or the soplus activation function in the output layer. Finally, if you want to guarantee that the predictions will fall within a given range of values, then you can use the logistic function or the hyperbolic tangent, and scale the labels to the appropriate range: 0 to 1 for the logistic function, or $^ { - 1 }$ to 1 for the hyperbolic tangent.

The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can use the Huber loss, which is a combination of both.

![](images/ecca75d1162a8d09fc292ea21e948ac95546358a2fc91290e99f3afc844a063b.jpg)

The Huber loss is quadratic when the error is smaller than a thres‐ hold δ (typically 1), but linear when the error is larger than δ. This makes it less sensitive to outliers than the mean squared error, and it is often more precise and converges faster than the mean abso‐ lute error.

Table 10-1 summarizes the typical architecture of a regression MLP.   
Table 10-1. Typical Regression MLP Architecture   

<table><tr><td>Hyperparameter</td><td>Typical Value</td></tr><tr><td># input neurons</td><td>One per input feature (e.g., 28 x 28 = 784 for MNIST)</td></tr><tr><td># hidden layers</td><td>Depends on the problem. Typically 1 to 5.</td></tr><tr><td># neurons per hidden layer</td><td>Depends on the problem. Typically 10 to 100.</td></tr><tr><td># output neurons</td><td>1 per prediction dimension</td></tr><tr><td>Hidden activation</td><td>ReLU (or SELU, see Chapter 11)</td></tr><tr><td>Output activation</td><td>None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)</td></tr><tr><td>Loss function</td><td>MSE or MAE/Huber (if outliers)</td></tr></table>

# Classication MLPs

MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probabil‐ ity of the positive class. Obviously, the estimated probability of the negative class is equal to one minus that number.

MLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent

or non-urgent email. In this case, you would need two output neurons, both using the logistic activation function: the first would output the probability that the email is spam and the second would output the probability that it is urgent. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do not necessarily add up to one. This lets the model output any combi‐ nation of labels: you can have non-urgent ham, urgent ham, non-urgent spam, and perhaps even urgent spam (although that would probably be an error).

If each instance can belong only to a single class, out of 3 or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the somax activation function for the whole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4) will ensure that all the estimated probabilities are between 0 and 1 and that they add up to one (which is required if the classes are exclusive). This is called multiclass clas‐ sification.

![](images/4781715a3c5884fdbab0bee9dc17d1c34d9ddbf7526c190e1ad5c75ec02d1d36.jpg)  
Figure 10-9. A modern MLP (including ReLU and somax) for classication

Regarding the loss function, since we are predicting probability distributions, the cross-entropy (also called the log loss, see Chapter 4) is generally a good choice.

Table 10-2 summarizes the typical architecture of a classification MLP.

Table 10-2. Typical Classication MLP Architecture   

<table><tr><td>Hyperparameter</td><td>Binary classification</td><td>Multilabel binary classification</td><td>Multiclass classification</td></tr><tr><td>Input and hidden layers</td><td>Same as regression</td><td>Same as regression</td><td>Same as regression</td></tr><tr><td># output neurons</td><td>1</td><td>1 per label</td><td>1 per class</td></tr><tr><td>Output layer activation</td><td>Logistic</td><td>Logistic</td><td>Softmax</td></tr><tr><td>Loss function</td><td>Cross-Entropy</td><td>Cross-Entropy</td><td>Cross-Entropy</td></tr></table>

![](images/7be04d2aec4e79c5a4cabcdee78f27e2bf3c92f2e5c39b146e1076d3ad32d565.jpg)

Before we go on, I recommend you go through exercise 1, at the end of this chapter. You will play with various neural network architectures and visualize their outputs using the TensorFlow Play‐ ground. This will be very useful to better understand MLPs, for example the effects of all the hyperparameters (number of layers and neurons, activation functions, and more).

Now you have all the concepts you need to start implementing MLPs with Keras!

# Implementing MLPs with Keras

Keras is a high-level Deep Learning API that allows you to easily build, train, evaluate and execute all sorts of neural networks. Its documentation (or specification) is avail‐ able at https://keras.io. The reference implementation is simply called Keras as well, so to avoid any confusion we will call it keras-team (since it is available at https:// github.com/keras-team/keras). It was developed by François Chollet as part of a research project12 and released as an open source project in March 2015. It quickly gained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐ form the heavy computations required by neural networks, keras-team relies on a computation backend. At the present, you can choose from three popular open source deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or Theano.

Moreover, since late 2016, other implementations have been released. You can now run Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras code in a web browser), or PlaidML (which can run on all sorts of GPU devices, not just Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras implementation called tf.keras. It only supports TensorFlow as the backend, but it has the advantage of offering some very useful extra features (see Figure 10-10): for example, it supports TensorFlow’s Data API which makes it quite easy to load and preprocess data efficiently. For this reason, we will use tf.keras in this book. However, in this chapter we will not use any of the TensorFlow-specific features, so the code should run fine on other Keras implementations as well (at least in Python), with only minor modifications, such as changing the imports.

![](images/d7a2b458528ce692c9b91cea99fd527dd3b5691bd701fad1262bf4065e08d258.jpg)  
Figure 10-10. Two Keras implementations: keras-team (le) and tf.keras (right)

As tf.keras is bundled with TensorFlow, let’s install TensorFlow!

# Installing TensorFlow 2

Assuming you installed Jupyter and Scikit-Learn by following the installation instruc‐ tions in Chapter 2, you can simply use pip to install TensorFlow. If you created an isolated environment using virtualenv, you first need to activate it:

$\$ 5$ cd $ML_PATH # Your ML working directory (e.g., $HOME/ml)   
$ source env/bin/activate # on Linux or MacOSX   
$ .\env\Scripts\activate # on Windows

Next, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐ trator rights, or to add the --user option):

$ python3 -m pip install --upgrade tensorflow

![](images/842d730771f558ff70a6d192165d339a5c4377536ea3d7d51822e8e0ac0c3d10.jpg)

For GPU support, you need to install tensorflow-gpu instead of tensorflow, and there are other libraries to install. See https:// tensorow.org/install/gpu for more details.

To test your installation, open a Python shell or a Jupyter notebook, then import Ten‐ sorFlow and tf.keras, and print their versions:

```python
>>> import tensorflow as tf
>>> fromtensorflow import keras
>>> tf._version__
'2.0.0'
>>> keras._version__
'2.2.4-tf' 
```

The second version is the version of the Keras API implemented by tf.keras. Note that it ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus some extra TensorFlow-specific features.

Now let’s use tf.keras! Let’s start by building a simple image classifier.

# Building an Image Classier Using the Sequential API

First, we need to load a dataset. We will tackle Fashion MNIST, which is a drop-in replacement of MNIST (introduced in Chapter 3). It has the exact same format as MNIST (70,000 grayscale images of $2 8 \times 2 8$ pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse and the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about $9 2 \%$ accuracy on MNIST, but only about $8 3 \%$ on Fashion MNIST.

# Using Keras to Load the Dataset

Keras provides some utility functions to fetch and load common datasets, including MNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load Fashion MNIST:

```python
fashion_mnist = keras:datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data() 
```

When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one important difference is that every image is represented as a $2 8 \times 2 8$ array rather than a 1D array of size 784. Moreover, the pixel intensities are represented as integers (from 0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the training set:

```html
>>X_train_full.shape (60000,28,28)   
>>X_train_full.dtyp dtype('uint8') 
```

Note that the dataset is already split into a training set and a test set, but there is no validation set, so let’s create one. Moreover, since we are going to train the neural net‐ work using Gradient Descent, we must scale the input features. For simplicity, we just scale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also converts them to floats):

```python
X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0  
y_valid, y_train = y_train_full[:5000], y_train_full[5000:] 
```

With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class names to know what we are dealing with:

```python
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"] 
```

For example, the first image in the training set represents a coat:

```txt
>>> class_names[y_train[0]]
'Coat' 
```

Figure 10-11 shows a few samples from the Fashion MNIST dataset:

![](images/323c11259d340263a23ab9349112e14fca14fd8b0d056173e9812d007b04f141.jpg)  
Figure 10-11. Samples from Fashion MNIST

# Creating the Model Using the Sequential API

Now let’s build the neural network! Here is a classification MLP with two hidden lay‐ ers:

```txt
model = keras.models Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="relu"))
model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dense(10, activation="softmax")) 
```

Let’s go through this code line by line:

• The first line creates a Sequential model. This is the simplest kind of Keras model, for neural networks that are just composed of a single stack of layers, con‐ nected sequentially. This is called the sequential API.   
• Next, we build the first layer and add it to the model. It is a Flatten layer whose role is simply to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters, it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape: this does not include the batch size, only the shape of the instances. Alternatively, you could add a keras.layers.InputLayer as the first layer, setting shape $^ { 1 } =$ [28,28].   
• Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activa‐ tion function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vec‐

tor of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2.

• Next we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.   
• Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive).

![](images/472f4057d509eb8fd418527455347cb3b96c886a4df4aa85238d4adea728c3e9.jpg)

Specifying activation="relu" is equivalent to activa tion=keras.activations.relu. Other activation functions are available in the keras.activations package, we will use many of them in this book. See https://keras.io/activations/ for the full list.

Instead of adding the layers one by one as we just did, you can pass a list of layers when creating the Sequential model:

model $=$ keras.models Sequential([ keras.layers.Flatten(input_shape $\coloneqq$ [28，28])， keras.layers.Dense(300，activation $\equiv$ "relu")， keras.layers.Dense(100，activation $\equiv$ "relu")， keras.layers.Dense(10，activation $\equiv$ "softmax") ])

# Using Code Examples From keras.io

Code examples documented on keras.io will work fine with tf.keras, but you need to change the imports. For example, consider this keras.io code:

```python
from keras.layers import Dense  
output_layer = Dense(10) 
```

You must change the imports like this:

```python
from tensorflow.keras.layers import Dense
output_layer = Dense(10) 
```

Or simply use full paths, if you prefer:

```python
from tensorflow import keras
output_layer = keras.layers(Dense(10) 
```

This is more verbose, but I use this approach in this book so you can easily see which packages to use, and to avoid confusion between standard classes and custom classes. In production code, I use the previous approach, as do most people.

The model’s summary() method displays all the model’s layers13, including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parame‐ ters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters (we will see exam‐ ples of non-trainable parameters in Chapter 11):

>>> model.summary()   
```txt
Layer (type) Output Shape Param #  
flatten_1 (Flatten) (None, 784) 0  
dense_3 (Dense) (None, 300) 235500  
dense_4 (Dense) (None, 100) 30100  
dense_5 (Dense) (None, 10) 1010 
```

Total params: 266,610

Trainable params: 266,610

Non-trainable params: 0

Note that Dense layers often have a lot of parameters. For example, the first hidden layer has $7 8 4 \times 3 0 0$ connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data. We will come back to this later.

You can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch it by name:

```txt
>>> model.layers
[<tensorflow.pythn.keras.layers.core Flatten at 0x132414e48>, <tensorflow.pythn.keras.layers.core.Dense at 0x1324149b0>, <tensorflow.pythn.keras.layers.core.Dense at 0x1356ba8d0>, <tensorflow.pythn.keras.layers.core.Dense at 0x13240d240>]  
>>> model.layers[1].name
'dense_3'
>>> model.get_layer('dense_3').name
'dense_3' 
```

All the parameters of a layer can be accessed using its get_weights() and set_weights() method. For a Dense layer, this includes both the connection weights and the bias terms:

>>>weights，biases $\equiv$ hidden1.getweights()   
>>>weights   
array([[0.03854964，-0.04054524，0.00599282，...，0.02566582, 0.01032123，0.06914985]， ...， [0.02632413，-0.05105981，-0.00332005，...，0.04175945, 0.0443138，-0.05558084]]，dtype $\equiv$ float32)   
>>>weights.shape   
(784，300)   
>>>biases   
array([0.，0.，0.，0.，0.，0.，0.，0.，...，0.，0.，0.]，dtype $\equiv$ float32)   
>>>biases.shape   
(300,)

Notice that the Dense layer initialized the connection weights randomly (which is needed to break symmetry, as we discussed earlier), and the biases were just initial‐ ized to zeros, which is fine. If you ever want to use a different initialization method, you can set kernel_initializer (kernel is another name for the matrix of connec‐ tion weights) or bias_initializer when creating the layer. We will discuss initializ‐ ers further in Chapter 11, but if you want the full list, see https://keras.io/initializers/.

![](images/2c9c74c55cd4a84469b274a26936a66c54f48adb2debf2f1d961f170543774e9.jpg)

The shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the input_shape when creating the first layer in a Sequential model. However, if you do not specify the input shape, it’s okay: Keras will simply wait until it knows the input shape before it actually builds the model. This will happen either when you feed it actual data (e.g., during training), or when you call its build() method. Until the model is really built, the layers will not have any weights, and you will not be able to do certain things (such as print the model summary or save the model), so if you know the input shape when creating the model, it is best to specify it.

# Compiling the Model

After a model is created, you must call its compile() method to specify the loss func‐ tion and the optimizer to use. Optionally, you can also specify a list of extra metrics to compute during training and evaluation:

```python
model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd", metrics=['accuracy']) 
```

![](images/4ff502fd07d9eaf47b5972dfd2ede56987760dd98dd1191e7719dc09ee972d19.jpg)

Using loss="sparse_categorical_crossentropy" is equivalent to loss=keras.losses.sparse_categorical_crossentropy. Simi‐ larly, optimizer="sgd" is equivalent to optimizer=keras.optimiz ers.SGD() and metrics=["accuracy"] is equivalent to metrics $=$ [keras.metrics.sparse_categorical_accuracy] (when using this loss). We will use many other losses, optimizers and met‐ rics in this book, but for the full lists see https://keras.io/losses/, https://keras.io/optimizers/ and https://keras.io/metrics/.

This requires some explanation. First, we use the "sparse_categorical_crossen tropy" loss because we have sparse labels (i.e., for each instance there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0., $\Theta . , ~ \Theta . , ~ 1 . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ \Theta . , ~ ]$ to represent class 3), then we would need to use the "categorical_crossentropy" loss instead. If we were doing binary classi‐ fication (with one or more binary labels), then we would use the "sigmoid" (i.e., logistic) activation function in the output layer instead of the "softmax" activation function, and we would use the "binary_crossentropy" loss.

![](images/8b389fb022d521e25ddd81e0ebca7cc6bf223e427c81ad08530d68fb068b03bd.jpg)

If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, you can use the keras.utils.to_categorical() function. To go the other way round, you can just use the np.arg max() function with axis $^ { = 1 }$ .

Secondly, regarding the optimizer, "sgd" simply means that we will train the model using simple Stochastic Gradient Descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff $^ +$ Gradient Descent). We will discuss more efficient optimizers in Chapter 11 (they improve the Gradient Descent part, not the autodiff).

Finally, since this is a classifier, it’s useful to measure its "accuracy" during training and evaluation.

# Training and Evaluating the Model

Now the model is ready to be trained. For this we simply need to call its fit() method. We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a good solution). We also pass a validation set (this is optional): Keras will measure the loss and the extra metrics on this set at the end of each epoch, which is very useful to see how well the model really performs: if the performance on the training set is much better than on the validation set, your

model is probably overfitting the training set (or there is a bug, such as a data mis‐ match between the training set and the validation set):

```txt
>>> history = model.fit(X_train, y_train, epochs=30,
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
.
...
...
.
...
.
...
.
...
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
.
>
.
....
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
...
.
...
 ...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
... 
...
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... 
... ...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
....
...
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
....
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
..
.
..
.
..
.
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
25
1/30
55000/55000 [========] - 3s 55us/sample - loss: 1.4948 - acc: 0.5757
		- val_loss: 1.0042 - val_acc: 0.7166
Epoch 2/30
55000/55000 [========] - 3s 55us/sample - loss: 0.8690 - acc: 0.7318
		- val_loss: 0.7549 - val_acc: 0.7616
[...] 
```

And that’s it! The neural network is trained. At each epoch during training, Keras dis‐ plays the number of instances processed so far (along with a progress bar), the mean training time per sample, the loss and accuracy (or any other extra metrics you asked for), both on the training set and the validation set. You can see that the training loss went down, which is a good sign, and the validation accuracy reached $8 7 . 2 8 \%$ after 50 epochs, not too far from the training accuracy, so there does not seem to be much overfitting going on.

![](images/f01e07a1a4e324f2c5534304eb2030e43ac57e45eece43a1b4dafcd21b3cf4f1.jpg)

Instead of passing a validation set using the validation_data argument, you could instead set validation_split to the ratio of the training set that you want Keras to use for validation (e.g., 0.1).

If the training set was very skewed, with some classes being overrepresented and oth‐ ers underrepresented, it would be useful to set the class_weight argument when calling the fit() method, giving a larger weight to underrepresented classes, and a lower weight to overrepresented classes. These weights would be used by Keras when computing the loss. If you need per-instance weights instead, you can set the sam ple_weight argument (it supersedes class_weight). This could be useful for exam‐ ple if some instances were labeled by experts while others were labeled using a crowdsourcing platform: you might want to give more weight to the former. You can also provide sample weights (but not class weights) for the validation set by adding them as a third item in the validation_data tuple.

The fit() method returns a History object containing the training parameters (his tory.params), the list of epochs it went through (history.epoch), and most impor‐ tantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if

any). If you create a Pandas DataFrame using this dictionary and call its plot() method, you get the learning curves shown in Figure 10-12:

import pandas as pd

```python
pd.DataFrame(history-history).plot(figsize=(8,5))  
plt.grid(True)  
plt.gca().set_ylim(0,1) # set the vertical range to [0-1]  
plt.show() 
```

![](images/23701100d49017980e464b881e8f276b70992c6da76d20084f7ab0184c8f2a8c.jpg)  
Figure 10-12. Learning Curves

You can see that both the training and validation accuracy steadily increase during training, while the training and validation loss decrease. Good! Moreover, the valida‐ tion curves are quite close to the training curves, which means that there is not too much overfitting. In this particular case, the model performed better on the valida‐ tion set than on the training set at the beginning of training: this sometimes happens by chance (especially when the validation set is fairly small). However, the training set performance ends up beating the validation performance, as is generally the case when you train for long enough. You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue train‐ ing. It’s as simple as calling the fit() method again, since Keras just continues train‐ ing where it left off (you should be able to reach close to $8 9 \%$ validation accuracy).

If you are not satisfied with the performance of your model, you should go back and tune the model’s hyperparameters, for example the number of layers, the number of neurons per layer, the types of activation functions we use for each hidden layer, the

number of training epochs, the batch size (it can be set in the fit() method using the batch_size argument, which defaults to 32). We will get back to hyperparameter tuning at the end of this chapter. Once you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this using the evalu ate() method (it also supports several other arguments, such as batch_size or sam ple_weight, please check the documentation for more details):

```txt
>>> model.evaluetest(X_test, y_test)
8832/10000 [===] - ETA: 0s - loss: 0.4074 - acc: 0.8540
[0.40738476498126985, 0.854] 
```

As we saw in Chapter 2, it is common to get slightly lower performance on the test set than on the validation set, because the hyperparameters are tuned on the validation set, not the test set (however, in this example, we did not do any hyperparameter tun‐ ing, so the lower accuracy is just bad luck). Remember to resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the generalization error will be too optimistic.

# Using the Model to Make Predictions

Next, we can use the model’s predict() method to make predictions on new instan‐ ces. Since we don’t have actual new instances, we will just use the first 3 instances of the test set:

```python
>>> X_new = X_test[:3]
>>> y_prob = model.predict(X_new)
>>> y_prob.round(2)
array([[0. , 0. , 0. , 0. , 0. , 0.09, 0. , 0.12, 0. , 0.79],
[0. , 0. , 0.94, 0. , 0.02, 0. , 0.04, 0. , 0. ]',
[0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]',
dtype=float32) 
```

As you can see, for each instance the model estimates one probability per class, from class 0 to class 9. For example, for the first image it estimates that the probability of class 9 (ankle boot) is $7 9 \%$ , the probability of class 7 (sneaker) is $1 2 \%$ , the probability of class 5 (sandal) is $9 \%$ , and the other classes are negligible. In other words, it “believes” it’s footwear, probably ankle boots, but it’s not entirely sure, it might be sneakers or sandals instead. If you only care about the class with the highest estima‐ ted probability (even if that probability is quite low) then you can use the pre dict_classes() method instead:

```python
>>> y_pred = model.predict_classes Classes(X_new)
>>> y_pred
array([9, 2, 1])
>>> np.array(class_names)[y_pred]
array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11') 
```

And the classifier actually classified all three images correctly:

```txt
>> y_new = y_test[:3]  
>> y_new  
array([9, 2, 1]) 
```

Now you know how to build, train, evaluate and use a classification MLP using the Sequential API. But what about regression?

# Building a Regression MLP Using the Sequential API

Let’s switch to the California housing problem and tackle it using a regression neural network. For simplicity, we will use Scikit-Learn’s fetch_california_housing() function to load the data: this dataset is simpler than the one we used in Chapter 2, since it contains only numerical features (there is no ocean_proximity feature), and there is no missing value. After loading the data, we split it into a training set, a vali‐ dation set and a test set, and we scale all the features:

from sklearn.datasets import fetch_california_housing   
from sklearn.model_selection import train_test_split   
from sklearn.preprocessing import StandardScaler   
housing $=$ fetch_california_housing()   
X_train_full,X_test,y_train_full,y_test $=$ train_test_split( housing.data,housing.target)   
X_train,X_valid,y_train,y_valid $=$ train_test_split( X_train_full,y_train_full)   
scalar $=$ StandardScaler()   
X_trainScaled $\equiv$ scorer.fit_transform(X_train)   
X_validScaled $\equiv$ scorer.transform(X_valid)   
X_testScaled $\equiv$ scorer.transform(X_test)

Building, training, evaluating and using a regression MLP using the Sequential API to make predictions is quite similar to what we did for classification. The main differ‐ ences are the fact that the output layer has a single neuron (since we only want to predict a single value) and uses no activation function, and the loss function is the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:

model $=$ keras.models Sequential([ keras.layers.Dense(30,activation $\equiv$ "relu",input_shape $\equiv$ X_train.shape[1:]), keras.layers.Dense(1)   
]）   
model.compile(loss $\equiv$ "mean_squared_error",optimizer $\equiv$ "sgd")   
history $=$ model.fit(X_train,y_train,epochs $= 20$ validation_data $\equiv$ (X_valid,y_valid))   
mse_test $=$ model.evalate(X_test,y_test)   
X_new $=$ X_test[:3] #pretend these are new instances   
y_pred $=$ model.predict(X_new)

As you can see, the Sequential API is quite easy to use. However, although sequential models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, Keras offers the Functional API.

# Building Complex Models Using the Functional API

One example of a non-sequential neural network is a Wide & Deep neural network. This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng et al.14. It connects all or part of the inputs directly to the output layer, as shown in Figure 10-13. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers, thus simple patterns in the data may end up being distorted by this sequence of transfor‐ mations.

![](images/cc46961ead78937a16d1dfb56c9804a051957fb1be4a5b78a510e8377b427fe1.jpg)  
Figure 10-13. Wide and Deep Neural Network

Let’s build such a neural network to tackle the California housing problem:

```python
input = keras.layers.Input(shape=X_train.shape[1:])
hidden1 = keras.layers(Dense(30, activation="relu")(input))
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers CONCATenate()[input, hidden2])
output = keras.layers.Dense(1)(concat)
model = keras.models.Model(input=[input], outputs=[output]) 
```

Let’s go through each line of this code:

• First, we need to create an Input object. This is needed because we may have multiple inputs, as we will see later.   
• Next, we create a Dense layer with 30 neurons and using the ReLU activation function. As soon as it is created, notice that we call it like a function, passing it the input. This is why this is called the Functional API. Note that we are just tell‐

ing Keras how it should connect the layers together, no actual data is being pro‐ cessed yet.

• We then create a second hidden layer, and again we use it as a function. Note however that we pass it the output of the first hidden layer.   
• Next, we create a Concatenate() layer, and once again we immediately use it like a function, to concatenate the input and the output of the second hidden layer (you may prefer the keras.layers.concatenate() function, which creates a Con catenate layer and immediately calls it with the given inputs).   
• Then we create the output layer, with a single neuron and no activation function, and we call it like a function, passing it the result of the concatenation.   
• Lastly, we create a Keras Model, specifying which inputs and outputs to use.

Once you have built the Keras model, everything is exactly like earlier, so no need to repeat it here: you must compile the model, train it, evaluate it and use it to make predictions.

But what if you want to send a subset of the features through the wide path, and a different subset (possibly overlapping) through the deep path (see Figure 10-14)? In this case, one solution is to use multiple inputs. For example, suppose we want to send 5 features through the deep path (features 0 to 4), and 6 features through the wide path (features 2 to 7):

```txt
input_A = keras.layers.Input(shape=[5])
input_B = keras.layers.Input(shape=[6])
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.trainable([input_A, hidden2])
output = keras.layers.trainable([input_A, hidden2])
model = keras.models.Model(input_A, input_B, outputs=[output]) 
```

![](images/52cb44d0ad84e0970cb69fd7b7fbf768d76fe31d86e5753d8cd94a7a528dba39.jpg)  
Figure 10-14. Handling Multiple Inputs

The code is self-explanatory. Note that we specified inputs=[input_A, input_B] when creating the model. Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():

```python
model.compile(loss="mse", optimizer="sgd")  
X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]  
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]  
X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]  
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]  
history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))  
mse_test = model.eval((X_test_A, X_test_B), y_test)  
y_pred = model.predict((X_new_A, X_new_B)) 
```

There are also many use cases in which you may want to have multiple outputs:

• The task may demand it, for example you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object’s center, as well as its width and height) and a classification task.   
• Similarly, you may have multiple independent tasks to perform based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks.   
• Another use case is as a regularization technique (i.e., a training constraint whose objective is to reduce overfitting and thus improve the model’s ability to general‐ ize). For example, you may want to add some auxiliary outputs in a neural net‐ work architecture (see Figure 10-15) to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network.

![](images/7c825e4612fed53a6ecd38ec2324f56ddf11fbb55fb2dc644d1e0e16911e8c6d.jpg)  
Figure 10-15. Handling Multiple Outputs – Auxiliary Output for Regularization

Adding extra outputs is quite easy: just connect them to the appropriate layers and add them to your model’s list of outputs. For example, the following code builds the network represented in Figure 10-15:

```python
[...] # Same as above, up to the main output layer  
output = keras.layers.Dense(1)(concat)  
aux_output = keras.layers.Dense(1)(hidden2)  
model = keras.models.Model(input_A, input_B),  
outputs = [output, aux_output]) 
```

Each output will need its own loss function, so when we compile the model we should pass a list of losses (if we pass a single loss, Keras will assume that the same loss must be used for all outputs). By default, Keras will compute all these losses and simply add them up to get the final loss used for training. However, we care much more about the main output than about the auxiliary output (as it is just used for reg‐ ularization), so we want to give the main output’s loss a much greater weight. Fortu‐ nately, it is possible to set all the loss weights when compiling the model:

```txt
model.compile(loss=['mse","mse"],lossweights=[0.9,0.1],optimizer="sgd") 
```

Now when we train the model, we need to provide some labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing y_train, we just need to pass (y_train, y_train) (and the same goes for y_valid and y_test):

history $=$ model.fit( [X_train_A,X_train_B],[y_train,y_train],epochs=20, validation_data $\equiv$ ([X_valid_A,X_valid_B],[y_valid,y_valid]))

When we evaluate the model, Keras will return the total loss, as well as all the individ‐ ual losses:

```txt
total_loss, main_loss, aux_loss = model.evaluate(X_test_A, X_test_B, [y_test, y_test]) 
```

Similarly, the predict() method will return predictions for each output:

```python
y_pred_main, y_pred(aux = model.predict([X_new_A, X_new_B]) 
```

As you can see, you can build any sort of architecture you want quite easily with the Functional API. Let’s look at one last way you can build Keras models.

# Building Dynamic Models Using the Subclassing API

Both the Sequential API and the Functional API are declarative: you start by declar‐ ing which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages: the model can easily be saved, cloned, shared, its structure can be dis‐ played and analyzed, the framework can infer shapes and check types, so errors can be caught early (i.e., before any data ever goes through the model). It’s also fairly easy to debug, since the whole model is just a static graph of layers. But the flip side is just that: it’s static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more impera‐ tive programming style, the Subclassing API is for you.

Simply subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, cre‐ ating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the Functional API. You can then compile it, eval‐ uate it and use it to make predictions, exactly like we just did.

classWideAndDeepModel(keras.models.Model): def__init__(self，units $= 30$ ，activation $\equiv$ "relu"，\*\*kwarges): super()._init_（\*\*kwarges）#handles standard args(e.g.,name) self-hidden1=keras.layers.Dense(units，activation $\equiv$ activation) self-hidden2=keras.layers.Dense(units，activation $\equiv$ activation) self.main_output $\equiv$ keras.layers.Dense(1) self.aux_output $\equiv$ keras.layers.Dense(1) def call(self，inputs): input_A，input_B $\equiv$ inputs hidden1 $\equiv$ self-hidden1(input_B) hidden2 $\equiv$ self-hidden2(hiden1) concat $\equiv$ keras.layers concatenate([input_A，hidden2]) main_output $\equiv$ self.main_output(concat) aux_output $\equiv$ self.aux_output(hiden2) return main_output，aux_output   
model $\equiv$ WideAndDeepModel()

This example looks very much like the Functional API, except we do not need to cre‐ ate the inputs, we just use the input argument to the call() method, and we separate the creation of the layers15 in the constructor from their usage in the call() method. However, the big difference is that you can do pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations, your imagination is the limit (see Chapter 12)! This makes it a great API for researchers experimenting with new ideas.

However, this extra flexibility comes at a cost: your model’s architecture is hidden within the call() method, so Keras cannot easily inspect it, it cannot save or clone it, and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. So unless you really need that extra flexibility, you should probably stick to the Sequential API or the Functional API.

Now that you know how to build and train neural nets using Keras, you will want to save them!

# Saving and Restoring a Model

Saving a trained Keras model is as simple as it gets:

model.save("my_keras_model.h5")

Keras will save both the model’s architecture (including every layer’s hyperparame‐ ters) and the value of all the model parameters for every layer (e.g., connection weights and biases), using the HDF5 format. It also saves the optimizer (including its hyperparameters and any state it may have).

You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to make predictions. Loading the model is just as easy:

model $=$ keras.models.load_model("my_keras_model.h5")

![](images/2edca46e25caf1972b1fac537e7ffc1194c6d8e9afe72ff1f5b11d3cde043e6c.jpg)

This will work when using the Sequential API or the Functional API, but unfortunately not when using Model subclassing. How‐ ever, you can use save_weights() and load_weights() to at least save and restore the model parameters (but you will need to save and restore everything else yourself).

But what if training lasts several hours? This is quite common, especially when train‐ ing on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training. But how can you tell the fit() method to save checkpoints? The answer is: using callbacks.

# Using Callbacks

The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call during training at the start and end of training, at the start and end of each epoch and even before and after processing each batch. For example, the Mod elCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:

```txt
[...] # build and compile the model  
checkpoint_cb = kerasallelouts.ModelCheckpoint("my_keras_model.h5")  
history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb]) 
```

Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set. This is a simple way to implement early stopping (introduced in Chapter 4):

```python
checkpoint_cb = keras callbacks.ModelCheckpoint("my_keras_model.h5", save_best_only=True)  
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])  
model = keras.models.load_model("my_keras_model.h5") # rollback to best model 
```

Another way to implement early stopping is to simply use the EarlyStopping call‐ back. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. You can combine both callbacks to both save checkpoints of your model (in case your computer crashes), and actually interrupt training early when there is no more progress (to avoid wasting time and resources):

```python
early_stopping_cb = kerasallelbacks.EarlyStopping(patience=10, restore_bestweights=True)  
history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb]) 
```

The number of epochs can be set to a large value since training will stop automati‐ cally when there is no more progress. Moreover, there is no need to restore the best model saved in this case since the EarlyStopping callback will keep track of the best weights and restore them for us at the end of training.

![](images/f9ceab05442148ce6eeae7b2c93b33dc7cff678509dea8bab72ee2a5fb02a622.jpg)

There are many other callbacks available in the keras.callbacks package. See https://keras.io/callbacks/.

If you need extra control, you can easily write your own custom callbacks. For exam‐ ple, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):

```python
classPrintValTrainRatioCallback(keras.trainbackscCallback): defon_epoch_end(self,epoch，logs): print("\nval/train:{.2f}"．format(logs["val_loss"]/ logs["loss"])) 
```

As you might expect, you can implement on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_begin(), on_batch_end() and on_batch_end(). Moreover, callbacks can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). In this case, you should implement on_test_begin(), on_test_end(), on_test_batch_begin(), or on_test_batch_end() (called by evaluate()), or on_predict_begin(), on_pre dict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by predict()).

Now let’s take a look at one more tool you should definitely have in your toolbox when using tf.keras: TensorBoard.

# Visualization Using TensorBoard

TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, vis‐ ualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, and more! This tool is installed automatically when you install TensorFlow, so you already have it!

To use it, you must modify your program so that it outputs the data you want to visu‐ alize to special binary log files called event les. Each binary data record is called a summary. The TensorBoard server will monitor the log directory, and it will automat‐ ically pick up the changes and update the visualizations: this allows you to visualize live data (with a short delay), such as the learning curves during training. In general, you want to point the TensorBoard server to a root log directory, and configure your program so that it writes to a different subdirectory every time it runs. This way, the same TensorBoard server instance will allow you to visualize and compare data from multiple runs of your program, without getting everything mixed up.

So let’s start by defining the root log directory we will use for our TensorBoard logs, plus a small function that will generate a subdirectory path based on the current date and time, so that it is different at every run. You may want to include extra informa‐ tion in the log directory name, such as hyperparameter values that you are testing, to make it easier to know what you are looking at in TensorBoard:

```python
root_logdir = os.path.join(os.curdir, "my_logs")  
def get_run_logdir():
    import time
    run_id = time=strftime("run_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id) 
```

```python
run_logdir = get_run_logdir() # e.g., '.my_logs/run_2019_01_16-11_28_43' 
```

Next, the good news is that Keras provides a nice TensorBoard callback:

```txt
[...] # Build and compile your model  
tensorboard_cb = kerasallelbacks.TensorBoard(run_logdir)  
history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb]) 
```

And that’s all there is to it! It could hardly be easier to use. If you run this code, the TensorBoard callback will take care of creating the log directory for you (along with its parent directories if needed), and during training it will create event files and write summaries to them. After running the program a second time (perhaps changing some hyperparameter value), you will end up with a directory structure similar to this one:

```txt
my_logs run_2019_01_16-16_51_02 events.out.tfevents.1547628669.mycomputer.local.v2 run_2019_01_16-16_56_50 events.out.tfevents.1547629020.mycomputer.local.v2 
```

Next you need to start the TensorBoard server. If you installed TensorFlow within a virtualenv, you should activate it. Next, run the following command at the root of the project (or from anywhere else as long as you point to the appropriate log directory). If your shell cannot find the tensorboard script, then you must update your PATH environment variable so that it contains the directory in which the script was installed (alternatively, you can just replace tensorboard with python3 -m tensor board.main).

```powershell
$ tensorboard --logdir=/my_logs --port=6006
TensorBoard 2.0.0 at http://mycomputer.local:6006 (Press CTRL+C to quit) 
```

Finally, open up a web browser to http://localhost:6006. You should see TensorBoard’s web interface. Click on the SCALARS tab to view the learning curves (see Figure 10-16). Notice that the training loss went down nicely during both runs, but the second run went down much faster. Indeed, we used a larger learning rate by set‐ ting optimizer=keras.optimizers.SGD $\mathsf { l r } = \Theta . \Theta 5 \cdot$ ) instead of optimizer="sgd", which defaults to a learning rate of 0.001.

![](images/0dca1880b5b6174e1851223015a8d35ce5df415205d7ed2d42efc5b563289ca8.jpg)  
Figure 10-16. Visualizing Learning Curves with TensorBoard

Unfortunately, at the time of writing, no other data is exported by the TensorBoard callback, but this issue will probably be fixed by the time you read these lines. In Ten‐ sorFlow 1, this callback exported the computation graph and many useful statistics: type help(keras.callbacks.TensorBoard) to see all the options.

Let’s summarize what you learned so far in this chapter: we saw where neural nets came from, what an MLP is and how you can use it for classification and regression, how to build MLPs using tf.keras’s Sequential API, or more complex architectures using the Functional API or Model Subclassing, you learned how to save and restore a model, use callbacks for checkpointing, early stopping, and more, and finally how to use TensorBoard for visualization. You can already go ahead and use neural networks to tackle many problems! However, you may wonder how to choose the number of hidden layers, the number of neurons in the network, and all the other hyperparame‐ ters. Let’s look at this now.

# Fine-Tuning Neural Network Hyperparameters

The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architec‐ ture, but even in a simple MLP you can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initi‐

alization logic, and much more. How do you know what combination of hyperpara‐ meters is the best for your task?

One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (or using K-fold cross-validation). For this, one approach is simply use GridSearchCV or RandomizedSearchCV to explore the hyper‐ parameter space, as we did in Chapter 2. For this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors. The first step is to create a func‐ tion that will build and compile a Keras model, given a set of hyperparameters:

```python
def build_model(n Hidden=1, n Neurons=30, learning_rate=3e-3, input_shape=[8]):  
    model = keras.models Sequential()  
    options = {"input_shape": input_shape}  
    for layer in range(n Hidden):  
        model.add(keras.layers.Dense(n Neurons, activation="relu", **options))  
        options = {}  
    model.add(keras.layers(Dense(1, **options))  
        optimizer = keras optimizer.SGD(learning_rate)  
        model.compile(loss="mse", optimizer=optimizer)  
    return model 
```

This function creates a simple Sequential model for univariate regression (only one output neuron), with the given input shape and the given number of hidden layers and neurons, and it compiles it using an SGD optimizer configured with the given learning rate. The options dict is used to ensure that the first layer is properly given the input shape (note that if n_hidden $_ { 1 = 0 }$ , the first layer will be the output layer). It is good practice to provide reasonable defaults to as many hyperparameters as you can, as Scikit-Learn does.

Next, let’s create a KerasRegressor based on this build_model() function:

```python
keras_reg = keras.whappers.scikit_learning.KerasRegressor(build_model) 
```

The KerasRegressor object is a thin wrapper around the Keras model built using build_model(). Since we did not specify any hyperparameter when creating it, it will just use the default hyperparameters we defined in build_model(). Now we can use this object like a regular Scikit-Learn regressor: we can train it using its fit() method, then evaluate it using its score() method, and use it to make predictions using its predict() method. Note that any extra parameter you pass to the fit() method will simply get passed to the underlying Keras model. Also note that the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e., higher should be better).

```python
keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[kerasCallbacks.EarlyStopping(patience=10)])  
mse_test = keras_reg.score(X_test, y_test)  
y_pred = keras_reg.predict(X_new) 
```

However, we do not actually want to train and evaluate a single model like this, we want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search (as we discussed in Chapter 2). Let’s try to explore the number of hidden layers, the number of neurons and the learning rate:

from scipy.stats import reciprocal   
from sklearn.model_selection import RandomizedSearchCV   
param_distribis $=$ { "n Hidden":[0,1,2,3], "n_neurons":np.arange(1,100), "learning_rate":reciprocal(3e-4,3e-2),   
}   
rnd_search_cv $=$ RandomizedSearchCV(keras_reg, param_distribis, n_iter=10, cv=3)   
rnd_search_cv.fit(X_train,y_train,epochs=100, validation_data $\equiv$ (X_valid,y_valid), callbacks=[keras callbacks. EarlyStopping(patience=10)]

As you can see, this is identical to what we did in Chapter 2, with the exception that we pass extra parameters to the fit() method: they simply get relayed to the under‐ lying Keras models. Note that RandomizedSearchCV uses K-fold cross-validation, so it does not use X_valid and y_valid. These are just used for early stopping.

The exploration may last many hours depending on the hardware, the size of the dataset, the complexity of the model and the value of n_iter and cv. When it is over, you can access the best parameters found, the best score, and the trained Keras model like this:

```python
>>> rnd_search(cv.best.params_  
{'learning_rate': 0.0033625641252688094, 'n_hiden': 2, 'n_neurons': 42}  
>>> rnd_search(cv.best_score_  
-0.3189529188278931  
>>> model = rnd_search(cv.best_estimator_.model 
```

You can now save this model, evaluate it on the test set, and if you are satisfied with its performance, deploy it to production. Using randomized search is not too hard, and it works well for many fairly simple problems. However, when training is slow (e.g., for more complex problems with larger datasets), this approach will only explore a tiny portion of the hyperparameter space. You can partially alleviate this problem by assisting the search process manually: first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller ranges of values centered on the best ones found during the first run, and so on. This will hopefully zoom in to a good set of hyperparameters. However, this is very time consuming, and probably not the best use of your time.

Fortunately, there are many techniques to explore a search space much more effi‐ ciently than randomly. Their core idea is simple: when a region of the space turns out

to be good, it should be explored more. This takes care of the “zooming” process for you and leads to much better solutions in much less time. Here are a few Python libraries you can use to optimize hyperparameters:

• Hyperopt: a popular Python library for optimizing over all sorts of complex search spaces (including real values such as the learning rate, or discrete values such as the number of layers).   
• Hyperas, kopt or Talos: optimizing hyperparameters for Keras model (the first two are based on Hyperopt).   
• Scikit-Optimize (skopt): a general-purpose optimization library. The Bayes SearchCV class performs Bayesian optimization using an interface similar to Grid SearchCV.   
• Spearmint: a Bayesian optimization library.   
• Sklearn-Deap: a hyperparameter optimization library based on evolutionary algorithms, also with a GridSearchCV-like interface.   
• And many more!

Moreover, many companies offer services for hyperparameter optimization. For example Google Cloud ML Engine has a hyperparameter tuning service. Other com‐ panies provide APIs for hyperparameter optimization, such as Arimo, SigOpt, Oscar and many more.

Hyperparameter tuning is still an active area of research. Evolutionary algorithms are making a comeback lately. For example, check out DeepMind’s excellent 2017 paper16, where they jointly optimize a population of models and their hyperparameters. Goo‐ gle also used an evolutionary approach, not just to search for hyperparameters, but also to look for the best neural network architecture for the problem. They call this AutoML, and it is already available as a cloud service. Perhaps the days of building neural networks manually will soon be over? Check out Google’s post on this topic. In fact, evolutionary algorithms have also been used successfully to train individual neu‐ ral networks, replacing the ubiquitous Gradient Descent! See this 2017 post by Uber where they introduce their Deep Neuroevolution technique.

Despite all this exciting progress, and all these tools and services, it still helps to have an idea of what values are reasonable for each hyperparameter, so you can build a quick prototype, and restrict the search space. Here are a few guidelines for choosing the number of hidden layers and neurons in an MLP, and selecting good values for some of the main hyperparameters.

# Number of Hidden Layers

For many problems, you can just begin with a single hidden layer and you will get reasonable results. It has actually been shown that an MLP with just one hidden layer can model even the most complex functions provided it has enough neurons. For a long time, these facts convinced researchers that there was no need to investigate any deeper neural networks. But they overlooked the fact that deep networks have a much higher parameter eciency than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data.

To understand why, suppose you are asked to draw a forest using some drawing soft‐ ware, but you are forbidden to use copy/paste. You would have to draw each tree individually, branch per branch, leaf per leaf. If you could instead draw one leaf, copy/paste it to draw a branch, then copy/paste that branch to create a tree, and finally copy/paste this tree to make a forest, you would be finished in no time. Realworld data is often structured in such a hierarchical way and Deep Neural Networks automatically take advantage of this fact: lower hidden layers model low-level struc‐ tures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g., faces).

Not only does this hierarchical architecture help DNNs converge faster to a good sol‐ ution, it also improves their ability to generalize to new datasets. For example, if you have already trained a model to recognize faces in pictures, and you now want to train a new neural network to recognize hairstyles, then you can kickstart training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the value of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called transfer learning.

In summary, for many problems you can start with just one or two hidden layers and it will work just fine (e.g., you can easily reach above $9 7 \%$ accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, and above $9 8 \%$ accu‐ racy using two hidden layers with the same total amount of neurons, in roughly the same amount of training time). For more complex problems, you can gradually ramp up the number of hidden layers, until you start overfitting the training set. Very com‐ plex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones, as we will see in Chapter 14), and they need a huge amount of training data. However, you will rarely have to train such networks from scratch: it is much more common to

reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will be a lot faster and require much less data (we will discuss this in Chap‐ ter 11).

# Number of Neurons per Hidden Layer

Obviously the number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 $\mathbf { x } 2 8 = 7 8 4$ input neurons and 10 output neurons.

As for the hidden layers, it used to be a common practice to size them to form a pyra‐ mid, with fewer and fewer neurons at each layer—the rationale being that many lowlevel features can coalesce into far fewer high-level features. For example, a typical neural network for MNIST may have three hidden layers, the first with 300 neurons, the second with 200, and the third with 100. However, this practice has been largely abandoned now, as it seems that simply using the same number of neurons in all hid‐ den layers performs just as well in most cases, or even better, and there is just one hyperparameter to tune instead of one per layer—for example, all hidden layers could simply have 150 neurons. However, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.

Just like for the number of layers, you can try increasing the number of neurons grad‐ ually until the network starts overfitting. In general you will get more bang for the buck by increasing the number of layers than the number of neurons per layer. Unfortunately, as you can see, finding the perfect amount of neurons is still somewhat of a dark art.

A simpler approach is to pick a model with more layers and neurons than you actually need, then use early stopping to prevent it from overfitting (and other regu‐ larization techniques, such as dropout, as we will see in Chapter 11). This has been dubbed the “stretch pants” approach:17 instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size.

# Learning Rate, Batch Size and Other Hyperparameters

The number of hidden layers and neurons are not the only hyperparameters you can tweak in an MLP. Here are some of the most important ones, and some tips on how to set them:

• The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learn‐

ing rate above which the training algorithm diverges, as we saw in Chapter 4). So a simple approach for tuning the learning rate is to start with a large value that makes the training algorithm diverge, then divide this value by 3 and try again, and repeat until the training algorithm stops diverging. At that point, you gener‐ ally won’t be too far from the optimal learning rate. That said, it is sometimes useful to reduce the learning rate during training: we will discuss this in Chap‐ ter 11.

• Choosing a better optimizer than plain old Mini-batch Gradient Descent (and tuning its hyperparameters) is also quite important. We will discuss this in Chap‐ ter 11.   
• The batch size can also have a significant impact on your model’s performance and the training time. In general the optimal batch size will be lower than 32 (in April 2018, Yann Lecun even tweeted "Friends don’t let friends use mini-batches larger than $3 2 ^ { \alpha }$ ). A small batch size ensures that each training iteration is very fast, and although a large batch size will give a more precise estimate of the gradi‐ ents, in practice this does not matter much since the optimization landscape is quite complex and the direction of the true gradients do not point precisely in the direction of the optimum. However, having a batch size greater than 10 helps take advantage of hardware and software optimizations, in particular for matrix multiplications, so it will speed up training. Moreover, if you use Batch Normal‐ ization (see Chapter 11), the batch size should not be too small (in general no less than 20).   
• We discussed the choice of the activation function earlier in this chapter: in gen‐ eral, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on your task.   
• In most cases, the number of training iterations does not actually need to be tweaked: just use early stopping instead.

For more best practices, make sure to read Yoshua Bengio’s great 2012 paper18, which presents many practical recommendations for deep networks.

This concludes this introduction to artificial neural networks and their implementa‐ tion with Keras. In the next few chapters, we will discuss techniques to train very deep nets, we will see how to customize your models using TensorFlow’s lower-level API and how to load and preprocess data efficiently using the Data API, and we will dive into other popular neural network architectures: convolutional neural networks for image processing, recurrent neural networks for sequential data, autoencoders for

representation learning, and generative adversarial networks to model and generate data.19

# Exercises

1. Visit the TensorFlow Playground at https://playground.tensorow.org/

• Layers and patterns: try training the default neural network by clicking the run button (top left). Notice how it quickly finds a good solution for the classifica‐ tion task. Notice that the neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to com‐ bine the simple patterns of the first hidden layer into more complex patterns. In general, the more layers, the more complex the patterns can be.   
• Activation function: try replacing the Tanh activation function with the ReLU activation function, and train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.   
• Local minima: modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, click the reset button next to the play button). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.   
• Too small: now remove one neuron to keep just 2. Notice that the neural net‐ work is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and it systematically underfits the training set.   
• Large enough: next, set the number of neurons to 8 and train the network sev‐ eral times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural net‐ works almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.   
• Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐ tom right dataset under “DATA”). Change the network architecture to have 4 hidden layers with 8 neurons each. Notice that training takes much longer, and often gets stuck on plateaus for long periods of time. Also notice that the neu‐ rons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐ rons in the lowest layers (i.e. on the left). This problem, called the “vanishing gradients” problem, can be alleviated using better weight initialization and

other techniques, better optimizers (such as AdaGrad or Adam), or using Batch Normalization.

• More: go ahead and play with the other parameters to get a feel of what they do. In fact, you should definitely play with this UI for at least one hour, it will grow your intuitions about neural networks significantly.

2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes $A \oplus B$ (where $\oplus$ represents the XOR operation). Hint: $A \oplus B = ( A$ $\neg B ) \lor ( \neg A \land B )$ .

3. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?

4. Why was the logistic activation function a key ingredient in training the first MLPs?

5. Name three popular activation functions. Can you draw them?

6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa‐ tion function.

• What is the shape of the input matrix X?

• What about the shape of the hidden layer’s weight vector $\mathbf { W } _ { h } ,$ and the shape of its bias vector $ { \mathbf { b } } _ { h }$ ?

• What is the shape of the output layer’s weight vector $\mathbf { W } _ { o } ,$ and its bias vector ${ \mathbf b } _ { o }$

• What is the shape of the network’s output matrix Y?

• Write the equation that computes the network’s output matrix Y as a function of X, ${ \mathbf W } _ { h } , { \mathbf b } _ { h } , { \mathbf W } _ { o }$ and ${ \mathbf b } _ { o }$ .

7. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the out‐ put layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2.

8. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?

9. Can you list all the hyperparameters you can tweak in an MLP? If the MLP over‐ fits the training data, how could you tweak these hyperparameters to try to solve the problem?

10. Train a deep MLP on the MNIST dataset and see if you can get over $9 8 \%$ preci‐ sion. Try adding all the bells and whistles (i.e., save checkpoints, use early stop‐ ping, plot learning curves using TensorBoard, and so on).

Solutions to these exercises are available in ???.

# Training Deep Neural Networks

![](images/59721cf2f33858d7ebd971a2d5d0cc8787b15c95dd7ec655a545cf9126ea9787.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 11 in the final release of the book.

In Chapter 10 we introduced artificial neural networks and trained our first deep neural networks. But they were very shallow nets, with just a few hidden layers. What if you need to tackle a very complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN, per‐ haps with 10 layers or much more, each containing hundreds of neurons, connected by hundreds of thousands of connections. This would not be a walk in the park:

• First, you would be faced with the tricky vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train.   
• Second, you might not have enough training data for such a large network, or it might be too costly to label.   
• Third, training may be extremely slow.   
• Fourth, a model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances, or they are too noisy.

In this chapter, we will go through each of these problems in turn and present techni‐ ques to solve them. We will start by explaining the vanishing gradients problem and exploring some of the most popular solutions to this problem. Next, we will look at transfer learning and unsupervised pretraining, which can help you tackle complex

tasks even when you have little labeled data. Then we will discuss various optimizers that can speed up training large models tremendously compared to plain Gradient Descent. Finally, we will go through a few popular regularization techniques for large neural networks.

With these tools, you will be able to train very deep nets: welcome to Deep Learning!

# Vanishing/Exploding Gradients Problems

As we discussed in Chapter 10, the backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. Once the algorithm has computed the gradient of the cost function with regards to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.

Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients prob‐ lem, which is mostly encountered in recurrent neural networks (see ???). More gener‐ ally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.

Although this unfortunate behavior has been empirically observed for quite a while (it was one of the reasons why deep neural networks were mostly abandoned for a long time), it is only around 2010 that significant progress was made in understand‐ ing it. A paper titled “Understanding the Difficulty of Training Deep Feedforward Neural Networks” by Xavier Glorot and Yoshua Bengio1 found a few suspects, includ‐ ing the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time, namely random ini‐ tialization using a normal distribution with a mean of 0 and a standard deviation of 1. In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks).

Looking at the logistic activation function (see Figure 11-1), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually no gradient to propagate back through the network, and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers.

![](images/03101403ef8c56b14e874b6ba9c3d7fdc8111cff2c8988628f75329a1fae302e.jpg)  
Figure 11-1. Logistic activation function saturation

# Glorot and He Initialization

In their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐ lem. We need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradi‐ ents. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs,2 and we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction (please check out the paper if you are interested in the mathematical details). It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but they proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as

described in Equation 11-1, where $f a n _ { \mathrm { a v g } } = \bigl ( f a n _ { \mathrm { i n } } + f a n _ { \mathrm { o u t } } \bigr ) / 2$ . This initialization strategy is called Xavier initialization (after the author’s first name) or Glorot initiali‐ zation (after his last name).

Equation 11-1. Glorot initialization (when using the logistic activation function)

Normal distribution with mean 0 and variance $\begin{array} { r } { \sigma ^ { 2 } = \frac { 1 } { \mathrm { f a n } _ { \mathrm { a v g } } } } \end{array}$

Or a uniform distribution between $- r$ and $+ \ \boldsymbol { r } ,$ with $r = \sqrt { \frac { 3 } { \mathrm { f a n } _ { \mathrm { a v g } } } }$ fanavg

If you just replace $f a n _ { \mathrm { a v g } }$ with $f a n _ { \mathrm { i n } }$ in Equation 11-1, you get an initialization strategy that was actually already proposed by Yann LeCun in the 1990s, called LeCun initiali‐ zation, which was even recommended in the 1998 book Neural Networks: Tricks of the Trade by Genevieve Orr and Klaus-Robert Müller (Springer). It is equivalent to Glorot initialization when $f a n _ { \mathrm { i n } } = f a n _ { \mathrm { o u t } } .$ It took over a decade for researchers to realize just how important this trick really is. Using Glorot initialization can speed up train‐ ing considerably, and it is one of the tricks that led to the current success of Deep Learning.

Some papers3 have provided similar strategies for different activation functions. These strategies differ only by the scale of the variance and whether they use fanavg or $f a n _ { \mathrm { i n } } ,$ as shown in Table 11-1 (for the uniform distribution, just compute $r = \sqrt { 3 \sigma ^ { 2 } } )$ . The initialization strategy for the ReLU activation function (and its variants, includ‐ ing the ELU activation described shortly) is sometimes called He initialization (after the last name of its author). The SELU activation function will be explained later in this chapter. It should be used with LeCun initialization (preferably with a normal distribution, as we will see).

Table 11-1. Initialization parameters for each type of activation function   

<table><tr><td>Initialization</td><td>Activation functions</td><td>σ2 (Normal)</td></tr><tr><td>Glorot</td><td>None, Tanh, Logistic, Softmax</td><td>1 / fanavg</td></tr><tr><td>He</td><td>ReLU &amp; variants</td><td>2 / fanin</td></tr><tr><td>LeCun</td><td>SELU</td><td>1 / fanin</td></tr></table>

By default, Keras uses Glorot initialization with a uniform distribution. You can change this to He initialization by setting kernel_initializer $\mathbf { \tilde { \mathbf { \tilde { \mathbf { \tilde { \tilde { \tilde { \tilde { \mathbf { \tilde { \tilde } } } } } } } } } } }$ "he_uniform" or ker nel_initializer $=$ "he_normal" when creating a layer, like this:

keras.layers.Dense(10, activation="relu", kernel_initializer="he_normal")

If you want He initialization with a uniform distribution, but based on $f a n _ { \mathrm { a v g } }$ rather than fanin, you can use the VarianceScaling initializer like this:

he_avg_init $=$ keras.initializers.VarianceScaling(scale=2., mode $\iota =$ 'fan_avg', distribution $| = "$ uniform') keras.layers.Dense(10, activation="sigmoid", kernel_initializer=he_avg_init)

# Nonsaturating Activation Functions

One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/ exploding gradients problems were in part due to a poor choice of activation func‐ tion. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks, in particular the ReLU activation function, mostly because it does not saturate for positive values (and also because it is quite fast to compute).

Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neu‐ ron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐ ent of the ReLU function is 0 when its input is negative.4

To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. This function is defined as Lea $\mathrm { k y R e L U } _ { \alpha } ( z ) ~ = ~ \operatorname* { m a x } ( \alpha z , ~ z )$ (see Figure 11-2). The hyperparameter $\alpha$ defines how much the function “leaks”: it is the slope of the function for $z < 0$ , and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. A 2015 paper5 compared several variants of the ReLU activation function and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting $\alpha = 0 . 2$ (huge leak) seemed to result in better performance than $\alpha = 0 . 0 1$ (small leak). They also evaluated the randomized leaky ReLU (RReLU), where $\alpha$ is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set).

Finally, they also evaluated the parametric leaky ReLU (PReLU), where $\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). This was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.

![](images/17f95fe06e4c13ca5f6791b1b876928cd4cef97bf47c93c1ea88f07c7a32b0ae.jpg)  
Figure 11-2. Leaky ReLU

Last but not least, a 2015 paper by Djork-Arné Clevert et al.6 proposed a new activa‐ tion function called the exponential linear unit (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced and the neural network per‐ formed better on the test set. It is represented in Figure 11-3, and Equation 11-2 shows its definition.

Equation 11-2. ELU activation function

$$
\operatorname {E L U} _ {\alpha} (z) = \left\{ \begin{array}{l l} \alpha (\exp (z) - 1) & \text {i f} z <   0 \\ z & \text {i f} z \geq 0 \end{array} \right.
$$

![](images/b8eccbd341d8e5faf3940bed77efd1c6f0374e8c37be6216f72c94cd82817d79.jpg)  
Figure 11-3. ELU activation function

It looks a lot like the ReLU function, with a few major differences:

• First it takes on negative values when $z < 0$ , which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem, as discussed earlier. The hyperparameter $\alpha$ defines the value that the ELU func‐ tion approaches when $z$ is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter if you want.   
• Second, it has a nonzero gradient for $z < 0$ , which avoids the dead neurons prob‐ lem.   
• Third, if $\alpha$ is equal to 1 then the function is smooth everywhere, including around $z = 0$ , which helps speed up Gradient Descent, since it does not bounce as much left and right of $z = 0$ .

The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but dur‐ ing training this is compensated by the faster convergence rate. However, at test time an ELU network will be slower than a ReLU network.

Moreover, in a 2017 paper7 by Günter Klambauer et al., called “Self-Normalizing Neural Networks”, the authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function (which is just a scaled version of the ELU activation function, as its name suggests), then the network will self-normalize: the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which solves the vanish‐ ing/exploding gradients problem. As a result, this activation function often outper‐

forms other activation functions very significantly for such neural nets (especially deep ones). However, there are a few conditions for self-normalization to happen:

• The input features must be standardized (mean 0 and standard deviation 1).   
• Every hidden layer’s weights must also be initialized using LeCun normal initiali‐ zation. In Keras, this means setting kernel_initializer $=$ "lecun_normal".   
• The network’s architecture must be sequential. Unfortunately, if you try to use SELU in non-sequential architectures, such as recurrent networks (see ???) or networks with skip connections (i.e., connections that skip layers, such as in wide & deep nets), self-normalization will not be guaranteed, so SELU will not neces‐ sarily outperform other activation functions.   
• The paper only guarantees self-normalization if all layers are dense. However, in practice the SELU activation function seems to work great with convolutional neural nets as well (see Chapter 14).

![](images/4ffabb50cbccf294e004de9ce5a9952a72c400cd0302533367a12047aeeacfea.jpg)

So which activation function should you use for the hidden layers of your deep neural networks? Although your mileage will vary, in general $\mathrm { S E L U } > \mathrm { E L U } >$ leaky ReLU (and its variants) $> \mathrm { R e L U } >$ tanh $>$ logistic. If the network’s architecture prevents it from selfnormalizing, then ELU may perform better than SELU (since SELU is not smooth at $z = 0$ ). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another hyperparameter, you may just use the default $\alpha$ values used by Keras (e.g., 0.3 for the leaky ReLU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is over‐ fitting, or PReLU if you have a huge training set.

To use the leaky ReLU activation function, you must create a LeakyReLU instance like this:

```python
leaky_relu = keras.layers.LeakyReLU(alpha=0.2)  
layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer="he_normal") 
```

For PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no official implementation of RReLU in Keras, but you can fairly easily implement your own (see the exercises at the end of Chapter 12).

For SELU activation, just set activation $=$ "selu" and kernel_initial izer="lecun_normal" when creating a layer:

layer $=$ keras.layers(Dense(10,activation $\equiv$ "relu", kernel_initializer $\equiv$ "lecun_normal")

# Batch Normalization

Although using He initialization along with ELU (or any variant of ReLU) can signifi‐ cantly reduce the vanishing/exploding gradients problems at the beginning of train‐ ing, it doesn’t guarantee that they won’t come back during training.

In a 2015 paper,8 Sergey Ioffe and Christian Szegedy proposed a technique called Batch Normalization (BN) to address the vanishing/exploding gradients problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer, simply zero-centering and normalizing each input, then scaling and shifting the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, this operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler): the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).

In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and stan‐ dard deviation of each input over the current mini-batch (hence the name “Batch Normalization”). The whole operation is summarized in Equation 11-3.

Equation 11-3. Batch Normalization algorithm

1 . $\mu _ { B } = \frac { 1 } { m _ { B } } \sum _ { i = 1 } ^ { m _ { B } } \mathbf { x } ^ { ( i ) }$ x i   
mB 2 . σ x i − μB mB i   
3 . x i = x i − μB σ B 2 + ?   
4 .

• $\mu _ { B }$ is the vector of input means, evaluated over the whole mini-batch $B$ (it con‐ tains one mean per input).

• ${ \pmb { \sigma } } _ { B }$ is the vector of input standard deviations, also evaluated over the whole minibatch (it contains one standard deviation per input).   
• $m _ { B }$ is the number of instances in the mini-batch.   
• $\widehat { \mathbf { X } } ^ { ( i ) }$ is the vector of zero-centered and normalized inputs for instance i.   
• $\gamma$ is the output scale parameter vector for the layer (it contains one scale parame‐ ter per input).   
• $\otimes$ represents element-wise multiplication (each input is multiplied by its corre‐ sponding output scale parameter).   
• $\beta$ is the output shift (offset) parameter vector for the layer (it contains one offset parameter per input). Each input is offset by its corresponding shift parameter.   
• $\epsilon$ is a tiny number to avoid division by zero (typically $1 0 ^ { - 5 }$ ). This is called a smoothing term.   
• $\mathbf { z } ^ { ( i ) }$ is the output of the BN operation: it is a rescaled and shifted version of the inputs.

So during training, BN just standardizes its inputs then rescales and offsets them. Good! What about at test time? Well it is not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instan‐ ces may not be independent and identically distributed (IID), so computing statistics over the batch instances would be unreliable (during training, the batches should not be too small, if possible more than 30 instances, and all instances should be IID, as we saw in Chapter 4). One solution could be to wait until the end of training, then run the whole training set through the neural network, and compute the mean and stan‐ dard deviation of each input of the BN layer. These “final” input means and standard deviations can then be used instead of the batch input means and standard deviations when making predictions. However, it is often preferred to estimate these final statis‐ tics during training using a moving average of the layer’s input means and standard deviations. To sum up, four parameter vectors are learned in each batch-normalized layer: $\gamma$ (the ouput scale vector) and $\beta$ (the output offset vector) are learned through regular backpropagation, and $\mu$ (the final input mean vector), and $\pmb { \sigma }$ (the final input standard deviation vector) are estimated using an exponential moving average. Note that $\mu$ and $\pmb { \sigma }$ are estimated during training, but they are not used at all during train‐ ing, only after training (to replace the batch input means and standard deviations in Equation 11-3).

The authors demonstrated that this technique considerably improved all the deep neural networks they experimented with, leading to a huge improvement in the ImageNet classification task (ImageNet is a large database of images classified into many classes and commonly used to evaluate computer vision systems). The vanish‐

ing gradients problem was strongly reduced, to the point that they could use saturat‐ ing activation functions such as the tanh and even the logistic activation function. The networks were also much less sensitive to the weight initialization. They were able to use much larger learning rates, significantly speeding up the learning process. Specifically, they note that “Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. […] Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching $4 . 9 \%$ top-5 validation error (and $4 . 8 \%$ test error), exceeding the accuracy of human raters.” Finally, like a gift that keeps on giving, Batch Normal‐ ization also acts like a regularizer, reducing the need for other regularization techni‐ ques (such as dropout, described later in this chapter).

Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). More‐ over, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. So if you need predictions to be lightning-fast, you may want to check how well plain ELU + He initialization perform before playing with Batch Normalization.

![](images/d8c12e0588c438f9a62912462349bec70fd35adb24ca9b19935ade19b1e1c5c1.jpg)

You may find that training is rather slow, because each epoch takes much more time when you use batch normalization. However, this is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same per‐ formance. All in all, wall time will usually be smaller (this is the time measured by the clock on your wall).

# Implementing Batch Normalization with Keras

As with most things with Keras, implementing Batch Normalization is quite simple. Just add a BatchNormalization layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model. For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images):

model $=$ keras.models Sequential([ keras.layers.Flatten(input_shape $\equiv$ [28，28]）， keras.layers.batchNormalization()， keras.layers.Dense(300，activation $\equiv$ "elu"，kernel_initializer $\equiv$ "he_normal")， keras.layers.batchNormalization()， keras.layers.Dense(100，activation $\equiv$ "elu"，kernel_initializer $\equiv$ "he_normal")， keras.layers.batchNormalization()， keras.layers.Dense(10，activation $\equiv$ "softmax") ]）

That’s all! In this tiny example with just two hidden layers, it’s unlikely that Batch Normalization will have a very positive impact, but for deeper networks it can make a tremendous difference.

Let’s zoom in a bit. If you display the model summary, you can see that each BN layer adds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136 parameters, which is 4 times 784). The last two parameters, $\mu$ and σ, are the moving averages, they are not affected by backpropagation, so Keras calls them “Nontrainable”9 (if you count the total number of BN parameters, $3 1 3 6 + 1 2 0 0 + 4 0 0$ , and divide by two, you get 2,368, which is the total number of non-trainable params in this model).

```txt
>>> model.summary()
Model: "sequential_3" 
```

```txt
Layer (type) Output Shape Param #  
flatten_3 (Flatten) (None, 784) 0  
batch_normalization_v2 (Batc (None, 784) 3136  
dense_50 (Dense) (None, 300) 235500  
batch_normalization_v2_1 (Ba (None, 300) 1200  
dense_51 (Dense) (None, 100) 30100  
batch_normalization_v2_2 (Ba (None, 100) 400  
dense_52 (Dense) (None, 10) 1010  
Total params: 271,346  
Trainable params: 268,978  
Non-trainable params: 2,368 
```

Let’s look at the parameters of the first BN layer. Two are trainable (by backprop), and two are not:

```txt
>>> [var.name, var.trainable) for var in model.layers[1].variables]  
[(‘batch_normalization_v2/gamma:0', True),  
('batch_normalization_v2/beta:0', True),  
('batch_normalization_v2/moving_mean:0', False),  
('batch_normalization_v2/moving-variance:0', False)] 
```

Now when you create a BN layer in Keras, it also creates two operations that will be called by Keras at each iteration during training. These operations will update the

moving averages. Since we are using the TensorFlow backend, these operations are TensorFlow operations (we will discuss TF operations in Chapter 12).

```txt
>>> model.layers[1].updates
[<tf.Operation 'cond_2/Identity' type=Identity>, <tf.Operation 'cond_3/Identity' type=Identity>] 
```

The authors of the BN paper argued in favor of adding the BN layers before the acti‐ vation functions, rather than after (as we just did). There is some debate about this, as it seems to depend on the task. So that’s one more thing you can experiment with to see which option works best on your dataset. To add the BN layers before the activa‐ tion functions, we must remove the activation function from the hidden layers, and add them as separate layers after the BN layers. Moreover, since a Batch Normaliza‐ tion layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass use_bias $=$ False when creating it):

model $=$ keras.models Sequential([ keras.layers.Flatten(input_shape $\coloneqq$ [28,28]), keras.layers.batchNormalization(), keras.layers.Dense(300,kernel_initializer $\equiv$ "he_normal",use.bias $\equiv$ False), keras.layers.batchNormalization(), keras.layers.Activation("elu"), keras.layers.Dense(100,kernel_initializer $\equiv$ "he_normal",use.bias $\equiv$ False), keras.layers.Activation("elu"), keras.layers BatchNormalization(), keras.layers.Dense(10,activation $\equiv$ "softmax") ])

The BatchNormalization class has quite a few hyperparameters you can tweak. The defaults will usually be fine, but you may occasionally need to tweak the momentum. This hyperparameter is used when updating the exponential moving averages: given a new value v (i.e., a new vector of input means or standard deviations computed over the current batch), the running average $\widehat { \mathbf { v } }$ is updated using the following equation:

$\widehat{\mathbf{v}} \gets \widehat{\mathbf{v}} \times \text{momentum} + \mathbf{v} \times (1 - \text{momentum})$

A good momentum value is typically close to 1—for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and smaller mini-batches).

Another important hyperparameter is axis: it determines which axis should be nor‐ malized. It defaults to $^ { - 1 }$ , meaning that by default it will normalize the last axis (using the means and standard deviations computed across the other axes). For example, when the input batch is 2D (i.e., the batch shape is [batch size, features]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. For example, the first BN layer in the previous code example will independently normalize (and rescale and shift) each of the 784 input features. However, if we move the first BN layer before the Flatten

layer, then the input batches will be 3D, with shape [batch size, height, width], there‐ fore the BN layer will compute 28 means and 28 standard deviations (one per column of pixels, computed across all instances in the batch, and all rows in the column), and it will normalize all pixels in a given column using the same mean and standard devi‐ ation. There will also be just 28 scale parameters and 28 shift parameters. If instead you still want to treat each of the 784 pixels independently, then you should set axis=[1, 2].

Notice that the BN layer does not perform the same computation during training and after training: it uses batch statistics during training, and the “final” statistics after training (i.e., the final value of the moving averages). Let’s take a peek at the source code of this class to see how this is handled:

```python
class BatchNormalization(Layer): [...] def call(self, inputs, training=None): if training is None: training = keras.backend_learning_phase() [...] 
```

The call() method is the one that actually performs the computations, and as you can see it has an extra training argument: if it is None it falls back to keras.back end.learning_phase(), which returns 1 during training (the fit() method ensures that). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to behave differently during training and testing, simply use the same pattern (we will discuss custom layers in Chapter 12).

Batch Normalization has become one of the most used layers in deep neural net‐ works, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. However, a very recent paper10 by Hongyi Zhang et al. may well change this: the authors show that by using a novel fixed-update (fixup) weight initialization technique, they manage to train a very deep neural network (10,000 lay‐ ers!) without BN, achieving state-of-the-art performance on complex image classifi‐ cation tasks.

# Gradient Clipping

Another popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold. This is called Gradient Clipping.11 This technique is most often used in recurrent neu‐

ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???. For other types of networks, BN is usually sufficient.

In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer. For example:

```python
optimizer = keras optimizers.SGD(clipvalue=1.0)  
model.compile(loss="mse", optimizer=optimizer) 
```

This will clip every component of the gradient vector to a value between $- 1 . 0$ and 1.0. This means that all the partial derivatives of the loss (with regards to each and every trainable parameter) will be clipped between $- 1 . 0$ and 1.0. The threshold is a hyper‐ parameter you can tune. Note that it may change the orientation of the gradient vec‐ tor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis, but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice however, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue. This will clip the whole gradient if its $\ell _ { 2 }$ norm is greater than the threshold you picked. For example, if you set clipnor $\boldsymbol { \mathfrak { n } } \boldsymbol { = } 1 . 0$ , then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but almost eliminating the first component. If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different threshold, and see which option performs best on the validation set.

# Reusing Pretrained Layers

It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle (we will discuss how to find them in Chapter 14), then just reuse the lower layers of this network: this is called transfer learning. It will not only speed up training considerably, but will also require much less training data.

For example, suppose that you have access to a DNN that was trained to classify pic‐ tures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network (see Figure 11-4).

![](images/9729500cfd0eb58e0eef354f98d5b9619b16c246e08f22ba9935ce39a6898d76.jpg)  
Figure 11-4. Reusing pretrained layers

![](images/7f00a1810ec2de18ded325f21d52e41ba3ef2e164ab04cc714364392903e4814.jpg)

If the input pictures of your new task don’t have the same size as the ones used in the original task, you will usually have to add a preprocessing step to resize them to the size expected by the origi‐ nal model. More generally, transfer learning will work best when the inputs have similar low-level features.

The output layer of the original model should usually be replaced since it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.

Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse.

![](images/14f827bd2afbe6fa1599fd25bc4a9c7398751e9d78a9978a7b9ade6cf348de4f.jpg)

The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, you can try keeping all the hidden layers and just replace the output layer.

Try freezing all the reused layers first (i.e., make their weights non-trainable, so gradi‐ ent descent won’t modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more

layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.

If you still cannot get good performance, and you have little training data, try drop‐ ping the top hidden layer(s) and freeze all remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of train‐ ing data, you may try replacing the top hidden layers instead of dropping them, and even add more hidden layers.

# Transfer Learning With Keras

Let’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes, for example all classes except for sandals and shirts. Someone built and trained a Keras model on that set and got reasonably good performance $( > 9 0 \%$ accuracy). Let’s call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (positive $: =$ shirts, negative $=$ san‐ dals). However, your dataset is quite small, you only have 200 labeled images. When you train a new model for this task (let’s call it model B), with the same architecture as model A, it performs reasonably well $( 9 7 . 2 \%$ accuracy), but since it’s a much easier task (there are just 2 classes), you were hoping for more. While drinking your morn‐ ing coffee, you realize that your task is quite similar to task A, so perhaps transfer learning can help? Let’s find out!

First, you need to load model A, and create a new model based on the model A’s lay‐ ers. Let’s reuse all layers except for the output layer:

```python
model_A = keras.models.load_model("my_model_A.h5")  
model_B_on_A = keras.modelsSequential(model_A.layers[:, -1])  
model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid")) 
```

Note that model_A and model_B_on_A now share some layers. When you train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone model_A before you reuse its layers. To do this, you must clone model A’s architecture, then copy its weights (since clone_model() does not clone the weights):

```python
model_A_clone = keras.models.clone_model(model_A)  
model_A_clone.set_weights(model_A.getweights()) 
```

Now we could just train model_B_on_A for task B, but since the new output layer was initialized randomly, it will make large errors, at least during the first few epochs, so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, simply set every layer’s train able attribute to False and compile the model:

```python
for layer in model_B_on_A.layers[: -1]: layer.trainable = False 
```

```python
model_B_on_A.compile(loss="binary CROSSentropy", optimizer="sgd", metrics=['accuracy']) 
```

![](images/3c01423ce9e2c3ea9f3b11f6c19d63e1444bb3d785b4e1ae83f3e2fbbebd65ad.jpg)

You must always compile your model after you freeze or unfreeze layers.

Next, we can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:

history $=$ model_B_on_A.fit(X_train_B,y_train_B,epochs $\coloneqq 4$ validation_data $\equiv$ (X_valid_B,y_valid_B))   
for layer in model_B_on_A.layers[-1]: layer.trainable $=$ True   
optimizer $=$ keras optimizer.SGD(lr=1e-4)# the default lr is 1e-3   
model_B_on_A.compile(loss $\equiv$ "binary_crossoverpy",optimizer $\equiv$ optimizer, metrics $\equiv$ ["accuracy"]   
history $=$ model_B_on_A.fit(X_train_B,y_train_B,epochs $\coloneqq 16$ validation_data $\equiv$ (X_valid_B,y_valid_B))

So, what’s the final verdict? Well this model’s test accuracy is $9 9 . 2 5 \%$ , which means that transfer learning reduced the error rate from $2 . 8 \%$ down to almost $0 . 7 \% !$ That’s a factor of 4!

```txt
>>> model_B_on_A.evaluate(X_test_B, y_test_B) [0.06887910133600235, 0.9925] 
```

Are you convinced? Well you shouldn’t be: I cheated! :) I tried many configurations until I found one that demonstrated a strong improvement. If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses. What I did is called “torturing the data until it confesses”. When a paper just looks too positive, you should be suspicious: perhaps the flashy new technique does not help much (in fact, it may even degrade performance), but the authors tried many variants and reported only the best results (which may be due to shear luck), without mentioning how many failures they encountered on the way. Most of the time, this is not malicious at all, but it is part of the reason why so many results in Science can never be reproduced.

So why did I cheat? Well it turns out that transfer learning does not work very well with small dense networks: it works best with deep convolutional neural networks, so we will revisit transfer learning in Chapter 14, using the same techniques (and this time there will be no cheating, I promise!).

# Unsupervised Pretraining

Suppose you want to tackle a complex task for which you don’t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don’t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper‐ vised pretraining (see Figure 11-5). It is often rather cheap to gather unlabeled train‐ ing examples, but quite expensive to label them. If you can gather plenty of unlabeled training data, you can try to train the layers one by one, starting with the lowest layer and then going up, using an unsupervised feature detector algorithm such as Restric‐ ted Boltzmann Machines (RBMs; see ???) or autoencoders (see ???). Each layer is trained on the output of the previously trained layers (all layers except the one being trained are frozen). Once all layers have been trained this way, you can add the output layer for your task, and fine-tune the final network using supervised learning (i.e., with the labeled training examples). At this point, you can unfreeze all the pretrained layers, or just some of the upper ones.

![](images/c92a767aa5727eb89b92f1bbb1a0dcd8588ffe33002fdf4389b4b99534fce87f.jpg)  
Figure 11-5. Unsupervised pretraining

This is a rather long and tedious process, but it often works well; in fact, it is this technique that Geoffrey Hinton and his team used in 2006 and which led to the revival of neural networks and the success of Deep Learning. Until 2010, unsuper‐ vised pretraining (typically using RBMs) was the norm for deep nets, and it was only after the vanishing gradients problem was alleviated that it became much more com‐

mon to train DNNs purely using supervised learning. However, unsupervised pre‐ training (today typically using autoencoders rather than RBMs) is still a good option when you have a complex task to solve, no similar model you can reuse, and little labeled training data but plenty of unlabeled training data.

# Pretraining on an Auxiliary Task

If you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network’s lower layers will learn feature detectors that will likely be reusa‐ ble by the second neural network.

For example, if you want to build a system to recognize faces, you may only have a few pictures of each individual—clearly not enough to train a good classifier. Gather‐ ing hundreds of pictures of each person would not be practical. However, you could gather a lot of pictures of random people on the web and train a first neural network to detect whether or not two different pictures feature the same person. Such a net‐ work would learn good feature detectors for faces, so reusing its lower layers would allow you to train a good face classifier using little training data.

For natural language processing (NLP) applications, you can easily download millions of text documents and automatically generate labeled data from it. For example, you could randomly mask out some words and train a model to predict what the missing words are (e.g., it should predict that the missing word in the sentence “What you saying?” is probably “are” or “were”). If you can train a model to reach good per‐ formance on this task, then it will already know quite a lot about language, and you can certainly reuse it for your actual task, and fine-tune it on your labeled data (we will discuss more pretraining tasks in ???).

![](images/94f1517a9342fa8fd68645d323e6277b73a6c8048e6776d8d6abc74443e96243.jpg)

Self-supervised learning is when you automatically generate the labels from the data itself, then you train a model on the resulting “labeled” dataset using supervised learning techniques. Since this approach requires no human labeling whatsoever, it is best classi‐ fied as a form of unsupervised learning.

# Faster Optimizers

Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): applying a good initiali‐ zation strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning). Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer. In this section

we will present the most popular ones: Momentum optimization, Nesterov Acceler‐ ated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.

# Momentum Optimization

Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.12 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom.

Recall that Gradient Descent simply updates the weights θ by directly subtracting the gradient of the cost function $J ( \pmb \theta )$ with regards to the weights $\left( \nabla _ { \boldsymbol { \theta } } J ( \boldsymbol { \theta } ) \right)$ multiplied by the learning rate $\eta$ . The equation is: $\boldsymbol { \Theta } \gets \boldsymbol { \Theta } - \eta \nabla _ { \boldsymbol { \Theta } } \boldsymbol { J } ( \boldsymbol { \Theta } )$ . It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.

Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multi‐ plied by the learning rate $\eta ,$ ), and it updates the weights by simply adding this momentum vector (see Equation 11-4). In other words, the gradient is used for accel‐ eration, not for speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\beta$ , simply called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.

Equation 11-4. Momentum algorithm

1 .   
2 .

You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate $\eta$ multiplied by $\frac { 1 } { 1 - \beta }$ (ignoring the sign). For example, if $\beta = 0 . 9$ , then the terminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐ tum optimization ends up going 10 times faster than Gradient Descent! This allows Momentum optimization to escape from plateaus much faster than Gradient Descent. In particular, we saw in Chapter 4 that when the inputs have very different scales the cost function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes down the steep slope quite fast, but then it takes a very long time to go down the val‐

ley. In contrast, Momentum optimization will roll down the valley faster and faster until it reaches the bottom (the optimum). In deep neural networks that don’t use Batch Normalization, the upper layers will often end up having inputs with very dif‐ ferent scales, so using Momentum optimization helps a lot. It can also help roll past local optima.

![](images/66a13cec465682b6b92c96c7313bc98a93652e4bd19e5271b8a57f2b5bbe9e00.jpg)

Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons why it is good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence.

Implementing Momentum optimization in Keras is a no-brainer: just use the SGD optimizer and set its momentum hyperparameter, then lie back and profit!

$$
o p t i m i z e r = k e r a s. o p t i m i z e r s. S G D (l r = 0. 0 0 1, \text {m o m e n t u m} = 0. 9)
$$

The one drawback of Momentum optimization is that it adds yet another hyperpara‐ meter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent.

# Nesterov Accelerated Gradient

One small variant to Momentum optimization, proposed by Yurii Nesterov in 1983,13 is almost always faster than vanilla Momentum optimization. The idea of Nesterov Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the gradient of the cost function not at the local position but slightly ahead in the direc‐ tion of the momentum (see Equation 11-5). The only difference from vanilla Momentum optimization is that the gradient is measured at ${ \bf \theta } + \beta { \bf m }$ rather than at θ.

Equation 11-5. Nesterov Accelerated Gradient algorithm

1 .   
2 .

This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradi‐ ent at the original position, as you can see in Figure 11-6 (where $\nabla _ { 1 }$ represents the gradient of the cost function measured at the starting point θ, and $\nabla _ { 2 }$ represents the

gradient at the point located at $\mathbf { \theta } + \beta \mathbf { m } )$ . As you can see, the Nesterov update ends up slightly closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular Momentum optimization. More‐ over, note that when the momentum pushes the weights across a valley, $\nabla _ { 1 }$ continues to push further across the valley, while $\nabla _ { 2 }$ pushes back toward the bottom of the val‐ ley. This helps reduce oscillations and thus converges faster.

NAG will almost always speed up training compared to regular Momentum optimi‐ zation. To use it, simply set nesterov=True when creating the SGD optimizer:

optimizer $=$ keras.optimizers.SGD(lr=0.001, momentum $1 { = } 0 \ . \ 9$ , nesterov=True)

![](images/80195901c2d6d0ec5d0be9a2de40eb9ad8a6391643a98c28def830934b426b41.jpg)  
Figure 11-6. Regular versus Nesterov Momentum optimization

# AdaGrad

Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum.

The AdaGrad algorithm14 achieves this by scaling down the gradient vector along the steepest dimensions (see Equation 11-6):

Equation 11-6. AdaGrad algorithm

1 . s s + ∇θJ θ ⊗ ∇θJ θ   
2 . $\Theta  \Theta - \eta \nabla _ { \Theta } J ( \Theta ) \oslash \sqrt { s + \epsilon }$

The first step accumulates the square of the gradients into the vector s (recall that the $\otimes$ symbol represents the element-wise multiplication). This vectorized form is equiv‐ alent to computing $s _ { i } \gets s _ { i } + ( \partial J ( \pmb { \theta } ) / \partial \theta _ { i } ) ^ { 2 }$ for each element $s _ { i }$ of the vector s; in other words, each $s _ { i }$ accumulates the squares of the partial derivative of the cost function with regards to parameter $\theta _ { i }$ . If the cost function is steep along the ith dimension, then $s _ { i }$ will get larger and larger at each iteration.

The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor of $\sqrt { \mathbf { s } + \epsilon }$ (the $\oslash$ symbol represents the element-wise division, and ϵ is a smoothing term to avoid division by zero, typically set to $1 0 ^ { - 1 0 ^ { \circ } } ,$ ). This vectorized form is equivalent to computing $\theta _ { i }  \theta _ { i } - \eta \partial J \big ( \pmb { \theta } \big ) / \partial \theta _ { i } / \sqrt { s _ { i } + \epsilon }$ for all parameters $\theta _ { i }$ (simultaneously).

In short, this algorithm decays the learning rate, but it does so faster for steep dimen‐ sions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum (see Figure 11-7). One additional benefit is that it requires much less tuning of the learn‐ ing rate hyperparameter $\eta$ .

![](images/2f1d95e451eeb7ad1f912b159559d3066bfbbb7b335412f59f1bac1396d09da6.jpg)  
Figure 11-7. AdaGrad versus Gradient Descent

AdaGrad often performs well for simple quadratic problems, but unfortunately it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an Adagrad optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though). However, understanding Adagrad is helpful to grasp the other adaptive learning rate optimizers.

# RMSProp

Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm15 fixes this by accumulating only the gradi‐ ents from the most recent iterations (as opposed to all the gradients since the begin‐ ning of training). It does so by using exponential decay in the first step (see Equation 11-7).

Equation 11-7. RMSProp algorithm

1 .   
2 .

The decay rate $\beta$ is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all.

As you might expect, Keras has an RMSProp optimizer:

$$
\text {o p t i m i z e r} = \text {k e r a s . o p t i m i z e r s . R M S p r o p} (\mathrm {l r} = 0. 0 0 1, \mathrm {r h o} = 0. 9)
$$

Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research‐ ers until Adam optimization came around.

# Adam and Nadam Optimization

Adam,16 which stands for adaptive moment estimation, combines the ideas of Momen‐ tum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients (see Equation 11-8).17

Equation 11-8. Adam algorithm

1 .   
2 .   
3 .   
4 .   
5 .

• $t$ represents the iteration number (starting at 1).

If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both Momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just $1 - \beta _ { 1 }$ times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since m and s are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost m and s at the beginning of training.

The momentum decay hyperparameter $\beta _ { 1 }$ is typically initialized to 0.9, while the scal‐ ing decay hyperparameter $\beta _ { 2 }$ is often initialized to 0.999. As earlier, the smoothing term $\epsilon$ is usually initialized to a tiny number such as $1 0 ^ { - 7 }$ . These are the default values for the Adam class (to be precise, epsilon defaults to None, which tells Keras to use keras.backend.epsilon(), which defaults to $1 0 ^ { - 7 }$ ; you can change it using keras.backend.set_epsilon()).

optimizer $=$ keras.optimizers.Adam(lr=0.001, beta $\scriptstyle 1 = 0 , 9$ , beta_2=0.999)

Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter $\eta$ . You can often use the default value $\eta = 0 . 0 0 1$ , making Adam even easier to use than Gradient Descent.

![](images/aa973b56f556dbeb9c6f6c552b3e5cba7493f9755eb9533b8f1a0ee4d41778b9.jpg)

If you are starting to feel overwhelmed by all these different techni‐ ques, and wondering how to choose the right ones for your task, don’t worry: some practical guidelines are provided at the end of this chapter.

Finally, two variants of Adam are worth mentioning:

• Adamax, introduced in the same paper as Adam: notice that in step 2 of Equation 11-8, Adam accumulates the squares of the gradients in s (with a greater weight for more recent weights). In step 5, if we ignore $\epsilon$ and steps 3 and 4 (which are technical details anyway), Adam just scales down the parameter updates by the square root of s. In short, Adam scales down the parameter updates by the $\ell _ { 2 }$ norm of the time-decayed gradients (recall that the $\ell _ { 2 }$ norm is the square root of the sum of squares). Adamax just replaces the $\ell _ { 2 }$ norm with the $\ell _ { \infty }$ norm (a fancy way of saying the max). Specifically, it replaces step 2 in Equation 11-8 with $\mathbf { s } \gets \operatorname* { m a x } \big ( \beta _ { 2 } \mathbf { s } , \nabla _ { \boldsymbol { \theta } } J ( \boldsymbol { \theta } ) \big ) .$ , it drops step 4, and in step 5 it scales down the gradient updates by a factor of s, which is just the max of the time-decayed gradients. In practice, this can make Adamax more stable than Adam, but this really depends on the dataset, and in general Adam actually performs better. So it’s just one more optimizer you can try if you experience problems with Adam on some task.   
• Nadam optimization18 is more important: it is simply Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. In his report, Timothy Dozat compares many different optimizers on various tasks, and finds that Nadam generally outperforms Adam, but is sometimes outperformed by RMSProp.

![](images/48f95f5128c88230633db3369c5aef4cd48b85a705c165aed3f7274ff82b84af.jpg)

Adaptive optimization methods (including RMSProp, Adam and Nadam optimization) are often great, converging fast to a good sol‐ ution. However, a 2017 paper19 by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some data‐ sets. So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also check out the latest research, it is moving fast (e.g., AdaBound).

All the optimization techniques discussed so far only rely on the rst-order partial derivatives (Jacobians). The optimization literature contains amazing algorithms based on the second-order partial derivatives (the Hessians, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are $n ^ { 2 }$ Hessians per output (where n is the number of parameters), as opposed to just $n$ Jacobians per output. Since DNNs typi‐ cally have tens of thousands of parameters, the second-order optimization algorithms

often don’t even fit in memory, and even when they do, computing the Hessians is just too slow.

# Training Sparse Models

All the optimization algorithms just presented produce dense models, meaning that most parameters will be nonzero. If you need a blazingly fast model at runtime, or if you need it to take up less memory, you may prefer to end up with a sparse model instead.

One trivial way to achieve this is to train the model as usual, then get rid of the tiny weights (set them to 0). However, this will typically not lead to a very sparse model, and it may degrade the model’s performance.

A better option is to apply strong $\ell _ { 1 }$ regularization during training, as it pushes the optimizer to zero out as many weights as it can (as discussed in Chapter 4 about Lasso Regression).

However, in some cases these techniques may remain insufficient. One last option is to apply Dual Averaging, often called Follow e Regularized Leader (FTRL), a techni‐ que proposed by Yurii Nesterov.20 When used with $\ell _ { 1 }$ regularization, this technique often leads to very sparse models. Keras implements a variant of FTRL called FTRL-Proximal21 in the FTRL optimizer.

# Learning Rate Scheduling

Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge (as we discussed in Chapter 4). If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8).

![](images/6ffd66d6f9413e03f3d8316b7a4f449eb679d10b0359ab793c8a2e5a90bbd1d4.jpg)  
Figure 11-8. Learning curves for various learning rates η

As we discussed in Chapter 10, one approach is to start with a large learning rate, and divide it by 3 until the training algorithm stops diverging. You will not be too far from the optimal learning rate, which will learn quickly and converge to good solu‐ tion.

However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif‐ ferent strategies to reduce the learning rate during training. These strategies are called learning schedules (we briefly introduced this concept in Chapter 4), the most com‐ mon of which are:

# Power scheduling

Set the learning rate to a function of the iteration number t: $\eta ( t ) = \eta _ { 0 } / \left( 1 + t / k \right) ^ { c }$ . The initial learning rate $\eta _ { 0 } ,$ the power c (typically set to 1) and the steps s are hyperparameters. The learning rate drops at each step, and after s steps it is down to $\eta _ { \mathrm { 0 } } / 2$ . After s more steps, it is down to $\eta _ { \mathrm { 0 } } / 3$ . Then down to $\eta _ { 0 } / 4$ , then $\eta _ { 0 } / 5$ and so on. As you can see, this schedule first drops quickly, then more and more slowly. Of course, this requires tuning $\eta _ { 0 } , s$ (and possibly $c$ ).

# Exponential scheduling

Set the learning rate to: $\eta ( t ) = \eta _ { 0 } 0 . 1 ^ { t / s }$ . The learning rate will gradually drop by a factor of 10 every s steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every s steps.

# Piecewise constant scheduling

Use a constant learning rate for a number of epochs (e.g., $\eta _ { 0 } = 0 . 1$ for 5 epochs), then a smaller learning rate for another number of epochs (e.g., $\eta _ { 1 } = 0 . 0 0 1$ for 50 epochs), and so on. Although this solution can work very well, it requires fid‐

dling around to figure out the right sequence of learning rates, and how long to use each of them.

# Performance scheduling

Measure the validation error every $N$ steps (just like for early stopping) and reduce the learning rate by a factor of λ when the error stops dropping.

A 2013 paper22 by Andrew Senior et al. compared the performance of some of the most popular learning schedules when training deep neural networks for speech rec‐ ognition using Momentum optimization. The authors concluded that, in this setting, both performance scheduling and exponential scheduling performed well. They favored exponential scheduling because it was easy to tune and it converged slightly faster to the optimal solution (they also mentioned that it was easier to implement than performance scheduling, but in Keras both options are easy).

Implementing power scheduling in Keras is the easiest option: just set the decay hyperparameter when creating an optimizer. The decay is the inverse of $s$ (the num‐ ber of steps it takes to divide the learning rate by one more unit), and Keras assumes that $c$ is equal to 1. For example:

```python
optimizer = keras.estimators.SGD(lr=0.01, decay=1e-4) 
```

Exponential scheduling and piecewise scheduling are quite simple too. You first need to define a function that takes the current epoch and returns the learning rate. For example, let’s implement exponential scheduling:

```python
def exponential Decay_fn(epoch): return 0.01 * 0.1** (epoch / 20) 
```

If you do not want to hard-code $\eta _ { 0 }$ and s, you can create a function that returns a configured function:

```python
def exponential Decay(lr0, s):
    def exponential Decay_fn(epoch):
        return lr0 * 0.1** (epoch / s)
    return exponential Decay_fn
exponential Decay_fn = exponential Decay(lr0=0.01, s=20) 
```

Next, just create a LearningRateScheduler callback, giving it the schedule function, and pass this callback to the fit() method:

```python
lr_scheduler = kerasallelbacks.LearningRateScheduler(exponential Decay_fn)  
history = model.fit(X_train Sized, y_train, [...], callbacks=[lr_scheduler]) 
```

The LearningRateScheduler will update the optimizer’s learning_rate attribute at the beginning of each epoch. Updating the learning rate just once per epoch is usually enough, but if you want it to be updated more often, for example at every step, you need to write your own callback (see the notebook for an example). This can make sense if there are many steps per epoch.

The schedule function can optionally take the current learning rate as a second argu‐ ment. For example, the following schedule function just multiplies the previous learning rate by $0 . 1 ^ { \& 1 / 2 0 } .$ , which results in the same exponential decay (except the decay now starts at the beginning of epoch 0 instead of 1). This implementation relies on the optimizer’s initial learning rate (contrary to the previous implementation), so make sure to set it appropriately.

```python
def exponential Decay_fn(epoch, lr): return lr * 0.1** (1 / 20) 
```

When you save a model, the optimizer and its learning rate get saved along with it. This means that with this new schedule function, you could just load a trained model and continue training where it left off, no problem. However, things are not so simple if your schedule function uses the epoch argument: indeed, the epoch does not get saved, and it gets reset to 0 every time you call the fit() method. This could lead to a very large learning rate when you continue training a model where it left off, which would likely damage your model’s weights. One solution is to manually set the fit() method’s initial_epoch argument so the epoch starts at the right value.

For piecewise constant scheduling, you can use a schedule function like the following one (as earlier, you can define a more general function if you want, see the notebook for an example), then create a LearningRateScheduler callback with this function and pass it to the fit() method, just like we did for exponential scheduling:

```python
def piecewise_constant_fn(epoch):
    if epoch < 5:
        return 0.01
    elif epoch < 15:
        return 0.005
    else:
        return 0.001 
```

For performance scheduling, simply use the ReduceLROnPlateau callback. For exam‐ ple, if you pass the following callback to the fit() method, it will multiply the learn‐ ing rate by 0.5 whenever the best validation loss does not improve for 5 consecutive epochs (other options are available, please check the documentation for more details):

```javascript
lr_scheduler = keras��backs.reduceLROnPlateau(factor=0.5，patience=5) 
```

Lastly, tf.keras offers an alternative way to implement learning rate scheduling: just define the learning rate using one of the schedules available in keras.optimiz

ers.schedules, then pass this learning rate to any optimizer. This approach updates the learning rate at each step rather than at each epoch. For example, here is how to implement the same exponential schedule as earlier:

```python
s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)  
learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)  
optimizer = keras.optimizers.SGD(learning_rate) 
```

This is nice and simple, plus when you save the model, the learning rate and its schedule (including its state) get saved as well. However, this approach is not part of the Keras API, it is specific to tf.keras.

To sum up, exponential decay or performance scheduling can considerably speed up convergence, so give them a try!

# Avoiding Overtting Through Regularization

With four parameters I can fit an elephant and with five I can make him wiggle his trunk.

—John von Neumann, cited by Enrico Fermi in Nature 427

With thousands of parameters you can fit the whole zoo. Deep neural networks typi‐ cally have tens of thousands of parameters, sometimes even millions. With so many parameters, the network has an incredible amount of freedom and can fit a huge vari‐ ety of complex datasets. But this great flexibility also means that it is prone to overfit‐ ting the training set. We need regularization.

We already implemented one of the best regularization techniques in Chapter 10: early stopping. Moreover, even though Batch Normalization was designed to solve the vanishing/exploding gradients problems, is also acts like a pretty good regularizer. In this section we will present other popular regularization techniques for neural net‐ works: $\ell _ { 1 }$ and $\ell _ { 2 }$ regularization, dropout and max-norm regularization.

# ℓ and ℓ Regularization

Just like you did in Chapter 4 for simple linear models, you can use $\ell _ { 1 }$ and $\ell _ { 2 }$ regulari‐ zation to constrain a neural network’s connection weights (but typically not its bia‐ ses). Here is how to apply $\ell _ { 2 }$ regularization to a Keras layer’s connection weights, using a regularization factor of 0.01:

layer $=$ keras.layers.Dense(100，activation $\equiv$ "elu", kernel_initializer $\equiv$ "he_normal", kernelregularizer $\equiv$ keras.regularizers.l2(0.01))

The l2() function returns a regularizer that will be called to compute the regulariza‐ tion loss, at each step during training. This regularization loss is then added to the final loss. As you might expect, you can just use keras.regularizers.l1() if you

want $\ell _ { 1 }$ regularization, and if you want both $\ell _ { 1 }$ and $\ell _ { 2 }$ regularization, use keras.regu larizers.l1_l2() (specifying both regularization factors).

Since you will typically want to apply the same regularizer to all layers in your net‐ work, as well as the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments over and over. This makes it ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is to use Python’s functools.partial() function: it lets you create a thin wrapper for any callable, with some default argument values. For example:

from functools import partial   
RegularizedDense $=$ partial(keras.layers.Dense, activation $\equiv$ "elu", kernel_initializer $\equiv$ "he_normal", kernelRegularExpressionizer=keras.regularizers.l2(0.01))   
model $=$ keras.models Sequential([ keras.layers.Flatten(input_shape=[28，28])， RegularizedDense(300)， RegularizedDense(100), RegularizedDense(10，activation $\equiv$ "softmax"， kernel_initializer $\equiv$ "glorot.uniform")   
]）

# Dropout

Dropout is one of the most popular regularization techniques for deep neural net‐ works. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24 by Nitish Srivastava et al., and it has proven to be highly successful: even the state-ofthe-art neural networks got a $1 - 2 \%$ accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has $9 5 \%$ accuracy, getting a $2 \%$ accuracy boost means dropping the error rate by almost $4 0 \%$ (going from $5 \%$ error to roughly $3 \%$ ).

It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability $\boldsymbol { p }$ of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-9). The hyperparameter $\boldsymbol { p }$ is called the dropout rate, and it is typically set to $5 0 \%$ . After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).

![](images/3b0d128963870f1787f3dfbb891cf29bdb291d0a64451aa5922f2a40b22454b9.jpg)  
Figure 11-9. Dropout regularization

It is quite surprising at first that this rather brutal technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows; perhaps it would! The com‐ pany would obviously be forced to adapt its organization; it could not rely on any sin‐ gle person to fill in the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them. The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for compa‐ nies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes bet‐ ter.

Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of $2 ^ { N }$ possible networks (where $N$ is the total number of drop‐ pable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run a 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent since they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging ensemble of all these smaller neural networks.

There is one small but important technical detail. Suppose $\mathit { p } = 5 0 \%$ , in which case during testing a neuron will be connected to twice as many input neurons as it was (on average) during training. To compensate for this fact, we need to multiply each

neuron’s input connection weights by 0.5 after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on, and it is unlikely to perform well. More generally, we need to multiply each input connec‐ tion weight by the keep probability $( 1 - p )$ after training. Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well).

To implement dropout using Keras, you can use the keras.layers.Dropout layer. During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability. After training, it does nothing at all, it just passes the inputs to the next layer. For example, the following code applies dropout regularization before every Dense layer, using a dropout rate of 0.2:

model $=$ keras.models Sequential([ keras.layers.Flatten(input_shape $\coloneqq$ [28,28]）, keras.layers.Dropout(rate=0.2), keras.layers.Dense(300，activation $\equiv$ "elu"，kernel_initializer $\equiv$ "he_normal")， keras.layers.Dropout(rate=0.2)， keras.layers.Dense(100，activation $\equiv$ "elu"，kernel_initializer $\equiv$ "he_normal")， keras.layers.Dropout(rate=0.2)， keras.layers.Dense(10，activation $\equiv$ "softmax") ]

![](images/26e60b881ebf7b6781bb02f219ad07f838d611f9c0c9fc7c30c3fd1d012949f3.jpg)

Since dropout is only active during training, the training loss is penalized compared to the validation loss, so comparing the two can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training). Alternatively, you can call the fit() method inside a with keras.backend.learning_phase_scope(1) block: this will force dropout to be active during both training and validation.25

If you observe that the model is overfitting, you can increase the dropout rate. Con‐ versely, you should try decreasing the dropout rate if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong.

Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.

# Monte-Carlo (MC) Dropout

In 2016, a paper26 by Yarin Gal and Zoubin Ghahramani added more good reasons to use dropout:

• First, the paper establishes a profound connection between dropout networks (i.e., neural networks containing a dropout layer before every weight layer) and approximate Bayesian inference27, giving dropout a solid mathematical justifica‐ tion.   
• Second, they introduce a powerful technique called MC Dropout, which can boost the performance of any trained dropout model, without having to retrain it or even modify it at all!   
• Moreover, MC Dropout also provides a much better measure of the model’s uncertainty.   
• Finally, it is also amazingly simple to implement. If this all sounds like a “one weird trick” advertisement, then take a look at the following code. It is the full implementation of MC Dropout, boosting the dropout model we trained earlier, without retraining it:

```python
with keras.backend_learning_phase_scope(1): # force training mode = dropout on y_prob = np.stack([model.predict(X_testScaled) for sample in range(100)])  
y_prob = y_prob.mean(axis=0) 
```

We first force training mode on, using a learning_phase_scope(1) context. This turns dropout on within the with block. Then we make 100 predictions over the test set, and we stack them. Since dropout is on, all predictions will be different. Recall that predict() returns a matrix with one row per instance, and one column per class. Since there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so y_probas is an array of shape [100, 10000, 10]. Once we average over the first dimension $( \mathsf { a x i s } = \mathsf { 0 } )$ ), we get y_proba, an array of shape [10000, 10], like we would get with a single prediction. That’s all! Averaging

over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off. For example, let’s look at the model’s prediction for the first instance in the test set, with dropout off:

```python
>>> np.round(model.predict(X_testScaled[:1]), 2)
array([[0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], dtype=np.float32) 
```

The model seems almost certain that this image belongs to class 9 (ankle boot). Should you trust it? Is there really so little room for doubt? Compare this with the predictions made when dropout is activated:

```txt
>> np.round(y_probas[:, :1], 2)  
array([[0. , 0. , 0. , 0. , 0. , 0.14, 0. , 0.17, 0. , 0.68]], [[0. , 0. , 0. , 0. , 0. , 0.16, 0. , 0.2 , 0. , 0.64]], [[0. , 0. , 0. , 0. , 0. , 0.02, 0. , 0.01, 0. , 0.97]], [...]] 
```

This tells a very different story: apparently, when we activate dropout, the model is not sure anymore. It still seems to prefer class 9, but sometimes it hesitates with classes 5 (sandal) and 7 (sneaker), which makes sense given they’re all footwear. Once we average over the first dimension, we get the following MC dropout predictions:

```txt
>>> np.round(y(probab[:1], 2)  
array([[0. , 0. , 0. , 0. , 0.22, 0. , 0.16, 0. , 0.62]], dtype=float32) 
```

The model still thinks this image belongs to class 9, but only with a $6 2 \%$ confidence, which seems much more reasonable than $9 9 \%$ . Plus it’s useful to know exactly which other classes it thinks are likely. And you can also take a look at the standard devia‐ tion of the probability estimates:

```python
>>> y_std = y_probas.std(axis=0)  
>>> np.round(y_std[:1], 2)  
array([[0. , 0. , 0. , 0. , 0.28, 0. , 0.21, 0.02, 0.32]], dtype=np.float32) 
```

Apparently there’s quite a lot of variance in the probability estimates: if you were building a risk-sensitive system (e.g., a medical or financial system), you should prob‐ ably treat such an uncertain prediction with extreme caution. You definitely would not treat it like a $9 9 \%$ confident prediction. Moreover, the model’s accuracy got a small boost from 86.8 to 86.9:

```txt
>> accuracy = np.sum(y_pred == y_test) / len(y_test)  
>> accuracy  
0.8694 
```

![](images/9a88a028a85e91248d5f7a732ce95240205a71d00d8de9cd1daf7b5cab9078e3.jpg)

The number of Monte Carlo samples you use (100 in this example) is a hyperparameter you can tweak. The higher it is, the more accu‐ rate the predictions and their uncertainty estimates will be. How‐ ever, it you double it, inference time will also be doubled. Moreover, above a certain number of samples, you will notice little improvement. So your job is to find the right tradeoff between latency and accuracy, depending on your application.

If your model contains other layers that behave in a special way during training (such as Batch Normalization layers), then you should not force training mode like we just did. Instead, you should replace the Dropout layers with the following MCDropout class:

```python
class MCDropout(keras.layers.Dropout): def call(self, inputs): return super().call(inputs, training=True) 
```

We just sublass the Dropout layer and override the call() method to force its train ing argument to True (see Chapter 12). Similarly, you could define an MCAlphaDrop out class by subclassing AlphaDropout instead. If you are creating a model from scratch, it’s just a matter of using MCDropout rather than Dropout. But if you have a model that was already trained using Dropout, you need to create a new model, iden‐ tical to the existing model except replacing the Dropout layers with MCDropout, then copy the existing model’s weights to your new model.

In short, MC Dropout is a fantastic technique that boosts dropout models and pro‐ vides better uncertainty estimates. And of course, since it is just regular dropout dur‐ ing training, it also acts like a regularizer.

# Max-Norm Regularization

Another regularization technique that is quite popular for neural networks is called max-norm regularization: for each neuron, it constrains the weights w of the incom‐ ing connections such that $\Vert \mathbf { \Psi } ^ { * } \mathbf { W } ^ { * } \Vert _ { 2 } \leq \underline { { \mathbf { r } } } _ { - } ,$ where $r$ is the max-norm hyperparameter and $\Vert \cdot \Vert _ { 2 }$ is the $\ell _ { 2 }$ norm.

Max-norm regularization does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing ${ \left\| { \bf { w } } \right\| } _ { 2 }$ after each training step and clipping w if needed (w w r∥ w ∥ ). $( \mathbf { w }  \mathbf { w } \frac { r } { \Vert \mathbf { w } \Vert _ { 2 } } )$

Reducing $r$ increases the amount of regularization and helps reduce overfitting. Maxnorm regularization can also help alleviate the vanishing/exploding gradients prob‐ lems (if you are not using Batch Normalization).

To implement max-norm regularization in Keras, just set every hidden layer’s ker nel_constraint argument to a max_norm() constraint, with the appropriate max value, for example:

keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal", kernel_constraint=keras.constraints.max_norm(1.))

After each training iteration, the model’s fit() method will call the object returned by max_norm(), passing it the layer’s weights and getting clipped weights in return, which then replace the layer’s weights. As we will see in Chapter 12, you can define your own custom constraint function if you ever need to, and use it as the ker nel_constraint. You can also constrain the bias terms by setting the bias_con straint argument.

The max_norm() function has an axis argument that defaults to 0. A Dense layer usu‐ ally has weights of shape [number of inputs, number of neurons], so using axis $_ { ; = 0 }$ means that the max norm constraint will apply independently to each neuron’s weight vector. If you want to use max-norm with convolutional layers (see Chapter 14), make sure to set the max_norm() constraint’s axis argument appropriately (usually $\mathsf { a x i s } = [ \odot , \ 1 , \ 2 ] )$ .

# Summary and Practical Guidelines

In this chapter, we have covered a wide range of techniques and you may be wonder‐ ing which ones you should use. The configuration in Table 11-2 will work fine in most cases, without requiring much hyperparameter tuning.

Table 11-2. Default DNN conguration   

<table><tr><td>Hyperparameter</td><td>Default value</td></tr><tr><td>Kernel initializer:</td><td>LeCun initialization</td></tr><tr><td>Activation function:</td><td>SELU</td></tr><tr><td>Normalization:</td><td>None (self-normalization)</td></tr><tr><td>Regularization:</td><td>Early stopping</td></tr><tr><td>Optimizer:</td><td>Nadam</td></tr><tr><td>Learning rate schedule:</td><td>Performance scheduling</td></tr></table>

Don’t forget to standardize the input features! Of course, you should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on an auxiliary task if you have a lot of labeled data for a similar task.

The default configuration in Table 11-2 may need to be tweaked:

• If your model self-normalizes:

— If it overfits the training set, then you should add alpha dropout (and always use early stopping as well). Do not use other regularization methods, or else they would break self-normalization.

• If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip connections):

— You can try using ELU (or another activation function) instead of SELU, it may perform better. Make sure to change the initialization method accord‐ ingly (e.g., He init for ELU or ReLU).   
— If it is a deep network, you should use Batch Normalization after every hidden layer. If it overfits the training set, you can also try using max-norm or $\ell _ { 2 }$ reg‐ ularization.   
• If you need a sparse model, you can use $\ell _ { 1 }$ regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can try using FTRL instead of Nadam optimization, along with $\ell _ { 1 }$ regularization. In any case, this will break self-normalization, so you will need to switch to BN if your model is deep.   
• If you need a low-latency model (one that performs lightning-fast predictions), you may need to use less layers, avoid Batch Normalization, and possibly replace the SELU activation function with the leaky ReLU. Having a sparse model will also help. You may also want to reduce the float precision from 32-bits to 16-bit (or even 8-bits) (see ???).   
• If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates.

With these guidelines, you are now ready to train very deep nets! I hope you are now convinced that you can go a very long way using just Keras. However, there may come a time when you need to have even more control, for example to write a custom loss function or to tweak the training algorithm. For such cases, you will need to use TensorFlow’s lower-level API, as we will see in the next chapter.

# Exercises

1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?   
2. Is it okay to initialize the bias terms to 0?   
3. Name three advantages of the SELU activation function over ReLU.

4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?   
5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?   
6. Name three ways you can produce a sparse model.   
7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What are about MC dropout?   
8. Deep Learning.   
a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.   
b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.   
c. Tune the hyperparameters using cross-validation and see what precision you can achieve.   
d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?   
e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?

9. Transfer learning.

a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.   
b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?   
c. Try caching the frozen layers, and train the model again: how much faster is it now?   
d. Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?   
e. Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?

10. Pretraining on an auxiliary task.

a. In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little

training data. Start by building two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. To do this, you should use a keras.layers.Concatenate layer to con‐ catenate the outputs of both DNNs for each instance, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function.

b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split $\# 2$ should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from dif‐ ferent classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.   
c. Train the DNN on this training set. For each image pair, you can simultane‐ ously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.   
d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.

Solutions to these exercises are available in ???.

# Custom Models and Training with TensorFlow

![](images/0df6d20d04e97f5ae9280dbdc05029f57d6e37da8588500d4e319ad04f7d61f8.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 12 in the final release of the book.

So far we have used only TensorFlow’s high level API, tf.keras, but it already got us pretty far: we built various neural network architectures, including regression and classification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐ niques, such as Batch Normalization, dropout, learning rate schedules, and more. In fact, $9 5 \%$ of the use cases you will encounter will not require anything else than tf.keras (and tf.data, see Chapter 13). But now it’s time to dive deeper into TensorFlow and take a look at its lower-level Python API. This will be useful when you need extra control, to write custom loss functions, custom metrics, layers, models, initializers, regularizers, weight constraints and more. You may even need to fully control the training loop itself, for example to apply special transformations or constraints to the gradients (beyond just clipping them), or to use multiple optimizers for different parts of the network. We will cover all these cases in this chapter, then we will also look at how you can boost your custom models and training algorithms using Ten‐ sorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐ sorFlow.

![](images/12882a2ef7343c7d86582270029f479ab20f99069e4592e6b8a6c849ce3519e8.jpg)

TensorFlow 2.0 was released in March 2019, making TensorFlow much easier to use. The first edition of this book used TF 1, while this edition uses TF 2.

# A Quick Tour of TensorFlow

As you know, TensorFlow is a powerful library for numerical computation, particu‐ larly well suited and fine-tuned for large-scale Machine Learning (but you could use it for anything else that requires heavy computations). It was developed by the Google Brain team and it powers many of Google’s large-scale services, such as Google Cloud Speech, Google Photos, and Google Search. It was open sourced in November 2015, and it is now the most popular deep learning library (in terms of citations in papers, adoption in companies, stars on github, etc.): countless projects use TensorFlow for all sorts of Machine Learning tasks, such as image classification, natural language processing (NLP), recommender systems, time series forecasting, and much more.

So what does TensorFlow actually offer? Here’s a summary:

• Its core is very similar to NumPy, but with GPU support.   
• It also supports distributed computing (across multiple devices and servers).   
• It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐ tations for speed and memory usage: it works by extracting the computation graph from a Python function, then optimizing it (e.g., by pruning unused nodes) and finally running it efficiently (e.g., by automatically running independent operations in parallel).   
• Computation graphs can be exported to a portable format, so you can train a TensorFlow model in one environment (e.g., using Python on Linux), and run it in another (e.g., using Java on an Android device).   
• It implements autodiff (see Chapter 10 and ???), and provides some excellent optimizers, such as RMSProp, Nadam and FTRL (see Chapter 11), so you can easily minimize all sorts of loss functions.   
• TensorFlow offers many more features, built on top of these core features: the most important is of course tf.keras1, but it also has data loading & preprocessing ops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops (tf.signal), and more (see Figure 12-1 for an overview of TensorFlow’s Python API).

![](images/abe4247858de522e0390ef6cf7944aab8ce4c5153a1854728a2e0edbb41876de.jpg)  
Figure 12-1. TensorFlow’s Python API

![](images/2de86392ec50f7d60201f21ed4a7384f52b20f6aa5c13d5215e360dee6bd7841.jpg)

We will cover many of the packages and functions of the Tensor‐ Flow API, but it’s impossible to cover them all so you should really take some time to browse through the API: you will find that it is quite rich and well documented.

At the lowest level, each TensorFlow operation is implemented using highly efficient $\mathrm { C } { + } { + }$ code2. Many operations (or ops for short) have multiple implementations, called kernels: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or even TPUs (Tensor Processing Units). As you may know, GPUs can dramatically speed up computations by splitting computations into many smaller chunks and running them in parallel across many GPU threads. TPUs are even faster. You can purchase your own GPU devices (for now, TensorFlow only supports Nvidia cards with CUDA Compute Capability $3 . 5 + { \mathrm { \AA } }$ ), but TPUs are only available on Google Cloud Machine Learning Engine (see ???).3

TensorFlow’s architecture is shown in Figure 12-2: most of the time your code will use the high level APIs (especially tf.keras and tf.data), but when you need more flexi‐ bility you will use the lower level Python API, handling tensors directly. Note that APIs for other languages are also available. In any case, TensorFlow’s execution

engine will take care of running the operations efficiently, even across multiple devi‐ ces and machines if you tell it to.

![](images/4ed4c69e9eb893d5e966410d53ac766737a22f1ade08712d95327c2cd605f68e.jpg)  
Figure 12-2. TensorFlow’s architecture

TensorFlow runs not only on Windows, Linux, and MacOS, but also on mobile devi‐ ces (using TensorFlow Lite), including both iOS and Android (see ???). If you do not want to use the Python API, there are also $\mathrm { C } { + } { + }$ , Java, Go and Swift APIs. There is even a Javascript implementation called TensorFlow.js that makes it possible to run your models directly in your browser.

There’s more to TensorFlow than just the library. TensorFlow is at the center of an extensive ecosystem of libraries. First, there’s TensorBoard for visualization (see Chapter 10). Next, there’s TensorFlow Extended (TFX), which is a set of libraries built by Google to productionize TensorFlow projects: it includes tools for data validation, preprocessing, model analysis and serving (with TF Serving, see ???). Google also launched TensorFlow Hub, a way to easily download and reuse pretrained neural net‐ works. You can also get many neural network architectures, some of them pretrained, in TensorFlow’s model garden. Check out the TensorFlow Resources, or https:// github.com/jtoy/awesome-tensorow for more TensorFlow-based projects. You will find hundreds of TensorFlow projects on GitHub, so it is often easy to find existing code for whatever you are trying to do.

![](images/00fba789c433e65875be7f760bef215bdc457e18b78f6f67c51121986ccc24e6.jpg)

More and more ML papers are released along with their implemen‐ tation, and sometimes even with pretrained models. Check out https://paperswithcode.com/ to easily find them.

Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐ opers, and a large community contributing to improving it. To ask technical ques‐ tions, you should use http://stackoverow.com/ and tag your question with tensorow and python. You can file bugs and feature requests through GitHub. For general dis‐ cussions, join the Google group.

Okay, it’s time to start coding!

# Using TensorFlow like NumPy

TensorFlow’s API revolves around tensors, hence the name Tensor-Flow. A tensor is usually a multidimensional array (exactly like a NumPy ndarray), but it can also hold a scalar (a simple value, such as 42). These tensors will be important when we create custom cost functions, custom metrics, custom layers and more, so let’s see how to create and manipulate them.

# Tensors and Operations

You can easily create a tensor, using tf.constant(). For example, here is a tensor representing a matrix with two rows and three columns of floats:

```python
>>> tf.constant([[1., 2., 3.], [4., 5., 6.]])
# matrix
<tf.Tensor: id=0, shape=(2, 3), dtype=np32, numpy=
array([[1., 2., 3.],
[4., 5., 6.]], dtype=np32])
>>> tf.constant(42) # scalar
<tfTensor: id=1, shape=(.), dtype=int32, numpy=42> 
```

Just like an ndarray, a tf.Tensor has a shape and a data type (dtype):

```txt
>>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])
>>> t.shape
TensorShape([2, 3])
>>> tdtype
tf.float32 
```

Indexing works much like in NumPy:

```txt
>>> t[:, 1]
<tf.Tensor: id=5, shape=(2, 2), dtype=np32, numpy=
array([[2., 3.],
[5., 6.]], dtype=np32)>
>>> t[,..., 1, tf.newaxis]
<tfTensor: id=15, shape=(2, 1), dtype=np32, numpy=
array([[2.],
[5.]], dtype=np32)> 
```

Most importantly, all sorts of tensor operations are available:

```txt
>>> t + 10  
<tf.Tensor: id=18, shape=(2,3), dtype=float32, numpy= 
```

```python
array([[11., 12., 13.],
[14., 15., 16.]], dtype=float32)>
>>> tf(square(t)
<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=
array([[1., 4., 9.],
[16., 25., 36.]], dtype=float32)>
>>> t @ tf.transpose(t)
<tfTensor: id=24, shape=(2, 2), dtype=float32, numpy=
array([[14., 32.],
[32., 77.]], dtype=float32) 
```

Note that writing $\ t ~ + ~ 1 0$ is equivalent to calling tf.add(t, 10) (indeed, Python calls the magic method t.__add__(10), which just calls tf.add(t, 10)). Other operators (like -, *, etc.) are also supported. The $\emptyset$ operator was added in Python 3.5, for matrix multiplication: it is equivalent to calling the tf.matmul() function.

You will find all the basic math operations you need (e.g., tf.add(), tf.multiply(), tf.square(), tf.exp(), tf.sqrt()…), and more generally most operations that you can find in NumPy (e.g., tf.reshape(), tf.squeeze(), tf.tile()), but sometimes with a different name (e.g., tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(), tf.math.log() are the equivalent of np.mean(), np.sum(), np.max() and np.log()). When the name differs, there is often a good reason for it: for example, in Tensor‐ Flow you must write tf.transpose(t), you cannot just write t.T like in NumPy. The reason is that it does not do exactly the same thing: in TensorFlow, a new tensor is created with its own copy of the transposed data, while in NumPy, t.T is just a trans‐ posed view on the same data. Similarly, the tf.reduce_sum() operation is named this way because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that does not guarantee the order in which the elements are added: because 32-bit floats have limited precision, this means that the result may change ever so slightly every time you call this operation. The same is true of tf.reduce_mean() (but of course tf.reduce_max() is deterministic).

![](images/b51232a0a16bc2b1fc93baf23dafd46e2c7915b7385685519364e586a91eff1e.jpg)

Many functions and classes have aliases. For example, tf.add() and tf.math.add() are the same function. This allows TensorFlow to have concise names for the most common operations4, while preserving well organized packages.

# Keras’ Low-Level API

The Keras API actually has its own low-level API, located in keras.backend. It includes functions like square(), exp(), sqrt() and so on. In tf.keras, these func‐ tions generally just call the corresponding TensorFlow operations. If you want to write code that will be portable to other Keras implementations, you should use these Keras functions. However, they only cover a subset of all functions available in Ten‐ sorFlow, so in this book we will use the TensorFlow operations directly. Here is as simple example using keras.backend, which is commonly named K for short:

```python
>>> from tensorflow import keras
>>> K = keras.backend
>>> K(square(K.transpose(t)) + 10
<tf.Tensor: id=39, shape=(3,2), dtype=np32, numpy=
array([[11., 26.],
[14., 35.],
[19., 46.]], dtype=np32)> 
```

# Tensors and NumPy

Tensors play nice with NumPy: you can create a tensor from a NumPy array, and vice versa, and you can even apply TensorFlow operations to NumPy arrays and NumPy operations to tensors:

```python
>>> a = np.array([2., 4., 5])
>>> tf.constant(a)
<tf.Tensor: id=111, shape=(3), dtype=np.float64, numpy=array([2., 4., 5])
>>> t.numpy() # or np.array(t)
array([[1., 2., 3],
[4., 5., 6]])
>>> tf(square(a)
<tfTensor: id=116, shape=(3), dtype=np.float64, numpy=array([4., 16., 25])
>>> np(square(t))
array([[1., 4., 9],
[16., 25., 36]])
>>> tf.constant(a) 
```

![](images/bfa0c1a14eab2e0e75f52da65e9dfdc86942f430d6b2c222ad379722f189bed0.jpg)

Notice that NumPy uses 64-bit precision by default, while Tensor‐ Flow uses 32-bit. This is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM. So when you create a tensor from a NumPy array, make sure to set dtype=tf.float32.

# Type Conversions

Type conversions can significantly hurt performance, and they can easily go unno‐ ticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversions automatically: it just raises an exception if you try to execute an operation on tensors with incompatible types. For example, you cannot add a float tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:

```typescript
>>> tf.constant(2.) + tf.constant(40)
Traceback[...]InvalidArgumentError[...]expected to be a float[...]  
>>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)
Traceback[...]InvalidArgumentError[...]expected to be a double[...] 
```

This may be a bit annoying at first, but remember that it’s for a good cause! And of course you can use tf.cast() when you really need to convert types:

```python
>>> t2 = tf.constant(40., dtype=tf.float64)
>>> tf.constant(2.0) + tf.cast(t2, tf.float32)
<tf.Tensor: id=136, shape=(), dtype=np.int32, numpy=42.0> 
```

# Variables

So far, we have used constant tensors: as their name suggests, you cannot modify them. However, the weights in a neural network need to be tweaked by backpropaga‐ tion, and other parameters may also need to change over time (e.g., a momentum optimizer keeps track of past gradients). What we need is a tf.Variable:

```txt
>>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.])
>>> v
<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=
array([[1., 2., 3.],
[4., 5., 6.]], dtype=float32)> 
```

A tf.Variable acts much like a constant tensor: you can perform the same opera‐ tions with it, it plays nicely with NumPy as well, and it is just as picky with types. But it can also be modified in place using the assign() method (or assign_add() or assign_sub() which increment or decrement the variable by the given value). You can also modify individual cells (or slices), using the cell’s (or slice’s) assign() method (direct item assignment will not work), or using the scatter_update() or scatter_nd_update() methods:

```javascript
v.assign(2 * v) # => [[2., 4., 6.], [8., 10., 12.]]  
v[θ, 1].assign(42) # => [[2., 42., 6.], [8., 10., 12.]] 
```

v[:，2].assign([0.，1.]) # $\Rightarrow$ [[2.,42.，0.], [8.,10.，1.]] v.scaner_nd_update(indices $= [[\theta ,0]$ ，[1,2]]，updates $=$ [100.，200.]) # $\Rightarrow$ [[100.，42.，0.], [8.,10.，200.]]

![](images/99dddd55dce209486bc6a46a5fc6ba2a3c251b48aea21b3576a5db45fae365b1.jpg)

In practice you will rarely have to create variables manually, since Keras provides an add_weight() method that will take care of it for you, as we will see. Moreover, model parameters will generally be updated directly by the optimizers, so you will rarely need to update variables manually.

# Other Data Structures

TensorFlow supports several other data structures, including the following (please see the notebook or ??? for more details):

• Sparse tensors (tf.SparseTensor) efficiently represent tensors containing mostly 0s. The tf.sparse package contains operations for sparse tensors.   
• Tensor arrays (tf.TensorArray) are lists of tensors. They have a fixed size by default, but can optionally be made dynamic. All tensors they contain must have the same shape and data type.   
• Ragged tensors (tf.RaggedTensor) represent static lists of lists of tensors, where every tensor has the same shape and data type. The tf.ragged package contains operations for ragged tensors.   
• String tensors are regular tensors of type tf.string. These actually represent byte strings, not Unicode strings, so if you create a string tensor using a Unicode string (e.g., a regular Python 3 string like "café"`), then it will get encoded to UTF-8 automatically (e.g., b"caf\xc3\xa9"). Alternatively, you can represent Unicode strings using tensors of type tf.int32, where each item represents a Unicode codepoint (e.g., [99, 97, 102, 233]). The tf.strings package (with an s) contains ops for byte strings and Unicode strings (and to convert one into the other).   
• Sets are just represented as regular tensors (or sparse tensors) containing one or more sets, and you can manipulate them using operations from the tf.sets package.   
• Queues, including First In, First Out (FIFO) queues (FIFOQueue), queues that can prioritize some items (PriorityQueue), queues that shuffle their items (Random ShuffleQueue), and queues that can batch items of different shapes by padding (PaddingFIFOQueue). These classes are all in the tf.queue package.

With tensors, operations, variables and various data structures at your disposal, you are now ready to customize your models and training algorithms!

# Customizing Models and Training Algorithms

Let’s start by creating a custom loss function, which is a simple and common use case.

# Custom Loss Functions

Suppose you want to train a regression model, but your training set is a bit noisy. Of course, you start by trying to clean up your dataset by removing or fixing the outliers, but it turns out to be insufficient, the dataset is still noisy. Which loss function should you use? The mean squared error might penalize large errors too much, so your model will end up being imprecise. The mean absolute error would not penalize out‐ liers as much, but training might take a while to converge and the trained model might not be very precise. This is probably a good time to use the Huber loss (intro‐ duced in Chapter 10) instead of the good old MSE. The Huber loss is not currently part of the official Keras API, but it is available in tf.keras (just use an instance of the keras.losses.Huber class). But let’s pretend it’s not there: implementing it is easy as pie! Just create a function that takes the labels and predictions as arguments, and use TensorFlow operations to compute every instance’s loss:

```python
def huber_fn(y_true, y_pred):
    error = y_true - y_pred
    is_small_error = tf.abs(error) < 1
    squared_loss = tf(square(error) / 2
    linear_loss = tf.abs(error) - 0.5
    return tf.where(is_small_error, squared_loss, linear_loss) 
```

![](images/7d868e2b08510c5d172563287676ef998181ede353df0238b1eb8161f6aa3b2d.jpg)

For better performance, you should use a vectorized implementa‐ tion, as in this example. Moreover, if you want to benefit from Ten‐ sorFlow’s graph features, you should use only TensorFlow operations.

It is also preferable to return a tensor containing one loss per instance, rather than returning the mean loss. This way, Keras can apply class weights or sample weights when requested (see Chapter 10).

Next, you can just use this loss when you compile the Keras model, then train your model:

```julia
model.compile(loss=huber_fn, optimizer="nadam")  
model.fit(X_train, y_train, ...) 
```

And that’s it! For each batch during training, Keras will call the huber_fn() function to compute the loss, and use it to perform a Gradient Descent step. Moreover, it will keep track of the total loss since the beginning of the epoch, and it will display the mean loss.

But what happens to this custom loss when we save the model?

# Saving and Loading Models That Contain Custom Components

Saving a model containing a custom loss function actually works fine, as Keras just saves the name of the function. However, whenever you load it, you need to provide a dictionary that maps the function name to the actual function. More generally, when you load a model containing custom objects, you need to map the names to the objects:

```python
model = keras.models.load_model("my_model_with_a_custom_loss.h5", custom_objects={'huber_fn':huber_fn}) 
```

With the current implementation, any error between -1 and 1 is considered “small”. But what if we want a different threshold? One solution is to create a function that creates a configured loss function:

```python
def create_huber(threshold=1.0): def huber_fn(y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) < threshold squared_loss = tf(square(error) / 2 linear_loss = threshold * tf.abs(error) - threshold**2 / 2 return tf.where(is_small_error, squared_loss, linear_loss) return huber_fn model.compile(loss=create_huber(2.0), optimizer="nadam") 
```

Unfortunately, when you save the model, the threshold will not be saved. This means that you will have to specify the threshold value when loading the model (note that the name to use is "huber_fn", which is the name of the function we gave Keras, not the name of the function that created it):

```txt
model = keras.models.load_model("my_model_with_a_custom_loss_threshold_2.h5", custom_objects={'huber_fn':create_huber(2.0)}) 
```

You can solve this by creating a subclass of the keras.losses.Loss class, and imple‐ ment its get_config() method:

```python
class HuberLoss(keras.losses.Loss): def __init__(self, threshold=1.0, **kwargs): self/threshold = threshold super().__init__(**kwargs) def call(self, y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) < self.threshold squared_loss = tf(square(error) / 2 linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2 return tf.where(is_small_error, squared_loss, linear_loss) def get_config(self): base_config = super().get_config() return {**base_config, "threshold": self/threshold} 
```

![](images/b08fe17123c51a74b8d2fe4abd17fb7e00bd203ca7daf358562fc01cc94d2271.jpg)

The Keras API only specifies how to use subclassing to define lay‐ ers, models, callbacks, and regularizers. If you build other compo‐ nents (such as losses, metrics, initializers or constraints) using subclassing, they may not be portable to other Keras implementa‐ tions.

# Let’s walk through this code:

• The constructor accepts **kwargs and passes them to the parent constructor, which handles standard hyperparameters: the name of the loss and the reduction algorithm to use to aggregate the individual instance losses. By default, it is "sum_over_batch_size", which means that the loss will be the sum of the instance losses, possibly weighted by the sample weights, if any, and then divide the result by the batch size (not by the sum of weights, so this is not the weighted mean).5. Other possible values are "sum" and None.   
• The call() method takes the labels and predictions, computes all the instance losses, and returns them.   
• The get_config() method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class’s get_config() method, then adds the new hyperparameters to this dictionary (note that the convenient $\{ ^ { \star \star } \times \}$ syn‐ tax was added in Python 3.5).

You can then use any instance of this class when you compile the model:

```python
model.compile(loss=HuberLoss(2.), optimizer="nedam") 
```

When you save the model, the threshold will be saved along with it, and when you load the model you just need to map the class name to the class itself:

```python
model = keras.models.load_model("my_model_with_acustom_loss_class.h5", custom_objects={'HuberLoss': HuberLoss}) 
```

When you save a model, Keras calls the loss instance’s get_config() method and saves the config as JSON in the HDF5 file. When you load the model, it calls the from_config() class method on the HuberLoss class: this method is implemented by the base class (Loss) and just creates an instance of the class, passing **config to the constructor.

That’s it for losses! It was not too hard, was it? Well it’s just as simple for custom acti‐ vation functions, initializers, regularizers, and constraints. Let’s look at these now.

# Custom Activation Functions, Initializers, Regularizers, and Constraints

Most Keras functionalities, such as losses, regularizers, constraints, initializers, met‐ rics, activation functions, layers and even full models can be customized in very much the same way. Most of the time, you will just need to write a simple function, with the appropriate inputs and outputs. For example, here are examples of a custom activa‐ tion function (equivalent to keras.activations.softplus or tf.nn.softplus), a custom Glorot initializer (equivalent to keras.initializers.glorot_normal), a cus‐ tom $\ell _ { 1 }$ regularizer (equivalent to keras.regularizers.l1(0.01)) and a custom con‐ straint that ensures weights are all positive (equivalent to keras.constraints.nonneg() or tf.nn.relu):

def mysoftplus(z): # return value is just tf.nnsoftplus(z) return tf.math.log(tf.exp(z) + 1.0)   
def my_glorot_initializer(shape, dtype=tf.float32): stddev $=$ tf.sqrt(2./ (shape[0] $^+$ shape[1])) return tf.random.normal(shape, stddev=stddev, dtype=dtype)   
def my_l1RegularExpression(weights): return tf.reduce_sum(tf.abs(0.01\*weights))   
def my_positiveweights(weights): # return value is just tf.nn.relu(weights) return tf.where(weights $<  0$ .tf.zeros_like(weights),weights)

As you can see, the arguments depend on the type of custom function. These custom functions can then be used normally, for example:

layer $=$ keras.layers.Dense(30,activation $\equiv$ my_softplus, kernel_initializer $\equiv$ my_glorot_initializer, kernelRegularExpression $\equiv$ my_l1RegularExpression, kernel_constraint $\equiv$ my_positiveweights)

The activation function will be applied to the output of this Dense layer, and its result will be passed on to the next layer. The layer’s weights will be initialized using the value returned by the initializer. At each training step the weights will be passed to the regularization function to compute the regularization loss, which will be added to the main loss to get the final loss used for training. Finally, the constraint function will be called after each training step, and the layer’s weights will be replaced by the con‐ strained weights.

If a function has some hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate class, such as keras.regulariz ers.Regularizer, keras.constraints.Constraint, keras.initializers.Initial izer or keras.layers.Layer (for any layer, including activation functions). For example, much like we did for the custom loss, here is a simple class for $\ell _ { 1 }$ regulariza‐

tion, that saves its factor hyperparameter (this time we do not need to call the parent constructor or the get_config() method, as they are not defined by the parent class):

```python
class MyL1Regularizer(keras.regularizers.Regularizer): def __init__(self, factor): self.factor = factor def __call__(self, weights): return tf.reduce_sum(tf.abs(self.factor * weights)) def get_config(self): return {"factor": self.factor} 
```

Note that you must implement the call() method for losses, layers (including activa‐ tion functions) and models, or the __call__() method for regularizers, initializers and constraints. For metrics, things are a bit different, as we will see now.

# Custom Metrics

Losses and metrics are conceptually not the same thing: losses are used by Gradient Descent to train a model, so they must be differentiable (at least where they are evalu‐ ated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not easily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to evaluate a model, they must be more easily interpretable, and they can be nondifferentiable or have 0 gradients everywhere (e.g., accuracy).

That said, in most cases, defining a custom metric function is exactly the same as defining a custom loss function. In fact, we could even use the Huber loss function we created earlier as a metric6, it would work just fine (and persistence would also work the same way, in this case only saving the name of the function, "huber_fn"):

```txt
model.compile(loss="mse", optimizer="nadam", metrics=[create Hubb(2.0)]) 
```

For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch. Most of the time, this is exactly what you want. But not always! Consider a binary classifier’s precision, for example. As we saw in Chapter 3, precision is the number of true positives divided by the number of posi‐ tive predictions (including both true positives and false positives). Suppose the model made 5 positive predictions in the first batch, 4 of which were correct: that’s $8 0 \%$ pre‐ cision. Then suppose the model made 3 positive predictions in the second batch, but they were all incorrect: that’s $0 \%$ precision for the second batch. If you just compute the mean of these two precisions, you get $4 0 \%$ . But wait a second, this is not the mod‐ el’s precision over these two batches! Indeed, there were a total of 4 true positives $( 4 +$ 0) out of 8 positive predictions $( 5 + 3 )$ , so the overall precision is $5 0 \%$ , not $4 0 \%$ . What we need is an object that can keep track of the number of true positives and the num‐

ber of false positives, and compute their ratio when requested. This is precisely what the keras.metrics.Precision class does:

```python
>>> precision = keras.metrics.Precision()
>>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])
<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>
>>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])
<tfTensor: id=581780, shape=(), dtype=float32, numpy=0.5> 
```

In this example, we created a Precision object, then we used it like a function, pass‐ ing it the labels and predictions for the first batch, then for the second batch (note that we could also have passed sample weights). We used the same number of true and false positives as in the example we just discussed. After the first batch, it returns the precision of $8 0 \%$ , then after the second batch it returns $5 0 \%$ (which is the overall precision so far, not the second batch’s precision). This is called a streaming metric (or stateful metric), as it is gradually updated, batch after batch.

At any point, we can call the result() method to get the current value of the metric. We can also look at its variables (tracking the number of true and false positives) using the variables attribute, and reset these variables using the reset_states() method:

```python
>>> p(result())
<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>
>>> p_variables
[<tf.Variable 'true_positive': 0' [...] numpy=array([4.], dtype=float32)], <tf.Variable 'false_positive': 0' [...] numpy=array([4.], dtype=float32)]>
>>> p.reset_states() # both variables get reset to 0.0 
```

If you need to create such a streaming metric, you can just create a subclass of the keras.metrics.Metric class. Here is a simple example that keeps track of the total Huber loss and the number of instances seen so far. When asked for the result, it returns the ratio, which is simply the mean Huber loss:

```python
class HuberMetric(keras.metrics.Metric): def __init__(self, threshold=1.0, **kwargs): super().__init__(**kwargs) # handles base args (e.g., dtype) self/threshold = threshold self.huber_fn = create_huber(threshold) self.total = self.add_weight("total", initializer="zeros") self.count = self.add_weight("count", initializer="zeros") def update_state(self, y_true, y_pred, sample_weight=None): metric = self.huber_fn(y_true, y_pred) self.total assign_add(tf.reduce_sum(metric)) self.count assign_add(tf.cast(tf.size(y_true), tf.float32)) def result(self): return self.total / self.count def get_config(self): base_config = super().get_config() return {*base_config, "threshold": self.threshold} 
```

Let’s walk through this code:7:

• The constructor uses the add_weight() method to create the variables needed to keep track of the metric’s state over multiple batches, in this case the sum of all Huber losses (total) and the number of instances seen so far (count). You could just create variables manually if you preferred. Keras tracks any tf.Variable that is set as an attribute (and more generally, any “trackable” object, such as layers or models).   
• The update_state() method is called when you use an instance of this class as a function (as we did with the Precision object). It updates the variables given the labels and predictions for one batch (and sample weights, but in this case we just ignore them).   
• The result() method computes and returns the final result, in this case just the mean Huber metric over all instances. When you use the metric as a function, the update_state() method gets called first, then the result() method is called, and its output is returned.   
• We also implement the get_config() method to ensure the threshold gets saved along with the model.   
• The default implementation of the reset_states() method just resets all vari‐ ables to 0.0 (but you can override it if needed).

![](images/33538945642fca808349ccd1c9c18d275cab088c9bb2db131277734c3d8f9b94.jpg)

Keras will take care of variable persistence seamlessly, no action is required.

When you define a metric using a simple function, Keras automatically calls it for each batch, and it keeps track of the mean during each epoch, just like we did man‐ ually. So the only benefit of our HuberMetric class is that the threshold will be saved. But of course, some metrics, like precision, cannot simply be averaged over batches: in thoses cases, there’s no other option than to implement a streaming metric.

Now that we have built a streaming metric, building a custom layer will seem like a walk in the park!

# Custom Layers

You may occasionally want to build an architecture that contains an exotic layer for which TensorFlow does not provide a default implementation. In this case, you will need to create a custom layer. Or sometimes you may simply want to build a very repetitive architecture, containing identical blocks of layers repeated many times, and it would be convenient to treat each block of layers as a single layer. For example, if the model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to define a custom layer D containing layers A, B, C, and your model would then simply be D, D, D. Let’s see how to build custom layers.

First, some layers have no weights, such as keras.layers.Flatten or keras.lay ers.ReLU. If you want to create a custom layer without any weights, the simplest option is to write a function and wrap it in a keras.layers.Lambda layer. For exam‐ ple, the following layer will apply the exponential function to its inputs:

exponential_layer $=$ keras.layers.Lambda(lambda x: tf.exp(x))

This custom layer can then be used like any other layer, using the sequential API, the functional API, or the subclassing API. You can also use it as an activation function (or you could just use activation=tf.exp, or activation=keras.activations.expo nential, or simply activation="exponential"). The exponential layer is sometimes used in the output layer of a regression model when the values to predict have very different scales (e.g., 0.001, 10., 1000.).

As you probably guessed by now, to build a custom stateful layer (i.e., a layer with weights), you need to create a subclass of the keras.layers.Layer class. For exam‐ ple, the following class implements a simplified version of the Dense layer:

```python
class MyDense(keras.layers Layer): def __init__(self, units, activation=None, **kwargs): super().__init__(**kwargs) self.units = units selfactivation = keras activations.get(activation) def build(self, batch_input_shape): self_kernel = self.add_weight( name="kernel", shape=[batch_input_shape[-1], self.units], initializer="glorot_normal") self.bias = self.add_weight( name="bias", shape=[self.units], initializer="zeros") super().build(batch_input_shape) # must be at the end def call(self, X): return self.activation(X @ self_kernel + self.bias) def compute_output_shape(self, batch_input_shape): return tf.TensorShape(batch_input_shape.as_list([-1] + [self.units]) 
```

```python
def get_config(self):
    base_config = super().get_config()
    return {**base_config, "units": self.units,
                    "activation": kerasactivations.serialize(self.activation)} 
```

Let’s walk through this code:

• The constructor takes all the hyperparameters as arguments (in this example just units and activation), and importantly it also takes a **kwargs argument. It calls the parent constructor, passing it the kwargs: this takes care of standard arguments such as input_shape, trainable, name, and so on. Then it saves the hyperparameters as attributes, converting the activation argument to the appropriate activation function using the keras.activations.get() function (it accepts functions, standard strings like "relu" or "selu", or simply None)8.   
• The build() method’s role is to create the layer’s variables, by calling the add_weight() method for each weight. The build() method is called the first time the layer is used. At that point, Keras will know the shape of this layer’s inputs, and it will pass it to the build() method9, which is often necessary to cre‐ ate some of the weights. For example, we need to know the number of neurons in the previous layer in order to create the connection weights matrix (i.e., the "ker nel"): this corresponds to the size of the last dimension of the inputs. At the end of the build() method (and only at the end), you must call the parent’s build() method: this tells Keras that the layer is built (it just sets self.built $=$ True).   
• The call() method actually performs the desired operations. In this case, we compute the matrix multiplication of the inputs X and the layer’s kernel, we add the bias vector, we apply the activation function to the result, and this gives us the output of the layer.   
• The compute_output_shape() method simply returns the shape of this layer’s outputs. In this case, it is the same shape as the inputs, except the last dimension is replaced with the number of neurons in the layer. Note that in tf.keras, shapes are instances of the tf.TensorShape class, which you can convert to Python lists using as_list().   
• The get_config() method is just like earlier. Note that we save the activation function’s full configuration by calling keras.activations.serialize().

You can now use a MyDense layer just like any other layer!

![](images/271471bb826685635116f687a96f518dbd1a8dc1f93f74ee665e41643fbaa912.jpg)

You can generally omit the compute_output_shape() method, as tf.keras automatically infers the output shape, except when the layer is dynamic (as we will see shortly). In other Keras implemen‐ tations, this method is either required or by default it assumes the output shape is the same as the input shape.

To create a layer with multiple inputs (e.g., Concatenate), the argument to the call() method should be a tuple containing all the inputs, and similarly the argument to the compute_output_shape() method should be a tuple containing each input’s batch shape. To create a layer with multiple outputs, the call() method should return the list of outputs, and the compute_output_shape() should return the list of batch out‐ put shapes (one per output). For example, the following toy layer takes two inputs and returns three outputs:

class MyMultiLayer(keras.layers Layer): def call(self, X): X1, $X2 = X$ return $[X1 + X2,X1*X2,X1 / X2]$ def compute_output_shape(self, batch_input_shape): b1, b2 = batch_input_shape return [b1, b1, b1] # should probably handle broadcasting rules

This layer may now be used like any other layer, but of course only using the func‐ tional and subclassing APIs, not the sequential API (which only accepts layers with one input and one output).

If your layer needs to have a different behavior during training and during testing (e.g., if it uses Dropout or BatchNormalization layers), then you must add a train ing argument to the call() method and use this argument to decide what to do. For example, let’s create a layer that adds Gaussian noise during training (for regulariza‐ tion), but does nothing during testing (Keras actually has a layer that does the same thing: keras.layers.GaussianNoise):

```python
class MyGaussianNoise(keras.layersayers):
    def __init__(self, stddev, **kwargs):
        super().__init__(**kwargs)
        self.stddev = stddev
    def call(self, X, training=None):
        if training:
            noise = tf.random.normal(tf.shape(X), stddevself.stddev)
            return X + noise
        else:
            return X
    def compute_output_shape(self, batch_input_shape):
        return batch_input_shape 
```

With that, you can now build any custom layer you need! Now let’s create custom models.

# Custom Models

We already looked at custom model classes in Chapter 10 when we discussed the sub‐ classing API.10 It is actually quite straightforward, just subclass the keras.mod els.Model class, create layers and variables in the constructor, and implement the call() method to do whatever you want the model to do. For example, suppose you want to build the model represented in Figure 12-3:

![](images/ac2279ddd95b98206d8c3d3c2cbb802005056cddd5552917e688c024091e6843.jpg)  
Figure 12-3. Custom Model Example

The inputs go through a first dense layer, then through a residual block composed of two dense layers and an addition operation (as we will see in Chapter 14, a residual block adds its inputs to its outputs), then through this same residual block 3 more times, then through a second residual block, and the final result goes through a dense output layer. Note that this model does not make much sense, it’s just an example to illustrate the fact that you can easily build any kind of model you want, even contain‐

ing loops and skip connections. To implement this model, it is best to first create a ResidualBlock layer, since we are going to create a couple identical blocks (and we might want to reuse it in another model):

```python
class ResidualBlock(keras.layers Layer): def __init__(self, n_layers, n_neurons, **kwargs): super().__init__(**kwargs) self-hidden = [keras.layers.Dense(n_neurons, activation="elu", kernel_initializer="he_normal") for _ in range(n_layers)] def call(self, inputs): Z = inputs for layer in self-hidden: Z = layer(Z) return inputs + Z 
```

This layer is a bit special since it contains other layers. This is handled transparently by Keras: it automatically detects that the hidden attribute contains trackable objects (layers in this case), so their variables are automatically added to this layer’s list of variables. The rest of this class is self-explanatory. Next, let’s use the subclassing API to define the model itself:

```python
class ResidualRegressor(keras.models.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self.hidden1 = keras.layers.Dense(30, activation="elu", kernel_initializer="he_normal") self.block1 = ResidualBlock(2, 30) self.block2 = ResidualBlock(2, 30) self.out = keras.layers.Dense(output_dim) def call(self, inputs): Z = self-hidden1(inputs) for in range(1 + 3): Z = self.block1(Z) Z = self.block2(Z) return self.out(Z) 
```

We create the layers in the constructor, and use them in the call() method. This model can then be used like any other model (compile it, fit it, evaluate it and use it to make predictions). If you also want to be able to save the model using the save() method, and load it using the keras.models.load_model() function, you must implement the get_config() method (as we did earlier) in both the ResidualBlock class and the ResidualRegressor class. Alternatively, you can just save and load the weights using the save_weights() and load_weights() methods.

The Model class is actually a subclass of the Layer class, so models can be defined and used exactly like layers. But a model also has some extra functionalities, including of course its compile(), fit(), evaluate() and predict() methods (and a few var‐

iants, such as train_on_batch() or fit_generator()), plus the get_layers() method (which can return any of the model’s layers by name or by index), and the save() method (and support for keras.models.load_model() and keras.mod els.clone_model()). So if models provide more functionalities than layers, why not just define every layer as a model? Well, technically you could, but it is probably cleaner to distinguish the internal components of your model (layers or reusable blocks of layers) from the model itself. The former should subclass the Layer class, while the latter should subclass the Model class.

With that, you can quite naturally and concisely build almost any model that you find in a paper, either using the sequential API, the functional API, the subclassing API, or even a mix of these. “Almost” any model? Yes, there are still a couple things that we need to look at: first, how to define losses or metrics based on model internals, and second how to build a custom training loop.

# Losses and Metrics Based on Model Internals

The custom losses and metrics we defined earlier were all based on the labels and the predictions (and optionally sample weights). However, you will occasionally want to define losses based on other parts of your model, such as the weights or activations of its hidden layers. This may be useful for regularization purposes, or to monitor some internal aspect of your model.

To define a custom loss based on model internals, just compute it based on any part of the model you want, then pass the result to the add_loss() method. For example, the following custom model represents a standard MLP regressor with 5 hidden lay‐ ers, except it also implements a reconstruction loss (see ???): we add an extra Dense layer on top of the last hidden layer, and its role is to try to reconstruct the inputs of the model. Since the reconstruction must have the same shape as the model’s inputs, we need to create this Dense layer in the build() method to have access to the shape of the inputs. In the call() method, we compute both the regular output of the MLP, plus the output of the reconstruction layer. We then compute the mean squared dif‐ ference between the reconstructions and the inputs, and we add this value (times 0.05) to the model’s list of losses by calling add_loss(). During training, Keras will add this loss to the main loss (which is why we scaled down the reconstruction loss, to ensure the main loss dominates). As a result, the model will be forced to preserve as much information as possible through the hidden layers, even information that is not directly useful for the regression task itself. In practice, this loss sometimes improves generalization; it is a regularization loss:

```python
class ReconstructingRegressor(keras.models.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self-hidden = [keras.layers.Dense(30, activation="relu", kernel_initializer="lecun_normal") 
```

for_in range(5)]   
self.out $=$ keras.layers.Dense(output_dim)   
def build(self, batch_input_shape): n Inputs $=$ batch_input_shape[-1] self.reconstruct $=$ keras.layers.Dense(nInputs) super().build(batch_input_shape)   
def call(self, inputs): Z $=$ inputs for layer in self-hidden: Z $=$ layer(Z) reconstruction $=$ self.reconstruct(Z) recon_loss $=$ tf.reduce_mean(tf(square(reconstruction - inputs)) self.add_loss(0.05\* recon_loss) return self.out(Z)

Similarly, you can add a custom metric based on model internals by computing it in any way you want, as long at the result is the output of a metric object. For example, you can create a keras.metrics.Mean() object in the constructor, then call it in the call() method, passing it the recon_loss, and finally add it to the model by calling the model’s add_metric() method. This way, when you train the model, Keras will display both the mean loss over each epoch (the loss is the sum of the main loss plus 0.05 times the reconstruction loss) and the mean reconstruction error over each epoch. Both will go down during training:

```txt
Epoch 1/5  
11610/11610 [===] [...] loss: 4.3092 - reconstruction_error: 1.7360  
Epoch 2/5  
11610/11610 [===] [...] loss: 1.1232 - reconstruction_error: 0.8964  
[...] 
```

In over $9 9 \%$ of the cases, everything we have discussed so far will be sufficient to implement whatever model you want to build, even with complex architectures, los‐ ses, metrics, and so on. However, in some rare cases you may need to customize the training loop itself. However, before we get there, we need to look at how to compute gradients automatically in TensorFlow.

# Computing Gradients Using Autodi

To understand how to use autodiff (see Chapter 10 and ???) to compute gradients automatically, let’s consider a simple toy function:

```python
def f(w1, w2):
    return 3 * w1 ** 2 + 2 * w1 * w2 
```

If you know calculus, you can analytically find that the partial derivative of this func‐ tion with regards to $w 1$ is $6 ~ ^ { \star } ~ { \mathsf { w } } 1 + 2 ~ ^ { \star } ~ { \mathsf { w } } 2 .$ You can also find that its partial derivative with regards to $w 2$ is $2 ~ { } ^ { \star } ~ w 1$ . For example, at the point $( \mathsf { w 1 } , \mathsf { w 2 } ) = ( \mathsf { 5 } , \mathsf { 3 } )$ , these par‐

tial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point is (36, 10). But if this were a neural network, the function would be much more com‐ plex, typically with tens of thousands of parameters, and finding the partial deriva‐ tives analytically by hand would be an almost impossible task. One solution could be to compute an approximation of each partial derivative by measuring how much the function’s output changes when you tweak the corresponding parameter:

>>>w1，w2 $= 5$ ，3   
>>>eps $= 1e - 6$ >>>（f(w1+eps,w2)-f(w1,w2))/eps   
36.000003007075065   
>>>（f(w1，w2+eps）-f(w1，w2)）/eps   
10.000000003174137

Looks about right! This works rather well and it is trivial to implement, but it is just an approximation, and importantly you need to call f() at least once per parameter (not twice, since we could compute $\mathsf { f } ( \mathsf { w } 1 , \mathsf { w } 2 )$ just once). This makes this approach intractable for large neural networks. So instead we should use autodiff (see Chap‐ ter 10 and ???). TensorFlow makes this pretty simple:

```python
w1, w2 = tf.Variable(5.), tf.Variable(3.)  
with tfGradientTape() as tape:  
    z = f(w1, w2) 
```

```python
gradients = tape.gradient(z, [w1, w2]) 
```

We first define two variables w1 and $w 2$ , then we create a tf.GradientTape context that will automatically record every operation that involves a variable, and finally we ask this tape to compute the gradients of the result z with regards to both variables $[ w 1 , \ w 2 ]$ . Let’s take a look at the gradients that TensorFlow computed:

```txt
>>> gradients  
[ <tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>, <tfTensor: id=828229, shape=(), dtype=float32, numpy=10.0> ] 
```

Perfect! Not only is the result accurate (the precision is only limited by the floating point errors), but the gradient() method only goes through the recorded computa‐ tions once (in reverse order), no matter how many variables there are, so it is incredi‐ bly efficient. It’s like magic!

![](images/5ccf418ca81a7481b906e0d3c504afcea6ad880f0b9fd54b729291f071980979.jpg)

Only put the strict minimum inside the tf.GradientTape() block, to save memory. Alternatively, you can pause recording by creating a with tape.stop_recording() block inside the tf.Gradient Tape() block.

The tape is automatically erased immediately after you call its gradient() method, so you will get an exception if you try to call gradient() twice:

```python
with tfGradientTape() as tape:  
    z = f(w1, w2)  
dz_dw1 = tape.gradient(z, w1) # => tensor 36.0  
dz_dw2 = tape.gradient(z, w2) # RuntimeError! 
```

If you need to call gradient() more than once, you must make the tape persistent, and delete it when you are done with it to free resources:

```python
with tfGradientTape(persistent=True) as tape:  
    z = f(w1, w2)  
dz_dw1 = tape.gradient(z, w1) # => tensor 36.0  
dz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now!  
del tape 
```

By default, the tape will only track operations involving variables, so if you try to compute the gradient of z with regards to anything else than a variable, the result will be None:

```python
c1, c2 = tf.constant(5.), tf.constant(3.)  
with tfGradientTape() as tape:  
    z = f(c1, c2) 
```

gradients $=$ tape.gradient(z, [c1, c2]) # returns [None, None]

However, you can force the tape to watch any tensors you like, to record every opera‐ tion that involves them. You can then compute gradients with regards to these ten‐ sors, as if they were variables:

with tfGradientTape() as tape: tape.watch(c1) tape.watch(c2) $z = f(c1,c2)$

gradients $=$ tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]

This can be useful in some cases, for example if you want to implement a regulariza‐ tion loss that penalizes activations that vary a lot when the inputs vary little: the loss will be based on the gradient of the activations with regards to the inputs. Since the inputs are not variables, you would need to tell the tape to watch them.

If you compute the gradient of a list of tensors (e.g., [z1, z2, z3]) with regards to some variables (e.g., $[ w 1 , \ w 2 ] )$ , TensorFlow actually efficiently computes the sum of the gradients of these tensors (i.e., gradient(z1, $[ \mathsf { w } 1 , \mathsf { w } 2 ] ;$ ), plus gradient(z2, $[ \mathsf { w } 1 , \mathsf { w } 2 ] ;$ ), plus gradient(z3, $[ { \ w } 1 , \ w 2 ] )$ ). Due to the way reverse-mode autodiff works, it is not possible to compute the individual gradients (z1, z2 and z3) without actually calling gradient() multiple times (once for z1, once for z2 and once for z3), which requires making the tape persistent (and deleting it afterwards).

Moreover, it is actually possible to compute second order partial derivatives (the Hes‐ sians, i.e., the partial derivatives of the partial derivatives)! To do this, we need to record the operations that are performed when computing the first-order partial derivatives (the Jacobians): this requires a second tape. Here is how it works:

with tfGradientTape(persistent=True) as hessian_tape: with tfGradientTape() as jacobian_tape: $z = f(w1, w2)$ jacobian $\equiv$ jacobian_tapegradient(z, [w1, w2]) hessian $\equiv$ [hessian_tape~- gradient(jacobian, [w1, w2]) for jacobian in jacobians] del hessian_tape

The inner tape is used to compute the Jacobians, as we did earlier. The outer tape is used to compute the partial derivatives of each Jacobian. Since we need to call gradi ent() once for each Jacobian (or else we would get the sum of the partial derivatives over all the Jabobians, as explained earlier), we need the outer tape to be persistent, so we delete it at the end. The Jacobians are obviously the same as earlier (36 and 5), but now we also have the Hessians:

```python
>>>hessians # dz_dw1_dw1,dz_dw1_dw2,dz_dw2_dw1,dz_dw2_dw2  
[<tf.Tensor: id=830578, shape=(), dtype=float32, numpy=6.0>, <tfTensor: id=830595, shape=(), dtype=float32, numpy=2.0>], <tfTensor: id=830600, shape=(), dtype=float32, numpy=2.0>, None]] 
```

Let’s verify these Hessians. The first two are the partial derivatives of $6 ~ ^ { \star } ~ { \mathsf { w } } 1 + 2 ~ ^ { \star } ~ { \mathsf { w } } 2$ (which is, as we saw earlier, the partial derivative of $\mathsf { f }$ with regards to $\boldsymbol { \mathsf { w } } \boldsymbol { 1 }$ ), with regards to w1 and $w 2$ . The result is correct: 6 for w1 and 2 for $w 2$ . The next two are the partial derivatives of $2 \texttt { \star } _ { \mathsf { W } 1 }$ (the partial derivative of $\boldsymbol { \mathsf { f } }$ with regards to $w 2$ ), with regards to $w 1$ and $w 2$ , which are 2 for w1 and 0 for $w 2$ . Note that TensorFlow returns None instead of 0 since $w 2$ does not appear at all in $2 ~ \star _ { \phantom { } \phantom { } \infty 1 }$ . TensorFlow also returns None when you use an operation whose gradients are not defined (e.g., tf.argmax()).

In some rare cases you may want to stop gradients from backpropagating through some part of your neural network. To do this, you must use the tf.stop_gradient() function: it just returns its inputs during the forward pass (like tf.identity()), but it does not let gradients through during backpropagation (it acts like a constant). For example:

```python
def f(w1, w2): return 3 * w1 ** 2 + tf.stop_grad(2 * w1 * w2) with tf.gradTape() as tape: z = f(w1, w2) # same result as without stop_grad() gradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None] 
```

Finally, you may occasionally run into some numerical issues when computing gradi‐ ents. For example, if you compute the gradients of the my_softplus() function for large inputs, the result will be NaN:

```python
>>> x = tf.Variable([100])
>>> with tfGradientTape() as tape:
...
...     z = mysoftplus(x)
...
>>> tape*Tangent(z, [x])
<tf.Tensor: [... ] numpy=array([nan], dtype=float32)> 
```

This is because computing the gradients of this function using autodiff leads to some numerical difficulties: due to floating point precision errors, autodiff ends up com‐ puting infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐ cally find that the derivative of the softplus function is just $1 / \left( 1 + 1 / \exp ( \mathrm { x } ) \right)$ , which is numerically stable. Next, we can tell TensorFlow to use this stable function when computing the gradients of the my_softplus() function, by decorating it with @tf.custom_gradient, and making it return both its normal output and the function that computes the derivatives (note that it will receive as input the gradients that were backpropagated so far, down to the softplus function, and according to the chain rule we should multiply them with this function’s gradients):

@tf(custom_GRadient   
def mybettersoftplus(z): exp $=$ tf.exp(z) defmysoftplus_gradients(grad): returngrad/ $(1 + 1 / \exp)$ returntf.math.log(exp+1),mysoftplus_gradients

Now when we compute the gradients of the my_better_softplus() function, we get the proper result, even for large input values (however, the main output still explodes because of the exponential: one workaround is to use tf.where() to just return the inputs when they are large).

Congratulations! You can now compute the gradients of any function (provided it is differentiable at the point where you compute it), you can even compute Hessians, block backpropagation when needed and even write your own gradient functions! This is probably more flexibility than you will ever need, even if you build your own custom training loops, as we will see now.

# Custom Training Loops

In some rare cases, the fit() method may not be flexible enough for what you need to do. For example, the Wide and Deep paper we discussed in Chapter 10 actually uses two different optimizers: one for the wide path and the other for the deep path. Since the fit() method only uses one optimizer (the one that we specify when

compiling the model), implementing this paper requires writing your own custom loop.

You may also like to write your own custom training loops simply to feel more confi‐ dent that it does precisely what you intent it to do (perhaps you are unsure about some details of the fit() method). It can sometimes feel safer to make everything explicit. However, remember that writing a custom training loop will make your code longer, more error prone and harder to maintain.

![](images/ba747b4922b84750491f277c839ba163a29c46f856afbef0a6e04e2d3e014fd6.jpg)

Unless you really need the extra flexibility, you should prefer using the fit() method rather than implementing your own training loop, especially if you work in a team.

First, let’s build a simple model. No need to compile it, since we will handle the train‐ ing loop manually:

```python
l2_reg = keras.regularizers.l2(0.05)  
model = keras.models Sequential([  
    keras.layers(Dense(30, activation="elu", kernel_initializer="he_normal"),  
        kernel Registrarizer=l2_reg),  
    keras.layers(Dense(1, kernel Registrarizer=l2_reg))]) 
```

Next, let’s create a tiny function that will randomly sample a batch of instances from the training set (in Chapter 13 we will discuss the Data API, which offers a much bet‐ ter alternative):

```python
def random_batch(X, y, batch_size=32):
    idx = np.random.randint(len(X), size=batch_size)
    return X[idx], y[ix] 
```

Let’s also define a function that will display the training status, including the number of steps, the total number of steps, the mean loss since the start of the epoch (i.e., we will use the Mean metric to compute it), and other metrics:

```python
def print_status_bar iteration, total, loss, metrics=None): metrics = " - ".join(['{}: {:.4f}".format(m.name, m(result)) for m in [loss] + (metrics or ()) end = "" if iteration < total else "\n" print("\\r/{}/\{-".format(iteration, total) + metrics, end=end) 
```

This code is self-explanatory, unless you are unfamiliar with Python string format‐ ting: {:.4f} will format a float with 4 digits after the decimal point. Moreover, using \r (carriage return) along with end="" ensures that the status bar always gets printed on the same line. In the notebook, the print_status_bar() function also includes a progress bar, but you could use the handy tqdm library instead.

With that, let’s get down to business! First, we need to define some hyperparameters, choose the optimizer, the loss function and the metrics (just the MAE in this exam‐ ple):

```python
n_epochs = 5  
batch_size = 32  
n_steps = len(X_train) // batch_size  
optimizer = keras.optimizers.Nadam(lr=0.01)  
loss_fn = keras.losses.mean_squared_error  
mean_loss = keras.metrics.mean()  
metrics = [keras.metrics MeanAbsoluteError()] 
```

And now we are ready to build the custom loop!

```python
for epoch in range(1, n_epochs + 1):  
    print("Epoch {}/{}".format epoch, n_epochs))  
for step in range(1, n_steps + 1):  
    X_batch, y_batch = random_batch(X_trainScaled, y_train)  
    with tfGradientTape() as tape:  
        y_pred = model(X_batch, training=True)  
        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))  
        loss = tf.add_n([main_loss] + model.losses)  
        gradients = tape.gradient(loss, model.trainable_variables)  
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  
        mean_loss(loss)  
    for metric in metrics:  
        metric(y_batch, y_pred)  
        print.status_bar(step * batch_size, len(y_train), mean_loss, metrics)  
print.status_bar(len(y_train), len(y_train), mean_loss, metrics)  
for metric in [mean_loss] + metrics:  
    metric.reset_states() 
```

There’s a lot going on in this code, so let’s walk through it:

• We create two nested loops: one for the epochs, the other for the batches within an epoch.   
• Then we sample a random batch from the training set.   
• Inside the tf.GradientTape() block, we make a prediction for one batch (using the model as a function), and we compute the loss: it is equal to the main loss plus the other losses (in this model, there is one regularization loss per layer). Since the mean_squared_error() function returns one loss per instance, we compute the mean over the batch using tf.reduce_mean() (if you wanted to apply different weights to each instance, this is where you would do it). The regu‐ larization losses are already reduced to a single scalar each, so we just need to sum them (using tf.add_n(), which sums multiple tensors of the same shape and data type).

• Next, we ask the tape to compute the gradient of the loss with regards to each trainable variable (not all variables!), and we apply them to the optimizer to per‐ form a Gradient Descent step.   
• Next we update the mean loss and the metrics (over the current epoch), and we display the status bar.   
• At the end of each epoch, we display the status bar again to make it look com‐ plete11 and to print a line feed, and we reset the states of the mean loss and the metrics.

If you set the optimizer’s clipnorm or clipvalue hyperparameters, it will take care of this for you. If you want to apply any other transformation to the gradients, simply do so before calling the apply_gradients() method.

If you add weight constraints to your model (e.g., by setting kernel_constraint or bias_constraint when creating a layer), you should update the training loop to apply these constraints just after apply_gradients():

```txt
for variable in model.variables: if variableconstraint is not None: variable.assign(variableconstraint(variable)) 
```

Most importantly, this training loop does not handle layers that behave differently during training and testing (e.g., BatchNormalization or Dropout). To handle these, you need to call the model with training=True and make sure it propagates this to every layer that needs it.12

As you can see, there are quite a lot of things you need to get right, it is easy to make a mistake. But on the bright side, you get full control, so it’s your call.

Now that you know how to customize any part of your models13 and training algo‐ rithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it can speed up your custom code considerably, and it will also make it portable to any platform supported by TensorFlow (see ???).

# TensorFlow Functions and Graphs

In TensorFlow 1, graphs were unavoidable (as were the complexities that came with them): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still

there, but not as central, and much (much!) simpler to use. To demonstrate this, let’s start with a trivial function that just computes the cube of its input:

```python
def cube(x):
    return x ** 3 
```

We can obviously call this function with a Python value, such as an int or a float, or we can call it with a tensor:

>>> cube(2)   
8   
>>> cube(tf.constant(2.0))   
<tf.Tensor: id=18634148, shape $\equiv$ ()，dtype $\equiv$ float32，numpy $= 8.0>$

Now, let’s use tf.function() to convert this Python function to a TensorFlow Func‐ tion:

```txt
>>> tf_cube = tf.function(cube)
>>> tf_cube
<tensorflow.python.eager.def_function.Function at 0x1546fc080> 
```

This TF Function can then be used exactly like the original Python function, and it will return the same result (but as tensors):

```python
>>> tf_cube(2)
<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>
>>> tf_cube(tf.constant(2.0))
<tfTensor: id=18634211, shape=(), dtype(float32, numpy=8.0) 
```

Under the hood, tf.function() analyzed the computations performed by the cube() function and generated an equivalent computation graph! As you can see, it was rather painless (we will see how this works shortly). Alternatively, we could have used tf.function as a decorator; this is actually more common:

@tf.function def tf_cube(x): return $\times \text{**}$ 3

The original Python function is still available via the TF Function’s python_function attribute, in case you ever need it:

```txt
>>tf_cube.python_function(2) 8 
```

TensorFlow optimizes the computation graph, pruning unused nodes, simplifying expressions (e.g., $1 + 2$ would get replaced with 3), and more. Once the optimized graph is ready, the TF Function efficiently executes the operations in the graph, in the appropriate order (and in parallel when it can). As a result, a TF Function will usually run much faster than the original Python function, especially if it performs complex

computations.14 Most of the time you will not really need to know more than that: when you want to boost a Python function, just transform it into a TF Function. That’s all!

Moreover, when you write a custom loss function, a custom metric, a custom layer or any other custom function, and you use it in a Keras model (as we did throughout this chapter), Keras automatically converts your function into a TF Function, no need to use tf.function(). So most of the time, all this magic is $1 0 0 \%$ transparent.

![](images/4851b1df70a327cbc531750b4fa622bc830084efb34327b4ddcb53af5dfa2a01.jpg)

You can tell Keras not to convert your Python functions to TF Functions by setting dynamic $\equiv$ True when creating a custom layer or a custom model. Alternatively, you can set run_eagerly=True when calling the model’s compile() method.

TF Function generates a new graph for every unique set of input shapes and data types, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con stant(10)), a graph will be generated for int32 tensors of shape []. Then if you call tf_cube(tf.constant(20)), the same graph will be reused. But if you then call tf_cube(tf.constant([10, 20])), a new graph will be generated for int32 tensors of shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument types and shapes). However, this is only true for tensor arguments: if you pass numer‐ ical Python values to a TF Function, a new graph will be generated for every distinct value: for example, calling tf_cube(10) and tf_cube(20) will generate two graphs.

![](images/03493789d91bfb69d9f8ac603afb77809e7a904d584bfae398de53bcbacab3f6.jpg)

If you call a TF Function many times with different numerical Python values, then many graphs will be generated, slowing down your program and using up a lot of RAM. Python values should be reserved for arguments that will have few unique values, such as hyperparameters like the number of neurons per layer. This allows TensorFlow to better optimize each variant of your model.

# Autograph and Tracing

So how does TensorFlow generate graphs? Well, first it starts by analyzing the Python function’s source code to capture all the control flow statements, such as for loops and while loops, if statements, as well as break, continue and return statements. This first step is called autograph. The reason TensorFlow has to analyze the source code is that Python does not provide any other way to capture control flow state‐ ments: it offers magic methods like __add__() or __mul__() to capture operators like

$^ +$ and *, but there are no __while__() or __if__() magic methods. After analyzing the function’s code, autograph outputs an upgraded version of that function in which all the control flow statements are replaced by the appropriate TensorFlow opera‐ tions, such as tf.while_loop() for loops and tf.cond() for if statements. For example, in Figure 12-4, autograph analyzes the source code of the sum_squares() Python function, and it generates the tf__sum_squares() function. In this function, the for loop is replaced by the definition of the loop_body() function (containing the body of the original for loop), followed by a call to the for_stmt() function. This call will build the appropriate tf.while_loop() operation in the computation graph.

![](images/8a88e89c3896d676e580117e0ac373a741aa75b3cb00d6eed94c73c9c4f84a74.jpg)  
Figure 12-4. How TensorFlow generates graphs using autograph and tracing

Next, TensorFlow calls this “upgraded” function, but instead of passing the actual argument, it passes a symbolic tensor, meaning a tensor without any actual value, only a name, a data type, and a shape. For example, if you call sum_squares(tf.con stant(10)), then the tf__sum_squares() function will actually be called with a sym‐ bolic tensor of type int32 and shape []. The function will run in graph mode, meaning that each TensorFlow operation will just add a node in the graph to represent itself and its output tensor(s) (as opposed to the regular mode, called eager execution, or eager mode). In graph mode, TF operations do not perform any actual computations. This should feel familiar if you know TensorFlow 1, as graph mode was the default mode. In Figure 12-4, you can see the tf__sum_squares() function being called with a symbolic tensor as argument (in this case, an int32 tensor of shape []), and the final graph generated during tracing. The ellipses represent operations, and the arrows represent tensors (both the generated function and the graph are simplified).

# TF Function Rules

Most of the time, converting a Python function that performs TensorFlow operations into a TF Function is trivial: just decorate it with @tf.function or let Keras take care of it for you. However, there are a few rules to respect:

• If you call any external library, including NumPy or even the standard library, this call will run only during tracing, it will not be part of the graph. Indeed, a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, and so on). So make sure you use tf.reduce_sum() instead of np.sum(), and tf.sort() instead of the built-in sorted() function, and so on (unless you really want the code to run only during tracing).

— For example, if you define a TF function $\mathsf { f } ( { \mathsf { x } } )$ that just returns np.ran dom.rand(), a random number will only be generated when the function is traced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the same random number, but f(tf.constant([2., 3.])) will return a different one. If you replace np.random.rand() with tf.random.uniform([]), then a new random number will be generated upon every call, since the operation will be part of the graph.   
— If your non-TensorFlow code has side-effects (such as logging something or updating a Python counter), then you should not expect that side-effect to occur every time you call the TF Function, as it will only occur when the func‐ tion is traced.   
— You can wrap arbitrary Python code in a tf.py_function() operation, but this will hinder performance, as TensorFlow will not be able to do any graph optimization on this code, and it will also reduce portability, as the graph will only run on platforms where Python is available (and the right libraries installed).   
• You can call other Python functions or TF Functions, but they should follow the same rules, as TensorFlow will also capture their operations in the computation graph. Note that these other functions do not need to be decorated with @tf.function.   
• If the function creates a TensorFlow variable (or any other stateful TensorFlow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else you will get an exception. It is usually preferable to create vari‐ ables outside of the TF Function (e.g., in the build() method of a custom layer).

• The source code of your Python function should be available to TensorFlow. If the source code is unavailable (for example, if you define your function in the Python shell, which does not give access to the source code, or if you deploy only the compiled Python files *.pyc to production), then the graph generation pro‐ cess will fail or have limited functionality.   
• TensorFlow will only capture for loops that iterate over a tensor or a Dataset. So make sure you use for i in tf.range(10) rather than for i in range(10), or else the loop will not be captured in the graph. Instead, it will run during tracing. This may be what you want, if the for loop is meant to build the graph, for exam‐ ple to create each layer in a neural network.   
• And as always, for performance reasons, you should prefer a vectorized imple‐ mentation whenever you can, rather than using loops.

It’s time to sum up! In this chapter we started with a brief overview of TensorFlow, then we looked at TensorFlow’s low-level API, including tensors, operations, variables and special data structures. We then used these tools to customize almost every com‐ ponent in tf.keras. Finally, we looked at how TF Functions can boost performance, how graphs are generated using autograph and tracing, and what rules to follow when you write TF Functions (if you would like to open the black box a bit further, for example to explore the generated graphs, you will find further technical details in ???).

In the next chapter, we will look at how to efficiently load and preprocess data with TensorFlow.

# Loading and Preprocessing Data with TensorFlow

![](images/ce260b859276095d010d0cc1a4fe7cdbefcca27efb13b1dfb0ba0f0577d447e9.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 13 in the final release of the book.

So far we have used only datasets that fit in memory, but Deep Learning systems are often trained on very large datasets that will not fit in RAM. Ingesting a large dataset and preprocessing it efficiently can be tricky to implement with other Deep Learning libraries, but TensorFlow makes it easy thanks to the Data API: you just create a data‐ set object, tell it where to get the data, then transform it in any way you want, and TensorFlow takes care of all the implementation details, such as multithreading, queuing, batching, prefetching, and so on.

Off the shelf, the Data API can read from text files (such as CSV files), binary files with fixed-size records, and binary files that use TensorFlow’s TFRecord format, which supports records of varying sizes. TFRecord is a flexible and efficient binary format based on Protocol Buffers (an open source binary format). The Data API also has support for reading from SQL databases. Moreover, many Open Source exten‐ sions are available to read from all sorts of data sources, such as Google’s BigQuery service.

However, reading huge datasets efficiently is not the only difficulty: the data also needs to be preprocessed. Indeed, it is not always composed strictly of convenient numerical fields: sometimes there will be text features, categorical features, and so on. To handle this, TensorFlow provides the Features API: it lets you easily convert these features to numerical features that can be consumed by your neural network. For

example, categorical features with a large number of categories (such as cities, or words) can be encoded using embeddings (as we will see, an embedding is a trainable dense vector that represents a category).

![](images/7c33c248d11970e086e91a7f05cee6df5ec4d9c3b622acd9eab4fea6798a0968.jpg)

Both the Data API and the Features API work seamlessly with tf.keras.

In this chapter, we will cover the Data API, the TFRecord format and the Features API in detail. We will also take a quick look at a few related projects from Tensor‐ Flow’s ecosystem:

• TF Transform (tf.Transform) makes it possible to write a single preprocessing function that can be run both in batch mode on your full training set, before training (to speed it up), and then exported to a TF Function and incorporated into your trained model, so that once it is deployed in production, it can take care of preprocessing new instances on the fly.   
• TF Datasets (TFDS) provides a convenient function to download many common datasets of all kinds, including large ones like ImageNet, and it provides conve‐ nient dataset objects to manipulate them using the Data API.

So let’s get started!

# The Data API

The whole Data API revolves around the concept of a dataset: as you might suspect, this represents a sequence of data items. Usually you will use datasets that gradually read data from disk, but for simplicity let’s just create a dataset entirely in RAM using tf.data.Dataset.from_tensor_slices():

```python
>>> X = tf(range(10)  # any data tensor
>>> dataset = tf.data.Dataset.from_tensor_slices(X)
>>> dataset <TensorSliceDataset shapes: (), types: tf.int32> 
```

The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X (along the first dimension), so this dataset con‐ tains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same dataset if we had used tf.data.Dataset.range(10).

You can simply iterate over a dataset’s items like this:

```rst
>>> for item in dataset:  
... print(item) 
```

```txt
tf.Tensor(0, shape=(), dtype=int32)  
tf.Tensor(1, shape=(), dtype=int32)  
tf.Tensor(2, shape=(), dtype=int32)  
[...]  
tf.Tensor(9, shape=(), dtype=int32) 
```

# Chaining Transformations

Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain trans‐ formations like this (this chain is illustrated in Figure 13-1):

```python
>>> dataset = datasetrepeat(3).batch(7)  
>>> for item in dataset:  
... print(item)  
...  
tf.Tensor([0 1 2 3 4 5 6], shape=(7,) dtype=int32)  
tfTensor([7 8 9 0 1 2 3], shape=(7,) dtype=int32)  
tfTensor([4 5 6 7 8 9 0], shape=(7,) dtype=int32)  
tfTensor([1 2 3 4 5 6 7], shape=(7,) dtype=int32)  
tfTensor([8 9], shape=(2,) dtype=int32) 
```

![](images/30de68397398ca525d034e334dc25fe3495154a78ba06075fc2f6934a788eae5.jpg)  
Figure 13-1. Chaining Dataset Transformations

In this example, we first call the repeat() method on the original dataset, and it returns a new dataset that will repeat the items of the original dataset 3 times. Of course, this will not copy the whole data in memory 3 times! In fact, if you call this method with no arguments, the new dataset will repeat the source dataset forever. Then we call the batch() method on this new dataset, and again this creates a new dataset. This one will group the items of the previous dataset in batches of 7 items. Finally, we iterate over the items of this final dataset. As you can see, the batch() method had to output a final batch of size 2 instead of 7, but you can call it with drop_remainder $\mathbf { \bar { \mathbf { \alpha } } } =$ True if you want it to drop this final batch so that all batches have the exact same size.

![](images/9c18a01cba9e81693dbf9d5aac4368670f392ba0b4f2157f9fcf2cc51c9d2443.jpg)

The dataset methods do not modify datasets, they create new ones, so make sure to keep a reference to these new datasets (e.g., data set = ...), or else nothing will happen.

You can also apply any transformation you want to the items by calling the map() method. For example, this creates a new dataset with all items doubled:

>>> dataset $=$ dataset.map(lambda $\times \colon \textsf { x } ^ { \textsf { * } } 2$ ) # Items: [0,2,4,6,8,10,12]

This function is the one you will call to apply any preprocessing you want to your data. Sometimes, this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speed things up: it’s as simple as setting the num_parallel_calls argument.

While the map() applies a transformation to each item, the apply() method applies a transformation to the dataset as a whole. For example, the following code “unbatches” the dataset, by applying the unbatch() function to the dataset (this function is cur‐ rently experimental, but it will most likely move to the core API in a future release). Each item in the new dataset will be a single integer tensor instead of a batch of 7 integers:

>>> dataset $=$ dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...

It is also possible to simply filter the dataset using the filter() method:

>>> dataset $=$ dataset.filter(lambda $\mathbf { x } \colon \mathbf { \textnormal { x } } < \mathbf { \textnormal { \textbf { \textsf { x } } } }$ ) # Items: 0 2 4 6 8 0 2 4 6...

You will often want to look at just a few items from a dataset. You can use the take() method for that:

```python
>>> for item in dataset.take(3):  
... print(item)  
...  
tf.Tensor(0, shape=(), dtype=int64)  
tfTensor(2, shape=(), dtype=int64)  
tfTensor(4, shape=(), dtype=int64) 
```

# Shuing the Data

As you know, Gradient Descent works best when the instances in the training set are independent and identically distributed (see Chapter 4). A simple way to ensure this is to shuffle the instances. For this, you can just use the shuffle() method. It will create a new dataset that will start by filling up a buffer with the first items of the source dataset, then whenever it is asked for an item, it will pull one out randomly from the buffer, and replace it with a fresh one from the source dataset, until it has iterated entirely through the source dataset. At this point it continues to pull out items randomly from the buffer until it is empty. You must specify the buffer size, and

it is important to make it large enough or else shuffling will not be very efficient.1 However, obviously do not exceed the amount of RAM you have, and even if you have plenty of it, there’s no need to go well beyond the dataset’s size. You can provide a random seed if you want the same random order every time you run your program.

```python
>>> dataset = tf.data.Dataset(range(10).repeat(3) # 0 to 9, three times  
>>> dataset = datasetshuffle(buffer_size=5, seed=42).batch(7)  
>>> for item in dataset:  
... print(item)  
...  
tf.Tensor([[0 2 3 6 7 9 4], shape=(7,) , dtype=int64)  
tfTensor([[5 0 1 1 8 6 5], shape=(7,) , dtype=int64)  
tfTensor([[4 8 7 1 2 3 0], shape=(7,) , dtype=int64)  
tfTensor([[5 4 2 7 8 9 9], shape=(7,) , dtype=int64)  
tfTensor([[3 6], shape=(2,) , dtype=int64) 
```

![](images/574d0281878282d95cd4b45546d290024d54e761eb37937bd677cf6f3767f408.jpg)

If you call repeat() on a shuffled dataset, by default it will generate a new order at every iteration. This is generally a good idea, but if you prefer to reuse the same order at each iteration (e.g., for tests or debugging), you can set reshuffle_each_iteration=False.

For a large dataset that does not fit in memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to the dataset. One sol‐ ution is to shuffle the source data itself (for example, on Linux you can shuffle text files using the shuf command). This will definitely improve shuffling a lot! However, even if the source data is shuffled, you will usually want to shuffle it some more, or else the same order will be repeated at each epoch, and the model may end up being biased (e.g., due to some spurious patterns present by chance in the source data’s order). To shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training. However, instances located in the same file will still end up close to each other. To avoid this you can pick multiple files randomly, and read them simultaneously, interleaving their lines. Then on top of that you can add a shuffling buffer using the shuffle() method. If all this sounds like a lot of work, don’t worry: the Data API actually makes all this possible in just a few lines of code. Let’s see how to do this.

# Interleaving Lines From Multiple Files

First, let’s suppose that you loaded the California housing dataset, you shuffled it (unless it was already shuffled), you split it into a training set, a validation set and a test set, then you split each set into many CSV files that each look like this (each row contains 8 input features plus the target median house value):

```csv
MedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue 3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442 5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687 3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621 [...] 
```

Let’s also suppose train_filepaths contains the list of file paths (and you also have valid_filepaths and test_filepaths):

```txt
>>> train_filepaths
['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ... ] 
```

Now let’s create a dataset containing only these file paths:

```python
filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42) 
```

By default, the list_files() function returns a dataset that shuffles the file paths. In general this is a good thing, but you can set shuffle=False if you do not want that, for some reason.

Next, we can call the interleave() method to read from 5 files at a time and inter‐ leave their lines (skipping the first line of each file, which is the header row, using the skip() method):

n_readers $= 5$ dataset $\equiv$ filepath_dataset.interleave( lambdafilepath:tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers)

The interleave() method will create a dataset that will pull 5 file paths from the filepath_dataset, and for each one it will call the function we gave it (a lambda in this example) to create a new dataset, in this case a TextLineDataset. It will then cycle through these 5 datasets, reading one line at a time from each until all datasets are out of items. Then it will get the next 5 file paths from the filepath_dataset, and interleave them the same way, and so on until it runs out of file paths.

![](images/009be8e3490d93e6094459463b77801627df591498d2010e077835b537d0737d.jpg)

For interleaving to work best, it is preferable to have files of identi‐ cal length, or else the end of the longest files will not be interleaved.

By default, interleave() does not use parallelism, it just reads one line at a time from each file, sequentially. However, if you want it to actually read files in parallel, you can set the num_parallel_calls argument to the number of threads you want. You can even set it to tf.data.experimental.AUTOTUNE to make TensorFlow choose the right number of threads dynamically based on the available CPU (however, this is an experimental feature for now). Let’s look at what the dataset contains now:

```csv
>>> for line in dataset.take(5):  
... print(line.numpy())  
...  
b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'  
b'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'  
b'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'  
b'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'  
b'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442' 
```

These are the first rows (ignoring the header row) of 5 CSV files, chosen randomly. Looks good! But as you can see, these are just byte strings, we need to parse them, and also scale the data.

# Preprocessing the Data

Let’s implement a small function that will perform this preprocessing:

```python
X_mean, X_std = [... ] # mean and scale of each feature in the training set  
n_entries = 8 
```

```python
def preprocess(line):
    defends = [0.]*n_input + [tf.constant([], dtype=tf.float32)]
    fields = tf.io Decode_csv(line, record_defaults= defends)
    x = tf.stack.fields[-1])
    y = tf.stack.fields[-1])
    return (x - X_mean) / X_std, y 
```

Let’s walk through this code:

• First, we assume that you have precomputed the mean and standard deviation of each feature in the training set. X_mean and X_std are just 1D tensors (or NumPy arrays) containing 8 floats, one per input feature.   
• The preprocess() function takes one CSV line, and starts by parsing it. For this, it uses the tf.io.decode_csv() function, which takes two arguments: the first is the line to parse, and the second is an array containing the default value for each column in the CSV file. This tells TensorFlow not only the default value for each column, but also the number of columns and the type of each column. In this example, we tell it that all feature columns are floats and missing values should default to 0, but we provide an empty array of type tf.float32 as the default value for the last column (the target): this tells TensorFlow that this column con‐

tains floats, but that there is no default value, so it will raise an exception if it encounters a missing value.

• The decode_csv() function returns a list of scalar tensors (one per column) but we need to return 1D tensor arrays. So we call tf.stack() on all tensors except for the last one (the target): this will stack these tensors into a 1D array. We then do the same for the target value (this makes it a 1D tensor array with a single value, rather than a scalar tensor).   
• Finally, we scale the input features by subtracting the feature means and then dividing by the feature standard deviations, and we return a tuple containing the scaled features and the target.

Let’s test this preprocessing function:

```txt
>>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')
(<tf.Tensor: id=6227, shape=(8), dtype=float32, numpy=
array([0.16579159, 1.216324, -0.05204564, -0.39215982, -0.5277444,
-0.2633488, 0.8543046, -1.3072058], dtype=float32)>
<tfTensor: [...], numpy=array([2.782], dtype=float32>) 
```

We can now apply this preprocessing function to the dataset.

# Putting Everything Together

To make the code reusable, let’s put together everything we have discussed so far into a small helper function: it will create and return a dataset that will efficiently load Cal‐ ifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it (see Figure 13-2):

def csv_reader_dataset(filepaths,repeat $\equiv$ None，n_readers $= 5$ n_read Threads $\equiv$ None，shuffle_buffer_size $= 10000$ n_scan Threads $= 5$ ，batch_size $= 32$ ): dataset $=$ tf.data.Dataset.list_files(filepaths).repeat(repeat) dataset $=$ dataset.interleave( lambdafilepath:tf.data.TextLineDataset(filepath).skip(1), cycle_length $\equiv$ n_readers，num_parallel Calls $\equiv$ n_read Threads) dataset $=$ dataset/shuffle(shuffle_buffer_size) dataset $=$ dataset.map(preprocess，num_parallel Calls $\equiv$ n_scan threads) dataset $=$ dataset.batch(batch_size) return dataset��.hetch(1)

![](images/207a8d5ef838cac76ffe16d57aa3b695e1efcb0a0a78f271ebf7b3c7bb1f0aa4.jpg)  
Figure 13-2. Loading and Preprocessing Data From Multiple CSV Files

Everything should make sense in this code, except the very last line (prefetch(1)), which is actually quite important for performance.

# Prefetching

By calling prefetch(1) at the end, we are creating a dataset that will do its best to always be one batch ahead2. In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready. This can improve performance dramatically, as is illustrated on Figure 13-3. If we also ensure that loading and preprocessing are multithreaded (by setting num_par allel_calls when calling interleave() and map()), we can exploit multiple cores on the CPU and hopefully make preparing one batch of data shorter than running a training step on the GPU: this way the GPU will be almost $1 0 0 \%$ utilized (except for the data transfer time from the CPU to the GPU), and training will run much faster.

# Without prefetching

![](images/c8691ae2aa982c2e28cc2c7bb03260b920b8d6692b08afbca2048b8324876b96.jpg)

# With Prefetching

![](images/d8a1ca4200d2451ed5f2ceae8a593a840cb9f39942dbd49f2148980949382906.jpg)

# With Prefetching $^ +$ Multithreaded Loading & Preprocessing

![](images/dbeec212c6dc31ad64e05a75caa50cd48d4148dd5dfaaa240ed2ba5bb0de05bb.jpg)

![](images/05af382b8c508092de6934211fe89cc5619f8452c066654c9ce7250143e5542c.jpg)  
Figure 13-3. Speedup Training anks to Prefetching and Multithreading

If you plan to purchase a GPU card, its processing power and its memory size are of course very important (in particular, a large RAM is crucial for computer vision), but its memory bandwidth is just as important as the processing power to get good performance: this is the number of gigabytes of data it can get in or out of its RAM per second.

With that, you can now build efficient input pipelines to load and preprocess data from multiple text files. We have discussed the most common dataset methods, but there are a few more you may want to look at: concatenate(), zip(), window(), reduce(), cache(), shard(), flat_map() and padded_batch(). There are also a cou‐ ple more class methods: from_generator() and from_tensors(), which create a new dataset from a Python generator or a list of tensors respectively. Please check the API documentation for more details. Also note that there are experimental features avail‐ able in tf.data.experimental, many of which will most likely make it to the core API in future releases (e.g., check out the CsvDataset class and the SqlDataset classes).

# Using the Dataset With tf.keras

Now we can use the csv_reader_dataset() function to create a dataset for the train‐ ing set (ensuring it repeats the data forever), the validation set and the test set:

```txt
train_set = csv_reader_dataset(train_filepaths, repeat=None)  
valid_set = csv_reader_dataset(valid_filepaths)  
test_set = csv_reader_dataset(test_filepaths) 
```

And now we can simply build and train a Keras model using these datasets.3 All we need to do is to call the fit() method with the datasets instead of X_train and y_train, and specify the number of steps per epoch for each set:4

```txt
model = keras.models Sequential([...])
model.compile([...])
model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,
						validation_data=valid_set,
						validation_steps=len(X_valid) // batch_size) 
```

Similarly, we can pass a dataset to the evaluate() and predict() methods (and again specify the number of steps per epoch):

```txt
model.evaluate(test_set, steps=len(X_test) // batch_size)  
model.predict(new_set, steps=len(X_new) // batch_size) 
```

Unlike the other sets, the new_set will usually not contain labels (if it does, Keras will just ignore them). Note that in all these cases, you can still use NumPy arrays instead of datasets if you want (but of course they need to have been loaded and preprocessed first).

If you want to build your own custom training loop (as in Chapter 12), you can just iterate over the training set, very naturally:

```python
for X_batch, y_batch in train_set:  
    [...] # perform one gradient descent step 
```

In fact, it is even possible to create a tf.function (see Chapter 12) that performs the whole training loop!5

@tf.function   
def train(model, optimizer, loss_fn, n_epochs, [..]): train_set $=$ csv_reader_dataset(train_filepaths, repeat $\equiv$ n_epochs, [..]) for X_batch, y_batch in train_set: with tfGradientTape() as tape:

```python
y_pred = model(X_batch)  
main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))  
loss = tf.add_n([[main_loss] + model.losses)  
grads = tape.gradient(loss, model.trainable_variables)  
optimizer.apply_gradients(zip(graids, model.trainable_variables)) 
```

Congratulations, you now know how to build powerful input pipelines using the Data API! However, so far we have used CSV files, which are common, simple and conve‐ nient, but they are not really efficient, and they do not support large or complex data structures very well, such as images or audio. So let’s use TFRecords instead.

![](images/0f1fed24d8422949a0f44dceb43006c48f39737d741380f7d01ab72a1c4aa681.jpg)

If you are happy with CSV files (or whatever other format you are using), you do not have to use TFRecords. As the saying goes, if it ain’t broke, don’t fix it! TFRecords are useful when the bottleneck during training is loading and parsing the data.

# The TFRecord Format

The TFRecord format is TensorFlow’s preferred format for storing large amounts of data and reading it efficiently. It is a very simple binary format that just contains a sequence of binary records of varying sizes (each record just has a length, a CRC checksum to check that the length was not corrupted, then the actual data, and finally a CRC checksum for the data). You can easily create a TFRecord file using the tf.io.TFRecordWriter class:

```python
with tf.io.TFRecordWriter("my_data.tfrecord") as f:  
    f.write(b"This is the first record")  
    f.write(b"And this is the second record") 
```

And you can then use a tf.data.TFRecordDataset to read one or more TFRecord files:

filepaths $=$ ["my_data.tfrecord"] dataset $=$ tf.data.TFRecordDataset(filepaths) for item in dataset: print(item)

This will output:

tf.Tensor(b'This is the first record', shape=(), dtype=string) tf.Tensor(b'And this is the second record', shape=(), dtype $\mathbf { = }$ string)

![](images/68584a86260435f27df9a1e5cf66ec1ac1c81c93efe5781b029b0093e5d7dbbd.jpg)

By default, a TFRecordDataset will read files one by one, but you can make it read multiple files in parallel and interleave their records by setting num_parallel_reads. Alternatively, you could obtain the same result by using list_files() and interleave() as we did earlier to read multiple CSV files.

# Compressed TFRecord Files

It can sometimes be useful to compress your TFRecord files, especially if they need to be loaded via a network connection. You can create a compressed TFRecord file by setting the options argument:

```python
options = tf.io.TFrecordOptions(compression_type="GZIP") with tf.io.TFrecordWriter("my_compressed.tfrecord", options) as f: [...] 
```

When reading a compressed TFRecord file, you need to specify the compression type:

```python
dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'], compression_type="GZIP") 
```

# A Brief Introduction to Protocol Buers

Even though each record can use any binary format you want, TFRecord files usually contain serialized Protocol Buffers (also called protobufs). This is a portable, extensi‐ ble and efficient binary format developed at Google back in 2001 and Open Sourced in 2008, and they are now widely used, in particular in gRPC, Google’s remote proce‐ dure call system. Protocol Buffers are defined using a simple language that looks like this:

syntax $=$ "proto3";   
message Person { string name $= 1$ int32 id $= 2$ repeated string email $= 3$

This definition says we are using the protobuf format version 3, and it specifies that each Person object6 may (optionally) have a name of type string, an id of type int32, and zero or more email fields, each of type string. The numbers 1, 2 and 3 are the field identifiers: they will be used in each record’s binary representation. Once you have a definition in a .proto file, you can compile it. This requires protoc, the proto‐ buf compiler, to generate access classes in Python (or some other language). Note that the protobuf definitions we will use have already been compiled for you, and their Python classes are part of TensorFlow, so you will not need to use protoc. All you need to know is how to use protobuf access classes in Python. To illustrate the basics, let’s look at a simple example that uses the access classes generated for the Person protobuf (the code is explained in the comments):

```python
>>> from person_pb2 import Person # import the generated access class
>>> person = Person(name="Al", id=123, email=['a@b.com']) # create a Person
>>> print(person) # display the Person 
```

```txt
name: "Al"  
id: 123  
email: "a@b.com"  
>>> person.name # read a field  
"Al"  
>>> person.name = "Alice" # modify a field  
>>> person.email[0] # repeated fields can be accessed like arrays  
"a@b.com"  
>>> person.email.append("c@d.com") # add an email address  
>>> s = person_seralyzeToString() # serialize the object to a byte string  
>>> s  
b'\n\x05Alice\x10{\x1a\x07a@b.com\x1a\x07c@d.com'  
>>> person2 = Person() # create a new Person  
>>> person2.ParseFromString(s) # parse the byte string (27 bytes long)  
27  
>>> person == person2 # now they are equal  
True 
```

In short, we import the Person class generated by protoc, we create an instance and we play with it, visualizing it, reading and writing some fields, then we serialize it using the SerializeToString() method. This is the binary data that is ready to be saved or transmitted over the network. When reading or receiving this binary data, we can parse it using the ParseFromString() method, and we get a copy of the object that was serialized.7

We could save the serialized Person object to a TFRecord file, then we could load and parse it: everything would work fine. However, SerializeToString() and ParseFrom String() are not TensorFlow operations (and neither are the other operations in this code), so they cannot be included in a TensorFlow Function (except by wrapping them in a tf.py_function() operation, which would make the code slower and less portable, as we saw in Chapter 12). Fortunately, TensorFlow does include special pro‐ tobuf definitions for which it provides parsing operations.

# TensorFlow Protobufs

The main protobuf typically used in a TFRecord file is the Example protobuf, which represents one instance in a dataset. It contains a list of named features, where each feature can either be a list of byte strings, a list of floats or a list of integers. Here is the protobuf definition:

syntax $=$ "proto3";   
message BytesList{repeated bytes value $= 1$ ；}   
message FloatList{repeated float value $= 1$ [packed $\equiv$ true]；}   
message Int64List{repeated int64 value $= 1$ [packed $\equiv$ true]；}

```proto
message Feature {
    oneof kind {
        BytesList bytes_list = 1;
        FloatList float_list = 2;
        Int64List int64_list = 3;
    }
};  
message Features { map<string, Feature> feature = 1; };  
message Example { Features features = 1; }; 
```

The definitions of BytesList, FloatList and Int64List are straightforward enough ([packed $=$ true] is used for repeated numerical fields, for a more efficient encod‐ ing). A Feature either contains a BytesList, a FloatList or an Int64List. A Fea tures (with an s) contains a dictionary that maps a feature name to the corresponding feature value. And finally, an Example just contains a Features object.8 Here is how you could create a tf.train.Example representing the same person as earlier, and write it to TFRecord file:

from tensorflow.train import BytesList, FloatList, Int64List   
from tensorflow.train import Feature, Features, Example   
person_example $=$ Example( features=Features( feature $\coloneqq$ { "name":Feature(bytes_list $\equiv$ BytesList(value $\equiv$ [b"Alice])）， "id":Feature(int64_list $\equiv$ Int64List(value $\equiv$ [123])， "emails":Feature(bytes_list $\equiv$ BytesList(value $\equiv$ [b"a@b.com"， b"c@d.com"]） ）

The code is a bit verbose and repetitive, but it’s rather straightforward (and you could easily wrap it inside a small helper function). Now that we have an Example protobuf, we can serialize it by calling its SerializeToString() method, then write the result‐ ing data to a TFRecord file:

```python
with tf.io.TFrecordWriter("myContacts.tfrecord") as f:  
    f.write(person_example.methodToString()) 
```

Normally you would write much more than just one example! Typically, you would create a conversion script that reads from your current format (say, CSV files), creates an Example protobuf for each instance, serializes them and saves them to several TFRecord files, ideally shuffling them in the process. This requires a bit of work, so once again make sure it is really necessary (perhaps your pipeline works fine with CSV files).

Now that we have a nice TFRecord file containing a serialized Example, let’s try to load it.

# Loading and Parsing Examples

To load the serialized Example protobufs, we will use a tf.data.TFRecordDataset once again, and we will parse each Example using tf.io.parse_single_example(). This is a TensorFlow operation so it can be included in a TF Function. It requires at least two arguments: a string scalar tensor containing the serialized data, and a description of each feature. The description is a dictionary that maps each feature name to either a tf.io.FixedLenFeature descriptor indicating the feature’s shape, type and default value, or a tf.io.VarLenFeature descriptor indicating only the type (if the length may vary, such as for the "emails" feature). For example:

```python
feature_description = {
    "name": tf.io.FixedLenFeature,[], tf.string, default_value=""),
    "id": tf.io.FixedLenFeature,[], tf.int64, default_value=0),
    "emails": tf.io.VarLenFeature(tf.string),
}  
for serialized_example in tf.data.TFRecordDataset [["my Contacts.tfrecord"]),  
    parsed_example = tf.io.parse_single_example(seriesized_example, feature_description) 
```

The fixed length features are parsed as regular tensors, but the variable length fea‐ tures are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor using tf.sparse.to_dense(), but in this case it is simpler to just access its values:

```python
>>> tfsparse.to_dense(parsed_example["emails"], default_value=b)

<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])
>>> parsed_example["emails"].values
<tfTensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])> 
```

A BytesList can contain any binary data you want, including any serialized object. For example, you can use tf.io.encode_jpeg() to encode an image using the JPEG format, and put this binary data in a BytesList. Later, when your code reads the TFRecord, it will start by parsing the Example, then you will need to call tf.io.decode_jpeg() to parse the data and get the original image (or you can use tf.io.decode_image(), which can decode any BMP, GIF, JPEG or PNG image). You can also store any tensor you want in a BytesList by serializing the tensor using tf.io.serialize_tensor(), then putting the resulting byte string in a BytesList feature. Later, when you parse the TFRecord, you can parse this data using tf.io.parse_tensor().

Instead of parsing examples one by one using tf.io.parse_single_example(), you may want to parse them batch by batch using tf.io.parse_example():

dataset $=$ tf.data.TFrecordDataset([myContacts.tfrecord]).batch(10)   
for serializedexamples in dataset: parsedexamples $=$ tf.io.sleepExample serializeexamples, feature_description)

As you can see, the Example proto will probably be sufficient for most use cases. However, it may be a bit cumbersome to use when you are dealing with lists of lists. For example, suppose you want to classify text documents. Each document may be represented as a list of sentences, where each sentence is represented as a list of words. And perhaps each document also has a list of comments, where each com‐ ment is also represented as a list of words. Moreover, there may be some contextual data as well, such as the document’s author, title and publication date. TensorFlow’s SequenceExample protobuf is designed for such use cases.

# Handling Lists of Lists Using the SequenceExample Protobuf

Here is the definition of the SequenceExample protobuf:

message FeatureList { repeated Feature feature $= 1$ ;};   
message FeatureLists { map<string,FeatureList> feature_list $= 1$ };   
message SequenceExample{ Features context $= 1$ . FeatureLists featureLists $= 2$ ..   
}；

A SequenceExample contains a Features object for the contextual data and a Fea tureLists object which contains one or more named FeatureList objects (e.g., a FeatureList named "content" and another named "comments"). Each FeatureList just contains a list of Feature objects, each of which may be a list of byte strings, a list of 64-bit integers or a list of floats (in this example, each Feature would represent a sentence or a comment, perhaps in the form of a list of word identifiers). Building a SequenceExample, serializing it and parsing it is very similar to building, serializing and parsing an Example, but you must use tf.io.parse_single_sequence_exam ple() to parse a single SequenceExample or tf.io.parse_sequence_example() to parse a batch, and both functions return a tuple containing the context features (as a dictionary) and the feature lists (also as a dictionary). If the feature lists contain sequences of varying sizes (as in the example above), you may want to convert them to ragged tensors using tf.RaggedTensor.from_sparse() (see the notebook for the full code):

```python
parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(
serialized_sequence_example, context_feature descriptions,
(sequence_featureDescriptions)
 parsed_content = tf.RaggedTensor.from Sparse(parsed_featureLists["content']) 
```

Now that you know how to efficiently store, load and parse data, the next step is to prepare it so that it can be fed to a neural network. This means converting all features

into numerical features (ideally not too sparse), scaling them, and more. In particular, if your data contains categorical features or text features, they need to be converted to numbers. For this, the Features API can help.

# The Features API

Preprocessing your data can be performed in many ways: it can be done ahead of time when preparing your data files, using any tool you like. Or you can preprocess your data on the fly when loading it with the Data API (e.g., using the dataset’s map() method, as we saw earlier). Or you can include a preprocessing layer directly in your model. Whichever solution you prefer, the Features API can help you: it is a set of functions available in the tf.feature_column package, which let you define how each feature (or group of features) in your data should be preprocessed (therefore you can think of this API as the analog of Scikit-Learn’s ColumnTransformer class). We will start by looking at the different types of columns available, and then we will look at how to use them.

Let’s go back to the variant of the California housing dataset that we used in Chap‐ ter 2, since it includes a categorical feature and missing data. Here is a simple numeri‐ cal column named "housing_median_age":

```python
housing_median_age = tf.feature_column.numeric_column("housing_median_age") 
```

Numeric columns let you specify a normalization function using the normalizer_fn argument. For example, let’s tweak the "housing_median_age" column to define how it should be scaled. Note that this requires computing ahead of time the mean and standard deviation of this feature in the training set:

```python
age_mean, age_std = X_mean[1], X_std[1] # The median age is column in 1
housing_median_age = tf.feature_columnouncing_colnum( "housing_median_age", normalizer_fn=lambda x: (x - age_mean) / age_std) 
```

In some cases, it might improve performance to bucketize some numerical features, effectively transforming a numerical feature into a categorical feature. For example, let’s create a bucketized column based on the median_income column, with 5 buckets: less than 1.5 $( \$ 15,000)$ , then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when you specify 4 boundaries, there are actually 5 buckets):

```python
median_income = tf.feature_column.numeric_column("median_income")  
bucketized_income = tf.feature_column.bucketized_column(  
  median_income, boundaries=[1.5, 3., 4.5, 6.]) 
```

If the median_income feature is equal to, say, 3.2, then the bucketized_income feature will automatically be equal to 2 (i.e., the index of the corresponding income bucket). Choosing the right boundaries can be somewhat of an art, but one approach is to just use percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If a feature is multimodal, meaning it has separate peaks in its distribution, you may

want to define a bucket for each mode, placing the boundaries in between the peaks. Whether you use the percentiles or the modes, you need to analyze the distribution of your data ahead of time, just like we had to measure the mean and standard deviation ahead of time to normalize the housing_median_age column.

# Categorical Features

For categorical features such as ocean_proximity, there are several options. If it is already represented as a category ID (i.e., an integer from 0 to the max ID), then you can use the categorical_column_with_identity() function (specifying the max ID). If not, and you know the list of all possible categories, then you can use categori cal_column_with_vocabulary_list():

```python
oceanprox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']  
ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list("ocean_proximity", oceanprox_vocab) 
```

If you prefer to have TensorFlow load the vocabulary from a file, you can call catego rical_column_with_vocabulary_file() instead. As you might expect, these two functions will simply map each category to its index in the vocabulary (e.g., NEAR BAY will be mapped to 3), and unknown categories will be mapped to -1.

For categorical columns with a large vocabulary (e.g., for zipcodes, cities, words, products, users, etc.), it may not be convenient to get the full list of possible cate‐ gories, or perhaps categories may be added or removed so frequently that using cate‐ gory indices would be too unreliable. In this case, you may prefer to use a categorical_column_with_hash_bucket(). If we had a "city" feature in the dataset, we could encode it like this:

```txt
city_hash = tf.feature_column.categorical_column_with_hash_buckets( "city", hash;bucket_size=1000) 
```

This feature will compute a hash for each category (i.e., for each city), modulo the number of hash buckets (hash_bucket_size). You must set the number of buckets high enough to avoid getting too many collisions (i.e., different categories ending up in the same bucket), but the higher you set it, the more RAM will be used (by the embedding table, as we will see shortly).

# Crossed Categorical Features

If you suspect that two (or more) categorical features are more meaningful when used jointly, then you can create a crossed column. For example, suppose people are partic‐ ularly fond of old houses inland and new houses near the ocean, then it might help to

create a bucketized column for the housing_median_age feature9, and cross it with the ocean_proximity column. The crossed column will compute a hash of every age & ocean proximity combination it comes across, modulo the hash_bucket_size, and this will give it the cross category ID. You may then choose to use only this crossed column in your model, or also include the individual columns.

```python
bucketized_age = tf.feature_column(bucketized_column(housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled  
age_and_ocean_proximity = tf.feature_column CROSSED_column(bucketized_age, ocean_proximity], hash_buckets_size=100) 
```

Another common use case for crossed columns is to cross latitude and longitude into a single categorical feature: you start by bucketizing the latitude and longitude, for example into 20 buckets each, then you cross these bucketized features into a loca tion column. This will create a $2 0 \times 2 0$ grid over California, and each cell in the grid will correspond to one category:

latitude $=$ tf.feature_column[numeric_column("latitude")   
longitude $=$ tf.feature_column[numeric_column("longitude")   
bucketizedLatitude $=$ tf.feature_column.bucketized_column( latitude，boundaries $\equiv$ list(np.linspace(32.，42.，20-1)))   
bucketized_longitude $=$ tf.feature_column.bucketized_column( longitude，boundaries $\equiv$ list(np.linspace(-125.，-114.，20-1)))   
location $=$ tf.feature_column.crossed_column( [bucketizedLatitude，bucketized_longitude]，hash:bucket_size=1000)

# Encoding Categorical Features Using One-Hot Vectors

No matter which option you choose to build a categorical feature (categorical col‐ umns, bucketized columns or crossed columns), it must be encoded before you can feed it to a neural network. There are two options to encode a categorical feature: one-hot vectors or embeddings. For the first option, simply use the indicator_col umn() function:

```python
ocean_proximity_one-hot = tf.feature_column.indicator_column(ocean_proximity) 
```

A one-hot vector encoding has the size of the vocabulary length, which is fine if there are just a few possible categories, but if the vocabulary is large, you will end up with too many inputs fed to your neural network: it will have too many weights to learn and it will probably not perform very well. In particular, this will typically be the case when you use hash buckets. In this case, you should probably encode them using embeddings instead.

![](images/de43e4c50b6565b25b97ea9430af072777f54c5a7fd0abeedcc7d24e97eda684.jpg)

As a rule of thumb (but your mileage may vary!), if the number of categories is lower than 10, then one-hot encoding is generally the way to go. If the number of categories is greater than 50 (which is often the case when you use hash buckets), then embeddings are usually preferable. In between 10 and 50 categories, you may want to experiment with both options and see which one works best for your use case. Also, embeddings typically require more training data, unless you can reuse pretrained embeddings.

# Encoding Categorical Features Using Embeddings

An embedding is a trainable dense vector that represents a category. By default, embeddings are initialized randomly, so for example the "NEAR BAY" category could be represented initially by a random vector such as [0.131, 0.890], while the "NEAR OCEAN" category may be represented by another random vector such as [0.631, 0.791] (in this example, we are using 2D embeddings, but the number of dimensions is a hyperparameter you can tweak). Since these embeddings are trainable, they will gradually improve during training, and as they represent fairly similar categories, Gradient Descent will certainly end up pushing them closer together, while it will tend to move them away from the "INLAND" category’s embedding (see Figure 13-4). Indeed, the better the representation, the easier it will be for the neural network to make accurate predictions, so training tends to make embeddings useful representa‐ tions of the categories. This is called representation learning (we will see other types of representation learning in ???).

![](images/2d56ec465c35ceafedaca9a6c91801d70b705f29af73cd2a6e291b3ecdff0bdd.jpg)  
Figure 13-4. Embeddings Will Gradually Improve During Training

# Word Embeddings

Not only will embeddings generally be useful representations for the task at hand, but quite often these same embeddings can be reused successfully for other tasks as well. The most common example of this is word embeddings (i.e., embeddings of individual words): when you are working on a natural language processing task, you are often better off reusing pretrained word embeddings than training your own. The idea of using vectors to represent words dates back to the 1960s, and many sophisticated techniques have been used to generate useful vectors, including using neural net‐ works, but things really took off in 2013, when Tomáš Mikolov and other Google researchers published a paper10 describing how to learn word embeddings using deep neural networks, much faster than previous attempts. This allowed them to learn embeddings on a very large corpus of text: they trained a deep neural network to pre‐ dict the words near any given word. This allowed them to obtain astounding word embeddings. For example, synonyms had very close embeddings, and semantically related words such as France, Spain, Italy, and so on, ended up clustered together. But it’s not just about proximity: word embeddings were also organized along meaningful axes in the embedding space. Here is a famous example: if you compute King – Man $^ +$ Woman (adding and subtracting the embedding vectors of these words), then the result will be very close to the embedding of the word Queen (see Figure 13-5). In other words, the word embeddings encode the concept of gender! Similarly, you can compute Madrid – Spain $^ +$ France, and of course the result is close to Paris, which seems to show that the notion of capital city was also encoded in the embeddings.

![](images/4df0e9396363adffa678be62ad5b2e61df367ed3f142db704e8117610f32899c.jpg)  
Figure 13-5. Word Embeddings

Let’s go back to the Features API. Here is how you could encode the ocean_proxim ity categories as 2D embeddings:

```python
ocean_proximity_embedding = tf.feature_column_embedding_column(ocean_proximity, dimension=2) 
```

Each of the five ocean_proximity categories will now be represented as a 2D vector. These vectors are stored in an embedding matrix with one row per category, and one column per embedding dimension, so in this example it is a $5 { \times } 2$ matrix. When an embedding column is given a category index as input (say, 3, which corresponds to the category "NEAR BAY"), it just performs a lookup in the embedding matrix and returns the corresponding row (say, [0.331, 0.190]). Unfortunately, the embedding matrix can be quite large, especially when you have a large vocabulary: if this is the case, the model can only learn good representations for the categories for which it has sufficient training data. To reduce the size of the embedding matrix, you can of course try lowering the dimension hyperparameter, but if you reduce this parameter too much, the representations may not be as good. Another option is to reduce the vocabulary size (e.g., if you are dealing with text, you can try dropping the rare words from the vocabulary, and replace them all with a token like "<unknown>" or "<UNK>"). If you are using hash buckets, you can also try reducing the hash_bucket_size (but not too much, or else you will get collisions).

![](images/66cc4d9ff3f0bd7fe236f6434fea0f3f6a49dc9e25cbbe569cdec3f0678c9bb6.jpg)

If there are no pretrained embeddings that you can reuse for the task you are trying to tackle, and if you do not have enough train‐ ing data to learn them, then you can try to learn them on some auxiliary task for which it is easier to obtain plenty of training data. After that, you can reuse the trained embeddings for your main task.

# Using Feature Columns for Parsing

Let’s suppose you have created feature columns for each of your input features, as well as for the target. What can you do with them? Well, for one you can pass them to the make_parse_example_spec() function to generate feature descriptions (so you don’t have to do it manually, as we did earlier):

```python
columns = [bucketized_age, ..., median_house_value] # all features + target feature descriptions = tf.feature_column.make数据分析_example_spec(columns) 
```

![](images/f56d9cb9f6f9451769094c289b50dcaa88264e22d92ae960bdf54310b79acf2e.jpg)

You don’t always have to create a separate feature column for each and every feature. For example, instead of having 2 numerical fea‐ ture columns, you could choose to have a single 2D column: just set shape=[2] when calling numerical_column().

You can then create a function that parses serialized examples using these feature descriptions, and separates the target column from the input features:

```python
def parse/examples(serialized/examples):
    examples = tf.io.parse_example(serialized/examples, featureDescriptions)
    targets = examples.pop("median_house_value") # separate the targets
    return examples, targets 
```

Next, you can create a TFRecordDataset that will read batches of serialized examples (assuming the TFRecord file contains serialized Example protobufs with the appropri‐ ate features):

```python
batch_size = 32  
dataset = tf.data.TFrecordDataset(['my_data_with_features.tfrecords'])  
dataset = datasetrepeat().shuffle(10000).batch(batch_size).map(parse/examples) 
```

# Using Feature Columns in Your Models

Feature columns can also be used directly in your model, to convert all your input features into a single dense vector which the neural network can then process. For this, all you need to do is add a keras.layers.DenseFeatures layer as the first layer in your model, passing it the list of feature columns (excluding the target column):

```python
columns_without_target = columns[: -1]  
model = keras.models Sequential([  
    keras.layers.DenseFeatures feature.columns=columns_without_target), 
```

keras.layers.Dense(1)   
]}   
model.compile(loss $=$ "mse",optimizer $=$ "sgd",metrics $=$ ["accuracy"]   
steps_per_epoch $\equiv$ len(X_train）//batch_size   
history $=$ model.fit(dataset,steps_per_epoch $\equiv$ steps_per_epoch,epochs $= 5$

The DenseFeatures layer will take care of converting every input feature to a dense representation, and it will also apply any extra transformation we specified, such as scaling the housing_median_age using the normalizer_fn function we provided. You can take a closer look at what the DenseFeatures layer does by calling it directly:

```python
>>> some-columns = [oceanproximity_embedding, bucketized_income]
>>> dense_features = keras.layers(DenseFeatures(some-columns)
>>> dense_features({
...
    "oceanproximity": ["NEAR OCEAN"], ["INLAND"], ["INLAND"]
,
...
    "median_income": [[3.], [7.2], [1.]]
}
...
<tf.Tensor: id=559790, shape=(3, 7), dtype(float32, numpy=
array([[0. , 0. , 1. , 0. , 0. , -0.36277947 , 0.30109018],
[0. , 0. , 0. , 0. , 1. , 0.22548223 , 0.33142096],
[1. , 0. , 0. , 0. , 0. , 0.22548223 , 0.33142096]], dtype(float32)> 
```

In this example, we create a DenseFeatures layer with just two columns, and we call it with some data, in the form of a dictionary of features. In this case, since the bucke tized_income column relies on the median_income column, the dictionary must include the "median_income" key, and similarly since the ocean_proximity_embed column is based on the ocean_proximity column, the dictionary must include the "ocean_proximity" key. Columns are handled in alphabetical order, so first we look at the bucketized income column (its name is the same as the median_income column name, plus "_bucketized"). The incomes 3, 7.2 and 1 get mapped respectively to cat‐ egory 2 (for incomes between 1.5 and 3), category 0 (for incomes below 1.5), and cat‐ egory 4 (for incomes greater than 6). Then these category IDs get one-hot encoded: category 2 gets encoded as $[ \Theta \cdot , ~ \Theta . , ~ 1 . , ~ \Theta . , ~ \Theta . ]$ and so on (note that bucketized columns get one-hot encoded by default, no need to call indicator_column()). Now on to the ocean_proximity_embed column. The "NEAR OCEAN" and "INLAND" cate‐ gories just get mapped to their respective embeddings (which were initialized ran‐ domly). The resulting tensor is the concatenation of the one-hot vectors and the embeddings.

Now you can feed all kinds of features to a neural network, including numerical fea‐ tures, categorical features, and even text (by splitting the text into words, then using word embedding)! However, performing all the preprocessing on the fly can slow down training. Let’s see how this can be improved.

# TF Transform

If preprocessing is computationally expensive, then handling it before training rather than on the fly may give you a significant speedup: the data will be preprocessed just once per instance before training, rather than once per instance and per epoch during training. Tools like Apache Beam let you run efficient data processing pipelines over large amounts of data, even distributed across multiple servers, so why not use it to preprocess all the training data? This works great and indeed can speed up training, but there is one problem: once your model is trained, suppose you want to deploy it to a mobile app: you will need to write some code in your app to take care of prepro‐ cessing the data before it is fed to the model. And suppose you also want to deploy the model to TensorFlow.js so it runs in a web browser? Once again, you will need to write some preprocessing code. This can become a maintenance nightmare: when‐ ever you want to change the preprocessing logic, you will need to update your Apache Beam code, your mobile app code and your Javascript code. It is not only time con‐ suming, but also error prone: you may end up with subtle differences between the preprocessing operations performed before training and the ones performed in your app or in the browser. This training/serving skew will lead to bugs or degraded perfor‐ mance.

One improvement would be to take the trained model (trained on data that was pre‐ processed by your Apache Beam code), and before deploying it to your app or the browser, add an extra input layer to take care of preprocessing on the fly (either by writing a custom layer or by using a DenseFeatures layer). That’s definitely better, since now you just have two versions of your preprocessing code: the Apache Beam code and the preprocessing layer’s code.

But what if you could define your preprocessing operations just once? This is what TF Transform was designed for. It is part of TensorFlow Extended (TFX), an end-toend platform for productionizing TensorFlow models. First, to use a TFX component, such as TF Transform, you must install it, it does not come bundled with TensorFlow. You define your preprocessing function just once (in Python), by using TF Transform functions for scaling, bucketizing, crossing features, and more. You can also use any TensorFlow operation you need. Here is what this preprocessing function might look like if we just had two features:

import tensorflow_transform as tfet   
def preprocess(inputs): # inputs is a batch of input features median_age $=$ inputs["housing_median_age"] ocean_proximity $=$ inputs["ocean_proximity"] standardized_age $=$ tft.score_to_z_score(median_age-tft.mean(median_age)) ocean_proximity_id $=$ tft.Compute_and_apply_vocabulary(ocean_proximity) return { "standardized_median_age": standardized_age,

```txt
"ocean_proximity_id": ocean_proximity_id 
```

Next, TF Transform lets you apply this preprocess() function to the whole training set using Apache Beam (it provides an AnalyzeAndTransformDataset class that you can use for this purpose in your Apache Beam pipeline). In the process, it will also compute all the necessary statistics over the whole training set: in this example, the mean and standard deviation of the housing_median_age feature, and the vocabulary for the ocean_proximity feature. The components that compute these statistics are called analyzers.

Importantly, TF Transform will also generate an equivalent TensorFlow Function that you can plug into the model you deploy. This TF Function contains all the necessary statistics computed by Apache Beam (the mean, standard deviation, and vocabulary), simply included as constants.

![](images/6d1c90fe4603255589ad2fc8dfcb2ce334f03936c913735e01e2a7710c3abc78.jpg)

At the time of this writing, TF Transform only supports Tensor‐ Flow 1. Moreover, Apache Beam only has partial support for Python 3. That said, both these limitations will likely be fixed by the time your read this.

With the Data API, TFRecords, the Features API and TF Transform, you can build highly scalable input pipelines for training, and also benefit from fast and portable data preprocessing in production.

But what if you just wanted to use a standard dataset? Well in that case, things are much simpler: just use TFDS!

# The TensorFlow Datasets (TFDS) Project

The TensorFlow Datasets project makes it trivial to download common datasets, from small ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will need quite a bit of disk space!). The list includes image datasets, text datasets (includ‐ ing translation datasets), audio and video datasets, and more. You can visit https:// homl.info/tfds to view the full list, along with a description of each dataset.

TFDS is not bundled with TensorFlow, so you need to install the tensorflowdatasets library (e.g., using pip). Then all you need to do is call the tfds.load() function, and it will download the data you want (unless it was already downloaded earlier), and return the data as a dictionary of Datasets (typically one for training,

and one for testing, but this depends on the dataset you choose). For example, let’s download MNIST:

import tensorflow_datasets as tfds

dataset $=$ tfds.load(name $\equiv$ "mnist") mnist_train, mnist_test $=$ dataset["train"], dataset["test"]

You can then apply any transformation you want (typically repeating, batching and prefetching), and you’re ready to train your model. Here is a simple example:

mnist_train = mnist_trainrepeat(5).batch(32).prefetch(1)   
for item in mnist_train: images $=$ item["image"] labels $=$ item["label"] [...]

![](images/d9918455d95122f29f1ebcd6eb5873196a4116b5f9dbaeb5531c2e4c36599ee9.jpg)

In general, load() returns a shuffled training set, so there’s no need to shuffle it some more.

Note that each item in the dataset is a dictionary containing both the features and the labels. But Keras expects each item to be a tuple containing 2 elements (again, the fea‐ tures and the labels). You could transform the dataset using the map() method, like this:

```python
mnist_train = mnist_trainrepeat(5).batch(32)  
mnist_train = mnist_train.map(lambda items: (items["image"], items["label']))  
mnist_train = mnist_train prefetch(1) 
```

Or you can just ask the load() function to do this for you by setting as_super vised=True (obviously this works only for labeled datasets). You can also specify the batch size if you want. Then the dataset can be passed directly to your tf.keras model:

dataset $=$ tfds.load(name $\equiv$ "mnist",batch_size=32,as supervised=True) mnist_train $=$ dataset["train"].repeat().prefetch(1) model $=$ keras.models Sequential([...]) model.compile(loss $\equiv$ "sparse_categorical CROSSentropy",optimizer $\equiv$ "sgd") model.fit(mnist_train，steps_per_epoch $= 60000 / / 32$ ,epochs $= 5$

This was quite a technical chapter, and you may feel that it is a bit far from the abstract beauty of neural networks, but the fact is deep learning often involves large amounts of data, and knowing how to load, parse and preprocess it efficiently is a crucial skill to have. In the next chapter, we will look at Convolutional Neural Net‐ works, which are among the most successful neural net architectures for image pro‐ cessing, and many other applications.

# Deep Computer Vision Using Convolutional Neural Networks

![](images/693dc773d024aa7ceb844bad61d7e3ba68e9ede0469cd46656ebc7824da22399.jpg)

With Early Release ebooks, you get books in their earliest form— the author’s raw and unedited content as he or she writes—so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 14 in the final release of the book.

Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐ parov back in 1996, it wasn’t until fairly recently that computers were able to reliably perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these tasks so effortless to us humans? The answer lies in the fact that perception largely takes place outside the realm of our consciousness, within specialized visual, auditory, and other sensory modules in our brains. By the time sensory information reaches our consciousness, it is already adorned with high-level features; for example, when you look at a picture of a cute puppy, you cannot choose not to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐ ognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective experience: perception is not trivial at all, and to understand it we must look at how the sensory modules work.

Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex, and they have been used in image recognition since the 1980s. In the last few years, thanks to the increase in computational power, the amount of available training data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐ aged to achieve superhuman performance on some complex visual tasks. They power image search services, self-driving cars, automatic video classification systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful

at many other tasks, such as voice recognition or natural language processing (NLP); however, we will focus on visual applications for now.

In this chapter we will present where CNNs came from, what their building blocks look like, and how to implement them using TensorFlow and Keras. Then we will dis‐ cuss some of the best CNN architectures, and discuss other visual tasks, including object detection (classifying multiple objects in an image and placing bounding boxes around them) and semantic segmentation (classifying each pixel according to the class of the object it belongs to).

# The Architecture of the Visual Cortex

David H. Hubel and Torsten Wiesel performed a series of experiments on cats in $1 9 5 8 ^ { 1 }$ and $1 9 5 9 ^ { 2 }$ (and a few years later on monkeys3), giving crucial insights on the structure of the visual cortex (the authors received the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular, they showed that many neurons in the visual cortex have a small local receptive eld, meaning they react only to visual stimuli located in a limited region of the visual field (see Figure 14-1, in which the local receptive fields of five neurons are represented by dashed circles). The receptive fields of different neurons may overlap, and together they tile the whole visual field. Moreover, the authors showed that some neurons react only to images of horizontal lines, while others react only to lines with different orientations (two neurons may have the same receptive field but react to different line orientations). They also noticed that some neurons have larger receptive fields, and they react to more com‐ plex patterns that are combinations of the lower-level patterns. These observations led to the idea that the higher-level neurons are based on the outputs of neighboring lower-level neurons (in Figure 14-1, notice that each neuron is connected only to a few neurons from the previous layer). This powerful architecture is able to detect all sorts of complex patterns in any area of the visual field.

![](images/90f46bb6013d32829e55d59d964129f2e6fbffd942087bfd7c664d10b7c0ea64.jpg)  
Figure 14-1. Local receptive elds in the visual cortex

These studies of the visual cortex inspired the neocognitron, introduced in 1980,4 which gradually evolved into what we now call convolutional neural networks. An important milestone was a 1998 paper5 by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner, which introduced the famous LeNet-5 architecture, widely used to recognize handwritten check numbers. This architecture has some building blocks that you already know, such as fully connected layers and sigmoid activation func‐ tions, but it also introduces two new building blocks: convolutional layers and pooling layers. Let’s look at them now.

![](images/c69866c999bd181208ebffa4085cb8234d749ab4877bb58ebcdf4ddd02dc39c6.jpg)

Why not simply use a regular deep neural network with fully con‐ nected layers for image recognition tasks? Unfortunately, although this works fine for small images (e.g., MNIST), it breaks down for larger images because of the huge number of parameters it requires. For example, a $1 0 0 \times 1 0 0$ image has 10,000 pixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of information transmitted to the next layer), this means a total of 10 million connections. And that’s just the first layer. CNNs solve this problem using partially connected layers and weight sharing.

# Convolutional Layer

The most important building block of a CNN is the convolutional layer:6 neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 14-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition.

![](images/d00382b761424ad5e0eb653300acc80347969030c67e7cc8e5464fd39ed35aa9.jpg)  
Figure 14-2. CNN layers with rectangular local receptive elds

![](images/d74d35ba47a27fc58867233ec3d5d4cdb6da6eaecd20c332632b61eb72cc096f.jpg)

Until now, all multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images to 1D before feeding them to the neural network. Now each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs.

A neuron located in row i, column $j$ of a given layer is connected to the outputs of the neurons in the previous layer located in rows $i$ to $i + f _ { h } - 1$ , columns $j$ to $j + f _ { w } - 1$ , where $f _ { h }$ and $f _ { w }$ are the height and width of the receptive field (see Figure 14-3). In order for a layer to have the same height and width as the previous layer, it is com‐

mon to add zeros around the inputs, as shown in the diagram. This is called zero pad‐ ding.

![](images/5fbe3db8b54fff38efe0d537f46481f858ee33d1dd51b9da19502815939839b1.jpg)  
Figure 14-3. Connections between layers and zero padding

It is also possible to connect a large input layer to a much smaller layer by spacing out the receptive fields, as shown in Figure 14-4. The shift from one receptive field to the next is called the stride. In the diagram, a $5 \times 7$ input layer (plus zero padding) is con‐ nected to a $3 \times 4$ layer, using $3 \times 3$ receptive fields and a stride of 2 (in this example the stride is the same in both directions, but it does not have to be so). A neuron loca‐ ted in row i, column $j$ in the upper layer is connected to the outputs of the neurons in the previous layer located in rows $i \times s _ { h }$ to $i \times s _ { h } + f _ { h } - 1$ , columns $j \times s _ { w }$ to $j \times s _ { w } + f _ { w } -$ 1, where $s _ { h }$ and $s _ { w }$ are the vertical and horizontal strides.

![](images/1402ec90ae8309abf682a2e7d71e4f51d9f23f4a7ba28c867528fbd5d3f283a9.jpg)  
Figure 14-4. Reducing dimensionality using a stride of 2

# Filters

A neuron’s weights can be represented as a small image the size of the receptive field. For example, Figure 14-5 shows two possible sets of weights, called lters (or convolu‐ tion kernels). The first one is represented as a black square with a vertical white line in the middle (it is a $7 \times 7$ matrix full of 0s except for the central column, which is full of 1s); neurons using these weights will ignore everything in their receptive field except for the central vertical line (since all inputs will get multiplied by 0, except for the ones located in the central vertical line). The second filter is a black square with a horizontal white line in the middle. Once again, neurons using these weights will ignore everything in their receptive field except for the central horizontal line.

Now if all neurons in a layer use the same vertical line filter (and the same bias term), and you feed the network the input image shown in Figure 14-5 (bottom image), the layer will output the top-left image. Notice that the vertical white lines get enhanced while the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐ rons use the same horizontal line filter; notice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer full of neurons using the same filter outputs a feature map, which highlights the areas in an image that activate the filter the most. Of course you do not have to define the filters manually: instead, dur‐ ing training the convolutional layer will automatically learn the most useful filters for its task, and the layers above will learn to combine them into more complex patterns.

![](images/8a168375fdbeaad15bcc3c9d9162239142b78a43c46dd44d72305daf05bdbfed.jpg)  
Figure 14-5. Applying two dierent lters to get two feature maps

# Stacking Multiple Feature Maps

Up to now, for simplicity, I have represented the output of each convolutional layer as a thin 2D layer, but in reality a convolutional layer has multiple filters (you decide how many), and it outputs one feature map per filter, so it is more accurately repre‐ sented in 3D (see Figure 14-6). To do so, it has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same weights and bias term). However, neurons in different feature maps use differ‐ ent parameters. A neuron’s receptive field is the same as described earlier, but it extends across all the previous layers’ feature maps. In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs.

![](images/aae2802e75fca6c7f9263550b52e4c9da05b6bb8b9400ec4d31596bdff9c8c47.jpg)

The fact that all neurons in a feature map share the same parame‐ ters dramatically reduces the number of parameters in the model. Moreover, once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location.

Moreover, input images are also composed of multiple sublayers: one per color chan‐ nel. There are typically three: red, green, and blue (RGB). Grayscale images have just

one channel, but some images may have much more—for example, satellite images that capture extra light frequencies (such as infrared).

![](images/e0cac52d86770ea288d78e0378a7d24904a6133bbac6308da28da9c61bc16b18.jpg)  
Figure 14-6. Convolution layers with multiple feature maps, and images with three color channels

Specifically, a neuron located in row i, column $j$ of the feature map $k$ in a given convo‐ lutional layer $l$ is connected to the outputs of the neurons in the previous layer $l - 1$ , located in rows $i \times s _ { h }$ to $i \times s _ { h } + f _ { h } - 1$ and columns $j \times s _ { w }$ to $j \times s _ { w } + f _ { w } - 1 $ , across all feature maps (in layer l – 1). Note that all neurons located in the same row $i$ and col‐ umn $j$ but in different feature maps are connected to the outputs of the exact same neurons in the previous layer.

Equation 14-1 summarizes the preceding explanations in one big mathematical equa‐ tion: it shows how to compute the output of a given neuron in a convolutional layer.

It is a bit ugly due to all the different indices, but all it does is calculate the weighted sum of all the inputs, plus the bias term.

Equation 14-1. Computing the output of a neuron in a convolutional layer

$$
z _ {i, j, k} = b _ {k} + \sum_ {u = 0} ^ {f _ {h} - 1} \sum_ {\nu = 0} ^ {f _ {w} - 1} \sum_ {k ^ {\prime} = 0} ^ {f _ {n ^ {\prime}} - 1} x _ {i ^ {\prime}, j ^ {\prime}, k ^ {\prime}} \cdot w _ {u, \nu , k ^ {\prime}, k} \quad \text {w i t h} \left\{ \begin{array}{l} i ^ {\prime} = i \times s _ {h} + u \\ j ^ {\prime} = j \times s _ {w} + \nu \end{array} \right.
$$

• $z _ { i , j , k }$ is the output of the neuron located in row i, column $j$ in feature map $k$ of the convolutional layer (layer l).   
• As explained earlier, $s _ { h }$ and $s _ { w }$ are the vertical and horizontal strides, $f _ { h }$ and $f _ { w }$ are the height and width of the receptive field, and $f _ { n ^ { \prime } }$ is the number of feature maps in the previous layer (layer l – 1).   
• $x _ { i ^ { \prime } , j ^ { \prime } , k ^ { \prime } }$ is the output of the neuron located in layer l – 1, row $i ^ { \prime }$ , column $j ^ { \prime }$ , feature map $k ^ { \prime }$ (or channel $k ^ { \prime }$ if the previous layer is the input layer).   
• $b _ { k }$ is the bias term for feature map $k$ (in layer l). You can think of it as a knob that tweaks the overall brightness of the feature map $k$ .   
• $\boldsymbol { w } _ { u , \nu , k ^ { \prime } , k }$ is the connection weight between any neuron in feature map $k$ of the layer l and its input located at row $u$ , column $\nu$ (relative to the neuron’s receptive field), and feature map $k ^ { \prime }$ .

# TensorFlow Implementation

In TensorFlow, each input image is typically represented as a 3D tensor of shape [height, width, channels]. A mini-batch is represented as a 4D tensor of shape [mini-batch size, height, width, channels]. The weights of a convolutional layer are represented as a 4D tensor of shape $[ f _ { h } , f _ { w } , f _ { n ^ { \prime } } , f _ { n } ]$ . The bias terms of a convo‐ lutional layer are simply represented as a 1D tensor of shape $[ f _ { n } ]$ .

Let’s look at a simple example. The following code loads two sample images, using Scikit-Learn’s load_sample_images() (which loads two color images, one of a Chi‐ nese temple, and the other of a flower). The pixel intensities (for each color channel) is represented as a byte from 0 to 255, so we scale these features simply by dividing by 255, to get floats ranging from 0 to 1. Then we create two $7 \times 7$ filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle), and we apply them to both images using the tf.nn.conv2d() function, which is part of TensorFlow’s low-level Deep Learning API. In this example, we use zero padding (padding="SAME") and a stride of 2. Finally, we plot one of the resulting feature maps (similar to the top-right image in Figure 14-5).

```python
from sklearn.datasets import load_sample_image
# Load sample images
china = load_sample_image("china.jpg") / 255
flower = load_sample_image("flower.jpg") / 255
images = np.array([china, flower])
batch_size, height, width, channels = images.shape
# Create 2 filters
filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
filters[:, 3, :, 0] = 1 # vertical line
filters[3, :, :, 1] = 1 # horizontal line
outputs = tf.nn.conv2d/images, filters, strides=1, padding="SAME")
plt.imshow(output[0, :, :, 1], cmap="gray") # plot 1st image's 2nd feature map
plt.show() 
```

Most of this code is self-explanatory, but the tf.nn.conv2d() line deserves a bit of explanation:

• images is the input mini-batch (a 4D tensor, as explained earlier).   
• filters is the set of filters to apply (also a 4D tensor, as explained earlier).   
• strides is equal to 1, but it could also be a 1D array with 4 elements, where the two central elements are the vertical and horizontal strides $\cdot { \boldsymbol { s } } _ { h }$ and $s _ { w } ,$ ). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).   
• padding must be either "VALID" or "SAME":

— If set to "VALID", the convolutional layer does not use zero padding, and may ignore some rows and columns at the bottom and right of the input image, depending on the stride, as shown in Figure 14-7 (for simplicity, only the hor‐ izontal dimension is shown here, but of course the same logic applies to the vertical dimension).   
— If set to "SAME", the convolutional layer uses zero padding if necessary. In this case, the number of output neurons is equal to the number of input neurons divided by the stride, rounded up (in this example, $1 3 / 5 = 2 . 6$ , rounded up to 3). Then zeros are added as evenly as possible around the inputs.

![](images/ffdf2c7f0e985816e50c6c02fbbd1d86f84cbd4ccf27808d1aca3305281fb08d.jpg)  
Figure 14-7. Padding options—input width: 13, lter width: 6, stride: 5

In this example, we manually defined the filters, but in a real CNN you would nor‐ mally define filters as trainable variables, so the neural net can learn which filters work best, as explained earlier. Instead of manually creating the variables, however, you can simply use the keras.layers.Conv2D layer:

```python
conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding="SAME", activation="relu") 
```

This code creates a Conv2D layer with 32 filters, each $3 \times 3$ , using a stride of 1 (both horizontally and vertically), SAME padding, and applying the ReLU activation func‐ tion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐ meters: you must choose the number of filters, their height and width, the strides, and the padding type. As always, you can use cross-validation to find the right hyperpara‐ meter values, but this is very time-consuming. We will discuss common CNN archi‐ tectures later, to give you some idea of what hyperparameter values work best in practice.

# Memory Requirements

Another problem with CNNs is that the convolutional layers require a huge amount of RAM. This is especially true during training, because the reverse pass of backpro‐ pagation requires all the intermediate values computed during the forward pass.

For example, consider a convolutional layer with $5 \times 5$ filters, outputting 200 feature maps of size $1 5 0 \times 1 0 0$ , with stride 1 and SAME padding. If the input is a $1 5 0 \times 1 0 0$

RGB image (three channels), then the number of parameters is $( 5 \times 5 \times 3 + 1 ) \times 2 0 0$ $= 1 5 { , } 2 0 0$ (the $+ 1$ corresponds to the bias terms), which is fairly small compared to a fully connected layer.7 However, each of the 200 feature maps contains $1 5 0 \times 1 0 0$ neu‐ rons, and each of these neurons needs to compute a weighted sum of its $5 \times 5 \times 3 =$ 75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐ nected layer, but still quite computationally intensive. Moreover, if the feature maps are represented using 32-bit floats, then the convolutional layer’s output will occupy $2 0 0 \times 1 5 0 \times 1 0 0 \times 3 2 = 9 6$ million bits (12 MB) of RAM.8 And that’s just for one instance! If a training batch contains 100 instances, then this layer will use up 1.2 GB of RAM!

During inference (i.e., when making a prediction for a new instance) the RAM occu‐ pied by one layer can be released as soon as the next layer has been computed, so you only need as much RAM as required by two consecutive layers. But during training everything computed during the forward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at least) the total amount of RAM required by all layers.

![](images/9b83105cb627715ac8ac389eea0be3d9d193a4387bae9374b06c73a20c7ff603.jpg)

If training crashes because of an out-of-memory error, you can try reducing the mini-batch size. Alternatively, you can try reducing dimensionality using a stride, or removing a few layers. Or you can try using 16-bit floats instead of 32-bit floats. Or you could distrib‐ ute the CNN across multiple devices.

Now let’s look at the second common building block of CNNs: the pooling layer.

# Pooling Layer

Once you understand how convolutional layers work, the pooling layers are quite easy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting).

Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the inputs using an aggregation function such as the max or mean. Figure 14-8 shows a max pooling layer, which is the most common type of pooling layer. In this example,

we use a $2 \times 2$ _pooling kernel_9, with a stride of 2, and no padding. Only the max input value in each receptive field makes it to the next layer, while the other inputs are dropped. For example, in the lower left receptive field in Figure 14-8, the input values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the stride of 2, the output image has half the height and half the width of the input image (rounded down since we use no padding).

![](images/e90ca5a97d6e1bc2b9948b467a025328bb4ef1e1aecccbae355e7a45c4e06753.jpg)  
Figure 14-8. Max pooling layer $2 \times 2$ pooling kernel, stride 2, no padding)

![](images/40bd753ec64fba2b2318a8e0d0f1fe797fc8e9ca55c91de402e3283fb897bf80.jpg)

A pooling layer typically works on every input channel independ‐ ently, so the output depth is the same as the input depth.

Other than reducing computations, memory usage and the number of parameters, a max pooling layer also introduces some level of invariance to small translations, as shown in Figure 14-9. Here we assume that the bright pixels have a lower value than dark pixels, and we consider 3 images (A, B, C) going through a max pooling layer with a $2 \times 2$ kernel and stride 2. Images B and C are the same as image A, but shifted by one and two pixels to the right. As you can see, the outputs of the max pooling layer for images A and B are identical. This is what translation invariance means. However, for image C, the output is different: it is shifted by one pixel to the right (but there is still $7 5 \%$ invariance). By inserting a max pooling layer every few layers in a CNN, it is possible to get some level of translation invariance at a larger scale. Moreover, max pooling also offers a small amount of rotational invariance and a slight scale invariance. Such invariance (even if it is limited) can be useful in cases where the prediction should not depend on these details, such as in classification tasks.

![](images/a32b537b9aeadcd58320942df800901bd130620812d3c0ee09e7bd8e2a0304c2.jpg)  
Figure 14-9. Invariance to small translations

But max pooling has some downsides: firstly, it is obviously very destructive: even with a tiny $2 \times 2$ kernel and a stride of 2, the output will be two times smaller in both directions (so its area will be four times smaller), simply dropping $7 5 \%$ of the input values. And in some applications, invariance is not desirable, for example for seman‐ tic segmentation: this is the task of classifying each pixel in an image depending on the object that pixel belongs to: obviously, if the input image is translated by 1 pixel to the right, the output should also be translated by 1 pixel to the right. The goal in this case is equivariance, not invariance: a small change to the inputs should lead to a corre‐ sponding small change in the output.

# TensorFlow Implementation

Implementing a max pooling layer in TensorFlow is quite easy. The following code creates a max pooling layer using a $2 \times 2$ kernel. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses VALID padding (i.e., no padding at all):

$$
\max _ {\text {p o o l}} = \text {k e r a s . l a y e r s . M a x P o o l 2 D (p o o l s i z e} = 2)
$$

To create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you might expect, it works exactly like a max pooling layer, except it computes the mean rather than the max. Average pooling layers used to be very popular, but people

mostly use max pooling layers now, as they generally perform better. This may seem surprising, since computing the mean generally loses less information than comput‐ ing the max. But on the other hand, max pooling preserves only the strongest feature, getting rid of all the meaningless ones, so the next layers get a cleaner signal to work with. Moreover, max pooling offers stronger translation invariance than average pooling.

Note that max pooling and average pooling can be performed along the depth dimen‐ sion rather than the spatial dimensions, although this is not as common. This can allow the CNN to learn to be invariant to various features. For example, it could learn multiple filters, each detecting a different rotation of the same pattern, such as handwritten digits (see Figure 14-10), and the depth-wise max pooling layer would ensure that the output is the same regardless of the rotation. The CNN could similarly learn to be invariant to anything else: thickness, brightness, skew, color, and so on.

![](images/f29b6da709a80b5546d9f02871e812f5b5f27b64dc4bb5bbbcaa1d3807cbee15.jpg)  
Figure 14-10. Depth-wise max pooling can help the CNN learn any invariance

Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level Deep Learning API does: just use the tf.nn.max_pool() function, and specify the kernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐ cates that the kernel size and stride along the batch, height and width dimensions shoud be 1. The last value should be whatever kernel size and stride you want along the depth dimension, for example 3 (this must be a divisor of the input depth; for example, it will not work if the previous layer outputs 20 feature maps, since 20 is not a multiple of 3):

```python
output = tf.nn.max_pool/images,  
    ksize=(1, 1, 1, 3),  
    strides=(1, 1, 1, 3),  
    padding="VALID") 
```

If you want to include this as a layer in your Keras models, you can simply wrap it in a Lambda layer (or create a custom Keras layer):

```python
depth_pool = keras.layers Lambda( lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding="VALID")) 
```

One last type of pooling layer that you will often see in modern architectures is the global average pooling layer. It works very differently: all it does is compute the mean of each entire feature map (it’s like an average pooling layer using a pooling kernel with the same spatial dimensions as the inputs). This means that it just outputs a sin‐ gle number per feature map and per instance. Although this is of course extremely destructive (most of the information in the feature map is lost), it can be useful as the output layer, as we will see later in this chapter. To create such a layer, simply use the keras.layers.GlobalAvgPool2D class:

```txt
global_avg_pool = keras.layers.GlobalAvgPool2D() 
```

It is actually equivalent to this simple Lamba layer, which computes the mean over the spatial dimensions (height and width):

```python
global_avg_pool = keras.layers Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2])) 
```

Now you know all the building blocks to create a convolutional neural network. Let’s see how to assemble them.

# CNN Architectures

Typical CNN architectures stack a few convolutional layers (each one generally fol‐ lowed by a ReLU layer), then a pooling layer, then another few convolutional layers $\mathrm { ( + R e L U ) }$ , then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps) thanks to the convolutional layers (see Figure 14-11). At the top of the stack, a regular feedforward neural network is added, composed of a few

fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).

![](images/3450780d7f34601d8dbd2b09d86a509dc9fc17e0e3c11f0f5a8304a07251a004.jpg)  
Figure 14-11. Typical CNN architecture

![](images/33582c1b58620d3e489f7ee6daccb8d8cd8b526c1732ccdfbf6af4266b51072f.jpg)

A common mistake is to use convolution kernels that are too large. For example, instead of using a convolutional layer with a $5 \times 5$ kernel, it is generally preferable to stack two layers with $3 \times 3$ ker‐ nels: it will use less parameters and require less computations, and it will usually perform better. One exception to this recommenda‐ tion is for the first convolutional layer: it can typically have a large kernel (e.g., $5 \times 5$ ), usually with stride of 2 or more: this will reduce the spatial dimension of the image without losing too much infor‐ mation, and since the input image only has 3 channels in general, it will not be too costly.

Here is how you can implement a simple CNN to tackle the fashion MNIST dataset (introduced in Chapter 10):

from functools import partial   
DefaultConv2D $=$ partial(keras.layers.Conv2D, kernel_size=3，activation $\equiv$ 'relu'，padding $\equiv$ "SAME")   
model $=$ keras.models Sequential([ DefaultConv2D(filters=64，kernel_size=7，input_shape=[28，28，1]）， keras.layers.MaxPooling2D(pool_size=2)， DefaultConv2D(filters=128)， DefaultConv2D(filters=128)， keras.layers.MaxPooling2D(pool_size=2)， DefaultConv2D(filters=256)， DefaultConv2D(filters=256)， keras.layers.MaxPooling2D(pool_size=2)， keras.layers.Flatten()， keras.layers.Dense(units $\coloneqq$ 128，activation $\equiv$ 'relu')， keras.layers.Dropout(0.5)， keras.layers.Dense(units $\coloneqq$ 64，activation $\equiv$ 'relu')， keras.layers.Dropout(0.5)， keras.layers.Dense(units $\coloneqq$ 10，activation $\equiv$ 'softmax')， ]）

• In this code, we start by using the partial() function to define a thin wrapper around the Conv2D class, called DefaultConv2D: it simply avoids having to repeat the same hyperparameter values over and over again.   
• The first layer uses a large kernel size, but no stride because the input images are not very large. It also sets input_shape $^ { 1 = }$ [28, 28, 1], which means the images are $2 8 \times 2 8$ pixels, with a single color channel (i.e., grayscale).   
• Next, we have a max pooling layer, which divides each spatial dimension by a fac‐ tor of two (since pool_size=2).   
• Then we repeat the same structure twice: two convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several times (the number of repetitions is a hyperparameter you can tune).   
• Note that the number of filters grows as we climb up the CNN towards the out‐ put layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since the number of low level features is often fairly low (e.g., small circles, horizontal lines, etc.), but there are many different ways to combine them into higher level features. It is a common practice to double the number of filters after each pool‐ ing layer: since a pooling layer divides each spatial dimension by a factor of 2, we can afford doubling the number of feature maps in the next layer, without fear of exploding the number of parameters, memory usage, or computational load.   
• Next is the fully connected network, composed of 2 hidden dense layers and a dense output layer. Note that we must flatten its inputs, since a dense network expects a 1D array of features for each instance. We also add two dropout layers, with a dropout rate of $5 0 \%$ each, to reduce overfitting.

This CNN reaches over $9 2 \%$ accuracy on the test set. It’s not the state of the art, but it is pretty good, and clearly much better than what we achieved with dense networks in Chapter 10.

Over the years, variants of this fundamental architecture have been developed, lead‐ ing to amazing advances in the field. A good measure of this progress is the error rate in competitions such as the ILSVRC ImageNet challenge. In this competition the top-5 error rate for image classification fell from over $2 6 \%$ to less than $2 . 3 \%$ in just six years. The top-five error rate is the number of test images for which the system’s top 5 predictions did not include the correct answer. The images are large (256 pixels high) and there are 1,000 classes, some of which are really subtle (try distinguishing 120 dog breeds). Looking at the evolution of the winning entries is a good way to under‐ stand how CNNs work.

We will first look at the classical LeNet-5 architecture (1998), then three of the win‐ ners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet (2015).

# LeNet-5

The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As mentioned earlier, it was created by Yann LeCun in 1998 and widely used for hand‐ written digit recognition (MNIST). It is composed of the layers shown in Table 14-1.

Table 14-1. LeNet-5 architecture   

<table><tr><td>Layer</td><td>Type</td><td>Maps</td><td>Size</td><td>Kernel size</td><td>Stride</td><td>Activation</td></tr><tr><td>Out</td><td>Fully Connected</td><td>-</td><td>10</td><td>-</td><td>-</td><td>RBF</td></tr><tr><td>F6</td><td>Fully Connected</td><td>-</td><td>84</td><td>-</td><td>-</td><td>tanh</td></tr><tr><td>C5</td><td>Convolution</td><td>120</td><td>1 × 1</td><td>5 × 5</td><td>1</td><td>tanh</td></tr><tr><td>S4</td><td>Avg Pooling</td><td>16</td><td>5 × 5</td><td>2 × 2</td><td>2</td><td>tanh</td></tr><tr><td>C3</td><td>Convolution</td><td>16</td><td>10 × 10</td><td>5 × 5</td><td>1</td><td>tanh</td></tr><tr><td>S2</td><td>Avg Pooling</td><td>6</td><td>14 × 14</td><td>2 × 2</td><td>2</td><td>tanh</td></tr><tr><td>C1</td><td>Convolution</td><td>6</td><td>28 × 28</td><td>5 × 5</td><td>1</td><td>tanh</td></tr><tr><td>In</td><td>Input</td><td>1</td><td>32 × 32</td><td>-</td><td>-</td><td>-</td></tr></table>

There are a few extra details to be noted:

• MNIST images are $2 8 \times 2 8$ pixels, but they are zero-padded to $3 2 \times 3 2$ pixels and normalized before being fed to the network. The rest of the network does not use any padding, which is why the size keeps shrinking as the image progresses through the network.   
• The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coeffi‐ cient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function.   
• Most neurons in C3 maps are connected to neurons in only three or four S2 maps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for details.   
• The output layer is a bit special: instead of computing the matrix multiplication of the inputs and the weight vector, each neuron outputs the square of the Eucli‐ dian distance between its input vector and its weight vector. Each output meas‐ ures how much the image belongs to a particular digit class. The cross entropy cost function is now preferred, as it penalizes bad predictions much more, pro‐ ducing larger gradients and converging faster.

Yann LeCun’s website (“LENET” section) features great demos of LeNet-5 classifying digits.

# AlexNet

The AlexNet CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a large margin: it achieved $1 7 \%$ top-5 error rate while the second best achieved only $2 6 \% !$ It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it was the first to stack convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. Table 14-2 presents this architecture.

Table 14-2. AlexNet architecture   

<table><tr><td>Layer</td><td>Type</td><td>Maps</td><td>Size</td><td>Kernel size</td><td>Stride</td><td>Padding</td><td>Activation</td></tr><tr><td>Out</td><td>Fully Connected</td><td>-</td><td>1,000</td><td>-</td><td>-</td><td>-</td><td>Softmax</td></tr><tr><td>F9</td><td>Fully Connected</td><td>-</td><td>4,096</td><td>-</td><td>-</td><td>-</td><td>ReLU</td></tr><tr><td>F8</td><td>Fully Connected</td><td>-</td><td>4,096</td><td>-</td><td>-</td><td>-</td><td>ReLU</td></tr><tr><td>C7</td><td>Convolution</td><td>256</td><td>13 × 13</td><td>3 × 3</td><td>1</td><td>SAME</td><td>ReLU</td></tr><tr><td>C6</td><td>Convolution</td><td>384</td><td>13 × 13</td><td>3 × 3</td><td>1</td><td>SAME</td><td>ReLU</td></tr><tr><td>C5</td><td>Convolution</td><td>384</td><td>13 × 13</td><td>3 × 3</td><td>1</td><td>SAME</td><td>ReLU</td></tr><tr><td>S4</td><td>Max Pooling</td><td>256</td><td>13 × 13</td><td>3 × 3</td><td>2</td><td>VALID</td><td>-</td></tr><tr><td>C3</td><td>Convolution</td><td>256</td><td>27 × 27</td><td>5 × 5</td><td>1</td><td>SAME</td><td>ReLU</td></tr><tr><td>S2</td><td>Max Pooling</td><td>96</td><td>27 × 27</td><td>3 × 3</td><td>2</td><td>VALID</td><td>-</td></tr><tr><td>C1</td><td>Convolution</td><td>96</td><td>55 × 55</td><td>11 × 11</td><td>4</td><td>VALID</td><td>ReLU</td></tr><tr><td>In</td><td>Input</td><td>3 (RGB)</td><td>227 × 227</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></table>

To reduce overfitting, the authors used two regularization techniques: first they applied dropout (introduced in Chapter 11) with a $5 0 \%$ dropout rate during training to the outputs of layers F8 and F9. Second, they performed data augmentation by ran‐ domly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.

# Data Augmentation

Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. This reduces overfitting, making this a regularization technique. The generated instances should be as realistic as possible:

ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. Moreover, simply adding white noise will not help; the modifications should be learnable (white noise is not).

For example, you can slightly shift, rotate, and resize every picture in the training set by various amounts and add the resulting pictures to the training set (see Figure 14-12). This forces the model to be more tolerant to variations in the position, orientation, and size of the objects in the pictures. If you want the model to be more tolerant to different lighting conditions, you can similarly generate many images with various contrasts. In general, you can also flip the pictures horizontally (except for text, and other non-symmetrical objects). By combining these transformations you can greatly increase the size of your training set.

![](images/7926b78b9791f7f389245f93eb5e2136c13018af01fc4d306c67fc4e9d510632.jpg)  
Figure 14-12. Generating new training instances from existing ones

AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called local response normalization. The most strongly activated neurons inhibit other neurons located at the same position in neighboring feature maps (such competitive activation has been observed in biological neurons). This encourages different feature maps to specialize, pushing them apart and forcing them

to explore a wider range of features, ultimately improving generalization. Equation 14-2 shows how to apply LRN.

Equation 14-2. Local response normalization

$$
b _ {i} = a _ {i} \left(k + \alpha \sum_ {j = j _ {\mathrm {l o w}}} ^ {j _ {\mathrm {h i g h}}} a _ {j} ^ {2}\right) ^ {- \beta} \quad \text {w i t h} \left\{ \begin{array}{l} j _ {\mathrm {h i g h}} = \min  \left(i + \frac {r}{2}, f _ {n} - 1\right) \\ j _ {\mathrm {l o w}} = \max  \left(0, i - \frac {r}{2}\right) \end{array} \right.
$$

• $b _ { i }$ is the normalized output of the neuron located in feature map i, at some row $u$ and column $\nu$ (note that in this equation we consider only neurons located at this row and column, so $u$ and $\nu$ are not shown).   
• $a _ { i }$ is the activation of that neuron after the ReLU step, but before normalization.   
• $k , \alpha , \beta ,$ , and $r$ are hyperparameters. $k$ is called the bias, and $r$ is called the depth radius.   
• $f _ { n }$ is the number of feature maps.

For example, if $r = 2$ and a neuron has a strong activation, it will inhibit the activation of the neurons located in the feature maps immediately above and below its own.

In AlexNet, the hyperparameters are set as follows: $r = 2$ , $\alpha = 0 . 0 0 0 0 2$ , $\beta = 0 . 7 5$ , and $k$ $= ~ 1$ . This step can be implemented using the tf.nn.local_response_normaliza tion() function (which you can wrap in a Lambda layer if you want to use it in a Keras model).

A variant of AlexNet called ZF Net was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters (number of feature maps, kernel size, stride, etc.).

# GoogLeNet

The GoogLeNet architecture was developed by Christian Szegedy et al. from Google Research,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate below $7 \%$ . This great performance came in large part from the fact that the network was much deeper than previous CNNs (see Figure 14-14). This was made possible by sub-networks called inception modules,13 which allow GoogLeNet to use parameters

much more efficiently than previous architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).

Figure 14-13 shows the architecture of an inception module. The notation $^ { \infty } 3 \times 3 +$ 1(S)” means that the layer uses a $3 \times 3$ kernel, stride 1, and SAME padding. The input signal is first copied and fed to four different layers. All convolutional layers use the ReLU activation function. Note that the second set of convolutional layers uses differ‐ ent kernel sizes $( 1 \times 1 , 3 \times 3$ , and $5 \times 5$ ), allowing them to capture patterns at different scales. Also note that every single layer uses a stride of 1 and SAME padding (even the max pooling layer), so their outputs all have the same height and width as their inputs. This makes it possible to concatenate all the outputs along the depth dimen‐ sion in the final depth concat layer (i.e., stack the feature maps from all four top con‐ volutional layers). This concatenation layer can be implemented in TensorFlow using the tf.concat() operation, with axi $s { = } 3$ (axis 3 is the depth).

![](images/fd3c1143ac50b3b6377f3de17a2392b25ecc01943169e14acbbe290c2e701616.jpg)  
Figure 14-13. Inception module

You may wonder why inception modules have convolutional layers with $1 \times 1$ ker‐ nels. Surely these layers cannot capture any features since they look at only one pixel at a time? In fact, these layers serve three purposes:

• First, although they cannot capture spatial patterns, they can capture patterns along the depth dimension.   
• Second, they are configured to output fewer feature maps than their inputs, so they serve as bottleneck layers, meaning they reduce dimensionality. This cuts the computational cost and the number of parameters, speeding up training and improving generalization.   
• Lastly, each pair of convolutional layers $( [ 1 \times 1 , 3 \times 3 ]$ and $[ 1 \times 1 , 5 \times 5 ] _ { , }$ ) acts like a single, powerful convolutional layer, capable of capturing more complex pat‐ terns. Indeed, instead of sweeping a simple linear classifier across the image (as a

single convolutional layer does), this pair of convolutional layers sweeps a twolayer neural network across the image.

In short, you can think of the whole inception module as a convolutional layer on steroids, able to output feature maps that capture complex patterns at various scales.

![](images/b0f496eb8c15a1724260134e3733f562a68d7978b43d5f1c22f89cca86f82687.jpg)

The number of convolutional kernels for each convolutional layer is a hyperparameter. Unfortunately, this means that you have six more hyperparameters to tweak for every inception layer you add.

Now let’s look at the architecture of the GoogLeNet CNN (see Figure 14-14). The number of feature maps output by each convolutional layer and each pooling layer is shown before the kernel size. The architecture is so deep that it has to be represented in three columns, but GoogLeNet is actually one tall stack, including nine inception modules (the boxes with the spinning tops). The six numbers in the inception mod‐ ules represent the number of feature maps output by each convolutional layer in the module (in the same order as in Figure 14-13). Note that all the convolutional layers use the ReLU activation function.

![](images/d70f1b89e32f539395a0f48687fb3549670f08c2113ed1db2aa1e5b9bad18d9b.jpg)  
Figure 14-14. GoogLeNet architecture

Let’s go through this network:

• The first two layers divide the image’s height and width by 4 (so its area is divided by 16), to reduce the computational load. The first layer uses a large kernel size, so that much of the information is still preserved.   
• Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier).   
• Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional layer.   
• Again, a local response normalization layer ensures that the previous layers cap‐ ture a wide variety of patterns.

• Next a max pooling layer reduces the image height and width by 2, again to speed up computations.   
• Then comes the tall stack of nine inception modules, interleaved with a couple max pooling layers to reduce dimensionality and speed up the net.   
• Next, the global average pooling layer simply outputs the mean of each feature map: this drops any remaining spatial information, which is fine since there was not much spatial information left at that point. Indeed, GoogLeNet input images are typically expected to be $2 2 4 \times 2 2 4$ pixels, so after 5 max pooling layers, each dividing the height and width by 2, the feature maps are down to $7 \times 7$ . More‐ over, it is a classification task, not localization, so it does not matter where the object is. Thanks to the dimensionality reduction brought by this layer, there is no need to have several fully connected layers at the top of the CNN (like in AlexNet), and this considerably reduces the number of parameters in the net‐ work and limits the risk of overfitting.   
• The last layers are self-explanatory: dropout for regularization, then a fully con‐ nected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐ vation function to output estimated class probabilities.

This diagram is slightly simplified: the original GoogLeNet architecture also included two auxiliary classifiers plugged on top of the third and sixth inception modules. They were both composed of one average pooling layer, one convolutional layer, two fully connected layers, and a softmax activation layer. During training, their loss (scaled down by $7 0 \%$ ) was added to the overall loss. The goal was to fight the vanish‐ ing gradients problem and regularize the network. However, it was later shown that their effect was relatively minor.

Several variants of the GoogLeNet architecture were later proposed by Google researchers, including Inception-v3 and Inception-v4, using slightly different incep‐ tion modules, and reaching even better performance.

# VGGNet

The runner up in the ILSVRC 2014 challenge was VGGNet14, developed by K. Simon‐ yan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐ volutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling layer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐ work with 2 hidden layers and the output layer. It used only $3 \times 3$ filters, but many filters.

# ResNet

The ILSVRC 2015 challenge was won using a Residual Network (or ResNet), devel‐ oped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under $3 . 6 \%$ , using an extremely deep CNN composed of 152 layers. It confirmed the general trend: models are getting deeper and deeper, with fewer and fewer parameters. The key to being able to train such a deep network is to use skip connections (also called shortcut connections): the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack. Let’s see why this is useful.

When training a neural network, the goal is to make it model a target function $h ( \mathbf { x } )$ . If you add the input x to the output of the network (i.e., you add a skip connection), then the network will be forced to model $f ( \mathbf { x } ) = h ( \mathbf { x } ) - \mathbf { x }$ rather than $h ( \mathbf { x } )$ . This is called residual learning (see Figure 14-15).

![](images/dfb49419be072b12816de1ab442239e51be494be1c0215a1032682ca369e910b.jpg)

![](images/4b77bfc296074b11ad56591a6448763f583de4b384c34423fc5a206fab5adf9c.jpg)  
Figure 14-15. Residual learning

When you initialize a regular neural network, its weights are close to zero, so the net‐ work just outputs values close to zero. If you add a skip connection, the resulting net‐ work just outputs a copy of its inputs; in other words, it initially models the identity function. If the target function is fairly close to the identity function (which is often the case), this will speed up training considerably.

Moreover, if you add many skip connections, the network can start making progress even if several layers have not started learning yet (see Figure 14-16). Thanks to skip connections, the signal can easily make its way across the whole network. The deep residual network can be seen as a stack of residual units, where each residual unit is a small neural network with a skip connection.

![](images/d03adee261b9e450a12920dbe0b1352d637dac732979b0e514474d623168e7ef.jpg)

![](images/0a3d32f78058efd61e987cab5805fb7579fedd7455502cc09e31b5687cf54d73.jpg)  
Figure 14-16. Regular deep neural network (le) and deep residual network (right)

Now let’s look at ResNet’s architecture (see Figure 14-17). It is actually surprisingly simple. It starts and ends exactly like GoogLeNet (except without a dropout layer), and in between is just a very deep stack of simple residual units. Each residual unit is composed of two convolutional layers (and no pooling layer!), with Batch Normaliza‐ tion (BN) and ReLU activation, using $3 \times 3$ kernels and preserving spatial dimensions (stride 1, SAME padding).

![](images/c41e45290945b315bdae530686a525cd971404dd1487bc455878880a0ff30216.jpg)  
Figure 14-17. ResNet architecture

Note that the number of feature maps is doubled every few residual units, at the same time as their height and width are halved (using a convolutional layer with stride 2). When this happens the inputs cannot be added directly to the outputs of the residual unit since they don’t have the same shape (for example, this problem affects the skip

connection represented by the dashed arrow in Figure 14-17). To solve this problem, the inputs are passed through a $1 \times 1$ convolutional layer with stride 2 and the right number of output feature maps (see Figure 14-18).

![](images/f28786349345dc377f2eb72acc0a5fc11feee80569a42ced9137ddc31598ecb9.jpg)  
Figure 14-18. Skip connection when changing feature map size and depth

ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and the fully connected layer) containing three residual units that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐ ment this architecture later in this chapter.

ResNets deeper than that, such as ResNet-152, use slightly different residual units. Instead of two $3 \times 3$ convolutional layers with (say) 256 feature maps, they use three convolutional layers: first a $1 \times 1$ convolutional layer with just 64 feature maps (4 times less), which acts as a bottleneck layer (as discussed already), then a $3 \times 3$ layer with 64 feature maps, and finally another $1 \times 1$ convolutional layer with 256 feature maps (4 times 64) that restores the original depth. ResNet-152 contains three such RUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.

![](images/bb177254f3ab40b9e701ff8a2c28a34fe74046ec5421bcf9c9fa8ee9bc1343f6.jpg)

Google’s Inception- $\mathrm { \cdot v } 4 ^ { 1 6 }$ architecture merged the ideas of GoogLe‐ Net and ResNet and achieved close to $3 \%$ top-5 error rate on ImageNet classification.

# Xception

Another variant of the GoogLeNet architecture is also worth noting: Xception17 (which stands for Extreme Inception) was proposed in 2016 by François Chollet (the

author of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350 million images and 17,000 classes). Just like Inception-v4, it also merges the ideas of GoogLeNet and ResNet, but it replaces the inception modules with a special type of layer called a depthwise separable convolution (or separable convolution for short18). These layers had been used before in some CNN architectures, but they were not as central as in the Xception architecture. While a regular convolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and crosschannel patterns (e.g., mouth $+ \mathrm { n o s e } + \mathrm { e y e s } = \mathrm { f a c e } )$ $^ +$ , a separable convolutional layer makes the strong assumption that spatial patterns and cross-channel patterns can be modeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part applies a single spatial filter for each input feature map, then the second part looks exclusively for cross-channel patterns—it is just a regular convolutional layer with $1 \times$ 1 filters.

![](images/64cab55fc8a0aa7d7be7a0c65e09a838d3141e93cd7a559c78f715448e7e2bc4.jpg)  
Figure 14-19. Depthwise Separable Convolutional Layer

Since separable convolutional layers only have one spatial filter per input channel, you should avoid using them after layers that have too few channels, such as the input layer (granted, that’s what Figure 14-19 represents, but it is just for illustration pur‐ poses). For this reason, the Xception architecture starts with 2 regular convolutional layers, but then the rest of the architecture uses only separable convolutions (34 in

all), plus a few max pooling layers and the usual final layers (a global average pooling layer, and a dense output layer).

You might wonder why Xception is considered a variant of GoogLeNet, since it con‐ tains no inception module at all? Well, as we discussed earlier, an Inception module contains convolutional layers with $1 \times 1$ filters: these look exclusively for crosschannel patterns. However, the convolution layers that sit on top of them are regular convolutional layers that look both for spatial and cross-channel patterns. So you can think of an Inception module as an intermediate between a regular convolutional layer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐ rable convolutional layer (which considers them separately). In practice, it seems that separable convolutions generally perform better.

![](images/d09a629b89ada0a9f46f7e4ae6ffb881560610e38b542ad41b7f852e4faaa72f.jpg)

Separable convolutions use less parameters, less memory and less computations than regular convolutional layers, and in general they even perform better, so you should consider using them by default (except after layers with few channels).

The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐ versity of Hong Kong. They used an ensemble of many different techniques, includ‐ ing a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error rate below $3 \%$ . Although this result is unquestionably impressive, the complexity of the solution contrasted with the simplicity of ResNets. Moreover, one year later another fairly simple architecture performed even better, as we will see now.

# SENet

The winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-Excitation Network (SENet)20. This architecture extends existing architectures such as inception networks or ResNets, and boosts their performance. This allowed SENet to win the competition with an astonishing $2 . 2 5 \%$ top-5 error rate! The extended ver‐ sions of inception networks and ResNet are called SE-Inception and SE-ResNet respec‐ tively. The boost comes from the fact that a SENet adds a small neural network, called a SE Block, to every unit in the original architecture (i.e., every inception module or every residual unit), as shown in Figure 14-20.

![](images/06fe855c187ab6300d65af805946c6b44cc8c838a8159ef0f4e9b2c617ad449b.jpg)  
Figure 14-20. SE-Inception Module (le) and SE-ResNet Unit (right)

A SE Block analyzes the output of the unit it is attached to, focusing exclusively on the depth dimension (it does not look for any spatial pattern), and it learns which fea‐ tures are usually most active together. It then uses this information to recalibrate the feature maps, as shown in Figure 14-21. For example, a SE Block may learn that mouths, noses and eyes usually appear together in pictures: if you see a mouth and a nose, you should expect to see eyes as well. So if a SE Block sees a strong activation in the mouth and nose feature maps, but only mild activation in the eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant feature maps). If the eyes were somewhat confused with something else, this feature map recalibration will help resolve the ambiguity.

![](images/f03d8dac6097f205f41b1a3bda6e1c16098de19db3dea74837dfb033bd4554cd.jpg)  
Figure 14-21. An SE Block Performs Feature Map Recalibration

A SE Block is composed of just 3 layers: a global average pooling layer, a hidden dense layer using the ReLU activation function, and a dense output layer using the sigmoid activation function (see Figure 14-22):

![](images/000d65b326898a6006cd1b2e2f1e1398012e71205b68f16a966adbd2270df89c.jpg)  
Figure 14-22. SE Block Architecture

As earlier, the global average pooling layer computes the mean activation for each fea‐ ture map: for example, if its input contains 256 feature maps, it will output 256 num‐ bers representing the overall level of response for each filter. The next layer is where the “squeeze” happens: this layer has much less than 256 neurons, typically 16 times less than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐ pressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector representation (i.e., an embedding) of the distribution of feature responses. This bot‐ tleneck step forces the SE Block to learn a general representation of the feature com‐ binations (we will see this principle in action again when we discuss autoencoders in ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐ tor containing one number per feature map (e.g., 256), each between 0 and 1. The feature maps are then multiplied by this recalibration vector, so irrelevant features (with a low recalibration score) get scaled down while relevant features (with a recali‐ bration score close to 1) are left alone.

# Implementing a ResNet-34 CNN Using Keras

Most CNN architectures described so far are fairly straightforward to implement (although generally you would load a pretrained network instead, as we will see). To illustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s create a ResidualUnit layer:

DefaultConv2D $=$ partial(keras.layers.Conv2D,kernel_size $\equiv 3$ ,strides $\coloneqq 1$ padding $\equiv$ "SAME",use.bias $\equiv$ False)   
class ResidualUnit(keras.layersayers): def__init__(self,filters,strides $\coloneqq 1$ ,activation $\equiv$ "relu"，\*\*kwarges) super().__init__(\*\*kwarges) self.activation $=$ kerasactivations.get(activation) self.main_layers $=$ [ DefaultConv2D(filters,strides $\coloneqq$ strides), keras.layers.batchNormalization(), self.activation, DefaultConv2D(filters), keras.layers.batchNormalization()] self.skip_layers $= [\cdot ]$ if strides $>1$ : self.skip_layers $=$ [ DefaultConv2D(filters,kernel_size $\coloneqq 1$ ,strides $\coloneqq$ strides), keras.layers.batchNormalization())]   
def call(self,inputs): Z $=$ inputs for layer in self.main_layers: Z $=$ layer(Z) skip_Z $=$ inputs for layer in self.skip_layers:

skip_Z $=$ layer skip_Z) return selfactivation(Z $^+$ skip_Z)

As you can see, this code matches Figure 14-18 pretty closely. In the constructor, we create all the layers we will need: the main layers are the ones on the right side of the diagram, and the skip layers are the ones on the left (only needed if the stride is greater than 1). Then in the call() method, we simply make the inputs go through the main layers, and the skip layers (if any), then we add both outputs and we apply the activation function.

Next, we can build the ResNet-34 simply using a Sequential model, since it is really just a long sequence of layers (we can treat each residual unit as a single layer now that we have the ResidualUnit class):

```python
model = keras.models Sequential()
model.add(DefaultConv2D(64, kernel_size=7, strides=2,
                                     input_shape=[224, 224, 3])
model.add(keras.layers BatchNormalization())
model.add(keras.layers.Activation("relu"))
model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding="SAME"))
prev_filters = 64
for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
    strides = 1 if filters == prev_filters else 2
    model.add(ResidualUnit(filters, strides=strides))
    prev_filters = filters
model.add(keras.layers.GlobalAvgPool2D|)
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(10, activation="softmax")) 
```

The only slightly tricky part in this code is the loop that adds the ResidualUnit layers to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have 128 filters, and so on. We then set the strides to 1 when the number of filters is the same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit, and finally we update prev_filters.

It is quite amazing that in less than 40 lines of code, we can build the model that won the ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model, and the expressiveness of the Keras API. Implementing the other CNN architectures is not much harder. However, Keras comes with several of these architectures built in, so why not use them instead?

# Using Pretrained Models From Keras

In general, you won’t have to implement standard models like GoogLeNet or ResNet manually, since pretrained networks are readily available with a single line of code, in the keras.applications package. For example:

```txt
model = keras.applications.resnet50.ResNet50(weights="imagenet") 
```

That’s all! This will create a ResNet-50 model and download weights pretrained on the ImageNet dataset. To use it, you first need to ensure that the images have the right size. A ResNet-50 model expects $2 2 4 \times 2 2 4$ images (other models may expect other sizes, such as $2 9 9 \times 2 9 9$ ), so let’s use TensorFlow’s tf.image.resize() function to resize the images we loaded earlier:

images_resized $=$ tf.image.resize(images, [224, 224])

![](images/8bc829b90b0166d477cb9474ef53726a239c3a481bd1be6bef38694bde27bb45.jpg)

The tf.image.resize() will not preserve the aspect ratio. If this is a problem, you can try cropping the images to the appropriate aspect ratio before resizing. Both operations can be done in one shot with tf.image.crop_and_resize().

The pretrained models assume that the images are preprocessed in a specific way. In some cases they may expect the inputs to be scaled from 0 to 1, or -1 to 1, and so on. Each model provides a preprocess_input() function that you can use to preprocess your images. These functions assume that the pixel values range from 0 to 255, so we must multiply them by 255 (since earlier we scaled them to the 0–1 range):

```python
inputs = keras.applications.resnet50.preprocess_input/images resized * 255) 
```

Now we can use the pretrained model to make predictions:

```txt
Y proba = model.predict(inputs) 
```

As usual, the output Y_proba is a matrix with one row per image and one column per class (in this case, there are 1,000 classes). If you want to display the top K predic‐ tions, including the class name and the estimated probability of each predicted class, you can use the decode_predictions() function. For each image, it returns an array containing the top K predictions, where each prediction is represented as an array containing the class identifier21, its name and the corresponding confidence score:

```python
top_K = keras.applications.resnet50decodepredictions(Y_prob, top=3)  
for image_index in range(len/images)):  
    print("Image {}.format(image_index))  
    for class_id, name, y_prob in top_K-image_index:  
        print("\{} - {:12s} {:.2f}%.format(class_id, name, y_prob * 100))  
print() 
```

The output looks like this:

```txt
Image #0  
n03877845 - palace 42.87%  
n02825657 - bell_cote 40.57%  
n03781244 - monastery 14.56% 
```

```txt
Image #1  
n04522168 - vase 46.83%  
n07930864 - cup 7.78%  
n11939491 - daisy 4.87% 
```

The correct classes (monastery and daisy) appear in the top 3 results for both images. That’s pretty good considering that the model had to choose among 1,000 classes.

As you can see, it is very easy to create a pretty good image classifier using a pre‐ trained model. Other vision models are available in keras.applications, including several ResNet variants, GoogLeNet variants like InceptionV3 and Xception, VGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in mobile applications), and more.

But what if you want to use an image classifier for classes of images that are not part of ImageNet? In that case, you may still benefit from the pretrained models to per‐ form transfer learning.

# Pretrained Models for Transfer Learning

If you want to build an image classifier, but you do not have enough training data, then it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐ cussed in Chapter 11. For example, let’s train a model to classify pictures of flowers, reusing a pretrained Xception model. First, let’s load the dataset using TensorFlow Datasets (see Chapter 13):

```python
import tensorflow_datasets as tfds 
```

```python
dataset, info = tfds.load("tf_flowers", as supervised=True, with_info=True)  
dataset_size = info.split["train"].numexamples # 3670  
class_names = info_features["label"].names # ["dandelion", "daisy", ...]  
n_classeses = info_features["label"].num_classeses # 5 
```

Note that you can get information about the dataset by setting with_info=True. Here, we get the dataset size and the names of the classes. Unfortunately, there is only a "train" dataset, no test set or validation set, so we need to split the training set. The TF Datasets project provides an API for this. For example, let’s take the first $1 0 \%$ of the dataset for testing, the next $1 5 \%$ for validation, and the remaining $7 5 \%$ for train‐ ing:

```python
test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75]) 
```

```python
test_set = tfds.load("tf_flowers", split=test_split, as supervised=True)  
valid_set = tfds.load("tf_flowers", split=valid_split, as supervised=True)  
train_set = tfds.load("tf_flowers", split=train_split, as supervised=True) 
```

Next we must preprocess the images. The CNN expects $2 2 4 \times 2 2 4$ images, so we need to resize them. We also need to run the image through Xception’s prepro cess_input() function:

```python
def preprocess(image, label):
    resized_image = tf(image.reshape(image, [224, 224])
    final_image = keras.applications.xception preprocess_input(resized_image)
    return final_image, label 
```

Let’s apply this preprocessing function to all 3 datasets, and let’s also shuffle & repeat the training set, and add batching & prefetching to all datasets:

```python
batch_size = 32
train_set = train_setshuffle(1000).repeat()
train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)
valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set.map(preprocess).batch(batch_size).prefetch(1) 
```

If you want to perform some data augmentation, you can just change the preprocess‐ ing function for the training set, adding some random transformations to the training images. For example, use tf.image.random_crop() to randomly crop the images, use tf.image.random_flip_left_right() to randomly flip the images horizontally, and so on (see the notebook for an example).

Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the network (by setting include_top=False): this excludes the global average pooling layer and the dense output layer. We then add our own global average pooling layer, based on the output of the base model, followed by a dense output layer with 1 unit per class, using the softmax activation function. Finally, we create the Keras Model:

```python
base_model = keras.applications.xception.Xception(weights="imagenet", include_top=False)  
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)  
output = keras.layers.Dense(n_classes, activation="softmax")(avg)  
model = keras.models.Model(input=base_model input, outputs=output) 
```

As explained in Chapter 11, it’s usually a good idea to freeze the weights of the pre‐ trained layers, at least at the beginning of training:

```txt
for layer in base_model.layers: layer.trainable = False 
```

![](images/c9513e274315370cc39f70d9022803e58e94d49276c997b41859b7a5f9c38575.jpg)

Since our model uses the base model’s layers directly, rather than the base_model object itself, setting base_model.trainable=False would have no effect.

Finally, we can compile the model and start training:

optimizer $=$ keras optimizers.SGD(lr=0.2,momentum $\equiv$ 0.9,decay $= 0.01$ model.compile(loss $=$ "sparse_categorical_crossentropy",optimizer $\equiv$ optimizer, metrics $=$ ["accuracy"]   
history $=$ model.fit(train_set, steps_per_epoch $\equiv$ int(0.75\*dataset_size/batch_size), validation_data $\equiv$ valid_set, validation_steps $\equiv$ int(0.15\* dataset_size/batch_size), epochs $= 5$

![](images/259d2681e8ed459514e6e38186ed06a64b7b160dcbceede44b3856a52b3ca06b.jpg)

This will be very slow, unless you have a GPU. If you do not, then you should run this chapter’s notebook in Colab, using a GPU run‐ time (it’s free!). See the instructions at https://github.com/ageron/ handson-ml2.

After training the model for a few epochs, its validation accuracy should reach about $7 5 - 8 0 \%$ , and stop making much progress. This means that the top layers are now pretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing just the top ones), and continue training (don’t forget to compile the model when you freeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐ aging the pretrained weights:

```python
for layer in base_model.layers:  
    layer.trainable = True  
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)  
model.compile(...)  
history = model.fit(...) 
```

It will take a while, but this model should reach around $9 5 \%$ accuracy on the test set. With that, you can start training amazing image classifiers! But there’s more to com‐ puter vision than just classification. For example, what if you also want to know where the flower is in the picture? Let’s look at this now.

# Classication and Localization

Localizing an object in a picture can be expressed as a regression task, as discussed in Chapter 10: to predict a bounding box around the object, a common approach is to predict the horizontal and vertical coordinates of the object’s center, as well as its height and width. This means we have 4 numbers to predict. It does not require much change to the model, we just need to add a second dense output layer with 4 units (typically on top of the global average pooling layer), and it can be trained using the MSE loss:

```txt
base_model = keras.applications.xception.Xception(weights="imagenet", include_top=False)  
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)  
class_output = keras.layers(Dense(n_classes, activation="softmax")(avg)) 
```

```python
loc_output = keras.layers(Dense(4)(avg)  
model = keras.models.Model(input=base_model.input,  
  outputs=[class_output, loc_output])  
model.compile(loss=['sparse_categorical_crossentropy","mse"],  
  lossweights=[0.8,0.2], #depends on what you care most about  
  optimizer=optimizer, metrics=['accuracy']) 
```

But now we have a problem: the flowers dataset does not have bounding boxes around the flowers. So we need to add them ourselves. This is often one of the hard‐ est and most costly part of a Machine Learning project: getting the labels. It’s a good idea to spend time looking for the right tools. To annotate images with bounding boxes, you may want to use an open source image labeling tool like VGG Image Annotator, LabelImg, OpenLabeler or ImgLab, or perhaps a commercial tool like LabelBox or Supervisely. You may also want to consider crowdsourcing platforms such as Amazon Mechanical Turk or CrowdFlower if you have a very large number of images to annotate. However, it is quite a lot of work to setup a crowdsourcing plat‐ form, prepare the form to be sent to the workers, to supervise them and ensure the quality of the bounding boxes they produce is good, so make sure it is worth the effort: if there are just a few thousand images to label, and you don’t plan to do this frequently, it may be preferable to do it yourself. Adriana Kovashka et al. wrote a very practical paper22 about crowdsourcing in Computer Vision, I recommend you check it out, even if you do not plan to use crowdsourcing.

So let’s suppose you obtained the bounding boxes for every image in the flowers data‐ set (for now we will assume there is a single bounding box per image), you then need to create a dataset whose items will be batches of preprocessed images along with their class labels and their bounding boxes. Each item should be a tuple of the form: (images, (class_labels, bounding_boxes)). Then you are ready to train your model!

![](images/cf8428fc2358d3c950f4dba920e90ee2ad811dcd28c389dce14500b5f977ccb6.jpg)

The bounding boxes should be normalized so that the horizontal and vertical coordinates, as well as the height and width all range from 0 to 1. Also, it is common to predict the square root of the height and width rather than the height and width directly: this way, a 10 pixel error for a large bounding box will not be penalized as much as a 10 pixel error for a small bounding box.

The MSE often works fairly well as a cost function to train the model, but it is not a great metric to evaluate how well the model can predict bounding boxes. The most common metric for this is the Intersection over Union (IoU): it is the area of overlap between the predicted bounding box and the target bounding box, divided by the

area of their union (see Figure 14-23). In tf.keras, it is implemented by the tf.keras.metrics.MeanIoU class.

![](images/91377af29336bb7765242493026c1dc92e01d65dde6a06c34bc4f75a692030ea.jpg)  
Figure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes

Classifying and localizing a single object is nice, but what if the images contain multi‐ ple objects (as is often the case in the flowers dataset)?

# Object Detection

The task of classifying and localizing multiple objects in an image is called object detection. Until a few years ago, a common approach was to take a CNN that was trained to classify and locate a single object, then slide it across the image, as shown in Figure 14-24. In this example, the image was chopped into a $6 \times 8$ grid, and we show a CNN (the thick black rectangle) sliding across all $3 \times 3$ regions. When the CNN was looking at the top left of the image, it detected part of the left-most rose, and then it detected that same rose again when it was first shifted one step to the right. At the next step, it started detecting part of the top-most rose, and then it detec‐ ted it again once it was shifted one more step to the right. You would then continue to slide the CNN through the whole image, looking at all $3 \times 3$ regions. Moreover, since objects can have varying sizes, you would also slide the CNN across regions of differ‐ ent sizes. For example, once you are done with the $3 \times 3$ regions, you might want to slide the CNN across all $4 \times 4$ regions as well.

![](images/a632c492d571dc16415e43a082825cfb802acbd74c153be1f0a1f36b1866fbd2.jpg)  
Figure 14-24. Detecting Multiple Objects by Sliding a CNN Across the Image

This technique is fairly straightforward, but as you can see it will detect the same object multiple times, at slightly different positions. Some post-processing will then be needed to get rid of all the unnecessary bounding boxes. A common approach for this is called non-max suppression:

• First, you need to add an extra objectness output to your CNN, to estimate the probability that a flower is indeed present in the image (alternatively, you could add a “no-flower” class, but this usually does not work as well). It must use the sigmoid activation function and you can train it using the "binary_crossen tropy" loss. Then just get rid of all the bounding boxes for which the objectness score is below some threshold: this will drop all the bounding boxes that don’t actually contain a flower.   
• Second, find the bounding box with the highest objectness score, and get rid of all the other bounding boxes that overlap a lot with it (e.g., with an IoU greater than $6 0 \%$ ). For example, in Figure 14-24, the bounding box with the max object‐ ness score is the thick bounding box over the top-most rose (the objectness score is represented by the thickness of the bounding boxes). The other bounding box over that same rose overlaps a lot with the max bounding box, so we will get rid of it.

• Third, repeat step two until there are no more bounding boxes to get rid of.

This simple approach to object detection works pretty well, but it requires running the CNN many times, so it is quite slow. Fortunately, there is a much faster way to slide a CNN across an image: using a Fully Convolutional Network.

# Fully Convolutional Networks (FCNs)

The idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for semantic segmentation (the task of classifying every pixel in an image according to the class of the object it belongs to). They pointed out that you could replace the dense layers at the top of a CNN by convolutional layers. To understand this, let’s look at an example: suppose a dense layer with 200 neurons sits on top of a convolutional layer that outputs 100 feature maps, each of size $7 \times 7$ (this is the feature map size, not the kernel size). Each neuron will compute a weighted sum of all $1 0 0 \times 7 \times 7$ activa‐ tions from the convolutional layer (plus a bias term). Now let’s see what happens if we replace the dense layer with a convolution layer using 200 filters, each $7 \times 7$ , and with VALID padding. This layer will output 200 feature maps, each $1 \times 1$ (since the kernel is exactly the size of the input feature maps and we are using VALID padding). In other words, it will output 200 numbers, just like the dense layer did, and if you look closely at the computations performed by a convolutional layer, you will notice that these numbers will be precisely the same as the dense layer produced. The only differ‐ ence is that the dense layer’s output was a tensor of shape [batch size, 200] while the convolutional layer will output a tensor of shape [batch size, 1, 1, 200].

![](images/8d4b8620d2d52cd53f80ae7a7ed9ee8e8c99ac7199907c74e1e0bc470ffc3e67.jpg)

To convert a dense layer to a convolutional layer, the number of fil‐ ters in the convolutional layer must be equal to the number of units in the dense layer, the filter size must be equal to the size of the input feature maps, and you must use VALID padding. The stride may be set to 1 or more, as we will see shortly.

Why is this important? Well, while a dense layer expects a specific input size (since it has one weight per input feature), a convolutional layer will happily process images of any size24 (however, it does expect its inputs to have a specific number of channels, since each kernel contains a different set of weights for each input channel). Since an FCN contains only convolutional layers (and pooling layers, which have the same property), it can be trained and executed on images of any size!

For example, suppose we already trained a CNN for flower classification and localiza‐ tion. It was trained on $2 2 4 \times 2 2 4$ images and it outputs 10 numbers: outputs 0 to 4 are sent through the softmax activation function, and this gives the class probabilities (one per class); output 5 is sent through the logistic activation function, and this gives the objectness score; outputs 6 to 9 do not use any activation function, and they rep‐ resent the bounding box’s center coordinates, and its height and width. We can now convert its dense layers to convolutional layers. In fact, we don’t even need to retrain it, we can just copy the weights from the dense layers to the convolutional layers! Alternatively, we could have converted the CNN into an FCN before training.

Now suppose the last convolutional layer before the output layer (also called the bot‐ tleneck layer) outputs $7 \times 7$ feature maps when the network is fed a $2 2 4 \times 2 2 4$ image (see the left side of Figure 14-25). If we feed the FCN a $4 4 8 \times 4 4 8$ image (see the right side of Figure 14-25), the bottleneck layer will now output $1 4 \times 1 4$ feature maps.25 Since the dense output layer was replaced by a convolutional layer using 10 filters of size $7 \times 7$ , VALID padding and stride 1, the output will be composed of 10 features maps, each of size $8 \times 8$ (since $1 4 - 7 + 1 = 8 )$ ). In other words, the FCN will process the whole image only once and it will output an $8 \times 8$ grid where each cell contains 10 numbers (5 class probabilities, 1 objectness score and 4 bounding box coordinates). It’s exactly like taking the original CNN and sliding it across the image using 8 steps per row and 8 steps per column: to visualize this, imagine chopping the original image into a $1 4 \times 1 4$ grid, then sliding a $7 \times 7$ window across this grid: there will be 8 $\times \ 8 = 6 4$ possible locations for the window, hence $8 \times 8$ predictions. However, the FCN approach is much more efficient, since the network only looks at the image once. In fact, You Only Look Once (YOLO) is the name of a very popular object detec‐ tion architecture!

![](images/44f0d990fec8f45809725ca9d17a2755a772c308ecc71405360e9a4fdc283b93.jpg)  
Figure 14-25. A Fully Convolutional Network Processing a Small Image (le) and a Large One (right)

# You Only Look Once (YOLO)

YOLO is an extremely fast and accurate object detection architecture proposed by Joseph Redmon et al. in a 2015 paper26, and subsequently improved in $2 0 1 6 ^ { 2 7 }$ (YOLOv2) and in $2 0 1 8 ^ { 2 8 }$ (YOLOv3). It is so fast that it can run in realtime on a video (check out this nice demo).

YOLOv3’s architecture is quite similar to the one we just discussed, but with a few important differences:

• First, it outputs 5 bounding boxes for each grid cell (instead of just 1), and each bounding box comes with an objectness score. It also outputs 20 class probabili‐ ties per grid cell, as it was trained on the PASCAL VOC dataset, which contains 20 classes. That’s a total of 45 numbers per grid cell $5 ^ { * } 4$ bounding box coordi‐ nates, plus 5 objectness scores, plus 20 class probabilities).   
• Second, instead of predicting the absolute coordinates of the bounding box cen‐ ters, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where (0, 0) means the top left of that cell, and (1, 1) means the bottom right. For each grid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in that cell (but the bounding box itself generally extends well beyond the grid cell). YOLOv3 applies the logistic activation function to the bounding box coordinates to ensure they remain in the 0 to 1 range.   
• Third, before training the neural net, YOLOv3 finds 5 representative bounding box dimensions, called anchor boxes (or bounding box priors): it does this by applying the K-Means algorithm (see ???) to the height and width of the training set bounding boxes. For example, if the training images contain many pedes‐ trians, then one of the anchor boxes will likely have the dimensions of a typical pedestrian. Then when the neural net predicts 5 bounding boxes per grid cell, it actually predicts how much to rescale each of the anchor boxes. For example, suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for one of the grid cells), this will result in a predicted bounding box of size $1 5 0 \times 4 5$ pixels. To be more precise, for each grid cell and each anchor box, the network predicts the log of the vertical and horizontal rescaling factors. Having these pri‐ ors makes the network more likely to predict bounding boxes of the appropriate dimensions, and it also speeds up training since it will more quickly learn what reasonable bounding boxes look like.   
• Fourth, the network is trained using images of different scales: every few batches during training, the network randomly chooses a new image dimension (from $3 3 0 \times 3 3 0$ to $6 0 8 \times 6 0 8$ pixels). This allows the network to learn to detect objects at different scales. Moreover, it makes it possible to use YOLOv3 at different scales: the smaller scale will be less accurate but faster than the larger scale, so you can choose the right tradeoff for your use case.

There are a few more innovations you might be interested in, such as the use of skip connections to recover some of the spatial resolution that is lost in the CNN (we will discuss this shortly when we look at semantic segmentation). Moreover, in the 2016 paper, the authors introduce the YOLO9000 model that uses hierarchical classifica‐ tion: the model predicts a probability for each node in a visual hierarchy called Word‐ Tree. This makes it possible for the network to predict with high confidence that an image represents, say, a dog, even though it is unsure what specific type of dog it is.

So I encourage you to go ahead and read all three papers: they are quite pleasant to read, and it is an excellent example of how Deep Learning systems can be incremen‐ tally improved.

# Mean Average Precision (mAP)

A very common metric used in object detection tasks is the mean Average Precision (mAP). “Mean Average” sounds a bit redundant, doesn’t it? To understand this met‐ ric, let’s go back to two classification metrics we discussed in Chapter 3: precision and recall. Remember the tradeoff: the higher the recall, the lower the precision. You can visualize this in a Precision/Recall curve (see Figure 3-5). To summarize this curve into a single number, we could compute its Area Under the Curve (AUC). But note that the Precision/Recall curve may contain a few sections where precision actually goes up when recall increases, especially at low recall values (you can see this at the top left of Figure 3-5). This is one of the motivations for the mAP metric.

Suppose the classifier has a $9 0 \%$ precision at $1 0 \%$ recall, but a $9 6 \%$ precision at $2 0 \%$ recall: there’s really no tradeoff here: it simply makes more sense to use the classifier at $2 0 \%$ recall rather than at $1 0 \%$ recall, as you will get both higher recall and higher precision. So instead of looking at the precision at $1 0 \%$ recall, we should really be looking at the maximum precision that the classifier can offer with at least $1 0 \%$ recall. It would be $9 6 \%$ , not $9 0 \%$ . So one way to get a fair idea of the model’s performance is to compute the maximum precision you can get with at least $0 \%$ recall, then $1 0 \%$ recall, $2 0 \%$ , and so on up to $1 0 0 \%$ , and then calculate the mean of these maximum precisions. This is called the Average Precision (AP) metric. Now when there are more than 2 classes, we can compute the AP for each class, and then compute the mean AP (mAP). That’s it!

However, in an object detection systems, there is an additional level of complexity: what if the system detected the correct class, but at the wrong location (i.e., the bounding box is completely off)? Surely we should not count this as a positive predic‐ tion. So one approach is to define an IOU threshold: for example, we may consider that a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted class is correct. The corresponding mAP is generally noted $\mathrm { m A P @ 0 . 5 }$ (or $\mathrm { m A P @ \mathit { 5 0 ^ { 0 } } }$ , or sometimes just $\mathrm { A P } _ { 5 0 } \mathrm { \stackrel { \cdot } { } }$ ). In some competitions (such as the Pascal VOC challenge), this is what is done. In others (such as the COCO competition), the mAP is computed for different IOU thresholds $( 0 . 5 0 , 0 . 5 5 , 0 . 6 0 , . . . , 0 . 9 5 )$ , and the final metric is the mean of all these mAPs (noted $\operatorname { A P } @ [ . 5 0 ; . 9 5 ]$ or $\operatorname { A P } @ [ . 5 0 ; 0 . 0 5 ; . 9 5 ] )$ ). Yes, that’s a mean mean average.

Several YOLO implementations built using TensorFlow are available on github, some with pretrained weights. At the time of writing, they are based on TensorFlow 1, but by the time you read this, TF 2 implementations will certainly be available. Moreover, other object detection models are available in the TensorFlow Models project, many

with pretrained weights, and some have even been ported to TF Hub, making them extremely easy to use, such as $\mathrm { S S D } ^ { 2 9 }$ and Faster-RCNN.30, which are both quite popu‐ lar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-CNN is more complex: the image first goes through a CNN, and the output is passed to a Region Proposal Network (RPN) which proposes bounding boxes that are most likely to contain an object, and a classifier is run for each bounding box, based on the cropped output of the CNN.

The choice of detection system depends on many factors: speed, accuracy, available pretrained models, training time, complexity, etc. The papers contain tables of met‐ rics, but there is quite a lot of variability in the testing environments, and the technol‐ ogies evolve so fast that it is difficulty to make a fair comparison that will be useful for most people and remain valid for more than a few months.

Great! So we can locate objects by drawing bounding boxes around them. But per‐ haps you might want to be a bit more precise. Let’s see how to go down to the pixel level.

# Semantic Segmentation

In semantic segmentation, each pixel is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26. Note that different objects of the same class are not distinguished. For example, all the bicy‐ cles on the right side of the segmented image end up as one big lump of pixels. The main difficulty in this task is that when images go through a regular CNN, they grad‐ ually lose their spatial resolution (due to the layers with strides greater than 1): so a regular CNN may end up knowing that there’s a person in the image, somewhere in the bottom left of the image, but it will not be much more precise than that.

![](images/8c7f0625dd46fde3472b42cea7450b8bd8f4ccc8edf35dba0c70e794a558aa13.jpg)

![](images/9a0680cb0fb96b2d83424f933626592b9735d7cd7d2eff0d4799b50bb1886f63.jpg)  
Figure 14-26. Semantic segmentation

Just like for object detection, there are many different approaches to tackle this prob‐ lem, some quite complex. However, a fairly simple solution was proposed in the 2015 paper by Jonathan Long et al. we discussed earlier. They start by taking a pretrained CNN and turning into an FCN, as discussed earlier. The CNN applies a stride of 32 to the input image overall (i.e., if you add up all the strides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than the input image. This is clearly too coarse, so they add a single upsampling layer that multiplies the resolution by 32. There are several solutions available for upsampling (increasing the size of an image), such as bilinear interpolation, but it only works reasonably well up to $\times 4$ or $\times 8$ . Instead, they used a transposed convolutional layer:31 it is equivalent to first stretching the image by inserting empty rows and columns (full of zeros), then per‐ forming a regular convolution (see Figure 14-27). Alternatively, some people prefer to think of it as a regular convolutional layer that uses fractional strides (e.g., 1/2 in Figure 14-27). The transposed convolutional layer can be initialized to perform some‐ thing close to linear interpolation, but since it is a trainable layer, it will learn to do better during training.

![](images/3876b8940808a7e83ea1546f4a8f3c5c90ef4e031c3cb37865cef9bec5c8cfb2.jpg)  
Figure 14-27. Upsampling Using a Transpose Convolutional Layer

![](images/c87655e3b64b223fc14e0bac10089013eea000e3c6cbd225a6b430a846a0caea.jpg)

In a transposed convolution layer, the stride defines how much the input will be stretched, not the size of the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling layers).

# TensorFlow Convolution Operations

TensorFlow also offers a few other kinds of convolutional layers:

• keras.layers.Conv1D creates a convolutional layer for 1D inputs, such as time series or text (sequences of letters or words), as we will see in ???.   
• keras.layers.Conv3D creates a convolutional layer for 3D inputs, such as 3D PET scan.   
• Setting the dilation_rate hyperparameter of any convolutional layer to a value of 2 or more creates an $\grave { a }$ -trous convolutional layer (“à trous” is French for “with holes”). This is equivalent to using a regular convolutional layer with a filter dila‐ ted by inserting rows and columns of zeros (i.e., holes). For example, a $1 \times 3$ filter equal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated lter [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This allows the convolutional layer to

have a larger receptive field at no computational price and using no extra param‐ eters.   
• tf.nn.depthwise_conv2d() can be used to create a depthwise convolutional layer (but you need to create the variables yourself). It applies every filter to every individual input channel independently. Thus, if there are $f _ { n }$ filters and $f _ { n ^ { \prime } }$ input channels, then this will output $f _ { n } \times f _ { n ^ { \prime } }$ feature maps.

This solution is okay, but still too imprecise. To do better, the authors added skip con‐ nections from lower layers: for example, they upsampled the output image by a factor of 2 (instead of 32), and they added the output of a lower layer that had this double resolution. Then they upsampled the result by a factor of 16, leading to a total upsam‐ pling factor of 32 (see Figure 14-28). This recovered some of the spatial resolution that was lost in earlier pooling layers. In their best architecture, they used a second similar skip connection to recover even finer details from an even lower layer: in short, the output of the original CNN goes through the following extra steps: upscale $\times 2 .$ , add the output of a lower layer (of the appropriate scale), upscale $\times 2$ , add the out‐ put of an even lower layer, and finally upscale $\times 8$ . It is even possible to scale up beyond the size of the original image: this can be used to increase the resolution of an image, which is a technique called super-resolution.

![](images/d958a0ba67494b9b55e62972823572953d6356280dae231f1dae0812b3f5fcc4.jpg)  
Figure 14-28. Skip layers recover some spatial resolution from lower layers

Once again, many github repositories provide TensorFlow implementations of semantic segmentation (TensorFlow 1 for now), and you will even find a pretrained instance segmentation model in the TensorFlow Models project. Instance segmenta‐ tion is similar to semantic segmentation, but instead of merging all objects of the same class into one big lump, each object is distinguished from the others (e.g., it identifies each individual bicycle). At the present, they provide multiple implementa‐ tions of the Mask R-CNN architecture, which was proposed in a 2017 paper: it extends the Faster R-CNN model by additionally producing a pixel-mask for each bounding box. So not only do you get a bounding box around each object, with a set of estimated class probabilities, you also get a pixel mask that locates pixels in the bounding box that belong to the object.

As you can see, the field of Deep Computer Vision is vast and moving fast, with all sorts of architectures popping out every year, all based on Convolutional Neural Net‐ works. The progress made in just a few years has been astounding, and researchers are now focusing on harder and harder problems, such as adversarial learning (which attempts to make the network more resistant to images designed to fool it), explaina‐ bility (understanding why the network makes a specific classification), realistic image generation (which we will come back to in ???), single-shot learning (a system that can recognize an object after it has seen it just once), and much more. Some even explore completely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐ sented them in a couple videos, with the corresponding code in a notebook). Now on to the next chapter, where we will look at how to process sequential data such as time series using Recurrent Neural Networks and Convolutional Neural Networks.

# Exercises

1. What are the advantages of a CNN over a fully connected DNN for image classi‐ fication?   
2. Consider a CNN composed of three convolutional layers, each with $3 \times 3$ kernels, a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of $2 0 0 \times 3 0 0$ pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?   
3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?   
4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?   
5. When would you want to add a local response normalization layer?   
6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet and Xception?   
7. What is a Fully Convolutional Network? How can you convert a dense layer into a convolutional layer?   
8. What is the main technical difficulty of semantic segmentation?   
9. Build your own CNN from scratch and try to achieve the highest possible accu‐ racy on MNIST.

10. Use transfer learning for large image classification.

a. Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can just use an existing dataset (e.g., from Tensor‐ Flow Datasets).   
b. Split it into a training set, a validation set and a test set.   
c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.   
d. Fine-tune a pretrained model on this dataset.

11. Go through TensorFlow’s DeepDream tutorial. It is a fun way to familiarize your‐ self with various ways of visualizing the patterns learned by a CNN, and to gener‐ ate art using Deep Learning.

Solutions to these exercises are available in ???.

# About the Author

Aurélien Géron is a Machine Learning consultant. A former Googler, he led the You‐ Tube video classification team from 2013 to 2016. He was also a founder and CTO of Wifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO of Polyconseil in 2001, the firm that now manages the electric car sharing service Autolib’.

Before this he worked as an engineer in a variety of domains: finance (JP Morgan and Société Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He published a few technical books (on $\mathrm { C } { + } { + }$ , WiFi, and internet architectures), and was a Computer Science lecturer in a French engineering school.

A few fun facts: he taught his three children to count in binary with their fingers (up to 1023), he studied microbiology and evolutionary genetics before going into soft‐ ware engineering, and his parachute didn’t open on the second jump.

# Colophon

The animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐ sorFlow is the fire salamander (Salamandra salamandra), an amphibian found across most of Europe. Its black, glossy skin features large yellow spots on the head and back, signaling the presence of alkaloid toxins. This is a possible source of this amphibian’s common name: contact with these toxins (which they can also spray short distances) causes convulsions and hyperventilation. Either the painful poisons or the moistness of the salamander’s skin (or both) led to a misguided belief that these creatures not only could survive being placed in fire but could extinguish it as well.

Fire salamanders live in shaded forests, hiding in moist crevices and under logs near the pools or other freshwater bodies that facilitate their breeding. Though they spend most of their life on land, they give birth to their young in water. They subsist mostly on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot in length, and in captivity, may live as long as 50 years.

The fire salamander’s numbers have been reduced by destruction of their forest habi‐ tat and capture for the pet trade, but the greatest threat is the susceptibility of their moisture-permeable skin to pollutants and microbes. Since 2014, they have become extinct in parts of the Netherlands and Belgium due to an introduced fungus.

Many of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com.

The cover image is from Wood’s Illustrated Natural History. The cover fonts are URW Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.

# INTRODUCTION

# TO

# MACHINE LEARNING

# AN EARLY DRAFT OF A PROPOSED TEXTBOOK

Nils J. Nilsson

Robotics Laboratory

Department of Computer Science

Stanford University

Stanford, CA 94305

e-mail: nilsson@cs.stanford.edu

November 3, 1998

Copyright 
c 2005 Nils J. Nilsson

This material may not be copied, reproduced, or distributed without the written permission of the copyright holder.

# Contents

# 1 Preliminaries 1

1.1 Introduction . . 1

1.1.1 What is Machine Learning? 1   
1.1.2 Wellsprings of Machine Learning . . . . 3   
1.1.3 Varieties of Machine Learning . . . . 4

1.2 Learning Input-Output Functions . . . 5

1.2.1 Types of Learning . . . 5   
1.2.2 Input Vectors . . . 7   
1.2.3 Outputs . . . 8   
1.2.4 Training Regimes . . . . 8   
1.2.5 Noise 9   
1.2.6 Performance Evaluation . . . 9

1.3 Learning Requires Bias . . . 9

1.4 Sample Applications . . . 11   
1.5 Sources 13   
1.6 Bibliographical and Historical Remarks 13

# 2 Boolean Functions 15

2.1 Representation 15

2.1.1 Boolean Algebra . . 15   
2.1.2 Diagrammatic Representations . . . 16

2.2 Classes of Boolean Functions 17

2.2.1 Terms and Clauses . . . 17   
2.2.2 DNF Functions . . . 18   
2.2.3 CNF Functions . . . 21   
2.2.4 Decision Lists . . . 22   
2.2.5 Symmetric and Voting Functions . . . . 23   
2.2.6 Linearly Separable Functions . . 23

2.3 Summary 24   
2.4 Bibliographical and Historical Remarks 25

# 3 Using Version Spaces for Learning 27

3.1 Version Spaces and Mistake Bounds 27   
3.2 Version Graphs . . . 29   
3.3 Learning as Search of a Version Space . . 32   
3.4 The Candidate Elimination Method 32   
3.5 Bibliographical and Historical Remarks 34

# 4 Neural Networks 35

4.1 Threshold Logic Units . . . 35

4.1.1 Definitions and Geometry . . . 35   
4.1.2 Special Cases of Linearly Separable Functions . . . . . . . 37   
4.1.3 Error-Correction Training of a TLU . . . 38   
4.1.4 Weight Space . . . . 40   
4.1.5 The Widrow-Hoff Procedure . . . . . 42   
4.1.6 Training a TLU on Non-Linearly-Separable Training Sets 44

4.2 Linear Machines 44

4.3 Networks of TLUs 46

4.3.1 Motivation and Examples . . . 46   
4.3.2 Madalines . . . 49   
4.3.3 Piecewise Linear Machines . . . . 50   
4.3.4 Cascade Networks . . 51

4.4 Training Feedforward Networks by Backpropagation . . . . . . . 52

4.4.1 Notation . . . 52   
4.4.2 The Backpropagation Method . . . . 53   
4.4.3 Computing Weight Changes in the Final Layer . . . . . . 56   
4.4.4 Computing Changes to the Weights in Intermediate Layers 58   
4.4.5 Variations on Backprop . . . 59   
4.4.6 An Application: Steering a Van . . . . 60

4.5 Synergies Between Neural Network and Knowledge-Based Methods 61   
4.6 Bibliographical and Historical Remarks 61

# 5 Statistical Learning 63

5.1 Using Statistical Decision Theory . . . . 63

5.1.1 Background and General Method . . . . 63   
5.1.2 Gaussian (or Normal) Distributions 65   
5.1.3 Conditionally Independent Binary Components . . . . . . 68

5.2 Learning Belief Networks 70   
5.3 Nearest-Neighbor Methods . . . 70   
5.4 Bibliographical and Historical Remarks 72

# 6 Decision Trees 73

6.1 Definitions . 73   
6.2 Supervised Learning of Univariate Decision Trees . . . . . . . . . 74

6.2.1 Selecting the Type of Test . . . 75   
6.2.2 Using Uncertainty Reduction to Select Tests . . . . . . . 75   
6.2.3 Non-Binary Attributes . . . 79

6.3 Networks Equivalent to Decision Trees . . . . 79   
6.4 Overfitting and Evaluation 80

6.4.1 Overfitting 80   
6.4.2 Validation Methods 81   
6.4.3 Avoiding Overfitting in Decision Trees . . . . . 82   
6.4.4 Minimum-Description Length Methods . . . . . . 83   
6.4.5 Noise in Data . . . . 84

6.5 The Problem of Replicated Subtrees . . 84   
6.6 The Problem of Missing Attributes . . . 86   
6.7 Comparisons 86   
6.8 Bibliographical and Historical Remarks . . 87

# 7 Inductive Logic Programming 89

7.1 Notation and Definitions . . . 90   
7.2 A Generic ILP Algorithm . . 91   
7.3 An Example . . . 94   
7.4 Inducing Recursive Programs . . . 98   
7.5 Choosing Literals to Add . . 100   
7.6 Relationships Between ILP and Decision Tree Induction . . . . . 101   
7.7 Bibliographical and Historical Remarks . 104

# 8 Computational Learning Theory 107

8.1 Notation and Assumptions for PAC Learning Theory . . . . . . . 107   
8.2 PAC Learning . . . . . 109

8.2.1 The Fundamental Theorem . 109   
8.2.2 Examples . . . . 111   
8.2.3 Some Properly PAC-Learnable Classes . . . . . . . . . . . 112

8.3 The Vapnik-Chervonenkis Dimension . . . 113

8.3.1 Linear Dichotomies . . . 113   
8.3.2 Capacity . . . . 115   
8.3.3 A More General Capacity Result . . . . . 116   
8.3.4 Some Facts and Speculations About the VC Dimension . 117

8.4 VC Dimension and PAC Learning . . . . 118   
8.5 Bibliographical and Historical Remarks 118

# 9 Unsupervised Learning 119

9.1 What is Unsupervised Learning? . 119   
9.2 Clustering Methods . . . . 120   
9.2.1 A Method Based on Euclidean Distance . . . . . . . . . . 120   
9.2.2 A Method Based on Probabilities . . . . . . 124

9.3 Hierarchical Clustering Methods . 125

9.3.1 A Method Based on Euclidean Distance . . . . . . . . . . 125   
9.3.2 A Method Based on Probabilities . . . . . 126

9.4 Bibliographical and Historical Remarks . . 130

# 10 Temporal-Difference Learning 131

10.1 Temporal Patterns and Prediction Problems . . . . . 131   
10.2 Supervised and Temporal-Difference Methods . . . . . . . . . 131   
10.3 Incremental Computation of the $( \Delta \mathbf { W } ) _ { i }$ . . . . . 134   
10.4 An Experiment with TD Methods . 135   
10.5 Theoretical Results . . . . 138   
10.6 Intra-Sequence Weight Updating . . . . 138   
10.7 An Example Application: TD-gammon . . . . . 140   
10.8 Bibliographical and Historical Remarks . . 141

# 11 Delayed-Reinforcement Learning 143

11.1 The General Problem 143   
11.2 An Example . . . . 144   
11.3 Temporal Discounting and Optimal Policies . . . . . . 145   
11.4 $Q$ -Learning . 147   
11.5 Discussion, Limitations, and Extensions of Q-Learning . . . . . . 150

11.5.1 An Illustrative Example . . . . . . 150   
11.5.2 Using Random Actions 152   
11.5.3 Generalizing Over Inputs . . . . . 153   
11.5.4 Partially Observable States . . . 154   
11.5.5 Scaling Problems . . . . 154

11.6 Bibliographical and Historical Remarks . . 155

# 12 Explanation-Based Learning 157

12.1 Deductive Learning . . . 157   
12.2 Domain Theories . . 158   
12.3 An Example . . . 159   
12.4 Evaluable Predicates . . 162   
12.5 More General Proofs . . . 164   
12.6 Utility of EBL 164   
12.7 Applications . . 164

12.7.1 Macro-Operators in Planning . . . . 164

12.7.2 Learning Search Control Knowledge . . . . . 167

12.8 Bibliographical and Historical Remarks . . . 168

# Preface

These notes are in the process of becoming a textbook. The process is quite unfinished, and the author solicits corrections, criticisms, and suggestions from students and other readers. Although I have tried to eliminate errors, some undoubtedly remain—caveat lector. Many typographical infelicities will no doubt persist until the final version. More material has yet to be added. Please let me have your suggestions about topics that are too important to be left out. I hope that future versions will cover Hopfield nets, Elman nets and other recurrent nets, radial basis functions, grammar and automata learning, genetic algorithms, and Bayes networks . . .. I am also collecting exercises and project suggestions which will appear in future versions.

My intention is to pursue a middle ground between a theoretical textbook and one that focusses on applications. The book concentrates on the important ideas in machine learning. I do not give proofs of many of the theorems that I state, but I do give plausibility arguments and citations to formal proofs. And, I do not treat many matters that would be of practical importance in applications; the book is not a handbook of machine learning practice. Instead, my goal is to give the reader sufficient preparation to make the extensive literature on machine learning accessible.

Students in my Stanford courses on machine learning have already made several useful suggestions, as have my colleague, Pat Langley, and my teaching assistants, Ron Kohavi, Karl Pfleger, Robert Allen, and Lise Getoor.

Some of my plans for additions and other reminders are mentioned in marginal notes.

# Chapter 1

# Preliminaries

# 1.1 Introduction

# 1.1.1 What is Machine Learning?

Learning, like intelligence, covers such a broad range of processes that it is difficult to define precisely. A dictionary definition includes phrases such as “to gain knowledge, or understanding of, or skill in, by study, instruction, or experience,” and “modification of a behavioral tendency by experience.” Zoologists and psychologists study learning in animals and humans. In this book we focus on learning in machines. There are several parallels between animal and machine learning. Certainly, many techniques in machine learning derive from the efforts of psychologists to make more precise their theories of animal and human learning through computational models. It seems likely also that the concepts and techniques being explored by researchers in machine learning may illuminate certain aspects of biological learning.

As regards machines, we might say, very broadly, that a machine learns whenever it changes its structure, program, or data (based on its inputs or in response to external information) in such a manner that its expected future performance improves. Some of these changes, such as the addition of a record to a data base, fall comfortably within the province of other disciplines and are not necessarily better understood for being called learning. But, for example, when the performance of a speech-recognition machine improves after hearing several samples of a person’s speech, we feel quite justified in that case to say that the machine has learned.

Machine learning usually refers to the changes in systems that perform tasks associated with artificial intelligence (AI). Such tasks involve recognition, diagnosis, planning, robot control, prediction, etc. The “changes” might be either enhancements to already performing systems or ab initio synthesis of new systems. To be slightly more specific, we show the architecture of a typical AI

“agent” in Fig. 1.1. This agent perceives and models its environment and computes appropriate actions, perhaps by anticipating their effects. Changes made to any of the components shown in the figure might count as learning. Different learning mechanisms might be employed depending on which subsystem is being changed. We will study several different learning methods in this book.

![](images/1c0d5e7daa34be62cec728d476f5fd206bcfbd2cf4c1dc4cf79f2dac562c5153.jpg)  
Figure 1.1: An AI System

One might ask “Why should machines have to learn? Why not design machines to perform as desired in the first place?” There are several reasons why machine learning is important. Of course, we have already mentioned that the achievement of learning in machines might help us understand how animals and humans learn. But there are important engineering reasons as well. Some of these are:

• Some tasks cannot be defined well except by example; that is, we might be able to specify input/output pairs but not a concise relationship between inputs and desired outputs. We would like machines to be able to adjust their internal structure to produce correct outputs for a large number of sample inputs and thus suitably constrain their input/output function to approximate the relationship implicit in the examples.   
• It is possible that hidden among large piles of data are important relationships and correlations. Machine learning methods can often be used to extract these relationships (data mining).

# 1.1. INTRODUCTION

• Human designers often produce machines that do not work as well as desired in the environments in which they are used. In fact, certain characteristics of the working environment might not be completely known at design time. Machine learning methods can be used for on-the-job improvement of existing machine designs.   
• The amount of knowledge available about certain tasks might be too large for explicit encoding by humans. Machines that learn this knowledge gradually might be able to capture more of it than humans would want to write down.   
• Environments change over time. Machines that can adapt to a changing environment would reduce the need for constant redesign.   
• New knowledge about tasks is constantly being discovered by humans. Vocabulary changes. There is a constant stream of new events in the world. Continuing redesign of AI systems to conform to new knowledge is impractical, but machine learning methods might be able to track much of it.

# 1.1.2 Wellsprings of Machine Learning

Work in machine learning is now converging from several sources. These different traditions each bring different methods and different vocabulary which are now being assimilated into a more unified discipline. Here is a brief listing of some of the separate disciplines that have contributed to machine learning; more details will follow in the the appropriate chapters:

• Statistics: A long-standing problem in statistics is how best to use samples drawn from unknown probability distributions to help decide from which distribution some new sample is drawn. A related problem is how to estimate the value of an unknown function at a new point given the values of this function at a set of sample points. Statistical methods for dealing with these problems can be considered instances of machine learning because the decision and estimation rules depend on a corpus of samples drawn from the problem environment. We will explore some of the statistical methods later in the book. Details about the statistical theory underlying these methods can be found in statistical textbooks such as [Anderson, 1958].   
• Brain Models: Non-linear elements with weighted inputs have been suggested as simple models of biological neurons. Networks of these elements have been studied by several researchers including [McCulloch & Pitts, 1943, Hebb, 1949, Rosenblatt, 1958] and, more recently by [Gluck & Rumelhart, 1989, Sejnowski, Koch, & Churchland, 1988]. Brain modelers are interested in how closely these networks approximate the learning phenomena of

living brains. We shall see that several important machine learning techniques are based on networks of nonlinear elements—often called neural networks. Work inspired by this school is sometimes called connectionism, brain-style computation, or sub-symbolic processing.

• Adaptive Control Theory: Control theorists study the problem of controlling a process having unknown parameters which must be estimated during operation. Often, the parameters change during operation, and the control process must track these changes. Some aspects of controlling a robot based on sensory inputs represent instances of this sort of problem. For an introduction see [Bollinger & Duffie, 1988].

• Psychological Models: Psychologists have studied the performance of humans in various learning tasks. An early example is the EPAM network for storing and retrieving one member of a pair of words when given another [Feigenbaum, 1961]. Related work led to a number of early decision tree [Hunt, Marin, & Stone, 1966] and semantic network [Anderson & Bower, 1973] methods. More recent work of this sort has been influenced by activities in artificial intelligence which we will be presenting.

Some of the work in reinforcement learning can be traced to efforts to model how reward stimuli influence the learning of goal-seeking behavior in animals [Sutton & Barto, 1987]. Reinforcement learning is an important theme in machine learning research.

• Artificial Intelligence: From the beginning, AI research has been concerned with machine learning. Samuel developed a prominent early program that learned parameters of a function for evaluating board positions in the game of checkers [Samuel, 1959]. AI researchers have also explored the role of analogies in learning [Carbonell, 1983] and how future actions and decisions can be based on previous exemplary cases [Kolodner, 1993]. Recent work has been directed at discovering rules for expert systems using decision-tree methods [Quinlan, 1990] and inductive logic programming [Muggleton, 1991, Lavraˇc & Dˇzeroski, 1994]. Another theme has been saving and generalizing the results of problem solving using explanation-based learning [DeJong & Mooney, 1986, Laird, et al., 1986, Minton, 1988, Etzioni, 1993].

# • Evolutionary Models:

In nature, not only do individual animals learn to perform better, but species evolve to be better fit in their individual niches. Since the distinction between evolving and learning can be blurred in computer systems, techniques that model certain aspects of biological evolution have been proposed as learning methods to improve the performance of computer programs. Genetic algorithms [Holland, 1975] and genetic programming [Koza, 1992, Koza, 1994] are the most prominent computational techniques for evolution.

# 1.1.3 Varieties of Machine Learning

Orthogonal to the question of the historical source of any learning technique is the more important question of what is to be learned. In this book, we take it that the thing to be learned is a computational structure of some sort. We will consider a variety of different computational structures:

• Functions   
• Logic programs and rule sets   
• Finite-state machines   
• Grammars   
• Problem solving systems

We will present methods both for the synthesis of these structures from examples and for changing existing structures. In the latter case, the change to the existing structure might be simply to make it more computationally efficient rather than to increase the coverage of the situations it can handle. Much of the terminology that we shall be using throughout the book is best introduced by discussing the problem of learning functions, and we turn to that matter first.

# 1.2 Learning Input-Output Functions

We use Fig. 1.2 to help define some of the terminology used in describing the problem of learning a function. Imagine that there is a function, $f$ , and the task of the learner is to guess what it is. Our hypothesis about the function to be learned is denoted by $h$ . Both $f$ and $h$ are functions of a vector-valued input $\mathbf { X } = ( x _ { 1 } , x _ { 2 } , \ldots , x _ { i } , \ldots , x _ { n } )$ which has $n$ components. We think of $h$ as being implemented by a device that has $\mathbf { X }$ as input and $h ( \mathbf { X } )$ as output. Both $f$ and $h$ themselves may be vector-valued. We assume $a$ priori that the hypothesized function, $h$ , is selected from a class of functions $\mathcal { H }$ . Sometimes we know that $f$ also belongs to this class or to a subset of this class. We select $h$ based on a training set, $\Xi$ , of $m$ input vector examples. Many important details depend on the nature of the assumptions made about all of these entities.

# 1.2.1 Types of Learning

There are two major settings in which we wish to learn a function. In one, called supervised learning, we know (sometimes only approximately) the values of $f$ for the $m$ samples in the training set, $\Xi$ . We assume that if we can find a hypothesis, $h$ , that closely agrees with $f$ for the members of $\Xi$ , then this hypothesis will be a good guess for $f$ —especially if $\Xi$ is large.

Training Set:

$$
\Xi = \{\mathbf {X} _ {1}, \mathbf {X} _ {2}, \dots \mathbf {X} _ {\mathrm {i}}, \dots , \mathbf {X} _ {\mathrm {m}} \}
$$

![](images/d98eb0a47f5520ba0b3af7df3cb1f829931ead70bdf0e74e517fea143169245c.jpg)  
Figure 1.2: An Input-Output Function

Curve-fitting is a simple example of supervised learning of a function. Suppose we are given the values of a two-dimensional function, $f$ , at the four sample points shown by the solid circles in Fig. 1.3. We want to fit these four points with a function, $h$ , drawn from the set, $\mathcal { H }$ , of second-degree functions. We show there a two-dimensional parabolic surface above the $x _ { 1 }$ , $x _ { 2 }$ plane that fits the points. This parabolic function, $h$ , is our hypothesis about the function, $f$ , that produced the four samples. In this case, $h = f$ at the four samples, but we need not have required exact matches.

In the other setting, termed unsupervised learning, we simply have a training set of vectors without function values for them. The problem in this case, typically, is to partition the training set into subsets, $\Xi _ { 1 }$ , . . . , $\Xi _ { R }$ , in some appropriate way. (We can still regard the problem as one of learning a function; the value of the function is the name of the subset to which an input vector belongs.) Unsupervised learning methods have application in taxonomic problems in which it is desired to invent ways to classify data into meaningful categories.

We shall also describe methods that are intermediate between supervised and unsupervised learning.

We might either be trying to find a new function, $h$ , or to modify an existing one. An interesting special case is that of changing an existing function into an equivalent one that is computationally more efficient. This type of learning is sometimes called speed-up learning. A very simple example of speed-up learning involves deduction processes. From the formulas $A \supset B$ and $B \supset C$ , we can deduce $C$ if we are given $A$ . From this deductive process, we can create the formula $A \supset C$ —a new formula but one that does not sanction any more con-

![](images/0a8afecd8da929c7224e6248f101ef5a178351177e64ddac6e969c05c8d038bc.jpg)  
Figure 1.3: A Surface that Fits Four Points

clusions than those that could be derived from the formulas that we previously had. But with this new formula we can derive $C$ more quickly, given $A$ , than we could have done before. We can contrast speed-up learning with methods that create genuinely new functions—ones that might give different results after learning than they did before. We say that the latter methods involve inductive learning. As opposed to deduction, there are no correct inductions—only useful ones.

# 1.2.2 Input Vectors

Because machine learning methods derive from so many different traditions, its terminology is rife with synonyms, and we will be using most of them in this book. For example, the input vector is called by a variety of names. Some of these are: input vector, pattern vector, feature vector, sample, example, and instance. The components, $x _ { i }$ , of the input vector are variously called features, attributes, input variables, and components.

The values of the components can be of three main types. They might be real-valued numbers, discrete-valued numbers, or categorical values. As an example illustrating categorical values, information about a student might be represented by the values of the attributes class, major, sex, adviser. A particular student would then be represented by a vector such as: (sophomore, history, male, higgins). Additionally, categorical values may be ordered (as in {small, medium, large}) or unordered (as in the example just given). Of course, mixtures of all these types of values are possible.

In all cases, it is possible to represent the input in unordered form by listing the names of the attributes together with their values. The vector form assumes that the attributes are ordered and given implicitly by a form. As an example of an attribute-value representation, we might have: (major: history, sex: male,

class: sophomore, adviser: higgins, age: 19). We will be using the vector form exclusively.

An important specialization uses Boolean values, which can be regarded as a special case of either discrete numbers (1,0) or of categorical variables (True, False).

# 1.2.3 Outputs

The output may be a real number, in which case the process embodying the function, $h$ , is called a function estimator, and the output is called an output value or estimate.

Alternatively, the output may be a categorical value, in which case the process embodying $h$ is variously called a classifier, a recognizer, or a categorizer, and the output itself is called a label, a class, a category, or a decision. Classifiers have application in a number of recognition problems, for example in the recognition of hand-printed characters. The input in that case is some suitable representation of the printed character, and the classifier maps this input into one of, say, 64 categories.

Vector-valued outputs are also possible with components being real numbers or categorical values.

An important special case is that of Boolean output values. In that case, a training pattern having value 1 is called a positive instance, and a training sample having value 0 is called a negative instance. When the input is also Boolean, the classifier implements a Boolean function. We study the Boolean case in some detail because it allows us to make important general points in a simplified setting. Learning a Boolean function is sometimes called concept learning, and the function is called a concept.

# 1.2.4 Training Regimes

There are several ways in which the training set, $\Xi$ , can be used to produce a hypothesized function. In the batch method, the entire training set is available and used all at once to compute the function, $h$ . A variation of this method uses the entire training set to modify a current hypothesis iteratively until an acceptable hypothesis is obtained. By contrast, in the incremental method, we select one member at a time from the training set and use this instance alone to modify a current hypothesis. Then another member of the training set is selected, and so on. The selection method can be random (with replacement) or it can cycle through the training set iteratively. If the entire training set becomes available one member at a time, then we might also use an incremental method—selecting and using training set members as they arrive. (Alternatively, at any stage all training set members so far available could be used in a “batch” process.) Using the training set members as they become available is called an online method. Online methods might be used, for example, when the

next training instance is some function of the current hypothesis and the previous instance—as it would be when a classifier is used to decide on a robot’s next action given its current set of sensory inputs. The next set of sensory inputs will depend on which action was selected.

# 1.2.5 Noise

Sometimes the vectors in the training set are corrupted by noise. There are two kinds of noise. Class noise randomly alters the value of the function; attribute noise randomly alters the values of the components of the input vector. In either case, it would be inappropriate to insist that the hypothesized function agree precisely with the values of the samples in the training set.

# 1.2.6 Performance Evaluation

Even though there is no correct answer in inductive learning, it is important to have methods to evaluate the result of learning. We will discuss this matter in more detail later, but, briefly, in supervised learning the induced function is usually evaluated on a separate set of inputs and function values for them called the testing set . A hypothesized function is said to generalize when it guesses well on the testing set. Both mean-squared-error and the total number of errors are common measures.

# 1.3 Learning Requires Bias

Long before now the reader has undoubtedly asked why is learning a function possible at all? Certainly, for example, there are an uncountable number of different functions having values that agree with the four samples shown in Fig. 1.3. Why would a learning procedure happen to select the quadratic one shown in that figure? In order to make that selection we had at least to limit a priori the set of hypotheses to quadratic functions and then to insist that the one we chose passed through all four sample points. This kind of a priori information is called bias, and useful learning without bias is impossible.

We can gain more insight into the role of bias by considering the special case of learning a Boolean function of $n$ dimensions. There are $2 ^ { n }$ different Boolean inputs possible. Suppose we had no bias; that is $\mathcal { H }$ is the set of all $2 ^ { 2 ^ { n } }$ Boolean functions, and we have no preference among those that fit the samples in the training set. In this case, after being presented with one member of the training set and its value we can rule out precisely one-half of the members of $\mathcal { H }$ —those Boolean functions that would misclassify this labeled sample. The remaining functions constitute what is called a “version space;” we’ll explore that concept in more detail later. As we present more members of the training set, the graph of the number of hypotheses not yet ruled out as a function of the number of different patterns presented is as shown in Fig. 1.4. At any stage of the process,

half of the remaining Boolean functions have value 1 and half have value 0 for any training pattern not yet seen. No generalization is possible in this case because the training patterns give no clue about the value of a pattern not yet seen. Only memorization is possible here, which is a trivial sort of learning.

![](images/d4374ee6d1d10819887f70d7c7a777f450ddf5aebd91c267cdf04e9d9c5016db.jpg)  
Figure 1.4: Hypotheses Remaining as a Function of Labeled Patterns Presented

But suppose we limited $\mathcal { H }$ to some subset, $\mathcal { H } _ { c }$ , of all Boolean functions. Depending on the subset and on the order of presentation of training patterns, a curve of hypotheses not yet ruled out might look something like the one shown in Fig. 1.5. In this case it is even possible that after seeing fewer than all $2 ^ { n }$ labeled samples, there might be only one hypothesis that agrees with the training set. Certainly, even if there is more than one hypothesis remaining, most of them may have the same value for most of the patterns not yet seen! The theory of Probably Approximately Correct $( P A C )$ learning makes this intuitive idea precise. We’ll examine that theory later.

Let’s look at a specific example of how bias aids learning. A Boolean function can be represented by a hypercube each of whose vertices represents a different input pattern. We show a 3-dimensional version in Fig. 1.6. There, we show a training set of six sample patterns and have marked those having a value of 1 by a small square and those having a value of 0 by a small circle. If the hypothesis set consists of just the linearly separable functions—those for which the positive and negative instances can be separated by a linear surface, then there is only one function remaining in this hypothsis set that is consistent with the training set. So, in this case, even though the training set does not contain all possible patterns, we can already pin down what the function must be—given the bias.

![](images/683ac6be2c311c91f33992ff28543e8e6b201fdeea37d445980b7634829fd9bd.jpg)  
Figure 1.5: Hypotheses Remaining From a Restricted Subset

Machine learning researchers have identified two main varieties of bias, absolute and preference. In absolute bias (also called restricted hypothesis-space bias), one restricts $\mathcal { H }$ to a definite subset of functions. In our example of Fig. 1.6, the restriction was to linearly separable Boolean functions. In preference bias, one selects that hypothesis that is minimal according to some ordering scheme over all hypotheses. For example, if we had some way of measuring the complexity of a hypothesis, we might select the one that was simplest among those that performed satisfactorily on the training set. The principle of Occam’s razor, used in science to prefer simple explanations to more complex ones, is a type of preference bias. (William of Occam, 1285-?1349, was an English philosopher who said: “non sunt multiplicanda entia praeter necessitatem,” which means “entities should not be multiplied unnecessarily.”)

# 1.4 Sample Applications

Our main emphasis in this book is on the concepts of machine learning—not on its applications. Nevertheless, if these concepts were irrelevant to real-world problems they would probably not be of much interest. As motivation, we give a short summary of some areas in which machine learning techniques have been successfully applied. [Langley, 1992] cites some of the following applications and others:

a. Rule discovery using a variant of ID3 for a printing industry problem

![](images/4a14d4137fbfd375dff7ddd1e69e7acc18b15c9e1d36106236c4609b2c5982bc.jpg)  
Figure 1.6: A Training Set That Completely Determines a Linearly Separable Function

[Evans & Fisher, 1992].

b. Electric power load forecasting using a $k$ -nearest-neighbor rule system [Jabbour, K., et al., 1987].   
c. Automatic “help desk” assistant using a nearest-neighbor system [Acorn & Walden, 1992].   
d. Planning and scheduling for a steel mill using ExpertEase, a marketed (ID3-like) system [Michie, 1992].   
e. Classification of stars and galaxies [Fayyad, et al., 1993].

Many application-oriented papers are presented at the annual conferences on Neural Information Processing Systems. Among these are papers on: speech recognition, dolphin echo recognition, image processing, bio-engineering, diagnosis, commodity trading, face recognition, music composition, optical character recognition, and various control applications [Various Editors, 1989-1994].

As additional examples, [Hammerstrom, 1993] mentions:

a. Sharp’s Japanese kanji character recognition system processes 200 characters per second with 99+% accuracy. It recognizes 3000+ characters.   
b. NeuroForecasting Centre’s (London Business School and University College London) trading strategy selection network earned an average annual profit of 18% against a conventional system’s $1 2 . 3 \%$ .

# 1.5. SOURCES

c. Fujitsu’s (plus a partner’s) neural network for monitoring a continuous steel casting operation has been in successful operation since early 1990.

In summary, it is rather easy nowadays to find applications of machine learning techniques. This fact should come as no surprise inasmuch as many machine learning techniques can be viewed as extensions of well known statistical methods which have been successfully applied for many years.

# 1.5 Sources

Besides the rich literature in machine learning (a small part of which is referenced in the Bibliography), there are several textbooks that are worth mentioning [Hertz, Krogh, & Palmer, 1991, Weiss & Kulikowski, 1991, Natarjan, 1991, Fu, 1994, Langley, 1996]. [Shavlik & Dietterich, 1990, Buchanan & Wilkins, 1993] are edited volumes containing some of the most important papers. A survey paper by [Dietterich, 1990] gives a good overview of many important topics. There are also well established conferences and publications where papers are given and appear including:

• The Annual Conferences on Advances in Neural Information Processing Systems   
• The Annual Workshops on Computational Learning Theory   
• The Annual International Workshops on Machine Learning   
• The Annual International Conferences on Genetic Algorithms (The Proceedings of the above-listed four conferences are published by Morgan Kaufmann.)   
• The journal Machine Learning (published by Kluwer Academic Publishers).

There is also much information, as well as programs and datasets, available over the Internet through the World Wide Web.

# 1.6 Bibliographical and Historical Remarks

To be added. Every chapter will contain a brief survey of the history of the material covered in that chapter.

# Chapter 2

# Boolean Functions

# 2.1 Representation

# 2.1.1 Boolean Algebra

Many important ideas about learning of functions are most easily presented using the special case of Boolean functions. There are several important subclasses of Boolean functions that are used as hypothesis classes for function learning. Therefore, we digress in this chapter to present a review of Boolean functions and their properties. (For a more thorough treatment see, for example, [Unger, 1989].)

A Boolean function, $f ( x _ { 1 } , x _ { 2 } , \ldots , x _ { n } )$ maps an $n$ -tuple of (0,1) values to $\{ 0 , 1 \}$ . Boolean algebra is a convenient notation for representing Boolean functions. Boolean algebra uses the connectives ·, $^ +$ , and $^ -$ . For example, the and function of two variables is written $x _ { 1 } \cdot x _ { 2 }$ . By convention, the connective, “·” is usually suppressed, and the and function is written $x _ { 1 } x _ { 2 }$ . $x _ { 1 } x _ { 2 }$ has value 1 if and only if both $x _ { 1 }$ and $x _ { 2 }$ have value 1; if either $x _ { 1 }$ or $x _ { 2 }$ has value $0$ , $x _ { 1 } x _ { 2 }$ has value 0. The (inclusive) or function of two variables is written $x _ { 1 } + x _ { 2 }$ . $x _ { 1 } + x _ { 2 }$ has value 1 if and only if either or both of $x _ { 1 }$ or $x _ { 2 }$ has value 1; if both $x _ { 1 }$ and $x _ { 2 }$ have value $0$ , $x _ { 1 } + x _ { 2 }$ has value 0. The complement or negation of a variable, $x$ , is written $x$ . $x$ has value 1 if and only if $x$ has value $0$ ; if $x$ has value 1, $\scriptstyle { \overline { { x } } }$ has value 0.

These definitions are compactly given by the following rules for Boolean algebra:

$$
1 + 1 = 1, 1 + 0 = 1, 0 + 0 = 0,
$$

$$
1 \cdot 1 = 1, 1 \cdot 0 = 0, 0 \cdot 0 = 0, \text {a n d}
$$

$$
\begin{array}{c} \overline {{1}} = 0, \overline {{0}} = 1. \end{array}
$$

Sometimes the arguments and values of Boolean functions are expressed in terms of the constants $T$ (True) and $F$ (False) instead of 1 and 0, respectively.

The connectives · and $^ +$ are each commutative and associative. Thus, for example, $x _ { 1 } ( x _ { 2 } x _ { 3 } ) \ = \ ( x _ { 1 } x _ { 2 } ) x _ { 3 }$ , and both can be written simply as $x _ { 1 } x _ { 2 } x _ { 3 }$ . Similarly for $^ +$ .

A Boolean formula consisting of a single variable, such as $x _ { 1 }$ is called an atom. One consisting of either a single variable or its complement, such as $\overline { { x _ { 1 } } }$ is called a literal.

The operators · and $^ +$ do not commute between themselves. Instead, we have DeMorgan’s laws (which can be verified by using the above definitions):

$$
\overline {{x _ {1} x _ {2}}} = \overline {{x _ {1}}} + \overline {{x _ {2}}}, \text {a n d}
$$

$$
\overline {{x _ {1} + x _ {2}}} = \overline {{x _ {1}}} \overline {{x _ {2}}}.
$$

# 2.1.2 Diagrammatic Representations

We saw in the last chapter that a Boolean function could be represented by labeling the vertices of a cube. For a function of $n$ variables, we would need an $n$ -dimensional hypercube. In Fig. 2.1 we show some 2- and 3-dimensional examples. Vertices having value 1 are labeled with a small square, and vertices having value 0 are labeled with a small circle.

![](images/0b2823cbb353baab1991f47de1928bbad5925928e4607c5bac89678161080840.jpg)

![](images/68d45898c2b2a55ce74d8864ee2d13104d8e6e0a9261ffe9f8b6614a685913a8.jpg)

![](images/bf11aa38064516256069e0d6c1c1f50ebc87a6b68c170eb92fb727a6e8c1263e.jpg)

![](images/ab7fe756b84edf6299fa6708f33e412207259d30451b861c7435fc0fb09c9b0d.jpg)  
Figure 2.1: Representing Boolean Functions on Cubes

Using the hypercube representations, it is easy to see how many Boolean functions of $n$ dimensions there are. A 3-dimensional cube has $2 ^ { 3 } = 8$ vertices, and each may be labeled in two different ways; thus there are $2 ^ { ( 2 ^ { 3 } ) } ~ = ~ 2 5 6$

different Boolean functions of 3 variables. In general, there are $2 ^ { 2 ^ { \prime \prime } }$ Boolean functions of $n$ variables.

We will be using 2- and 3-dimensional cubes later to provide some intuition about the properties of certain Boolean functions. Of course, we cannot visualize hypercubes (for $n > 3$ ), and there are many surprising properties of higher dimensional spaces, so we must be careful in using intuitions gained in low dimensions. One diagrammatic technique for dimensions slightly higher than 3 is the Karnaugh map. A Karnaugh map is an array of values of a Boolean function in which the horizontal rows are indexed by the values of some of the variables and the vertical columns are indexed by the rest. The rows and columns are arranged in such a way that entries that are adjacent in the map correspond to vertices that are adjacent in the hypercube representation. We show an example of the 4-dimensional even parity function in Fig. 2.2. (An even parity function is a Boolean function that has value 1 if there are an even number of its arguments that have value 1; otherwise it has value 0.) Note that all adjacent cells in the table correspond to inputs differing in only one component.

Also describe general logic diagrams, [Wnek, et al., 1990].

![](images/3f2eb0d98ceab04f884995e43162f280f27110d5337de5243223d99662d49d9f.jpg)  
Figure 2.2: A Karnaugh Map

# 2.2 Classes of Boolean Functions

# 2.2.1 Terms and Clauses

To use absolute bias in machine learning, we limit the class of hypotheses. In learning Boolean functions, we frequently use some of the common sub-classes of those functions. Therefore, it will be important to know about these subclasses.

One basic subclass is called terms. A term is any function written in the form $l _ { 1 } l _ { 2 } \cdots l _ { k }$ , where the $l _ { i }$ are literals. Such a form is called a conjunction of literals. Some example terms are $x _ { 1 } x _ { 7 }$ and $x _ { 1 } x _ { 2 } \overline { { x _ { 4 } } }$ . The size of a term is the number of literals it contains. The examples are of sizes 2 and 3, respectively. (Strictly speaking, the class of conjunctions of literals is called the monomials,

Probably I’ll put in a simple term-learning algorithm here—so we can get started on learning! Also for DNF functions and decision lists—as they are defined in the next few pages.

and a conjunction of literals itself is called a term. This distinction is a fine one which we elect to blur here.)

It is easy to show that there are exactly umber o, where $k$ or less is bounded from above by is the binomial coefficient. $3 ^ { n }$ possible terms of $\textstyle \sum _ { i = 0 } ^ { k } C ( 2 n , i ) =$ $n$ variables. O(nk) C(i, j) = i!(i−j)!j! $O ( n ^ { k } )$ $\begin{array} { r } { C ( i , j ) = \frac { i ! } { ( i - j ) ! j ! } } \end{array}$

A clause is any function written in the form $l _ { 1 } + l _ { 2 } + \cdots + l _ { k }$ , where the $l _ { i }$ are literals. Such a form is called a disjunction of literals. Some example clauses are con $x _ { 3 } + x _ { 5 } + x _ { 6 }$ $x _ { 1 } + \overline { { x _ { 4 } } }$ . The size of a clause isle clauses and fewer than literals it clauses of $3 ^ { n }$ $\textstyle \sum _ { i = 0 } ^ { k } C ( 2 n , i )$ $k$ $f$ $f$ versa. Thus, terms and clauses are duals of each other.

In psychological experiments, conjunctions of literals seem easier for humans to learn than disjunctions of literals.

# 2.2.2 DNF Functions

A Boolean function is said to be in disjunctive normal form (DNF) if it can be written as a disjunction of terms. Some examples in DNF are: $f = x _ { 1 } x _ { 2 } + x _ { 2 } x _ { 3 } x _ { 4 }$ and $f = x _ { 1 } { \overline { { x _ { 3 } } } } + { \overline { { x _ { 2 } } } } { \ \overline { { x _ { 3 } } } } + x _ { 1 } x _ { 2 } { \overline { { x _ { 3 } } } }$ . A DNF expression is called a $k$ -term DNF expression if it is a disjunction of $k$ terms; it is in the class $k$ -DNF if the size of its largest term is $k$ . The examples above are 2-term and 3-term expressions, respectively. Both expressions are in the class 3-DNF.

Each term in a DNF expression for a function is called an implicant because it “implies” the function (if the term has value 1, so does the function). In general, a term, $t$ , is an implicant of a function, $f$ , if $f$ has value 1 whenever $t$ does. A term, $t$ , is a prime implicant of $f$ if the term, $t ^ { \prime }$ , formed by taking any literal out of an implicant $t$ is no longer an implicant of $f$ . (The implicant cannot be “divided” by any term and remain an implicant.)

Thus, both $x _ { 2 } x _ { 3 }$ and ${ \overline { { x _ { 1 } } } } { \overline { { x _ { 3 } } } }$ are prime implicants of $f = x _ { 2 } \overline { { x _ { 3 } } } + \overline { { x _ { 1 } } } \overline { { x _ { 3 } } } + x _ { 2 } x _ { 1 } \overline { { x _ { 3 } } }$ but $x _ { 2 } x _ { 1 } \overline { { x _ { 3 } } }$ is not.

The relationship between implicants and prime implicants can be geometrically illustrated using the cube representation for Boolean functions. Consider, for example, the function $f = x _ { 2 } { \overline { { x _ { 3 } } } } + { \overline { { x _ { 1 } } } } { \ \overline { { x _ { 3 } } } } + x _ { 2 } x _ { 1 } { \overline { { x _ { 3 } } } }$ . We illustrate it in Fig. 2.3. Note that each of the three planes in the figure “cuts off” a group of vertices having value 1, but none cuts off any vertices having value 0. These planes are pictorial devices used to isolate certain lower dimensional subfaces of the cube. Two of them isolate one-dimensional edges, and the third isolates a zero-dimensional vertex. Each group of vertices on a subface corresponds to one of the implicants of the function, $f$ , and thus each implicant corresponds to a subface of some dimension. A $k$ -dimensional subface corresponds to an $( n - k )$ -size implicant term. The function is written as the disjunction of the implicants—corresponding to the union of all the vertices cut off by all of the planes. Geometrically, an implicant is prime if and only if its corresponding subface is the largest dimensional subface that includes all of its vertices and

no other vertices having value 0. Note that the term $x _ { 2 } x _ { 1 } \overline { { x _ { 3 } } }$ is not a prime implicant of $f$ . (In this case, we don’t even have to include this term in the function because the vertex cut off by the plane corresponding to $x _ { 2 } x _ { 1 } \overline { { x _ { 3 } } }$ is already cut off by the plane corresponding to $x _ { 2 } \overline { { x _ { 3 } } }$ .) The other two implicants are prime because their corresponding subfaces cannot be expanded without including vertices having value 0.

![](images/efeb019baabd23958921ec9d9052478a2f1607312cc36d5640914fa07cd6cd1b.jpg)  
Figure 2.3: A Function and its Implicants

$$
= x _ {2} \overline {{x _ {3}}} + \overline {{x _ {1}}} \overline {{x _ {3}}}
$$

$\mathsf { x } _ { 2 } \bar { \mathsf { x } _ { 3 } }$ and $\overline { { \mathsf { x } _ { 1 } } } \overline { { \mathsf { x } _ { 3 } } }$ are prime implicants

Note that all Boolean functions can be represented in DNF—trivially by disjunctions of terms of size $n$ where each term corresponds to one of the vertices whose value is 1. Whereas there are $2 ^ { 2 ^ { n } }$ functions of $n$ dimensions in DNF (since any Boolean function can be written in DNF), there are just $2 ^ { O ( n ^ { k } ) }$ functions in $k$ -DNF.

All Boolean functions can also be represented in DNF in which each term is a prime implicant, but that representation is not unique, as shown in Fig. 2.4.

If we can express a function in DNF form, we can use the consensus method to find an expression for the function in which each term is a prime implicant. The consensus method relies on two results:

• Consensus:

We may replace this section with one describing the Quine-McCluskey method instead.

![](images/ce4e5e5235c4225e60245b283346c3bec642ab23d7bfb974817acab4b093e6b2.jpg)  
All of the terms are prime implicants, but there is not a unique representation   
Figure 2.4: Non-Uniqueness of Representation by Prime Implicants

$$
x _ {i} \cdot f _ {1} + \overline {{x _ {i}}} \cdot f _ {2} = x _ {i} \cdot f _ {1} + \overline {{x _ {i}}} \cdot f _ {2} + f _ {1} \cdot f _ {2}
$$

where $f _ { 1 }$ and $f _ { 2 }$ are terms such that no literal appearing in $f _ { 1 }$ appears complemented in $f _ { 2 }$ . $f _ { 1 } \cdot f _ { 2 }$ is called the consensus of $x _ { i } \cdot f _ { 1 }$ and ${ \overline { { x _ { i } } } } \cdot$ $f _ { 2 }$ . Readers familiar with the resolution rule of inference will note that consensus is the dual of resolution.

Examples: $x _ { 1 }$ is the consensus of $x _ { 1 } x _ { 2 }$ and $x _ { 1 } x _ { 2 }$ . The terms ${ \overline { { x _ { 1 } } } } x _ { 2 }$ and $x _ { 1 } x _ { 2 }$ have no consensus since each term has more than one literal appearing complemented in the other.

• Subsumption:

$$
x _ {i} \cdot f _ {1} + f _ {1} = f _ {1}
$$

where $f _ { 1 }$ is a term. We say that $f _ { 1 }$ subsumes $x _ { i } \cdot f _ { 1 }$ .

Example: $\overline { { x _ { 1 } } } ~ \overline { { x _ { 4 } } } x _ { 5 }$ subsumes $\overline { { x _ { 1 } } } \ \overline { { x _ { 4 } } } \ \overline { { x _ { 2 } } } x _ { 5 }$

The consensus method for finding a set of prime implicants for a function, $f$ , iterates the following operations on the terms of a DNF expression for $f$ until no more such operations can be applied:

a. initialize the process with the set, $\tau$ , of terms in the DNF expression of $f$ ,   
b. compute the consensus of a pair of terms in $\tau$ and add the result to $\tau$ ,   
c. eliminate any terms in $\tau$ that are subsumed by other terms in $\tau$ .

When this process halts, the terms remaining in $\tau$ are all prime implicants of $f$ .

Example: Let $f = { \overline { { x _ { 1 } } } } x _ { 2 } + { \overline { { x _ { 1 } } } } { \overline { { x _ { 2 } } } } x _ { 3 } + { \overline { { x _ { 1 } } } } { \overline { { x _ { 2 } } } } { \overline { { x _ { 3 } } } } { \overline { { x _ { 4 } } } } x _ { 5 }$ . We show a derivation of a set of prime implicants in the consensus tree of Fig. 2.5. The circled numbers adjoining the terms indicate the order in which the consensus and subsumption operations were performed. Shaded boxes surrounding a term indicate that it was subsumed. The final form of the function in which all terms are prime implicants is: $f = { \overline { { x _ { 1 } } } } x _ { 2 } + { \overline { { x _ { 1 } } } } x _ { 3 } + { \overline { { x _ { 1 } } } } \ { \overline { { x _ { 4 } } } } x _ { 5 }$ . Its terms are all of the non-subsumed terms in the consensus tree.

![](images/02709e94d2d3d8ee50a4932895bd3f1ba6bb5ba5f2b2bda1b253b8bb0eb95119.jpg)  
Figure 2.5: A Consensus Tree

# 2.2.3 CNF Functions

Disjunctive normal form has a dual: conjunctive normal form (CNF). A Boolean function is said to be in CNF if it can be written as a conjunction of clauses.

An example in CNF is: $f = ( x _ { 1 } + x _ { 2 } ) ( x _ { 2 } + x _ { 3 } + x _ { 4 } )$ . A CNF expression is called a $k$ -clause CNF expression if it is a conjunction of $k$ clauses; it is in the class $k$ -CNF if the size of its largest clause is $k$ . The example is a 2-clause expression in 3-CNF. If $f$ is written in DNF, an application of De Morgan’s law renders $\overline { { f } }$ in CNF, and vice versa. Because CNF and DNF are duals, there are also $2 ^ { O ( n ^ { k } ) }$ functions in $k$ -CNF.

# 2.2.4 Decision Lists

Rivest has proposed a class of Boolean functions called decision lists [Rivest, 1987]. A decision list is written as an ordered list of pairs:

$$
\begin{array}{l} \left(t _ {q}, v _ {q}\right) \\ \left(t _ {q - 1}, v _ {q - 1}\right) \\ \begin{array}{c c c} \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot \end{array} \\ \left(t _ {i}, v _ {i}\right) \\ \cdot \cdot \cdot \\ \left(t _ {2}, v _ {2}\right) \\ (T, v _ {1}) \\ \end{array}
$$

where the $v _ { i }$ are either 0 or $^ { 1 }$ , the $t _ { i }$ are terms in $( x _ { 1 } , \ldots , x _ { n } )$ , and $T$ is a term whose value is 1 (regardless of the values of the $x _ { i }$ ). The value of a decision list is the value of $v _ { i }$ for the first $t _ { i }$ in the list that has value 1. (At least one $t _ { i }$ will have value 1, because the last one does; $v _ { 1 }$ can be regarded as a default value of the decision list.) The decision list is of size $k$ , if the size of the largest term in it is $k$ . The class of decision lists of size $k$ or less is called $k$ -DL.

An example decision list is:

$$
\begin{array}{l} f = \\ (\overline {{x _ {1}}} x _ {2}, 1) \\ (\overline {{x _ {1}}} \overline {{x _ {2}}} x _ {3}, 0) \\ \overline {{x _ {2}}} x _ {3}, 1) \\ (1, 0) \\ \end{array}
$$

$f$ has value 0 for $x _ { 1 } = 0$ , $x _ { 2 } = 0$ , and $x _ { 3 } = 1$ . It has value 1 for $x _ { 1 } = 1$ , $x _ { 2 } = 0$ , and $x _ { 3 } = 1$ . This function is in 3-DL.

It has been shown that the class $k$ -DL is a strict superset of the union of $k$ -DNF and $k$ -CNF. There are $2 ^ { O [ n ^ { k } k \log ( n ) ] }$ functions in $k$ -DL [Rivest, 1987].

Interesting generalizations of decision lists use other Boolean functions in place of the terms, $t _ { i }$ . For example we might use linearly separable functions in place of the $t _ { i }$ (see below and [Marchand & Golea, 1993]).

# 2.2.5 Symmetric and Voting Functions

A Boolean function is called symmetric if it is invariant under permutations of the input variables. For example, any function that is dependent only on the number of input variables whose values are 1 is a symmetric function. The parity functions, which have value 1 depending on whether or not the number of input variables with value 1 is even or odd is a symmetric function. (The exclusive or function, illustrated in Fig. 2.1, is an odd-parity function of two dimensions. The or and and functions of two dimensions are also symmetric.)

An important subclass of the symmetric functions is the class of voting functions (also called $m$ -of- $n$ functions). A $k$ -voting function has value 1 if and only if $k$ or more of its $n$ inputs has value 1. If $k = 1$ , a voting function is the same as an $n$ -sized clause; if $k = n$ , a voting function is the same as an $n$ -sized term; if $k = ( n + 1 ) / 2$ for $n$ odd or $k = 1 + n / 2$ for $n$ even, we have the majority function.

# 2.2.6 Linearly Separable Functions

The linearly separable functions are those that can be expressed as follows:

$$
f = \operatorname {t h r e s h} \left(\sum_ {i = 1} ^ {n} w _ {i} x _ {i}, \theta\right)
$$

where $w _ { i }$ , $i = 1 , \ldots , n$ , are real-valued numbers called weights, $\theta$ is a real-valued number called the threshold, and thresh $( \sigma , \theta )$ is 1 if $\sigma \geq \theta$ and 0 otherwise. (Note that the concept of linearly separable functions can be extended to non-Boolean inputs.) The $k$ -voting functions are all members of the class of linearly separable functions in which the weights all have unit value and the threshold depends on $k$ . Thus, terms and clauses are special cases of linearly separable functions.

A convenient way to write linearly separable functions uses vector notation:

$$
f = \mathrm {t h r e s h} (\mathbf {X} \cdot \mathbf {W}, \theta)
$$

where $\mathbf { X } = \left( x _ { 1 } , \ldots , x _ { n } \right)$ is an $n$ -dimensional vector of input variables, ${ \textbf { W } } =$ $( w _ { 1 } , \ldots , w _ { n } )$ is an $n$ -dimensional vector of weight values, and $\mathbf { X } \cdot \mathbf { W }$ is the dot (or inner) product of the two vectors. Input vectors for which $f$ has value 1 lie in a half-space on one side of (and on) a hyperplane whose orientation is normal to $\mathbf { W }$ and whose position (with respect to the origin) is determined by $\theta$ . We saw an example of such a separating plane in Fig. 1.6. With this idea in mind, it is easy to see that two of the functions in Fig. 2.1 are linearly separable, while two are not. Also note that the terms in Figs. 2.3 and 2.4 are linearly separable functions as evidenced by the separating planes shown.

There is no closed-form expression for the number of linearly separable functions of $n$ dimensions, but the following table gives the numbers for $n$ up to 6.

<table><tr><td>n</td><td>Boolean Functions</td><td>Linearly Separable Functions</td></tr><tr><td>1</td><td>4</td><td>4</td></tr><tr><td>2</td><td>16</td><td>14</td></tr><tr><td>3</td><td>256</td><td>104</td></tr><tr><td>4</td><td>65,536</td><td>1,882</td></tr><tr><td>5</td><td>≈4.3×109</td><td>94,572</td></tr><tr><td>6</td><td>≈1.8×1019</td><td>15,028,134</td></tr></table>

[Muroga, 1971] has shown that (for $n > 1$ ) there are no more than $2 ^ { n ^ { 2 } }$ linearly separable functions of $n$ dimensions. (See also [Winder, 1961, Winder, 1962].)

# 2.3 Summary

The diagram in Fig. 2.6 shows some of the set inclusions of the classes of Boolean functions that we have considered. We will be confronting these classes again in later chapters.

![](images/f83c50e97246fc3f6a5c7225064f5cc41ce98f684907c105b16b7464895ecca5.jpg)  
Figure 2.6: Classes of Boolean Functions

The sizes of the various classes are given in the following table (adapted from [Dietterich, 1990, page 262]):

# 2.4. BIBLIOGRAPHICAL AND HISTORICAL REMARKS

<table><tr><td>Class</td><td>Size of Class</td></tr><tr><td>terms</td><td>3n</td></tr><tr><td>clauses</td><td>3n</td></tr><tr><td>k-term DNF</td><td>2O(kn)</td></tr><tr><td>k-clause CNF</td><td>2O(kn)</td></tr><tr><td>k-DNF</td><td>2O(nk)</td></tr><tr><td>k-CNF</td><td>2O(nk)</td></tr><tr><td>k-DL</td><td>2O[nk]k log(n)]</td></tr><tr><td>lin sep</td><td>2O(n2)</td></tr><tr><td>DNF</td><td>22n</td></tr></table>

# 2.4 Bibliographical and Historical Remarks

To be added.

# Chapter 3

# Using Version Spaces for Learning

# 3.1 Version Spaces and Mistake Bounds

The first learning methods we present are based on the concepts of version spaces and version graphs. These ideas are most clearly explained for the case of Boolean function learning. Given an initial hypothesis set $\mathcal { H }$ (a subset of all Boolean functions) and the values of $f ( \mathbf { X } )$ for each $\mathbf { X }$ in a training set, $\Xi$ , the version space is that subset of hypotheses, $\mathcal { H } _ { v }$ , that is consistent with these values. A hypothesis, $h$ , is consistent with the values of $\mathbf { X }$ in $\Xi$ if and only if $h ( \mathbf { X } ) = f ( \mathbf { X } )$ for all $\mathbf { X }$ in $\Xi$ . We say that the hypotheses in $\mathcal { H }$ that are not consistent with the values in the training set are ruled out by the training set.

We could imagine (conceptually only!) that we have devices for implementing every function in $\mathcal { H }$ . An incremental training procedure could then be defined which presented each pattern in $\Xi$ to each of these functions and then eliminated those functions whose values for that pattern did not agree with its given value. At any stage of the process we would then have left some subset of functions that are consistent with the patterns presented so far; this subset is the version space for the patterns already presented. This idea is illustrated in Fig. 3.1.

Consider the following procedure for classifying an arbitrary input pattern, X: the pattern is put in the same class (0 or 1) as are the majority of the outputs of the functions in the version space. During the learning procedure, if this majority is not equal to the value of the pattern presented, we say a mistake is made, and we revise the version space accordingly—eliminating all those (majority of the) functions voting incorrectly. Thus, whenever a mistake is made, we rule out at least half of the functions remaining in the version space.

How many mistakes can such a procedure make? Obviously, we can make no more than $\log _ { 2 } ( | \mathscr { H } | )$ mistakes, where $| \mathcal { H } |$ is the number of hypotheses in the

![](images/d5762c98947b50406bd3666aa6e0ac1a5ff42c1404254194ea21d2faceded22c.jpg)  
Figure 3.1: Implementing the Version Space

original hypothesis set, $\mathcal { H }$ . (Note, though, that the number of training patterns seen before this maximum number of mistakes is made might be much greater.) This theoretical (and very impractical!) result (due to [Littlestone, 1988]) is an example of a mistake bound—an important concept in machine learning theory. It shows that there must exist a learning procedure that makes no more mistakes than this upper bound. Later, we’ll derive other mistake bounds.

As a special case, if our bias was to limit $\mathcal { H }$ to terms, we would make no more than $\log _ { 2 } ( 3 ^ { n } ) = n \log _ { 2 } ( 3 ) = 1 . 5 8 5 n$ mistakes before exhausting the version space. This result means that if $f$ were a term, we would make no more than $1 . 5 8 5 n$ mistakes before learning $f$ , and otherwise we would make no more than that number of mistakes before being able to decide that $f$ is not a term.

Even if we do not have sufficient training patterns to reduce the version space to a single function, it may be that there are enough training patterns to reduce the version space to a set of functions such that most of them assign the same values to most of the patterns we will see henceforth. We could select one of the remaining functions at random and be reasonably assured that it will generalize satisfactorily. We next discuss a computationally more feasible method for representing the version space.

# 3.2 Version Graphs

Boolean functions can be ordered by generality. A Boolean function, $f _ { 1 }$ , is more general than a function, $f _ { 2 }$ , (and $f _ { 2 }$ is more specific than $f _ { 1 }$ ), if $f _ { 1 }$ has value 1 for all of the arguments for which $f _ { 2 }$ has value 1, and $f _ { 1 } \neq f _ { 2 }$ . For example, $x _ { 3 }$ is more general than $x _ { 2 } x _ { 3 }$ but is not more general than $x _ { 3 } + x _ { 2 }$ .

We can form a graph with the hypotheses, $\{ h _ { i } \}$ , in the version space as nodes. A node in the graph, $h _ { i }$ , has an arc directed to node, $h _ { j }$ , if and only if $h _ { j }$ is more general than $h _ { i }$ . We call such a graph a version graph. In Fig. 3.2, we show an example of a version graph over a 3-dimensional input space for hypotheses restricted to terms (with none of them yet ruled out).

![](images/8df3a29f74652728cfd4d2755a41feff530f1d5491043f2767c7210fe9480b42.jpg)  
Figure 3.2: A Version Graph for Terms

That function, denoted here by “1,” which has value 1 for all inputs, corresponds to the node at the top of the graph. (It is more general than any other term.) Similarly, the function “0” is at the bottom of the graph. Just below “1” is a row of nodes corresponding to all terms having just one literal, and just below them is a row of nodes corresponding to terms having two literals, and

so on. There are $3 ^ { 3 } = 2 7$ functions altogether (the function “0,” included in the graph, is technically not a term). To make our portrayal of the graph less cluttered only some of the arcs are shown; each node in the actual graph has an arc directed to all of the nodes above it that are more general.

We use this same example to show how the version graph changes as we consider a set of labeled samples in a training set, $\Xi$ . Suppose we first consider the training pattern (1, 0, 1) with value 0. Some of the functions in the version graph of Fig. 3.2 are inconsistent with this training pattern. These ruled out nodes are no longer in the version graph and are shown shaded in Fig. 3.3. We also show there the three-dimensional cube representation in which the vertex (1, 0, 1) has value 0.

![](images/0614bc262a23b03104a8532dad03b413700f06e065128c0147c6f521132d825c.jpg)  
Figure 3.3: The Version Graph Upon Seeing (1, 0, 1)

In a version graph, there are always a set of hypotheses that are maximally general and a set of hypotheses that are maximally specific. These are called the general boundary set (gbs) and the specific boundary set (sbs), respectively. In Fig. 3.4, we have the version graph as it exists after learning that (1,0,1) has value 0 and (1, 0, 0) has value 1. The gbs and sbs are shown.

![](images/b074759a78b3654df2614fdc5a4c649e33a2d8808649dce80e9c97406dcecb1a.jpg)  
Figure 3.4: The Version Graph Upon Seeing (1, 0, 1) and (1, 0, 0)

Boundary sets are important because they provide an alternative to representing the entire version space explicitly, which would be impractical. Given only the boundary sets, it is possible to determine whether or not any hypothesis (in the prescribed class of Boolean functions we are using) is a member or not of the version space. This determination is possible because of the fact that any member of the version space (that is not a member of one of the boundary sets) is more specific than some member of the general boundary set and is more general than some member of the specific boundary set.

If we limit our Boolean functions that can be in the version space to terms, it is a simple matter to determine maximally general and maximally specific functions (assuming that there is some term that is in the version space). A maximally specific one corresponds to a subface of minimal dimension that contains all the members of the training set labelled by a 1 and no members labelled by a 0. A maximally general one corresponds to a subface of maximal dimension that contains all the members of the training set labelled by a 1 and no members labelled by a 0. Looking at Fig. 3.4, we see that the subface of minimal dimension that contains (1, 0, 0) but does not contain (1, 0, 1) is just the vertex (1, 0, 0) itself—corresponding to the function $x _ { 1 } \overline { { x _ { 2 } } } \ \overline { { x _ { 3 } } }$ . The subface

of maximal dimension that contains (1, 0, 0) but does not contain (1, 0, 1) is the bottom face of the cube—corresponding to the function $x _ { 3 }$ . In Figs. 3.2 through 3.4 the sbs is always singular. Version spaces for terms always have singular specific boundary sets. As seen in Fig. 3.3, however, the gbs of a version space for terms need not be singular.

# 3.3 Learning as Search of a Version Space

[To be written. Relate to term learning algorithm presented in Chapter Two. Also discuss best-first search methods. See Pat Langley’s example using “pseudo-cells” of how to generate and eliminate hypotheses.]

Selecting a hypothesis from the version space can be thought of as a search problem. One can start with a very general function and specialize it through various specialization operators until one finds a function that is consistent (or adequately so) with a set of training patterns. Such procedures are usually called top-down methods. Or, one can start with a very special function and generalize it—resulting in bottom-up methods. We shall see instances of both styles of learning in this book.

# 3.4 The Candidate Elimination Method

The candidate elimination method, is an incremental method for computing the boundary sets. Quoting from [Hirsh, 1994, page 6]:

“The candidate-elimination algorithm manipulates the boundary-set representation of a version space to create boundary sets that represent a new version space consistent with all the previous instances plus the new one. For a positive exmple the algorithm generalizes the elements of the [sbs] as little as possible so that they cover the new instance yet remain consistent with past data, and removes those elements of the [gbs] that do not cover the new instance. For a negative instance the algorithm specializes elements of the [gbs] so that they no longer cover the new instance yet remain consistent with past data, and removes from the [sbs] those elements that mistakenly cover the new, negative instance.”

The method uses the following definitions (adapted from [Genesereth & Nilsson, 1987]):

• a hypothesis is called sufficient if and only if it has value 1 for all training samples labeled by a 1,   
• a hypothesis is called necessary if and only if it has value 0 for all training samples labeled by a 0.

Compare this view of top-down versus bottom-up with the divide-and-conquer and the covering (or AQ) methods of decision-tree induction.

Here is how to think about these definitions: A hypothesis implements a sufficient condition that a training sample has value 1 if the hypothesis has value 1 for all of the positive instances; a hypothesis implements a necessary condition that a training sample has value 1 if the hypothesis has value 0 for all of the negative instances. A hypothesis is consistent with the training set (and thus is in the version space) if and only if it is both sufficient and necessary.

We start (before receiving any members of the training set) with the function “0” as the singleton element of the specific boundary set and with the function “1” as the singleton element of the general boundary set. Upon receiving a new labeled input vector, the boundary sets are changed as follows:

a. If the new vector is labelled with a 1:

The new general boundary set is obtained from the previous one by excluding any elements in it that are not sufficient. (That is, we exclude any elements that have value 0 for the new vector.)

The new specific boundary set is obtained from the previous one by replacing each element, $h _ { i }$ , in it by all of its least generalizations.

The hypothesis $h _ { g }$ is a least generalization of $h$ if and only if: a) $h$ is more specific than $h _ { g }$ , b) $h _ { g }$ is sufficient, c) no function (including $h$ ) that is more specific than $h _ { g }$ is sufficient, and d) $h _ { g }$ is more specific than some member of the new general boundary set. It might be that $h _ { g } = h$ . Also, least generalizations of two different functions in the specific boundary set may be identical.

b. If the new vector is labelled with a 0:

The new specific boundary set is obtained from the previous one by excluding any elements in it that are not necessary. (That is, we exclude any elements that have value 1 for the new vector.)

The new general boundary set is obtained from the previous one by replacing each element, $h _ { i }$ , in it by all of its least specializations.

The hypothesis $h _ { s }$ is a least specialization of $h$ if and only if: a) $h$ is more general than $h _ { s }$ , b) $h _ { s }$ is necessary, c) no function (including $h$ ) that is more general than $h _ { s }$ is necessary, and d) $h _ { s }$ is more general than some member of the new specific boundary set. Again, it might be that $h _ { s } = h$ , and least specializations of two different functions in the general boundary set may be identical.

As an example, suppose we present the vectors in the following order:

<table><tr><td>vector</td><td>label</td></tr><tr><td>(1, 0, 1)</td><td>0</td></tr><tr><td>(1, 0, 0)</td><td>1</td></tr><tr><td>(1, 1, 1)</td><td>0</td></tr><tr><td>(0, 0, 1)</td><td>0</td></tr></table>

We start with general boundary set, “1”, and specific boundary set, “0.” After seeing the first sample, (1, 0, 1), labeled with a 0, the specific boundary set stays at “0” (it is necessary), and we change the general boundary set to $\left\{ { \overline { { x _ { 1 } } } } , x _ { 2 } , { \overline { { x _ { 3 } } } } \right\}$ . Each of the functions, $\overline { { x _ { 1 } } }$ , $x _ { 2 }$ , and $\overline { { x _ { 3 } } }$ , are least specializations of “1” (they are necessary, “1” is not, they are more general than “0”, and there are no functions that are more general than they and also necessary).

Then, after seeing $( 1 , \ 0 , \ 0 )$ , labeled with a 1, the general boundary set changes to $\left\{ { \overline { { x _ { 3 } } } } \right\}$ (because $\overline { { x _ { 1 } } }$ and $x _ { 2 }$ are not sufficient), and the specific boundary set is changed to $\{ x _ { 1 } \overline { { x _ { 2 } } } ~ \overline { { x _ { 3 } } } \}$ . This single function is a least generalization of “0” (it is sufficient, “ $0$ ” is more specific than it, no function (including “0”) that is more specific than it is sufficient, and it is more specific than some member of the general boundary set.

When we see (1, 1, 1), labeled with a 0, we do not change the specific boundary set because its function is still necessary. We do not change the general boundary set either because $\overline { { x _ { 3 } } }$ is still necessary.

Finally, when we see (0, 0, 1), labeled with a 0, we do not change the specific boundary set because its function is still necessary. We do not change the general boundary set either because $\overline { { x _ { 3 } } }$ is still necessary.

# 3.5 Bibliographical and Historical Remarks

The concept of version spaces and their role in learning was first investigated by Tom Mitchell [Mitchell, 1982]. Although these ideas are not used in practical machine learning procedures, they do provide insight into the nature of hypothesis selection. In order to accomodate noisy data, version spaces have been generalized by [Hirsh, 1994] to allow hypotheses that are not necessarily consistent with the training set.

Maybe I’ll put in an example of a version graph for non-Boolean functions.

More to be added.

# Chapter 4

# Neural Networks

In chapter two we defined several important subsets of Boolean functions. Suppose we decide to use one of these subsets as a hypothesis set for supervised function learning. We next have the question of how best to implement the function as a device that gives the outputs prescribed by the function for arbitrary inputs. In this chapter we describe how networks of non-linear elements can be used to implement various input-output functions and how they can be trained using supervised learning methods.

Networks of non-linear elements, interconnected through adjustable weights, play a prominent role in machine learning. They are called neural networks because the non-linear elements have as their inputs a weighted sum of the outputs of other elements—much like networks of biological neurons do. These networks commonly use the threshold element which we encountered in chapter two in our study of linearly separable Boolean functions. We begin our treatment of neural nets by studying this threshold element and how it can be used in the simplest of all networks, namely ones composed of a single threshold element.

# 4.1 Threshold Logic Units

# 4.1.1 Definitions and Geometry

Linearly separable (threshold) functions are implemented in a straightforward way by summing the weighted inputs and comparing this sum to a threshold value as shown in Fig. 4.1. This structure we call a threshold logic unit (TLU). Its output is 1 or 0 depending on whether or not the weighted sum of its inputs is greater than or equal to a threshold value, $\theta$ . It has also been called an Adaline (for adaptive linear element) [Widrow, 1962, Widrow & Lehr, 1990], an LTU (linear threshold unit), a perceptron, and a neuron. (Although the word “perceptron” is often used nowadays to refer to a single TLU, Rosenblatt originally defined it as a class of networks of threshold elements [Rosenblatt, 1958].)

![](images/3456c77e52c2d913dfca5682b617e492679be69557303b99b2eb7c1b8b5f5391.jpg)  
Figure 4.1: A Threshold Logic Unit (TLU)

The $n$ -dimensional feature or input vector is denoted by $\mathbf { X } = \left( x _ { 1 } , \ldots , x _ { n } \right)$ . When we want to distinguish among different feature vectors, we will attach subscripts, such as $\mathbf { X } _ { i }$ . The components of $\mathbf { X }$ can be any real-valued numbers, but we often specialize to the binary numbers 0 and 1. The weights of a TLU are represented by an $n$ -dimensional weight vector, $\mathbf { W } \ = \ ( w _ { 1 } , \ldots , w _ { n } )$ . Its The TLU has output 1 if components are real-valued numbers (but we sometimes specialize to integers). $\textstyle \sum _ { i = 1 } ^ { n } x _ { i } w _ { i } \ \geq \ \theta$ ; otherwise it has output 0. The weighted sum that is calculated by the TLU can be simply represented as a vector dot product, X•W. (If the pattern and weight vectors are thought of as “column” vectors, this dot product is then sometimes written as $\mathbf { X } ^ { t } \mathbf { W }$ , where the “row” vector $\mathbf { X } ^ { t }$ is the transpose of $\mathbf { X }$ .) Often, the threshold, $\theta$ , of the TLU is fixed at 0; in that case, arbitrary thresholds are achieved by using $( n + 1 )$ - dimensional “augmented” vectors, $\mathbf { Y }$ , and $\mathbf { V }$ , whose first $n$ components are the same as those of $\mathbf { X }$ and $\mathbf { W }$ , respectively. The $( n + 1 )$ -st component, $x _ { n + 1 }$ , of the augmented feature vector, $\mathbf { Y }$ , always has value 1; the $( n + 1 )$ -st component, $w _ { n + 1 }$ , of the augmented weight vector, $\mathbf { V }$ , is set equal to the negative of the desired threshold value. (When we want to emphasize the use of augmented vectors, we’ll use the Y,V notation; however when the context of the discussion makes it clear about what sort of vectors we are talking about, we’ll lapse back into the more familiar $\mathbf { X } , \mathbf { W }$ notation.) In the Y,V notation, the TLU has an output of 1 if $\mathbf { Y } { \bullet \mathbf { V } } \geq 0$ . Otherwise, the output is 0.

We can give an intuitively useful geometric description of a TLU. A TLU divides the input space by a hyperplane as sketched in Fig. 4.2. The hyperplane is the boundary between patterns for which $\mathbf { X } \bullet \mathbf { W } + w _ { n + 1 } > 0$ and patterns for which $\mathbf { \Delta x _ { 0 } W } + w _ { n + 1 } < \mathbf { \Gamma } 0$ . Thus, the equation of the hyperplane itself is $\mathbf { X } \bullet \mathbf { W } + w _ { n + 1 } = 0$ . The unit vector that is normal to the hyperplane is $\begin{array} { r } { \mathbf { n } = \frac { \mathbf { W } } { | \mathbf { W } | } } \end{array}$ , where $| \mathbf { W } | = \bigtriangledown \big ( w _ { 1 } ^ { 2 } + \ldots + w _ { n } ^ { 2 } \big )$ is the length of the vector $\mathbf { W }$ . (The normal

form of the hyperplane eqhyperplane to the origin is n is , an $\begin{array} { r } { \mathbf { X } \bullet \mathbf { n } + \frac { \mathbf { W } } { | \mathbf { W } | } = 0 } \end{array}$ .) The distance fromom an arbitrary point, $\frac { \boldsymbol { w } _ { n + 1 } } { | \mathbf { W } | }$ $\mathbf { X }$ to the hyperplane is X•W+wn+1W . When the distance from the hyperplane to the $\frac { \mathbf { X 6 } \mathbf { \bullet } \mathbf { W } + w _ { n + 1 } } { | \mathbf { W } | }$ origin is negative (that is, when $w _ { n + 1 } < 0$ ), then the origin is on the negative side of the hyperplane (that is, the side for which $\mathbf { X } \bullet \mathbf { W } + w _ { n + 1 } < 0$ ).

Equations of hyperplane:

$$
\mathbf {X} \cdot \mathbf {W} + w _ {n + 1} = 0
$$

![](images/14fa61ae3a1d6c3432614c8a8994274201e91b730615c0b88cdac6508e257c47.jpg)  
Figure 4.2: TLU Geometry

Adjusting the weight vector, W, changes the orientation of the hyperplane; adjusting $w _ { n + 1 }$ changes the position of the hyperplane (relative to the origin). Thus, training of a TLU can be achieved by adjusting the values of the weights. In this way the hyperplane can be moved so that the TLU implements different (linearly separable) functions of the input.

# 4.1.2 Special Cases of Linearly Separable Functions

# Terms

Any term of size $k$ can be implemented by a TLU with a weight from each of those inputs corresponding to variables occurring in the term. A weight of +1 is used from an input corresponding to a positive literal, and a weight of $^ { - 1 }$ is used from an input corresponding to a negative literal. (Literals not mentioned in the term have weights of zero—that is, no connection at all—from their inputs.) The threshold, $\theta$ , is set equal to $k _ { p } - 1 / 2$ , where $k _ { p }$ is the number of positive literals in the term. Such a TLU implements a hyperplane boundary that is

parallel to a subface of dimension $( n - k )$ of the unit hypercube. We show a three-dimensional example in Fig. 4.3. Thus, linearly separable functions are a superset of terms.

![](images/2a717f63b238a54d2983a22fca4d28a3d101cdf4ccab7075e3126c8bbb91f2b4.jpg)  
Figure 4.3: Implementing a Term

# Clauses

The negation of a clause is a term. For example, the negation of the clause $f = x _ { 1 } + x _ { 2 } + x _ { 3 }$ is the term ${ \overline { { f } } } \ = { \overline { { x _ { 1 } } } } \ { \overline { { x _ { 2 } } } } \ { \overline { { x _ { 3 } } } }$ . A hyperplane can be used to implement this term. If we “invert” the hyperplane, it will implement the clause instead. Inverting a hyperplane is done by multiplying all of the TLU weights—even $w _ { n + 1 }$ —by $^ { - 1 }$ . This process simply changes the orientation of the hyperplane—flipping it around by 180 degrees and thus changing its “positive side.” Therefore, linearly separable functions are also a superset of clauses. We show an example in Fig. 4.4.

# 4.1.3 Error-Correction Training of a TLU

There are several procedures that have been proposed for adjusting the weights of a TLU. We present next a family of incremental training procedures with parameter $c$ . These methods make adjustments to the weight vector only when the TLU being trained makes an error on a training pattern; they are called error-correction procedures. We use augmented feature and weight vectors in describing them.

a. We start with a finite training set, $\Xi$ , of vectors, $\mathbf { Y } _ { i }$ , and their binary labels.

$$
f = x _ {1} + x _ {2} + x _ {3}
$$

![](images/2ed9f5652e1d927a2a0831a88010e20a463aa540950b1f1e41765808a758c0b5.jpg)  
Figure 4.4: Implementing a Clause

b. Compose an infinite training sequence, $\Sigma$ , of vectors from $\Xi$ and their labels such that each member of $\Xi$ occurs infinitely often in $\Sigma$ . Set the initial weight values of an TLU to arbitrary values.   
c. Repeat forever:

Present the next vector, $\mathbf { Y } _ { i }$ , in $\Sigma$ to the TLU and note its response.

(a) If the TLU responds correctly, make no change in the weight vector.   
(b) If $\mathbf { Y } _ { i }$ is supposed to produce an output of 0 and produces an output of 1 instead, modify the weight vector as follows:

$$
\mathbf {V} \leftarrow \mathbf {V} - c _ {i} \mathbf {Y} _ {i}
$$

where $c _ { i }$ is a positive real number called the learning rate parameter (whose value is differerent in different instances of this family of procedures and may depend on $i$ ).

Note that after this adjustment the new dot product will be $( \mathbf { V } -$ $c _ { i } \mathbf { Y } _ { i } ) \bullet \mathbf { Y } _ { i } = \mathbf { V } \bullet \mathbf { Y } _ { i } - c _ { i } \mathbf { Y } _ { i } \bullet \mathbf { Y } _ { i }$ , which is smaller than it was before the weight adjustment.

(c) If $\mathbf { Y } _ { i }$ is supposed to produce an output of 1 and produces an output of 0 instead, modify the weight vector as follows:

$$
\mathbf {V} \leftarrow \mathbf {V} + c _ {i} \mathbf {Y} _ {i}
$$

In this case, the new dot product will be $( \mathbf { V } + c _ { i } \mathbf { Y } _ { i } ) \bullet \mathbf { Y } _ { i } = \mathbf { V } \bullet \mathbf { Y } _ { i } +$ $c _ { i } \mathbf { Y } _ { i } { \bullet \mathbf { Y } _ { i } }$ , which is larger than it was before the weight adjustment.

Note that all three of these cases can be combined in the following rule:

$$
\mathbf {V} \leftarrow \mathbf {V} + c _ {i} (d _ {i} - f _ {i}) \mathbf {Y} _ {i}
$$

where $d _ { i }$ is the desired response (1 or 0) for $\mathbf { Y } _ { i }$ , and $f _ { i }$ is the actual response (1 or $0$ ) for $\mathbf { Y } _ { i }$ .]

Note also that because the weight vector $\mathbf { V }$ now includes the thresh-$w _ { n + 1 }$ old component, the threshold of the TLU is also changed by these adjustments.

We identify two versions of this procedure:

1) In the fixed-increment procedure, the learning rate parameter, $c _ { i }$ , is the same fixed, positive constant for all $_ i$ . Depending on the value of this constant, the weight adjustment may or may not correct the response to an erroneously classified feature vector.   
2) In the fractional-correction procedure, the parameter $c _ { i }$ is set to $\lambda \frac { \mathbf { Y _ { i } \bullet V } } { \mathbf { Y _ { i } \bullet Y _ { i } } }$ , where $\mathbf { V }$ is the weight vector before it is changed. Note that if $\lambda \ : = \ : 0$ , no correction takes place at all. If $\lambda = 1$ , the correction is just sufficient to make $\mathbf { Y _ { i } \bullet V _ { } } = 0$ . If $\lambda > 1$ , the error will be corrected.

It can be proved that if there is some weight vector, $\mathbf { V }$ , that produces a correct output for all of the feature vectors in $\Xi$ , then after a finite number of feature vector presentations, the fixed-increment procedure will find such a weight vector and thus make no more weight changes. The same result holds for the fractional-correction procedure if $1 < \lambda \leq 2$ .

For additional background, proofs, and examples of error-correction procedures, see [Nilsson, 1990].

# 4.1.4 Weight Space

We can give an intuitive idea about how these procedures work by considering what happens to the augmented weight vector in “weight space” as corrections are made. We use augmented vectors in our discussion here so that the threshold function compares the dot product, $\mathbf { Y } _ { i } { \bullet } \mathbf { V }$ , against a threshold of 0. A particular weight vector, $\mathbf { V }$ , then corresponds to a point in $( n + 1 )$ -dimensional weight space. Now, for any pattern vector, $\mathbf { Y } _ { i }$ , consider the locus of all points in weight space corresponding to weight vectors yielding $\mathbf { Y } _ { i } \bullet \mathbf { V } = 0$ . This locus is a hyperplane passing through the origin of the $( n + 1 )$ -dimensional space. Each pattern vector will have such a hyperplane corresponding to it. Weight points in one of the half-spaces defined by this hyperplane will cause the corresponding pattern to yield a dot product less than 0, and weight points in the other halfspace will cause the corresponding pattern to yield a dot product greater than 0.

We show a schematic representation of such a weight space in Fig. 4.5. There are four pattern hyperplanes, 1, 2, 3, 4 , corresponding to patterns $\mathbf { Y } _ { 1 }$

See [Maass & Tur´an, 1994] for a hyperplane-finding procedure that makes no more than $O ( n ^ { 2 } \log n )$ mistakes.

$\mathbf { Y } _ { 2 }$ , ${ \bf Y } _ { 3 }$ , $\mathbf { Y } _ { 4 }$ , respectively, and we indicate by an arrow the half-space for each in which weight vectors give dot products greater than 0. Suppose we wanted weight values that would give positive responses for patterns $\mathbf { Y } _ { 1 }$ , ${ \bf Y } _ { 3 }$ , and $\mathbf { Y } _ { 4 }$ , and a negative response for pattern $\mathbf { Y } _ { 2 }$ . The weight point, $\mathbf { V }$ , indicated in the figure is one such set of weight values.

![](images/c95321c482004eaf47c9360fdecb4d6b89e4199ae54eaf2b63faca5057c034a8.jpg)  
Figure 4.5: Weight Space

The question of whether or not there exists a weight vector that gives desired responses for a given set of patterns can be given a geometric interpretation. To do so involves reversing the “polarity” of those hyperplanes corresponding to patterns for which a negative response is desired. If we do that for our example above, we get the weight space diagram shown in Fig. 4.6.

![](images/68abc705d842c0bdb40cfead9781056bd1aa92901c795009a5887b08102f7174.jpg)  
Figure 4.6: Solution Region in Weight Space

If a weight vector exists that correctly classifies a set of patterns, then the half-spaces defined by the correct responses for these patterns will have a nonempty intersection, called the solution region. The solution region will be a “hyper-wedge” region whose vertex is at the origin of weight space and whose cross-section increases with increasing distance from the origin. This region is shown shaded in Fig. 4.6. (The boxed numbers show, for later purposes, the number of errors made by weight vectors in each of the regions.) The fixed-increment error-correction procedure changes a weight vector by moving it normal to any pattern hyperplane for which that weight vector gives an incorrect response. Suppose in our example that we present the patterns in the sequence $\mathbf { Y } _ { 1 }$ , $\mathbf { Y } _ { 2 }$ , ${ \bf Y } _ { 3 }$ , $\mathbf { Y } _ { 4 }$ , and start the process with a weight point $\mathbf { V } _ { 1 }$ , as shown in Fig. 4.7. Starting at $\mathbf { V } _ { 1 }$ , we see that it gives an incorrect response for pattern $\mathbf { Y } _ { 1 }$ , so we move $\mathbf { V } _ { 1 }$ to $\mathbf { V } _ { 2 }$ in a direction normal to plane 1. (That is what adding $\mathbf { Y } _ { 1 }$ to $\mathbf { V } _ { 1 }$ does.) $\mathbf { Y } _ { 2 }$ gives an incorrect response for pattern $\mathbf { Y } _ { 2 }$ , and so on. Ultimately, the responses are only incorrect for planes bounding the solution region. Some of the subsequent corrections may overshoot the solution region, but eventually we work our way out far enough in the solution region that corrections (for a fixed increment size) take us within it. The proofs for convergence of the fixed-increment rule make this intuitive argument precise.

![](images/3348fbd64b97b8d257d841f830f701748afedff58eefb205dfa42b06f68efa69.jpg)  
Figure 4.7: Moving Into the Solution Region

# 4.1.5 The Widrow-Hoff Procedure

The Widrow-Hoff procedure (also called the LMS or the delta procedure) attempts to find weights that minimize a squared-error function between the pattern labels and the dot product computed by a TLU. For this purpose, the pattern labels are assumed to be either $+ 1$ or $^ { - 1 }$ (instead of 1 or $0$ ). The

squared error for a pattern, $\mathbf { X } _ { i }$ , with label $d _ { i }$ (for desired output) is:

$$
\varepsilon_ {i} = (d _ {i} - \sum_ {j = 1} ^ {n + 1} x _ {i j} w _ {j}) ^ {2}
$$

where $x _ { i j }$ is the $j$ -th component of $\mathbf { X } _ { i }$ . The total squared error (over all patterns in a training set, $\Xi$ , containing $m$ patterns) is then:

$$
\varepsilon = \sum_ {i = 1} ^ {m} (d _ {i} - \sum_ {j = 1} ^ {n + 1} x _ {i j} w _ {j}) ^ {2}
$$

We want to choose the weights $w _ { j }$ to minimize this squared error. One way to find such a set of weights is to start with an arbitrary weight vector and move it along the negative gradient of $\varepsilon$ as a function of the weights. Since $\varepsilon$ is quadratic in the $w _ { j }$ , we know that it has a global minimum, and thus this steepest descent procedure is guaranteed to find the minimum. Each component of the gradient is the partial derivative of $\varepsilon$ with respect to one of the weights. One problem with taking the partial derivative of $\varepsilon$ is that $\varepsilon$ depends on all the input vectors in $\Xi$ . Often, it is preferable to use an incremental procedure in which we try the TLU on just one element, $\mathbf { X } _ { i }$ , of $\Xi$ at a time, compute the gradient of the singlepattern squared error, $\varepsilon _ { i }$ , make the appropriate adjustment to the weights, and then try another member of $\Xi$ . Of course, the results of the incremental version can only approximate those of the batch one, but the approximation is usually quite effective. We will be describing the incremental version here.

The $j$ -th component of the gradient of the single-pattern error is:

$$
\frac {\partial \varepsilon_ {i}}{\partial w _ {j}} = - 2 (d _ {i} - \sum_ {j = 1} ^ {n + 1} x _ {i j} w _ {j}) x _ {i j}
$$

An adjustment in the direction of the negative gradient would then change each weight as follows:

$$
w _ {j} \longleftarrow w _ {j} + c _ {i} \left(d _ {i} - f _ {i}\right) x _ {i j}
$$

where $\begin{array} { r } { f _ { i } = \sum _ { j = 1 } ^ { n + 1 } x _ { i j } w _ { j } } \end{array}$ , and $c _ { i }$ governs the size of the adjustment. The entire weight vector (in augmented, or $\mathbf { V }$ , notation) is thus adjusted according to the following rule:

$$
\mathbf {V} \longleftarrow \mathbf {V} + c _ {i} (d _ {i} - f _ {i}) \mathbf {Y} _ {i}
$$

where, as before, $\mathbf { Y } _ { i }$ is the $i$ -th augmented pattern vector.

The Widrow-Hoff procedure makes adjustments to the weight vector whenever the dot product itself, $\mathbf { Y } _ { i } { \bullet } \mathbf { V }$ , does not equal the specified desired target

Examples of training curves for TLU’s; performance on training set; performance on test set; cumulative number of corrections.

value, $d _ { i }$ (which is either 1 or $^ { - 1 }$ ). The learning-rate factor, $c _ { i }$ , might decrease with time toward 0 to achieve asymptotic convergence. The Widrow-Hoff formula for changing the weight vector has the same form as the standard fixed-increment error-correction formula. The only difference is that $f _ { i }$ is the thresholded response of the TLU in the error-correction case while it is the dot product itself for the Widrow-Hoff procedure.

Finding weight values that give the desired dot products corresponds to solving a set of linear equalities, and the Widrow-Hoff procedure can be interpreted as a descent procedure that attempts to minimize the mean-squared-error between the actual and desired values of the dot product. (For more on Widrow-Hoff and other related procedures, see [Duda & Hart, 1973, pp. 151ff].)

# 4.1.6 Training a TLU on Non-Linearly-Separable Training Sets

When the training set is not linearly separable (perhaps because of noise or perhaps inherently), it may still be desired to find a “best” separating hyperplane. Typically, the error-correction procedures will not do well on nonlinearly-separable training sets because they will continue to attempt to correct inevitable errors, and the hyperplane will never settle into an acceptable place.

Several methods have been proposed to deal with this case. First, we might use the Widrow-Hoff procedure, which (although it will not converge to zero error on non-linearly separable problems) will give us a weight vector that minimizes the mean-squared-error. A mean-squared-error criterion often gives unsatisfactory results, however, because it prefers many small errors to a few large ones. As an alternative, error correction with a continuous decrease toward zero of the value of the learning rate constant, $c$ , will result in ever decreasing changes to the hyperplane. Duda [Duda, 1966] has suggested keeping track of the average value of the weight vector during error correction and using this average to give a separating hyperplane that performs reasonably well on non-linearly-separable problems. Gallant [Gallant, 1986] proposed what he called the “pocket algorithm.” As described in [Hertz, Krogh, & Palmer, 1991, p. 160]:

. . . the pocket algorithm . . . consists simply in storing (or “putting in your pocket”) the set of weights which has had the longest unmodified run of successes so far. The algorithm is stopped after some chosen time t . . .

After stopping, the weights in the pocket are used as a set that should give a small number of errors on the training set. Error-correction proceeds as usual with the ordinary set of weights.

# 4.2 Linear Machines

The natural generalization of a (two-category) TLU to an $R$ -category classifier is the structure, shown in Fig. 4.8, called a linear machine. Here, to use more

Also see methods proposed by [John, 1995] and by [Marchand & Golea, 1993]. The latter is claimed to outperform the pocket algorithm.

familiar notation, the Ws and $\mathbf { X }$ are meant to be augmented vectors (with an (n+1)-st component). Such a structure is also sometimes called a “competitive” net or a “winner-take-all” net. The output of the linear machine is one of the numbers, $\{ 1 , \ldots , R \}$ , corresponding to which dot product is largest. Note that when $R = 2$ , the linear machine reduces to a TLU with weight vector $\mathbf { W } = ( \mathbf { W } _ { 1 } - \mathbf { W } _ { 2 } )$ .

![](images/25781ce47e792b681df0fa95ba8ee80e2e7b303bb14fb66fcc0b9e5bc2c227fd.jpg)  
Figure 4.8: A Linear Machine

The diagram in Fig. 4.9 shows the character of the regions in a 2-dimensional space created by a linear machine for $R = 5$ . In $n$ dimensions, every pair of regions is either separated by a section of a hyperplane or is non-adjacent.

![](images/d5489b2eeda0af1d39d6ea82ad1343b7eab781c99b5712db4fe658bb8d62295c.jpg)  
Figure 4.9: Regions For a Linear Machine

To train a linear machine, there is a straightforward generalization of the 2-category error-correction rule. Assemble the patterns in the training set into a sequence as before.

a. If the machine classifies a pattern correctly, no change is made to any of

the weight vectors.

b. If the machine mistakenly classifies a category $u$ pattern, $\mathbf { X } _ { i }$ , in category $\boldsymbol { v }$ ( $u \neq v$ ), then:

$$
\mathbf {W} _ {u} \leftarrow \mathbf {W} _ {u} + c _ {i} \mathbf {X} _ {i}
$$

and

$$
\mathbf {W} _ {v} \leftarrow \mathbf {W} _ {v} - c _ {i} \mathbf {X} _ {i}
$$

and all other weight vectors are not changed.

This correction increases the value of the $u$ -th dot product and decreases the value of the $\boldsymbol { v }$ -th dot product. Just as in the 2-category fixed increment procedure, this procedure is guaranteed to terminate, for constant $c _ { i }$ , if there exists weight vectors that make correct separations of the training set. Note that when $R = 2$ , this procedure reduces to the ordinary TLU error-correction procedure. A proof that this procedure terminates is given in [Nilsson, 1990, pp. 88-90] and in [Duda & Hart, 1973, pp. 174-177].

# 4.3 Networks of TLUs

# 4.3.1 Motivation and Examples

# Layered Networks

To classify correctly all of the patterns in non-linearly-separable training sets requires separating surfaces more complex than hyperplanes. One way to achieve more complex surfaces is with networks of TLUs. Consider, for example, the 2- dimensional, even parity function, $f = x _ { 1 } x _ { 2 } + { \overline { { x _ { 1 } } } } { \overline { { x _ { 2 } } } }$ . No single line through the 2-dimensional square can separate the vertices (1,1) and (0,0) from the vertices (1,0) and (0,1)—the function is not linearly separable and thus cannot be implemented by a single TLU. But, the network of three TLUs shown in Fig. 4.10 does implement this function. In the figure, we show the weight values along input lines to each TLU and the threshold value inside the circle representing the TLU.

The function implemented by a network of TLUs depends on its topology as well as on the weights of the individual TLUs. Feedforward networks have no cycles; in a feedforward network no TLU’s input depends (through zero or more intermediate TLUs) on that TLU’s output. (Networks that are not feedforward are called recurrent networks). If the TLUs of a feedforward network are arranged in layers, with the elements of layer $j$ receiving inputs only from TLUs in layer $j - 1$ , then we say that the network is a layered, feedforward

![](images/8e058977e343166900aab9585dce436814fda1ccd10d871f054108ccd3afbfdd.jpg)  
Figure 4.10: A Network for the Even Parity Function

network. The network shown in Fig. 4.10 is a layered, feedforward network having two layers (of weights). (Some people count the layers of TLUs and include the inputs as a layer also; they would call this network a three-layer network.) In general, a feedforward, layered network has the structure shown in Fig. 4.11. All of the TLUs except the “output” units are called hidden units (they are “hidden” from the output).

![](images/c3e751be56dcbcbc007f7527e33cdb49ed98aa343e5bd8dc1da755bbaffd727f.jpg)  
Figure 4.11: A Layered, Feedforward Network

# Implementing DNF Functions by Two-Layer Networks

We have already defined $k$ -term DNF functions—they are DNF functions having $k$ terms. A $k$ -term DNF function can be implemented by a two-layer network with $k$ units in the hidden layer—to implement the $k$ terms—and one output unit to implement the disjunction of these terms. Since any Boolean function has a DNF form, any Boolean function can be implemented by some two-layer network of TLUs. As an example, consider the function $f = x _ { 1 } x _ { 2 } + x _ { 2 } { \overline { { x _ { 3 } } } } +$ $x _ { 1 } x _ { 3 }$ . The form of the network that implements this function is shown in Fig. 4.12. (We leave it to the reader to calculate appropriate values of weights and

thresholds.) The 3-cube representation of the function is shown in Fig. 4.13. The network of Fig. 4.12 can be designed so that each hidden unit implements one of the planar boundaries shown in Fig. 4.13.

![](images/c9a8e2988fac97c1505c904cb17f441411794f158681abfe8bb6dfe1518db6ae.jpg)  
Figure 4.12: A Two-Layer Network

$$
\mathsf {f} = \mathsf {x} _ {1} \mathsf {x} _ {2} + \mathsf {x} _ {2} \overline {{\mathsf {x} _ {3}}} + \mathsf {x} _ {1} \overline {{\mathsf {x} _ {3}}}
$$

![](images/63f5fb4d3e37dda9782f09cfeb902177f3ee7542dc1e0a1bf25f03cf439a48d5.jpg)  
Figure 4.13: Three Planes Implemented by the Hidden Units

To train a two-layer network that implements a $k$ -term DNF function, we first note that the output unit implements a disjunction, so the weights in the final layer are fixed. The weights in the first layer (except for the “threshold weights”) can all have values of 1, $^ { - 1 }$ , or 0. Later, we will present a training procedure for this first layer of weights.

Discuss half-space intersections, half-space unions, NP-hardness of optimal versions, single-side-error-hypeplane methods, relation to “AQ” methods.

# Important Comment About Layered Networks

Adding additional layers cannot compensate for an inadequate first layer of TLUs. The first layer of TLUs partitions the feature space so that no two differently labeled vectors are in the same region (that is, so that no two such vectors yield the same set of outputs of the first-layer units). If the first layer does not partition the feature space in this way, then regardless of what subsequent layers do, the final outputs will not be consistent with the labeled training set.

Add diagrams showing the non-linear transformation performed by a layered network.

# 4.3.2 Madalines

# Two-Category Networks

An interesting example of a layered, feedforward network is the two-layer one which has an odd number of hidden units, and a “vote-taking” TLU as the output unit. Such a network was called a “Madaline” (for many adalines by Widrow. Typically, the response of the vote taking unit is defined to be the response of the majority of the hidden units, although other output logics are possible. Ridgway [Ridgway, 1962] proposed the following error-correction rule for adjusting the weights of the hidden units of a Madaline:

• If the Madaline correctly classifies a pattern, $\mathbf { X } _ { i }$ , no corrections are made to any of the hidden units’ weight vectors,   
• If the Madaline incorrectly classifies a pattern, $\mathbf { X } _ { i }$ , then determine the minimum number of hidden units whose responses need to be changed (from 0 to 1 or from 1 to 0—depending on the type of error) in order that the Madaline would correctly classify $\mathbf { X } _ { i }$ . Suppose that minimum number is $k _ { i }$ . Of those hidden units voting incorrectly, change the weight vectors of those $k _ { i }$ of them whose dot products are closest to 0 by using the error correction rule:

$$
\mathbf {W} \leftarrow \mathbf {W} + c _ {i} \left(d _ {i} - f _ {i}\right) \mathbf {X} _ {i}
$$

where $d _ { i }$ is the desired response of the hidden unit (0 or 1) and $f _ { i }$ is the actual response ( $0$ or 1). (We assume augmented vectors here even though we are using $\mathbf { X }$ , W notation.)

That is, we perform error-correction on just enough hidden units to correct the vote to a majority voting correctly, and we change those that are easiest to change. There are example problems in which even though a set of weight values exists for a given Madaline structure such that it could classify all members of a training set correctly, this procedure will fail to find them. Nevertheless, the procedure works effectively in most experiments with it.

We leave it to the reader to think about how this training procedure could be modified if the output TLU implemented an or function (or an and function).

# $R$ -Category Madalines and Error-Correcting Output Codes

If there are $k$ hidden units ( $k > 1$ ) in a two-layer network, their responses correspond to vertices of a $k$ -dimensional hypercube. The ordinary two-category Madaline identifies two special points in this space, namely the vertex consisting of $k$ 1’s and the vertex consisting of $k$ 0’s. The Madaline’s response is 1 if the point in “hidden-unit-space” is closer to the all 1’s vertex than it is to the all $0$ ’s vertex. We could design an $R$ -category Madaline by identifying $R$ vertices in hidden-unit space and then classifying a pattern according to which of these vertices the hidden-unit response is closest to. A machine using that idea was implemented in the early 1960s at SRI [Brain, et al., 1962]. It used the fact that the $2 ^ { p }$ so-called maximal-length shift-register sequences [Peterson, 1961, pp. 147ff] in a $( 2 ^ { p } - 1 )$ -dimensional Boolean space are mutually equidistant (for any integer $p$ ). For similar, more recent work see [Dietterich & Bakiri, 1991].

# 4.3.3 Piecewise Linear Machines

A two-category training set is linearly separable if there exists a threshold function that correctly classifies all members of the training set. Similarly, we can say that an $R$ -category training set is linearly separable if there exists a linear machine that correctly classifies all members of the training set. When an $R$ - category problem is not linearly separable, we need a more powerful classifier. A candidate is a structure called a piecewise linear (PWL) machine illustrated in Fig. 4.14.

![](images/9bbe1c9d5198b4c492e45806efbfea1f024ebe7319c26dc184292f01bf5daa7b.jpg)  
Figure 4.14: A Piecewise Linear Machine

The PWL machine groups its weighted summing units into $R$ banks corresponding to the $R$ categories. An input vector $\mathbf { X }$ is assigned to that category corresponding to the bank with the largest weighted sum. We can use an errorcorrection training algorithm similar to that used for a linear machine. If a pattern is classified incorrectly, we subtract (a constant times) the pattern vector from the weight vector producing the largest dot product (it was incorrectly the largest) and add (a constant times) the pattern vector to that weight vector in the correct bank of weight vectors whose dot product is locally largest in that bank. (Again, we use augmented vectors here.) Unfortunately, there are example training sets that are separable by a given PWL machine structure but for which this error-correction training method fails to find a solution. The method does appear to work well in some situations [Duda & Fossum, 1966], although [Nilsson, 1965, page 89] observed that “it is probably not a very effective method for training PWL machines having more than three [weight vectors] in each bank.”

# 4.3.4 Cascade Networks

Another interesting class of feedforward networks is that in which all of the TLUs are ordered and each TLU receives inputs from all of the pattern components and from all TLUs lower in the ordering. Such a network is called a cascade network. An example is shown in Fig. 4.15 in which the TLUs are labeled by the linearly separable functions (of their inputs) that they implement. Each TLU in the network implements a set of $2 ^ { k }$ parallel hyperplanes, where $k$ i s the number of TLUs from which it receives inputs. (Each of the $k$ preceding TLUs can have an output of 1 or 0; that’s $2 ^ { k }$ different combinations—resulting in $2 ^ { k }$ different positions for the parallel hyperplanes.) We show a 3-dimensional sketch for a network of two TLUs in Fig. 4.16. The reader might consider how the $n$ -dimensional parity function might be implemented by a cascade network having $\log _ { 2 } n$ TLUs.

![](images/0d4a5d9283ebd5de0aa5cbbe68d6b7f1ca794cadb2bba834be638ee61a4a8558.jpg)  
Figure 4.15: A Cascade Network

![](images/f17cd07f22bafc1a0419c2ce7ff013445cb93288133f9269d9481794891cb299.jpg)  
Figure 4.16: Planes Implemented by a Cascade Network with Two TLUs

Cascade networks might be trained by first training $L _ { 1 }$ to do as good a job as possible at separating all the training patterns (perhaps by using the pocket algorithm, for example), then training $L _ { 2 }$ (including the weight from $L _ { 1 }$ to $L _ { 2 }$ ) also to do as good a job as possible at separating all the training patterns, and so on until the resulting network classifies the patterns in the training set satisfactorily.

# 4.4 Training Feedforward Networks by Backpropagation

# 4.4.1 Notation

The general problem of training a network of TLUs is difficult. Consider, for example, the layered, feedforward network of Fig. 4.11. If such a network makes an error on a pattern, there are usually several different ways in which the error can be corrected. It is difficult to assign “blame” for the error to any particular TLU in the network. Intuitively, one looks for weight-adjusting procedures that move the network in the correct direction (relative to the error) by making minimal changes. In this spirit, the Widrow-Hoff method of gradient descent has been generalized to deal with multilayer networks.

In explaining this generalization, we use Fig. 4.17 to introduce some notation. This network has only one output unit, but, of course, it is possible to have several TLUs in the output layer—each implementing a different function. Each of the layers of TLUs will have outputs that we take to be the components of vectors, just as the input features are components of an input vector. The $j$ -th layer of TLUs ( $1 \leq j < k$ ) will have as their outputs the vector $\mathbf { X } ^ { ( j ) }$ . The input feature vector is denoted by $\mathbf { X } ^ { ( 0 ) }$ , and the final output (of the $k$ -th layer TLU) is $f$ . Each TLU in each layer has a weight vector (connecting it to its inputs) and a threshold; the $i$ -th TLU in the $j$ -th layer has a weight vector denoted by W(j). $\mathbf { W } _ { i } ^ { ( j ) }$ (We will assume that the “threshold weight” is the last component of the associated weight vector; we might have used $\mathbf { V }$ notation instead to include

Also mention the “cascade-correlation” method of [Fahlman & Lebiere, 1990].

this threshold component, but we have chosen here to use the familiar $\mathbf { X } , \mathbf { W }$ notation, assuming that these vectors are “augmented” as appropriate.) We denote the weighted sum input to the $i$ -th threshold unit in the $j$ -th layer by given by $s _ { i } ^ { ( j ) }$ si . (That is, $m _ { j }$ . The vector $s _ { i } ^ { ( j ) } = \mathbf { X } ^ { ( j - 1 ) } \bullet \mathbf { W } _ { i } ^ { ( j ) }$ $\mathbf { W } _ { i } ^ { ( j ) }$ has components .) The number of TLUs in the $w _ { l , i } ^ { ( j ) }$ for $l = 1 , \ldots , m _ { ( j - 1 ) } + 1$ $j$ -th layer is .

![](images/08493c7bd6075a96dc3a3574d20b245798ec4d8dbfd88e0d46b05139cd387b02.jpg)  
Figure 4.17: A $k$ -layer Network

# 4.4.2 The Backpropagation Method

A gradient descent method, similar to that used in the Widrow Hoff method, has been proposed by various authors for training a multi-layer, feedforward network. As before, we define an error function on the final output of the network and we adjust each weight in the network so as to minimize the error. If we have a desired response, $d _ { i }$ , for the $i$ -th input vector, $\mathbf { X } _ { i }$ , in the training set, $\Xi$ , we can compute the squared error over the entire training set to be:

$$
\varepsilon = \sum_ {\mathbf {X} _ {i} \in \Xi} (d _ {i} - f _ {i}) ^ {2}
$$

where $f _ { i }$ is the actual response of the network for input $\mathbf { X } _ { i }$ . To do gradient descent on this squared error, we adjust each weight in the network by an amount proportional to the negative of the partial derivative of $\varepsilon$ with respect to that weight. Again, we use a single-pattern error function so that we can use an incremental weight adjustment procedure. The squared error for a single input vector, $\mathbf { X }$ , evoking an output of $f$ when the desired output is $d$ is:

$$
\varepsilon = (d - f) ^ {2}
$$

It is convenient to take the partial derivatives of $\varepsilon$ with respect to the various weights in groups corresponding to the weight vectors. We define a partial derivative of a quantity $\phi$ , say, with respect to a weight vector, $\mathbf { W } _ { i } ^ { ( j ) }$ , thus:

$$
\frac {\partial \phi}{\partial \mathbf {W} _ {i} ^ {(j)}} \stackrel {\mathrm {d e f}} {=} \left[ \frac {\partial \phi}{\partial w _ {1 i} ^ {(j)}}, \dots , \frac {\partial \phi}{\partial w _ {l i} ^ {(j)}}, \dots , \frac {\partial \phi}{\partial w _ {m _ {j - 1} + 1 , i} ^ {(j)}} \right]
$$

where $w _ { l i } ^ { ( j ) }$ is the $\it { \Delta } l$ -th component of $\mathbf { W } _ { i } ^ { ( j ) }$ This vector partial derivative of $\phi$ called the gradient of $\phi$ with respect to $\mathbf { W }$ and is sometimes denoted by $\nabla _ { \mathbf { W } } \phi$ .

Since $\varepsilon$ ’s dependence o n W(j) $\mathbf { W } _ { i } ^ { ( j ) }$ is entirely through $s _ { i } ^ { ( j ) }$ s , we can use the chain rule to write:

$$
\frac {\partial \varepsilon}{\partial \mathbf {W} _ {i} ^ {(j)}} = \frac {\partial \varepsilon}{\partial s _ {i} ^ {(j)}} \frac {\partial s _ {i} ^ {(j)}}{\partial \mathbf {W} _ {i} ^ {(j)}}
$$

Becaus e s(j) = $s _ { i } ^ { ( j ) } = \mathbf { X } ^ { ( j - 1 ) } \bullet \mathbf { W } _ { i } ^ { ( j ) }$ , ∂ W ( j ) $\frac { \partial s _ { i } ^ { ( j ) } } { \partial \mathbf { W } _ { i } ^ { ( j ) } } = \mathbf { X } ^ { ( j - 1 ) }$ . Substituting yields:

$$
\frac {\partial \varepsilon}{\partial \mathbf {W} _ {i} ^ {(j)}} = \frac {\partial \varepsilon}{\partial s _ {i} ^ {(j)}} \mathbf {X} ^ {(j - 1)}
$$

Note that $\begin{array} { r } { \frac { \partial \varepsilon } { \partial s _ { i } ^ { ( j ) } } = - 2 ( d - f ) \frac { \partial f } { \partial s _ { i } ^ { ( j ) } } } \end{array}$ . Thus,

$$
\frac {\partial \varepsilon}{\partial \mathbf {W} _ {i} ^ {(j)}} = - 2 (d - f) \frac {\partial f}{\partial s _ {i} ^ {(j)}} \mathbf {X} ^ {(j - 1)}
$$

The quantity $\begin{array} { r } { ( d - f ) \frac { \partial f } { \partial s _ { i } ^ { ( j ) } } } \end{array}$ plays an important role in our calculations; we shall denote it by $\delta _ { i } ^ { ( j ) }$ . Each of the $\delta _ { i } ^ { ( j ) }$ ’s tells us how sensitive the squared error of the network output is to changes in the input to each threshold function. Since we will be changing weight vectors in directions along their negative gradient, our fundamental rule for weight changes throughout the network will be:

$$
\mathbf {W} _ {i} ^ {(j)} \gets \mathbf {W} _ {i} ^ {(j)} + c _ {i} ^ {(j)} \delta_ {i} ^ {(j)} \mathbf {X} ^ {(j - 1)}
$$

where $c _ { i } ^ { ( j ) }$ is the learning rate constant for this weight vector. (Usually, the learning rate constants for all weight vectors in the network are the same.) We see that this rule is quite similar to that used in the error correction procedure

for a single TLU. A weight vector is changed by the addition of a constant times its vector of (unweighted) inputs.

Now, we must turn our attention to the calculation of the $\delta _ { i } ^ { ( j ) }$ ’s. Using the definition, we have:

$$
\delta_ {i} ^ {(j)} = (d - f) \frac {\partial f}{\partial s _ {i} ^ {(j)}}
$$

We have a problem, however, in attempting to carry out the partial derivatives of $f$ with respect to the $s$ ’s. The network output, $f$ , is not continuously differentiable with respect to the $s$ ’s because of the presence of the threshold functions. Most small changes in these sums do not change $f$ at all, and when $f$ does change, it changes abruptly from 1 to 0 or vice versa.

A way around this difficulty was proposed by Werbos [Werbos, 1974] and (perhaps independently) pursued by several other researchers, for example [Rumelhart, Hinton, & Williams, 1986]. The trick involves replacing all the threshold functions by differentiable functions called sigmoids.1 The output of a sigmoid function, superimposed on that of a threshold function, is shown in Fig. 4.18. Usually, the sigmoid function used is $\begin{array} { r } { f ( s ) = \frac { 1 } { 1 + e ^ { - s } } } \end{array}$ , where $s$ is the input and $f$ is the output.

![](images/082aef622fad406e7312e6b764db05e005ee19d86634d02f1fac8317b2a099c1.jpg)  
Figure 4.18: A Sigmoid Function

We show the network containing sigmoid units in place of TLUs in Fig. 4.19. The output of the $_ i$ -th sigmoid unit in the $j$ -th layer is denoted by $f _ { i } ^ { ( j ) }$ . (That is , $\begin{array} { r } { f _ { i } ^ { ( j ) } = \frac { 1 } { 1 + e ^ { - s _ { i } ^ { ( j ) } } } } \end{array}$ − s ( j ) . )

![](images/6e63ee99f494ed7a4d274c9b59b1eefd8c3957906e5c47ae45a40189f9bd48a4.jpg)  
Figure 4.19: A Network with Sigmoid Units

# 4.4.3 Computing Weight Changes in the Final Layer

We first calculate $\delta ^ { ( k ) }$ in order to compute the weight change for the final sigmoid unit:

$$
\delta^ {(k)} = (d - f ^ {(k)}) \frac {\partial f ^ {(k)}}{\partial s ^ {(k)}}
$$

Given the sigmoid function that we are using, namely $\begin{array} { r } { f ( s ) = \frac { 1 } { 1 + e ^ { - s } } } \end{array}$ , we have that $\begin{array} { r } { \frac { \partial f } { \partial s } = f ( 1 - f ) } \end{array}$ . Substituting gives us:

$$
\delta^ {(k)} = (d - f ^ {(k)}) f ^ {(k)} (1 - f ^ {(k)})
$$

Rewriting our general rule for weight vector changes, the weight vector in the final layer is changed according to the rule:

$$
\mathbf {W} ^ {(k)} \leftarrow \mathbf {W} ^ {(k)} + c ^ {(k)} \delta^ {(k)} \mathbf {X} ^ {(k - 1)}
$$

where $\delta ^ { ( k ) } = ( d - f ^ { ( k ) } ) f ^ { ( k ) } ( 1 - f ^ { ( k ) } )$

It is interesting to compare backpropagation to the error-correction rule and to the Widrow-Hoff rule. The backpropagation weight adjustment for the single element in the final layer can be written as:

$$
\mathbf {W} \longleftarrow \mathbf {W} + c (d - f) f (1 - f) \mathbf {X}
$$

Written in the same format, the error-correction rule is:

$$
\mathbf {W} \longleftarrow \mathbf {W} + c (d - f) \mathbf {X}
$$

and the Widrow-Hoff rule is:

$$
\mathbf {W} \leftarrow \mathbf {W} + c (d - f) \mathbf {X}
$$

The only difference (except for the fact that $f$ is not thresholded in Widrow-Hoff) is the $f ( 1 - f )$ term due to the presence of the sigmoid function. With the sigmoid function, $f ( 1 - f )$ can vary in value from 0 to 1. When $f$ is $0$ , $f ( 1 - f )$ is also 0; when $f$ is 1, $f ( 1 - f )$ is $0$ ; $f ( 1 - f )$ obtains its maximum value of $1 / 4$ when $f$ is $1 / 2$ (that is, when the input to the sigmoid is $0$ ). The sigmoid function can be thought of as implementing a “fuzzy” hyperplane. For a pattern far away from this fuzzy hyperplane, $f ( 1 - f )$ has value close to 0, and the backpropagation rule makes little or no change to the weight values regardless of the desired output. (Small changes in the weights will have little effect on the output for inputs far from the hyperplane.) Weight changes are only made within the region of “fuzz” surrounding the hyperplane, and these changes are in the direction of correcting the error, just as in the error-correction and Widrow-Hoff rules.

# 4.4.4 Computing Changes to the Weights in Intermediate Layers

Using our expression for the $\delta$ ’s, we can similarly compute how to change each of the weight vectors in the network. Recall:

$$
\delta_ {i} ^ {(j)} = (d - f) \frac {\partial f}{\partial s _ {i} ^ {(j)}}
$$

Again we use a chain rule. The final output, $f$ , depends on $s _ { i } ^ { ( j ) }$ si through each of the summed inputs to the sigmoids in the $( j + 1 )$ -th layer. So:

$$
\begin{array}{l} \delta_ {i} ^ {(j)} = (d - f) \frac {\partial f}{\partial s _ {i} ^ {(j)}} \\ = (d - f) \left[ \frac {\partial f}{\partial s _ {1} ^ {(j + 1)}} \frac {\partial s _ {1} ^ {(j + 1)}}{\partial s _ {i} ^ {(j)}} + \dots + \frac {\partial f}{\partial s _ {l} ^ {(j + 1)}} \frac {\partial s _ {l} ^ {(j + 1)}}{\partial s _ {i} ^ {(j)}} + \dots + \frac {\partial f}{\partial s _ {m _ {j + 1}} ^ {(j + 1)}} \frac {\partial s _ {m _ {j + 1}} ^ {(j + 1)}}{\partial s _ {i} ^ {(j)}} \right] \\ = \sum_ {l = 1} ^ {m _ {j + 1}} (d - f) \frac {\partial f}{\partial s _ {l} ^ {(j + 1)}} \frac {\partial s _ {l} ^ {(j + 1)}}{\partial s _ {i} ^ {(j)}} = \sum_ {l = 1} ^ {m _ {j + 1}} \delta_ {l} ^ {(j + 1)} \frac {\partial s _ {l} ^ {(j + 1)}}{\partial s _ {i} ^ {(j)}} \\ \end{array}
$$

It remains to compute the ∂s(j+1)l $\frac { \partial s _ { l } ^ { ( j + 1 ) } } { \partial s _ { i } ^ { ( j ) } }$ ∂ s(j)i ’s. To do that we first write:

$$
\begin{array}{l} s _ {l} ^ {(j + 1)} = \mathbf {X} ^ {(j)} \bullet \mathbf {W} _ {l} ^ {(j + 1)} \\ = \sum_ {\nu = 1} ^ {m _ {j} + 1} f _ {\nu} ^ {(j)} w _ {\nu l} ^ {(j + 1)} \\ \end{array}
$$

And then, since the weights do not depend on the $s$ ’s:

$$
\frac {\partial s _ {l} ^ {(j + 1)}}{\partial s _ {i} ^ {(j)}} = \frac {\partial \left[ \sum_ {\nu = 1} ^ {m _ {j} + 1} f _ {\nu} ^ {(j)} w _ {\nu l} ^ {(j + 1)} \right]}{\partial s _ {i} ^ {(j)}} = \sum_ {\nu = 1} ^ {m _ {j} + 1} w _ {\nu l} ^ {(j + 1)} \frac {\partial f _ {\nu} ^ {(j)}}{\partial s _ {i} ^ {(j)}}
$$

Now, we note that $\begin{array} { r } { \frac { \partial f _ { \nu } ^ { ( j ) } } { \partial s _ { i } ^ { ( j ) } } = 0 } \end{array}$ unless $\nu = i$ , in which case $\begin{array} { r } { \frac { \partial f _ { \nu } ^ { ( j ) } } { \partial s _ { \nu } ^ { ( j ) } } = f _ { \nu } ^ { ( j ) } ( 1 - f _ { \nu } ^ { ( j ) } ) } \end{array}$ . Therefore:

$$
\frac {\partial s _ {l} ^ {(j + 1)}}{\partial s _ {i} ^ {(j)}} = w _ {i l} ^ {(j + 1)} f _ {i} ^ {(j)} (1 - f _ {i} ^ {(j)})
$$

We use this result in our expression for $\delta _ { i } ^ { ( j ) }$ to give:

$$
\delta_ {i} ^ {(j)} = f _ {i} ^ {(j)} (1 - f _ {i} ^ {(j)}) \sum_ {l = 1} ^ {m _ {j + 1}} \delta_ {l} ^ {(j + 1)} w _ {i l} ^ {(j + 1)}
$$

The above equation is recursive in the $\delta$ ’s. (It is interesting to note that this expression is independent of the error function; the error function explicitly affects only the computation of $\delta ^ { ( k ) }$ .) Having computed the $\delta _ { i } ^ { ( j + 1 ) }$ ’s for layer $j + 1$ , we can use this equation to compute the $\delta _ { i } ^ { ( j ) }$ ’s. The base case is $\delta ^ { ( k ) }$ , which we have already computed:

$$
\delta^ {(k)} = (d - f ^ {(k)}) f ^ {(k)} (1 - f ^ {(k)})
$$

We use this expression for the $\delta$ ’s in our generic weight changing rule, namely:

$$
\mathbf {W} _ {i} ^ {(j)} \gets \mathbf {W} _ {i} ^ {(j)} + c _ {i} ^ {(j)} \delta_ {i} ^ {(j)} \mathbf {X} ^ {(j - 1)}
$$

Although this rule appears complex, it has an intuitively reasonable explanation. The quantity $\delta ^ { ( k ) } = ( d - f ) f ( 1 - f )$ controls the overall amount and sign of all weight adjustments in the network. (Adjustments diminish as the final output, $f$ , approaches either 0 or 1, because they have vanishing effect on $f$ then.) As the recursion equation for the $\delta$ ’s shows, the adjustments for the weights going in to a sigmoid unit in the $j$ -th layer are proportional to the effect that such adjustments have on that sigmoid unit’s output (its $f ^ { ( j ) } ( 1 - f ^ { ( j ) } )$ factor). They are also proportional to a kind of “average” effect that any change in the output of that sigmoid unit will have on the final output. This average effect depends on the weights going out of the sigmoid unit in the $j$ -th layer (small weights produce little downstream effect) and the effects that changes in the outputs of $( j + 1 )$ -th layer sigmoid units will have on the final output (as measured by the $\delta ^ { ( j + 1 ) }$ ’s). These calculations can be simply implemented by “backpropagating” the $\delta$ ’s through the weights in reverse direction (thus, the name backprop for this algorithm).

# 4.4.5 Variations on Backprop

[To be written: problem of local minima, simulated annealing, momemtum (Plaut, et al., 1986, see [Hertz, Krogh, & Palmer, 1991]), quickprop, regularization methods]

# Simulated Annealing

To apply simulated annealing, the value of the learning rate constant is gradually decreased with time. If we fall early into an error-function valley that is not very deep (a local minimum), it typically will neither be very broad, and soon

a subsequent large correction will jostle us out of it. It is less likely that we will move out of deep valleys, and at the end of the process (with very small values of the learning rate constant), we descend to its deepest point. The process gets its name by analogy with annealing in metallurgy, in which a material’s temperature is gradually decreased allowing its crystalline structure to reach a minimal energy state.

# 4.4.6 An Application: Steering a Van

A neural network system called ALVINN (Autonomous Land Vehicle in a Neural Network) has been trained to steer a Chevy van successfully on ordinary roads and highways at speeds of 55 mph [Pomerleau, 1991, Pomerleau, 1993]. The input to the network is derived from a low-resolution $( 3 0 \times 3 2 )$ ) television image. The TV camera is mounted on the van and looks at the road straight ahead. This image is sampled and produces a stream of 960-dimensional input vectors to the neural network. The network is shown in Fig. 4.20.

![](images/aa0f175bd2004d302d7b3fd015ecff65db4f0b92b54b54c5320df5ab3314e2b3.jpg)  
Figure 4.20: The ALVINN Network

The network has five hidden units in its first layer and 30 output units in the second layer; all are sigmoid units. The output units are arranged in a linear order and control the van’s steering angle. If a unit near the top of the array of output units has a higher output than most of the other units, the van is steered to the left; if a unit near the bottom of the array has a high output, the van is steered to the right. The “centroid” of the responses of all of the output

# 4.5. SYNERGIES BETWEEN NEURAL NETWORK AND KNOWLEDGE-BASED METHODS61

units is computed, and the van’s steering angle is set at a corresponding value between hard left and hard right.

The system is trained by a modified on-line training regime. A driver drives the van, and his actual steering angles are taken as the correct labels for the corresponding inputs. The network is trained incrementally by backprop to produce the driver-specified steering angles in response to each visual pattern as it occurs in real time while driving.

This simple procedure has been augmented to avoid two potential problems. First, since the driver is usually driving well, the network would never get any experience with far-from-center vehicle positions and/or incorrect vehicle orientations. Also, on long, straight stretches of road, the network would be trained for a long time only to produce straight-ahead steering angles; this training would swamp out earlier training to follow a curved road. We wouldn’t want to try to avoid these problems by instructing the driver to drive erratically occasionally, because the system would learn to mimic this erratic behavior.

Instead, each original image is shifted and rotated in software to create 14 additional images in which the vehicle appears to be situated differently relative to the road. Using a model that tells the system what steering angle ought to be used for each of these shifted images, given the driver-specified steering angle for the original image, the system constructs an additional 14 labeled training patterns to add to those encountered during ordinary driver training.

# 4.5 Synergies Between Neural Network and Knowledge-Based Methods

# 4.6 Bibliographical and Historical Remarks

To be written; discuss rule-generating procedures (such as [Towell & Shavlik, 1992]) and how expert-provided rules can aid neural net training and vice-versa [Towell, Shavlik, & Noordweier, 1990]. To be added.

# Chapter 5

# Statistical Learning

# 5.1 Using Statistical Decision Theory

# 5.1.1 Background and General Method

Suppose the pattern vector, $\mathbf { X }$ , is a random variable whose probability distribution for category 1 is different than it is for category 2. (The treatment given here can easily be generalized to $R$ -category problems.) Specifically, suppose we have the two probability distributions (perhaps probability density functions), $p ( \mathbf { X } \mid \mathbf { 1 } )$ and $p ( \mathbf { X } \mid 2 )$ . Given a pattern, $\mathbf { X }$ , we want to use statistical techniques to determine its category—that is, to determine from which distribution it was drawn. These techniques are based on the idea of minimizing the expected value of a quantity similar to the error function we used in deriving the weight-changing rules for backprop.

In developing a decision method, it is necessary to know the relative seriousness of the two kinds of mistakes that might be made. (We might decide that a pattern really in category 1 is in category 2, and vice versa.) We describe this information by a loss function, $\lambda ( i \mid j )$ , for $i , j = 1 , 2$ . $\lambda ( i \mid j )$ represents the loss incurred when we decide a pattern is in category $_ i$ when really it is in category $j$ . We assume here that $\lambda ( 1 \mid 1 )$ and $\lambda ( 2 \mid 2 )$ are both 0. For any given pattern, $\mathbf { X }$ , we want to decide its category in such a way that minimizes the expected value of this loss.

Given a pattern, $\mathbf { X }$ , if we decide category $i$ , the expected value of the loss will be:

$$
L _ {\mathbf {X}} (i) = \lambda (i \mid 1) p (1 \mid \mathbf {X}) + \lambda (i \mid 2) p (2 \mid \mathbf {X})
$$

where $p ( j \mid \mathbf { X } )$ is the probability that given a pattern $\mathbf { X }$ , its category is $j$ . Our decision rule will be to decide that $\mathbf { X }$ belongs to category 1 if $L _ { \mathbf { X } } ( 1 ) \leq L _ { \mathbf { X } } ( 2 )$ , and to decide on category 2 otherwise.

We can use Bayes’ Rule to get expressions for $p ( j \mid \mathbf { X } )$ in terms of $p ( \mathbf { X } \mid j )$ , which we assume to be known (or estimatible):

$$
p (j \mid \mathbf {X}) = \frac {p (\mathbf {X} \mid j) p (j)}{p (\mathbf {X})}
$$

where $p ( j )$ is the (a priori) probability of category $j$ (one category may be much more probable than the other); and $p ( \mathbf { X } )$ is the (a priori) probability of pattern $\mathbf { X }$ being the pattern we are asked to classify. Performing the substitutions given by Bayes’ Rule, our decision rule becomes:

Decide category 1 iff:

$$
\begin{array}{l} \lambda (1 \mid 1) \frac {p (\mathbf {X} \mid 1) p (1)}{p (\mathbf {X})} + \lambda (1 \mid 2) \frac {p (\mathbf {X} \mid 2) p (2)}{p (\mathbf {X})} \\ \leq \lambda (2 \mid 1) \frac {p (\mathbf {X} \mid 1) p (1)}{p (\mathbf {X})} + \lambda (2 \mid 2) \frac {p (\mathbf {X} \mid 2) p (2)}{p (\mathbf {X})} \\ \end{array}
$$

Using the fact that $\lambda ( i \mid i ) = 0$ , and noticing that $p ( \mathbf { X } )$ is common to both expressions, we obtain,

Decide category 1 iff:

$$
\lambda (1 \mid 2) p (\mathbf {X} \mid 2) p (2) \leq \lambda (2 \mid 1) p (\mathbf {X} \mid 1) p (1)
$$

If $\lambda ( 1 \mid 2 ) = \lambda ( 2 \mid 1 )$ and if $p ( 1 ) = p ( 2 )$ , then the decision becomes particularly simple:

Decide category 1 iff:

$$
p (\mathbf {X} \mid 2) \leq p (\mathbf {X} \mid 1)
$$

Since $p ( \mathbf { X } \mid j )$ is called the likelihood of $j$ with respect to $\mathbf { X }$ , this simple decision rule implements what is called a maximum-likelihood decision. More generally, if we define $k ( i \mid j )$ as $\lambda ( i \mid j ) p ( j )$ , then our decision rule is simply,

Decide category1 iff:

$$
k (1 \mid 2) p (\mathbf {X} \mid 2) \leq k (2 \mid 1) p (\mathbf {X} \mid 1)
$$

In any case, we need to compare the (perhaps weighted) quantities $p ( \mathbf { X } \mid i )$ for $i = 1$ and 2. The exact decision rule depends on the the probability distributions assumed. We will treat two interesting distributions.

# 5.1.2 Gaussian (or Normal) Distributions

The multivariate ( $n$ -dimensional) Gaussian distribution is given by the probability density function:

$$
p (\mathbf {X}) = \frac {1}{(2 \pi) ^ {n / 2} | \boldsymbol {\Sigma} | ^ {1 / 2}} e ^ {\frac {- (\mathbf {X} - \mathbf {M}) ^ {t} \boldsymbol {\Sigma} ^ {- 1} (\mathbf {X} - \mathbf {M})}{2}}
$$

where $n$ is the dimension of the column vector $\mathbf { X }$ , the column vector $\mathbf { M }$ is called the mean vector, $( \mathbf { X } - \mathbf { M } ) ^ { t }$ is the transpose of the vector $( \mathbf { X } - \mathbf { M } )$ , $\pmb { \Sigma }$ is the covariance matrix of the distribution (an $n \times n$ symmetric, positive definite matrix), $\pmb { \Sigma } ^ { - 1 }$ is the inverse of the covariance matrix, and $| \Sigma |$ is the determinant of the covariance matrix.

The mean vector, M, with components $( m _ { 1 } , \ldots , m _ { n } )$ , is the expected value of $\mathbf { X }$ (using this distribution); that is, $\mathbf { M } = E | \mathbf { X } |$ . The components of the covariance matrix are given by:

$$
\sigma_ {i j} ^ {2} = E \left[ \left(x _ {i} - m _ {i}\right) \left(x _ {j} - m _ {j}\right) \right]
$$

In particular, $\sigma _ { i i } ^ { 2 }$ is called the variance of $x _ { i }$

Although the formula appears complex, an intuitive idea for Gaussian distributions can be given when $n = 2$ . We show a two-dimensional Gaussian distribution in Fig. 5.1. A three-dimensional plot of the distribution is shown at the top of the figure, and contours of equal probability are shown at the bottom. In this case, the covariance matrix, $\pmb { \Sigma }$ , is such that the elliptical contours of equal probability are skewed. If the covariance matrix were diagonal, that is if all off-diagonal terms were 0, then the major axes of the elliptical contours would be aligned with the coordinate axes. In general the principal axes are given by the eigenvectors of $\pmb { \Sigma }$ . In any case, the equi-probability contours are all centered on the mean vector, $\mathbf { M }$ , which in our figure happens to be at the origin. In general, the formula in the exponent in the Gaussian distribution is a positive definite quadratic form (that is, its value is always positive); thus equi-probability contours are hyper-ellipsoids in $n$ -dimensional space.

Suppose we now assume that the two classes of pattern vectors that we want to distinguish are each distributed according to a Gaussian distribution but with different means and covariance matrices. That is, one class tends to have patterns clustered around one point in the $n$ -dimensional space, and the other class tends to have patterns clustered around another point. We show a two-dimensional instance of this problem in Fig. 5.2. (In that figure, we have plotted the sum of the two distributions.) What decision rule should we use to separate patterns into the two appropriate categories?

Substituting the Gaussian distributions into our maximum likelihood formula yields:

![](images/0dd87e2009bf057d208790c5dd76bd0ec62267ef997a0db13bcd7805058ce5ba.jpg)

![](images/90d44b3a91d207a8ac11112a59e7ea830872a084421f20bc17b8ff73f52b6990.jpg)  
Figure 5.1: The Two-Dimensional Gaussian Distribution

Decide category 1 iff:

$$
\frac {1}{(2 \pi) ^ {n / 2} | \boldsymbol {\Sigma} _ {2} | ^ {1 / 2}} e ^ {- 1 / 2 (\mathbf {X} - \mathbf {M} _ {2}) ^ {t} \boldsymbol {\Sigma} _ {2} ^ {- 1} (\mathbf {X} - \mathbf {M} _ {2})}
$$

is less than or equal to

$$
\frac {1}{(2 \pi) ^ {n / 2} | \boldsymbol {\Sigma} _ {1} | ^ {1 / 2}} e ^ {- 1 / 2 (\mathbf {X} - \mathbf {M} _ {1}) ^ {t} \boldsymbol {\Sigma} _ {1} ^ {- 1} (\mathbf {X} - \mathbf {M} _ {1})}
$$

where the category 1 patterns are distributed with mean and covariance $\mathbf { M } _ { 1 }$ and $\Sigma _ { 1 }$ , respectively, and the category 2 patterns are distributed with mean and covariance $\mathbf { M } _ { 2 }$ and $\Sigma _ { 2 }$ .

The result of the comparison isn’t changed if we compare logarithms instead. After some manipulation, our decision rule is then:

![](images/971b77079fb165d3fe9a2aa10233c727fec488d8a477fed7e298b25250ecd119.jpg)

![](images/9ab2395f38dc5b5db7a5d3b0362890b96787a37369d310dbb2466bec6ef37274.jpg)  
Figure 5.2: The Sum of Two Gaussian Distributions

Decide category 1 iff:

$$
\left(\mathbf {X} - \mathbf {M} _ {1}\right) ^ {t} \boldsymbol {\Sigma} _ {1} ^ {- 1} \left(\mathbf {X} - \mathbf {M} _ {1}\right) <   \left(\mathbf {X} - \mathbf {M} _ {2}\right) ^ {t} \boldsymbol {\Sigma} _ {2} ^ {- 1} \left(\mathbf {X} - \mathbf {M} _ {2}\right) + B
$$

where $B$ , a constant bias term, incorporates the logarithms of the fractions preceding the exponential, etc.

When the quadratic forms are multiplied out and represented in terms of the components $x _ { i }$ , the decision rule involves a quadric surface (a hyperquadric) in $n$ -dimensional space. The exact shape and position of this hyperquadric is determined by the means and the covariance matrices. The surface separates the space into two parts, one of which contains points that will be assigned to category 1 and the other contains points that will be assigned to category 2.

It is interesting to look at a special case of this surface. If the covariance matrices for each category are identical and diagonal, with all $\sigma _ { i i }$ equal to each other, then the contours of equal probability for each of the two distributions

are hyperspherical. The quadric forms then become $( 1 / | \pmb { \Sigma } | ) ( \pmb { \mathrm { X } } - \pmb { \mathrm { M } } _ { i } ) ^ { t } ( \pmb { \mathrm { X } } - \pmb { \mathrm { M } } _ { i } )$ , and the decision rule is:

Decide category 1 iff:

$$
\left(\mathbf {X} - \mathbf {M} _ {1}\right) ^ {t} \left(\mathbf {X} - \mathbf {M} _ {1}\right) <   \left(\mathbf {X} - \mathbf {M} _ {2}\right) ^ {t} \left(\mathbf {X} - \mathbf {M} _ {2}\right)
$$

Multiplying out yields:

$$
\mathbf {X} \bullet \mathbf {X} - 2 \mathbf {X} \bullet \mathbf {M} _ {1} + \mathbf {M} _ {1} \bullet \mathbf {M} _ {1} <   \mathbf {X} \bullet \mathbf {X} - 2 \mathbf {X} \bullet \mathbf {M} _ {2} + \mathbf {M} _ {2} \bullet \mathbf {M} _ {2}
$$

or finally,

Decide category 1 iff:

$$
\mathbf {X} \bullet \mathbf {M} _ {1} \geq \mathbf {X} \bullet \mathbf {M} _ {2} + \text {C o n s t a n t}
$$

or

$$
\mathbf {X} \bullet (\mathbf {M} _ {1} - \mathbf {M} _ {2}) \geq \text {C o n s t a n t}
$$

where the constant depends on the lengths of the mean vectors.

We see that the optimal decision surface in this special case is a hyperplane. In fact, the hyperplane is perpendicular to the line joining the two means. The weights in a TLU implementation are equal to the difference in the mean vectors.

If the parameters $\left( \mathbf { M } _ { i } , \pmb { \Sigma } _ { i } \right)$ of the probability distributions of the categories are not known, there are various techniques for estimating them, and then using those estimates in the decision rule. For example, if there are sufficient training patterns, one can use sample means and sample covariance matrices. (Caution: the sample covariance matrix will be singular if the training patterns happen to lie on a subspace of the whole $n$ -dimensional space—as they certainly will, for example, if the number of training patterns is less than $n$ .)

# 5.1.3 Conditionally Independent Binary Components

Suppose the vector $\mathbf { X }$ is a random variable having binary (0,1) components. We continue to denote the two probability distributions by $p ( \mathbf { X } \mid \mathbf { 1 } )$ and $p ( \mathbf { X } \mid$ 2). Further suppose that the components of these vectors are conditionally independent given the category. By conditional independence in this case, we mean that the formulas for the distribution can be expanded as follows:

$$
p (\mathbf {X} \mid i) = p (x _ {1} \mid i) p (x _ {2} \mid i) \dots p (x _ {n} \mid i)
$$

for $i = 1 , 2$

Recall the minimum-average-loss decision rule,

Decide category 1 iff:

$$
\lambda (1 \mid 2) p (\mathbf {X} \mid 2) p (2) \leq \lambda (2 \mid 1) p (\mathbf {X} \mid 1) p (1)
$$

Assuming conditional independence of the components and that $\lambda ( 1 \mid 2 ) = \lambda ( 2 \mid$ 1), we obtain,

Decide category 1 iff:

$$
p (1) p \left(x _ {1} \mid 1\right) p \left(x _ {2} \mid 1\right) \dots p \left(x _ {n} \mid 1\right) \geq p \left(x _ {1} \mid 2\right) p \left(x _ {2} \mid 2\right) \dots p \left(x _ {n} \mid 2\right) p (2)
$$

or iff:

$$
\frac {p \left(x _ {1} \mid 1\right) p \left(x _ {2} \mid 1\right) \dots p \left(x _ {n} \mid 1\right)}{p \left(x _ {1} \mid 2\right) p \left(x _ {2} \mid 2\right) \dots p \left(x _ {n} \mid 2\right)} \geq \frac {p (2)}{p (1)}
$$

or iff:

$$
\log \frac {p (x _ {1} \mid 1)}{p (x _ {1} \mid 2)} + \log \frac {p (x _ {2} \mid 1)}{p (x _ {2} \mid 2)} + \dots + \log \frac {p (x _ {n} \mid 1)}{p (x _ {n} \mid 2)} + \log \frac {p (1)}{p (2)} \geq 0
$$

Let us define values of the components of the distribution for specific values of their arguments, $x _ { i }$ :

$$
p (x _ {i} = 1 \mid 1) = p _ {i}
$$

$$
p (x _ {i} = 0 \mid 1) = 1 - p _ {i}
$$

$$
p (x _ {i} = 1 \mid 2) = q _ {i}
$$

$$
p (x _ {i} = 0 \mid 2) = 1 - q _ {i}
$$

Now, we note that since $x _ { i }$ can only assume the values of 1 or 0:

$$
\log \frac {p (x _ {i} \mid 1)}{p (x _ {i} \mid 2)} = x _ {i} \log \frac {p _ {i}}{q _ {i}} + (1 - x _ {i}) \log \frac {(1 - p _ {i})}{(1 - q _ {i})}
$$

$$
= x _ {i} \log \frac {p _ {i} (1 - q _ {i})}{q _ {i} (1 - p _ {i})} + \log \frac {(1 - p _ {i})}{(1 - q _ {i})}
$$

Substituting these expressions into our decision rule yields:

Decide category 1 iff:

$$
\sum_ {i = 1} ^ {n} x _ {i} \log \frac {p _ {i} (1 - q _ {i})}{q _ {i} (1 - p _ {i})} + \sum_ {i = 1} ^ {n} \log \frac {(1 - p _ {i})}{(1 - q _ {i})} + \log \frac {p (1)}{p (2)} \geq 0
$$

We see that we can achieve this decision with a TLU with weight values as follows:

$$
w _ {i} = \log \frac {p _ {i} (1 - q _ {i})}{q _ {i} (1 - p _ {i})}
$$

for $i = 1 , \ldots , n$ , and

$$
w _ {n + 1} = \log \frac {p (1)}{1 - p (1)} + \sum_ {i = 1} ^ {n} \log \frac {\left(1 - p _ {i}\right)}{\left(1 - q _ {i}\right)}
$$

If we do not know the $p _ { i } , q _ { i }$ and $p ( 1 )$ , we can use a sample of labeled training patterns to estimate these parameters.

# 5.2 Learning Belief Networks

# 5.3 Nearest-Neighbor Methods

Another class of methods can be related to the statistical ones. These are called nearest-neighbor methods or, sometimes, memory-based methods. (A collection of papers on this subject is in [Dasarathy, 1991].) Given a training set $\Xi$ of $m$ labeled patterns, a nearest-neighbor procedure decides that some new pattern, $\mathbf { X }$ , belongs to the same category as do its closest neighbors in $\Xi$ . More precisely, a $k$ -nearest-neighbor method assigns a new pattern, $\mathbf { X }$ , to that category to which the plurality of its $k$ closest neighbors belong. Using relatively large values of $k$ decreases the chance that the decision will be unduly influenced by a noisy training pattern close to $\mathbf { X }$ . But large values of $k$ also reduce the acuity of the method. The $k$ -nearest-neighbor method can be thought of as estimating the values of the probabilities of the classes given $\mathbf { X }$ . Of course the denser are the points around $\mathbf { X }$ , and the larger the value of $k$ , the better the estimate.

To be added.

The distance metric used in nearest-neighbor methods (for numerical attributes) can be simple Euclidean distance. That is, the distance between two patterns $( x _ { 1 1 } , x _ { 1 2 } , \ldots , x _ { 1 n } )$ and $( x _ { 2 1 } , x _ { 2 2 } , \ldots , x _ { 2 n } )$ is $\sqrt { \textstyle { \sum _ { j = 1 } ^ { n } ( x _ { 1 j } - x _ { 2 j } ) ^ { 2 } } }$ . This distance measure is often modified by scaling the features so that the spread of attribute values along each dimension is approximately the same. In that case, the distance between the two vectors would be $\textstyle { \sqrt { \sum _ { j = 1 } ^ { n } a _ { j } ^ { 2 } ( x _ { 1 j } - x _ { 2 j } ) ^ { 2 } } }$ , where $a _ { j }$ is the scale factor for dimension $j$ .

An example of a nearest-neighbor decision problem is shown in Fig. 5.3. In the figure the class of a training pattern is indicated by the number next to it.

![](images/a850c3c21cc6ac9296cf0ab7f3cec0a6284216b2a86040a06864652bb38f14b0.jpg)  
Figure 5.3: An 8-Nearest-Neighbor Decision

Nearest-neighbor methods are memory intensive because a large number of training patterns must be stored to achieve good generalization. Since memory cost is now reasonably low, the method and its derivatives have seen several practical applications. (See, for example, [Moore, 1992, Moore, et al., 1994]. Also, the distance calculations required to find nearest neighbors can often be efficiently computed by kd-tree methods [Friedman, et al., 1977].

A theorem by Cover and Hart [Cover & Hart, 1967] relates the performance of the 1-nearest-neighbor method to the performance of a minimum-probabilityof-error classifier. As mentioned earlier, the minimum-probability-of-error classifier would assign a new pattern $\mathbf { X }$ to that category that maximized $p ( i ) p ( \mathbf { X } \mid i )$ , where $p ( i )$ is the a priori probability of category $i$ , and $p ( \mathbf { X } \mid i )$ is the probability (or probability density function) of $\mathbf { X }$ given that $\mathbf { X }$ belongs to category $i$ , for categories $i = 1 , \ldots , R$ . Suppose the probability of error in classifying patterns of such a minimum-probability-of-error classifier is $\varepsilon$ . The Cover-Hart theorem states that under very mild conditions (having to do with the smoothness

See [Baum, 1994] for theoretical analysis of error rate as a function of the number of training patterns for the case in which points are randomly distributed on the surface of a unit sphere and underlying function is linearly separable.

of probability density functions) the probability of error, $\varepsilon _ { n n }$ , of a 1-nearestneighbor classifier is bounded by:

$$
\varepsilon \leq \varepsilon_ {n n} \leq \varepsilon \left(2 - \varepsilon \frac {R}{R - 1}\right) \leq 2 \varepsilon
$$

Also see [Aha, 1991].

where $R$ is the number of categories.

# 5.4 Bibliographical and Historical Remarks

To be added.

# Chapter 6

# Decision Trees

# 6.1 Definitions

A decision tree (generally defined) is a tree whose internal nodes are tests (on input patterns) and whose leaf nodes are categories (of patterns). We show an example in Fig. 6.1. A decision tree assigns a class number (or output) to an input pattern by filtering the pattern down through the tests in the tree. Each test has mutually exclusive and exhaustive outcomes. For example, test $T _ { 2 }$ in the tree of Fig. 6.1 has three outcomes; the left-most one assigns the input pattern to class 3, the middle one sends the input pattern down to test $T _ { 4 }$ , and the right-most one assigns the pattern to class 1. We follow the usual convention of depicting the leaf nodes by the class number.1 Note that in discussing decision trees we are not limited to implementing Boolean functions—they are useful for general, categorically valued functions.

There are several dimensions along which decision trees might differ:

a. The tests might be multivariate (testing on several features of the input at once) or univariate (testing on only one of the features).   
b. The tests might have two outcomes or more than two. (If all of the tests have two outcomes, we have a binary decision tree.)   
c. The features or attributes might be categorical or numeric. (Binary-valued ones can be regarded as either.)

![](images/39f0e518b06b290b0d27d2fc1a750dcd60a84e45cc64b50e4b06ef73e064dbad.jpg)  
Figure 6.1: A Decision Tree

d. We might have two classes or more than two. If we have two classes and binary inputs, the tree implements a Boolean function, and is called a Boolean decision tree.

It is straightforward to represent the function implemented by a univariate Boolean decision tree in DNF form. The DNF form implemented by such a tree can be obtained by tracing down each path leading to a tip node corresponding to an output value of 1, forming the conjunction of the tests along this path, and then taking the disjunction of these conjunctions. We show an example in Fig. 6.2. In drawing univariate decision trees, each non-leaf node is depicted by a single attribute. If the attribute has value 0 in the input pattern, we branch left; if it has value 1, we branch right.

The $k$ -DL class of Boolean functions can be implemented by a multivariate decision tree having the (highly unbalanced) form shown in Fig. 6.3. Each test, $c _ { i }$ , is a term of size k or less. The $v _ { i }$ all have values of 0 or 1.

# 6.2 Supervised Learning of Univariate Decision Trees

Several systems for learning decision trees have been proposed. Prominent among these are ID3 and its new version, C4.5 [Quinlan, 1986, Quinlan, 1993], and CART [Breiman, et al., 1984] We discuss here only batch methods, although incremental ones have also been proposed [Utgoff, 1989].

![](images/410aea2d4d884ca58adc36c50ae3e27d120c10910765b8a34d1e6bc722bf081c.jpg)  
Figure 6.2: A Decision Tree Implementing a DNF Function

# 6.2.1 Selecting the Type of Test

As usual, we have $n$ features or attributes. If the attributes are binary, the tests are simply whether the attribute’s value is 0 or 1. If the attributes are categorical, but non-binary, the tests might be formed by dividing the attribute values into mutually exclusive and exhaustive subsets. A decision tree with such tests is shown in Fig. 6.4. If the attributes are numeric, the tests might involve “interval tests,” for example $7 \leq x _ { i } \leq 1 3 . 2$ .

# 6.2.2 Using Uncertainty Reduction to Select Tests

The main problem in learning decision trees for the binary-attribute case is selecting the order of the tests. For categorical and numeric attributes, we must also decide what the tests should be (besides selecting the order). Several techniques have been tried; the most popular one is at each stage to select that test that maximally reduces an entropy-like measure.

We show how this technique works for the simple case of tests with binary outcomes. Extension to multiple-outcome tests is straightforward computationally but gives poor results because entropy is always decreased by having more outcomes.

The entropy or uncertainty still remaining about the class of a pattern— knowing that it is in some set, $\Xi$ , of patterns is defined as:

$$
H (\Xi) = - \sum_ {i} p (i | \Xi) \log_ {2} p (i | \Xi)
$$

![](images/a12459440cc895ebfaeca3871dcd2c3d41f7f2dd6ef50bb129574c5f03ba9a4e.jpg)  
Figure 6.3: A Decision Tree Implementing a Decision List

where $p ( i | \Xi )$ is the probability that a pattern drawn at random from $\Xi$ belongs to class $i$ , and the summation is over all of the classes. We want to select tests at each node such that as we travel down the decision tree, the uncertainty about the class of a pattern becomes less and less.

Since we do not in general have the probabilities $p ( i | \Xi )$ , we estimate them by sample statistics. Although these estimates might be errorful, they are nevertheless useful in estimating uncertainties. Let $\hat { p } ( i | \Xi )$ be the number of patterns in $\Xi$ belonging to class $_ i$ divided by the total number of patterns in $\Xi$ . Then an estimate of the uncertainty is:

$$
\hat {H} (\Xi) = - \sum_ {i} \hat {p} (i | \Xi) \log_ {2} \hat {p} (i | \Xi)
$$

For simplicity, from now on we’ll drop the “hats” and use sample statistics as if they were real probabilities.

If we perform a test, $T$ , having $k$ possible outcomes on the patterns in $\Xi$ , we will create $k$ subsets, $\Xi _ { 1 } , \Xi _ { 2 } , \ldots , \Xi _ { k }$ . Suppose that $n _ { i }$ of the patterns in $\Xi$ are in $\Xi _ { i }$ for $i = 1 , . . . , k$ . (Some $n _ { i }$ may be $0$ .) If we knew that $T$ applied to a pattern in $\Xi$ resulted in the $j$ -th outcome (that is, we knew that the pattern was in $\Xi _ { j }$ ), the uncertainty about its class would be:

$$
H \left(\Xi_ {j}\right) = - \sum_ {i} p \left(i \mid \Xi_ {j}\right) \log_ {2} p \left(i \mid \Xi_ {j}\right)
$$

and the reduction in uncertainty (beyond knowing only that the pattern was in $\Xi$ ) would be:

![](images/86a3c8b2f220e4a7e7bbb3bb4a582d24e67d0bebcb480d78d48ad395336c3c21.jpg)  
Figure 6.4: A Decision Tree with Categorical Attributes

$$
H (\Xi) - H (\Xi_ {j})
$$

Of course we cannot say that the test $T$ is guaranteed always to produce that amount of reduction in uncertainty because we don’t know that the result of the test will be the $j$ -th outcome. But we can estimate the average uncertainty over all the $\Xi _ { j }$ , by:

$$
E [ H _ {T} (\Xi) ] = \sum_ {j} p (\Xi_ {j}) H (\Xi_ {j})
$$

where by $H _ { T } ( \Xi )$ we mean the average uncertainty after performing test $T$ on the patterns in $\Xi$ , $p ( \Xi _ { j } )$ is the probability that the test has outcome $j$ , and the sum is taken from 1 to $k$ . Again, we don’t know the probabilities $p ( \Xi _ { j } )$ , but we can use sample values. The estimate $\hat { p } ( \Xi _ { j } )$ of $p ( \Xi _ { j } )$ is just the number of those patterns in $\Xi$ that have outcome $j$ divided by the total number of patterns in $\Xi$ . The average reduction in uncertainty achieved by test $T$ (applied to patterns in $\Xi$ ) is then:

$$
R _ {T} (\Xi) = H (\Xi) - E [ H _ {T} (\Xi) ]
$$

An important family of decision tree learning algorithms selects for the root of the tree that test that gives maximum reduction of uncertainty, and then applies this criterion recursively until some termination condition is met (which we shall discuss in more detail later). The uncertainty calculations are particularly simple when the tests have binary outcomes and when the attributes have

binary values. We’ll give a simple example to illustrate how the test selection mechanism works in that case.

Suppose we want to use the uncertainty-reduction method to build a decision tree to classify the following patterns:

<table><tr><td>pattern</td><td>class</td></tr><tr><td>(0, 0, 0)</td><td>0</td></tr><tr><td>(0, 0, 1)</td><td>0</td></tr><tr><td>(0, 1, 0)</td><td>0</td></tr><tr><td>(0, 1, 1)</td><td>0</td></tr><tr><td>(1, 0, 0)</td><td>0</td></tr><tr><td>(1, 0, 1)</td><td>1</td></tr><tr><td>(1, 1, 0)</td><td>0</td></tr><tr><td>(1, 1, 1)</td><td>1</td></tr></table>

What single test, $x _ { 1 }$ , $x _ { 2 }$ , or $x _ { 3 }$ , should be performed first? The illustration in Fig. 6.5 gives geometric intuition about the problem.

![](images/104a1f239294efa60185fa240ff0de08ec33aa44f1ad4f3fe8034ba59072c421.jpg)  
Figure 6.5: Eight Patterns to be Classified by a Decision Tree

The initial uncertainty for the set, $\Xi$ , containing all eight points is:

$$
H (\Xi) = - (6 / 8) \log_ {2} (6 / 8) - (2 / 8) \log_ {2} (2 / 8) = 0. 8 1
$$

Next, we calculate the uncertainty reduction if we perform $x _ { 1 }$ first. The lefthand branch has only patterns belonging to class 0 (we call them the set $\Xi _ { l }$ ), and the right-hand-branch (Ξr) has two patterns in each class. So, the uncertainty of the left-hand branch is:

$$
H _ {x _ {1}} (\Xi_ {l}) = - (4 / 4) \log_ {2} (4 / 4) - (0 / 4) \log_ {2} (0 / 4) = 0
$$

And the uncertainty of the right-hand branch is:

$$
H _ {x _ {1}} \left(\Xi_ {r}\right) = - (2 / 4) \log_ {2} (2 / 4) - (2 / 4) \log_ {2} (2 / 4) = 1
$$

Half of the patterns “go left” and half “go right” on test $x _ { 1 }$ . Thus, the average uncertainty after performing the $x _ { 1 }$ test is:

$$
1 / 2 H _ {x _ {1}} (\Xi_ {l}) + 1 / 2 H _ {x _ {1}} (\Xi_ {r}) = 0. 5
$$

Therefore the uncertainty reduction on $\Xi$ achieved by $x _ { 1 }$ is:

$$
R _ {x _ {1}} (\Xi) = 0. 8 1 - 0. 5 = 0. 3 1
$$

By similar calculations, we see that the test $x _ { 3 }$ achieves exactly the same uncertainty reduction, but $x _ { 2 }$ achieves no reduction whatsoever. Thus, our “greedy” algorithm for selecting a first test would select either $x _ { 1 }$ or $x _ { 3 }$ . Suppose $x _ { 1 }$ is selected. The uncertainty-reduction procedure would select $x _ { 3 }$ as the next test. The decision tree that this procedure creates thus implements the Boolean function: f = x1x3. $f = x _ { 1 } x _ { 3 }$

See [Quinlan, 1986, sect. 4] for another example.

# 6.2.3 Non-Binary Attributes

If the attributes are non-binary, we can still use the uncertainty-reduction technique to select tests. But now, in addition to selecting an attribute, we must select a test on that attribute. Suppose for example that the value of an attribute is a real number and that the test to be performed is to set a threshold and to test to see if the number is greater than or less than that threshold. In principle, given a set of labeled patterns, we can measure the uncertainty reduction for each test that is achieved by every possible threshold (there are only a finite number of thresholds that give different test results if there are only a finite number of training patterns). Similarly, if an attribute is categorical (with a finite number of categories), there are only a finite number of mutually exclusive and exhaustive subsets into which the values of the attribute can be split. We can calculate the uncertainty reduction for each split.

# 6.3 Networks Equivalent to Decision Trees

Since univariate Boolean decision trees are implementations of DNF functions, they are also equivalent to two-layer, feedforward neural networks. We show an example in Fig. 6.6. The decision tree at the left of the figure implements

the same function as the network at the right of the figure. Of course, when implemented as a network, all of the features are evaluated in parallel for any input pattern, whereas when implemented as a decision tree only those features on the branch traveled down by the input pattern need to be evaluated. The decision-tree induction methods discussed in this chapter can thus be thought of as particular ways to establish the structure and the weight values for networks.

![](images/4b385123288d5b2faf445f28597b23896c578aaf1b4089846c42b66bc47615e1.jpg)

![](images/7b2ca0232640718b24b9c1c430fe8c2146d93472254c8100b196a816abaaec48.jpg)  
Figure 6.6: A Univariate Decision Tree and its Equivalent Network

Multivariate decision trees with linearly separable functions at each node can also be implemented by feedforward networks—in this case three-layer ones. We show an example in Fig. 6.7 in which the linearly separable functions, each implemented by a TLU, are indicated by $L _ { 1 } , L _ { 2 } , L _ { 3 }$ , and $L _ { 4 }$ . Again, the final layer has fixed weights, but the weights in the first two layers must be trained. Different approaches to training procedures have been discussed by [Brent, 1990], by [John, 1995], and (for a special case) by [Marchand & Golea, 1993].

# 6.4 Overfitting and Evaluation

# 6.4.1 Overfitting

In supervised learning, we must choose a function to fit the training set from among a set of hypotheses. We have already showed that generalization is impossible without bias. When we know a priori that the function we are trying to guess belongs to a small subset of all possible functions, then, even with an incomplete set of training samples, it is possible to reduce the subset of functions that are consistent with the training set sufficiently to make useful guesses about the value of the function for inputs not in the training set. And,

![](images/a72ad8c5de75e1778e18e24605bfab3bc4721bb390406ded37f225db93060e47.jpg)

![](images/150fcfd750b7ae4b473fe0d636672d17ad6832f735cfd9c3f2e6f0a27fc8e39d.jpg)  
Figure 6.7: A Multivariate Decision Tree and its Equivalent Network

the larger the training set, the more likely it is that even a randomly selected consistent function will have appropriate outputs for patterns not yet seen.

However, even with bias, if the training set is not sufficiently large compared with the size of the hypothesis space, there will still be too many consistent functions for us to make useful guesses, and generalization performance will be poor. When there are too many hypotheses that are consistent with the training set, we say that we are overfitting the training data. Overfitting is a problem that we must address for all learning methods.

Since a decision tree of sufficient size can implement any Boolean function there is a danger of overfitting—especially if the training set is small. That is, even if the decision tree is synthesized to classify all the members of the training set correctly, it might perform poorly on new patterns that were not used to build the decision tree. Several techniques have been proposed to avoid overfitting, and we shall examine some of them here. They make use of methods for estimating how well a given decision tree might generalize—methods we shall describe next.

# 6.4.2 Validation Methods

The most straightforward way to estimate how well a hypothesized function (such as a decision tree) performs on a test set is to test it on the test set! But, if we are comparing several learning systems (for example, if we are comparing different decision trees) so that we can select the one that performs the best on the test set, then such a comparison amounts to “training on the test data.” True, training on the test data enlarges the training set, with a consequent expected improvement in generalization, but there is still the danger of overfitting if we are comparing several different learning systems. Another technique is to

split the training set—using (say) two-thirds for training and the other third for estimating generalization performance. But splitting reduces the size of the training set and thereby increases the possibility of overfitting. We next describe some validation techniques that attempt to avoid these problems.

# Cross-Validation

In cross-validation, we divide the training set $\Xi$ into $K$ mutually exclusive and exhaustive equal-sized subsets: $\Xi _ { 1 } , \dots , \Xi _ { K }$ . For each subset, $\Xi _ { i }$ , train on the union of all of the other subsets, and empirically determine the error rate, $\varepsilon _ { i }$ , on $\Xi _ { i }$ . (The error rate is the number of classification errors made on $\Xi _ { i }$ divided by the number of patterns in $\Xi _ { i }$ .) An estimate of the error rate that can be expected on new patterns of a classifier trained on all the patterns in $\Xi$ is then the average of the $\varepsilon _ { i }$ .

# Leave-one-out Validation

Leave-one-out validation is the same as cross validation for the special case in which $K$ equals the number of patterns in $\Xi$ , and each $\Xi _ { i }$ consists of a single pattern. When testing on each $\Xi _ { i }$ , we simply note whether or not a mistake was made. We count the total number of mistakes and divide by $K$ to get the estimated error rate. This type of validation is, of course, more expensive computationally, but useful when a more accurate estimate of the error rate for a classifier is needed.

# 6.4.3 Avoiding Overfitting in Decision Trees

Near the tips of a decision tree there may be only a few patterns per node. For these nodes, we are selecting a test based on a very small sample, and thus we are likely to be overfitting. This problem can be dealt with by terminating the test-generating procedure before all patterns are perfectly split into their separate categories. That is, a leaf node may contain patterns of more than one class, but we can decide in favor of the most numerous class. This procedure will result in a few errors but often accepting a small number of errors on the training set results in fewer errors on a testing set.

This behavior is illustrated in Fig. 6.8.

One can use cross-validation techniques to determine when to stop splitting nodes. If the cross validation error increases as a consequence of a node split, then don’t split. One has to be careful about when to stop, though, because underfitting usually leads to more errors on test sets than does overfitting. There is a general rule that the lowest error-rate attainable by a sub-tree of a fully expanded tree can be no less than 1/2 of the error rate of the fully expanded tree [Weiss & Kulikowski, 1991, page 126].

Describe “bootstrapping” also [Efron, 1982].

Figure 6.8: Determining When Overfitting Begins   
![](images/ac789f94d20309d2b1b0099856f4b4ee689727440a0bdaf867da76637ccefbd3.jpg)  
(From Weiss, S., and Kulikowski, C., Computer Systems that Learn, Morgan Kaufmann, 1991)

Rather than stopping the growth of a decision tree, one might grow it to its full size and then prune away leaf nodes and their ancestors until crossvalidation accuracy no longer increases. This technique is called post-pruning. Various techniques for pruning are discussed in [Weiss & Kulikowski, 1991].

# 6.4.4 Minimum-Description Length Methods

An important tree-growing and pruning technique is based on the minimumdescription-length (MDL) principle. (MDL is an important idea that extends beyond decision-tree methods [Rissanen, 1978].) The idea is that the simplest decision tree that can predict the classes of the training patterns is the best one. Consider the problem of transmitting just the labels of a training set of patterns, assuming that the receiver of this information already has the ordered set of patterns. If there are $m$ patterns, each labeled by one of $R$ classes, one could transmit a list of $m$ $R$ -valued numbers. Assuming equally probable classes, this transmission would require $m \log _ { 2 } R$ bits. Or, one could transmit a decision tree that correctly labelled all of the patterns. The number of bits that this transmission would require depends on the technique for encoding decision trees and on the size of the tree. If the tree is small and accurately classifies all of the patterns, it might be more economical to transmit the tree than to transmit the labels directly. In between these extremes, we might transmit a tree plus a list of labels of all the patterns that the tree misclassifies.

In general, the number of bits (or description length of the binary encoded message) is $t + d$ , where $t$ is the length of the message required to transmit the tree, and $d$ is the length of the message required to transmit the labels of

the patterns misclassified by the tree. In a sense, that tree associated with the smallest value of $t + d$ is the best or most economical tree. The MDL method is one way of adhering to the Occam’s razor principle.

Quinlan and Rivest [Quinlan & Rivest, 1989] have proposed techniques for encoding decision trees and lists of exception labels and for calculating the description length $( t + d )$ of these trees and labels. They then use the description length as a measure of quality of a tree in two ways:

a. In growing a tree, they use the reduction in description length to select tests (instead of reduction in uncertainty).   
b. In pruning a tree after it has been grown to zero error, they prune away those nodes (starting at the tips) that achieve a decrease in the description length.

These techniques compare favorably with the uncertainty-reduction method, although they are quite sensitive to the coding schemes used.

# 6.4.5 Noise in Data

Noise in the data means that one must inevitably accept some number of errors—depending on the noise level. Refusal to tolerate errors on the training set when there is noise leads to the problem of “fitting the noise.” Dealing with noise, then, requires accepting some errors at the leaf nodes just as does the fact that there are a small number of patterns at leaf nodes.

# 6.5 The Problem of Replicated Subtrees

Decision trees are not the most economical means of implementing some Boolean functions. Consider, for example, the function $f = x _ { 1 } x _ { 2 } + x _ { 3 } x _ { 4 }$ . A decision tree for this function is shown in Fig. 6.9. Notice the replicated subtrees shown circled. The DNF-form equivalent to the function implemented by this decision tree is $f = x _ { 1 } x _ { 2 } + x _ { 1 } { \overline { { x _ { 2 } } } } x _ { 3 } x _ { 4 } + { \overline { { x _ { 1 } } } } x _ { 3 } x _ { 4 }$ . This DNF form is non-minimal (in the number of disjunctions) and is equivalent to $f = x _ { 1 } x _ { 2 } + x _ { 3 } x _ { 4 }$ .

The need for replication means that it takes longer to learn the tree and that subtrees replicated further down the tree must be learned using a smaller training subset. This problem is sometimes called the fragmentation problem.

Several approaches might be suggested for dealing with fragmentation. One is to attempt to build a decision graph instead of a tree [Oliver, Dowe, & Wallace, 1992, Kohavi, 1994]. A decision graph that implements the same decisions as that of the decision tree of Fig. 6.9 is shown in Fig. 6.10.

Another approach is to use multivariate (rather than univariate tests at each node). In our example of learning $f = x _ { 1 } x _ { 2 } + x _ { 3 } x _ { 4 }$ , if we had a test for $x _ { 1 } x _ { 2 }$

![](images/a095f613886716c320c206ae9e414ad8ab5c0bb0685e1334ccbaec2589c56a2c.jpg)  
Figure 6.9: A Decision Tree with Subtree Replication

and a test for $x _ { 3 } x _ { 4 }$ , the decision tree could be much simplified, as shown in Fig. 6.11. Several researchers have proposed techniques for learning decision trees in which the tests at each node are linearly separable functions. [John, 1995] gives a nice overview (with several citations) of learning such linear discriminant trees and presents a method based on “soft entropy.”

A third method for dealing with the replicated subtree problem involves extracting propositional “rules” from the decision tree. The rules will have as antecedents the conjunctions that lead down to the leaf nodes, and as consequents the name of the class at the corresponding leaf node. An example rule from the tree with the repeating subtree of our example would be: $x _ { 1 } \wedge \neg x _ { 2 } \wedge x _ { 3 } \wedge x _ { 4 } \supset 1$ . Quinlan [Quinlan, 1987] discusses methods for reducing a set of rules to a simpler set by 1) eliminating from the antecedent of each rule any “unnecessary” conjuncts, and then 2) eliminating “unnecessary” rules. A conjunct or rule is determined to be unnecessary if its elimination has little effect on classification accuracy—as determined by a chi-square test, for example. After a rule set is processed, it might be the case that more than one rule is “active” for any given pattern, and care must be taken that the active rules do not conflict in their decision about the class of a pattern.

![](images/9b2d460ba9f89d3b10e81ea4fb501f1a216c592ea23a239b1edfe3c4b4541e9e.jpg)  
Figure 6.10: A Decision Graph

# 6.6 The Problem of Missing Attributes

To be added.

# 6.7 Comparisons

Several experimenters have compared decision-tree, neural-net, and nearestneighbor classifiers on a wide variety of problems. For a comparison of neural nets versus decision trees, for example, see [Dietterich, et al., 1990, Shavlik, Mooney, & Towell, 1991, Quinlan, 1994]. In their StatLog project, [Taylor, Michie, & Spiegalhalter, 1994] give thorough comparisons of several machine learning algorithms on several different types of problems. There seems

![](images/6a110a5b2aaffbab0b1ec6c13ce4dab2422c95517e935a99ad7d32e075da896b.jpg)  
Figure 6.11: A Multivariate Decision Tree

to be no single type of classifier that is best for all problems. And, there do not seem to be any general conclusions that would enable one to say which classifier method is best for which sorts of classification problems, although [Quinlan, 1994] does provide some intuition about properties of problems that might render them ill suited for decision trees, on the one hand, or backpropagation, on the other.

# 6.8 Bibliographical and Historical Remarks

To be added.

# Chapter 7

# Inductive Logic Programming

There are many different representational forms for functions of input variables. So far, we have seen (Boolean) algebraic expressions, decision trees, and neural networks, plus other computational mechanisms such as techniques for computing nearest neighbors. Of course, the representation most important in computer science is a computer program. For example, a Lisp predicate of binary-valued inputs computes a Boolean function of those inputs. Similarly, a logic program (whose ordinary application is to compute bindings for variables) can also be used simply to decide whether or not a predicate has value True (T) or False (F). For example, the Boolean exclusive-or (odd parity) function of two variables can be computed by the following logic program:

$$
\begin{array}{l} \operatorname {P a r i t y} (x, y) := \text {T r u e} (x), \neg \text {T r u e} (y) \\ : - \text {T r u e} (\mathrm {y}), \neg \text {T r u e} (\mathrm {x}) \\ \end{array}
$$

We follow Prolog syntax (see, for example, [Mueller & Page, 1988]), except that our convention is to write variables as strings beginning with lower-case letters and predicates as strings beginning with upper-case letters. The unary function “True” returns $T$ if and only if the value of its argument is $T$ . (We now think of Boolean functions and arguments as having values of $T$ and $F$ instead of 0 and 1.) Programs will be written in “typewriter” font.

In this chapter, we consider the matter of learning logic programs given a set of variable values for which the logic program should return $T$ (the positive instances) and a set of variable values for which it should return $F ^ { \prime }$ (the negative instances). The subspecialty of machine learning that deals with learning logic programs is called inductive logic programming (ILP) [Lavraˇc & Dˇzeroski, 1994]. As with any learning problem, this one can be quite complex and intractably difficult unless we constrain it with biases of some sort.

In ILP, there are a variety of possible biases (called language biases). One might restrict the program to Horn clauses, not allow recursion, not allow functions, and so on.

As an example of an ILP problem, suppose we are trying to induce a function Nonstop(x,y), that is to have value $T$ for pairs of cities connected by a non-stop air flight and $F$ for all other pairs of cities. We are given a training set consisting of positive and negative examples. As positive examples, we might have (A,B), (A, A1), and some other pairs; as negative examples, we might have (A1, A2), and some other pairs. In ILP, we usually have additional information about the examples, called “background knowledge.” In our air-flight problem, the background information might be such ground facts as ${ \mathrm { H u b } } ( { \mathrm { A } } )$ , Hub(B), Satellite(A1,A), plus others. (Hub(A) is intended to mean that the city denoted by A is a hub city, and Satellite(A1,A) is intended to mean that the city denoted by A1 is a satellite of the city denoted by A.) From these training facts, we want to induce a program Nonstop $( \mathbf { x } , \mathbf { y } )$ , written in terms of the background relations Hub and Satellite, that has value $T$ for all the positive instances and has value $F$ for all the negative instances. Depending on the exact set of examples, we might induce the program:

$$
\begin{array}{l} \text {N o n s t o p} (x, y): - \operatorname {H u b} (x), \operatorname {H u b} (y) \\ : - S a t e l l i t e (x, y) \\ : - S a t e l l i t e (y, x) \\ \end{array}
$$

which would have value $T$ if both of the two cities were hub cities or if one were a satellite of the other. As with other learning problems, we want the induced program to generalize well; that is, if presented with arguments not represented in the training set (but for which we have the needed background knowledge), we would like the function to guess well.

# 7.1 Notation and Definitions

In evaluating logic programs in ILP, we implicitly append the background facts to the program and adopt the usual convention that a program has value $T$ for a set of inputs if and only if the program interpreter returns $T$ when actually running the program (with background facts appended) on those inputs; otherwise it has value $F$ . Using the given background facts, the program above would return $T$ for input (A, A1), for example. If a logic program, $\pi$ , returns $T$ for a set of arguments $\mathbf { X }$ , we say that the program covers the arguments and write covers $( \pi , \mathbf { X } )$ . Following our terminology introduced in connection with version spaces, we will say that a program is sufficient if it covers all of the positive instances and that it is necessary if it does not cover any of the negative instances. (That is, a program implements a sufficient condition that a training instance is positive if it covers all of the positive training instances; it

implements a necessary condition if it covers none of the negative instances.) In the noiseless case, we want to induce a program that is both sufficient and necessary, in which case we will call it consistent. With imperfect (noisy) training sets, we might relax this criterion and settle for a program that covers all but some fraction of the positive instances while allowing it to cover some fraction of the negative instances. We illustrate these definitions schematically in Fig. 7.1.

![](images/0777e36a1da29660b811ae11bdfc62cd34e4031c3944cc890247933b9312d334.jpg)  
Figure 7.1: Sufficient, Necessary, and Consistent Programs

As in version spaces, if a program is sufficient but not necessary it can be made to cover fewer examples by specializing it. Conversely, if it is necessary but not sufficient, it can be made to cover more examples by generalizing it. Suppose we are attempting to induce a logic program to compute the relation $\rho$ . The most general logic program, which is certainly sufficient, is the one that has value $T$ for all inputs, namely a single clause with an empty body, $[ \rho : -$ ], which is called a fact in Prolog. The most special logic program, which is certainly necessary, is the one that has value $F$ for all inputs, namely $[ \rho : -$ F ]. Two of the many different ways to search for a consistent logic program are: 1) start with $\left[ \rho \ : - \ \right]$ and specialize until the program is consistent, or 2) start with $\left[ \rho \ : - \ \mathtt { F } \ \right]$ and generalize until the program is consistent. We will be discussing a method that starts with $\left[ \rho \ : - \ \right]$ , specializes until the program is necessary (but might no longer be sufficient), then reachieves sufficiency in stages by generalizing—ensuring within each stage that the program remains necessary (by specializing).

# 7.2 A Generic ILP Algorithm

Since the primary operators in our search for a consistent program are specialization and generalization, we must next discuss those operations. There are

three major ways in which a logic program might be generalized:

a. Replace some terms in a program clause by variables. (Readers familiar with substitutions in the predicate calculus will note that this process is the inverse of substitution.)   
b. Remove literals from the body of a clause.   
c. Add a clause to the program

Analogously, there are three ways in which a logic program might be specialized:

a. Replace some variables in a program clause by terms (a substitution).   
b. Add literals to the body of a clause.   
c. Remove a clause from the program

We will be presenting an ILP learning method that adds clauses to a program when generalizing and that adds literals to the body of a clause when specializing. When we add a clause, we will always add the clause $[ \rho : - \mathbf { \sigma } ]$ and then specialize it by adding literals to the body. Thus, we need only describe the process for adding literals.

Clauses can be partially ordered by the specialization relation. In general, clause $c _ { 1 }$ is more special than clause $c _ { 2 }$ if $c _ { 2 } \succeq c _ { 1 }$ . A special case, which is what we use here, is that a clause $c _ { 1 }$ is more special than a clause $c _ { 2 }$ if the set of literals in the body of $c _ { 2 }$ is a subset of those in $c _ { 1 }$ . This ordering relation can be used in a structure of partially ordered clauses, called the refinement graph, that is similar to a version space. Clause $c _ { 1 }$ is an immediate successor of clause $c _ { 2 }$ in this graph if and only if clause $c _ { 1 }$ can be obtained from clause $c _ { 2 }$ by adding a literal to the body of $c _ { 2 }$ . A refinement graph then tells us the ways in which we can specialize a clause by adding a literal to it.

Of course there are unlimited possible literals we might add to the body of a clause. Practical ILP systems restrict the literals in various ways. Typical allowed additions are:

a. Literals used in the background knowledge.   
b. Literals whose arguments are a subset of those in the head of the clause.   
c. Literals that introduce a new distinct variable different from those in the head of the clause.   
d. A literal that equates a variable in the head of the clause with another such variable or with a term mentioned in the background knowledge. (This possibility is equivalent to forming a specialization by making a substitution.)

e. A literal that is the same (except for its arguments) as that in the head of the clause. (This possibility admits recursive programs, which are disallowed in some systems.)

We can illustrate these possibilities using our air-flight example. We start with the program [Nonstop(x,y) :- ]. The literals used in the background knowledge are Hub and Satellite. Thus the literals that we might consider adding are:

```txt
Hub(x)  
Hub(y)  
Hub(z)  
Satellite(x,y)  
Satellite(y,x)  
Satellite(x,z)  
Satellite(z,y)  
(x = y) 
```

(If recursive programs are allowed, we could also add the literals Nonstop(x,z) and Nonstop(z,y).) These possibilities are among those illustrated in the refinement graph shown in Fig. 7.2. Whatever restrictions on additional literals are imposed, they are all syntactic ones from which the successors in the refinement graph are easily computed. ILP programs that follow the approach we are discussing (of specializing clauses by adding a literal) thus have well defined methods of computing the possible literals to add to a clause.

Now we are ready to write down a simple generic algorithm for inducing a logic program, $\pi$ for inducing a relation $\rho$ . We are given a training set, $\Xi$ of argument sets some known to be in the relation and some not in ; $\Xi ^ { + }$ are $\rho$ $\rho$ the positive instances, and $\Xi ^ { - }$ are the negative instances. The algorithm has an outer loop in which it successively adds clauses to make $\pi$ more and more sufficient. It has an inner loop for constructing a clause, $c$ , that is more and more necessary and in which it refers only to a subset, $\Xi _ { c u r }$ , of the training instances. (The positive instances in $\Xi _ { c u r }$ will be denoted by $\Xi _ { c u r } ^ { + }$ , and the negative ones by $\Xi _ { c u r } ^ { - }$ .) The algorithm is also given background relations and the means for adding literals to a clause. It uses a logic program interpreter to compute whether or not the program it is inducing covers training instances. The algorithm can be written as follows:

# Generic ILP Algorithm

(Adapted from [Lavraˇc & Dˇzeroski, 1994, p. 60].)

![](images/92506ee9ee015a8e37292f63622f09c487a9d9001a0e71526dc94644c8056db2.jpg)  
Figure 7.2: Part of a Refinement Graph

Initialize $\Xi _ { c u r } : = \Xi$ .

Initialize $\pi : =$ empty set of clauses.

repeat [The outer loop works to make $\pi$ sufficient.]

Initialize $c : = \rho : -$ .

repeat [The inner loop makes $c$ necessary.]

Select a literal $\it { \Delta } l$ to add to $c$ . [This is a nondeterministic choice point.]

Assign $c : = c , l$ .

until $c$ is necessary. [That is, until $c$ covers no negative instances in $\Xi _ { c u r }$

Assign $\pi : = \pi , c$ . [We add the clause $c$ to the program.]

Assign $\Xi _ { c u r } : = \Xi _ { c u r }$ − (the positive instances in $\Xi _ { c u r }$ covered by $\pi$ ).

until $\pi$ is sufficient.

(The termination tests for the inner and outer loops can be relaxed as appropriate for the case of noisy instances.)

# 7.3 An Example

We illustrate how the algorithm works by returning to our example of airline flights. Consider the portion of an airline route map, shown in Fig. 7.3. Cities $A$ , $B$ , and $C$ are “hub” cities, and we know that there are nonstop flights between all hub cities (even those not shown on this portion of the route map). The other

cities are “satellites” of one of the hubs, and we know that there are nonstop flights between each satellite city and its hub. The learning program is given a set of positive instances, $\Xi ^ { + }$ , of pairs of cities between which there are nonstop flights and a set of negative instances, $\Xi ^ { - }$ , of pairs of cities between which there are not nonstop flights. $\Xi ^ { + }$ contains just the pairs:

$$
\begin{array}{l} \{<   A, B >, <   A, C >, <   B, C >, <   B, A >, <   C, A >, <   C, B >, \\ <   A, A 1 >, <   A, A 2 >, <   A 1, A >, <   A 2, A >, <   B, B 1 >, <   B, B 2 >, \\ <   B 1, B >, <   B 2, B >, <   C, C 1 >, <   C, C 2 >, <   C 1, C >, <   C 2, C > \} \\ \end{array}
$$

For our example, we will assume that $\Xi ^ { - }$ contains all those pairs of cities shown in Fig. 7.3 that are not in $\Xi ^ { + }$ (a type of closed-world assumption). These are:

$$
\begin{array}{l} \{<   A, B 1 >, <   A, B 2 >, <   A, C 1 >, <   A, C 2 >, <   B, C 1 >, <   B, C 2 >, \\ <   B, A 1 >, <   B, A 2 >, <   C, A 1 >, <   C, A 2 >, <   C, B 1 >, <   C, B 2 >, \\ <   B 1, A >, <   B 2, A >, <   C 1, A >, <   C 2, A >, <   C 1, B >, <   C 2, B >, \\ <   A 1, B >, <   A 2, B >, <   A 1, C >, <   A 2, C >, <   B 1, C >, <   B 2, C > \} \\ \end{array}
$$

There may be other cities not shown on this map, so the training set does not necessarily exhaust all the cities.

![](images/8a4d5c527aa6d748c86fcef72e9d0e12767f3a65ccae9f3a943da61ff30b2057.jpg)  
Figure 7.3: Part of an Airline Route Map

We want the learning program to induce a program for computing the value of the relation Nonstop. The training set, Ξ, can be thought of as a partial

description of this relation in extensional form—it explicitly names some pairs in the relation and some pairs not in the relation. We desire to learn the Nonstop relation as a logic program in terms of the background relations, Hub and Satellite, which are also given in extensional form. Doing so will give us a more compact, intensional, description of the relation, and this description could well generalize usefully to other cities not mentioned in the map.

We assume the learning program has the following extensional definitions of the relations Hub and Satellite:

Hub

$$
\{<   A >, <   B >, <   C > \}
$$

All other cities mentioned in the map are assumed not in the relation Hub. We will use the notation Hub(x) to express that the city named $x$ is in the relation Hub.

# Satellite

$$
\{<   A 1, A, >, <   A 2, A >, <   B 1, B >, <   B 2, B >, <   C 1, C >, <   C 2, C > \}
$$

All other pairs of cities mentioned in the map are not in the relation Satellite. We will use the notation Satellite $( \mathbf { x } , \mathbf { y } )$ to express that the pair $< x , y >$ i s in the relation Satellite.

Knowing that the predicate Nonstop is a two-place predicate, the inner loop of our algorithm initializes the first clause to Nonstop(x,y) :- . This clause is not necessary because it covers all the negative examples (since it covers all examples). So we must add a literal to its (empty) body. Suppose (selecting a literal from the refinement graph) the algorithm adds $\operatorname { H u b } \left( \mathbf { x } \right)$ . The following positive instances in $\Xi$ are covered by Nonstop(x,y) :- Hub(x):

$$
\begin{array}{l} \{<   A, B >, <   A, C >, <   B, C >, <   B, A >, <   C, A >, <   C, B >, \\ <   A, A 1 >, <   A, A 2 >, <   B, B 1 >, <   B, B 2 >, <   C, C 1 >, <   C, C 2 > \} \\ \end{array}
$$

To compute this covering, we interpret the logic program Nonstop(x,y) :- $\operatorname { H u b } ( \mathbf { x } )$ for all pairs of cities in $\Xi$ , using the pairs given in the background relation Hub as ground facts. The following negative instances are also covered:

# 7.3. AN EXAMPLE

$$
\begin{array}{l} \{<   A, B 1 >, <   A, B 2 >, <   A, C 1 >, <   A, C 2 >, <   C, A 1 >, <   C, A 2 >, \\ <   C, B 1 >, <   C, B 2 >, <   B, A 1 >, <   B, A 2 >, <   B, C 1 >, <   B, C 2 > \} \\ \end{array}
$$

Thus, the clause is not yet necessary and another literal must be added. Suppose we next add Hub(y). The following positive instances are covered by Nonstop(x,y) :- Hub(x), Hub(y):

$$
\{<   A, B >, <   A, C >, <   B, C >, <   B, A >, <   C, A >, <   C, B > \}
$$

There are no longer any negative instances in $\Xi$ covered so the clause Nonstop(x,y) :- Hub(x), Hub(y) is necessary, and we can terminate the first pass through the inner loop.

But the program, $\pi$ , consisting of just this clause is not sufficient. These positive instances are not covered by the clause:

$$
\begin{array}{l} \{<   A, A 1 >, <   A, A 2 >, <   A 1, A >, <   A 2, A >, <   B, B 1 >, <   B, B 2 >, \\ <   B 1, B >, <   B 2, B >, <   C, C 1 >, <   C, C 2 >, <   C 1, C >, <   C 2, C > \} \\ \end{array}
$$

The positive instances that were covered by Nonstop $( \mathbf { x } , \mathbf { y } ) \ : - \ \mathrm { H u b } ( \mathbf { x } )$ , Hub(y) are removed from $\Xi$ to form the $\Xi _ { c u r }$ to be used in the next pass through the inner loop. $\Xi _ { c u r }$ consists of all the negative instances in Ξ plus the positive instances (listed above) that are not yet covered. In order to attempt to cover them, the inner loop creates another clause $c$ , initially set to Nonstop $( \mathbf { x } , \mathbf { y } )$ :- . This clause covers all the negative instances, and so we must add literals to make it necessary. Suppose we add the literal Satellite(x,y). The clause Nonstop(x,y) :- Satellite(x,y) covers no negative instances, so it is necessary. It does cover the following positive instances in $\Xi _ { c u r }$ :

$$
\{<   A 1, A >, <   A 2, A >, <   B 1, B >, <   B 2, B >, <   C 1, C >, <   C 2, C > \}
$$

These instances are removed from $\Xi _ { c u r }$ for the next pass through the inner loop. The program now contains two clauses:

$$
\begin{array}{l} \text {N o n s t o p} (x, y): - \operatorname {H u b} (x), \operatorname {H u b} (y) \\ : - \quad S a t e l l i t e (x, y) \\ \end{array}
$$

This program is not yet sufficient since it does not cover the following positive instances:

$$
\{<   A, A 1 >, <   A, A 2 >, <   B, B 1 >, <   B, B 2 >, <   C, C 1 >, <   C, C 2 > \}
$$

During the next pass through the inner loop, we add the clause Nonstop(x,y) :- Satellite(y,x). This clause is necessary, and since the program containing all three clauses is now sufficient, the procedure terminates with:

Nonstop(x,y) :- Hub(x), Hub(y)   
:- Satellite(x,y)   
:- Satellite(y,x)

Since each clause is necessary, and the whole program is sufficient, the program is also consistent with all instances of the training set. Note that this program can be applied (perhaps with good generalization) to other cities besides those in our partial map—so long as we can evaluate the relations Hub and Satellite for these other cities. In the next section, we show how the technique can be extended to use recursion on the relation we are inducing. With that extension, the method can be used to induce more general logic programs.

# 7.4 Inducing Recursive Programs

To induce a recursive program, we allow the addition of a literal having the same predicate letter as that in the head of the clause. Various mechanisms must be used to ensure that such a program will terminate; one such is to make sure that the new literal has different variables than those in the head literal. The process is best illustrated with another example. Our example continues the one using the airline map, but we make the map somewhat simpler in order to reduce the size of the extensional relations used. Consider the map shown in Fig. 7.4. Again, $B$ and $C$ are hub cities, $B 1$ and $B \mathcal { Q }$ are satellites of $B$ , $\mathit { C 1 }$ and $C 2$ are satellites of $ { C }$ . We have introduced two new cities, $B 3$ and $\zeta 3$ . No flights exist between these cities and any other cities—perhaps there are only bus routes as shown by the grey lines in the map.

We now seek to learn a program for Canfly(x,y) that covers only those pairs of cities that can be reached by one or more nonstop flights. The relation Canfly is satisfied by the following pairs of postive instances:

$$
\begin{array}{l} \{<   B 1, B >, <   B 1, B 2 >, <   B 1, C >, <   B 1, C 1 >, <   B 1, C 2 >, \\ <   B, B 1 >, <   B 2, B 1 >, <   C, B 1 >, <   C 1, B 1 >, <   C 2, B 1 >, \\ <   B 2, B >, <   B 2, C >, <   B 2, C 1 >, <   B 2, C 2 >, <   B, B 2 >, \\ <   C, B 2 >, <   C 1, B 2 >, <   C 2, B 2 >, <   B, C >, <   B, C 1 >, \\ <   B, C 2 >, <   C, B >, <   C 1, B >, <   C 2, B >, <   C, C 1 >, \\ <   C, C 2 >, <   C 1, C >, <   C 2, C >, <   C 1, C 2 >, <   C 2, C 1 > \} \\ \end{array}
$$

![](images/9f0cfe30114a5f764d1e6b392a72a54ef9e90a1b1590283d41696530d021e1b0.jpg)  
Figure 7.4: Another Airline Route Map

Using a closed-world assumption on our map, we take the negative instances of Canfly to be:

$$
\begin{array}{l} \{<   B 3, B 2 >, <   B 3, B >, <   B 3, B 1 >, <   B 3, C >, <   B 3, C 1 >, \\ <   B 3, C 2 >, <   B 3, C 3 >, <   B 2, B 3 >, <   B, B 3 >, <   B 1, B 3 >, \\ <   C, B 3 >, <   C 1, B 3 >, <   C 2, B 3 >, <   C 3, B 3 >, <   C 3, B 2 >, \\ <   C 3, B >, <   C 3, B 1 >, <   C 3, C >, <   C 3, C 1 >, <   C 3, C 2 >, \\ <   B 2, C 3 >, <   B, C 3 >, <   B 1, C 3 >, <   C, C 3 >, <   C 1, C 3 >, \\ <   C 2, C 3 > \} \\ \end{array}
$$

We will induce Canfly(x,y) using the extensionally defined background relation Nonstop given earlier (modified as required for our reduced airline map) and Canfly itself (recursively).

As before, we start with the empty program and proceed to the inner loop to construct a clause that is necessary. Suppose that the inner loop adds the background literal Nonstop $( \mathbf { x } , \mathbf { y } )$ . The clause Canfly(x,y) :- Nonstop(x,y) is necessary; it covers no negative instances. But it is not sufficient because it does not cover the following positive instances:

$$
\begin{array}{l} \{<   B 1, B 2 >, <   B 1, C >, <   B 1, C 1 >, <   B 1, C 2 >, <   B 2, B 1 >, \\ <   C, B 1 >, <   C 1, B 1 >, <   C 2, B 1 >, <   B 2, C >, <   B 2, C 1 >, \\ <   B 2, C 2 >, <   C, B 2 >, <   C 1, B 2 >, <   C 2, B 2 >, <   B, C 1 >, \\ \end{array}
$$

$$
<   B, C 2 >, <   C 1, B >, <   C 2, B >, <   C 1, C 2 >, <   C 2, C 1 > \}
$$

Thus, we must add another clause to the program. In the inner loop, we first create the clause Canfly(x,y) :- Nonstop(x,z) which introduces the new variable $z$ . We digress briefly to describe how a program containing a clause with unbound variables in its body is interpreted. Suppose we try to interpret it for the positive instance Canfly(B1,B2). The interpreter attempts to establish Nonstop(B1,z) for some $z$ . Since Nonstop(B1, B), for example, is a background fact, the interpreter returns $T$ —which means that the instance $< B 1 , B 2 >$ is covered. Suppose now, we attempt to interpret the clause for the negative instance Canfly(B3,B). The interpreter attempts to establish Nonstop(B3,z) for some $z$ . There are no background facts that match, so the clause does not cover $< B 3 , B >$ . Using the interpreter, we see that the clause Canfly $( \mathbf { x } , \mathbf { y } )$ :- Nonstop(x,z) covers all of the positive instances not already covered by the first clause, but it also covers many negative instances such as $< B 2 , B 3 >$ , and $< B , B 3 >$ . So the inner loop must add another literal. This time, suppose it adds Canfly(z,y) to yield the clause Canfly(x,y) :- Nonstop $( \mathbf { x } , z )$ , $\mathtt { C a n f l y ( z , y ) }$ . This clause is necessary; no negative instances are covered. The program is now sufficient and consistent; it is:

```csv
Canfly(x,y) :- Nonstop(x,y) :- Nonstop(x,z), Canfly(z,y) 
```

# 7.5 Choosing Literals to Add

One of the first practical ILP systems was Quinlan’s FOIL [Quinlan, 1990]. A major problem involves deciding how to select a literal to add in the inner loop (from among the literals that are allowed). In FOIL, Quinlan suggested that candidate literals can be compared using an information-like measure—similar to the measures used in inducing decision trees. A measure that gives the same comparison as does Quinlan’s is based on the amount by which adding a literal increases the odds that an instance drawn at random from those covered by the new clause is a positive instance beyond what these odds were before adding the literal.

Let $p$ be an estimate of the probability that an instance drawn at random from those covered by a clause before adding the literal is a positive instance. That is, $p =$ (number of positive instances covered by the clause)/(total number of instances covered by the clause). It is convenient to express this probability in “odds form.” The odds, $o$ , that a covered instance is positive is defined to be $o = p / ( 1 - p )$ . Expressing the probability in terms of the odds, we obtain $p = o / ( 1 + o )$ .

After selecting a literal, $\it l$ , to add to a clause, some of the instances previously covered are still covered; some of these are positive and some are negative. Let $p _ { l }$ denote the probability that an instance drawn at random from the instances covered by the new clause (with $\it l$ added) is positive. The odds will be denoted by $o _ { l }$ . We want to select a literal, $\it { \Delta } l$ , that gives maximal increase in these odds. That is, if we define $\lambda _ { l } ~ = ~ o _ { l } / o$ , we want a literal that gives a high value of $\lambda _ { l }$ . Specializing the clause in such a way that it fails to cover many of the negative instances previously covered but still covers most of the positive instances previously covered will result in a high value of $\lambda _ { l }$ . (It turns out that the value of Quinlan’s information theoretic measure increases monotonically with $\lambda _ { l }$ , so we could just as well use the latter instead.)

Besides finding a literal with a high value of $\lambda _ { l }$ , Quinlan’s FOIL system also restricts the choice to literals that:

a) contain at least one variable that has already been used,   
b) place further restrictions on the variables if the literal selected has the same predicate letter as the literal being induced (in order to prevent infinite recursion), and   
c) survive a pruning test based on the values of $\lambda _ { l }$ for those literals selected so far.

We refer the reader to Quinlan’s paper for further discussion of these points. Quinlan also discusses post-processing pruning methods and presents experimental results of the method applied to learning recursive relations on lists, on learning rules for chess endgames and for the card game Eleusis, and for some other standard tasks mentioned in the machine learning literature.

The reader should also refer to [Pazzani & Kibler, 1992, Lavraˇc & Dˇzeroski, 1994, Muggleton, 1991, Muggleton, 1992].

Discuss preprocessing, postprocessing, bottom-up methods, and LINUS.

# 7.6 Relationships Between ILP and Decision Tree Induction

The generic ILP algorithm can also be understood as a type of decision tree induction. Recall the problem of inducing decision trees when the values of attributes are categorical. When splitting on a single variable, the split at each node involves asking to which of several mutually exclusive and exhaustive subsets the value of a variable belongs. For example, if a node tested the variable $x _ { i }$ , and if $x _ { i }$ could have values drawn from $\{ A , B , C , D , E , F \}$ , then one possible split (among many) might be according to whether the value of $x _ { i }$ had as value one of $\{ A , B , C \}$ or one of $\{ D , E , F \}$ .

It is also possible to make a multi-variate split—testing the values of two or more variables at a time. With categorical variables, an $n$ -variable split would be based on which of several $n$ -ary relations the values of the variables satisfied. For example, if a node tested the variables $x _ { i }$ and $x _ { j }$ , and if $x _ { i }$ and $x _ { j }$ both could have values drawn from $\{ A , B , C , D , E , F \}$ , then one possible binary split

(among many) might be according to whether or not $< x _ { i } , x _ { j } >$ satisfied the relation $\{ < A , C > , < C , D > \}$ . (Note that our subset method of forming singlevariable splits could equivalently have been framed using 1-ary relations—which are usually called properties.)

In this framework, the ILP problem is as follows: We are given a training set, $\Xi$ , of positively and negatively labeled patterns whose components are drawn from a set of variables $\{ x , y , z , \ldots \}$ . The positively labeled patterns in $\Xi$ form an extensional definition of a relation, $R$ . We are also given background relations, $R _ { 1 } , \ldots , R _ { k }$ , on various subsets of these variables. (That is, we are given sets of tuples that are in these relations.) We desire to construct an intensional definition of $R$ in terms of the $R _ { 1 } , \ldots , R _ { k }$ , such that all of the positively labeled patterns in $\Xi$ are satisfied by $R$ and none of the negatively labeled patterns are. The intensional definition will be in terms of a logic program in which the relation $R$ is the head of a set of clauses whose bodies involve the background relations.

The generic ILP algorithm can be understood as decision tree induction, where each node of the decision tree is itself a sub-decision tree, and each subdecision tree consists of nodes that make binary splits on several variables using the background relations, $R _ { i }$ . Thus we will speak of a top-level decision tree and various sub-decision trees. (Actually, our decision trees will be decision lists—a special case of decision trees, but we will refer to them as trees in our discussions.)

In broad outline, the method for inducing an intensional version of the relation $R$ is illustrated by considering the decision tree shown in Fig. 7.5. In this diagram, the patterns in $\Xi$ are first filtered through the decision tree in toplevel node 1. The background relation $R _ { 1 }$ is satisfied by some of these patterns; these are filtered to the right (to relation $R _ { 2 }$ ), and the rest are filtered to the left (more on what happens to these later). Right-going patterns are filtered through a sequence of relational tests until only positively labeled patterns satisfy the last relation—in this case $R _ { 3 }$ . That is, the subset of patterns satisfying all the relations, $R _ { 1 }$ , $R _ { 2 }$ , and $R _ { 3 }$ contains only positive instances from $\Xi$ . (We might say that this combination of tests is necessary. They correspond to the clause created in the first pass through the inner loop of the generic ILP algorithm.) Let us call the subset of patterns satisfying these relations, $\Xi _ { 1 }$ ; these satisfy Node 1 at the top level. All other patterns, that is $\{ \Xi - \Xi _ { 1 } \} = \Xi _ { 2 }$ are filtered to the left by Node 1.

$\Xi _ { 2 }$ is then filtered by top-level Node 2 in much the same manner, so that Node 2 is satisfied only by the positively labeled samples in $\Xi _ { 2 }$ . We continue filtering through top-level nodes until only the negatively labeled patterns fail to satisfy a top node. In our example, $\Xi _ { 4 }$ contains only negatively labeled patterns and the union of $\Xi _ { 1 }$ and $\Xi _ { 3 }$ contains all the positively labeled patterns. The relation, $R$ , that distinguishes positive from negative patterns in $\Xi$ is then given in terms of the following logic program:

![](images/9249223c73c1c0b14f88639ced013d95007a2b213d18e022f5c0ec362e9853e5.jpg)  
Figure 7.5: A Decision Tree for ILP

:- R4, R5

If we apply this sort of decision-tree induction procedure to the problem of generating a logic program for the relation Nonstop (refer to Fig. 7.3), we obtain the decision tree shown in Fig. 7.6. The logic program resulting from this decision tree is the same as that produced by the generic ILP algorithm.

In setting up the problem, the training set, $\Xi$ can be expressed as a set of 2- dimensional vectors with components $x$ and $y$ . The values of these components range over the cities $\{ A , B , C , A 1 , A 2 , B 1 , B 2 , C 1 , C 2 \}$ except (for simplicity) we do not allow patterns in which $x$ and $y$ have the same value. As before, the relation, Nonstop, contains the following pairs of cities, which are the positive instances:

$$
\begin{array}{l} \{<   A, B >, <   A, C >, <   B, C >, <   B, A >, <   C, A >, <   C, B >, \\ <   A, A 1 >, <   A, A 2 >, <   A 1, A >, <   A 2, A >, <   B, B 1 >, <   B, B 2 >, \\ <   B 1, B >, <   B 2, B >, <   C, C 1 >, <   C, C 2 >, <   C 1, C >, <   C 2, C > \} \\ \end{array}
$$

All other pairs of cities named in the map of Fig. 7.3 (using the closed world assumption) are not in the relation Nonstop and thus are negative instances.

Because the values of $x$ and $y$ are categorical, decision-tree induction would be a very difficult task—involving as it does the need to invent relations on

$x$ and $y$ to be used as tests. But with the background relations, $R _ { i }$ (in this case Hub and Satellite), the problem is made much easier. We select these relations in the same way that we select literals; from among the available tests, we make a selection based on which leads to the largest value of $\lambda _ { R _ { i } }$ .

# 7.7 Bibliographical and Historical Remarks

To be added.

![](images/e0ef44dece19b74812c4498ef779befba09c6b1079d336bf4c1d39bf343712b5.jpg)  
Figure 7.6: A Decision Tree for the Airline Route Problem

# Chapter 8

# Computational Learning Theory

In chapter one we posed the problem of guessing a function given a set of sample inputs and their values. We gave some intuitive arguments to support the claim that after seeing only a small fraction of the possible inputs (and their values) that we could guess almost correctly the values of most subsequent inputs—if we knew that the function we were trying to guess belonged to an appropriately restricted subset of functions. That is, a given training set of sample patterns might be adequate to allow us to select a function, consistent with the labeled samples, from among a restricted set of hypotheses such that with high probability the function we select will be approximately correct (small probability of error) on subsequent samples drawn at random according to the same distribution from which the labeled samples were drawn. This insight led to the theory of probably approximately correct (PAC) learning—initially developed by Leslie Valiant [Valiant, 1984]. We present here a brief description of the theory for the case of Boolean functions. [Dietterich, 1990, Haussler, 1988, Haussler, 1990] give nice surveys of the important results.

Other overviews?

# 8.1 Notation and Assumptions for PAC Learning Theory

We assume a training set $\Xi$ of n-dimensional vectors, $\mathbf { X } _ { i }$ , $i = 1 , \ldots , m$ , each labeled (by 1 or 0) according to a target function, $f$ , which is unknown to the learner. The probability of any given vector $\mathbf { X }$ being in $\Xi$ , or later being presented to the learner, is $P ( \mathbf { X } )$ . The probability distribution, $P$ , can be arbitrary. (In the literature of PAC learning theory, the target function is usually called the target concept and is denoted by $c$ , but to be consistent with our previous notation we will continue to denote it by $f$ .) Our problem is to guess

a function, $h ( \mathbf { X } )$ , based on the labeled samples in $\Xi$ . In PAC theory such a guessed function is called the hypothesis. We assume that the target function is some element of a set of functions, $\boldsymbol { \mathscr { C } }$ . We also assume that the hypothesis, $h$ , is an element of a set, $\mathcal { H }$ , of hypotheses, which includes the set, $\boldsymbol { \mathscr { C } }$ , of target functions. $\mathcal { H }$ is called the hypothesis space.

In general, $h$ won’t be identical to $f$ , but we can strive to have the value of h(X) = the value of $f ( \mathbf { X } )$ for most $\mathbf { X }$ ’s. That is, we want $h$ to be approximately correct. To quantify this notion, we define the error of $h$ , $\varepsilon _ { h }$ , as the probability that an $\mathbf { X }$ drawn randomly according to $P$ will be misclassified:

$$
\varepsilon_ {h} = \sum_ {[ \mathbf {X}: h (\mathbf {X}) \neq f (\mathbf {X}) ]} P (\mathbf {X})
$$

Boldface symbols need to be smaller when they are subscripts in math environments.

We say that $h$ is approximately (except for $\varepsilon$ ) correct if $\varepsilon _ { h } \leq \varepsilon$ , where $\varepsilon$ is the accuracy parameter.

Suppose we are able to find an $h$ that classifies all $m$ randomly drawn training samples correctly; that is, $h$ is consistent with this randomly selected training set, $\Xi$ . If $m$ is large enough, will such an $h$ be approximately correct (and for what value of $\varepsilon$ )? On some training occasions, using $m$ randomly drawn training samples, such an $h$ might turn out to be approximately correct (for a given value of $\varepsilon$ ), and on others it might not. We say that $h$ is probably (except for $\delta$ ) approximately correct $( P A C )$ if the probability that it is approximately correct is greater than $1 - \delta$ , where $\delta$ is the confidence parameter. We shall show that if $m$ is greater than some bound whose value depends on $\varepsilon$ and $\delta$ , such an $h$ is guaranteed to be probably approximately correct.

In general, we say that a learning algorithm $P A C$ -learns functions from $\boldsymbol { \mathscr { C } }$ in terms of $\mathcal { H }$ iff for every function $f \epsilon \mathcal { C }$ , it outputs a hypothesis $h \epsilon ~ { \mathcal { H } }$ , such that with probability at least $( 1 - \delta )$ , $\varepsilon _ { h } \leq \varepsilon$ . Such a hypothesis is called probably (except for $\delta$ ) approximately (except for $\varepsilon$ ) correct.

We want learning algorithms that are tractable, so we want an algorithm that PAC-learns functions in polynomial time. This can only be done for certain classes of functions. If there are a finite number of hypotheses in a hypothesis set (as there are for many of the hypothesis sets we have considered), we could always produce a consistent hypothesis from this set by testing all of them against the training data. But if there are an exponential number of hypotheses, that would take exponential time. We seek training methods that produce consistent hypotheses in less time. The time complexities for various hypothesis sets have been determined, and these are summarized in a table to be presented later.

A class, $c$ , is polynomially $P A C$ learnable in terms of $\mathcal { H }$ provided there exists a polynomial-time learning algorithm (polynomial in the number of samples needed, $m$ , in the dimension, $n$ , in $1 / \varepsilon$ , and in $1 / \delta$ ) that PAC-learns functions in $c$ in terms of $\mathcal { H }$ .

Initial work on PAC assumed $\mathcal { H } = \mathcal { C }$ , but it was later shown that some functions cannot be polynomially PAC-learned under such an assumption (assuming

P 6= NP)—but can be polynomially PAC-learned if $\mathcal { H }$ is a strict superset of $\zeta !$ ! Also our definition does not specify the distribution, $P$ , from which patterns are drawn nor does it say anything about the properties of the learning algorithm. Since $c$ and $\mathcal { H }$ do not have to be identical, we have the further restrictive definition:

A properly PAC-learnable class is a class $\boldsymbol { \mathscr { C } }$ for which there exists an algorithm that polynomially PAC-learns functions from $\boldsymbol { \mathscr { C } }$ in terms of $\boldsymbol { \mathscr { C } }$ .

# 8.2 PAC Learning

# 8.2.1 The Fundamental Theorem

Suppose our learning algorithm selects some $h$ randomly from among those that are consistent with the values of $f$ on the $m$ training patterns. The probability that the error of this randomly selected $h$ is greater than some $\varepsilon$ , with $h$ consistent with the values of $f ( \mathbf { X } )$ for $m$ instances of $\mathbf { X }$ (drawn according to arbitrary $P$ ), is less than or equal to $| \mathcal { H } | e ^ { - \varepsilon m }$ , where $| \mathcal { H } |$ is the number of hypotheses in $\mathcal { H }$ . We state this result as a theorem [Blumer, et al., 1987]:

Theorem 8.1 (Blumer, et al.) Let $\mathcal { H }$ be any set of hypotheses, $\Xi$ be a set of $m \geq 1$ training examples drawn independently according to some distribution $P$ , $f$ be any classification function in $\mathcal { H }$ , and $\varepsilon > 0$ . Then, the probability that there exists a hypothesis h consistent with $f$ for the members of $\Xi$ but with error greater than $\varepsilon$ is at most $| \mathcal { H } | e ^ { - \varepsilon m }$ .

Proof:

Consider the set of all hypotheses, $\{ h _ { 1 } , h _ { 2 } , \ldots , h _ { i } , \ldots , h _ { S } \}$ , in $\mathcal { H }$ , where $S =$ $| \mathcal { H } |$ . The error for $h _ { i }$ is $\varepsilon _ { h _ { i } } =$ the probability that $h _ { i }$ will classify a pattern in error (that is, differently than $f$ would classify it). The probability that $h _ { i }$ will classify a pattern correctly is $\left( 1 - \varepsilon _ { h _ { i } } \right)$ . A subset, $\mathcal { H } _ { B }$ , of $\mathcal { H }$ will have error greater than $\varepsilon$ . We will call the hypotheses in this subset bad. The probability that any particular one of these bad hypotheses, say $h _ { b }$ , would classify a pattern correctly is $\left( 1 - \varepsilon _ { h _ { b } } \right)$ . Since $\varepsilon _ { h _ { b } } > \varepsilon$ , the probability that $h _ { b }$ (or any other bad hypothesis) would classify a pattern correctly is less than $( 1 - \varepsilon )$ . The probability that it would classify all $m$ independently drawn patterns correctly is then less than $( 1 - \varepsilon ) ^ { m }$ .

That is,

prob $\lfloor h _ { b }$ classifies all $m$ patterns correctly $| h _ { b } \ \epsilon \ \mathcal { H } _ { B } | \le ( 1 - \varepsilon ) ^ { m }$ .

prob[some h  HB classifies all $m$ patterns correctly]   
$\begin{array} { r } { = \sum _ { h _ { b } \ \epsilon \ \mathcal { H } _ { B } } \mathrm { p r o b } [ h _ { b } } \end{array}$ classifies all $m$ patterns correctly $| h _ { b } \ \epsilon \ \mathcal { H } _ { B } |$   
$\leq K ( 1 - \varepsilon ) ^ { m }$ , where $K = | \mathcal { H } _ { B } |$ .

That is,

prob[there is a bad hypothesis that classifies all $m$ patterns correctly]

$$
\leq K (1 - \varepsilon) ^ {m}.
$$

Since $K \leq | \mathcal { H } |$ and $( 1 - \varepsilon ) ^ { m } \leq e ^ { - \varepsilon m }$ , we have:

prob[there is a bad hypothesis that classifies all $m$ patterns correctly]

= prob[there is a hypothesis with error $> \varepsilon$ and that classifies all $m$ patterns correctly] $\leq | { \mathcal { H } } | e ^ { - \varepsilon m }$ .

QED

A corollary of this theorem is:

Corollary 8.2 Given $m \ge ( 1 / \varepsilon ) ( \ln | \mathcal { H } | + \ln ( 1 / \delta ) )$ independent samples, the probability that there exists a hypothesis in $\mathcal { H }$ that is consistent with $f$ on these samples and has error greater than $\varepsilon$ is at most $\delta$ .

Proof: We are to find a bound on $m$ that guarantees that

prob[there is a hypothesis with error $> \varepsilon$ and that classifies all $m$ patterns correctly] $\le ~ \delta$ . Thus, using the result of the theorem, we must show that $| \mathcal { H } | e ^ { - \varepsilon m } \leq \delta$ . Taking the natural logarithm of both sides yields:

$$
\ln | \mathcal {H} | - \varepsilon m \leq \ln \delta
$$

or

$$
m \geq (1 / \varepsilon) (\ln | \mathcal {H} | + \ln (1 / \delta))
$$

QED

This corollary is important for two reasons. First it clearly states that we can select any hypothesis consistent with the $m$ samples and be assured that with probability $( 1 - \delta )$ its error will be less than $\varepsilon$ . Also, it shows that in order for $m$ to increase no more than polynomially with $n$ , $| \mathcal { H } |$ can be no larger than $2 ^ { O ( n ^ { k } ) }$ . No class larger than that can be guaranteed to be properly PAC learnable.

Here is a possible point of confusion: The bound given in the corollary is an upper bound on the value of $m$ needed to guarantee polynomial probably approximately correct learning. Values of $m$ greater than that bound are sufficient (but might not be necessary). We will present a lower (necessary) bound later in the chapter.

# 8.2.2 Examples

Terms

Let $\mathcal { H }$ be the set of terms (conjunctions of literals). Then, $| \mathcal { H } | = 3 ^ { n }$ , and

$$
\begin{array}{l} m \geq (1 / \varepsilon) (\ln (3 ^ {n}) + \ln (1 / \delta)) \\ \geq (1 / \varepsilon) (1. 1 n + \ln (1 / \delta)) \\ \end{array}
$$

Note that the bound on $m$ increases only polynomially with $n$ , $1 / \varepsilon$ , and $1 / \delta$

For $n = 5 0$ , $\varepsilon = 0 . 0 1$ and $\delta = 0 . 0 1$ , $m \geq 5 , 9 6 1$ guarantees PAC learnability.

In order to show that terms are properly $P A C$ learnable, we additionally have to show that one can find in time polynomial in $m$ and $n$ a hypothesis $h$ consistent with a set of $m$ patterns labeled by the value of a term. The following procedure for finding such a consistent hypothesis requires $O ( n m )$ steps (adapted from [Dietterich, 1990, page 268]):

We are given a training sequence, $\Xi$ , of $m$ examples. Find the first pattern, say $\mathbf { X } _ { 1 }$ , in that list that is labeled with a 1. Initialize a Boolean function, $h$ , to the conjunction of the $n$ literals corresponding to the values of the $n$ components of $\mathbf { X } _ { 1 }$ . (Components with value 1 will have corresponding positive literals; components with value 0 will have corresponding negative literals.) If there are no patterns labeled by a 1, we exit with the null concept ( $h \equiv 0$ for all patterns). Then, for each additional pattern, $\mathbf { X } _ { i }$ , that is labeled with a 1, we delete from $h$ any Boolean variables appearing in $\mathbf { X } _ { i }$ with a sign different from their sign in $h$ . After processing all the patterns labeled with a 1, we check all of the patterns labeled with a 0 to make sure that none of them is assigned value 1 by $h$ . If, at any stage of the algorithm, any patterns labeled with a 0 are assigned a 1 by $h$ , then there exists no term that consistently classifies the patterns in $\Xi$ , and we exit with failure. Otherwise, we exit with $h$ .

As an example, consider the following patterns, all labeled with a 1 (from [Dietterich, 1990]):

After processing the first pattern, we have $h = { \overline { { x _ { 1 } } } } x _ { 2 } x _ { 3 } { \overline { { x _ { 4 } } } }$ ; after processing the second pattern, we have $h = x _ { 2 } x _ { 3 } { \overline { { x _ { 4 } } } }$ ; finally, after the third pattern, we have $h = x _ { 2 } { \overline { { x _ { 4 } } } }$ .

# Linearly Separable Functions

Let $\mathcal { H }$ be the set of all linearly separable functions. Then, $\vert \mathcal { H } \vert \le 2 ^ { n ^ { 2 } }$ , and

Change this paragraph if this algorithm was presented in Chapter Three.

$$
m \geq (1 / \varepsilon) \left(n ^ {2} \ln 2 + \ln (1 / \delta)\right)
$$

Again, note that the bound on $m$ increases only polynomially with $n$ , $1 / \varepsilon$ , and $1 / \delta$ .

For $n = 5 0$ , $\varepsilon = 0 . 0 1$ and $\delta = 0 . 0 1$ , $m \geq 1 7 3 , 7 4 8$ guarantees PAC learnability.

To show that linearly separable functions are properly $P A C$ learnable, we would have additionally to show that one can find in time polynomial in $m$ and $n$ a hypothesis $h$ consistent with a set of $m$ labeled linearly separable patterns.

# 8.2.3 Some Properly PAC-Learnable Classes

Some properly PAC-learnable classes of functions are given in the following table. (Adapted from [Dietterich, 1990, pages 262 and 268] which also gives references to proofs of some of the time complexities.)

<table><tr><td>H</td><td>|H|</td><td>Time Complexity</td><td>P. Learnable?</td></tr><tr><td>terms</td><td>3n</td><td>polynomial</td><td>yes</td></tr><tr><td>k-term DNF (k disjunctive terms)</td><td>2O(kn)</td><td>NP-hard</td><td>no</td></tr><tr><td>k-DFN (a disjunction of k-sized terms)</td><td>2O(nk)</td><td>polynomial</td><td>yes</td></tr><tr><td>k-CNF (a conjunction of k-sized clauses)</td><td>2O(nk)</td><td>polynomial</td><td>yes</td></tr><tr><td>k-DL (decision lists with k-sized terms)</td><td>2O(nklg n)</td><td>polynomial</td><td>yes</td></tr><tr><td>lin. sep.</td><td>2O(n2)</td><td>polynomial</td><td>yes</td></tr><tr><td>lin. sep. with (0,1) weights</td><td>?</td><td>NP-hard</td><td>no</td></tr><tr><td>k-2NN</td><td>?</td><td>NP-hard</td><td>no</td></tr><tr><td>DNF (all Boolean functions)</td><td>22n</td><td>polynomial</td><td>no</td></tr></table>

(Members of the class $k$ -2NN are two-layer, feedforward neural networks with exactly $k$ hidden units and one output unit.)

Summary: In order to show that a class of functions is Properly PAC-Learnable :

a. Show that there is an algorithm that produces a consistent hypothesis on m $n$ -dimensional samples in time polynomial in $m$ and $n$ .   
b. Show that the sample size, $m$ , needed to ensure PAC learnability is polynomial (or better) in $( 1 / \varepsilon$ ), $( 1 / \delta$ ), and $n$ by showing that $\ln \left| \mathcal { H } \right|$ is polynomial or better in the number of dimensions.

Linear programming is polynomial.

As hinted earlier, sometimes enlarging the class of hypotheses makes learning easier. For example, the table above shows that $k$ -CNF is PAC learnable, but $k$ -term-DNF is not. And yet, $k$ -term-DNF is a subclass of $k$ -CNF! So, even if the target function were in $k$ -term-DNF, one would be able to find a hypothesis in $k$ -CNF that is probably approximately correct for the target function. Similarly, linearly separable functions implemented by TLUs whose weight values are restricted to 0 and 1 are not properly PAC learnable, whereas unrestricted linearly separable functions are. It is possible that enlarging the space of hypotheses makes finding one that is consistent with the training examples easier. An interesting question is whether or not the class of functions in $k$ -2NN is polynomially PAC learnable if the hypotheses are drawn from $k ^ { \prime }$ -2NN with $k ^ { \prime } > k$ . (At the time of writing, this matter is still undecided.)

Although PAC learning theory is a powerful analytic tool, it (like complexity theory) deals mainly with worst-case results. The fact that the class of twolayer, feedforward neural networks is not polynomially PAC learnable is more an attack on the theory than it is on the networks, which have had many successful applications. As [Baum, 1994, page 416-17] says: “ . . . humans are capable of learning in the natural world. Therefore, a proof within some model of learning that learning is not feasible is an indictment of the model. We should examine the model to see what constraints can be relaxed and made more realistic.”

# 8.3 The Vapnik-Chervonenkis Dimension

# 8.3.1 Linear Dichotomies

Consider a set, $\mathcal { H }$ , of functions, and a set, $\Xi$ , of (unlabeled) patterns. One measure of the expressive power of a set of hypotheses, relative to $\Xi$ , is its ability to make arbitrary classifications of the patterns in $\Xi$ .1 If there are $m$ patterns in $\Xi$ , there are $2 ^ { m }$ different ways to divide these patterns into two disjoint and exhaustive subsets. We say there are $2 ^ { m }$ different dichotomies of $\Xi$ . If $\Xi$ were to include all of the $2 ^ { n }$ Boolean patterns, for example, there are $2 ^ { 2 ^ { n } }$ ways to dichotomize them, and (of course) the set of all possible Boolean functions dichotomizes them in all of these ways. But a subset, $\mathcal { H }$ , of the Boolean functions might not be able to dichotomize an arbitrary set, $\Xi$ , of $m$ Boolean patterns in all $2 ^ { m }$ ways. In general (that is, even in the non-Boolean case), we say that if a subset, $\mathcal { H }$ , of functions can dichotomize a set, $\Xi$ , of $m$ patterns in all $2 ^ { m }$ ways, then $\mathcal { H }$ shatters $\Xi$ .

As an example, consider a set $\Xi$ of $m$ patterns in the n-dimensional space, $\mathcal { R } ^ { n }$ . (That is, the $n$ components of these patterns are real numbers.) We define a linear dichotomy as one implemented by an $( n - 1 )$ -dimensional hyperplane in the $n$ -dimensional space. How many linear dichotomies of $m$ patterns in $n$ dimensions are there? For example, as shown in Fig. 8.1, there are 14 dichotomies

of four points in two dimensions (each separating line yields two dichotomies depending on whether the points on one side of the line are classified as 1 or $0$ ). (Note that even though there are an infinite number of hyperplanes, there are, nevertheless, only a finite number of ways in which hyperplanes can dichotomize a finite number of patterns. Small movements of a hyperplane typically do not change the classifications of any patterns.)

![](images/e51e70401eee78ddc81e56846f4cbe26cf3f95bcf985790c6a4856feaab39d58.jpg)  
14 dichotomies of 4 points in 2 dimensions   
Figure 8.1: Dichotomizing Points in Two Dimensions

The number of dichotomies achievable by hyperplanes depends on how the patterns are disposed. For the maximum number of linear dichotomies, the points must be in what is called general position. For $m > n$ , we say that a set of $m$ points is in general position in an $n$ -dimensional space if and only if no subset of $( n + 1 )$ points lies on an $( n - 1 )$ -dimensional hyperplane. When $m \leq n$ , a set of $m$ points is in general position if no $( m - 2 )$ -dimensional hyperplane contains the set. Thus, for example, a set of $m \geq 4$ points is in general position in a three-dimensional space if no four of them lie on a (two-dimensional) plane. We will denote the number of linear dichotomies of $m$ points in general position in an $n$ -dimensional space by the expression $\Pi _ { L } ( m , n )$ .

It is not too difficult to verify that:

$$
\begin{array}{l} \Pi_ {L} (m, n) = 2 \sum_ {i = 0} ^ {n} C (m - 1, i) \qquad \text {f o r} m > n, \text {a n d} \\ = 2 ^ {m} \qquad \mathrm {f o r} m \leq n \\ \end{array}
$$

Include the derivation.

where $C ( m - 1 , i )$ is the binomial coefficient $\frac { ( m - 1 ) ! } { ( m - 1 - i ) ! i ! }$ .

The table below shows some values for $\Pi _ { L } ( m , n )$ .

<table><tr><td rowspan="2">m
(no. of patterns)</td><td colspan="5">n
(length)</td></tr><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>1</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>2</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td></tr><tr><td>3</td><td>6</td><td>8</td><td>8</td><td>8</td><td>8</td></tr><tr><td>4</td><td>8</td><td>14</td><td>16</td><td>16</td><td>16</td></tr><tr><td>5</td><td>10</td><td>22</td><td>30</td><td>32</td><td>32</td></tr><tr><td>6</td><td>12</td><td>32</td><td>52</td><td>62</td><td>64</td></tr><tr><td>7</td><td>14</td><td>44</td><td>84</td><td>114</td><td>126</td></tr><tr><td>8</td><td>16</td><td>58</td><td>128</td><td>198</td><td>240</td></tr></table>

Note that the class of linear dichotomies shatters the $m$ patterns if $m \leq n + 1$ . The bold-face entries in the table correspond to the highest values of $m$ for which linear dichotomies shatter $m$ patterns in $n$ dimensions.

# 8.3.2 Capacity

Let Pm,n $\begin{array} { r } { P _ { m , n } = \frac { \Pi _ { L } ( m , n ) } { 2 ^ { m } } = \mathrm { t h e } } \end{array}$ ΠL(m,n)m = the probability that a randomly selected dichotomy (out of the $2 ^ { m }$ possible dichotomies of $m$ patterns in $n$ dimensions) will be linearly separable. In Fig. 8.2 we plot $P _ { \lambda ( n + 1 ) , n }$ versus $\lambda$ and $n$ , where $\lambda = m / ( n + 1 )$ .

Note that for large $n$ (say $n > 3 0$ ) how quickly $P _ { m , n }$ falls from 1 to 0 as $m$ goes above $2 ( n + 1 )$ . For $m < 2 ( n + 1 )$ , any dichotomy of the $m$ points is almost certainly linearly separable. But for $m > 2 ( n + 1 )$ , a randomly selected dichotomy of the $m$ points is almost certainly not linearly separable. For this reason $m = 2 ( n + 1 )$ is called the capacity of a TLU [Cover, 1965]. Unless the number of training patterns exceeds the capacity, the fact that a TLU separates those training patterns according to their labels means nothing in terms of how well that TLU will generalize to new patterns. There is nothing special about a separation found for $m < 2 ( n + 1 )$ patterns—almost any dichotomy of those patterns would have been linearly separable. To make sure that the separation found is forced by the training set and thus generalizes well, it has to be the case that there are very few linearly separable functions that would separate the $m$ training patterns.

Analogous results about the generalizing abilities of neural networks have been developed by [Baum & Haussler, 1989] and given intuitive and experimental justification in [Baum, 1994, page 438]:

“The results seemed to indicate the following heuristic rule holds. If $M$ examples [can be correctly classified by] a net with $W$ weights (for $M > > W$ ), the net will make a fraction $\varepsilon$ of errors on new examples chosen from the same [uniform] distribution where $\varepsilon = W / M$ .”

![](images/ccb257cab9db2ed58a753bfbea4ca39ee62415b1972be664e9d466f6fa321d6c.jpg)  
Figure 8.2: Probability that a Random Dichotomy is Linearly Separable

# 8.3.3 A More General Capacity Result

Corollary 7.2 gave us an expression for the number of training patterns sufficient to guarantee a required level of generalization—assuming that the function we were guessing was a function belonging to a class of known and finite cardinality. The capacity result just presented applies to linearly separable functions for nonbinary patterns. We can extend these ideas to general dichotomies of non-binary patterns.

In general, let us denote the maximum number of dichotomies of any set of $m$ $n$ -dimensional patterns by hypotheses in $\mathcal { H }$ as $\Pi _ { \mathcal { H } } ( m , n )$ . The number of dichotomies will, of course, depend on the disposition of the $m$ points in the $n$ -dimensional space; we take $\Pi _ { \mathcal { H } } ( m , n )$ to be the maximum over all possible arrangements of the $m$ points. (In the case of the class of linearly separable functions, the maximum number is achieved when the $m$ points are in general position.) For each class, $\mathcal { H }$ , there will be some maximum value of $m$ for which $\Pi _ { \mathcal { H } } ( m , n ) = 2 ^ { m }$ , that is, for which $\mathcal { H }$ shatters the $m$ patterns. This maximum number is called the Vapnik-Chervonenkis (VC) dimension and is denoted by VCdim $( { \mathcal { H } } )$ [Vapnik & Chervonenkis, 1971].

We saw that for the class of linear dichotomies, $\mathrm { V C d i m } ( L i n e a r ) = ( n + 1 )$ . As another example, let us calculate the VC dimension of the hypothesis space of single intervals on the real line—used to classify points on the real line. We show an example of how points on the line might be dichotomized by a single interval in Fig. 8.3. The set $\Xi$ could be, for example, $\{ 0 . 5 , 2 . 5 , - 2 . 3 , 3 . 1 4 \}$ , and one of the hypotheses in our set would be [1, 4.5]. This hypothesis would label the points 2.5 and 3.14 with a 1 and the points - 2.3 and 0.5 with a 0. This

set of hypotheses (single intervals on the real line) can arbitrarily classify any two points. But no single interval can classify three points such that the outer two are classified as 1 and the inner one as 0. Therefore the VC dimension of single intervals on the real line is 2. As soon as we have many more than 2 training patterns on the real line and provided we know that the classification function we are trying to guess is a single interval, then we begin to have good generalization.

![](images/73cd7871e244c23f5e1b262ac1152d565a8596508335d467ee8a0dce00f8df19.jpg)  
Figure 8.3: Dichotomizing Points by an Interval

The VC dimension is a useful measure of the expressive power of a hypothesis set. Since any dichotomy of VCdim(H) or fewer patterns in general position in $n$ dimensions can be achieved by some hypothesis in $\mathcal { H }$ , we must have many more than VCdim(H) patterns in the training set in order that a hypothesis consistent with the training set is sufficiently constrained to imply good generalization. Our examples have shown that the concept of VC dimension is not restricted to Boolean functions.

# 8.3.4 Some Facts and Speculations About the VC Dimension

• If there are a finite number, $| \mathcal { H } |$ , of hypotheses in $\mathcal { H }$ , then: $\mathrm { V C d i m } ( \mathcal { H } ) \leq \log ( | \mathcal { H } | )$   
• The VC dimension of terms in $n$ dimensions is $n$ .   
• Suppose we generalize our example that used a hypothesis set of single intervals on the real line. Now let us consider an $n$ -dimensional feature space and tests of the form $L _ { i } \leq x _ { i } \leq H _ { i }$ . We allow only one such test per dimension. A hypothesis space consisting of conjunctions of these tests (called axis-parallel hyper-rectangles) has VC dimension bounded by:

$$
n \leq \mathrm {V C d i m} \leq 2 n
$$

• As we have already seen, TLUs with $n$ inputs have a VC dimension of $n + 1$ .   
• [Baum, 1994, page 438] gives experimental evidence for the proposition that “ . . . multilayer [neural] nets have a VC dimension roughly equal to their total number of [adjustable] weights.”

# 8.4 VC Dimension and PAC Learning

There are two theorems that connect the idea of VC dimension with PAC learning [Blumer, et al., 1990]. We state these here without proof.

Theorem 8.3 (Blumer, et al.) A hypothesis space $\mathcal { H }$ is PAC learnable iff it has finite VC dimension.

Theorem 8.4 A set of hypotheses, $\mathcal { H }$ , is properly PAC learnable if:

a. $m \geq ( 1 / \varepsilon ) \operatorname* { m a x } \left[ 4 \log ( 2 / \delta ) \right.$ , 8 VCdim lg(13/ε)], and   
b. if there is an algorithm that outputs a hypothesis h  H consistent with the training set in polynomial (in m and n) time.

The second of these two theorems improves the bound on the number of training patterns needed for linearly separable functions to one that is linear in $n$ . In our previous example of how many training patterns were needed to ensure PAC learnability of a linearly separable function if $n = 5 0$ , $\varepsilon = 0 . 0 1$ , and $\delta = 0 . 0 1$ , we obtained $m \geq 1 7 3 , 7 4 8$ . Using the Blumer, et al. result we would get $m \geq 5 2 , 7 5 6$ .

As another example of the second theorem, let us take $\mathcal { H }$ to be the set of closed intervals on the real line. The VC dimension is 2 (as shown previously). With $n = 5 0$ , $\varepsilon = 0 . 0 1$ , and $\delta = 0 . 0 1$ , $m \geq 1 6 , 5 5 1$ ensures PAC learnability.

There is also a theorem that gives a lower (necessary) bound on the number of training patterns required for PAC learning [Ehrenfeucht, et al., 1988]:

Theorem 8.5 Any $P A C$ learning algorithm must examine at least $\Omega ( 1 / \varepsilon \log ( 1 / \delta ) + \mathrm { V C d i m } ( \mathcal { H } ) )$ training patterns.

The difference between the lower and upper bounds is $O ( \log ( 1 / \varepsilon ) \mathrm { V C d i m } ( \mathcal { H } ) / \varepsilon )$ .

# 8.5 Bibliographical and Historical Remarks

# Chapter 9

# Unsupervised Learning

# 9.1 What is Unsupervised Learning?

Consider the various sets of points in a two-dimensional space illustrated in Fig. 9.1. The first set (a) seems naturally partitionable into two classes, while the second (b) seems difficult to partition at all, and the third (c) is problematic. Unsupervised learning uses procedures that attempt to find natural partitions of patterns. There are two stages:

• Form an $R$ -way partition of a set $\Xi$ of unlabeled training patterns (where the value of $R$ , itself, may need to be induced from the patterns). The partition separates $\Xi$ into $R$ mutually exclusive and exhaustive subsets, $\Xi _ { 1 } , \dots , \Xi _ { R }$ , called clusters.   
• Design a classifier based on the labels assigned to the training patterns by the partition.

We will explain shortly various methods for deciding how many clusters there should be and for separating a set of patterns into that many clusters. We can base some of these methods, and their motivation, on minimum-descriptionlength (MDL) principles. In that setting, we assume that we want to encode a description of a set of points, $\Xi$ , into a message of minimal length. One encoding involves a description of each point separately; other, perhaps shorter, encodings might involve a description of clusters of points together with how each point in a cluster can be described given the cluster it belongs to. The specific techniques described in this chapter do not explicitly make use of MDL principles, but the MDL method has been applied with success. One of the MDL-based methods, Autoclass II [Cheeseman, et al., 1988] discovered a new classification of stars based on the properties of infrared sources.

Another type of unsupervised learning involves finding hierarchies of partitionings or clusters of clusters. A hierarchical partition is one in which $\Xi$ is

![](images/e00b94d5697e62214ceb2b1a07d63045fc8bc058dc84da2d83a7708d96883112.jpg)  
a) two clusters

![](images/e39c58eec11202b88bd2055f148db2da552251efba2bb17ba1430306ea1ccd25.jpg)  
c) ?

![](images/4e4d76c70478e82a932b9bf23b994fa971972b311d842d5fffadae52622af791.jpg)  
b) one cluster   
Figure 9.1: Unlabeled Patterns

divided into mutually exclusive and exhaustive subsets, $\Xi _ { 1 } , \dots , \Xi _ { R }$ ; each set, $\Xi _ { i }$ , $( i = 1 , \ldots , R$ ) is divided into mutually exclusive and exhaustive subsets, and so on. We show an example of such a hierarchical partition in Fig. 9.2. The hierarchical form is best displayed as a tree, as shown in Fig. 9.3. The tip nodes of the tree can further be expanded into their individual pattern elements. One application of such hierarchical partitions is in organizing individuals into taxonomic hierarchies such as those used in botany and zoology.

# 9.2 Clustering Methods

# 9.2.1 A Method Based on Euclidean Distance

Most of the unsupervised learning methods use a measure of similarity between patterns in order to group them into clusters. The simplest of these involves defining a distance between patterns. For patterns whose features are numeric, the distance measure can be ordinary Euclidean distance between two points in an $n$ -dimensional space.

There is a simple, iterative clustering method based on distance. It can be described as follows. Suppose we have $R$ randomly chosen cluster seekers, $\mathbf { C } _ { 1 } , \ldots , \mathbf { C } _ { R }$ . These are points in an $n$ -dimensional space that we want to adjust so that they each move toward the center of one of the clusters of patterns. We present the (unlabeled) patterns in the training set, $\Xi$ , to the algorithm

![](images/94942e3cabecfab4f1b32640c64cc6d3b2a89d27a43d8bc51ceb08a7486f4ef4.jpg)  
Figure 9.2: A Hierarchy of Clusters

one-by-one. For each pattern, $\mathbf { X } _ { i }$ , presented, we find that cluster seeker, $\mathbf { C } _ { j }$ , that is closest to $\mathbf { X } _ { i }$ and move it closer to $\mathbf { X } _ { i }$ :

$$
\mathbf {C} _ {j} \leftarrow (1 - \alpha_ {j}) \mathbf {C} _ {j} + \alpha_ {j} \mathbf {X} _ {i}
$$

where $\alpha _ { j }$ is a learning rate parameter for the $j$ -th cluster seeker; it determines how far $\mathbf { C } _ { j }$ is moved toward $\mathbf { X } _ { i }$ .

Refinements on this procedure make the cluster seekers move less far as training proceeds. Suppose each cluster seeker, $\mathbf { C } _ { j }$ , has a mass, $m _ { j }$ , equal to the number of times that it has moved. As a cluster seeker’s mass increases it moves less far towards a pattern. For example, we might set $\alpha _ { j } = 1 / ( 1 + m _ { j } )$ and use the above rule together with $m _ { j } \longleftarrow m _ { j } + 1$ . With this adjustment rule, a cluster seeker is always at the center of gravity (sample mean) of the set of patterns toward which it has so far moved. Intuitively, if a cluster seeker ever gets within some reasonably well clustered set of patterns (and if that cluster seeker is the only one so located), it will converge to the center of gravity of that cluster.

![](images/7a749cda21473fa0880010eb05f61b1c42de7120c21a77fbdcd01f47d5d18b63.jpg)  
Figure 9.3: Displaying a Hierarchy as a Tree

Once the cluster seekers have converged, the classifier implied by the nowlabeled patterns in $\Xi$ can be based on a Voronoi partitioning of the space (based on distances to the various cluster seekers). This kind of classification, an example of which is shown in Fig. 9.4, can be implemented by a linear machine.

When basing partitioning on distance, we seek clusters whose patterns are as close together as possible. We can measure the badness, $V$ , of a cluster of patterns, $\{ { \bf X } _ { i } \}$ , by computing its sample variance defined by:

$$
V = (1 / K) \sum_ {i} (\mathbf {X} _ {i} - \mathbf {M}) ^ {2}
$$

where $\mathbf { M }$ is the sample mean of the cluster, which is defined to be:

$$
\mathbf {M} = (1 / K) \sum_ {i} \mathbf {X} _ {i}
$$

and $K$ is the number of points in the cluster.

We would like to partition a set of patterns into clusters such that the sum of the sample variances (badnesses) of these clusters is small. Of course if we have one cluster for each pattern, the sample variances will all be zero, so we must arrange that our measure of the badness of a partition must increase with the number of clusters. In this way, we can seek a trade-off between the variances of

Georgy Fedoseevich Voronoi, was a Russian mathematician who lived from 1868 to 1909.

![](images/7fc8bbbc8a98d6679d84df66a69577a3e846b470c072e33502ef18c5a9158683.jpg)  
Figure 9.4: Minimum-Distance Classification

the clusters and the number of them in a way somewhat similar to the principle of minimal description length discussed earlier.

Elaborations of our basic cluster-seeking procedure allow the number of cluster seekers to vary depending on the distances between them and depending on the sample variances of the clusters. For example, if the distance, $d _ { i j }$ , between two cluster seekers, $\mathbf { C } _ { i }$ and $\mathbf { C } _ { j }$ , ever falls below some threshold $\varepsilon$ , then we can replace them both by a single cluster seeker placed at their center of gravity (taking into account their respective masses). In this way we can decrease the overall badness of a partition by reducing the number of clusters for comparatively little penalty in increased variance.

On the other hand, if any of the cluster seekers, say $\mathbf { C } _ { i }$ , defines a cluster whose sample variance is larger than some amount $\delta$ , then we can place a new cluster seeker, $\mathbf { C } _ { j }$ , at some random location somewhat adjacent to $\mathbf { C } _ { i }$ and reset the masses of both $\mathbf { C } _ { i }$ and $\mathbf { C } _ { j }$ to zero. In this way the badness of the partition might ultimately decrease by decreasing the total sample variance with comparatively little penalty for the additional cluster seeker. The values of the parameters $\varepsilon$ and $\delta$ are set depending on the relative weights given to sample variances and numbers of clusters.

In distance-based methods, it is important to scale the components of the pattern vectors. The variation of values along some dimensions of the pattern vector may be much different than that of other dimensions. One commonly used technique is to compute the standard deviation (i.e., the square root of the variance) of each of the components over the entire training set and normalize the values of the components so that their adjusted standard deviations are equal.

# 9.2.2 A Method Based on Probabilities

Suppose we have a partition of the training set, $\Xi$ , into $R$ mutually exclusive and exhaustive clusters, $C _ { 1 } , \ldots , C _ { R }$ . We can decide to which of these clusters some arbitrary pattern, $\mathbf { X }$ , should be assigned by selecting the $C _ { i }$ for which the probability, $p ( C _ { i } | \mathbf { X } )$ , is largest, providing $p ( C _ { i } | \mathbf { X } )$ is larger than some fixed threshold, $\delta$ . As we saw earlier, we can use Bayes rule and base our decision on maximizing $p ( \mathbf { X } | C _ { i } ) p ( C _ { i } )$ . Assuming conditional independence of the pattern components, $x _ { i }$ , the quantity to be maximized is:

$$
S (\mathbf {X}, C _ {i}) = p (x _ {1} | C _ {i}) p (x _ {2} | C _ {i}) \dots p (x _ {n} | C _ {i}) p (C _ {i})
$$

The $p ( x _ { j } | C _ { i } )$ can be estimated from the sample statistics of the patterns in the clusters and then used in the above expression. (Recall the linear form that this formula took in the case of binary-valued components.)

We call $S ( \mathbf { X } , C _ { i } )$ the similarity of $\mathbf { X }$ to a cluster, $C _ { i }$ , of patterns. Thus, we assign $\mathbf { X }$ to the cluster to which it is most similar, providing the similarity is larger than $\delta$ .

Just as before, we can define the sample mean of a cluster, $C _ { i }$ , to be:

$$
\mathbf {M} _ {i} = \left(1 / K _ {i}\right) \sum_ {\mathbf {X} _ {j} \in C _ {i}} \mathbf {X} _ {j}
$$

where $K _ { i }$ is the number of patterns in $C _ { i }$ .

We can base an iterative clustering algorithm on this measure of similarity [Mahadevan & Connell, 1992]. It can be described as follows:

a. Begin with a set of unlabeled patterns $\Xi$ and an empty list, $L$ , of clusters.   
b. For the next pattern, $\mathbf { X }$ , in $\Xi$ , compute $S ( \mathbf { X } , C _ { i } )$ for each cluster, $C _ { i }$ . (Initially, these similarities are all zero.) Suppose the largest of these similarities is $S ( \mathbf { X } , C _ { m a x } )$ .

(a) If $S ( \mathbf { X } , C _ { m a x } ) > \delta$ , assign $\mathbf { X }$ to $C _ { m a x }$ . That is,

$$
C _ {m a x} \leftarrow C _ {m a x} \cup \{\mathbf {X} \}
$$

Update the sample statistics $p ( x _ { 1 } | C _ { m a x } ) , p ( x _ { 2 } | C _ { m a x } ) , \dots , p ( x _ { n } | C _ { m a x } )$ and $p ( C _ { m a x } )$ to take the new pattern into account. Go to 3.

(b) If $S ( \mathbf { X } , C _ { m a x } ) \leq \delta$ , create a new cluster, $C _ { n e w } = \{ \mathbf { X } \}$ and add $C _ { n e w }$ to $L$ . Go to 3.

c. Merge any existing clusters, $C _ { i }$ and $C _ { j }$ if $( \mathbf { M } _ { i } - \mathbf { M } _ { j } ) ^ { 2 } \ < \ \varepsilon$ . Compute new sample statistics $p ( x _ { 1 } | C _ { m e r g e } ) , p ( x _ { 2 } | C _ { m e r g e } ) , \ldots , p ( x _ { n } | C _ { m e r g e } )$ , and $p ( C _ { m e r g e } )$ for the merged cluster, $C _ { m e r g e } = C _ { i } \cup C _ { j }$ .

d. If the sample statistics of the clusters have not changed during an entire iteration through $\Xi$ , then terminate with the clusters in $L$ ; otherwise go to 2.

The value of the parameter $\delta$ controls the number of clusters. If $\delta$ is high, there will be a large number of clusters with few patterns in each cluster. For small values of $\delta$ , there will be a small number of clusters with many patterns in each cluster. Similarly, the larger the value of $\varepsilon$ , the smaller the number clusters that will be found.

Designing a classifier based on the patterns labeled by the partitioning is straightforward. We assign any pattern, $\mathbf { X }$ , to that category that maximizes $S ( \mathbf { X } , C _ { i } )$ .

Mention “ $k$ -means and “EM” methods.

# 9.3 Hierarchical Clustering Methods

# 9.3.1 A Method Based on Euclidean Distance

Suppose we have a set, $\Xi$ , of unlabeled training patterns. We can form a hierarchical classification of the patterns in $\Xi$ by a simple agglomerative method. (The description of this algorithm is based on an unpublished manuscript by Pat Langley.) Our description here gives the general idea; we leave it to the reader to generate a precise algorithm.

We first compute the Euclidean distance between all pairs of patterns in $\Xi$ . (Again, appropriate scaling of the dimensions is assumed.) Suppose the smallest distance is between patterns $\mathbf { X } _ { i }$ and $\mathbf { X } _ { j }$ . We collect $\mathbf { X } _ { i }$ and $\mathbf { X } _ { j }$ into a cluster, $C$ , eliminate $\mathbf { X } _ { i }$ and $\mathbf { X } _ { j }$ from $\Xi$ and replace them by a cluster vector, $\mathbf { C }$ , equal to the average of $\mathbf { X } _ { i }$ and $\mathbf { X } _ { j }$ . Next we compute the Euclidean distance again between all pairs of points in $\Xi$ . If the smallest distance is between pairs of patterns, we form a new cluster, $C$ , as before and replace the pair of patterns in $\Xi$ by their average. If the shortest distance is between a pattern, $\mathbf { X } _ { i }$ , and a cluster vector, $\mathbf { C } _ { j }$ (representing a cluster, $C _ { j }$ ), we form a new cluster, $C$ , consisting of the union of $C _ { j }$ and $\{ { \bf X } _ { i } \}$ . In this case, we replace $\mathbf { C } _ { j }$ and $\mathbf { X } _ { i }$ in $\Xi$ by their (appropriately weighted) average and continue. If the shortest distance is between two cluster vectors, $\mathbf { C } _ { i }$ and $\mathbf { C } _ { j }$ , we form a new cluster, $C$ , consisting of the union of $C _ { i }$ and $C _ { j }$ . In this case, we replace $\mathbf { C } _ { i }$ and $\mathbf { C } _ { j }$ by their (appropriately weighted) average and continue. Since we reduce the number of points in $\Xi$ by one each time, we ultimately terminate with a tree of clusters rooted in the cluster containing all of the points in the original training set.

An example of how this method aggregates a set of two dimensional patterns is shown in Fig. 9.5. The numbers associated with each cluster indicate the order in which they were formed. These clusters can be organized hierarchically in a binary tree with cluster 9 as root, clusters 7 and 8 as the two descendants of the root, and so on. A ternary tree could be formed instead if one searches for the three points in $\Xi$ whose triangle defined by those patterns has minimal area.

![](images/882698ff0a4f74d6726a7e09349245b9c21e4f2cd8a584de3fc782860f12dacb.jpg)  
Figure 9.5: Agglommerative Clustering

# 9.3.2 A Method Based on Probabilities

# A probabilistic quality measure for partitions

We can develop a measure of the goodness of a partitioning based on how accurately we can guess a pattern given only what partition it is in. Suppose we are given a partitioning of $\Xi$ into $R$ classes, $C _ { 1 } , \ldots , C _ { R }$ . As before, we can compute the sample statistics $p ( x _ { i } | C _ { k } )$ which give probability values for each component given the class assigned to it by the partitioning. Suppose each component $x _ { i }$ of $\mathbf { X }$ can take on the values $v _ { i j }$ , where the index $j$ steps over the domain of that component. We use the notation $p _ { i } ( v _ { i j } | C _ { k } ) =$ $p _ { i } ( v _ { i j } | C _ { k } ) = \mathrm { p r o b a b i l i t y } ( x _ { i } =$ $x _ { i } =$ $v _ { i j } | C _ { k } )$ .

Suppose we use the following probabilistic guessing rule about the values of the components of a vector $\mathbf { X }$ given only that it is in class $k$ . Guess that $x _ { i } = v _ { i j }$ with probability $p _ { i } ( v _ { i j } | C _ { k } )$ . Then, the probability that we guess the $i$ -th component correctly is:

$$
\sum_ {j} \mathrm {p r o b a b i l i t y (g u e s s i s} v _ {i j}) p _ {i} (v _ {i j} | C _ {k}) = \sum_ {j} \left[ p _ {i} (v _ {i j} | C _ {k}) \right] ^ {2}
$$

The average number of (the $n$ ) components whose values are guessed correctly by this method is then given by the sum of these probabilities over all of the components of $\mathbf { X }$ :

$$
\sum_ {i} \sum_ {j} \left[ p _ {i} \left(v _ {i j} \mid C _ {k}\right) \right] ^ {2}
$$

Given our partitioning into $R$ classes, the goodness measure, $G$ , of this partitioning is the average of the above expression over all classes:

$$
G = \sum_ {k} p (C _ {k}) \sum_ {i} \sum_ {j} [ p _ {i} (v _ {i j} | C _ {k}) ] ^ {2}
$$

where $p ( C _ { k } )$ is the probability that a pattern is in class $C _ { k }$ . In order to penalize this measure for having a large number of classes, we divide it by $R$ to get an overall “quality” measure of a partitioning:

$$
Z = (1 / R) \sum_ {k} p (C _ {k}) \sum_ {i} \sum_ {j} [ p _ {i} (v _ {i j} | C _ {k}) ] ^ {2}
$$

We give an example of the use of this measure for a trivially simple clustering of the four three-dimensional patterns shown in Fig. 9.6. There are several different partitionings. Let’s evaluate $Z$ values for the following ones: $P _ { 1 } ~ = ~ \{ a , b , c , d \}$ , $P _ { 2 } ~ = ~ \{ \{ a , b \} , \{ c , d \} \}$ , $P _ { 3 } ~ = ~ \{ \{ a , c \} , \{ b , d \} \}$ , and $P _ { 4 } = \{ \{ a \} , \{ b \} , \{ c \} , \{ d \} \}$ . The first, $P _ { 1 }$ , puts all of the patterns into a single cluster. The sample probabilities $p _ { i } ( v _ { i 1 } = 1 )$ and $p _ { i } ( v _ { i 0 } = 0 )$ are all equal to $1 / 2$ for each of the three components. Summing over the values of the components (0 and 1) gives $( 1 / 2 ) ^ { 2 } + ( 1 / 2 ) ^ { 2 } = 1 / 2$ . Summing over the three components gives $3 / 2$ . Averaging over all of the clusters (there is just one) also gives $3 / 2$ . Finally, dividing by the number of clusters produces the final $Z$ value of this partition, $Z ( P _ { 1 } ) = 3 / 2$ .

The second partition, $P _ { 2 }$ , gives the following sample probabilities:

$$
p _ {1} (v _ {1 1} = 1 | C _ {1}) = 1
$$

$$
p _ {2} (v _ {2 1} = 1 | C _ {1}) = 1 / 2
$$

$$
p _ {3} (v _ {3 1} = 1 | C _ {1}) = 1
$$

Summing over the values of the components (0 and 1) gives $( 1 ) ^ { 2 } + ( 0 ) ^ { 2 } = 1$ for component 1, $( 1 / 2 ) ^ { 2 } + ( 1 / 2 ) ^ { 2 } = 1 / 2$ for component 2, and $( 1 ) ^ { 2 } + ( 0 ) ^ { 2 } = 1$ for component 3. Summing over the three components gives 2 1/2 for class 1. A similar calculation also gives 2 1/2 for class 2. Averaging over the two clusters also gives 2 1/2. Finally, dividing by the number of clusters produces the final $Z$ value of this partition, $Z ( P _ { 2 } ) = 1 ~ 1 / 4$ , not quite as high as $Z ( P _ { 1 } )$ .

Similar calculations yield $Z ( P _ { 3 } ) = 1$ and $Z ( P _ { 4 } ) = 3 / 4$ , so this method of evaluating partitions would favor placing all patterns in a single cluster.

![](images/a0f24661fb0eb706a7037a7c20353afecc93cf34e80bb48e7e0b9134a8bf4e6e.jpg)  
Figure 9.6: Patterns in 3-Dimensional Space

# An iterative method for hierarchical clustering

Evaluating all partitionings of $m$ patterns and then selecting the best would be computationally intractable. The following iterative method is based on a hierarchical clustering procedure called COBWEB [Fisher, 1987]. The procedure grows a tree each node of which is labeled by a set of patterns. At the end of the process, the root node contains all of the patterns in $\Xi$ . The successors of the root node will contain mutually exclusive and exhaustive subsets of $\Xi$ . In general, the successors of a node, $\eta$ , are labeled by mutually exclusive and exhaustive subsets of the pattern set labelling node $\eta$ . The tips of the tree will contain singleton sets. The method uses $Z$ values to place patterns at the various nodes; sample statistics are used to update the $Z$ values whenever a pattern is placed at a node. The algorithm is as follows:

a. We start with a tree whose root node contains all of the patterns in $\Xi$ and a single empty successor node. We arrange that at all times during the process every non-empty node in the tree has (besides any other successors) exactly one empty successor.   
b. Select a pattern $\mathbf { X } _ { i }$ in $\Xi$ (if there are no more patterns to select, terminate).   
c. Set $\mu$ to the root node.   
d. For each of the successors of $\mu$ (including the empty successor!), calculate the best host for $\mathbf { X } _ { i }$ . A best host is determined by tentatively placing $\mathbf { X } _ { i }$ in one of the successors and calculating the resulting $Z$ value for each

one of these ways of accomodating $\mathbf { X } _ { i }$ . The best host corresponds to the assignment with the highest $Z$ value.

e. If the best host is an empty node, $\eta$ , we place $\mathbf { X } _ { i }$ in $\eta$ , generate an empty successor node of $\eta$ , generate an empty sibling node of $\eta$ , and go to 2.   
f. If the best host is a non-empty, singleton (tip) node, $\eta$ , we place $\mathbf { X } _ { i }$ i n $\eta$ , create one successor node of $\eta$ containing the singleton pattern that was in $\eta$ , create another successor node of $\eta$ containing $\mathbf { X } _ { i }$ , create an empty successor node of $\eta$ , create empty successor nodes of the new non-empty successors of $\eta$ , and go to 2.   
g. If the best host is a non-empty, non-singleton node, $\eta$ , we place $\mathbf { X } _ { i }$ in $\eta$ set $\mu$ to $\eta$ , and go to 4.

This process is rather sensitive to the order in which patterns are presented. To make the final classification tree less order dependent, the COBWEB procedure incorporates node merging and splitting.

Node merging:

It may happen that two nodes having the same parent could be merged with an overall increase in the quality of the resulting classification performed by the successors of that parent. Rather than try all pairs to merge, a good heuristic is to attempt to merge the two best hosts. When such a merging improves the $Z$ value, a new node containing the union of the patterns in the merged nodes replaces the merged nodes, and the two nodes that were merged are installed as successors of the new node.

Node splitting:

A heuristic for node splitting is to consider replacing the best host among a group of siblings by that host’s successors. This operation is performed only if it increases the $Z$ value of the classification performed by a group of siblings.

# Example results from COBWEB

We mention two experiments with COBWEB. In the first, the program attempted to find two categories (we will call them Class 1 and Class 2) of United States Senators based on their votes (yes or no) on six issues. After the clusters were established, the majority vote in each class was computed. These are shown in the table below.

<table><tr><td>Issue</td><td>Class 1</td><td>Class 2</td></tr><tr><td>Toxic Waste</td><td>yes</td><td>no</td></tr><tr><td>Budget Cuts</td><td>yes</td><td>no</td></tr><tr><td>SDI Reduction</td><td>no</td><td>yes</td></tr><tr><td>Contra Aid</td><td>yes</td><td>no</td></tr><tr><td>Line-Item Veto</td><td>yes</td><td>no</td></tr><tr><td>MX Production</td><td>yes</td><td>no</td></tr></table>

In the second experiment, the program attempted to classify soybean diseases based on various characteristics. COBWEB grouped the diseases in the taxonomy shown in Fig. 9.7.

![](images/6501245399d5373dad1da3d0273a35b032e1485918b07e95b12279fbf83f8cfd.jpg)  
Figure 9.7: Taxonomy Induced for Soybean Diseases

To be added.

# 9.4 Bibliographical and Historical Remarks

# Chapter 10

# Temporal-Difference Learning

# 10.1 Temporal Patterns and Prediction Problems

In this chapter, we consider problems in which we wish to learn to predict the future value of some quantity, say $z$ , from an $n$ -dimensional input pattern, $\mathbf { X }$ . In many of these problems, the patterns occur in temporal sequence, $\mathbf { X } _ { 1 }$ , $\mathbf { X } _ { 2 }$ , . . ., $\mathbf { X } _ { i }$ , ${ \bf X } _ { i + 1 }$ , . . ., $\mathbf { X } _ { m }$ , and are generated by a dynamical process. The components of $\mathbf { X } _ { i }$ are features whose values are available at time, $t = i$ . We distinguish two kinds of prediction problems. In one, we desire to predict the value of $z$ at time $t = i + 1$ based on input $\mathbf { X } _ { i }$ for every $i$ . For example, we might wish to predict some aspects of tomorrow’s weather based on a set of measurements made today. In the other kind of prediction problem, we desire to make a sequence of predictions about the value of $z$ at some fixed time, say $t = m + 1$ , based on each of the $\mathbf { X } _ { i }$ , $i = 1 , \ldots , m$ . For example, we might wish to make a series of predictions about some aspect of the weather on next New Year’s Day, based on measurements taken every day before New Year’s. Sutton [Sutton, 1988] has called this latter problem, multi-step prediction, and that is the problem we consider here. In multi-step prediction, we might expect that the prediction accuracy should get better and better as $i$ increases toward $m$ .

# 10.2 Supervised and Temporal-Difference Methods

A training method that naturally suggests itself is to use the actual value of $z$ at time $m + 1$ (once it is known) in a supervised learning procedure using a

sequence of training patterns, $\{ { \mathbf X } _ { 1 }$ , $\mathbf { X } _ { 2 }$ , . . ., $\mathbf { X } _ { i }$ , ${ \bf X } _ { i + 1 }$ , . . ., $\mathbf { X } _ { m } \}$ . That is, we seek to learn a function, $f$ , such that $f ( \mathbf { X } _ { i } )$ is as close as possible to $z$ for each $i$ . Typically, we would need a training set, $\Xi$ , consisting of several such sequences. We will show that a method that is better than supervised learning for some important problems is to base learning on the difference between $f ( \mathbf { X } _ { i + 1 } )$ and $f ( \mathbf { X } _ { i } )$ rather than on the difference between $z$ and $f ( \mathbf { X } _ { i } )$ . Such methods involve what is called temporal-difference ( $T D$ ) learning.

We assume that our prediction, $f ( \mathbf { X } )$ , depends on a vector of modifiable weights, W. To make that dependence explicit, we write $f ( \mathbf { { X } } , \mathbf { { W } } )$ . For supervised learning, we consider procedures of the following type: For each $\mathbf { X } _ { i }$ , the prediction $f ( \mathbf { { X } } _ { i } , \mathbf { { W } } )$ is computed and compared to $z$ , and the learning rule (whatever it is) computes the change, $( \Delta \mathbf { W } _ { i } )$ , to be made to $\mathbf { W }$ . Then, taking into account the weight changes for each pattern in a sequence all at once after having made all of the predictions with the old weight vector, we change $\mathbf { W }$ as follows:

$$
\mathbf {W} \leftarrow \mathbf {W} + \sum_ {i = 1} ^ {m} (\Delta \mathbf {W}) _ {i}
$$

Whenever we are attempting to minimize the squared error between $z$ and $f ( \mathbf { { X } } _ { i } , \mathbf { { W } } )$ by gradient descent, the weight-changing rule for each pattern is:

$$
(\Delta \mathbf {W}) _ {i} = c (z - f _ {i}) \frac {\partial f _ {i}}{\partial \mathbf {W}}
$$

where $c$ is a learning rate parameter, $f _ { i }$ is our prediction of $z$ , $f ( \mathbf { X } _ { i } , \mathbf { W } )$ , at time $t \ = \ i$ , and $\frac { \partial f _ { i } } { \partial \mathbf { W } }$ is, by definition, the vector of partial derivatives ∂fi $\big ( \frac { \partial f _ { i } } { \partial w _ { 1 } } , \ldots , \frac { \partial f _ { i } } { \partial w _ { i } } , \ldots , \frac { \partial f _ { i } } { \partial w _ { n } } \big )$ ∂fi , ∂fi∂wn ) in which the wi are the individual components of W. wn $w _ { i }$ $\mathbf { W }$ (The expression $\frac { \partial f _ { i } } { \partial \mathbf { W } }$ is sometimes written $\nabla _ { \mathbf { W } } f _ { i }$ .) The reader will recall that we used an equivalent expression for $( \Delta \mathbf { W } ) _ { i }$ in deriving the backpropagation formulas used in training multi-layer neural networks.

The Widrow-Hoff rule results when $f ( \mathbf { X } , \mathbf { W } ) = \mathbf { X } \bullet \mathbf { W }$ . Then:

$$
(\Delta \mathbf {W}) _ {i} = c (z - f _ {i}) \mathbf {X} _ {i}
$$

An interesting form for $( \Delta \mathbf { W } ) _ { i }$ can be developed if we note that

$$
\left(z - f _ {i}\right) = \sum_ {k = i} ^ {m} \left(f _ {k + 1} - f _ {k}\right)
$$

where we define $f _ { m + 1 } = z$ . Substituting in our formula for $( \Delta \mathbf { W } ) _ { i }$ yields:

$$
(\Delta \mathbf {W}) _ {i} = c (z - f _ {i}) \frac {\partial f _ {i}}{\partial \mathbf {W}}
$$

$$
= c \frac {\partial f _ {i}}{\partial \mathbf {W}} \sum_ {k = i} ^ {m} \left(f _ {k + 1} - f _ {k}\right)
$$

In this form, instead of using the difference between a prediction and the value of $z$ , we use the differences between successive predictions—thus the phrase temporal-difference ( $T D$ ) learning.

In the case when $f ( \mathbf { X } , \mathbf { W } ) = \mathbf { X } \bullet \mathbf { W }$ , the temporal difference form of the Widrow-Hoff rule is:

$$
(\Delta \mathbf {W}) _ {i} = c \mathbf {X} _ {i} \sum_ {k = i} ^ {m} (f _ {k + 1} - f _ {k})
$$

One reason for writing $( \Delta \mathbf { W } ) _ { i }$ in temporal-difference form is to permit an interesting generalization as follows:

$$
(\Delta \mathbf {W}) _ {i} = c \frac {\partial f _ {i}}{\partial \mathbf {W}} \sum_ {k = i} ^ {m} \lambda^ {(k - i)} (f _ {k + 1} - f _ {k})
$$

where $0 < \lambda \leq 1$ . Here, the $\lambda$ term gives exponentially decreasing weight to differences later in time than $t = i$ . When $\lambda = 1$ , we have the same rule with which we began—weighting all differences equally, but as $\lambda  0$ , we weight only the $\left( f _ { i + 1 } - f _ { i } \right)$ difference. With the $\lambda$ term, the method is called TD(λ).

It is interesting to compare the two extreme cases:

For TD(0):

$$
(\Delta \mathbf {W}) _ {i} = c (f _ {i + 1} - f _ {i}) \frac {\partial f _ {i}}{\partial \mathbf {W}}
$$

For TD(1):

$$
(\Delta \mathbf {W}) _ {i} = c (z - f _ {i}) \frac {\partial f _ {i}}{\partial \mathbf {W}}
$$

Both extremes can be handled by the same learning mechanism; only the error term is different. In TD(0), the error is the difference between successive predictions, and in TD(1), the error is the difference between the finally revealed value of $z$ and the prediction. Intermediate values of $\lambda$ take into account differently weighted differences between future pairs of successive predictions.

Only TD(1) can be considered a pure supervised learning procedure, sensitive to the final value of $z$ provided by the teacher. For $\lambda < 1$ , we have various degrees of unsupervised learning, in which the prediction function strives to make each prediction more like successive ones (whatever they might be). We shall soon see that these unsupervised procedures result in better learning than do the supervised ones for an important class of problems.

# 10.3 Incremental Computation of the (∆W)i

We can rewrite our formula for (∆W)i, namely

$$
\left(\Delta \mathbf {W}\right) _ {i} = c \frac {\partial f _ {i}}{\partial \mathbf {W}} \sum_ {k = i} ^ {m} \lambda^ {(k - i)} \left(f _ {k + 1} - f _ {k}\right)
$$

to allow a type of incremental computation. First we write the expression for the weight change rule that takes into account all of the $( \Delta \mathbf { W } ) _ { i }$ :

$$
\mathbf {W} \leftarrow \mathbf {W} + \sum_ {i = 1} ^ {m} c \frac {\partial f _ {i}}{\partial \mathbf {W}} \sum_ {k = i} ^ {m} \lambda^ {(k - i)} \left(f _ {k + 1} - f _ {k}\right)
$$

Interchanging the order of the summations yields:

$$
\begin{array}{l} \mathbf {W} \leftarrow \mathbf {W} + \sum_ {k = 1} ^ {m} c \sum_ {i = 1} ^ {k} \lambda^ {(k - i)} \left(f _ {k + 1} - f _ {k}\right) \frac {\partial f _ {i}}{\partial \mathbf {W}} \\ = \mathbf {W} + \sum_ {k = 1} ^ {m} c (f _ {k + 1} - f _ {k}) \sum_ {i = 1} ^ {k} \lambda^ {(k - i)} \frac {\partial f _ {i}}{\partial \mathbf {W}} \\ \end{array}
$$

Interchanging the indices $k$ and $i$ finally yields:

$$
\mathbf {W} \leftarrow \mathbf {W} + \sum_ {i = 1} ^ {m} c \left(f _ {i + 1} - f _ {i}\right) \sum_ {k = 1} ^ {i} \lambda^ {(i - k)} \frac {\partial f _ {k}}{\partial \mathbf {W}}
$$

If, as earlier, we want to use an expression of the form $\begin{array} { r } { \mathbf { W } \longleftarrow \mathbf { W } + \sum _ { i = 1 } ^ { m } ( \Delta \mathbf { W } ) _ { i } } \end{array}$ we see that we can write:

$$
(\Delta \mathbf {W}) _ {i} = c (f _ {i + 1} - f _ {i}) \sum_ {k = 1} ^ {i} \lambda^ {(i - k)} \frac {\partial f _ {k}}{\partial \mathbf {W}}
$$

Now, if we let ei = Pik=1 $\begin{array} { r } { e _ { i } = \sum _ { k = 1 } ^ { i } \lambda ^ { ( i - k ) } \frac { \partial f _ { k } } { \partial \mathbf { W } } } \end{array}$ λ(i−k) ∂fk , we can develop a computationally efficient recurrence equation for $e _ { i + 1 }$ as follows:

$$
\begin{array}{l} e _ {i + 1} = \sum_ {k = 1} ^ {i + 1} \lambda^ {(i + 1 - k)} \frac {\partial f _ {k}}{\partial \mathbf {W}} \\ = \frac {\partial f _ {i + 1}}{\partial \mathbf {W}} + \sum_ {k = 1} ^ {i} \lambda^ {(i + 1 - k)} \frac {\partial f _ {k}}{\partial \mathbf {W}} \\ \end{array}
$$

$$
= \frac {\partial f _ {i + 1}}{\partial \mathbf {W}} + \lambda e _ {i}
$$

Rewriting $( \Delta \mathbf { W } ) _ { i }$ in these terms, we obtain:

$$
(\Delta \mathbf {W}) _ {i} = c \left(f _ {i + 1} - f _ {i}\right) e _ {i}
$$

where:

$$
e _ {1} = \frac {\partial f _ {1}}{\partial \mathbf {W}}
$$

$$
e _ {2} = \frac {\partial f _ {2}}{\partial \mathbf {W}} + \lambda e _ {1}
$$

etc.

Quoting Sutton [Sutton, 1988, page 15] (about a different equation, but the quote applies equally well to this one):

“. . . this equation can be computed incrementally, because each $( \Delta \mathbf { W } ) _ { i }$ depends only on a pair of successive predictions and on the [weighted] sum of all past values for $\frac { \partial f _ { i } } { \partial \mathbf { W } }$ . This saves substantially on memory, because it is no longer necessary to individually remember all past values of $\begin{array} { r } { \frac { \partial f _ { i } } { \partial \mathbf { W } } } \end{array}$ .”

# 10.4 An Experiment with TD Methods

TD prediction methods [especially TD(0)] are well suited to situations in which the patterns are generated by a dynamic process. In that case, sequences of temporally presented patterns contain important information that is ignored by a conventional supervised method such as the Widrow-Hoff rule. Sutton [Sutton, 1988, page 19] gives an interesting example involving a random walk, which we repeat here. In Fig. 10.1, sequences of vectors, $\mathbf { X }$ , are generated as follows: We start with vector $\mathbf { X } _ { D }$ ; the next vector in the sequence is equally likely to be one of the adjacent vectors in the diagram. If the next vector is $\mathbf { X } _ { C }$ (or $\mathbf { X } _ { E }$ ), the next one after that is equally likely to be one of the vectors adjacent to $\mathbf { X } _ { C }$ (or $\mathbf { X } _ { E }$ ). When $\mathbf { X } _ { B }$ is in the sequence, it is equally likely that the sequence terminates with $z = 0$ or that the next vector is $\mathbf { X } _ { C }$ . Similarly, when $\mathbf { X } _ { F }$ is in the sequence, it is equally likely that the sequence terminates with $z = 1$ or that the next vector is $\mathbf { X } _ { E }$ . Thus the sequences are random, but they always start with $\mathbf { X } _ { D }$ . Some sample sequences are shown in the figure.

![](images/df9e540b19fd125ec6e532196af3223eb58a631f759eae82882b4d7c8462dc41.jpg)  
Figure 10.1: A Markov Process

Typical Sequences:

$$
\mathbf {x} _ {D} \mathbf {x} _ {C} \mathbf {x} _ {D} \mathbf {x} _ {E} \mathbf {x} _ {F} 1
$$

$$
\mathbf {x} _ {D} \mathbf {x} _ {C} \mathbf {x} _ {B} \mathbf {x} _ {C} \mathbf {x} _ {D} \mathbf {x} _ {E} \mathbf {x} _ {D} \mathbf {x} _ {E} \mathbf {x} _ {F} 1
$$

$$
\mathbf {x} _ {\mathrm {D}} \mathbf {x} _ {\mathrm {E}} \mathbf {x} _ {\mathrm {D}} \mathbf {x} _ {\mathrm {C}} \mathbf {x} _ {\mathrm {B}} 0
$$

This random walk is an example of a Markov process; transitions from state $_ i$ to state $j$ occur with probabilities that depend only on $_ i$ and $j$ .

Given a set of sequences generated by this process as a training set, we want to be able to predict the value of $z$ for each $\mathbf { X }$ in a test sequence. We assume that the learning system does not know the transition probabilities.

For his experiments with this process, Sutton used a linear predictor, that is $f ( \mathbf { X } , \mathbf { W } ) = \mathbf { X } \bullet \mathbf { W }$ . The learning problem is to find a weight vector, $\mathbf { W }$ , that minimizes the mean-squared error between $z$ and the predicted value of $\boldsymbol { \mathrm { Z } }$ . Given the five different values that $\mathbf { X }$ can take on, we have the following predictions: $f ( \mathbf { X } _ { B } ) = w _ { 1 }$ , $f ( \mathbf { X } _ { C } ) = w _ { 2 }$ , $f ( { \bf { X } } _ { D } ) = w _ { 3 }$ , $f ( \mathbf { X } _ { E } ) = w _ { 4 }$ , $f ( { \bf { X } } _ { F } ) = w _ { 5 }$ , where $w _ { i }$ is the $i$ -th component of the weight vector. (Note that the values of the predictions are not limited to 1 or 0—even though $z$ can only have one of those values—because we are minimizing mean-squared error.) After training, these predictions will be compared with the optimal ones—given the transition probabilities.

The experimental setup was as follows: ten random sequences were generated using the transition probabilities. Each of these sequences was presented in turn to a TD(λ) method for various values of $\lambda$ . Weight vector increments, $( \Delta \mathbf { W } ) _ { i }$ , were computed after each pattern presentation but no weight changes were made until all ten sequences were presented. The weight vector increments were summed after all ten sequences were presented, and this sum was used to change the weight vector to be used for the next pass through the ten sequences. This process was repeated over and over (using the same training sequences) until (quoting Sutton) “the procedure no longer produced any significant changes in the weight vector. For small $c$ , the weight vector always converged in this way,

and always to the same final value [for 100 different training sets of ten random sequences], independent of its initial value.” (Even though, for fixed, small $c$ , the weight vector always converged to the same vector, it might converge to a somewhat different vector for different values of $c$ .)

After convergence, the predictions made by the final weight vector are compared with the optimal predictions made using the transition probabilities. These optimal predictions are simply $p ( z = 1 | \mathbf { X } )$ . We can compute these probabilities to be 1/6, 1/3, 1/2, 2/3, and 5/6 for $\mathbf { X } _ { B }$ , $\mathbf { X } _ { C }$ , $\mathbf { X } _ { D }$ , $\mathbf { X } _ { E }$ , $\mathbf { X } _ { F }$ , respectively. The root-mean-squared differences between the best learned predictions (over all $c$ ) and these optimal ones are plotted in Fig. 10.2 for seven different values of $\lambda$ . (For each data point, the standard error is approximately $\sigma = 0 . 0 1$ .)

![](images/cdb08b490e0d396ce9b67d099c051c53311afdd7c91e5d3d18a3f01fa67e473b.jpg)  
Figure 10.2: Prediction Errors for TD(λ)

Notice that the Widrow-Hoff procedure does not perform as well as other versions of TD(λ) for $\lambda < 1 !$ ! Quoting [Sutton, 1988, page 21]:

“This result contradicts conventional wisdom. It is well known that, under repeated presentations, the Widrow-Hoff procedure minimizes the RMS error between its predictions and the actual outcomes in the training set ([Widrow & Stearns, 1985]). How can it be that this optimal method peformed worse than all the TD methods for $\lambda <$ 1? The answer is that the Widrow-Hoff procedure only minimizes error on the training set; it does not necessarily minimize error for future experience. [Later] we prove that in fact it is linear TD(0) that converges to what can be considered the optimal estimates for

matching future experience—those consistent with the maximumlikelihood estimate of the underlying Markov process.”

# 10.5 Theoretical Results

It is possible to analyze the performance of the linear-prediction TD(λ) methods on Markov processes. We state some theorems here without proof.

Theorem 10.1 (Sutton, page 24, 1988) For any absorbing Markov chain, and for any linearly independent set of observation vectors $\{ { \bf X } _ { i } \}$ for the nonterminal states, there exists an $\varepsilon > 0$ such that for all positive $c < \varepsilon$ and for any initial weight vector, the predictions of linear TD(0) (with weight updates after each sequence) converge in expected value to the optimal (maximum likelihood) predictions of the true process.

Even though the expected values of the predictions converge, the predictions themselves do not converge but vary around their expected values depending on their most recent experience. Sutton conjectures that if $c$ is made to approach 0 as training progresses, the variance of the predictions will approach 0 also.

Dayan [Dayan, 1992] has extended the result of Theorem 9.1 to TD(λ) for arbitrary $\lambda$ between 0 and 1. (Also see [Dayan & Sejnowski, 1994].)

# 10.6 Intra-Sequence Weight Updating

Our standard weight updating rule for TD(λ) methods is:

$$
\mathbf {W} \leftarrow \mathbf {W} + \sum_ {i = 1} ^ {m} c \left(f _ {i + 1} - f _ {i}\right) \sum_ {k = 1} ^ {i} \lambda^ {(i - k)} \frac {\partial f _ {k}}{\partial \mathbf {W}}
$$

where the weight update occurs after an entire sequence is observed. To make the method truly incremental (in analogy with weight updating rules for neural nets), it would be desirable to change the weight vector after every pattern presentation. The obvious extension is:

$$
\mathbf {W} _ {i + 1} \longleftarrow \mathbf {W} _ {i} + c (f _ {i + 1} - f _ {i}) \sum_ {k = 1} ^ {i} \lambda^ {(i - k)} \frac {\partial f _ {k}}{\partial \mathbf {W}}
$$

where $f _ { i + 1 }$ is computed before making the weight change; that is, $f _ { i + 1 } =$ $f ( \mathbf { { X } } _ { i + 1 } , \mathbf { { W } } _ { i } )$ . But that would make $f _ { i } = f ( \mathbf { X } _ { i } , \mathbf { W } _ { i - 1 } )$ , and such a rule would make the prediction difference, namely $\left( f _ { i + 1 } - f _ { i } \right)$ , sensitive both to changes in $\mathbf { X }$ and changes in $\mathbf { W }$ and could lead to instabilities. Instead, we modify the rule so that, for every pair of predictions, $f _ { i + 1 } = f ( \mathbf { X } _ { i + 1 } , \mathbf { W } _ { i } )$ and $f _ { i } = f ( \mathbf { X } _ { i } , \mathbf { W } _ { i } )$ . This version of the rule has been used in practice with excellent results.

For TD(0) and linear predictors, the rule is:

$$
\mathbf {W} _ {i + 1} = \mathbf {W} _ {i} + c \left(f _ {i + 1} - f _ {i}\right) \mathbf {X} _ {i}
$$

The rule is implemented as follows:

a. Initialize the weight vector, $\mathbf { W }$ , arbitrarily.   
b. For $i = 1 , . . . , m$ , do:

(a) $f _ { i } \longleftarrow \mathbf { X } _ { i } \bullet \mathbf { W }$

(We compute $f _ { i }$ anew each time through rather than use the value of $f _ { i + 1 }$ the previous time through.)

(b) $f _ { i + 1 } \longleftarrow \mathbf { X } _ { i + 1 } \bullet \mathbf { W }$   
(c) $d _ { i + 1 } \longleftarrow f _ { i + 1 } - f _ { i }$   
(d) $\mathbf { W } \longleftarrow \mathbf { W } + c d _ { i + 1 } \mathbf { X } _ { i }$

(If $f _ { i }$ were computed again with this changed weight vector, its value would be closer to $f _ { i + 1 }$ as desired.)

The linear TD(0) method can be regarded as a technique for training a very simple network consisting of a single dot product unit (and no threshold or sigmoid function). TD methods can also be used in combination with backpropagation to train neural networks. For TD(0) we change the network weights according to the expression:

$$
\mathbf {W} _ {i + 1} = \mathbf {W} _ {i} + c (f _ {i + 1} - f _ {i}) \frac {\partial f _ {i}}{\partial \mathbf {W}}
$$

The only change that must be made to the standard backpropagation weightchanging rule is that the difference term between the desired output and the output of the unit in the final ( $k$ -th) layer, namely $( d - f ^ { ( k ) } )$ , must be replaced by a difference term between successive outputs, $\left( f _ { i + 1 } - f _ { i } \right)$ . This change has a direct effect only on the expression for $\delta ^ { ( k ) }$ which becomes:

$$
\delta^ {(k)} = 2 \left(f ^ {\prime (k)} - f ^ {(k)}\right) f ^ {(k)} \left(1 - f ^ {(k)}\right)
$$

where $f ^ { \prime ( k ) }$ and $f ^ { ( k ) }$ are two successive outputs of the network.

The weight changing rule for the i-th weight vector in the j-th layer of weights has the same form as before, namely:

$$
\mathbf {W} _ {i} ^ {(j)} \leftarrow \mathbf {W} _ {i} ^ {(j)} + c \delta_ {i} ^ {(j)} \mathbf {X} ^ {(j - 1)}
$$

where the $\delta _ { i } ^ { ( j ) }$ are given recursively by:

$$
\delta_ {i} ^ {(j)} = f _ {i} ^ {(j)} (1 - f _ {i} ^ {(j)}) \sum_ {l = 1} ^ {m _ {j + 1}} \delta_ {l} ^ {(j + 1)} w _ {i l} ^ {(j + 1)}
$$

and $w _ { i l } ^ { ( j + 1 ) }$ is the $\it { \Delta } l$ -th component of the $i$ -th weight or in $( j + 1 )$ -th layer $f ^ { \prime ( k ) }$ $f ^ { ( k ) }$ using the same weights and then the weights are changed. In the next section we shall see an interesting example of this application of TD learning.

# 10.7 An Example Application: TD-gammon

A program called TD-gammon [Tesauro, 1992] learns to play backgammon by training a neural network via temporal-difference methods. The structure of the neural net, and its coding is as shown in Fig. 10.3. The network is trained to minimize the error between actual payoff and estimated payoff, where the actual payoff is defined to be $d _ { f } = p _ { 1 } + 2 p _ { 2 } - p _ { 3 } - 2 p _ { 4 }$ , and the $p _ { i }$ are the actual probabilities of the various outcomes as defined in the figure.

![](images/695ca1ff3af34675dab37b2aa6cef121735f7408fa648a36b12c8a515c1b00cd.jpg)  
Figure 10.3: The TD-gammon Network

TD-gammon learned by using the network to select that move that results in the best predicted payoff. That is, at any stage of the game some finite set of moves is possible and these lead to the set, $\{ \mathbf { X } \}$ , of new board positions. Each member of this set is evaluated by the network, and the one with the largest

predicted payoff is selected if it is white’s move (and the smallest if it is black’s). The move is made, and the network weights are adjusted to make the predicted payoff from the original position closer to that of the resulting position.

The weight adjustment procedure combines temporal-difference (TD(λ)) learning with backpropagation. If $d _ { t }$ is the network’s estimate of the payoff at time $t$ (before a move is made), and $d _ { t + 1 }$ is the estimate at time $t + 1$ (after a move is made), the weight adjustment rule is:

$$
\Delta \mathbf {W} _ {t} = c (d _ {t + 1} - d _ {t}) \sum_ {k = 1} ^ {t} \lambda^ {t - k} \frac {\partial d _ {k}}{\partial \mathbf {W}}
$$

where $\mathbf { W } _ { t }$ is a vector of all weights in the network at time $t$ , and $\frac { \partial d _ { k } } { \partial \mathbf { W } }$ is the gradient of $d _ { k }$ in this weight space. (For a layered, feedforward network, such as that of TD-gammon, the weight changes for the weight vectors in each layer can be expressed in the usual manner.)

To make the special cases clear, recall that for TD(0), the network would be trained so that, for all $t$ , its output, $d _ { t }$ , for input $\mathbf { X } _ { t }$ tended toward its expected output, $d _ { t + 1 }$ , for input $\mathbf { X } _ { t + 1 }$ . For TD(1), the network would be trained so that, for all $t$ , its output, $d _ { t }$ , for input $\mathbf { X } _ { t }$ tended toward the expected final payoff, $d _ { f }$ , given that input. The latter case is the same as the Widrow-Hoff rule.

After about 200,000 games the following results were obtained. TD-gammon (with 40 hidden units, $\lambda = 0 . 7$ , and $c = 0 . 1$ ) won 66.2% of 10,000 games against SUN Microsystems Gammontool and $5 5 \%$ of 10,000 games against a neural network trained using expert moves. Commenting on a later version of TDgammon, incorporating special features as inputs, Tesauro said: “It appears to be the strongest program ever seen by this author.”

# 10.8 Bibliographical and Historical Remarks

To be added.

# Chapter 11

# Delayed-Reinforcement Learning

# 11.1 The General Problem

Imagine a robot that exists in an environment in which it can sense and act. Suppose (as an extreme case) that it has no idea about the effects of its actions. That is, it doesn’t know how acting will change its sensory inputs. Along with its sensory inputs are “rewards,” which it occasionally receives. How should it choose its actions so as to maximize its rewards over the long run? To maximize rewards, it will need to be able to predict how actions change inputs, and in particular, how actions lead to rewards.

We formalize the problem in the following way: The robot exists in an environment consisting of a set, $\boldsymbol { S }$ , of states. We assume that the robot’s sensory apparatus constructs an input vector, $\mathbf { X }$ , from the environment, which informs the robot about which state the environment is in. For the moment, we will assume that the mapping from states to vectors is one-to-one, and, in fact, will use the notation $\mathbf { X }$ to refer to the state of the environment as well as to the input vector. When presented with an input vector, the robot decides which action from a set, $\mathcal { A }$ , of actions to perform. Performing the action produces an effect on the environment—moving it to a new state. The new state results in the robot perceiving a new input vector, and the cycle repeats. We assume a discrete time model; the input vector at time $t = i$ is $\mathbf { X } _ { i }$ , the action taken at that time is $a _ { i }$ , and the expected reward, $r _ { i }$ , received at $t = i$ depends on the action taken and on the state, that is $r _ { i } = r ( \mathbf { X } _ { i } , a _ { i } )$ . The learner’s goal is to find a policy, $\pi ( \mathbf { X } )$ , that maps input vectors to actions in such a way that maximizes rewards accumulated over time. This type of learning is called reinforcement learning. The learner must find the policy by trial and error; it has no initial knowledge of the effects of its actions. The situation is as shown in Fig. 11.1.

![](images/a1e20dbf58a49e2780f3901734eb6923fba046d8f5a99ebf0322525e7d1e25b8.jpg)  
Figure 11.1: Reinforcement Learning

# 11.2 An Example

A “grid world,” such as the one shown in Fig. 11.2 is often used to illustrate reinforcement learning. Imagine a robot initially in cell (2,3). The robot receives input vector $( x _ { 1 } , x _ { 2 } )$ telling it what cell it is in; it is capable of four actions, $\pi , e , s , w$ moving the robot one cell up, right, down, or left, respectively. It is rewarded one negative unit whenever it bumps into the wall or into the blocked cells. For example, if the input to the robot is (1,3), and the robot chooses action $w$ , the next input to the robot is still (1,3) and it receives a reward of $^ { - 1 }$ . If the robot lands in the cell marked $G$ (for goal), it receives a reward of $+ 1 0$ . Let’s suppose that whenever the robot lands in the goal cell and gets its reward, it is immediately transported out to some random cell, and the quest for reward continues.

A policy for our robot is a specification of what action to take for every one of its inputs, that is, for every one of the cells in the grid. For example, a component of such a policy would be “when in cell (3,1), move right.” An optimal policy is a policy that maximizes long-term reward. One way of displaying a policy for our grid-world robot is by an arrow in each cell indicating the direction the robot should move when in that cell. In Fig. 11.3, we show an optimal policy displayed in this manner. In this chapter we will describe methods for learning optimal policies based on reward values received by the learner.

![](images/daf39a69b0a7db588ffecbc367a8aa168268e56ee4ea29e493f72f9d5d396cd2.jpg)  
Figure 11.2: A Grid World

# 11.3 Temporal Discounting and Optimal Policies

In delayed reinforcement learning, one often assumes that rewards in the distant future are not as valuable as are more immediate rewards. This preference can be accomodated by a temporal discount factor, $0 \leq \gamma < 1$ . The present value of a reward, $r _ { i }$ , occuring $_ i$ time units in the future, is taken to be $\gamma ^ { i } r _ { i }$ . Suppose we have a policy $\pi ( \mathbf { X } )$ that maps input vectors into actions, and let $r _ { i } ^ { \pi ( \mathbf { X } ) }$ b e the reward that will be received on the $i$ -th time step after one begins executing policy $\boldsymbol { \mathscr { u } }$ starting in state $\mathbf { X }$ . Then the total reward accumulated over all time steps by policy $\pi$ beginning in state $\mathbf { X }$ is:

$$
V ^ {\pi} (\mathbf {X}) = \sum_ {i = 0} ^ {\infty} \gamma^ {i} r _ {i} ^ {\pi (\mathbf {X})}
$$

One reason for using a temporal discount factor is so that the above sum will be finite. An optimal policy is one that maximizes $V ^ { \pi } ( \mathbf { X } )$ for all inputs, $\mathbf { X }$ .

In general, we want to consider the case in which the rewards, $r _ { i }$ , are random variables and in which the effects of actions on environmental states are random. In Markovian environments, for example, the probability that action $a$ in state $\mathbf { X } _ { i }$ will lead to state $\mathbf { X } _ { j }$ is given by a transition probability $p [ \mathbf { X } _ { j } | \mathbf { X } _ { i } , a ]$ . Then, we will want to maximize expected future reward and would define $V ^ { \pi } ( \mathbf { X } )$ as:

$$
V ^ {\pi} (\mathbf {X}) = E \left[ \sum_ {i = 0} ^ {\infty} \gamma^ {i} r _ {i} ^ {\pi (\mathbf {X})} \right]
$$

In either case, we call $V ^ { \pi } ( \mathbf { X } )$ the value of policy $\pi$ for input $\mathbf { X }$ .

![](images/6594bd7fac4e190a1e84971759ac9fde4cd9e386684ec31fe8898300adb7c3d4.jpg)  
Figure 11.3: An Optimal Policy in the Grid World

If the action prescribed by $\pi$ taken in state $\mathbf { X }$ leads to state $\mathbf { X } ^ { \prime }$ (randomly according to the transition probabilities), then we can write $V ^ { \pi } ( \mathbf { X } )$ in terms of $V ^ { \pi } ( \mathbf { X } ^ { \prime } )$ as follows:

$$
V ^ {\pi} (\mathbf {X}) = r [ \mathbf {X}, \pi (\mathbf {X}) ] + \gamma \sum_ {\mathbf {X} ^ {\prime}} p [ \mathbf {X} ^ {\prime} | \mathbf {X}, \pi (\mathbf {X}) ] V ^ {\pi} (\mathbf {X} ^ {\prime})
$$

where (in summary):

γ = the discount factor,

$V ^ { \pi } ( \mathbf { X } ) =$ the value of state $\mathbf { X }$ under policy $\pi$

$r [ { \bf X } , \pi ( { \bf X } ) ] =$ the expected immediate reward received when we execute the action prescribed by $\pi$ in state $\mathbf { X }$ , and

$p [ \mathbf { X } ^ { \prime } | \mathbf { X } , \pi ( \mathbf { X } ) ] =$ the probability that the environment transitions to state $\mathbf { X } ^ { \prime }$ when we execute the action prescribed by $\pi$ in state $\mathbf { X }$ .

In other words, the value of state $\mathbf { X }$ under policy $\pi$ is the expected value of the immediate reward received when executing the action recommended by $\pi$ plus the average value (under $\pi$ ) of all of the states accessible from $\mathbf { X }$ .

For an optimal policy, $\pi ^ { * }$ (and no others!), we have the famous “optimality equation:”

$$
V ^ {\pi^ {*}} (\mathbf {X}) = \max _ {a} \left[ r (\mathbf {X}, a) + \gamma \sum_ {\mathbf {X} ^ {\prime}} p [ \mathbf {X} ^ {\prime} | \mathbf {X}, a ] V ^ {\pi^ {*}} (\mathbf {X} ^ {\prime}) \right]
$$

The theory of dynamic programming (DP) [Bellman, 1957, Ross, 1983] assures us that there is at least one optimal policy, $\pi ^ { * }$ , that satisfies this equation. DP

also provides methods for calculating $V ^ { \pi ^ { * } } ( \mathbf { X } )$ and at least one $\pi ^ { * }$ , assuming that we know the average rewards and the transition probabilities. If we knew the transition probabilities, the average rewards, and $V ^ { \pi ^ { * } } ( \mathbf { X } )$ for all $\mathbf { X }$ and $a$ , then it would be easy to implement an optimal policy. We would simply select that $a$ that maximizes $\begin{array} { r } { r ( \mathbf { X } , a ) + \gamma \sum _ { \mathbf { X } ^ { \prime } } p [ \mathbf { X } ^ { \prime } | \mathbf { X } , a ] V ^ { \pi ^ { * } } ( \mathbf { X } ^ { \prime } ) } \end{array}$ . That is,

$$
\pi^ {*} (\mathbf {X}) = \arg \max _ {a} \left[ r (\mathbf {X}, a) + \gamma \sum_ {\mathbf {X} ^ {\prime}} p [ \mathbf {X} ^ {\prime} | \mathbf {X}, a ] V ^ {\pi^ {*}} (\mathbf {X} ^ {\prime}) \right]
$$

But, of course, we are assuming that we do not know these average rewards nor the transition probabilities, so we have to find a method that effectively learns them.

If we had a model of actions, that is, if we knew for every state, $\mathbf { X }$ , and action $a$ , which state, $\mathbf { X } ^ { \prime }$ resulted, then we could use a method called value iteration to find an optimal policy. Value iteration works as follows: We begin by assigning, randomly, an estimated value $\hat { V } ( { \mathbf { X } } )$ to every state, $\mathbf { X }$ . On the $i$ -th step of the process, suppose we are at state $\mathbf { X } _ { i }$ (that is, our input on the $i$ -th step is $\mathbf { X } _ { i }$ ), and that the estimated value of state $\mathbf { X } _ { i }$ on the $i$ -th step is $\hat { V } _ { i } ( { \mathbf { X } } _ { i } )$ . We then select that action $a$ that maximizes the estimated value of the predicted subsequent state. Suppose this subsequent state having the highest estimated value is $\mathbf { X } _ { i } ^ { \prime }$ . Then we update the estimated value, $\hat { V } _ { i } ( { \mathbf { X } } _ { i } )$ , of state $\mathbf { X } _ { i }$ as follows:

$$
\hat {V} _ {i} (\mathbf {X}) = (1 - c _ {i}) \hat {V} _ {i - 1} (\mathbf {X}) + c _ {i} \left[ r _ {i} + \gamma \hat {V} _ {i - 1} (\mathbf {X} _ {i} ^ {\prime}) \right]
$$

if $\mathbf { X } = \mathbf { X } _ { i }$ ,

$$
= \hat {V} _ {i - 1} (\mathbf {X})
$$

otherwise.

We see that this adjustment moves the value of $\hat { V } _ { i } ( { \mathbf { X } } _ { i } )$ an increment (depending on $c _ { i }$ ) closer to $\left[ r _ { i } + \gamma \hat { V } _ { i } ( \mathbf { X } _ { i } ^ { \prime } ) \right]$ . Assuming that $\hat { V } _ { i } ( \mathbf { X } _ { i } ^ { \prime } )$ is a good estimate for $V _ { i } ( \mathbf { X } _ { i } ^ { \prime } )$ , then this adjustment helps to make the two estimates more consistent. Providing that $0 < c _ { i } < 1$ and that we visit each state infinitely often, this process of value iteration will converge to the optimal values.

Discuss synchronous dynamic programming, asynchronous dynamic programming, and policy iteration.

# 11.4 $Q$ -Learning

Watkins [Watkins, 1989] has proposed a technique that he calls incremental dynamic programming. Let $a ; \pi$ stand for the policy that chooses action $a$ once, and thereafter chooses actions according to policy $\pi$ . We define:

$$
Q ^ {\pi} (\mathbf {X}, a) = V ^ {a; \pi} (\mathbf {X})
$$

Then the optimal value from state $\mathbf { X }$ is given by:

$$
V ^ {\pi^ {*}} (\mathbf {X}) = \max  _ {a} Q ^ {\pi^ {*}} (\mathbf {X}, a)
$$

This equation holds only for an optimal policy, $\pi ^ { * }$ . The optimal policy is given by:

$$
\pi^ {*} (\mathbf {X}) = \arg \max  _ {a} Q ^ {\pi^ {*}} (\mathbf {X}, a)
$$

Note that if an action $a$ makes $Q ^ { \pi } ( \mathbf { X } , a )$ larger than $V ^ { \pi } ( \mathbf { X } )$ , then we can improve $\pi$ by changing it so that $\pi ( \mathbf { X } ) = a$ . Making such a change is the basis for a powerful learning rule that we shall describe shortly.

Suppose action $a$ in state $\mathbf { X }$ leads to state $\mathbf { X } ^ { \prime }$ . Then using the definitions of $Q$ and $V$ , it is easy to show that:

$$
Q ^ {\pi} (\mathbf {X}, a) = r (\mathbf {X}, a) + \gamma E \left[ V ^ {\pi} \left(\mathbf {X} ^ {\prime}\right) \right]
$$

where $r ( \mathbf { X } , a )$ is the average value of the immediate reward received when we execute action $a$ in state $\mathbf { X }$ . For an optimal policy (and no others), we have another version of the optimality equation in terms of $Q$ values:

$$
Q ^ {\pi^ {*}} (\mathbf {X}, a) = \max  _ {a} \left[ r (\mathbf {X}, a) + \gamma E \left[ Q ^ {\pi^ {*}} \left(\mathbf {X} ^ {\prime}, a\right) \right] \right]
$$

for all actions, $a$ , and states, $\mathbf { X }$ . Now, if we had the optimal $Q$ values (for all $a$ and $\mathbf { X }$ ), then we could implement an optimal policy simply by selecting that action that maximized $r ( \mathbf { X } , a ) + \gamma E \bigl [ Q ^ { \pi ^ { * } } ( \mathbf { X } ^ { \prime } , a ) \bigr ]$ .

That is,

$$
\pi^ {*} (\mathbf {X}) = \arg \max  _ {a} \left[ r (\mathbf {X}, a) + \gamma E \left[ Q ^ {\pi^ {*}} \left(\mathbf {X} ^ {\prime}, a\right) \right] \right]
$$

Watkins’ proposal amounts to a TD(0) method of learning the $Q$ values. We quote (with minor notational changes) from [Watkins & Dayan, 1992, page 281]:

“In $Q$ -Learning, the agent’s experience consists of a sequence of distinct stages or episodes. In the $i$ -th episode, the agent:

• observes its current state $\mathbf { X } _ { i }$   
• selects [using the method described below] and performs an action $a _ { i }$ ,   
• observes the subsequent state $\mathbf { X } _ { i } ^ { \prime }$   
• receives an immediate reward $r _ { i }$ , and

• adjusts its $Q _ { i - 1 }$ values using a learning factor $c _ { i }$ , according to:

$$
Q _ {i} (\mathbf {X}, a) = (1 - c _ {i}) Q _ {i - 1} (\mathbf {X}, a) + c _ {i} [ r _ {i} + \gamma V _ {i - 1} (\mathbf {X} _ {i} ^ {\prime}) ]
$$

if $\mathbf { X } = \mathbf { X } _ { i }$ and $a = a _ { i }$ ,

$$
\mathbf {\Sigma} = Q _ {i - 1} (\mathbf {X}, a)
$$

otherwise,

where

$$
V _ {i - 1} (\mathbf {X} ^ {\prime}) = \max  _ {b} \left[ Q _ {i - 1} (\mathbf {X} ^ {\prime}, b) \right]
$$

is the best the agent thinks it can do from state $\mathbf { X } ^ { \prime }$ . . . . The initial $Q$ values, $Q _ { 0 } ( { \mathbf { X } } , a )$ , for all states and actions are assumed given.”

Using the current $Q$ values, $Q _ { i } ( \mathbf { X } , a )$ , the agent always selects that action that maximizes $Q _ { i } ( \mathbf { X } , a )$ . Note that only the $Q$ value corresponding to the state just exited and the action just taken is adjusted. And that $Q$ value is adjusted so that it is closer (by an amount determined by $c _ { i }$ ) to the sum of the immediate reward plus the discounted maximum (over all actions) of the $Q$ values of the state just entered. If we imagine the $Q$ values to be predictions of ultimate (infinite horizon) total reward, then the learning procedure described above is exactly a TD(0) method of learning how to predict these $Q$ values. $Q$ learning strengthens the usual TD methods, however, because TD (applied to reinforcement problems using value iteration) requires a one-step lookahead, using a model of the effects of actions, whereas $Q$ learning does not.

A convenient notation (proposed by [Schwartz, 1993]) for representing the change in $Q$ value is:

$$
Q (\mathbf {X}, a) \xleftarrow {\beta} r + \gamma V (\mathbf {X} ^ {\prime})
$$

where $Q ( \mathbf { X } , a )$ is the new $Q$ value for input $\mathbf { X }$ and action $a$ , $r$ is the immediate reward when action $a$ is taken in response to input $\mathbf { X }$ , $V ( \mathbf { X } ^ { \prime } )$ is the maximum (over all actions) of the $Q$ value of the state next reached when action $a$ is taken from state $\mathbf { X }$ , and $\beta$ is the fraction of the way toward which the new $Q$ value, $Q ( \mathbf { X } , a )$ , is adjusted to equal $r + \gamma V ( \mathbf { X } ^ { \prime } )$ .

Watkins and Dayan [Watkins & Dayan, 1992] prove that, under certain conditions, the $Q$ values computed by this learning procedure converge to optimal ones (that is, to ones on which an optimal policy can be based).

We define $n ^ { i } ( { \bf X } , a )$ as the index (episode number) of the $i$ -th time that action $a$ is tried in state $\mathbf { X }$ . Then, we have:

Theorem 11.1 (Watkins and Dayan) For Markov problems with states $\{ { \bf X } \}$ and actions $\{ a \}$ , and given bounded rewards $| r _ { n } | \leq R$ , learning rates $0 \leq c _ { n } < 1$ , and

$$
\sum_ {i = 0} ^ {\infty} c _ {n ^ {i} (\mathbf {X}, a)} = \infty , \quad \sum_ {i = 0} ^ {\infty} \left[ c _ {n ^ {i} (\mathbf {X}, a)} \right] ^ {2} <   \infty
$$

for all $\mathbf { X }$ and $a$ , then

$Q _ { n } ( \mathbf { X } , a ) \to Q _ { n } ^ { * } ( \mathbf { X } , a )$ as $n  \infty$ , for all $\mathbf { X }$ and a, with probability 1, where $Q _ { n } ^ { * } ( \mathbf { X } , a )$ corresponds to the $Q$ values of an optimal policy.

Again, we quote from [Watkins & Dayan, 1992, page 281]:

“The most important condition implicit in the convergence theorem . . . is that the sequence of episodes that forms the basis of learning must include an infinite number of episodes for each starting state and action. This may be considered a strong condition on the way states and actions are selected—however, under the stochastic conditions of the theorem, no method could be guaranteed to find an optimal policy under weaker conditions. Note, however, that the episodes need not form a continuous sequence—that is the $\mathbf { X } ^ { \prime }$ of one episode need not be the $\mathbf { X }$ of the next episode.”

The relationships among $Q$ learning, dynamic programming, and control are very well described in [Barto, Bradtke, & Singh, 1994]. $Q$ learning is best thought of as a stochastic approximation method for calculating the $Q$ values. Although the definition of the optimal $Q$ values for any state depends recursively on expected values of the $Q$ values for subsequent states (and on the expected values of rewards), no expected values are explicitly computed by the procedure. Instead, these values are approximated by iterative sampling using the actual stochastic mechanism that produces successor states.

# 11.5 Discussion, Limitations, and Extensions of Q-Learning

# 11.5.1 An Illustrative Example

The Q-learning procedure requires that we maintain a table of $Q ( \mathbf { X } , a )$ values for all state-action pairs. In the grid world that we described earlier, such a table would not be excessively large. We might start with random entries in the table; a portion of such an intial table might be as follows:

<table><tr><td>X</td><td>a</td><td>Q(X,a)</td><td>r(X,a)</td></tr><tr><td>(2,3)</td><td>w</td><td>7</td><td>0</td></tr><tr><td>(2,3)</td><td>n</td><td>4</td><td>0</td></tr><tr><td>(2,3)</td><td>e</td><td>3</td><td>0</td></tr><tr><td>(2,3)</td><td>s</td><td>6</td><td>0</td></tr><tr><td>(1,3)</td><td>w</td><td>4</td><td>-1</td></tr><tr><td>(1,3)</td><td>n</td><td>5</td><td>0</td></tr><tr><td>(1,3)</td><td>e</td><td>2</td><td>0</td></tr><tr><td>(1,3)</td><td>s</td><td>4</td><td>0</td></tr></table>

Suppose the robot is in cell (2,3). The maximum $Q$ value occurs for $a = w$ , so the robot moves west to cell (1,3)—receiving no immediate reward. The maximum $Q$ value in cell (1,3) is 5, and the learning mechanism attempts to make the value of $Q ( ( 2 , 3 ) , w )$ closer to the discounted value of 5 plus the immediate reward (which was 0 in this case). With a learning rate parameter $c = 0 . 5$ and $\gamma = 0 . 9$ , the $Q$ value of $Q ( ( 2 , 3 ) , w )$ is adjusted from 7 to 5.75. No other changes are made to the table at this episode. The reader might try this learning procedure on the grid world with a simple computer program. Notice that an optimal policy might not be discovered if some cells are not visited nor some actions not tried frequently enough.

The learning problem faced by the agent is to associate specific actions with specific input patterns. $Q$ learning gradually reinforces those actions that contribute to positive rewards by increasing the associated $Q$ values. Typically, as in this example, rewards occur somewhat after the actions that lead to them— hence the phrase delayed-reinforcement learning. One can imagine that better and better approximations to the optimal $Q$ values gradually propagate back from states producing rewards toward all of the other states that the agent frequently visits. With random $Q$ values to begin, the agent’s actions amount to a random walk through its space of states. Only when this random walk happens to stumble into rewarding states does $Q$ learning begin to produce $Q$ values that are useful, and, even then, the $Q$ values have to work their way outward from these rewarding states. The general problem of associating rewards with state-action pairs is called the temporal credit assignment problem—how should credit for a reward be apportioned to the actions leading up to it? $Q$ learning is, to date, the most successful technique for temporal credit assignment, although a related method, called the bucket brigade algorithm, has been proposed by [Holland, 1986].

Learning problems similar to that faced by the agent in our grid world have been thoroughly studied by Sutton who has proposed an architecture, called DYNA, for solving them [Sutton, 1990]. DYNA combines reinforcement learning with planning. Sutton characterizes planning as learning in a simulated world that models the world that the agent inhabits. The agent’s model of the world is obtained by $Q$ learning in its actual world, and planning is accomplished by $Q$ learning in its model of the world.

We should note that the learning problem faced by our grid-world robot

could be modified to have several places in the grid that give positive rewards. This possibility presents an interesting way to generalize the classical notion of a “goal” in AI planning systems—even in those that do no learning. Instead of representing a goal as a condition to be achieved, we represent a “goal structure” as a set of rewards to be given for achieving various conditions. Then, the generalized “goal” becomes maximizing discounted future reward instead of simply achieving some particular condition. This generalization can be made to encompass so-called goals of maintenance and goals of avoidance. The example presented above included avoiding bumping into the grid-world boundary. A goal of maintenance, of a particular state, could be expressed in terms of a reward that was earned whenever the agent was in that state and performed an action that transitioned back to that state in one step.

# 11.5.2 Using Random Actions

When the next pattern presentation in a sequence of patterns is the one caused by the agent’s own action in response to the last pattern, we have what is called an on-line learning method. In Watkins and Dayan’s terminology, in on-line learning the episodes form a continous sequence. As already mentioned, the convergence theorem for $Q$ learning does not require on-line learning; indeed, special precautions must be taken to ensure that on-line learning meets the conditions of the theorem. If on-line learning discovers some good paths to rewards, the agent may fixate on these and never discover a policy that leads to a possibly greater long-term reward. In reinforcement learning phraseology, this problem is referred to as the problem of exploitation (of already learned behavior) versus exploration (of possibly better behavior).

One way to force exploration is to perform occasional random actions (instead of that single action prescribed by the current $Q$ values). For example, in the grid-world problem, one could imagine selecting an action randomly according to a probability distribution over the actions $( n , e , s$ , and $w$ ). This distribution, in turn, could depend on the $Q$ values. For example, we might first find that action prescribed by the $Q$ values and then choose that action with probability $1 / 2$ , choose the two orthogonal actions with probability 3/16 each, and choose the opposite action with probability $1 / 8$ . This policy might be modified by “simulated annealing” which would gradually increase the probability of the action prescribed by the $Q$ values more and more as time goes on. This strategy would favor exploration at the beginning of learning and exploitation later.

Other methods, also, have been proposed for dealing with exploration, including making unvisited states intrinsically rewarding and using an “interval estimate,” which is related to the uncertainty in the estimate of a state’s value [Kaelbling, 1993].

# 11.5.3 Generalizing Over Inputs

For large problems it would be impractical to maintain a table like that used in our grid-world example. Various researchers have suggested mechanisms for computing $Q$ values, given pattern inputs and actions. One method that suggests itself is to use a neural network. For example, consider the simple linear machine shown in Fig. 11.4.

![](images/ee8f047b018ed7aacd51166a4a96f294b239f116934c703e93d6a368c701f2c2.jpg)  
Figure 11.4: A Net that Computes $Q$ Values

Such a neural net could be used by an agent that has $R$ actions to select from. The $Q$ values (as a function of the input pattern $\mathbf { X }$ and the action $a _ { i }$ ) are computed as dot products of weight vectors (one for each action) and the input vector. Weight adjustments are made according to a TD(0) procedure to bring the $Q$ value for the action last selected closer to the sum of the immediate reward (if any) and the (discounted) maximum $Q$ value for the next input pattern.

If the optimum $Q$ values for the problem (whatever they might be) are more complex than those that can be computed by a linear machine, a layered neural network might be used. Sigmoid units in the final layer would compute $Q$ values in the range 0 to 1. The TD(0) method for updating $Q$ values would then have to be combined with a multi-layer weight-changing rule, such as backpropagation.

Networks of this sort are able to aggregate different input vectors into regions for which the same action should be performed. This kind of aggregation is an example of what has been called structural credit assignment. Combining TD(λ) and backpropagation is a method for dealing with both the temporal and the structural credit assignment problems.

Interesting examples of delayed-reinforcement training of simulated and actual robots requiring structural credit assignment have been reported by [Lin, 1992, Mahadevan & Connell, 1992].

# 11.5.4 Partially Observable States

So far, we have identified the input vector, $\mathbf { X }$ , with the actual state of the environment. When the input vector results from an agent’s perceptual apparatus (as we assume it does), there is no reason to suppose that it uniquely identifies the environmental state. Because of inevitable perceptual limitations, several different environmental states might give rise to the same input vector. This phenomenon has been referred to as perceptual aliasing. With perceptual aliasing, we can no longer guarantee that $Q$ learning will result in even useful action policies, let alone optimal ones. Several researchers have attempted to deal with this problem using a variety of methods including attempting to model “hidden” states by using internal memory [Lin, 1993]. That is, if some aspect of the environment cannot be sensed currently, perhaps it was sensed once and can be remembered by the agent. When such is the case, we no longer have a Markov problem; that is, the next $\mathbf { X }$ vector, given any action, may depend on a sequence of previous ones rather than just the immediately preceding one. It might be possible to reinstate a Markov framework (over the $\mathbf { X }$ ’s) if $\mathbf { X }$ includes not only current sensory precepts but information from the agent’s memory.

# 11.5.5 Scaling Problems

Several difficulties have so far prohibited wide application of reinforcement learning to large problems. (The TD-gammon program, mentioned in the last chapter, is probably unique in terms of success on a high-dimensional problem.) We have already touched on some difficulties; these and others are summarized below with references to attempts to overcome them.

a. Exploration versus exploitation.

• use random actions   
• favor states not visited recently   
• separate the learning phase from the use phase   
• employ a teacher to guide exploration

b. Slow time to convergence

• combine learning with prior knowledge; use estimates of $Q$ values (rather than random values) initially   
• use a hierarchy of actions; learn primitive actions first and freeze the useful sequences into macros and then learn how to use the macros

• employ a teacher; use graded “lessons”—starting near the rewards and then backing away, and use examples of good behavior [Lin, 1992]   
• use more efficient computations; e.g. do several updates per episode [Moore & Atkeson, 1993]

c. Large state spaces

• use hand-coded features   
• use neural networks   
• use nearest-neighbor methods [Moore, 1990]

d. Temporal discounting problems. Using small $\gamma$ can make the learner too greedy for present rewards and indifferent to the future; but using large $\gamma$ slows down learning.

• use a learning method based on average rewards [Schwartz, 1993]

e. No “transfer” of learning . What is learned depends on the reward structure; if the rewards change, learning has to start over.

• Separate the learning into two parts: learn an “action model” which predicts how actions change states (and is constant over all problems), and then learn the “values” of states by reinforcement learning for each different set of rewards. Sometimes the reinforcement learning part can be replaced by a “planner” that uses the action model to produce plans to achieve goals.

Also see other articles in the special issue on reinforcement learning: Machine Learning, 8, May, 1992.

# 11.6 Bibliographical and Historical Remarks

To be added.

# Chapter 12

# Explanation-Based Learning

# 12.1 Deductive Learning

In the learning methods studied so far, typically the training set does not exhaust the version space. Using logical terminology, we could say that the classifier’s output does not logically follow from the training set. In this sense, these methods are inductive. In logic, a deductive system is one whose conclusions logically follow from a set of input facts, if the system is sound.1

To contrast inductive with deductive systems in a logical setting, suppose we have a set of facts (the training set) that includes the following formulas:

$$
\{R o u n d (O b j 1), R o u n d (O b j 2), R o u n d (O b j 3), R o u n d (O b j 4),
$$

$$
\left. \operatorname {B a l l} (\text {O b j 1}), \operatorname {B a l l} (\text {O b j 2}), \operatorname {B a l l} (\text {O b j 3}), \operatorname {B a l l} (\text {O b j 4}) \right\}
$$

A learning system that forms the conclusion $( \forall x ) [ B a l l ( x ) \supset R o u n d ( x ) ]$ is inductive. This conclusion may be useful (if there are no facts of the form $B a l l ( \sigma ) \land \lnot R o u n d ( \sigma ) )$ , but it does not logically follow from the facts. On the other hand, if we had the facts $G r e e n ( O b j 5 )$ and $G r e e n ( O b j 5 ) \supset R o u n d ( O b j 5 )$ , then we could logically conclude $R o u n d ( O b j 5 )$ . Making this conclusion and saving it is an instance of deductive learning—a topic we study in this chapter.

Suppose that some logical proposition, $\phi$ , logically follows from some set of facts, $\Delta$ . Under what circumstances might we say that the process of deducing $\phi$ from $\Delta$ results in our learning $\phi$ ? In a sense, we implicitly knew $\phi$ all along, since it was inherent in knowing $\Delta$ . Yet, $\phi$ might not be obvious given $\Delta$ , and

the deduction process to establish $\phi$ might have been arduous. Rather than have to deduce $\phi$ again, we might want to save it, perhaps along with its deduction, in case it is needed later. Shouldn’t that process count as learning? Dietterich [Dietterich, 1990] has called this type of learning speed-up learning.

Strictly speaking, speed-up learning does not result in a system being able to make decisions that, in principle, could not have been made before the learning took place. Speed-up learning simply makes it possible to make those decisions more efficiently. But, in practice, this type of learning might make possible certain decisions that might otherwise have been infeasible.

To take an extreme case, a chess player can be said to learn chess even though optimal play is inherent in the rules of chess. On the surface, there seems to be no real difference between the experience-based hypotheses that a chess player makes about what constitutes good play and the kind of learning we have been studying so far.

As another example, suppose we are given some theorems about geometry and are asked to prove that the sum of the angles of a right triangle is 180 degrees. Let us further suppose that the proof we constructed did not depend on the given triangle being a right triangle; in that case we can learn a more general fact. The learning technique that we are going to study next is related to this example. It is called explanation-based learning (EBL). EBL can be thought of as a process in which implicit knowledge is converted into explicit knowledge.

In EBL, we specialize parts of a domain theory to explain a particular example, then we generalize the explanation to produce another element of the domain theory that will be useful on similar examples. This process is illustrated in Fig. 12.1.

# 12.2 Domain Theories

Two types of information were present in the inductive methods we have studied: the information inherent in the training samples and the information about the domain that is implied by the “bias” (for example, the hypothesis set from which we choose functions). The learning methods are successful only if the hypothesis set is appropriate for the problem. Typically, the smaller the hypothesis set (that is, the more a priori information we have about the function being sought), the less dependent we are on information being supplied by a training set (that is, fewer samples). A priori information about a problem can be expressed in several ways. The methods we have studied so far restrict the hypotheses in a rather direct way. A less direct method involves making assertions in a logical language about the property we are trying to learn. A set of such assertions is usually called a “domain theory.”

Suppose, for example, that we wanted to classify people according to whether or not they were good credit risks. We might represent a person by a set of properties (income, marital status, type of employment, etc.), assemble such

![](images/9b10f00be87bf101e0c014db612b06f16a53324dda5694851448f8bc282f4826.jpg)  
Figure 12.1: The EBL Process

data about people who are known to be good and bad credit risks and train a classifier to make decisions. Or, we might go to a loan officer of a bank, ask him or her what sorts of things s/he looks for in making a decision about a loan, encode this knowledge into a set of rules for an expert system, and then use the expert system to make decisions. The knowledge used by the loan officer might have originated as a set of “policies” (the domain theory), but perhaps the application of these policies were specialized and made more efficient through experience with the special cases of loans made in his or her district.

# 12.3 An Example

To make our discussion more concrete, let’s consider the following fanciful example. We want to find a way to classify robots as “robust” or not. The attributes that we use to represent a robot might include some that are relevant to this decision and some that are not.

Suppose we have a domain theory of logical sentences that taken together, help to define whether or not a robot can be classified as robust. (The same domain theory may be useful for several other purposes also, but among other things, it describes the concept “robust.”)

In this example, let’s suppose that our domain theory includes the sentences:

$$
F i x e s (u, u) \supset R o b u s t (u)
$$

(An individual that can fix itself is robust.)

$$
S e e s (x, y) \land H a b i l e (x) \supset F i x e s (x, y)
$$

(A habile individual that can see another entity can fix that entity.)

$$
R o b o t (w) \supset S e e s (w, w)
$$

(All robots can see themselves.)

$$
R 2 D 2 (x) \supset H a b i l e (x)
$$

(R2D2-class individuals are habile.)

$$
C 3 P O (x) \supset H a b i l e (x)
$$

(C3PO-class individuals are habile.)

(By convention, variables are assumed to be universally quantified.) We could use theorem-proving methods operating on this domain theory to conclude whether certain robots are robust. These methods might be computationally quite expensive because extensive search may have to be performed to derive a conclusion. But after having found a proof for some particular robot, we might be able to derive some new sentence whose use allows a much faster conclusion.

We next show how such a new rule might be derived in this example. Suppose we are given a number of facts about Num5, such as:

$$
R o b o t (N u m 5)
$$

$$
R 2 D 2 (N u m 5)
$$

$$
A g e (N u m 5, 5)
$$

$$
M a n u f a c t u r e (N u m 5, G R)
$$

![](images/d242228e531966cd4e54c7c11be79bf261626fb315724ab591cffd007294bc3b.jpg)  
Figure 12.2: A Proof Tree

We are also told that $R o b u s t ( N u m 5 )$ is true, but we nevertheless attempt to find a proof of that assertion using these facts about Num5 and the domain theory. The facts about Num5 correspond to the features that we might use to represent Num5. In this example, not all of them are relevant to a decision about $R o b u s t ( N u m 5 )$ . The relevant ones are those used or needed in proving $R o b u s t ( N u m 5 )$ using the domain theory. The proof tree in Fig. 12.2 is one that a typical theorem-proving system might produce.

In the language of EBL, this proof is an explanation for the fact $R o b u s t ( N u m 5 )$ . We see from this explanation that the only facts about Num5 that were used were $R o b o t ( N u m 5 )$ and $R 2 D 2 ( N u m 5 )$ . In fact, we could construct the following rule from this explanation:

$$
R o b o t (N u m 5) \wedge R 2 D 2 (N u m 5) \supset R o b u s t (N u m 5)
$$

The explanation has allowed us to prune some attributes about Num5 that are irrelevant (at least for deciding $R o b u s t ( N u m 5 ) _ { , } ^ { }$ ). This type of pruning is the first sense in which an explanation is used to generalize the classification problem. ([DeJong & Mooney, 1986] call this aspect of explanation-based learning feature elimination.) But the rule we extracted from the explanation applies only to Num5. There might be little value in learning that rule since it is so specific. Can it be generalized so that it can be applied to other individuals as well?

Examination of the proof shows that the same proof structure, using the same sentences from the domain theory, could be used independently of whether we are talking about Num5 or some other individual. We can generalize the proof by a process that replaces constants in the tip nodes of the proof tree with variables and works upward—using unification to constrain the values of variables as needed to obtain a proof.

In this example, we replace $R o b o t ( N u m 5 )$ by $R o b o t ( r )$ and $R 2 D 2 ( N u m 5 )$ by $R 2 D 2 ( s )$ and redo the proof—using the explanation proof as a template. Note that we use different values for the two different occurrences of $N u m 5$ at the tip nodes. Doing so sometimes results in more general, but nevertheless valid rules. We now apply the rules used in the proof in the forward direction, keeping track of the substitutions imposed by the most general unifiers used in the proof. (Note that we always substitute terms that are already in the tree for variables in rules.) This process results in the generalized proof tree shown in Fig. 12.3. Note that the occurrence of $S e e s ( r , r )$ as a node in the tree forces the unification of $x$ with $y$ in the domain rule, $S e e s ( x , y ) \land H a b i l e ( y ) \supset F i x e s ( x , y )$ . The substitutions are then applied to the variables in the tip nodes and the root node to yield the general rule: $R o b o t ( r ) \wedge R 2 D 2 ( r ) \supset R o b u s t ( r )$ .

This rule is the end result of EBL for this example. The process by which Num5 in this example was generalized to a variable is what [DeJong & Mooney, 1986] call identity elimination (the precise identity of Num5 turned out to be irrelevant). (The generalization process described in this example is based on that of [DeJong & Mooney, 1986] and differs from that of [Mitchell, et al., 1986]. It is also similar to that used in [Fikes, et al., 1972].) Clearly, under certain assumptions, this general rule is more easily used to conclude Robust about an individual than the original proof process was.

It is important to note that we could have derived the general rule from the domain theory without using the example. (In the literature, doing so is called static analysis [Etzioni, 1991].) In fact, the example told us nothing new other than what it told us about Num5. The sole role of the example in this instance of EBL was to provide a template for a proof to help guide the generalization process. Basing the generalization process on examples helps to insure that we learn rules matched to the distribution of problems that occur.

There are a number of qualifications and elaborations about EBL that need to be mentioned.

# 12.4 Evaluable Predicates

The domain theory includes a number of predicates other than the one occuring in the formula we are trying to prove and other than those that might customarily be used to describe an individual. One might note, for example, that if we used $H a b i l e ( N u m 5 )$ to describe Num5, the proof would have been shorter. Why didn’t we? The situation is analogous to that of using a data base augmented by logical rules. In the latter application, the formulas in the actual data base

![](images/d4886ed0695e6d3fde97d5290d0f3f0adc31b4da0ba433a8c3b31600712c2d19.jpg)  
Figure 12.3: A Generalized Proof Tree

are “extensional,” and those in the logical rules are “intensional.” This usage reflects the fact that the predicates in the data base part are defined by their extension—we explicitly list all the tuples sastisfying a relation. The logical rules serve to connect the data base predicates with higher level abstractions that are described (if not defined) by the rules. We typically cannot look up the truth values of formulas containing these intensional predicates; they have to be derived using the rules and the database.

The EBL process assumes something similar. The domain theory is useful for connecting formulas that we might want to prove with those whose truth values can be “looked up” or otherwise evaluated. In the EBL literature, such formulas satisfy what is called the operationality criterion. Perhaps another analogy might be to neural networks. The evaluable predicates correspond to the components of the input pattern vector; the predicates in the domain theory correspond to the hidden units. Finding the new rule corresponds to finding a simpler expression for the formula to be proved in terms only of the evaluable predicates.

# 12.5 More General Proofs

Examining the domain theory for our example reveals that an alternative rule might have been: $R o b o t ( u ) \wedge C 3 P O ( u ) \supset R o b u s t ( u )$ . Such a rule might have resulted if we were given $\{ C 3 P O ( N u m 6 ) , R o b o t ( N u m 6 ) , . . . \}$ and proved $R o b u s t ( N u m 6 )$ . After considering these two examples (Num5 and Num6), the question arises, do we want to generalize the two rules to something like: $R o b o t ( u ) \land [ C 3 P O ( u ) \lor R 2 D 2 ( u ) ] \supset R o b u s t ( u ) ^ { . }$ ? Doing so is an example of what [DeJong $\&$ Mooney, 1986] call structural generalization (via disjunctive augmentation ).

Adding disjunctions for every alternative proof can soon become cumbersome and destroy any efficiency advantage of EBL. In our example, the efficiency might be retrieved if there were another evaluable predicate, say, $B i o n i c ( u )$ such that the domain theory also contained $R 2 D 2 ( x ) \supset B i o n i c ( x )$ and $C 3 P O ( x ) \supset$ $B i o n i c ( x )$ . After seeing a number of similar examples, we might be willing to induce the formula $B i o n i c ( u ) \supset [ C 3 P O ( u ) \lor R 2 D 2 ( u ) ]$ in which case the rule with the disjunction could be replaced with $R o b o t ( u ) \land B i o n i c ( u ) \supset R o b u s t ( u )$ .

# 12.6 Utility of EBL

It is well known in theorem proving that the complexity of finding a proof depends both on the number of formulas in the domain theory and on the depth of the shortest proof. Adding a new rule decreases the depth of the shortest proof but it also increases the number of formulas in the domain theory. In realistic applications, the added rules will be relevant for some tasks and not for others. Thus, it is unclear whether the overall utility of the new rules will turn out to be positive. EBL methods have been applied in several settings, usually with positive utility. (See [Minton, 1990] for an analysis).

# 12.7 Applications

There have been several applications of EBL methods. We mention two here, namely the formation of macro-operators in automatic plan generation and learning how to control search.

# 12.7.1 Macro-Operators in Planning

In automatic planning systems, efficiency can sometimes be enhanced by chaining together a sequence of operators into macro-operators. We show an example of a process for creating macro-operators based on techniques explored by [Fikes, et al., 1972].

Referring to Fig. 12.4, consider the problem of finding a plan for a robot in room $R 1$ to fetch a box, $B 1$ , by going to an adjacent room, $R 2$ , and pushing it

back to $R 1$ . The goal for the robot is $I N R O O M ( B 1 , R 1 )$ , and the facts that are true in the initial state are listed in the figure.

![](images/c0af80dc8f18f18163a9078ac78ba88216561300347086b05b26bac59b841090.jpg)  
Figure 12.4: Initial State of a Robot Problem

Initial State:

INROOM(ROBOT, R1)

INROOM(B1,R2)

CONNECTS(D1,R1,R2)

CONNECTS(D1,R2,R1)

We will construct the plan from a set of STRIPS operators that include:

GOTHRU $( d , r 1 , r 2 )$

Preconditions: $I N R O O M ( R O B O T , r 1 ) , C O N N E C T S ( d , r 1 , r 2 )$

Delete list: $I N R O O M ( R O B O T , r 1 )$

Add list: $I N R O O M ( R O B O T , r 2 )$

PUSHTHRU $\underline { { ( b , d , r 1 , r 2 ) } }$

Preconditions: $I N R O O M ( R O B O T , r 1 ) , C O N N E C T S ( d , r 1 , r 2 ) , I N R O O M ( b , r 1 )$

Delete list: $I N R O O M ( R O B O T , r 1 ) , I N R O O M ( b , r 1 )$

Add list: $I N R O O M ( R O B O T , r 2 ) , I N R O O M ( b , r 2 )$

A backward-reasoning STRIPS system might produce the plan shown in Fig. 12.5. We show there the main goal and the subgoals along a solution path. (The conditions in each subgoal that are true in the initial state are shown underlined.) The preconditions for this plan, true in the initial state, are:

$I N R O O M ( R O B O T , R 1 )$

$C O N N E C T S ( D 1 , R 1 , R 2 )$

$C O N N E C T S ( D 1 , R 2 , R 1 )$

$I N R O O M ( B 1 , R 2 )$

Saving this specific plan, valid only for the specific constants it mentions, would not be as useful as would be saving a more general one. We first generalize these preconditions by substituting variables for constants. We then follow the structure of the specific plan to produce the generalized plan shown in Fig. 12.6 that achieves $I N R O O M ( b 1 , r 4 )$ . Note that the generalized plan does not require pushing the box back to the place where the robot started. The preconditions for the generalized plan are:

$I N R O O M ( R O B O T , r 1 )$

$C O N N E C T S ( d 1 , r 1 , r 2 )$

$C O N N E C T S ( d 2 , r 2 , r 4 )$

$I N R O O M ( b , r 4 )$

![](images/4e41fcabce5d8810c5e7feff92644242874c0d164f17e883b3b8c8a418fc6d9a.jpg)

![](images/506221db79f452f97b1f40c82d2e346019b6d14c4f0b67e86968256e80d5ce66.jpg)  
Figure 12.5: A Plan for the Robot Problem

GOTHRU(D1,R1,R2) PUSHTHRU(B1,D1,R2,R1)

Another related technique that chains together sequences of operators to form more general ones is the chunking mechanism in Soar [Laird, et al., 1986].

![](images/fa71a92046bb6c170d52281d75554750054ed5e56f726e976749c822449a29dd.jpg)  
Figure 12.6: A Generalized Plan

# 12.7.2 Learning Search Control Knowledge

Besides their use in creating macro-operators, EBL methods can be used to improve the efficiency of planning in another way also. In his system called PRODIGY, Minton proposed using EBL to learn effective ways to control search [Minton, 1988]. PRODIGY is a STRIPS-like system that solves planning problems in the blocks-world, in a simple mobile robot world, and in job-shop scheduling. PRODIGY has a domain theory involving both the domain of the problem and a simple (meta) theory about planning. Its meta theory includes statements about whether a control choice about a subgoal to work on, an operator to apply, etc. either succeeds or fails. After producing a plan, it analyzes its successful and its unsuccessful choices and attempts to explain them in terms of its domain theory. Using an EBL-like process, it is able to produce useful control rules such as:

IF (AND (CURRENT − NODE node)

$$
\left(\text {C A N D I D A T E} - \text {G O A L n o d e (O N x y)}\right)
$$

$$
\left(\text {C A N D I D A T E} - \text {G O A L n o d e (O N y z)}\right))
$$

$$
\text {T H E N} (\text {P R E F E R G O A L} (\text {O N y z}) \text {T O} (\text {O N x y}))
$$

PRODIGY keeps statistics on how often these learned rules are used, their savings (in time to find plans), and their cost of application. It saves only the rules whose utility, thus measured, is judged to be high. Minton [Minton, 1990] has shown that there is an overall advantage of using these rules (as against not having any rules and as against hand-coded search control rules).

# 12.8 Bibliographical and Historical Remarks

To be added.

# Bibliography

[Acorn & Walden, 1992] Acorn, T., and Walden, S., “SMART: Support Management Automated Reasoning Technology for COMPAQ Customer Service,” Proc. Fourth Annual Conf. on Innovative Applications of Artificial Intelligence, Menlo Park, CA: AAAI Press, 1992.   
[Aha, 1991] Aha, D., Kibler, D., and Albert, M., “Instance-Based Learning Algorithms,” Machine Learning, 6, 37-66, 1991.   
[Anderson & Bower, 1973] Anderson, J. R., and Bower, G. H., Human Associative Memory, Hillsdale, NJ: Erlbaum, 1973.   
[Anderson, 1958] Anderson, T. W., An Introduction to Multivariate Statistical Analysis, New York: John Wiley, 1958.   
[Barto, Bradtke, & Singh, 1994] Barto, A., Bradtke, S., and Singh, S., “Learning to Act Using Real-Time Dynamic Programming,” to appear in Artificial Intelligence, 1994.   
[Baum & Haussler, 1989] Baum, E, and Haussler, D., “What Size Net Gives Valid Generalization?” Neural Computation, 1, pp. 151-160, 1989.   
[Baum, 1994] Baum, E., “When Are $k$ -Nearest Neighbor and Backpropagation Accurate for Feasible-Sized Sets of Examples?” in Hanson, S., Drastal, G., and Rivest, R., (eds.), Computational Learning Theory and Natural Learning Systems, Volume 1: Constraints and Prospects, pp. 415-442, Cambridge, MA: MIT Press, 1994.   
[Bellman, 1957] Bellman, R. E., Dynamic Programming, Princeton: Princeton University Press, 1957.   
[Blumer, et al., 1987] Blumer, A., et al., “Occam’s Razor,” Info. Process. Lett., vol 24, pp. 377-80, 1987.   
[Blumer, et al., 1990] Blumer, A., et al., “Learnability and the Vapnik-Chervonenkis Dimension,” JACM, 1990.   
[Bollinger & Duffie, 1988] Bollinger, J., and Duffie, N., Computer Control of Machines and Processes, Reading, MA: Addison-Wesley, 1988.

[Brain, et al., 1962] Brain, A. E., et al., “Graphical Data Processing Research Study and Experimental Investigation,” Report No. 8 (pp. 9-13) and No. 9 (pp. 3-10), Contract DA 36-039 SC-78343, SRI International, Menlo Park, CA, June 1962 and September 1962.   
[Breiman, et al., 1984] Breiman, L., Friedman, J., Olshen, R., and Stone, C., Classification and Regression Trees, Monterey, CA: Wadsworth, 1984.   
[Brent, 1990] Brent, R. P., “Fast Training Algorithms for Multi-Layer Neural Nets,” Numerical Analysis Project Manuscript NA-90-03, Computer Science Department, Stanford University, Stanford, CA 94305, March 1990.   
[Bryson & Ho 1969] Bryson, A., and Ho, Y.-C., Applied Optimal Control, New York: Blaisdell.   
[Buchanan & Wilkins, 1993] Buchanan, B. and Wilkins, D., (eds.), Readings in Knowledge Acquisition and Learning, San Francisco: Morgan Kaufmann, 1993.   
[Carbonell, 1983] Carbonell, J., “Learning by Analogy,” in Machine Learning: An Artificial Intelligence Approach, Michalski, R., Carbonell, J., and Mitchell, T., (eds.), San Francisco: Morgan Kaufmann, 1983.   
[Cheeseman, et al., 1988] Cheeseman, P., et al., “AutoClass: A Bayesian Classification System,” Proc. Fifth Intl. Workshop on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1988. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, Morgan Kaufmann, San Francisco, pp. 296-306, 1990.   
[Cover & Hart, 1967] Cover, T., and Hart, P., “Nearest Neighbor Pattern Classification,” IEEE Trans. on Information Theory, 13, 21-27, 1967.   
[Cover, 1965] Cover, T., “Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition,” IEEE Trans. Elec. Comp., EC-14, 326-334, June, 1965.   
[Dasarathy, 1991] Dasarathy, B. V., Nearest Neighbor Pattern Classification Techniques, IEEE Computer Society Press, 1991.   
[Dayan & Sejnowski, 1994] Dayan, P., and Sejnowski, T., “ $T D ( \lambda )$ Converges with Probability 1,” Machine Learning, 14, pp. 295-301, 1994.   
[Dayan, 1992] Dayan, P., “The Convergence of TD(λ) for General $\lambda$ ,” Machine Learning, 8, 341-362, 1992.   
[DeJong & Mooney, 1986] DeJong, G., and Mooney, R., “Explanation-Based Learning: An Alternative View,” Machine Learning, 1:145-176, 1986. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp 452-467.

[Dietterich & Bakiri, 1991] Dietterich, T. G., and Bakiri, G., “Error-Correcting Output Codes: A General Method for Improving Multiclass Inductive Learning Programs,” Proc. Ninth Nat. Conf. on A.I., pp. 572-577, AAAI-91, MIT Press, 1991.   
[Dietterich, et al., 1990] Dietterich, T., Hild, H., and Bakiri, G., “A Comparative Study of ID3 and Backpropagation for English Text-to-Speech Mapping,” Proc. Seventh Intl. Conf. Mach. Learning, Porter, B. and Mooney, R. (eds.), pp. 24-31, San Francisco: Morgan Kaufmann, 1990.   
[Dietterich, 1990] Dietterich, T., “Machine Learning,” Annu. Rev. Comput. Sci., 4:255-306, Palo Alto: Annual Reviews Inc., 1990.   
[Duda & Fossum, 1966] Duda, R. O., and Fossum, H., “Pattern Classification by Iteratively Determined Linear and Piecewise Linear Discriminant Functions,” IEEE Trans. on Elect. Computers, vol. EC-15, pp. 220-232, April, 1966.   
[Duda & Hart, 1973] Duda, R. O., and Hart, P.E., Pattern Classification and Scene Analysis, New York: Wiley, 1973.   
[Duda, 1966] Duda, R. O., “Training a Linear Machine on Mislabeled Patterns,” SRI Tech. Report prepared for ONR under Contract 3438(00), SRI International, Menlo Park, CA, April 1966.   
[Efron, 1982] Efron, B., The Jackknife, the Bootstrap and Other Resampling Plans, Philadelphia: SIAM, 1982.   
[Ehrenfeucht, et al., 1988] Ehrenfeucht, A., et al., “A General Lower Bound on the Number of Examples Needed for Learning,” in Proc. 1988 Workshop on Computational Learning Theory, pp. 110-120, San Francisco: Morgan Kaufmann, 1988.   
[Etzioni, 1991] Etzioni, O., “STATIC: A Problem-Space Compiler for PRODIGY,” Proc. of Ninth National Conf. on Artificial Intelligence, pp. 533-540, Menlo Park: AAAI Press, 1991.   
[Etzioni, 1993] Etzioni, O., “A Structural Theory of Explanation-Based Learning,” Artificial Intelligence, 60:1, pp. 93-139, March, 1993.   
[Evans & Fisher, 1992] Evans, B., and Fisher, D., Process Delay Analyses Using Decision-Tree Induction, Tech. Report CS92-06, Department of Computer Science, Vanderbilt University, TN, 1992.   
[Fahlman & Lebiere, 1990] Fahlman, S., and Lebiere, C., “The Cascade-Correlation Learning Architecture,” in Touretzky, D., (ed.), Advances in Neural Information Processing Systems, 2, pp. 524-532, San Francisco: Morgan Kaufmann, 1990.

[Fayyad, et al., 1993] Fayyad, U. M., Weir, N., and Djorgovski, S., “SKICAT: A Machine Learning System for Automated Cataloging of Large Scale Sky Surveys,” in Proc. Tenth Intl. Conf. on Machine Learning, pp. 112- 119, San Francisco: Morgan Kaufmann, 1993. (For a longer version of this paper see: Fayyad, U. Djorgovski, G., and Weir, N., “Automating the Analysis and Cataloging of Sky Surveys,” in Fayyad, U., et al.(eds.), Advances in Knowledge Discovery and Data Mining, Chapter 19, pp. 471ff., Cambridge: The MIT Press, March, 1996.)   
[Feigenbaum, 1961] Feigenbaum, E. A., “The Simulation of Verbal Learning Behavior,” Proceedings of the Western Joint Computer Conference, 19:121- 132, 1961.   
[Fikes, et al., 1972] Fikes, R., Hart, P., and Nilsson, N., “Learning and Executing Generalized Robot Plans,” Artificial Intelligence, pp 251-288, 1972. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp 468-486.   
[Fisher, 1987] Fisher, D., “Knowledge Acquisition via Incremental Conceptual Clustering,” Machine Learning, 2:139-172, 1987. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 267–283.   
[Friedman, et al., 1977] Friedman, J. H., Bentley, J. L., and Finkel, R. A., “An Algorithm for Finding Best Matches in Logarithmic Expected Time,” ACM Trans. on Math. Software, 3(3):209-226, September 1977.   
[Fu, 1994] Fu, L., Neural Networks in Artificial Intelligence, New York: McGraw-Hill, 1994.   
[Gallant, 1986] Gallant, S. I., “Optimal Linear Discriminants,” in Eighth International Conf. on Pattern Recognition, pp. 849-852, New York: IEEE, 1986.   
[Genesereth & Nilsson, 1987] Genesereth, M., and Nilsson, N., Logical Foundations of Artificial Intelligence, San Francisco: Morgan Kaufmann, 1987.   
[Gluck & Rumelhart, 1989] Gluck, M. and Rumelhart, D., Neuroscience and Connectionist Theory, The Developments in Connectionist Theory, Hillsdale, NJ: Erlbaum Associates, 1989.   
[Hammerstrom, 1993] Hammerstrom, D., “Neural Networks at Work,” IEEE Spectrum, pp. 26-32, June 1993.   
[Haussler, 1988] Haussler, D., “Quantifying Inductive Bias: AI Learning Algorithms and Valiant’s Learning Framework,” Artificial Intelligence, 36:177-221, 1988. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 96-107.

[Haussler, 1990] Haussler, D., “Probably Approximately Correct Learning,” Proc. Eighth Nat. Conf. on AI, pp. 1101-1108. Cambridge, MA: MIT Press, 1990.   
[Hebb, 1949] Hebb, D. O., The Organization of Behaviour, New York: John Wiley, 1949.   
[Hertz, Krogh, & Palmer, 1991] Hertz, J., Krogh, A, and Palmer, R., Introduction to the Theory of Neural Computation, Lecture Notes, vol. 1, Santa Fe Inst. Studies in the Sciences of Complexity, New York: Addison-Wesley, 1991.   
[Hirsh, 1994] Hirsh, H., “Generalizing Version Spaces,” Machine Learning, 17, 5-45, 1994.   
[Holland, 1975] Holland, J., Adaptation in Natural and Artificial Systems, Ann Arbor: The University of Michigan Press, 1975. (Second edition printed in 1992 by MIT Press, Cambridge, MA.)   
[Holland, 1986] Holland, J. H., “Escaping Brittleness; The Possibilities of General-Purpose Learning Algorithms Applied to Parallel Rule-Based Systems.” In Michalski, R., Carbonell, J., and Mitchell, T. (eds.) , Machine Learning: An Artificial Intelligence Approach, Volume 2, chapter 20, San Francisco: Morgan Kaufmann, 1986.   
[Hunt, Marin, & Stone, 1966] Hunt, E., Marin, J., and Stone, P., Experiments in Induction, New York: Academic Press, 1966.   
[Jabbour, K., et al., 1987] Jabbour, K., et al., “ALFA: Automated Load Forecasting Assistant,” Proc. of the IEEE Pwer Engineering Society Summer Meeting, San Francisco, CA, 1987.   
[John, 1995] John, G., “Robust Linear Discriminant Trees,” Proc. of the Conf. on Artificial Intelligence and Statistics, Ft. Lauderdale, FL, January, 1995.   
[Kaelbling, 1993] Kaelbling, L. P., Learning in Embedded Systems, Cambridge, MA: MIT Press, 1993.   
[Kohavi, 1994] Kohavi, R., “Bottom-Up Induction of Oblivious Read-Once Decision Graphs,” Proc. of European Conference on Machine Learning (ECML-94), 1994.   
[Kolodner, 1993] Kolodner, J., Case-Based Reasoning, San Francisco: Morgan Kaufmann, 1993.   
[Koza, 1992] Koza, J., Genetic Programming: On the Programming of Computers by Means of Natural Selection, Cambridge, MA: MIT Press, 1992.   
[Koza, 1994] Koza, J., Genetic Programming II: Automatic Discovery of Reusable Programs, Cambridge, MA: MIT Press, 1994.

[Laird, et al., 1986] Laird, J., Rosenbloom, P., and Newell, A., “Chunking in Soar: The Anatomy of a General Learning Mechanism,” Machine Learning, 1, pp. 11-46, 1986. Reprinted in Buchanan, B. and Wilkins, D., (eds.), Readings in Knowledge Acquisition and Learning, pp. 518-535, Morgan Kaufmann, San Francisco, CA, 1993.   
[Langley, 1992] Langley, P., “Areas of Application for Machine Learning,” Proc. of Fifth Int’l. Symp. on Knowledge Engineering, Sevilla, 1992.   
[Langley, 1996] Langley, P., Elements of Machine Learning, San Francisco: Morgan Kaufmann, 1996.   
[Lavraˇc & Dˇzeroski, 1994] Lavraˇc, N., and Dˇzeroski, S., Inductive Logic Programming, Chichester, England: Ellis Horwood, 1994.   
[Lin, 1992] Lin, L., “Self-Improving Reactive Agents Based on Reinforcement Learning, Planning, and Teaching,” Machine Learning, 8, 293-321, 1992.   
[Lin, 1993] Lin, L., “Scaling Up Reinforcement Learning for Robot Control,” Proc. Tenth Intl. Conf. on Machine Learning, pp. 182-189, San Francisco: Morgan Kaufmann, 1993.   
[Littlestone, 1988] Littlestone, N., “Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm,” Machine Learning 2: 285-318, 1988.   
[Maass & Tur´an, 1994] Maass, W., and Tur´an, G., “How Fast Can a Threshold Gate Learn?,” in Hanson, S., Drastal, G., and Rivest, R., (eds.), Computational Learning Theory and Natural Learning Systems, Volume 1: Constraints and Prospects, pp. 381-414, Cambridge, MA: MIT Press, 1994.   
[Mahadevan & Connell, 1992] Mahadevan, S., and Connell, J., “Automatic Programming of Behavior-Based Robots Using Reinforcement Learning,” Artificial Intelligence, 55, pp. 311-365, 1992.   
[Marchand & Golea, 1993] Marchand, M., and Golea, M., “On Learning Simple Neural Concepts: From Halfspace Intersections to Neural Decision Lists,” Network, 4:67-85, 1993.   
[McCulloch & Pitts, 1943] McCulloch, W. S., and Pitts, W. H., “A Logical Calculus of the Ideas Immanent in Nervous Activity,” Bulletin of Mathematical Biophysics, Vol. 5, pp. 115-133, Chicago: University of Chicago Press, 1943.   
[Michie, 1992] Michie, D., “Some Directions in Machine Intelligence,” unpublished manuscript, The Turing Institute, Glasgow, Scotland, 1992.   
[Minton, 1988] Minton, S., Learning Search Control Knowledge: An Explanation-Based Approach, Kluwer Academic Publishers, Boston, MA, 1988.

[Minton, 1990] Minton, S., “Quantitative Results Concerning the Utility of Explanation-Based Learning,” Artificial Intelligence, 42, pp. 363-392, 1990. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 573-587.   
[Mitchell, et al., 1986] Mitchell, T., et al., “Explanation-Based Generalization: A Unifying View,” Machine Learning, 1:1, 1986. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 435-451.   
[Mitchell, 1982] Mitchell, T., “Generalization as Search,” Artificial Intelligence, 18:203-226, 1982. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 96–107.   
[Moore & Atkeson, 1993] Moore, A., and Atkeson, C., “Prioritized Sweeping: Reinforcement Learning with Less Data and Less Time,” Machine Learning, 13, pp. 103-130, 1993.   
[Moore, et al., 1994] Moore, A. W., Hill, D. J., and Johnson, M. P., “An Empirical Investigation of Brute Force to Choose Features, Smoothers, and Function Approximators,” in Hanson, S., Judd, S., and Petsche, T., (eds.), Computational Learning Theory and Natural Learning Systems, Vol. 3, Cambridge: MIT Press, 1994.   
[Moore, 1990] Moore, A., Efficient Memory-based Learning for Robot Control, PhD. Thesis; Technical Report No. 209, Computer Laboratory, University of Cambridge, October, 1990.   
[Moore, 1992] Moore, A., “Fast, Robust Adaptive Control by Learning Only Forward Models,” in Moody, J., Hanson, S., and Lippman, R., (eds.), Advances in Neural Information Processing Systems 4, San Francisco: Morgan Kaufmann, 1992.   
[Mueller & Page, 1988] Mueller, R. and Page, R., Symbolic Computing with Lisp and Prolog, New York: John Wiley & Sons, 1988.   
[Muggleton, 1991] Muggleton, S., “Inductive Logic Programming,” New Generation Computing, 8, pp. 295-318, 1991.   
[Muggleton, 1992] Muggleton, S., Inductive Logic Programming, London: Academic Press, 1992.   
[Muroga, 1971] Muroga, S., Threshold Logic and its Applications, New York: Wiley, 1971.   
[Natarjan, 1991] Natarajan, B., Machine Learning: A Theoretical Approach, San Francisco: Morgan Kaufmann, 1991.

[Nilsson, 1965] Nilsson, N. J., “Theoretical and Experimental Investigations in Trainable Pattern-Classifying Systems,” Tech. Report No. RADC-TR-65-257, Final Report on Contract AF30(602)-3448, Rome Air Development Center (Now Rome Laboratories), Griffiss Air Force Base, New York, September, 1965.   
[Nilsson, 1990] Nilsson, N. J., The Mathematical Foundations of Learning Machines, San Francisco: Morgan Kaufmann, 1990. (This book is a reprint of Learning Machines: Foundations of Trainable Pattern-Classifying Systems, New York: McGraw-Hill, 1965.)   
[Oliver, Dowe, & Wallace, 1992] Oliver, J., Dowe, D., and Wallace, C., “Inferring Decision Graphs using the Minimum Message Length Principle,” Proc. 1992 Australian Artificial Intelligence Conference, 1992.   
[Pagallo & Haussler, 1990] Pagallo, G. and Haussler, D., “Boolean Feature Discovery in Empirical Learning,” Machine Learning, vol.5, no.1, pp. 71-99, March 1990.   
[Pazzani & Kibler, 1992] Pazzani, M., and Kibler, D., “The Utility of Knowledge in Inductive Learning,” Machine Learning, 9, 57-94, 1992.   
[Peterson, 1961] Peterson, W., Error Correcting Codes, New York: John Wiley, 1961.   
[Pomerleau, 1991] Pomerleau, D., “Rapidly Adapting Artificial Neural Networks for Autonomous Navigation,” in Lippmann, P., et al. (eds.), Advances in Neural Information Processing Systems, 3, pp. 429-435, San Francisco: Morgan Kaufmann, 1991.   
[Pomerleau, 1993] Pomerleau, D, Neural Network Perception for Mobile Robot Guidance, Boston: Kluwer Academic Publishers, 1993.   
[Quinlan & Rivest, 1989] Quinlan, J. Ross, and Rivest, Ron, “Inferring Decision Trees Using the Minimum Description Length Principle,” Information and Computation, 80:227–248, March, 1989.   
[Quinlan, 1986] Quinlan, J. Ross, “Induction of Decision Trees,” Machine Learning, 1:81–106, 1986. Reprinted in Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990, pp. 57–69.   
[Quinlan, 1987] Quinlan, J. R., “Generating Production Rules from Decision Trees,” In IJCAI-87: Proceedings of the Tenth Intl. Joint Conf. on Artificial Intelligence, pp. 304-7, San Francisco: Morgan-Kaufmann, 1987.   
[Quinlan, 1990] Quinlan, J. R., “Learning Logical Definitions from Relations,” Machine Learning, 5, 239-266, 1990.

[Quinlan, 1993] Quinlan, J. Ross, C4.5: Programs for Machine Learning, San Francisco: Morgan Kaufmann, 1993.   
[Quinlan, 1994] Quinlan, J. R., “Comparing Connectionist and Symbolic Learning Methods,” in Hanson, S., Drastal, G., and Rivest, R., (eds.), Computational Learning Theory and Natural Learning Systems, Volume 1: Constraints and Prospects, pp. 445-456,, Cambridge, MA: MIT Press, 1994.   
[Ridgway, 1962] Ridgway, W. C., An Adaptive Logic System with Generalizing Properties, PhD thesis, Tech. Rep. 1556-1, Stanford Electronics Labs., Stanford, CA, April 1962.   
[Rissanen, 1978] Rissanen, J., “Modeling by Shortest Data Description,” Automatica, 14:465-471, 1978.   
[Rivest, 1987] Rivest, R. L., “Learning Decision Lists,” Machine Learning, 2, 229-246, 1987.   
[Rosenblatt, 1958] Rosenblatt, F., Principles of Neurodynamics, Washington: Spartan Books, 1961.   
[Ross, 1983] Ross, S., Introduction to Stochastic Dynamic Programming, New York: Academic Press, 1983.   
[Rumelhart, Hinton, & Williams, 1986] Rumelhart, D. E., Hinton, G. E., and Williams, R. J., “Learning Internal Representations by Error Propagation,” In Rumelhart, D. E., and McClelland, J. L., (eds.) Parallel Distributed Processing, Vol 1, 318–362, 1986.   
[Russell & Norvig 1995] Russell, S., and Norvig, P., Artificial Intelligence: A Modern Approach, Englewood Cliffs, NJ: Prentice Hall, 1995.   
[Samuel, 1959] Samuel, A., “Some Studies in Machine Learning Using the Game of Checkers,” IBM Journal of Research and Development, 3:211-229, July 1959.   
[Schwartz, 1993] Schwartz, A., “A Reinforcement Learning Method for Maximizing Undiscounted Rewards,” Proc. Tenth Intl. Conf. on Machine Learning, pp. 298-305, San Francisco: Morgan Kaufmann, 1993.   
[Sejnowski, Koch, & Churchland, 1988] Sejnowski, T., Koch, C., and Churchland, P., “Computational Neuroscience,” Science, 241: 1299-1306, 1988.   
[Shavlik, Mooney, & Towell, 1991] Shavlik, J., Mooney, R., and Towell, G., “Symbolic and Neural Learning Algorithms: An Experimental Comparison,” Machine Learning, 6, pp. 111-143, 1991.   
[Shavlik & Dietterich, 1990] Shavlik, J. and Dietterich, T., Readings in Machine Learning, San Francisco: Morgan Kaufmann, 1990.

[Sutton & Barto, 1987] Sutton, R. S., and Barto, A. G., “A Temporal-Difference Model of Classical Conditioning,” in Proceedings of the Ninth Annual Conference of the Cognitive Science Society, Hillsdale, NJ: Erlbaum, 1987.   
[Sutton, 1988] Sutton, R. S., “Learning to Predict by the Methods of Temporal Differences,” Machine Learning 3: 9-44, 1988.   
[Sutton, 1990] Sutton, R., “Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming,” Proc. of the Seventh Intl. Conf. on Machine Learning, pp. 216-224, San Francisco: Morgan Kaufmann, 1990.   
[Taylor, Michie, & Spiegalhalter, 1994] Taylor, C., Michie, D., and Spiegalhalter, D., Machine Learning, Neural and Statistical Classification, Paramount Publishing International.   
[Tesauro, 1992] Tesauro, G., “Practical Issues in Temporal Difference Learning,” Machine Learning, 8, nos. 3/4, pp. 257-277, 1992.   
[Towell & Shavlik, 1992] Towell G., and Shavlik, J., “Interpretation of Artificial Neural Networks: Mapping Knowledge-Based Neural Networks into Rules,” in Moody, J., Hanson, S., and Lippmann, R., (eds.), Advances in Neural Information Processing Systems, 4, pp. 977-984, San Francisco: Morgan Kaufmann, 1992.   
[Towell, Shavlik, & Noordweier, 1990] Towell, G., Shavlik, J., and Noordweier, M., “Refinement of Approximate Domain Theories by Knowledge-Based Artificial Neural Networks,” Proc. Eighth Natl., Conf. on Artificial Intelligence, pp. 861-866, 1990.   
[Unger, 1989] Unger, S., The Essence of Logic Circuits, Englewood Cliffs, NJ: Prentice-Hall, 1989.   
[Utgoff, 1989] Utgoff, P., “Incremental Induction of Decision Trees,” Machine Learning, 4:161–186, Nov., 1989.   
[Valiant, 1984] Valiant, L., “A Theory of the Learnable,” Communications of the ACM, Vol. 27, pp. 1134-1142, 1984.   
[Vapnik & Chervonenkis, 1971] Vapnik, V., and Chervonenkis, A., “On the Uniform Convergence of Relative Frequencies, Theory of Probability and its Applications, Vol. 16, No. 2, pp. 264-280, 1971.   
[Various Editors, 1989-1994] Advances in Neural Information Processing Systems, vols 1 through 6, San Francisco: Morgan Kaufmann, 1989 -1994.   
[Watkins & Dayan, 1992] Watkins, C. J. C. H., and Dayan, P., “Technical Note: Q-Learning,” Machine Learning, 8, 279-292, 1992.

[Watkins, 1989] Watkins, C. J. C. H., Learning From Delayed Rewards, PhD Thesis, University of Cambridge, England, 1989.   
[Weiss & Kulikowski, 1991] Weiss, S., and Kulikowski, C., Computer Systems that Learn, San Francisco: Morgan Kaufmann, 1991.   
[Werbos, 1974] Werbos, P., Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences, Ph.D. Thesis, Harvard University, 1974.   
[Widrow & Lehr, 1990] Widrow, B., and Lehr, M. A., “30 Years of Adaptive Neural Networks: Perceptron, Madaline and Backpropagation,” Proc. IEEE, vol. 78, no. 9, pp. 1415-1442, September, 1990.   
[Widrow & Stearns, 1985] Widrow, B., and Stearns, S., Adaptive Signal Processing, Englewood Cliffs, NJ: Prentice-Hall.   
[Widrow, 1962] Widrow, B., “Generalization and Storage in Networks of Adaline Neurons,” in Yovits, Jacobi, and Goldstein (eds.), Self-organizing Systems—1962, pp. 435-461, Washington, DC: Spartan Books, 1962.   
[Winder, 1961] Winder, R., “Single Stage Threshold Logic,” Proc. of the AIEE Symp. on Switching Circuits and Logical Design, Conf. paper CP-60- 1261, pp. 321-332, 1961.   
[Winder, 1962] Winder, R., Threshold Logic, PhD Dissertation, Princeton University, Princeton, NJ, 1962.   
[Wnek, et al., 1990] Wnek, J., et al., “Comparing Learning Paradigms via Diagrammatic Visualization,” in Proc. Fifth Intl. Symp. on Methodologies for Intelligent Systems, pp. 428-437, 1990. (Also Tech. Report MLI90-2, University of Illinois at Urbana-Champaign.)

# Mathematical Foundations of

# Reinforcement Learning

Shiyu Zhao

# Contents

# Contents v

# Preface vii

# Overview of this Book ix

# 1 Basic Concepts 1

1.1 A grid world example . 1   
1.2 State and action . 2   
1.3 State transition 3   
1.4 Policy 4   
1.5 Reward 6   
1.6 Trajectories, returns, and episodes . . 8   
1.7 Markov decision processes 11   
1.8 Summary 12   
1.9 Q&A 12

# 2 State Values and Bellman Equation 15

2.1 Motivating example 1: Why are returns important? 16   
2.2 Motivating example 2: How to calculate returns? 17   
2.3 State values 19   
2.4 Bellman equation 20   
2.5 Examples for illustrating the Bellman equation . . 22   
2.6 Matrix-vector form of the Bellman equation 25   
2.7 Solving state values from the Bellman equation 27

2.7.1 Closed-form solution 27   
2.7.2 Iterative solution 28   
2.7.3 Illustrative examples 28

2.8 From state value to action value 30

2.8.1 Illustrative examples 31   
2.8.2 The Bellman equation in terms of action values 32

2.9 Summary 33

2.10 Q&A . 33

# 3 Optimal State Values and Bellman Optimality Equation 35

3.1 Motivating example: How to improve policies? . 36   
3.2 Optimal state values and optimal policies . . 37   
3.3 Bellman optimality equation . 38

3.3.1 Maximization of the right-hand side of the BOE 39   
3.3.2 Matrix-vector form of the BOE 40   
3.3.3 Contraction mapping theorem . 40   
3.3.4 Contraction property of the right-hand side of the BOE . . 44

3.4 Solving an optimal policy from the BOE 46   
3.5 Factors that influence optimal policies 49   
3.6 Summary 53   
3.7 Q&A 54

# 4 Value Iteration and Policy Iteration 57

4.1 Value iteration 58

4.1.1 Elementwise form and implementation 58   
4.1.2 Illustrative examples 59

4.2 Policy iteration 62

4.2.1 Algorithm analysis 62   
4.2.2 Elementwise form and implementation 65   
4.2.3 Illustrative examples 67

4.3 Truncated policy iteration 70

4.3.1 Comparing value iteration and policy iteration . 70   
4.3.2 Truncated policy iteration algorithm 72

4.4 Summary 74   
4.5 Q&A 74

# 5 Monte Carlo Methods 77

5.1 Motivating example: Mean estimation 78   
5.2 MC Basic: The simplest MC-based algorithm 80

5.2.1 Converting policy iteration to be model-free 80   
5.2.2 The MC Basic algorithm . 81   
5.2.3 Illustrative examples 83

5.3 MC Exploring Starts 86

5.3.1 Utilizing samples more efficiently 86   
5.3.2 Updating policies more efficiently . . 87   
5.3.3 Algorithm description 88

5.4 MC $\epsilon$ -Greedy: Learning without exploring starts . . 89

5.4.1 -greedy policies . . 89

5.4.2 Algorithm description 90   
5.4.3 Illustrative examples 91

5.5 Exploration and exploitation of $\epsilon$ -greedy policies . . . 92   
5.6 Summary 97   
5.7 Q&A 97

# 6 Stochastic Approximation 101

6.1 Motivating example: Mean estimation 102   
6.2 Robbins-Monro algorithm 103

6.2.1 Convergence properties . 105   
6.2.2 Application to mean estimation 108

6.3 Dvoretzky’s convergence theorem 109

6.3.1 Proof of Dvoretzky’s theorem 110   
6.3.2 Application to mean estimation 112   
6.3.3 Application to the Robbins-Monro theorem 112   
6.3.4 An extension of Dvoretzky’s theorem 113

6.4 Stochastic gradient descent . . 114

6.4.1 Application to mean estimation 116   
6.4.2 Convergence pattern of SGD . 116   
6.4.3 A deterministic formulation of SGD . . 118   
6.4.4 BGD, SGD, and mini-batch GD 119   
6.4.5 Convergence of SGD 121

6.5 Summary 123   
6.6 Q&A 123

# 7 Temporal-Difference Methods 125

7.1 TD learning of state values . 126

7.1.1 Algorithm description 126   
7.1.2 Property analysis 128   
7.1.3 Convergence analysis 130

7.2 TD learning of action values: Sarsa 133

7.2.1 Algorithm description 133   
7.2.2 Optimal policy learning via Sarsa . . 134

7.3 TD learning of action values: $n$ -step Sarsa 138   
7.4 TD learning of optimal action values: Q-learning 140

7.4.1 Algorithm description 140   
7.4.2 Off-policy vs on-policy 141   
7.4.3 Implementation 144   
7.4.4 Illustrative examples 144

7.5 A unified viewpoint . . 145   
7.6 Summary 148

7.7 Q&A . 149

# 8 Value Function Methods 151

8.1 Value representation: From table to function . 152   
8.2 TD learning of state values based on function approximation . 155

8.2.1 Objective function 156   
8.2.2 Optimization algorithms 161   
8.2.3 Selection of function approximators 162   
8.2.4 Illustrative examples 164   
8.2.5 Theoretical analysis . 167

8.3 TD learning of action values based on function approximation 179

8.3.1 Sarsa with function approximation 179   
8.3.2 Q-learning with function approximation 180

8.4 Deep Q-learning . . . 181

8.4.1 Algorithm description 182   
8.4.2 Illustrative examples 184

8.5 Summary 186   
8.6 Q&A 187

# 9 Policy Gradient Methods 191

9.1 Policy representation: From table to function 192   
9.2 Metrics for defining optimal policies . 193

9.3 Gradients of the metrics 198

9.3.1 Derivation of the gradients in the discounted case . 200   
9.3.2 Derivation of the gradients in the undiscounted case . . . 205

9.4 Monte Carlo policy gradient (REINFORCE) 210   
9.5 Summary 213   
9.6 Q&A . 213

# 10 Actor-Critic Methods 215

10.1 The simplest actor-critic algorithm (QAC) . . 216   
10.2 Advantage actor-critic (A2C) 217

10.2.1 Baseline invariance 217   
10.2.2 Algorithm description 220

10.3 Off-policy actor-critic . 221

10.3.1 Importance sampling . 221   
10.3.2 The off-policy policy gradient theorem 224   
10.3.3 Algorithm description 226

10.4 Deterministic actor-critic 227

10.4.1 The deterministic policy gradient theorem 227   
10.4.2 Algorithm description 234

10.5 Summary 235   
10.6 Q&A . 236

A Preliminaries for Probability Theory 237   
B Measure-Theoretic Probability Theory 243   
C Convergence of Sequences 251

C.1 Convergence of deterministic sequences . . 251   
C.2 Convergence of stochastic sequences . . 254

D Preliminaries for Gradient Descent 259

Bibliography 270

Symbols 271

Index 273

# Preface

This book aims to provide a mathematical but friendly introduction to the fundamental concepts, basic problems, and classic algorithms in reinforcement learning. Some essential features of this book are highlighted as follows.

The book introduces reinforcement learning from a mathematical point of view. Hopefully, readers will not only know the procedure of an algorithm but also understand why the algorithm was designed in the first place and why it works effectively.   
The depth of the mathematics is carefully controlled to an adequate level. The mathematics is also presented in a carefully designed manner to ensure that the book is friendly to read. Readers can read the materials presented in gray boxes selectively according to their interests.   
Many illustrative examples are given to help readers better understand the topics. All the examples in this book are based on a grid world task, which is easy to understand and helpful for illustrating concepts and algorithms.   
 When introducing an algorithm, the book aims to separate its core idea from complications that may be distracting. In this way, readers can better grasp the core idea of an algorithm.   
 The contents of the book are coherently organized. Each chapter is built based on the preceding chapter and lays a necessary foundation for the subsequent one.

This book is designed for senior undergraduate students, graduate students, researchers, and practitioners who are interested in reinforcement learning. It does not require readers to have any background in reinforcement learning because it starts by introducing the most basic concepts. If the reader already has some background in reinforcement learning, I believe the book can help them understand some topics more deeply or provide different perspectives. This book, however, requires the reader to have some knowledge of probability theory and linear algebra. Some basics of the required mathematics are also included in the appendix of this book.

I have been teaching a graduate-level course on reinforcement learning since 2019. I want to thank the students in my class for their feedback on my teaching. I put the draft of this book online in August 2022. Up to now, I have received valuable feedback from many readers. I want to express my gratitude to these readers. Moreover, I would like

to thank my research assistant, Jialing Lv, for her excellent support in editing the book and my lecture videos; my teaching assistants, Jianan Li and Yize Mi, for their help in my teaching; my Ph.D. student Canlun Zheng for his help in the design of a picture in the book; and my family for their wonderful support. Finally, I would like to thank the editors of this book, Dr. Lanlan Chang and Mr. Sai Guo from Springer Nature Press and Tsinghua University Press, for their great support.

Please note that I have created an open course based on this textbook. Both the slides of the open course and the PDF of this textbook are available online for free download. For more information, you can visit the homepage of the textbook: https://github.com/ MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning

I sincerely hope this book can help readers smoothly enter the exciting field of reinforcement learning.

Shiyu Zhao

# Overview of this Book

![](images/25f51a7400dfdfacc592bc9ed79a5e30c04f610e611c9223c0ff38983e42988d.jpg)  
Figure 1: The map of this book.

Before we start the journey, it is important to look at the “map” of the book shown in Figure 1. This book contains ten chapters, which can be classified into two parts: the first part is about basic tools, and the second part is about algorithms. The ten chapters are highly correlated. In general, it is necessary to study the earlier chapters first before the later ones.

Next, please follow me on a quick tour through the ten chapters. Two aspects of each chapter will be covered. The first aspect is the contents introduced in each chapter, and the second aspect is its relationships with the previous and subsequent chapters. A heads up for you to read this overview is as follows. The purpose of this overview is to give you an impression of the contents and structure of this book. It is all right if you encounter many concepts you do not understand. Hopefully, you can make a proper study plan

that is suitable for you after reading this overview.

Chapter 1 introduces the basic concepts such as states, actions, rewards, returns, and policies, which are widely used in the subsequent chapters. These concepts are first introduced based on a grid world example, where a robot aims to reach a prespecified target. Then, the concepts are introduced in a more formal manner based on the framework of Markov decision processes.

Chapter 2 introduces two key elements. The first is a key concept, and the second is a key tool. The key concept is the state value, which is defined as the expected return that an agent can obtain when starting from a state if it follows a given policy. The greater the state value is, the better the corresponding policy is. Thus, state values can be used to evaluate whether a policy is good or not.

The key tool is the Bellman equation, which can be used to analyze state values. In a nutshell, the Bellman equation describes the relationship between the values of all states. By solving the Bellman equation, we can obtain the state values. Such a process is called policy evaluation, which is a fundamental concept in reinforcement learning. Finally, this chapter introduces the concept of action values.

Chapter 3 also introduces two key elements. The first is a key concept, and the second is a key tool. The key concept is the optimal policy. An optimal policy has the greatest state values compared to other policies. The key tool is the Bellman optimality equation. As its name suggests, the Bellman optimality equation is a special Bellman equation.

Here is a fundamental question: what is the ultimate goal of reinforcement learning? The answer is to obtain optimal policies. The Bellman optimality equation is important because it can be used to obtain optimal policies. We will see that the Bellman optimality equation is elegant and can help us thoroughly understand many fundamental problems.

The first three chapters constitute the first part of this book. This part lays the necessary foundations for the subsequent chapters. Starting in Chapter 4, the book introduces algorithms for learning optimal policies.

Chapter 4 introduces three algorithms: value iteration, policy iteration, and truncated policy iteration. The three algorithms have close relationships with each other. First, the value iteration algorithm is exactly the algorithm introduced in Chapter 3 for solving the Bellman optimality equation. Second, the policy iteration algorithm is an extension of the value iteration algorithm. It is also the foundation for Monte Carlo (MC) algorithms introduced in Chapter 5. Third, the truncated policy iteration algorithm is a unified version that includes the value iteration and policy iteration algorithms as special cases.

The three algorithms share the same structure. That is, every iteration has two steps. One step is to update the value, and the other step is to update the policy. The idea of the interaction between value and policy updates widely exists in reinforcement learning algorithms. This idea is also known as generalized policy iteration. In addition, the algorithms introduced in this chapter are actually dynamic programming algorithms, which require system models. By contrast, all the algorithms introduced in the subsequent chapters do not require models. It is important to well understand the contents of this chapter before proceeding to the subsequent ones.

Starting in Chapter 5, we introduce model-free reinforcement learning algorithms that do not require system models. While this is the first time we introduce model-free algorithms in this book, we must fill a knowledge gap: how to find optimal policies without models? The philosophy is simple. If we do not have a model, we must have some data. If we do not have data, we must have a model. If we have neither, then we can do nothing. The “data” in reinforcement learning refer to the experience samples generated when the agent interacts with the environment.

This chapter introduces three algorithms based on MC estimation that can learn optimal policies from experience samples. The first and simplest algorithm is MC Basic, which can be readily obtained by extending the policy iteration algorithm introduced in Chapter 4. Understanding the MC Basic algorithm is important for grasping the fundamental idea of MC-based reinforcement learning. By extending this algorithm, we further introduce two more complicated but more efficient MCbased algorithms. The fundamental trade-off between exploration and exploitation is also elaborated in this chapter.

Up to this point, the reader may have noticed that the contents of these chapters are highly correlated. For example, if we want to study the MC algorithms (Chapter 5), we must first understand the policy iteration algorithm (Chapter 4). To study the policy iteration algorithm, we must first know the value iteration algorithm (Chapter 4). To comprehend the value iteration algorithm, we first need to understand the Bellman optimality equation (Chapter 3). To understand the Bellman optimality equation, we need to study the Bellman equation (Chapter 2) first. Therefore, it is highly recommended to study the chapters one by one. Otherwise, it may be difficult to understand the contents in the later chapters.

There is a knowledge gap when we move from Chapter 5 to Chapter 7: the algorithms in Chapter 7 are incremental, but the algorithms in Chapter 5 are non-incremental. Chapter 6 is designed to fill this knowledge gap by introducing the stochastic approximation theory. Stochastic approximation refers to a broad class of stochastic iterative algorithms for solving root-finding or optimization problems. The classic Robbins-Monro and stochastic gradient descent algorithms are special stochastic approximation algorithms. Although this chapter does not introduce any reinforcement

learning algorithms, it is important because it lays the necessary foundations for studying Chapter 7.

Chapter 7 introduces the classic temporal-difference (TD) algorithms. With the preparation in Chapter 6, I believe the reader will not be surprised when seeing the TD algorithms. From a mathematical point of view, TD algorithms can be viewed as stochastic approximation algorithms for solving the Bellman or Bellman optimality equations. Like Monte Carlo learning, TD learning is also model-free, but it has some advantages due to its incremental form. For example, it can learn in an online manner: it can update the value estimate every time an experience sample is received. This chapter introduces quite a few TD algorithms such as Sarsa and Q-learning. The important concepts of on-policy and off-policy are also introduced.   
Chapter 8 introduces the value function approximation method. In fact, this chapter continues to introduce TD algorithms, but it uses a different way to represent state/action values. In the preceding chapters, state/action values are represented by tables. The tabular method is straightforward to understand, but it is inefficient for handling large state or action spaces. To solve this problem, we can employ the value function approximation method. The key to understanding this method is to understand the three steps in its optimization formulation. The first step is to select an objective function for defining optimal policies. The second step is to derive the gradient of the objective function. The third step is to apply a gradient-based algorithm to solve the optimization problem. This method is important because it has become the standard technique to represent values. It is also the location in which artificial neural networks are incorporated into reinforcement learning as function approximators. The famous deep Q-learning algorithm is also introduced in this chapter.   
Chapter 9 introduces the policy gradient method, which is the foundation of many modern reinforcement learning algorithms. The policy gradient method is policy-based. It is a large step forward in this book because all the methods in the previous chapters are value-based. The basic idea of the policy gradient method is simple: it selects an appropriate scalar metric and then optimizes it via a gradient-ascent algorithm. Chapter 9 has an intimate relationship with Chapter 8 because they both rely on the idea of function approximation. The advantages of the policy gradient method are numerous. For example, it is more efficient for handling large state/action spaces. It has stronger generalization abilities and is more efficient in sample usage.   
Chapter 10 introduces actor-critic methods. From one point of view, actor-critic refers to a structure that incorporates both policy-based and value-based methods. From another point of view, actor-critic methods are not new since they still fall into the scope of the policy gradient method. Specifically, they can be obtained by extending the policy gradient algorithm introduced in Chapter 9. It is necessary for the reader to properly understand the contents in Chapters 8 and 9 before studying Chapter 10.

# Chapter 1

# Basic Concepts

![](images/b9d0f686cb8883f6f1c413a9df978dc3197891f35277dc1a42bb1eee21fafcd5.jpg)  
Figure 1.1: Where we are in this book.

This chapter introduces the basic concepts of reinforcement learning. These concepts are important because they will be widely used in this book. We first introduce these concepts using examples and then formalize them in the framework of Markov decision processes.

# 1.1 A grid world example

Consider an example as shown in Figure 1.2, where a robot moves in a grid world. The robot, called agent, can move across adjacent cells in the grid. At each time step, it can

only occupy a single cell. The white cells are accessible for entry, and the orange cells are forbidden. There is a target cell that the robot would like to reach. We will use such grid world examples throughout this book since they are intuitive for illustrating new concepts and algorithms.

![](images/781ff628c60e1b9cec2c3ff253abd9f5927ebb920136b3b472b424b929bee542.jpg)  
Figure 1.2: The grid world example is used throughout the book.

The ultimate goal of the agent is to find a “good” policy that enables it to reach the target cell when starting from any initial cell. How can the “goodness” of a policy be defined? The idea is that the agent should reach the target without entering any forbidden cells, taking unnecessary detours, or colliding with the boundary of the grid.

It would be trivial to plan a path to reach the target cell if the agent knew the map of the grid world. The task becomes nontrivial if the agent does not know any information about the environment in advance. Then, the agent must interact with the environment to find a good policy by trial and error. To do that, the concepts presented in the rest of the chapter are necessary.

# 1.2 State and action

The first concept to be introduced is the state, which describes the agent’s status with respect to the environment. In the grid world example, the state corresponds to the agent’s location. Since there are nine cells, there are nine states as well. They are indexed as $s _ { 1 } , s _ { 2 } , \ldots , s _ { 9 }$ , as shown in Figure 1.3(a). The set of all the states is called the state space, denoted as $\boldsymbol { S } = \{ \boldsymbol { s } _ { 1 } , \ldots , \boldsymbol { s } _ { 9 } \}$ .

For each state, the agent can take five possible actions: moving upward, moving rightward, moving downward, moving leftward, and staying still. These five actions are denoted as $a _ { 1 } , a _ { 2 } , \ldots , a _ { 5 }$ , respectively (see Figure 1.3(b)). The set of all actions is called the action space, denoted as $\mathcal { A } = \{ a _ { 1 } , \ldots , a _ { 5 } \}$ . Different states can have different action spaces. For instance, considering that taking $a _ { 1 }$ or $a _ { 4 }$ at state $s _ { 1 }$ would lead to a collision with the boundary, we can set the action space for state $s _ { 1 }$ as $\mathcal { A } ( s _ { 1 } ) = \{ a _ { 2 } , a _ { 3 } , a _ { 5 } \}$ . In this book, we consider the most general case: $\mathcal { A } ( s _ { i } ) = \mathcal { A } = \{ a _ { 1 } , . . . , a _ { 5 } \}$ for all $i$ .

![](images/9586e2bb9beee9ceb0e113318db6fc5f3ee6b4874d1ae6bb4665147bf68ce5da.jpg)  
(a) States

![](images/9bf7a7756fc0cae6885c9d71d28c369c6b80231a0b4e1ae5f560c2d44489b4fe.jpg)  
(b) Actions   
Figure 1.3: Illustrations of the state and action concepts. (a) There are nine states $\{ s _ { 1 } , \ldots , s _ { 9 } \}$ . (b) Each state has five possible actions $\{ a _ { 1 } , a _ { 2 } , a _ { 3 } , a _ { 4 } , a _ { 5 } \}$ .

# 1.3 State transition

When taking an action, the agent may move from one state to another. Such a process is called state transition. For example, if the agent is in state $s _ { 1 }$ and selects action $a _ { 2 }$ (that is, moving rightward), then the agent moves to state $s _ { 2 }$ . Such a process can be expressed as

$$
s _ {1} \xrightarrow {a _ {2}} s _ {2}.
$$

We next examine two important examples.

What is the next state when the agent attempts to go beyond the boundary, for example, taking action $a _ { 1 }$ at state $s _ { 1 }$ ? The answer is that the agent will be bounced back because it is impossible for the agent to exit the state space. Hence, we have $s _ { 1 } \ \xrightarrow { a _ { 1 } } \ s _ { 1 }$ .   
What is the next state when the agent attempts to enter a forbidden cell, for example, taking action $a _ { 2 }$ at state $s _ { 5 }$ ? Two different scenarios may be encountered. In the first scenario, although $s _ { 6 }$ is forbidden, it is still accessible. In this case, the next state is $s _ { 6 }$ ; hence, the state transition process is $s _ { 5 } ~ \xrightarrow { a _ { 2 } } ~ s _ { 6 }$ . In the second scenario, $s _ { 6 }$ is not accessible because, for example, it is surrounded by walls. In this case, the agent is bounced back to $s _ { 5 }$ if it attempts to move rightward; hence, the state transition process is $s _ { 5 } \xrightarrow [ ] { a _ { 2 } } s _ { 5 }$ .

Which scenario should we consider? The answer depends on the physical environment. In this book, we consider the first scenario where the forbidden cells are accessible, although stepping into them may get punished. This scenario is more general and interesting. Moreover, since we are considering a simulation task, we can define the state transition process however we prefer. In real-world applications, the state transition process is determined by real-world dynamics.

The state transition process is defined for each state and its associated actions. This process can be described by a table as shown in Table 1.1. In this table, each row

corresponds to a state, and each column corresponds to an action. Each cell indicates the next state to transition to after the agent takes an action at the corresponding state.

Table 1.1: A tabular representation of the state transition process. Each cell indicates the next state to transition to after the agent takes an action at a state.   

<table><tr><td></td><td>a1 (upward)</td><td>a2 (rightward)</td><td>a3 (downward)</td><td>a4 (leftward)</td><td>a5 (still)</td></tr><tr><td>S1</td><td>S1</td><td>S2</td><td>S4</td><td>S1</td><td>S1</td></tr><tr><td>S2</td><td>S2</td><td>S3</td><td>S5</td><td>S1</td><td>S2</td></tr><tr><td>S3</td><td>S3</td><td>S3</td><td>S6</td><td>S2</td><td>S3</td></tr><tr><td>S4</td><td>S1</td><td>S5</td><td>S7</td><td>S4</td><td>S4</td></tr><tr><td>S5</td><td>S2</td><td>S6</td><td>S8</td><td>S4</td><td>S5</td></tr><tr><td>S6</td><td>S3</td><td>S6</td><td>S9</td><td>S5</td><td>S6</td></tr><tr><td>S7</td><td>S4</td><td>S8</td><td>S7</td><td>S7</td><td>S7</td></tr><tr><td>S8</td><td>S5</td><td>S9</td><td>S8</td><td>S7</td><td>S8</td></tr><tr><td>S9</td><td>S6</td><td>S9</td><td>S9</td><td>S8</td><td>S9</td></tr></table>

Mathematically, the state transition process can be described by conditional probabilities. For example, for $s _ { 1 }$ and $a _ { 2 }$ , the conditional probability distribution is

$$
p (s _ {1} | s _ {1}, a _ {2}) = 0,
$$

$$
p (s _ {2} | s _ {1}, a _ {2}) = 1,
$$

$$
p (s _ {3} | s _ {1}, a _ {2}) = 0,
$$

$$
p \left(s _ {4} \mid s _ {1}, a _ {2}\right) = 0,
$$

$$
p (s _ {5} | s _ {1}, a _ {2}) = 0,
$$

which indicates that, when taking $a _ { 2 }$ at $s _ { 1 }$ , the probability of the agent moving to $s _ { 2 }$ is one, and the probabilities of the agent moving to other states are zero. As a result, taking action $a _ { 2 }$ at $s _ { 1 }$ will certainly cause the agent to transition to $s _ { 2 }$ . The preliminaries of conditional probability are given in Appendix A. Readers are strongly advised to be familiar with probability theory since it is necessary for studying reinforcement learning.

Although it is intuitive, the tabular representation is only able to describe deterministic state transitions. In general, state transitions can be stochastic and must be described by conditional probability distributions. For instance, when random wind gusts are applied across the grid, if taking action $a _ { 2 }$ at $s _ { 1 }$ , the agent may be blown to $s _ { 5 }$ instead of $s _ { 2 }$ . We have $p ( s _ { 5 } | s _ { 1 } , a _ { 2 } ) > 0$ in this case. Nevertheless, we merely consider deterministic state transitions in the grid world examples for simplicity in this book.

# 1.4 Policy

A policy tells the agent which actions to take at every state. Intuitively, policies can be depicted as arrows (see Figure 1.4(a)). Following a policy, the agent can generate a trajectory starting from an initial state (see Figure 1.4(b)).

![](images/8d68ff4ac7557bd8d03fc156739ea3343465979309dccd02a7c30174be96df72.jpg)  
(a) A deterministic policy

![](images/5e7fc31991f2b8f6514a8516c4a8a9ac11b83de90957a8b4db21ccff5d11c3ee.jpg)

![](images/e28073cdd910b95de98ee5d5b9be7e5d60d5bd639af80746bd6e67ad6b4971bf.jpg)

![](images/4481d279ec679c6c7030e5c8bd6199814c5f336ced0f781be4c532eed5980259.jpg)  
(b) Trajectories obtained from the policy   
Figure 1.4: A policy represented by arrows and some trajectories obtained by starting from different initial states.

Mathematically, policies can be described by conditional probabilities. Denote the policy in Figure 1.4 as $\pi ( a | s )$ , which is a conditional probability distribution function defined for every state. For example, the policy for $s _ { 1 }$ is

$$
\pi \left(a _ {1} \mid s _ {1}\right) = 0,
$$

$$
\pi \left(a _ {2} \mid s _ {1}\right) = 1,
$$

$$
\pi \left(a _ {3} \mid s _ {1}\right) = 0,
$$

$$
\pi \left(a _ {4} \mid s _ {1}\right) = 0,
$$

$$
\pi \left(a _ {5} \mid s _ {1}\right) = 0,
$$

which indicates that the probability of taking action $a _ { 2 }$ at state $s _ { 1 }$ is one, and the probabilities of taking other actions are zero.

The above policy is deterministic. Policies may be stochastic in general. For example, the policy shown in Figure 1.5 is stochastic: in state $s _ { 1 }$ , the agent may take actions to go either rightward or downward. The probabilities of taking these two actions are the

same (both are 0.5). In this case, the policy for $s _ { 1 }$ is

$$
\pi \left(a _ {1} \mid s _ {1}\right) = 0,
$$

$$
\pi (a _ {2} | s _ {1}) = 0. 5,
$$

$$
\pi \left(a _ {3} \mid s _ {1}\right) = 0. 5,
$$

$$
\pi \left(a _ {4} \mid s _ {1}\right) = 0,
$$

$$
\pi \left(a _ {5} \mid s _ {1}\right) = 0.
$$

![](images/0605131f194b1f5ab844765a2c9952b82f16dffda121e96217f55cb341abf371.jpg)  
Figure 1.5: A stochastic policy. In state $s _ { 1 }$ , the agent may move rightward or downward with equal probabilities of 0.5.

Policies represented by conditional probabilities can be stored as tables. For example, Table 1.2 represents the stochastic policy depicted in Figure 1.5. The entry in the ith row and $j$ th column is the probability of taking the $j$ th action at the $i$ th state. Such a representation is called a tabular representation. We will introduce another way to represent policies as parameterized functions in Chapter 8.

Table 1.2: A tabular representation of a policy. Each entry indicates the probability of taking an action at a state.   

<table><tr><td></td><td>a1 (upward)</td><td>a2 (rightward)</td><td>a3 (downward)</td><td>a4 (leftward)</td><td>a5 (still)</td></tr><tr><td>s1</td><td>0</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td></tr><tr><td>s2</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>s3</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>s4</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>s5</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>s6</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>s7</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>s8</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>s9</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></table>

# 1.5 Reward

Reward is one of the most unique concepts in reinforcement learning.

After executing an action at a state, the agent obtains a reward, denoted as $r$ , as feedback from the environment. The reward is a function of the state $s$ and action $a$ . Hence, it is also denoted as $r ( s , a )$ . Its value can be a positive or negative real number or zero. Different rewards have different impacts on the policy that the agent would eventually learn. Generally speaking, with a positive reward, we encourage the agent to take the corresponding action. With a negative reward, we discourage the agent from taking that action.

In the grid world example, the rewards are designed as follows:

 If the agent attempts to exit the boundary, let $r _ { \mathrm { b o u n d a r y } } = - 1$ .   
 If the agent attempts to enter a forbidden cell, let $r _ { \mathrm { f o r b i d d e n } } = - 1$   
$\diamond$ If the agent reaches the target state, let $r _ { \mathrm { t a r g e t } } = + 1$ .   
 Otherwise, the agent obtains a reward of $r _ { \mathrm { o t h e r } } = 0$ .

Special attention should be given to the target state $s _ { 9 }$ . The reward process does not have to terminate after the agent reaches $s _ { 9 }$ . If the agent takes action $a _ { 5 }$ at $s _ { 9 }$ , the next state is again $s _ { 9 }$ , and the reward is $r _ { \mathrm { t a r g e t } } = + 1$ . If the agent takes action $a _ { 2 }$ , the next state is also $s _ { 9 }$ , but the reward is $r _ { \mathrm { b o u n d a r y } } = - 1$ .

A reward can be interpreted as a human-machine interface, with which we can guide the agent to behave as we expect. For example, with the rewards designed above, we can expect that the agent tends to avoid exiting the boundary or stepping into the forbidden cells. Designing appropriate rewards is an important step in reinforcement learning. This step is, however, nontrivial for complex tasks since it may require the user to understand the given problem well. Nevertheless, it may still be much easier than solving the problem with other approaches that require a professional background or a deep understanding of the given problem.

The process of getting a reward after executing an action can be intuitively represented as a table, as shown in Table 1.3. Each row of the table corresponds to a state, and each column corresponds to an action. The value in each cell of the table indicates the reward that can be obtained by taking an action at a state.

One question that beginners may ask is as follows: if given the table of rewards, can we find good policies by simply selecting the actions with the greatest rewards? The answer is no. That is because these rewards are immediate rewards that can be obtained after taking an action. To determine a good policy, we must consider the total reward obtained in the long run (see Section 1.6 for more information). An action with the greatest immediate reward may not lead to the greatest total reward.

Although intuitive, the tabular representation is only able to describe deterministic reward processes. A more general approach is to use conditional probabilities $p ( r | s , a )$ to describe reward processes. For example, for state $s _ { 1 }$ , we have

$$
p (r = - 1 | s _ {1}, a _ {1}) = 1, \quad p (r \neq - 1 | s _ {1}, a _ {1}) = 0.
$$

Table 1.3: A tabular representation of the process of obtaining rewards. Here, the process is deterministic. Each cell indicates how much reward can be obtained after the agent takes an action at a given state.   

<table><tr><td></td><td>a1 (upward)</td><td>a2 (rightward)</td><td>a3 (downward)</td><td>a4 (leftward)</td><td>a5 (still)</td></tr><tr><td>s1</td><td>rBoundary</td><td>0</td><td>0</td><td>rBoundary</td><td>0</td></tr><tr><td>s2</td><td>rBoundary</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>s3</td><td>rBoundary</td><td>rBoundary</td><td>rforbidden</td><td>0</td><td>0</td></tr><tr><td>s4</td><td>0</td><td>0</td><td>rforbidden</td><td>rboundary</td><td>0</td></tr><tr><td>s5</td><td>0</td><td>rforbidden</td><td>0</td><td>0</td><td>0</td></tr><tr><td>s6</td><td>0</td><td>rboundary</td><td>rtarget</td><td>0</td><td>rforbidden</td></tr><tr><td>s7</td><td>0</td><td>0</td><td>rboundary</td><td>rboundary</td><td>rforbidden</td></tr><tr><td>s8</td><td>0</td><td>rtarget</td><td>rboundary</td><td>rforbidden</td><td>0</td></tr><tr><td>s9</td><td>rforbidden</td><td>rboundary</td><td>rboundary</td><td>0</td><td>rtarget</td></tr></table>

This indicates that, when taking $a _ { 1 }$ at $s _ { 1 }$ , the agent obtains $r = - 1$ with certainty. In this example, the reward process is deterministic. In general, it can be stochastic. For example, if a student studies hard, he or she would receive a positive reward (e.g., higher grades on exams), but the specific value of the reward may be uncertain.

# 1.6 Trajectories, returns, and episodes

![](images/302941ff56829fa0f1a844242489983604f1eb4028ed3ab20709b5b93bc6b87f.jpg)  
(a) Policy 1 and the trajectory

![](images/0bf24d89bda3f0e7719679a0826b14daa47d4c6e4e5e18febd2ecb83e52b3333.jpg)  
(b) Policy 2 and the trajectory   
Figure 1.6: Trajectories obtained by following two policies. The trajectories are indicated by red dashed lines.

A trajectory is a state-action-reward chain. For example, given the policy shown in Figure 1.6(a), the agent can move along a trajectory as follows:

$$
s _ {1} \xrightarrow [ r = 0 ]{a _ {2}} s _ {2} \xrightarrow [ r = 0 ]{a _ {3}} s _ {5} \xrightarrow [ r = 0 ]{a _ {3}} s _ {8} \xrightarrow [ r = 1 ]{a _ {2}} s _ {9}.
$$

The return of this trajectory is defined as the sum of all the rewards collected along the trajectory:

$$
\mathrm {r e t u r n} = 0 + 0 + 0 + 1 = 1. \tag {1.1}
$$

Returns are also called total rewards or cumulative rewards.

Returns can be used to evaluate policies. For example, we can evaluate the two policies in Figure 1.6 by comparing their returns. In particular, starting from $s _ { 1 }$ , the return obtained by the left policy is 1 as calculated above. For the right policy, starting from $s _ { 1 }$ , the following trajectory is generated:

$$
s _ {1} \xrightarrow [ r = 0 ]{a _ {3}} s _ {4} \xrightarrow [ r = - 1 ]{a _ {3}} s _ {7} \xrightarrow [ r = 0 ]{a _ {2}} s _ {8} \xrightarrow [ r = + 1 ]{a _ {2}} s _ {9}.
$$

The corresponding return is

$$
\operatorname {r e t u r n} = 0 - 1 + 0 + 1 = 0. \tag {1.2}
$$

The returns in (1.1) and (1.2) indicate that the left policy is better than the right one since its return is greater. This mathematical conclusion is consistent with the intuition that the right policy is worse since it passes through a forbidden cell.

A return consists of an immediate reward and future rewards. Here, the immediate reward is the reward obtained after taking an action at the initial state; the future rewards refer to the rewards obtained after leaving the initial state. It is possible that the immediate reward is negative while the future reward is positive. Thus, which actions to take should be determined by the return (i.e., the total reward) rather than the immediate reward to avoid short-sighted decisions.

The return in (1.1) is defined for a finite-length trajectory. Return can also be defined for infinitely long trajectories. For example, the trajectory in Figure 1.6 stops after reaching $s _ { 9 }$ . Since the policy is well defined for $s _ { 9 }$ , the process does not have to stop after the agent reaches $s _ { 9 }$ . We can design a policy so that the agent stays still after reaching $s _ { 9 }$ . Then, the policy would generate the following infinitely long trajectory:

$$
s _ {1} \xrightarrow [ r = 0 ]{a _ {2}} s _ {2} \xrightarrow [ r = 0 ]{a _ {3}} s _ {5} \xrightarrow [ r = 0 ]{a _ {3}} s _ {8} \xrightarrow [ r = 1 ]{a _ {2}} s _ {9} \xrightarrow [ r = 1 ]{a _ {5}} s _ {9} \xrightarrow [ r = 1 ]{a _ {5}} s _ {9} \dots
$$

The direct sum of the rewards along this trajectory is

$$
\operatorname {r e t u r n} = 0 + 0 + 0 + 1 + 1 + \dots = \infty ,
$$

which unfortunately diverges. Therefore, we must introduce the discounted return concept for infinitely long trajectories. In particular, the discounted return is the sum of the discounted rewards:

$$
\text {d i s c o u n t e d} = 0 + \gamma 0 + \gamma^ {2} 0 + \gamma^ {3} 1 + \gamma^ {4} 1 + \gamma^ {5} 1 + \dots , \tag {1.3}
$$

where $\gamma \in ( 0 , 1 )$ is called the discount rate. When $\gamma \in ( 0 , 1 )$ , the value of (1.3) can be

calculated as

$$
\text {d i s c o u n t e d} = \gamma^ {3} (1 + \gamma + \gamma^ {2} + \dots) = \gamma^ {3} \frac {1}{1 - \gamma}.
$$

The introduction of the discount rate is useful for the following reasons. First, it removes the stop criterion and allows for infinitely long trajectories. Second, the discount rate can be used to adjust the emphasis placed on near- or far-future rewards. In particular, if $\gamma$ is close to 0, then the agent places more emphasis on rewards obtained in the near future. The resulting policy would be short-sighted. If $\gamma$ is close to 1, then the agent places more emphasis on the far future rewards. The resulting policy is far-sighted and dares to take risks of obtaining negative rewards in the near future. These points will be demonstrated in Section 3.5.

One important notion that was not explicitly mentioned in the above discussion is the episode. When interacting with the environment by following a policy, the agent may stop at some terminal states. The resulting trajectory is called an episode (or a trial). If the environment or policy is stochastic, we obtain different episodes when starting from the same state. However, if everything is deterministic, we always obtain the same episode when starting from the same state.

An episode is usually assumed to be a finite trajectory. Tasks with episodes are called episodic tasks. However, some tasks may have no terminal states, meaning that the process of interacting with the environment will never end. Such tasks are called continuing tasks. In fact, we can treat episodic and continuing tasks in a unified mathematical manner by converting episodic tasks to continuing ones. To do that, we need well define the process after the agent reaches the terminal state. Specifically, after reaching the terminal state in an episodic task, the agent can continue taking actions in the following two ways.

First, if we treat the terminal state as a special state, we can specifically design its action space or state transition so that the agent stays in this state forever. Such states are called absorbing states, meaning that the agent never leaves a state once reached. For example, for the target state $s _ { 9 }$ , we can specify $A ( s _ { 9 } ) = \{ a _ { 5 } \}$ or set $\mathcal { A } ( s _ { 9 } ) = \{ a _ { 1 } , . . . , a _ { 5 } \}$ with $p ( s _ { 9 } | s _ { 9 } , a _ { i } ) = 1$ for all $i = 1 , \dots , 5$ .   
Second, if we treat the terminal state as a normal state, we can simply set its action space to the same as the other states, and the agent may leave the state and come back again. Since a positive reward of $r = 1$ can be obtained every time $s _ { 9 }$ is reached, the agent will eventually learn to stay at $s _ { 9 }$ forever to collect more rewards. Notably, when an episode is infinitely long and the reward received for staying at $s _ { 9 }$ is positive, a discount rate must be used to calculate the discounted return to avoid divergence.

In this book, we consider the second scenario where the target state is treated as a normal state whose action space is $\mathcal { A } ( s _ { 9 } ) = \{ a _ { 1 } , . . . , a _ { 5 } \}$ .

# 1.7 Markov decision processes

The previous sections of this chapter illustrated some fundamental concepts in reinforcement learning through examples. This section presents these concepts in a more formal way under the framework of Markov decision processes (MDPs).

An MDP is a general framework for describing stochastic dynamical systems. The key ingredients of an MDP are listed below.

Sets:

- State space: the set of all states, denoted as $\boldsymbol { S }$ .   
- Action space: a set of actions, denoted as $\mathcal A ( s )$ , associated with each state $s \in S$ .   
- Reward set: a set of rewards, denoted as $\mathcal { R } ( s , a )$ , associated with each state-action pair $( s , a )$ .

Model:

- State transition probability: In state $s$ , when taking action $a$ , the probability of transitioning to state $s ^ { \prime }$ i s $p ( s ^ { \prime } | s , a )$ . It holds that $\begin{array} { r } { \sum _ { s ^ { \prime } \in \mathcal { S } } p ( s ^ { \prime } | s , a ) = 1 } \end{array}$ for any $( s , a )$ .   
- Reward probability: In state $s$ , when taking action $a$ , the probability of obtaining reward $r$ is $p ( r | s , a )$ . It holds that $\begin{array} { r } { \sum _ { r \in \mathcal { R } ( s , a ) } p ( r | s , a ) = 1 } \end{array}$ for any $( s , a )$ .

 Policy: In state $s$ , the probability of choosing action $a$ is $\pi ( a | s )$ . It holds that $\textstyle \sum _ { a \in { \mathcal { A } } ( s ) } \pi ( a | s ) = 1$ for any $s \in S$ .   
Markov property: The Markov property refers to the memoryless property of a stochastic process. Mathematically, it means that

$$
p \big (s _ {t + 1} | s _ {t}, a _ {t}, s _ {t - 1}, a _ {t - 1}, \ldots , s _ {0}, a _ {0} \big) = p \big (s _ {t + 1} | s _ {t}, a _ {t} \big),
$$

$$
p (r _ {t + 1} | s _ {t}, a _ {t}, s _ {t - 1}, a _ {t - 1}, \dots , s _ {0}, a _ {0}) = p (r _ {t + 1} | s _ {t}, a _ {t}), \tag {1.4}
$$

where $t$ represents the current time step and $t + 1$ represents the next time step. Equation (1.4) indicates that the next state or reward depends merely on the current state and action and is independent of the previous ones. The Markov property is important for deriving the fundamental Bellman equation of MDPs, as shown in the next chapter.

Here, $p ( s ^ { \prime } | s , a )$ and $p ( r | s , a )$ for all $( s , a )$ are called the model or dynamics. The model can be either stationary or nonstationary (or in other words, time-invariant or time-variant). A stationary model does not change over time; a nonstationary model may vary over time. For instance, in the grid world example, if a forbidden area may pop up or disappear sometimes, the model is nonstationary. In this book, we only consider stationary models.

One may have heard about the Markov processes (MPs). What is the difference between an MDP and an MP? The answer is that, once the policy in an MDP is fixed, the MDP degenerates into an MP. For example, the grid world example in Figure 1.7 can be abstracted as a Markov process. In the literature on stochastic processes, a Markov process is also called a Markov chain if it is a discrete-time process and the number of states is finite or countable [1]. In this book, the terms “Markov process” and “Markov chain” are used interchangeably when the context is clear. Moreover, this book mainly considers finite MDPs where the numbers of states and actions are finite. This is the simplest case that should be fully understood.

![](images/4c9e5f210eed286085786fd3b2deff65a29cc596fdde33de088697779254523f.jpg)

![](images/759ed8b3b5105145793223db2645f338b8726d77e8762b3eae92e14ef3cadfaa.jpg)  
Figure 1.7: Abstraction of the grid world example as a Markov process. Here, the circles represent states and the links with arrows represent state transitions.

Finally, reinforcement learning can be described as an agent-environment interaction process. The agent is a decision-maker that can sense its state, maintain policies, and execute actions. Everything outside of the agent is regarded as the environment. In the grid world examples, the agent and environment correspond to the robot and grid world, respectively. After the agent decides to take an action, the actuator executes such a decision. Then, the state of the agent would be changed and a reward can be obtained. By using interpreters, the agent can interpret the new state and the reward. Thus, a closed loop can be formed.

# 1.8 Summary

This chapter introduced the basic concepts that will be widely used in the remainder of the book. We used intuitive grid world examples to demonstrate these concepts and then formalized them in the framework of MDPs. For more information about MDPs, readers can see [1, 2].

# 1.9 Q&A

 Q: Can we set all the rewards as negative or positive?

A: In this chapter, we mentioned that a positive reward would encourage the agent to take an action and that a negative reward would discourage the agent from taking

the action. In fact, it is the relative reward values instead of the absolute values that determine encouragement or discouragement.

More specifically, we set $r _ { \mathrm { b o u n d a r y } } = - 1$ , $r _ { \mathrm { f o r b i d d e n } } = - 1$ , $r _ { \mathrm { t a r g e t } } = + 1$ , and $r _ { \mathrm { o t h e r } } = 0$ in this chapter. We can also add a common value to all these values without changing the resulting optimal policy. For example, we can add $- 2$ to all the rewards to obtain $r _ { \mathrm { b o u n d a r y } } = - 3$ , $r _ { \mathrm { f o r b i d d e n } } = - 3$ , $r _ { \mathrm { t a r g e t } } = - 1$ , and $r _ { \mathrm { o t h e r } } = - 2$ . Although the rewards are all negative, the resulting optimal policy is unchanged. That is because optimal policies are invariant to affine transformations of the rewards. Details will be given in Chapter 3.5.

Q: Is the reward a function of the next state?

A: We mentioned that the reward $r$ depends only on $s$ and $a$ but not the next state $s ^ { \prime }$ . However, this may be counterintuitive since it is the next state that determines the reward in many cases. For example, the reward is positive when the next state is the target state. As a result, a question that naturally follows is whether a reward should depend on the next state. A mathematical rephrasing of this question is whether we should use $p ( r | s , a , s ^ { \prime } )$ where $s ^ { \prime }$ is the next state rather than $p ( r | s , a )$ . The answer is that $r$ depends on $s$ , $a$ , and $s ^ { \prime }$ . However, since $s ^ { \prime }$ also depends on $s$ and $a$ , we can equivalently write $r$ as a function of $s$ and $a$ : $\begin{array} { r } { p ( r | s , a ) = \sum _ { s ^ { \prime } } p ( r | s , a , s ^ { \prime } ) p ( s ^ { \prime } | s , a ) } \end{array}$ . In this way, the Bellman equation can be easily established as shown in Chapter 2.

# Chapter 2

# State Values and Bellman Equation

![](images/5536355880a4007993a4c82548ba2effea793be723e4b3ea6acd2d2276112c29.jpg)  
Figure 2.1: Where we are in this book.

This chapter introduces a core concept and an important tool. The core concept is the state value, which is defined as the average reward that an agent can obtain if it follows a given policy. The greater the state value is, the better the corresponding policy is. State values can be used as a metric to evaluate whether a policy is good or not. While state values are important, how can we analyze them? The answer is the Bellman equation, which is an important tool for analyzing state values. In a nutshell, the Bellman equation describes the relationships between the values of all states. By solving the Bellman equation, we can obtain the state values. This process is called policy evaluation, which is a fundamental concept in reinforcement learning. Finally, this

chapter introduces another important concept called the action value.

# 2.1 Motivating example 1: Why are returns important?

The previous chapter introduced the concept of returns. In fact, returns play a fundamental role in reinforcement learning since they can evaluate whether a policy is good or not. This is demonstrated by the following examples.

![](images/be2d0fdac6b3bcb8716779033deacf9b6e262a39f1ab8c8f7f4495db56ef047b.jpg)

![](images/9a621206226faf7ececce906888a0444bd1fc5564c5edc52acad68afb3420127.jpg)

![](images/0b3fbd0b739214f453b7ab59f051c568909af5dd563b23f638f4ab53cd4f042e.jpg)  
Figure 2.2: Examples for demonstrating the importance of returns. The three examples have different policies for $s _ { 1 }$ .

Consider the three policies shown in Figure 2.2. It can be seen that the three policies are different at $s _ { 1 }$ . Which is the best and which is the worst? Intuitively, the leftmost policy is the best because the agent starting from $s _ { 1 }$ can avoid the forbidden area. The middle policy is intuitively worse because the agent starting from $s _ { 1 }$ moves to the forbidden area. The rightmost policy is in between the others because it has a probability of 0.5 to go to the forbidden area.

While the above analysis is based on intuition, a question that immediately follows is whether we can use mathematics to describe such intuition. The answer is yes and relies on the return concept. In particular, suppose that the agent starts from $s _ { 1 }$ .

 Following the first policy, the trajectory is $s _ { 1 } \to s _ { 3 } \to s _ { 4 } \to s _ { 4 } \cdot \cdot \cdot$ . The corresponding discounted return is

$$
\begin{array}{l} \mathrm {r e t u r n} _ {1} = 0 + \gamma 1 + \gamma^ {2} 1 + \dots \\ = \gamma (1 + \gamma + \gamma^ {2} + \dots) \\ = \frac {\gamma}{1 - \gamma}, \\ \end{array}
$$

where $\gamma \in ( 0 , 1 )$ is the discount rate.

 Following the second policy, the trajectory is $s _ { 1 }  s _ { 2 }  s _ { 4 }  s _ { 4 } \cdot \cdot \cdot$ . The discounted

return is

$$
\begin{array}{l} \mathrm {r e t u r n} _ {2} = - 1 + \gamma 1 + \gamma^ {2} 1 + \dots \\ = - 1 + \gamma (1 + \gamma + \gamma^ {2} + \dots) \\ = - 1 + \frac {\gamma}{1 - \gamma}. \\ \end{array}
$$

Following the third policy, two trajectories can possibly be obtained. One is $s _ { 1 } \ $ $s _ { 3 }  s _ { 4 }  s _ { 4 } \ldots$ , and the other is $s _ { 1 }  s _ { 2 }  s _ { 4 }  s _ { 4 } \cdot \cdot \cdot$ . The probability of either of the two trajectories is 0.5. Then, the average return that can be obtained starting from $s _ { 1 }$ is

$$
\begin{array}{l} \operatorname {r e t u r n} _ {3} = 0. 5 \left(- 1 + \frac {\gamma}{1 - \gamma}\right) + 0. 5 \left(\frac {\gamma}{1 - \gamma}\right) \\ = - 0. 5 + \frac {\gamma}{1 - \gamma}. \\ \end{array}
$$

By comparing the returns of the three policies, we notice that

$$
\operatorname {r e t u r n} _ {1} > \operatorname {r e t u r n} _ {3} > \operatorname {r e t u r n} _ {2} \tag {2.1}
$$

for any value of $\gamma$ . Inequality (2.1) suggests that the first policy is the best because its return is the greatest, and the second policy is the worst because its return is the smallest. This mathematical conclusion is consistent with the aforementioned intuition: the first policy is the best since it can avoid entering the forbidden area, and the second policy is the worst because it leads to the forbidden area.

The above examples demonstrate that returns can be used to evaluate policies: a policy is better if the return obtained by following that policy is greater. Finally, it is notable that return3 does not strictly comply with the definition of returns because it is more like an expected value. It will become clear later that return $^ 3$ is actually a state value.

# 2.2 Motivating example 2: How to calculate returns?

While we have demonstrated the importance of returns, a question that immediately follows is how to calculate the returns when following a given policy.

There are two ways to calculate returns.

The first is simply by definition: a return equals the discounted sum of all the rewards collected along a trajectory. Consider the example in Figure 2.3. Let $v _ { i }$ denote the return obtained by starting from $s _ { i }$ for $i = 1 , 2 , 3 , 4$ . Then, the returns obtained when

![](images/930a90205e20516433d6518992281c2e011da1cdf72b9b354678eeba72f12004.jpg)  
Figure 2.3: An example for demonstrating how to calculate returns. There are no target or forbidden cells in this example.

starting from the four states in Figure 2.3 can be calculated as

$$
v _ {1} = r _ {1} + \gamma r _ {2} + \gamma^ {2} r _ {3} + \dots ,
$$

$$
v _ {2} = r _ {2} + \gamma r _ {3} + \gamma^ {2} r _ {4} + \dots , \tag {2.2}
$$

$$
v _ {3} = r _ {3} + \gamma r _ {4} + \gamma^ {2} r _ {1} + \dots ,
$$

$$
v _ {4} = r _ {4} + \gamma r _ {1} + \gamma^ {2} r _ {2} + \dots .
$$

The second way, which is more important, is based on the idea of bootstrapping. By observing the expressions of the returns in (2.2), we can rewrite them as

$$
v _ {1} = r _ {1} + \gamma (r _ {2} + \gamma r _ {3} + \dots) = r _ {1} + \gamma v _ {2},
$$

$$
v _ {2} = r _ {2} + \gamma \left(r _ {3} + \gamma r _ {4} + \dots\right) = r _ {2} + \gamma v _ {3}, \tag {2.3}
$$

$$
v _ {3} = r _ {3} + \gamma (r _ {4} + \gamma r _ {1} + \dots) = r _ {3} + \gamma v _ {4},
$$

$$
v _ {4} = r _ {4} + \gamma (r _ {1} + \gamma r _ {2} + \dots) = r _ {4} + \gamma v _ {1}.
$$

The above equations indicate an interesting phenomenon that the values of the returns rely on each other. More specifically, $v _ { 1 }$ relies on $v _ { 2 }$ , $v _ { 2 }$ relies on $v _ { 3 }$ , $v _ { 3 }$ relies on $\boldsymbol { v } _ { 4 }$ , and $\boldsymbol { v } _ { 4 }$ relies on $v _ { 1 }$ . This reflects the idea of bootstrapping, which is to obtain the values of some quantities from themselves.

At first glance, bootstrapping is an endless loop because the calculation of an unknown value relies on another unknown value. In fact, bootstrapping is easier to understand if we view it from a mathematical perspective. In particular, the equations in (2.3) can be reformed into a linear matrix-vector equation:

$$
\underbrace {\left[ \begin{array}{c} v _ {1} \\ v _ {2} \\ v _ {3} \\ v _ {4} \end{array} \right]} _ {v} = \left[ \begin{array}{c} r _ {1} \\ r _ {2} \\ r _ {3} \\ r _ {4} \end{array} \right] + \left[ \begin{array}{c} \gamma v _ {2} \\ \gamma v _ {3} \\ \gamma v _ {4} \\ \gamma v _ {1} \end{array} \right] = \underbrace {\left[ \begin{array}{c} r _ {1} \\ r _ {2} \\ r _ {3} \\ r _ {4} \end{array} \right]} _ {r} + \gamma \underbrace {\left[ \begin{array}{c c c c} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 \end{array} \right]} _ {P} \underbrace {\left[ \begin{array}{c} v _ {1} \\ v _ {2} \\ v _ {3} \\ v _ {4} \end{array} \right]} _ {v},
$$

which can be written compactly as

$$
v = r + \gamma P v.
$$

Thus, the value of $\boldsymbol { v }$ can be calculated easily as $v = ( I - \gamma P ) ^ { - 1 } r$ , where $I$ is the identity matrix with appropriate dimensions. One may ask whether $I - \gamma P$ is always invertible. The answer is yes and explained in Section 2.7.1.

In fact, (2.3) is the Bellman equation for this simple example. Although it is simple, (2.3) demonstrates the core idea of the Bellman equation: the return obtained by starting from one state depends on those obtained when starting from other states. The idea of bootstrapping and the Bellman equation for general scenarios will be formalized in the following sections.

# 2.3 State values

We mentioned that returns can be used to evaluate policies. However, they are inapplicable to stochastic systems because starting from one state may lead to different returns. Motivated by this problem, we introduce the concept of state value in this section.

First, we need to introduce some necessary notations. Consider a sequence of time steps $t = 0 , 1 , 2 , \ldots .$ At time $t$ , the agent is in state $S _ { t }$ , and the action taken following a policy $\pi$ is $A _ { t }$ . The next state is $S _ { t + 1 }$ , and the immediate reward obtained is $R _ { t + 1 }$ . This process can be expressed concisely as

$$
S _ {t} \xrightarrow {A _ {t}} S _ {t + 1}, R _ {t + 1}.
$$

Note that $S _ { t } , S _ { t + 1 } , A _ { t } , R _ { t + 1 }$ are all random variables. Moreover, $S _ { t } , S _ { t + 1 } \in S$ , $A _ { t } \in \mathcal { A } ( S _ { t } )$ , and $R _ { t + 1 } \in \mathcal { R } ( S _ { t } , A _ { t } )$ .

Starting from $t$ , we can obtain a state-action-reward trajectory:

$$
S _ {t} \xrightarrow {A _ {t}} S _ {t + 1}, R _ {t + 1} \xrightarrow {A _ {t + 1}} S _ {t + 2}, R _ {t + 2} \xrightarrow {A _ {t + 2}} S _ {t + 3}, R _ {t + 3} \dots .
$$

By definition, the discounted return along the trajectory is

$$
G _ {t} \doteq R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \ldots ,
$$

where $\gamma \in ( 0 , 1 )$ is the discount rate. Note that $G _ { t }$ is a random variable since $R _ { t + 1 } , R _ { t + 2 } , . . .$ are all random variables.

Since $G _ { t }$ is a random variable, we can calculate its expected value (also called the expectation or mean):

$$
v _ {\pi} (s) \doteq \mathbb {E} [ G _ {t} | S _ {t} = s ].
$$

Here, $v _ { \pi } ( s )$ is called the state-value function or simply the state value of $s$ . Some important remarks are given below.

$v _ { \pi } ( s )$ depends on $s$ . This is because its definition is a conditional expectation with the condition that the agent starts from $S _ { t } = s$ .   
 $v _ { \pi } ( s )$ depends on $\pi$ . This is because the trajectories are generated by following the policy $\pi$ . For a different policy, the state value may be different.   
 $v _ { \pi } ( s )$ does not depend on $t$ . If the agent moves in the state space, $t$ represents the current time step. The value of $v _ { \pi } ( s )$ is determined once the policy is given.

The relationship between state values and returns is further clarified as follows. When both the policy and the system model are deterministic, starting from a state always leads to the same trajectory. In this case, the return obtained starting from a state is equal to the value of that state. By contrast, when either the policy or the system model is stochastic, starting from the same state may generate different trajectories. In this case, the returns of different trajectories are different, and the state value is the mean of these returns.

Although returns can be used to evaluate policies as shown in Section 2.1, it is more formal to use state values to evaluate policies: policies that generate greater state values are better. Therefore, state values constitute a core concept in reinforcement learning. While state values are important, a question that immediately follows is how to calculate them. This question is answered in the next section.

# 2.4 Bellman equation

We now introduce the Bellman equation, a mathematical tool for analyzing state values. In a nutshell, the Bellman equation is a set of linear equations that describe the relationships between the values of all the states.

We next derive the Bellman equation. First, note that $G _ { t }$ can be rewritten as

$$
\begin{array}{l} G _ {t} = R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \ldots \\ = R _ {t + 1} + \gamma (R _ {t + 2} + \gamma R _ {t + 3} + \ldots) \\ = R _ {t + 1} + \gamma G _ {t + 1}, \\ \end{array}
$$

where $G _ { t + 1 } = R _ { t + 2 } + \gamma R _ { t + 3 } + . . . .$ This equation establishes the relationship between $G _ { t }$ and $G _ { t + 1 }$ . Then, the state value can be written as

$$
\begin{array}{l} v _ {\pi} (s) = \mathbb {E} [ G _ {t} | S _ {t} = s ] \\ = \mathbb {E} \bigl [ R _ {t + 1} + \gamma G _ {t + 1} | S _ {t} = s \bigr ] \\ = \mathbb {E} \left[ R _ {t + 1} \mid S _ {t} = s \right] + \gamma \mathbb {E} \left[ G _ {t + 1} \mid S _ {t} = s \right]. \tag {2.4} \\ \end{array}
$$

The two terms in (2.4) are analyzed below.

The first term, $\mathbb { E } [ R _ { t + 1 } | S _ { t } = s ]$ , is the expectation of the immediate rewards. By using the law of total expectation (Appendix A), it can be calculated as

$$
\begin{array}{l} \mathbb {E} [ R _ {t + 1} | S _ {t} = s ] = \sum_ {a \in \mathcal {A}} \pi (a | s) \mathbb {E} [ R _ {t + 1} | S _ {t} = s, A _ {t} = a ] \\ = \sum_ {a \in \mathcal {A}} \pi (a | s) \sum_ {r \in \mathcal {R}} p (r | s, a) r. \tag {2.5} \\ \end{array}
$$

Here, $\mathcal { A }$ and $\mathcal { R }$ are the sets of possible actions and rewards, respectively. It should be noted that $\mathcal { A }$ may be different for different states. In this case, $\boldsymbol { A }$ should be written as $\boldsymbol { \mathcal { A } } ( \boldsymbol { s } )$ . Similarly, $_ { \mathcal { R } }$ may also depend on $( s , a )$ . We drop the dependence on $s$ or $( s , a )$ for the sake of simplicity in this book. Nevertheless, the conclusions are still valid in the presence of dependence.

The second term, $\mathbb { E } [ G _ { t + 1 } | S _ { t } = s ]$ , is the expectation of the future rewards. It can be calculated as

$$
\begin{array}{l} \mathbb {E} [ G _ {t + 1} | S _ {t} = s ] = \sum_ {s ^ {\prime} \in \mathcal {S}} \mathbb {E} [ G _ {t + 1} | S _ {t} = s, S _ {t + 1} = s ^ {\prime} ] p (s ^ {\prime} | s) \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} \mathbb {E} \left[ G _ {t + 1} \mid S _ {t + 1} = s ^ {\prime} \right] p \left(s ^ {\prime} \mid s\right) \quad (\text {d u e t o t h e M a r k o v p r o p e r t y}) \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} v _ {\pi} (s ^ {\prime}) p (s ^ {\prime} | s) \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} v _ {\pi} \left(s ^ {\prime}\right) \sum_ {a \in \mathcal {A}} p \left(s ^ {\prime} \mid s, a\right) \pi (a \mid s). \tag {2.6} \\ \end{array}
$$

The above derivation uses the fact that $\mathbb { E } [ G _ { t + 1 } | S _ { t } = s , S _ { t + 1 } = s ^ { \prime } ] = \mathbb { E } [ G _ { t + 1 } | S _ { t + 1 } = s ^ { \prime } ]$ , which is due to the Markov property that the future rewards depend merely on the present state rather than the previous ones.

Substituting (2.5)-(2.6) into (2.4) yields

$$
\begin{array}{l} v _ {\pi} (s) = \mathbb {E} [ R _ {t + 1} | S _ {t} = s ] + \gamma \mathbb {E} [ G _ {t + 1} | S _ {t} = s ], \\ = \underbrace{\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{r\in\mathcal{R}}p(r|s,a)r}_{\text{mean of immediate rewards}} + \underbrace{\gamma\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)v_{\pi}(s^{\prime})}_{\text{mean of future rewards}} \\ = \sum_ {a \in \mathcal {A}} \pi (a | s) \left[ \sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi} \left(s ^ {\prime}\right) \right], \quad \text {f o r a l l} s \in \mathcal {S}. \tag {2.7} \\ \end{array}
$$

This equation is the Bellman equation, which characterizes the relationships of state values. It is a fundamental tool for designing and analyzing reinforcement learning algorithms.

The Bellman equation seems complex at first glance. In fact, it has a clear structure. Some remarks are given below.

$v _ { \pi } ( s )$ and $v _ { \pi } ( s ^ { \prime } )$ are unknown state values to be calculated. It may be confusing to beginners how to calculate the unknown $v _ { \pi } ( s )$ given that it relies on another unknown $v _ { \pi } ( s ^ { \prime } )$ . It must be noted that the Bellman equation refers to a set of linear equations for all states rather than a single equation. If we put these equations together, it becomes clear how to calculate all the state values. Details will be given in Section 2.7.   
 $\pi ( a | s )$ is a given policy. Since state values can be used to evaluate a policy, solving the state values from the Bellman equation is a policy evaluation process, which is an important process in many reinforcement learning algorithms, as we will see later in the book.   
$p ( r | s , a )$ and $p ( s ^ { \prime } | s , a )$ represent the system model. We will first show how to calculate the state values with this model in Section 2.7, and then show how to do that without the model by using model-free algorithms later in this book.

In addition to the expression in (2.7), readers may also encounter other expressions of the Bellman equation in the literature. We next introduce two equivalent expressions.

First, it follows from the law of total probability that

$$
p (s ^ {\prime} | s, a) = \sum_ {r \in \mathcal {R}} p (s ^ {\prime}, r | s, a),
$$

$$
p (r | s, a) = \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime}, r | s, a).
$$

Then, equation (2.7) can be rewritten as

$$
v _ {\pi} (s) = \sum_ {a \in \mathcal {A}} \pi (a | s) \sum_ {s ^ {\prime} \in \mathcal {S}} \sum_ {r \in \mathcal {R}} p (s ^ {\prime}, r | s, a) [ r + \gamma v _ {\pi} (s ^ {\prime}) ].
$$

This is the expression used in [3].

Second, the reward $r$ may depend solely on the next state $s ^ { \prime }$ in some problems. As a result, we can write the reward as $r ( s ^ { \prime } )$ and hence $p ( r ( s ^ { \prime } ) | s , a ) = p ( s ^ { \prime } | s , a )$ , substituting which into (2.7) gives

$$
v _ {\pi} (s) = \sum_ {a \in \mathcal {A}} \pi (a | s) \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) [ r (s ^ {\prime}) + \gamma v _ {\pi} (s ^ {\prime}) ].
$$

# 2.5 Examples for illustrating the Bellman equation

We next use two examples to demonstrate how to write out the Bellman equation and calculate the state values step by step. Readers are advised to carefully go through the examples to gain a better understanding of the Bellman equation.

![](images/e72ead0960a45949cd7ab71affd7e26152630bdcafc72e2424e21ce0ed403ae7.jpg)  
Figure 2.4: An example for demonstrating the Bellman equation. The policy in this example is deterministic.

Consider the first example shown in Figure 2.4, where the policy is deterministic. We next write out the Bellman equation and then solve the state values from it.

First, consider state $s _ { 1 }$ . Under the policy, the probabilities of taking the actions are $\pi ( a \ = \ a _ { 3 } \vert s _ { 1 } ) \ = \ 1$ and $\pi ( a \ne a _ { 3 } | s _ { 1 } ) = 0$ . The state transition probabilities are $p ( s ^ { \prime } = s _ { 3 } \vert s _ { 1 } , a _ { 3 } ) = 1$ and $p ( s ^ { \prime } \ne s _ { 3 } \vert s _ { 1 } , a _ { 3 } ) = 0$ . The reward probabilities are $p ( r = 0 | s _ { 1 } , a _ { 3 } ) = 1$ and $p ( r \neq 0 | s _ { 1 } , a _ { 3 } ) = 0$ . Substituting these values into (2.7) gives

$$
v _ {\pi} (s _ {1}) = 0 + \gamma v _ {\pi} (s _ {3}).
$$

Interestingly, although the expression of the Bellman equation in (2.7) seems complex, the expression for this specific state is very simple.

Similarly, it can be obtained that

$$
v _ {\pi} (s _ {2}) = 1 + \gamma v _ {\pi} (s _ {4}),
$$

$$
v _ {\pi} (s _ {3}) = 1 + \gamma v _ {\pi} (s _ {4}),
$$

$$
v _ {\pi} (s _ {4}) = 1 + \gamma v _ {\pi} (s _ {4}).
$$

We can solve the state values from these equations. Since the equations are simple, we can manually solve them. More complicated equations can be solved by the algorithms presented in Section 2.7. Here, the state values can be solved as

$$
v _ {\pi} (s _ {4}) = \frac {1}{1 - \gamma},
$$

$$
v _ {\pi} (s _ {3}) = \frac {1}{1 - \gamma},
$$

$$
v _ {\pi} (s _ {2}) = \frac {1}{1 - \gamma},
$$

$$
v _ {\pi} (s _ {1}) = \frac {\gamma}{1 - \gamma}.
$$

Furthermore, if we set $\gamma = 0 . 9$ , then

$$
v _ {\pi} (s _ {4}) = \frac {1}{1 - 0 . 9} = 1 0,
$$

$$
v _ {\pi} (s _ {3}) = \frac {1}{1 - 0 . 9} = 1 0,
$$

$$
v _ {\pi} (s _ {2}) = \frac {1}{1 - 0 . 9} = 1 0,
$$

$$
v _ {\pi} (s _ {1}) = \frac {0 . 9}{1 - 0 . 9} = 9.
$$

![](images/70b08d65130b6234a035c8ef606c04b5113401234aa0b7d4048acea182af7c43.jpg)  
Figure 2.5: An example for demonstrating the Bellman equation. The policy in this example is stochastic.

 Consider the second example shown in Figure 2.5, where the policy is stochastic. We next write out the Bellman equation and then solve the state values from it.

In state $s _ { 1 }$ , the probabilities of going right and down equal 0.5. Mathematically, we have $\pi ( a = a _ { 2 } | s _ { 1 } ) = 0 . 5$ and $\pi ( a = a _ { 3 } | s _ { 1 } ) = 0 . 5$ . The state transition probability is deterministic since $p ( s ^ { \prime } = s _ { 3 } \vert s _ { 1 } , a _ { 3 } ) = 1$ and $p ( s ^ { \prime } = s _ { 2 } \vert s _ { 1 } , a _ { 2 } ) = 1$ . The reward probability is also deterministic since $p ( r = 0 | s _ { 1 } , a _ { 3 } ) = 1$ and $p ( r = - 1 | s _ { 1 } , a _ { 2 } ) = 1$ . Substituting these values into (2.7) gives

$$
v _ {\pi} (s _ {1}) = 0. 5 [ 0 + \gamma v _ {\pi} (s _ {3}) ] + 0. 5 [ - 1 + \gamma v _ {\pi} (s _ {2}) ].
$$

Similarly, it can be obtained that

$$
v _ {\pi} (s _ {2}) = 1 + \gamma v _ {\pi} (s _ {4}),
$$

$$
v _ {\pi} \left(s _ {3}\right) = 1 + \gamma v _ {\pi} \left(s _ {4}\right),
$$

$$
v _ {\pi} (s _ {4}) = 1 + \gamma v _ {\pi} (s _ {4}).
$$

The state values can be solved from the above equations. Since the equations are

simple, we can solve the state values manually and obtain

$$
v _ {\pi} (s _ {4}) = \frac {1}{1 - \gamma},
$$

$$
v _ {\pi} (s _ {3}) = \frac {1}{1 - \gamma},
$$

$$
v _ {\pi} (s _ {2}) = \frac {1}{1 - \gamma},
$$

$$
\begin{array}{l} v _ {\pi} (s _ {1}) = 0. 5 [ 0 + \gamma v _ {\pi} (s _ {3}) ] + 0. 5 [ - 1 + \gamma v _ {\pi} (s _ {2}) ], \\ = - 0. 5 + \frac {\gamma}{1 - \gamma}. \\ \end{array}
$$

Furthermore, if we set $\gamma = 0 . 9$ , then

$$
v _ {\pi} \left(s _ {4}\right) = 1 0,
$$

$$
v _ {\pi} (s _ {3}) = 1 0,
$$

$$
v _ {\pi} (s _ {2}) = 1 0,
$$

$$
v _ {\pi} (s _ {1}) = - 0. 5 + 9 = 8. 5.
$$

If we compare the state values of the two policies in the above examples, it can be seen that

$$
v _ {\pi_ {1}} (s _ {i}) \geq v _ {\pi_ {2}} (s _ {i}), \quad i = 1, 2, 3, 4,
$$

which indicates that the policy in Figure 2.4 is better because it has greater state values. This mathematical conclusion is consistent with the intuition that the first policy is better because it can avoid entering the forbidden area when the agent starts from $s _ { 1 }$ . As a result, the above two examples demonstrate that state values can be used to evaluate policies.

# 2.6 Matrix-vector form of the Bellman equation

The Bellman equation in (2.7) is in an elementwise form. Since it is valid for every state, we can combine all these equations and write them concisely in a matrix-vector form, which will be frequently used to analyze the Bellman equation.

To derive the matrix-vector form, we first rewrite the Bellman equation in (2.7) as

$$
v _ {\pi} (s) = r _ {\pi} (s) + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p _ {\pi} \left(s ^ {\prime} \mid s\right) v _ {\pi} \left(s ^ {\prime}\right), \tag {2.8}
$$

where

$$
r _ {\pi} (s) \doteq \sum_ {a \in \mathcal {A}} \pi (a | s) \sum_ {r \in \mathcal {R}} p (r | s, a) r,
$$

$$
p _ {\pi} (s ^ {\prime} | s) \doteq \sum_ {a \in \mathcal {A}} \pi (a | s) p (s ^ {\prime} | s, a).
$$

Here, $r _ { \pi } ( s )$ denotes the mean of the immediate rewards, and $p _ { \pi } ( s ^ { \prime } | s )$ is the probability of transitioning from $s$ to $s ^ { \prime }$ under policy $\pi$ .

Suppose that the states are indexed as $s _ { i }$ with $i = 1 , \ldots , n$ , where $n = | S |$ . For state $s _ { i }$ , (2.8) can be written as

$$
v _ {\pi} (s _ {i}) = r _ {\pi} (s _ {i}) + \gamma \sum_ {s _ {j} \in \mathcal {S}} p _ {\pi} (s _ {j} | s _ {i}) v _ {\pi} (s _ {j}). \tag {2.9}
$$

Let $v _ { \pi } = [ v _ { \pi } ( s _ { 1 } ) , \ldots , v _ { \pi } ( s _ { n } ) ] ^ { T } \in \mathbb { R } ^ { n }$ , $r _ { \pi } = [ r _ { \pi } ( s _ { 1 } ) , \ldots , r _ { \pi } ( s _ { n } ) ] ^ { \scriptscriptstyle T } \in \mathbb { R } ^ { n }$ , and $P _ { \pi } \in \mathbb { R } ^ { n \times n }$ with $[ P _ { \pi } ] _ { i j } = p _ { \pi } ( s _ { j } | s _ { i } )$ . Then, (2.9) can be written in the following matrix-vector form:

$$
v _ {\pi} = r _ {\pi} + \gamma P _ {\pi} v _ {\pi}, \tag {2.10}
$$

where $v _ { \pi }$ is the unknown to be solved, and $r _ { \pi } , P _ { \pi }$ are known.

The matrix $P _ { \pi }$ has some interesting properties. First, it is a nonnegative matrix, meaning that all its elements are equal to or greater than zero. This property is denoted as $P _ { \pi } \geq 0$ , where 0 denotes a zero matrix with appropriate dimensions. In this book, $\geq$ or $\leq$ represents an elementwise comparison operation. Second, $P _ { \pi }$ is a stochastic matrix, meaning that the sum of the values in every row is equal to one. This property is denoted as $P _ { \pi } { \bf 1 } = { \bf 1 }$ , where $\mathbf { 1 } = [ 1 , \ldots , 1 ] ^ { T }$ has appropriate dimensions.

Consider the example shown in Figure 2.6. The matrix-vector form of the Bellman equation is

$$
\underbrace {\left[ \begin{array}{l} v _ {\pi} (s _ {1}) \\ v _ {\pi} (s _ {2}) \\ v _ {\pi} (s _ {3}) \\ v _ {\pi} (s _ {4}) \end{array} \right]} _ {v _ {\pi}} = \underbrace {\left[ \begin{array}{l} r _ {\pi} (s _ {1}) \\ r _ {\pi} (s _ {2}) \\ r _ {\pi} (s _ {3}) \\ r _ {\pi} (s _ {4}) \end{array} \right]} _ {r _ {\pi}} + \gamma \underbrace {\left[ \begin{array}{l l l l} p _ {\pi} (s _ {1} | s _ {1}) & p _ {\pi} (s _ {2} | s _ {1}) & p _ {\pi} (s _ {3} | s _ {1}) & p _ {\pi} (s _ {4} | s _ {1}) \\ p _ {\pi} (s _ {1} | s _ {2}) & p _ {\pi} (s _ {2} | s _ {2}) & p _ {\pi} (s _ {3} | s _ {2}) & p _ {\pi} (s _ {4} | s _ {2}) \\ p _ {\pi} (s _ {1} | s _ {3}) & p _ {\pi} (s _ {2} | s _ {3}) & p _ {\pi} (s _ {3} | s _ {3}) & p _ {\pi} (s _ {4} | s _ {3}) \\ p _ {\pi} (s _ {1} | s _ {4}) & p _ {\pi} (s _ {2} | s _ {4}) & p _ {\pi} (s _ {3} | s _ {4}) & p _ {\pi} (s _ {4} | s _ {4}) \end{array} \right]} _ {P _ {\pi}} \underbrace {\left[ \begin{array}{l} v _ {\pi} (s _ {1}) \\ v _ {\pi} (s _ {2}) \\ v _ {\pi} (s _ {3}) \\ v _ {\pi} (s _ {4}) \end{array} \right]} _ {v _ {\pi}}.
$$

Substituting the specific values into the above equation gives

$$
\left[ \begin{array}{c} v _ {\pi} (s _ {1}) \\ v _ {\pi} (s _ {2}) \\ v _ {\pi} (s _ {3}) \\ v _ {\pi} (s _ {4}) \end{array} \right] = \left[ \begin{array}{c} 0. 5 (0) + 0. 5 (- 1) \\ 1 \\ 1 \\ 1 \end{array} \right] + \gamma \left[ \begin{array}{c c c c} 0 & 0. 5 & 0. 5 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{array} \right] \left[ \begin{array}{c} v _ {\pi} (s _ {1}) \\ v _ {\pi} (s _ {2}) \\ v _ {\pi} (s _ {3}) \\ v _ {\pi} (s _ {4}) \end{array} \right].
$$

It can be seen that $P _ { \pi }$ satisfies $P _ { \pi } { \bf 1 } = { \bf 1 }$ .

![](images/2e117e65fa09cb72643d9c304ad25f8419e90a04f0166a9ed968dd5df6ce78ca.jpg)  
Figure 2.6: An example for demonstrating the matrix-vector form of the Bellman equation.

# 2.7 Solving state values from the Bellman equation

Calculating the state values of a given policy is a fundamental problem in reinforcement learning. This problem is often referred to as policy evaluation. In this section, we present two methods for calculating state values from the Bellman equation.

# 2.7.1 Closed-form solution

Since $v _ { \pi } = r _ { \pi } + \gamma P _ { \pi } v _ { \pi }$ is a simple linear equation, its closed-form solution can be easily obtained as

$$
v _ {\pi} = \left(I - \gamma P _ {\pi}\right) ^ {- 1} r _ {\pi}.
$$

Some properties of $( I - \gamma P _ { \pi } ) ^ { - 1 }$ are given below.

$I - \gamma P _ { \pi }$ is invertible. The proof is as follows. According to the Gershgorin circle theorem [4], every eigenvalue of $I - \gamma P _ { \pi }$ lies within at least one of the Gershgorin circles. The $i$ th Gershgorin circle has a center at $[ I - \gamma P _ { \pi } ] _ { i i } = 1 - \gamma p _ { \pi } ( s _ { i } | s _ { i } )$ and a radius equal to $\begin{array} { r } { \sum _ { j \neq i } [ I - \gamma P _ { \pi } ] _ { i j } = - \sum _ { j \neq i } \gamma p _ { \pi } ( s _ { j } | s _ { i } ) } \end{array}$ . Since $\gamma < 1$ , we know that the radius is less than the magnitude of the center: $\begin{array} { r } { \sum _ { j \neq i } \gamma p _ { \pi } \big ( s _ { j } | s _ { i } \big ) < 1 - \gamma p _ { \pi } \big ( s _ { i } | s _ { i } \big ) } \end{array}$ . Therefore, all Gershgorin circles do not encircle the origin, and hence no eigenvalue of $I - \gamma P _ { \pi }$ is zero.   
 $( I - \gamma P _ { \pi } ) ^ { - 1 } \geq I$ , meaning that every element of $( I - \gamma P _ { \pi } ) ^ { - 1 }$ is nonnegative and, more specifically, no less than that of the identity matrix. This is because $P _ { \pi }$ has nonnegative entries, and hence $( I - \gamma P _ { \pi } ) ^ { - 1 } = I + \gamma P _ { \pi } + \gamma ^ { 2 } P _ { \pi } ^ { 2 } + \cdot \cdot \cdot \geq I \geq 0$ .   
 For any vector $r \geq 0$ , it holds that $( I - \gamma P _ { \pi } ) ^ { - 1 } r \ge r \ge 0$ . This property follows from the second property because $[ ( I - \gamma P _ { \pi } ) ^ { - 1 } - I ] r \geq 0$ . As a consequence, if $r _ { 1 } \geq r _ { 2 }$ , we have $( I - \gamma P _ { \pi } ) ^ { - 1 } r _ { 1 } \geq ( I - \gamma P _ { \pi } ) ^ { - 1 } r _ { 2 }$ .

# 2.7.2 Iterative solution

Although the closed-form solution is useful for theoretical analysis purposes, it is not applicable in practice because it involves a matrix inversion operation, which still needs to be calculated by other numerical algorithms. In fact, we can directly solve the Bellman equation using the following iterative algorithm:

$$
v _ {k + 1} = r _ {\pi} + \gamma P _ {\pi} v _ {k}, \quad k = 0, 1, 2, \dots . \tag {2.11}
$$

This algorithm generates a sequence of values $\{ v _ { 0 } , v _ { 1 } , v _ { 2 } , \ldots \}$ , where $v _ { 0 } \in \mathbb { R } ^ { n }$ is an initial guess of $v _ { \pi }$ . It holds that

$$
v _ {k} \rightarrow v _ {\pi} = \left(I - \gamma P _ {\pi}\right) ^ {- 1} r _ {\pi}, \quad \text {a s} k \rightarrow \infty . \tag {2.12}
$$

Interested readers may see the proof in Box 2.1.

# Box 2.1: Convergence proof of (2.12)

Define the error as $\delta _ { k } = v _ { k } - v _ { \pi }$ . We only need to show that $\delta _ { k } \to 0$ . Substituting $v _ { k + 1 } = \delta _ { k + 1 } + v _ { \pi }$ and $v _ { k } = \delta _ { k } + v _ { \pi }$ into $v _ { k + 1 } = r _ { \pi } + \gamma P _ { \pi } v _ { k }$ gives

$$
\delta_ {k + 1} + v _ {\pi} = r _ {\pi} + \gamma P _ {\pi} (\delta_ {k} + v _ {\pi}),
$$

which can be rewritten as

$$
\begin{array}{l} \delta_ {k + 1} = - v _ {\pi} + r _ {\pi} + \gamma P _ {\pi} \delta_ {k} + \gamma P _ {\pi} v _ {\pi}, \\ = \gamma P _ {\pi} \delta_ {k} - v _ {\pi} + \left(r _ {\pi} + \gamma P _ {\pi} v _ {\pi}\right), \\ = \gamma P _ {\pi} \delta_ {k}. \\ \end{array}
$$

As a result,

$$
\delta_ {k + 1} = \gamma P _ {\pi} \delta_ {k} = \gamma^ {2} P _ {\pi} ^ {2} \delta_ {k - 1} = \dots = \gamma^ {k + 1} P _ {\pi} ^ {k + 1} \delta_ {0}.
$$

Since every entry of $P _ { \pi }$ is nonnegative and no greater than one, we have that $0 \leq$ $P _ { \pi } ^ { k } \leq 1$ for any $k$ . That is, every entry of $P _ { \pi } ^ { k }$ is no greater than 1. On the other hand, since $\gamma < 1$ , we know that $\gamma ^ { k }  0$ , and hence $\delta _ { k + 1 } = \gamma ^ { k + 1 } P _ { \pi } ^ { k + 1 } \delta _ { 0 } \to 0$ as $k  \infty$ .

# 2.7.3 Illustrative examples

We next apply the algorithm in (2.11) to solve the state values of some examples.

The examples are shown in Figure 2.7. The orange cells represent forbidden areas. The blue cell represents the target area. The reward settings are $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1$

![](images/c50e1a61d8afdc3a625bdf9b2c8d71592d2faaf51a21336c28e620dff8c8efae.jpg)

![](images/0797b2403e2524e770bc9cbd7c4f85b55eaa93f0f23df69ca76c2afd11cd5e8c.jpg)  
(a) Two “good” policies and their state values. The state values of the two policies are the same, but the two policies are different at the top two states in the fourth column.

![](images/77151f67a65d0245a519f090489abfdf64ce17dd98f6c879346babbd5a9a3081.jpg)

Figure 2.7: Examples of policies and their corresponding state values.   
![](images/ebf83db71a25e506b72457caaad911c74d451f564d99ab010a03228c0939ea9c.jpg)  
(b) Two “bad” policies and their state values. The state values are smaller than those of the “good” policies.

and $r _ { \mathrm { t a r g e t } } = 1$ . Here, the discount rate is $\gamma = 0 . 9$ .

Figure 2.7(a) shows two “good” policies and their corresponding state values obtained by (2.11). The two policies have the same state values but differ at the top two states in the fourth column. Therefore, we know that different policies may have the same state values.

Figure 2.7(b) shows two “bad” policies and their corresponding state values. These two policies are bad because the actions of many states are intuitively unreasonable. Such intuition is supported by the obtained state values. As can be seen, the state values of these two policies are negative and much smaller than those of the good policies in Figure 2.7(a).

# 2.8 From state value to action value

While we have been discussing state values thus far in this chapter, we now turn to the action value, which indicates the “value” of taking an action at a state. While the concept of action value is important, the reason why it is introduced in the last section of this chapter is that it heavily relies on the concept of state values. It is important to understand state values well first before studying action values.

The action value of a state-action pair $( s , a )$ is defined as

$$
q _ {\pi} (s, a) \doteq \mathbb {E} [ G _ {t} | S _ {t} = s, A _ {t} = a ].
$$

As can be seen, the action value is defined as the expected return that can be obtained after taking an action at a state. It must be noted that $q _ { \pi } ( s , a )$ depends on a state-action pair $( s , a )$ rather than an action alone. It may be more rigorous to call this value a state-action value, but it is conventionally called an action value for simplicity.

What is the relationship between action values and state values?

 First, it follows from the properties of conditional expectation that

$$
\underbrace {\mathbb {E} [ G _ {t} | S _ {t} = s ]} _ {v _ {\pi} (s)} = \sum_ {a \in \mathcal {A}} \underbrace {\mathbb {E} [ G _ {t} | S _ {t} = s , A _ {t} = a ]} _ {q _ {\pi} (s, a)} \pi (a | s).
$$

It then follows that

$$
v _ {\pi} (s) = \sum_ {a \in \mathcal {A}} \pi (a | s) q _ {\pi} (s, a). \tag {2.13}
$$

As a result, a state value is the expectation of the action values associated with that state.

Second, since the state value is given by

$$
v _ {\pi} (s) = \sum_ {a \in \mathcal {A}} \pi (a | s) \Big [ \sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) v _ {\pi} (s ^ {\prime}) \Big ],
$$

comparing it with (2.13) leads to

$$
q _ {\pi} (s, a) = \sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi} \left(s ^ {\prime}\right). \tag {2.14}
$$

It can be seen that the action value consists of two terms. The first term is the mean of the immediate rewards, and the second term is the mean of the future rewards.

Both (2.13) and (2.14) describe the relationship between state values and action values. They are the two sides of the same coin: (2.13) shows how to obtain state values from action values, whereas (2.14) shows how to obtain action values from state values.

# 2.8.1 Illustrative examples

![](images/78e60f5e19730db888d5e25e0841ecc81e5fa859e7b355e2b57be59f26d90a41.jpg)  
Figure 2.8: An example for demonstrating the process of calculating action values.

We next present an example to illustrate the process of calculating action values and discuss a common mistake that beginners may make.

Consider the stochastic policy shown in Figure 2.8. We next only examine the actions of $s _ { 1 }$ . The other states can be examined similarly. The action value of $( s _ { 1 } , a _ { 2 } )$ is

$$
q _ {\pi} (s _ {1}, a _ {2}) = - 1 + \gamma v _ {\pi} (s _ {2}),
$$

where $s _ { 2 }$ is the next state. Similarly, it can be obtained that

$$
q _ {\pi} (s _ {1}, a _ {3}) = 0 + \gamma v _ {\pi} (s _ {3}).
$$

A common mistake that beginners may make is about the values of the actions that the given policy does not select. For example, the policy in Figure 2.8 can only select $a _ { 2 }$ or $a _ { 3 }$ and cannot select $a _ { 1 } , a _ { 4 } , a _ { 5 }$ . One may argue that since the policy does not select $a _ { 1 } , a _ { 4 } , a _ { 5 }$ , we do not need to calculate their action values, or we can simply set $q _ { \pi } ( s _ { 1 } , a _ { 1 } ) = q _ { \pi } ( s _ { 1 } , a _ { 4 } ) = q _ { \pi } ( s _ { 1 } , a _ { 5 } ) = 0$ . This is wrong.

First, even if an action would not be selected by a policy, it still has an action value.

In this example, although policy $\pi$ does not take $a _ { 1 }$ at $s _ { 1 }$ , we can still calculate its

action value by observing what we would obtain after taking this action. Specifically, after taking $a _ { 1 }$ , the agent is bounced back to $s _ { 1 }$ (hence, the immediate reward is $- 1$ ) and then continues moving in the state space starting from $s _ { 1 }$ by following $\pi$ (hence, the future reward is $\gamma v _ { \pi } ( s _ { 1 } )$ ). As a result, the action value of $( s _ { 1 } , a _ { 1 } )$ is

$$
q _ {\pi} (s _ {1}, a _ {1}) = - 1 + \gamma v _ {\pi} (s _ {1}).
$$

Similarly, for $a _ { 4 }$ and $a _ { 5 }$ , which cannot be possibly selected by the given policy either, we have

$$
q _ {\pi} (s _ {1}, a _ {4}) = - 1 + \gamma v _ {\pi} (s _ {1}),
$$

$$
q _ {\pi} (s _ {1}, a _ {5}) = 0 + \gamma v _ {\pi} (s _ {1}).
$$

Second, why do we care about the actions that the given policy would not select? Although some actions cannot be possibly selected by a given policy, this does not mean that these actions are not good. It is possible that the given policy is not good, so it cannot select the best action. The purpose of reinforcement learning is to find optimal policies. To that end, we must keep exploring all actions to determine better actions for each state.

Finally, after computing the action values, we can also calculate the state value according to (2.14):

$$
\begin{array}{l} v _ {\pi} (s _ {1}) = 0. 5 q _ {\pi} (s _ {1}, a _ {2}) + 0. 5 q _ {\pi} (s _ {1}, a _ {3}), \\ = 0. 5 \left[ 0 + \gamma v _ {\pi} \left(s _ {3}\right) \right] + 0. 5 \left[ - 1 + \gamma v _ {\pi} \left(s _ {2}\right) \right]. \\ \end{array}
$$

# 2.8.2 The Bellman equation in terms of action values

The Bellman equation that we previously introduced was defined based on state values. In fact, it can also be expressed in terms of action values.

In particular, substituting (2.13) into (2.14) yields

$$
q _ {\pi} (s, a) = \sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) \sum_ {a ^ {\prime} \in \mathcal {A} (s ^ {\prime})} \pi (a ^ {\prime} | s ^ {\prime}) q _ {\pi} (s ^ {\prime}, a ^ {\prime}),
$$

which is an equation of action values. The above equation is valid for every state-action pair. If we put all these equations together, their matrix-vector form is

$$
q _ {\pi} = \tilde {r} + \gamma P \Pi q _ {\pi}, \tag {2.15}
$$

where $q _ { \pi }$ is the action value vector indexed by the state-action pairs: its $( s , a )$ th element is $[ q _ { \pi } ] _ { ( s , a ) } ~ = ~ q _ { \pi } ( s , a )$ . $\tilde { r }$ is the immediate reward vector indexed by the state-action pairs: $\begin{array} { r } { \big [ \tilde { r } \big ] _ { ( s , a ) } = \sum _ { r \in \mathcal { R } } p ( r | s , a ) r } \end{array}$ . The matrix $P$ is the probability transition matrix, whose

row is indexed by the state-action pairs and whose column is indexed by the states: $[ P ] _ { ( s , a ) , s ^ { \prime } } = p ( s ^ { \prime } | s , a )$ . Moreover, $\Pi$ is a block diagonal matrix in which each block is a $1 \times | { \mathcal { A } } |$ vector: $\Pi _ { s ^ { \prime } , ( s ^ { \prime } , a ^ { \prime } ) } = \pi ( a ^ { \prime } | s ^ { \prime } )$ and the other entries of $\Pi$ are zero.

Compared to the Bellman equation defined in terms of state values, the equation defined in terms of action values has some unique features. For example, $\tilde { r }$ and $P$ are independent of the policy and are merely determined by the system model. The policy is embedded in $1 1$ . It can be verified that (2.15) is also a contraction mapping and has a unique solution that can be iteratively solved. More details can be found in [5].

# 2.9 Summary

The most important concept introduced in this chapter is the state value. Mathematically, a state value is the expected return that the agent can obtain by starting from a state. The values of different states are related to each other. That is, the value of state $s$ relies on the values of some other states, which may further rely on the value of state $s$ itself. This phenomenon might be the most confusing part of this chapter for beginners. It is related to an important concept called bootstrapping, which involves calculating something from itself. Although bootstrapping may be intuitively confusing, it is clear if we examine the matrix-vector form of the Bellman equation. In particular, the Bellman equation is a set of linear equations that describe the relationships between the values of all states.

Since state values can be used to evaluate whether a policy is good or not, the process of solving the state values of a policy from the Bellman equation is called policy evaluation. As we will see later in this book, policy evaluation is an important step in many reinforcement learning algorithms.

Another important concept, action value, was introduced to describe the value of taking one action at a state. As we will see later in this book, action values play a more direct role than state values when we attempt to find optimal policies. Finally, the Bellman equation is not restricted to the reinforcement learning field. Instead, it widely exists in many fields such as control theories and operation research. In different fields, the Bellman equation may have different expressions. In this book, the Bellman equation is studied under discrete Markov decision processes. More information about this topic can be found in [2].

# 2.10 Q&A

Q: What is the relationship between state values and returns?

A: The value of a state is the mean of the returns that can be obtained if the agent starts from that state.

Q: Why do we care about state values?

A: State values can be used to evaluate policies. In fact, optimal policies are defined based on state values. This point will become clearer in the next chapter.

 Q: Why do we care about the Bellman equation?

A: The Bellman equation describes the relationships among the values of all states. It is the tool for analyzing state values.

Q: Why is the process of solving the Bellman equation called policy evaluation?

A: Solving the Bellman equation yields state values. Since state values can be used to evaluate a policy, solving the Bellman equation can be interpreted as evaluating the corresponding policy.

 Q: Why do we need to study the matrix-vector form of the Bellman equation?

A: The Bellman equation refers to a set of linear equations established for all the states. To solve state values, we must put all the linear equations together. The matrix-vector form is a concise expression of these linear equations.

 Q: What is the relationship between state values and action values?

A: On the one hand, a state value is the mean of the action values for that state. On the other hand, an action value relies on the values of the next states that the agent may transition to after taking the action.

Q: Why do we care about the values of the actions that a given policy cannot select?

A: Although a given policy cannot select some actions, this does not mean that these actions are not good. On the contrary, it is possible that the given policy is not good and misses the best action. To find better policies, we must keep exploring different actions even though some of them may not be selected by the given policy.

# Chapter 3

# Optimal State Values and Bellman Optimality Equation

![](images/631b13888977a5199b58da8c062e0b3d7319d0942fe47e4d50410f2150211f0d.jpg)  
Figure 3.1: Where we are in this book.

The ultimate goal of reinforcement learning is to seek optimal policies. It is, therefore, necessary to define what optimal policies are. In this chapter, we introduce a core concept and an important tool. The core concept is the optimal state value, based on which we can define optimal policies. The important tool is the Bellman optimality equation, from which we can solve the optimal state values and policies.

The relationship between the previous, present, and subsequent chapters is as follows. The previous chapter (Chapter 2) introduced the Bellman equation of any given policy.

The present chapter introduces the Bellman optimality equation, which is a special Bellman equation whose corresponding policy is optimal. The next chapter (Chapter 4) will introduce an important algorithm called value iteration, which is exactly the algorithm for solving the Bellman optimality equation as introduced in the present chapter.

Be prepared that this chapter is slightly mathematically intensive. However, it is worth it because many fundamental questions can be clearly answered.

# 3.1 Motivating example: How to improve policies?

![](images/4962edeb4c3a08b7de5f45d5b3e1fb2a4fbb9ad98c782d16699e8d7998f29946.jpg)  
Figure 3.2: An example for demonstrating policy improvement.

Consider the policy shown in Figure 3.2. Here, the orange and blue cells represent the forbidden and target areas, respectively. The policy here is not good because it selects $a _ { 2 }$ (rightward) in state $s _ { 1 }$ . How can we improve the given policy to obtain a better policy? The answer lies in state values and action values.

Intuition: It is intuitively clear that the policy can improve if it selects $a _ { 3 }$ (downward) instead of $a _ { 2 }$ (rightward) at $s _ { 1 }$ . This is because moving downward enables the agent to avoid entering the forbidden area.   
Mathematics: The above intuition can be realized based on the calculation of state values and action values.

First, we calculate the state values of the given policy. In particular, the Bellman equation of this policy is

$$
\begin{array}{l} v _ {\pi} (s _ {1}) = - 1 + \gamma v _ {\pi} (s _ {2}), \\ v _ {\pi} (s _ {2}) = + 1 + \gamma v _ {\pi} (s _ {4}), \\ v _ {\pi} (s _ {3}) = + 1 + \gamma v _ {\pi} (s _ {4}), \\ v _ {\pi} (s _ {4}) = + 1 + \gamma v _ {\pi} (s _ {4}). \\ \end{array}
$$

Let $\gamma = 0 . 9$ . It can be easily solved that

$$
v _ {\pi} \left(s _ {4}\right) = v _ {\pi} \left(s _ {3}\right) = v _ {\pi} \left(s _ {2}\right) = 1 0,
$$

$$
v _ {\pi} (s _ {1}) = 8.
$$

Second, we calculate the action values for state $s _ { 1 }$ :

$$
q _ {\pi} \left(s _ {1}, a _ {1}\right) = - 1 + \gamma v _ {\pi} \left(s _ {1}\right) = 6. 2,
$$

$$
q _ {\pi} \left(s _ {1}, a _ {2}\right) = - 1 + \gamma v _ {\pi} \left(s _ {2}\right) = 8,
$$

$$
q _ {\pi} \left(s _ {1}, a _ {3}\right) = 0 + \gamma v _ {\pi} \left(s _ {3}\right) = 9,
$$

$$
q _ {\pi} \left(s _ {1}, a _ {4}\right) = - 1 + \gamma v _ {\pi} \left(s _ {1}\right) = 6. 2,
$$

$$
q _ {\pi} (s _ {1}, a _ {5}) = 0 + \gamma v _ {\pi} (s _ {1}) = 7. 2.
$$

It is notable that action $a _ { 3 }$ has the greatest action value:

$$
q _ {\pi} \left(s _ {1}, a _ {3}\right) \geq q _ {\pi} \left(s _ {1}, a _ {i}\right), \quad \text {f o r a l l} i \neq 3.
$$

Therefore, we can update the policy to select $a _ { 3 }$ at $s _ { 1 }$ .

This example illustrates that we can obtain a better policy if we update the policy to select the action with the greatest action value. This is the basic idea of many reinforcement learning algorithms.

This example is very simple in the sense that the given policy is only not good for state $s _ { 1 }$ . If the policy is also not good for the other states, will selecting the action with the greatest action value still generate a better policy? Moreover, whether there always exist optimal policies? What does an optimal policy look like? We will answer all of these questions in this chapter.

# 3.2 Optimal state values and optimal policies

While the ultimate goal of reinforcement learning is to obtain optimal policies, it is necessary to first define what an optimal policy is. The definition is based on state values. In particular, consider two given policies $\pi _ { 1 }$ and $\pi _ { 2 }$ . If the state value of $\pi _ { 1 }$ is greater than or equal to that of $\pi _ { 2 }$ for any state:

$$
v _ {\pi_ {1}} (s) \geq v _ {\pi_ {2}} (s), \quad \text {f o r a l l} s \in \mathcal {S},
$$

then $\pi _ { 1 }$ is said to be better than $\pi _ { 2 }$ . Furthermore, if a policy is better than all the other possible policies, then this policy is optimal. This is formally stated below.

Definition 3.1 (Optimal policy and optimal state value). A policy $\pi ^ { * }$ is optimal if $v _ { \pi ^ { * } } ( s ) \geq v _ { \pi } ( s )$ for all $s \in S$ and for any other policy $\pi$ . The state values of $\pi ^ { * }$ are the optimal state values.

The above definition indicates that an optimal policy has the greatest state value for every state compared to all the other policies. This definition also leads to many questions:

Existence: Does the optimal policy exist?   
$\diamond$ Uniqueness: Is the optimal policy unique?   
 Stochasticity: Is the optimal policy stochastic or deterministic?   
$\diamond$ Algorithm: How to obtain the optimal policy and the optimal state values?

These fundamental questions must be clearly answered to thoroughly understand optimal policies. For example, regarding the existence of optimal policies, if optimal policies do not exist, then we do not need to bother to design algorithms to find them.

We will answer all these questions in the remainder of this chapter.

# 3.3 Bellman optimality equation

The tool for analyzing optimal policies and optimal state values is the Bellman optimality equation (BOE). By solving this equation, we can obtain optimal policies and optimal state values. We next present the expression of the BOE and then analyze it in detail.

For every $s \in S$ , the elementwise expression of the BOE is

$$
\begin{array}{l} v (s) = \max  _ {\pi (s) \in \Pi (s)} \sum_ {a \in \mathcal {A}} \pi (a | s) \left(\sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) v (s ^ {\prime})\right) \\ = \max  _ {\pi (s) \in \Pi (s)} \sum_ {a \in \mathcal {A}} \pi (a | s) q (s, a), \tag {3.1} \\ \end{array}
$$

where $v ( s ) , v ( s ^ { \prime } )$ are unknown variables to be solved and

$$
q (s, a) \doteq \sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) v (s ^ {\prime}).
$$

Here, $\pi ( s )$ denotes a policy for state $s$ , and $\Pi ( s )$ is the set of all possible policies for $s$

The BOE is an elegant and powerful tool for analyzing optimal policies. However, it may be nontrivial to understand this equation. For example, this equation has two unknown variables $v ( s )$ and $\pi ( a | s )$ . It may be confusing to beginners how to solve two unknown variables from one equation. Moreover, the BOE is actually a special Bellman equation. However, it is nontrivial to see that since its expression is quite different from that of the Bellman equation. We also need to answer the following fundamental questions about the BOE.

Existence: Does this equation have a solution?   
Uniqueness: Is the solution unique?   
 Algorithm: How to solve this equation?   
 Optimality: How is the solution related to optimal policies?

Once we can answer these questions, we will clearly understand optimal state values and optimal policies.

# 3.3.1 Maximization of the right-hand side of the BOE

We next clarify how to solve the maximization problem on the right-hand side of the BOE in (3.1). At first glance, it may be confusing to beginners how to solve two unknown variables $v ( s )$ and $\pi ( a | s )$ from one equation. In fact, these two unknown variables can be solved one by one. This idea is illustrated by the following example.

Example 3.1. Consider two unknown variables $x , y \in \mathbb { R }$ that satisfy

$$
x = \max _ {y \in \mathbb {R}} (2 x - 1 - y ^ {2}).
$$

The first step is to solve y on the right-hand side of the equation. Regardless of the value of $x$ , we always have $\mathrm { m a x } _ { y } ( 2 x - 1 - y ^ { 2 } ) = 2 x - 1$ , where the maximum is achieved when $y = 0$ . The second step is to solve $x$ . When $y = 0$ , the equation becomes $x = 2 x - 1$ , which leads to $x = 1$ . Therefore, $y = 0$ and $x = 1$ are the solutions of the equation.

We now turn to the maximization problem on the right-hand side of the BOE. The BOE in (3.1) can be written concisely as

$$
v (s) = \max _ {\pi (s) \in \Pi (s)} \sum_ {a \in \mathcal {A}} \pi (a | s) q (s, a), \quad s \in \mathcal {S}.
$$

Inspired by Example 3.1, we can first solve the optimal $\pi$ on the right-hand side. How to do that? The following example demonstrates its basic idea.

Example 3.2. Given $q _ { 1 } , q _ { 2 } , q _ { 3 } \in \mathbb { R }$ , we would like to find the optimal values of $c _ { 1 } , c _ { 2 } , c _ { 3 }$ to maximize

$$
\sum_ {i = 1} ^ {3} c _ {i} q _ {i} = c _ {1} q _ {1} + c _ {2} q _ {2} + c _ {3} q _ {3},
$$

where $c _ { 1 } + c _ { 2 } + c _ { 3 } = 1$ and $c _ { 1 } , c _ { 2 } , c _ { 3 } \geq 0$ .

Without loss of generality, suppose that $q _ { 3 } \geq q _ { 1 } , q _ { 2 }$ . Then, the optimal solution is $c _ { 3 } ^ { * } = 1$ and $c _ { 1 } ^ { * } = c _ { 2 } ^ { * } = 0$ . This is because

$$
q _ {3} = (c _ {1} + c _ {2} + c _ {3}) q _ {3} = c _ {1} q _ {3} + c _ {2} q _ {3} + c _ {3} q _ {3} \geq c _ {1} q _ {1} + c _ {2} q _ {2} + c _ {3} q _ {3}
$$

for any $c _ { 1 } , c _ { 2 } , c _ { 3 }$ .

Inspired by the above example, since $\begin{array} { r } { \sum _ { a } \pi ( a | s ) = 1 } \end{array}$ , we have

$$
\sum_ {a \in \mathcal {A}} \pi (a | s) q (s, a) \leq \sum_ {a \in \mathcal {A}} \pi (a | s) \max _ {a \in \mathcal {A}} q (s, a) = \max _ {a \in \mathcal {A}} q (s, a),
$$

where equality is achieved when

$$
\pi (a | s) = \left\{ \begin{array}{l l} 1, & a = a ^ {*}, \\ 0, & a \neq a ^ {*}. \end{array} \right.
$$

Here, $a ^ { * } = \arg \operatorname* { m a x } _ { a } q ( s , a )$ . In summary, the optimal policy $\pi ( s )$ is the one that selects the action that has the greatest value of $\boldsymbol { q } ( s , a )$ .

# 3.3.2 Matrix-vector form of the BOE

The BOE refers to a set of equations defined for all states. If we combine these equations, we can obtain a concise matrix-vector form, which will be extensively used in this chapter.

The matrix-vector form of the BOE is

$$
v = \max  _ {\pi \in \Pi} (r _ {\pi} + \gamma P _ {\pi} v), \tag {3.2}
$$

where $v \in \mathbb { R } ^ { | s | }$ and max $\pi$ is performed in an elementwise manner. The structures of $r _ { \pi }$ and $P _ { \pi }$ are the same as those in the matrix-vector form of the normal Bellman equation:

$$
[ r _ {\pi} ] _ {s} \doteq \sum_ {a \in \mathcal {A}} \pi (a | s) \sum_ {r \in \mathcal {R}} p (r | s, a) r, \qquad [ P _ {\pi} ] _ {s, s ^ {\prime}} = p (s ^ {\prime} | s) \doteq \sum_ {a \in \mathcal {A}} \pi (a | s) p (s ^ {\prime} | s, a).
$$

Since the optimal value of $\pi$ is determined by $\boldsymbol { v }$ , the right-hand side of (3.2) is a function of $\boldsymbol { v }$ , denoted as

$$
f (v) \doteq \max  _ {\pi \in \Pi} (r _ {\pi} + \gamma P _ {\pi} v).
$$

Then, the BOE can be expressed in a concise form as

$$
v = f (v). \tag {3.3}
$$

In the remainder of this section, we show how to solve this nonlinear equation.

# 3.3.3 Contraction mapping theorem

Since the BOE can be expressed as a nonlinear equation $v = f ( v )$ , we next introduce the contraction mapping theorem [6] to analyze it. The contraction mapping theorem is a powerful tool for analyzing general nonlinear equations. It is also known as the fixedpoint theorem. Readers who already know this theorem can skip this part. Otherwise, the reader is advised to be familiar with this theorem since it is the key to analyzing the

BOE.

Consider a function $f ( x )$ , where $x \in \mathbb { R } ^ { d }$ and $f : \mathbb { R } ^ { d }  \mathbb { R } ^ { d }$ . A point $x ^ { * }$ is called a fixed point if

$$
f (x ^ {*}) = x ^ {*}.
$$

The interpretation of the above equation is that the map of $x ^ { * }$ is itself. This is the reason why $x ^ { * }$ is called “fixed”. The function $f$ is a contraction mapping (or contractive function) if there exists $\gamma \in ( 0 , 1 )$ such that

$$
\left\| f (x _ {1}) - f (x _ {2}) \right\| \leq \gamma \| x _ {1} - x _ {2} \|
$$

for any $x _ { 1 } , x _ { 2 } \in \mathbb { R } ^ { d }$ . In this book, $\| \cdot \|$ denotes a vector or matrix norm.

Example 3.3. We present three examples to demonstrate fixed points and contraction mappings.

$x = f ( x ) = 0 . 5 x , x \in \mathbb { R } .$

It is easy to verify that $x = 0$ is a fixed point since $0 = 0 . 5 \cdot 0$ . Moreover, $f ( x ) = 0 . 5 x$ is a contraction mapping because $\left\| 0 . 5 x _ { 1 } - 0 . 5 x _ { 2 } \right\| = 0 . 5 \| x _ { 1 } - x _ { 2 } \| \leq \gamma \| x _ { 1 } - x _ { 2 } \|$ for any $\gamma \in [ 0 . 5 , 1 )$ .

 $x = f ( x ) = A x$ , where $x \in \mathbb { R } ^ { n } , A \in \mathbb { R } ^ { n \times n }$ and $\| A \| \leq \gamma < 1$ .

It is easy to verify that $x = 0$ is a fixed point since $0 = A 0$ . To see the contraction property, $\| A x _ { 1 } - A x _ { 2 } \| = \| A ( x _ { 1 } - x _ { 2 } ) \| \leq \| A \| \| x _ { 1 } - x _ { 2 } \| \leq \gamma \| x _ { 1 } - x _ { 2 } \|$ . Therefore, $f ( x ) = A x$ is a contraction mapping.

 $x = f ( x ) = 0 . 5 \sin x , x \in \mathbb { R }$

It is easy to see that $x = 0$ is a fixed point since $0 = 0 . 5 \sin { 0 }$ . Moreover, it follows from the mean value theorem $[ \gamma , 8 ]$ that

$$
\left| \frac {0 . 5 \sin x _ {1} - 0 . 5 \sin x _ {2}}{x _ {1} - x _ {2}} \right| = | 0. 5 \cos x _ {3} | \leq 0. 5, \quad x _ {3} \in [ x _ {1}, x _ {2} ].
$$

As a result, $| 0 . 5 \sin x _ { 1 } - 0 . 5 \sin x _ { 2 } | \ \leq \ 0 . 5 | x _ { 1 } - x _ { 2 } |$ and hence $f ( x ) = 0 . 5 \sin { x }$ is a contraction mapping. □

The relationship between a fixed point and the contraction property is characterized by the following classic theorem.

Theorem 3.1 (Contraction mapping theorem). For any equation that has the form $x =$ $f ( x )$ where x and $f ( x )$ are real vectors, if $f$ is a contraction mapping, then the following properties hold.

Existence: There exists a fixed point $x ^ { * }$ satisfying $f ( x ^ { * } ) = x ^ { * }$ .

Uniqueness: The fixed point $x ^ { * }$ is unique.   
Algorithm: Consider the iterative process:

$$
x _ {k + 1} = f (x _ {k}),
$$

where $k = 0 , 1 , 2 , \ldots$ . Then, $x _ { k } \to x ^ { * }$ as $k  \infty$ for any initial guess $x _ { 0 }$ . Moreover, the convergence rate is exponentially fast.

The contraction mapping theorem not only can tell whether the solution of a nonlinear equation exists but also suggests a numerical algorithm for solving the equation. The proof of the theorem is given in Box 3.1.

The following example demonstrates how to calculate the fixed points of some equations using the iterative algorithm suggested by the contraction mapping theorem.

Example 3.4. Let us revisit the abovementioned examples: $x \ : = \ : 0 . 5 x$ , $x \ = \ A x$ , and $x = 0 . 5 \sin { x }$ . While it has been shown that the right-hand sides of these three equations are all contraction mappings, it follows from the contraction mapping theorem that they each have a unique fixed point, which can be easily verified to be $x ^ { * } = 0$ . Moreover, the fixed points of the three equations can be iteratively solved by the following algorithms:

$$
x _ {k + 1} = 0. 5 x _ {k},
$$

$$
x _ {k + 1} = A x _ {k},
$$

$$
x _ {k + 1} = 0. 5 \sin x _ {k},
$$

given any initial guess $x _ { 0 }$

![](images/b0b94afc2452afbb7362584711da6525cf36de75b3ffce0e37461e2f959f27fe.jpg)

# Box 3.1: Proof of the contraction mapping theorem

Part 1: We prove that the sequence $\{ x _ { k } \} _ { k = 1 } ^ { \infty }$ with $x _ { k } = f ( x _ { k - 1 } )$ is convergent.

The proof relies on Cauchy sequences. A sequence $x _ { 1 } , x _ { 2 } , \cdots \in \mathbb { R }$ is called Cauchy if for any small $\varepsilon > 0$ , there exists $N$ such that $\| x _ { m } - x _ { n } \| < \varepsilon$ for all $m , n > N$ . The intuitive interpretation is that there exists a finite integer $N$ such that all the elements after $N$ are sufficiently close to each other. Cauchy sequences are important because it is guaranteed that a Cauchy sequence converges to a limit. Its convergence property will be used to prove the contraction mapping theorem. Note that we must have $\| x _ { m } - x _ { n } \| < \varepsilon$ for all $m , n \ > \ N$ . If we simply have $x _ { n + 1 } - x _ { n } \to 0$ , it is insufficient to claim that the sequence is a Cauchy sequence. For example, it holds that $x _ { n + 1 } - x _ { n } \to 0$ for $x _ { n } = { \sqrt { n } }$ , but apparently, $x _ { n } = { \sqrt { n } }$ diverges.

We next show that $\{ x _ { k } = f ( x _ { k - 1 } ) \} _ { k = 1 } ^ { \infty }$ is a Cauchy sequence and hence converges.

First, since $f$ is a contraction mapping, we have

$$
\left\| x _ {k + 1} - x _ {k} \right\| = \left\| f (x _ {k}) - f (x _ {k - 1}) \right\| \leq \gamma \left\| x _ {k} - x _ {k - 1} \right\|.
$$

Similarly, we have $\| x _ { k } - x _ { k - 1 } \| \leq \gamma \| x _ { k - 1 } - x _ { k - 2 } \|$ , . . . , kx2 − x1k ≤ γkx1 − x0k. Thus, we have

$$
\begin{array}{l} \left\| x _ {k + 1} - x _ {k} \right\| \leq \gamma \left\| x _ {k} - x _ {k - 1} \right\| \\ \leq \gamma^ {2} \| x _ {k - 1} - x _ {k - 2} \| \\ \begin{array}{c} \bullet \\ \bullet \\ \bullet \end{array} \\ \leq \gamma^ {k} \| x _ {1} - x _ {0} \|. \\ \end{array}
$$

Since $\gamma < 1$ , we know that $\| x _ { k + 1 } - x _ { k } \|$ converges to zero exponentially fast as $k  \infty$ given any $x _ { 1 } , x _ { 0 }$ . Notably, the convergence of $\{ \| x _ { k + 1 } - x _ { k } \| \}$ is not sufficient for implying the convergence of $\{ x _ { k } \}$ . Therefore, we need to further consider $\| x _ { m } - x _ { n } \|$ for any $m > n$ . In particular,

$$
\begin{array}{l} \left\| x _ {m} - x _ {n} \right\| = \left\| x _ {m} - x _ {m - 1} + x _ {m - 1} - \dots - x _ {n + 1} + x _ {n + 1} - x _ {n} \right\| \\ \leq \left\| x _ {m} - x _ {m - 1} \right\| + \dots + \left\| x _ {n + 1} - x _ {n} \right\| \\ \leq \gamma^ {m - 1} \| x _ {1} - x _ {0} \| + \dots + \gamma^ {n} \| x _ {1} - x _ {0} \| \\ = \gamma^ {n} \left(\gamma^ {m - 1 - n} + \dots + 1\right) \| x _ {1} - x _ {0} \| \\ \leq \gamma^ {n} (1 + \dots + \gamma^ {m - 1 - n} + \gamma^ {m - n} + \gamma^ {m - n + 1} + \dots) \| x _ {1} - x _ {0} \| \\ = \frac {\gamma^ {n}}{1 - \gamma} \| x _ {1} - x _ {0} \|. \tag {3.4} \\ \end{array}
$$

As a result, for any $\varepsilon$ , we can always find $N$ such that $\| x _ { m } - x _ { n } \| < \varepsilon$ for all $m , n > N$ . Therefore, this sequence is Cauchy and hence converges to a limit point denoted as $x ^ { * } = \operatorname* { l i m } _ { k \to \infty } x _ { k }$ .

Part 2: We show that the limit $x ^ { * } = \operatorname* { l i m } _ { k \to \infty } x _ { k }$ is a fixed point. To do that, since

$$
\left\| f \left(x _ {k}\right) - x _ {k} \right\| = \left\| x _ {k + 1} - x _ {k} \right\| \leq \gamma^ {k} \left\| x _ {1} - x _ {0} \right\|,
$$

we know that $\| f ( x _ { k } ) - x _ { k } \|$ converges to zero exponentially fast. Hence, we have $f ( x ^ { * } ) = x ^ { * }$ at the limit.

Part 3: We show that the fixed point is unique. Suppose that there is another fixed point $x ^ { \prime }$ satisfying $f ( x ^ { \prime } ) = x ^ { \prime }$ . Then,

$$
\| x ^ {\prime} - x ^ {*} \| = \| f (x ^ {\prime}) - f (x ^ {*}) \| \leq \gamma \| x ^ {\prime} - x ^ {*} \|.
$$

Since $\gamma < 1$ , this inequality holds if and only if $\| x ^ { \prime } - x ^ { * } \| = 0$ . Therefore, $x ^ { \prime } = x ^ { \ast }$ .

Part 4: We show that $x _ { k }$ converges to $x ^ { * }$ exponentially fast. Recall that $\Vert { x _ { m } } -$ $\textstyle x _ { n } \| \leq { \frac { \gamma ^ { n } } { 1 - \gamma } } \| x _ { 1 } - x _ { 0 } \|$ , as proven in (3.4). Since $m$ can be arbitrarily large, we have

$$
\| x ^ {*} - x _ {n} \| = \lim _ {m \to \infty} \| x _ {m} - x _ {n} \| \leq \frac {\gamma^ {n}}{1 - \gamma} \| x _ {1} - x _ {0} \|.
$$

Since $\gamma < 1$ , the error converges to zero exponentially fast as $n \to \infty$ .

# 3.3.4 Contraction property of the right-hand side of the BOE

We next show that $f ( v )$ in the BOE in (3.3) is a contraction mapping. Thus, the contraction mapping theorem introduced in the previous subsection can be applied.

Theorem 3.2 (Contraction property of $f ( v )$ ). The function $f ( v )$ on the right-hand side of the BOE in (3.3) is a contraction mapping. In particular, for any $v _ { 1 } , v _ { 2 } \in \mathbb { R } ^ { | S | }$ , it holds that

$$
\left\| f (v _ {1}) - f (v _ {2}) \right\| _ {\infty} \leq \gamma \left\| v _ {1} - v _ {2} \right\| _ {\infty},
$$

where $\gamma \in \mathsf { \Gamma } ( 0 , 1 )$ is the discount rate, and $\| \cdot \| _ { \infty }$ is the maximum norm, which is the maximum absolute value of the elements of a vector.

The proof of the theorem is given in Box 3.2. This theorem is important because we can use the powerful contraction mapping theorem to analyze the BOE.

# Box 3.2: Proof of Theorem 3.2

Consider any two vectors $v _ { 1 } , v _ { 2 } \in \mathbb { R } ^ { | S | }$ , and suppose that $\pi _ { 1 } ^ { * } \doteq \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { 1 } )$ and $\pi _ { 2 } ^ { * } \doteq \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { 2 } )$ . Then,

$$
f (v _ {1}) = \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v _ {1}) = r _ {\pi_ {1} ^ {*}} + \gamma P _ {\pi_ {1} ^ {*}} v _ {1} \geq r _ {\pi_ {2} ^ {*}} + \gamma P _ {\pi_ {2} ^ {*}} v _ {1},
$$

$$
f (v _ {2}) = \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v _ {2}) = r _ {\pi_ {2} ^ {*}} + \gamma P _ {\pi_ {2} ^ {*}} v _ {2} \geq r _ {\pi_ {1} ^ {*}} + \gamma P _ {\pi_ {1} ^ {*}} v _ {2},
$$

where $\geq$ is an elementwise comparison. As a result,

$$
\begin{array}{l} f (v _ {1}) - f (v _ {2}) = r _ {\pi_ {1} ^ {*}} + \gamma P _ {\pi_ {1} ^ {*}} v _ {1} - (r _ {\pi_ {2} ^ {*}} + \gamma P _ {\pi_ {2} ^ {*}} v _ {2}) \\ \leq r _ {\pi_ {1} ^ {*}} + \gamma P _ {\pi_ {1} ^ {*}} v _ {1} - \left(r _ {\pi_ {1} ^ {*}} + \gamma P _ {\pi_ {1} ^ {*}} v _ {2}\right) \\ = \gamma P _ {\pi_ {1} ^ {*}} (v _ {1} - v _ {2}). \\ \end{array}
$$

Similarly, it can be shown that $f ( v _ { 2 } ) - f ( v _ { 1 } ) \leq \gamma P _ { \pi _ { 2 } ^ { * } } ( v _ { 2 } - v _ { 1 } )$ . Therefore,

$$
\gamma P _ {\pi_ {2} ^ {*}} (v _ {1} - v _ {2}) \leq f (v _ {1}) - f (v _ {2}) \leq \gamma P _ {\pi_ {1} ^ {*}} (v _ {1} - v _ {2}).
$$

Define

$$
z \doteq \max \left\{\left| \gamma P _ {\pi_ {2} ^ {*}} (v _ {1} - v _ {2}) \right|, \left| \gamma P _ {\pi_ {1} ^ {*}} (v _ {1} - v _ {2}) \right| \right\} \in \mathbb {R} ^ {| S |},
$$

where max $( \cdot )$ , $| \cdot |$ , and $\geq$ are all elementwise operators. By definition, $z \geq 0$ . On the one hand, it is easy to see that

$$
- z \leq \gamma P _ {\pi_ {2} ^ {*}} (v _ {1} - v _ {2}) \leq f (v _ {1}) - f (v _ {2}) \leq \gamma P _ {\pi_ {1} ^ {*}} (v _ {1} - v _ {2}) \leq z,
$$

which implies

$$
| f (v _ {1}) - f (v _ {2}) | \leq z.
$$

It then follows that

$$
\left\| f \left(v _ {1}\right) - f \left(v _ {2}\right) \right\| _ {\infty} \leq \| z \| _ {\infty}, \tag {3.5}
$$

where $\| \cdot \| _ { \infty }$ is the maximum norm.

On the other hand, suppose that $z _ { i }$ is the $i$ th entry of $z$ , and $p _ { i } ^ { { I } }$ and $q _ { i } ^ { T }$ are the $i$ th row of $P _ { \pi _ { 1 } ^ { * } }$ and $P _ { \pi _ { 2 } ^ { * } }$ , respectively. Then,

$$
z _ {i} = \max \{\gamma | p _ {i} ^ {T} (v _ {1} - v _ {2}) |, \gamma | q _ {i} ^ {T} (v _ {1} - v _ {2}) | \}.
$$

Since $p _ { i }$ is a vector with all nonnegative elements and the sum of the elements is equal to one, it follows that

$$
| p _ {i} ^ {T} (v _ {1} - v _ {2}) | \leq p _ {i} ^ {T} | v _ {1} - v _ {2} | \leq \| v _ {1} - v _ {2} \| _ {\infty}.
$$

Similarly, we have $| q _ { i } ^ { T } ( v _ { 1 } - v _ { 2 } ) | \leq \| v _ { 1 } - v _ { 2 } \| _ { \infty }$ . Therefore, $z _ { i } \leq \gamma \| v _ { 1 } - v _ { 2 } \| _ { \infty }$ and hence

$$
\left\| z \right\| _ {\infty} = \max _ {i} \left| z _ {i} \right| \leq \gamma \left\| v _ {1} - v _ {2} \right\| _ {\infty}.
$$

Substituting this inequality to (3.5) gives

$$
\left\| f (v _ {1}) - f (v _ {2}) \right\| _ {\infty} \leq \gamma \left\| v _ {1} - v _ {2} \right\| _ {\infty},
$$

which concludes the proof of the contraction property of $f ( v )$ .

# 3.4 Solving an optimal policy from the BOE

With the preparation in the last section, we are ready to solve the BOE to obtain the optimal state value $v ^ { * }$ and an optimal policy $\pi ^ { * }$ .

 Solving $v ^ { * }$ : If $v ^ { * }$ is a solution of the BOE, then it satisfies

$$
v ^ {*} = \max _ {\pi \in \Pi} (r _ {\pi} + \gamma P _ {\pi} v ^ {*}).
$$

Clearly, $v ^ { * }$ is a fixed point because $v ^ { * } = f ( v ^ { * } )$ . Then, the contraction mapping theorem suggests the following results.

Theorem 3.3 (Existence, uniqueness, and algorithm). For the BOE $v = f ( v ) = $ $\mathrm { m a x } _ { \pi \in \Pi } ( r _ { \pi } + \gamma P _ { \pi } v )$ , there always exists a unique solution $v ^ { * }$ , which can be solved iteratively by

$$
v _ {k + 1} = f (v _ {k}) = \max _ {\pi \in \Pi} (r _ {\pi} + \gamma P _ {\pi} v _ {k}), \quad k = 0, 1, 2, \ldots .
$$

The value of $v _ { k }$ converges to $v ^ { * }$ exponentially fast as $k  \infty$ given any initial guess $v _ { 0 }$ .

The proof of this theorem directly follows from the contraction mapping theorem since $f ( v )$ is a contraction mapping. This theorem is important because it answers some fundamental questions.

Existence of $v ^ { * }$ : The solution of the BOE always exists.   
- Uniqueness of $v ^ { * }$ : The solution $v ^ { * }$ is always unique.   
Algorithm for solving $v ^ { * }$ : The value of $v ^ { * }$ can be solved by the iterative algorithm suggested by Theorem 3.3. This iterative algorithm has a specific name called value iteration. Its implementation will be introduced in detail in Chapter 4. We mainly focus on the fundamental properties of the BOE in the present chapter.

 Solving $\pi ^ { * }$ : Once the value of $v ^ { * }$ has been obtained, we can easily obtain $\pi ^ { * }$ by solving

$$
\pi^ {*} = \arg \max  _ {\pi \in \Pi} (r _ {\pi} + \gamma P _ {\pi} v ^ {*}). \tag {3.6}
$$

The value of $\pi ^ { * }$ will be given in Theorem 3.5. Substituting (3.6) into the BOE yields

$$
v ^ {*} = r _ {\pi^ {*}} + \gamma P _ {\pi^ {*}} v ^ {*}.
$$

Therefore, $v ^ { * } = v _ { \pi ^ { * } }$ is the state value of $\pi ^ { * }$ , and the BOE is a special Bellman equation whose corresponding policy is $\pi ^ { * }$ .

At this point, although we can solve $v ^ { * }$ and $\pi ^ { * }$ , it is still unclear whether the solution is optimal. The following theorem reveals the optimality of the solution.

Theorem 3.4 (Optimality of $v ^ { * }$ and $\pi ^ { * }$ ). The solution $v ^ { * }$ is the optimal state value, and $\pi ^ { * }$ is an optimal policy. That is, for any policy $\pi$ , it holds that

$$
v ^ {*} = v _ {\pi^ {*}} \geq v _ {\pi},
$$

where $v _ { \pi }$ is the state value of $\pi$ , and $\geq$ is an elementwise comparison.

Now, it is clear why we must study the BOE: its solution corresponds to optimal state values and optimal policies. The proof of the above theorem is given in the following box.

# Box 3.3: Proof of Theorem 3.4

For any policy $\pi$ , it holds that

$$
v _ {\pi} = r _ {\pi} + \gamma P _ {\pi} v _ {\pi}.
$$

Since

$$
v ^ {*} = \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v ^ {*}) = r _ {\pi^ {*}} + \gamma P _ {\pi^ {*}} v ^ {*} \geq r _ {\pi} + \gamma P _ {\pi} v ^ {*},
$$

we have

$$
v ^ {*} - v _ {\pi} \geq (r _ {\pi} + \gamma P _ {\pi} v ^ {*}) - (r _ {\pi} + \gamma P _ {\pi} v _ {\pi}) = \gamma P _ {\pi} (v ^ {*} - v _ {\pi}).
$$

Repeatedly applying the above inequality gives $v ^ { * } - v _ { \pi } \geq \gamma P _ { \pi } ( v ^ { * } - v _ { \pi } ) \geq \gamma ^ { 2 } P _ { \pi } ^ { 2 } ( v ^ { * } -$ $v _ { \pi } ) \geq \dots \geq \gamma ^ { n } P _ { \pi } ^ { n } ( v ^ { * } - v _ { \pi } )$ . It follows that

$$
v ^ {*} - v _ {\pi} \geq \lim _ {n \to \infty} \gamma^ {n} P _ {\pi} ^ {n} (v ^ {*} - v _ {\pi}) = 0,
$$

where the last equality is true because $\gamma < 1$ and $P _ { \pi } ^ { n }$ is a nonnegative matrix with all its elements less than or equal to 1 (because $P _ { \pi } ^ { n } \mathbf { 1 } = \mathbf { 1 }$ ). Therefore, $v ^ { * } \geq v _ { \pi }$ for any $\pi$ .

We next examine $\pi ^ { * }$ in (3.6) more closely. In particular, the following theorem shows that there always exists a deterministic greedy policy that is optimal.

Theorem 3.5 (Greedy optimal policy). For any $s \in S$ , the deterministic greedy policy

$$
\pi^ {*} (a | s) = \left\{ \begin{array}{l l} 1, & a = a ^ {*} (s), \\ 0, & a \neq a ^ {*} (s), \end{array} \right. \tag {3.7}
$$

is an optimal policy for solving the BOE. Here,

$$
a ^ {*} (s) = \arg \max  _ {a} q ^ {*} (a, s),
$$

where

$$
q ^ {*} (s, a) \doteq \sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) v ^ {*} (s ^ {\prime}).
$$

# Box 3.4: Proof of Theorem 3.5

While the matrix-vector form of the optimal policy is $\pi ^ { * } = \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v ^ { * } )$ , its elementwise form is

$$
\pi^ {*} (s) = \arg \max _ {\pi \in \Pi} \sum_ {a \in \mathcal {A}} \pi (a | s) \underbrace {\left(\sum_ {r \in \mathcal {R}} p (r | s , a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s , a) v ^ {*} (s ^ {\prime})\right)} _ {q ^ {*} (s, a)}, \quad s \in \mathcal {S}.
$$

It is clear that $\scriptstyle \sum _ { a \in { \mathcal { A } } } \pi ( a | s ) q ^ { * } ( s , a )$ is maximized if $\pi ( s )$ selects the action with the greatest $\boldsymbol { q } ^ { * } ( s , a )$ .

The policy in (3.7) is called greedy because it seeks the actions with the greatest $\boldsymbol { q } ^ { * } ( s , a )$ . Finally, we discuss two important properties of $\pi ^ { * }$ .

Uniqueness of optimal policies: Although the value of $v ^ { * }$ is unique, the optimal policy that corresponds to $v ^ { * }$ may not be unique. This can be easily verified by counterexamples. For example, the two policies shown in Figure 3.3 are both optimal.   
Stochasticity of optimal policies: An optimal policy can be either stochastic or deterministic, as demonstrated in Figure 3.3. However, it is certain that there always exists a deterministic optimal policy according to Theorem 3.5.

![](images/7dff0fb62a665df2917d93ec56eed689747df2aad72100aa56542c672838db72.jpg)

![](images/d57647c31c9a7341480f6c143f374d43d3937499ed583389bff23291914c4e46.jpg)  
Figure 3.3: Examples for demonstrating that optimal policies may not be unique. The two policies are different but are both optimal.

# 3.5 Factors that influence optimal policies

The BOE is a powerful tool for analyzing optimal policies. We next apply the BOE to study what factors can influence optimal policies. This question can be easily answered by observing the elementwise expression of the BOE:

$$
v (s) = \max _ {\pi (s) \in \Pi (s)} \sum_ {a \in \mathcal {A}} \pi (a | s) \left(\sum_ {r \in \mathcal {R}} p (r | s, a) r + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) v (s ^ {\prime})\right), \quad s \in \mathcal {S}.
$$

The optimal state value and optimal policy are determined by the following parameters: 1) the immediate reward $r$ , 2) the discount rate $\gamma$ , and 3) the system model $p ( s ^ { \prime } | s , a ) , p ( r | s , a )$ . While the system model is fixed, we next discuss how the optimal policy varies when we change the values of $r$ and $\gamma$ . All the optimal policies presented in this section can be obtained via the algorithm in Theorem 3.3. The implementation details of the algorithm will be given in Chapter 4. The present chapter mainly focuses on the fundamental properties of optimal policies.

# A baseline example

Consider the example in Figure 3.4. The reward settings are $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1$ and $r _ { \mathrm { t a r g e t } } = 1$ . In addition, the agent receives a reward of $r _ { \mathrm { o t h e r } } = 0$ for every movement step. The discount rate is selected as $\gamma = 0 . 9$ .

With the above parameters, the optimal policy and optimal state values are given in Figure 3.4(a). It is interesting that the agent is not afraid of passing through forbidden areas to reach the target area. More specifically, starting from the state at (row=4, column=1), the agent has two options for reaching the target area. The first option is to avoid all the forbidden areas and travel a long distance to the target area. The second option is to pass through forbidden areas. Although the agent obtains negative rewards when entering forbidden areas, the cumulative reward of the second trajectory is greater than that of the first trajectory. Therefore, the optimal policy is far-sighted due to the relatively large value of $\gamma$ .

# Impact of the discount rate

If we change the discount rate from $\gamma = 0 . 9$ to $\gamma = 0 . 5$ and keep other parameters unchanged, the optimal policy becomes the one shown in Figure 3.4(b). It is interesting that the agent does not dare to take risks anymore. Instead, it would travel a long distance to reach the target while avoiding all the forbidden areas. This is because the optimal policy becomes short-sighted due to the relatively small value of $\gamma$ .

In the extreme case where $\gamma ~ = ~ 0$ , the corresponding optimal policy is shown in Figure 3.4(c). In this case, the agent is not able to reach the target area. This is

![](images/0a58dd50a30f332b7dd54dfd43b4cf296846242d06026729be78ffee59e49e9d.jpg)

![](images/b5ea4e44aaf67b0dd14f2763370a225517da8a06259e9155dba1e23ea8c51214.jpg)  
(a) Baseline example: $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1$ , $r _ { \mathrm { t a r g e t } } = 1$ , $\gamma = 0 . 9$

![](images/8075c3ac861d76262510858763217c4020b34da55225d679e8341a46adc8df3c.jpg)

![](images/21826d8f83390ec3a96d79a50b63c38b709a66ae94d68bda1b5f0f05a5c01034.jpg)  
(b) The discount rate is changed to $\gamma = 0 . 5$ . The other parameters are the same as those in (a).

![](images/0bbc72d99e088ada0d844119ec738c52e146df0ada0207ea31a7d0207b5b469e.jpg)

![](images/45a208a5890aac7f5dfba624fdaa0ef36477b0e3596ec0f163bce482fed0cd0a.jpg)  
(c) The discount rate is changed to $\gamma = 0$ . The other parameters are the same as those in (a).

![](images/f8991d7a8b759a91bd53a9607c4d039be86a3465ba1d20068004908d016c1536.jpg)

![](images/bf624d598778da69237763fba38a8994c1ce33a5d8f0f7d52ee19d81849496a0.jpg)  
(d) rforbidden is changed from $^ { - 1 }$ to $- 1 0$ . The other parameters are the same as those in (a).   
Figure 3.4: The optimal policies and optimal state values given different parameter values.

because the optimal policy for each state is extremely short-sighted and merely selects the action with the greatest immediate reward instead of the greatest total reward.

In addition, the spatial distribution of the state values exhibits an interesting pattern: the states close to the target have greater state values, whereas those far away have lower values. This pattern can be observed from all the examples shown in Figure 3.4. It can be explained by using the discount rate: if a state must travel along a longer trajectory to reach the target, its state value is smaller due to the discount rate.

# Impact of the reward values

If we want to strictly prohibit the agent from entering any forbidden area, we can increase the punishment received for doing so. For instance, if rforbidden is changed from $- 1$ to $- 1 0$ , the resulting optimal policy can avoid all the forbidden areas (see Figure 3.4(d)).

However, changing the rewards does not always lead to different optimal policies. One important fact is that optimal policies are invariant to affine transformations of the rewards. In other words, if we scale all the rewards or add the same value to all the rewards, the optimal policy remains the same.

Theorem 3.6 (Optimal policy invariance). Consider a Markov decision process with $v ^ { * } \in \mathbb { R } ^ { | S | }$ as the optimal state value satisfying $v ^ { * } = \mathrm { m a x } _ { \pi \in \Pi } ( r _ { \pi } + \gamma P _ { \pi } v ^ { * } )$ . If every reward $r \in \mathcal { R }$ is changed by an affine transformation to $\alpha r + \beta$ , where $\alpha , \beta \in \mathbb { R }$ and $\alpha > 0$ , then the corresponding optimal state value $v ^ { \prime }$ is also an affine transformation of $v ^ { * }$ :

$$
v ^ {\prime} = \alpha v ^ {*} + \frac {\beta}{1 - \gamma} \mathbf {1}, \tag {3.8}
$$

where $\gamma \in ( 0 , 1 )$ is the discount rate and $\mathbf { 1 } = [ 1 , \ldots , 1 ] ^ { T }$ . Consequently, the optimal policy derived from $v ^ { \prime }$ is invariant to the affine transformation of the reward values.

# Box 3.5: Proof of Theorem 3.6

For any policy $\pi$ , define $\boldsymbol { r } _ { \pi } = [ . . . , r _ { \pi } ( s ) , . . . ] ^ { \boldsymbol { I } ^ { \intercal } }$ where

$$
r _ {\pi} (s) = \sum_ {a \in \mathcal {A}} \pi (a | s) \sum_ {r \in \mathcal {R}} p (r | s, a) r, \quad s \in \mathcal {S}.
$$

If $r  \alpha r + \beta$ , then $r _ { \pi } ( s ) \to \alpha r _ { \pi } ( s ) + \beta$ and hence $r _ { \pi }  \alpha r _ { \pi } + \beta { \bf 1 }$ , where ${ \textbf { 1 } } =$ $[ 1 , \ldots , 1 ] ^ { T }$ . In this case, the BOE becomes

$$
v ^ {\prime} = \max  _ {\pi \in \Pi} (\alpha r _ {\pi} + \beta \mathbf {1} + \gamma P _ {\pi} v ^ {\prime}). \tag {3.9}
$$

We next solve the new BOE in (3.9) by showing that $\boldsymbol { v ^ { \prime } } = \alpha \boldsymbol { v } ^ { * } + c \mathbf { 1 }$ with $c = \beta / ( 1 - \gamma )$ is a solution of (3.9). In particular, substituting $v ^ { \prime } = \alpha v ^ { * } + c \mathbf { 1 }$ into (3.9) gives

$$
\alpha v ^ {*} + c \mathbf {1} = \max _ {\pi \in \Pi} (\alpha r _ {\pi} + \beta \mathbf {1} + \gamma P _ {\pi} (\alpha v ^ {*} + c \mathbf {1})) = \max _ {\pi \in \Pi} (\alpha r _ {\pi} + \beta \mathbf {1} + \alpha \gamma P _ {\pi} v ^ {*} + c \gamma \mathbf {1}),
$$

where the last equality is due to the fact that $P _ { \pi } { \bf 1 } = { \bf 1 }$ . The above equation can be reorganized as

$$
\alpha v ^ {*} = \max _ {\pi \in \Pi} (\alpha r _ {\pi} + \alpha \gamma P _ {\pi} v ^ {*}) + \beta \mathbf {1} + c \gamma \mathbf {1} - c \mathbf {1},
$$

which is equivalent to

$$
\beta \mathbf {1} + c \gamma \mathbf {1} - c \mathbf {1} = 0.
$$

Since $c = \beta / ( 1 - \gamma )$ , the above equation is valid and hence $v ^ { \prime } = \alpha v ^ { * } + c \mathbf { 1 }$ is the solution of (3.9). Since (3.9) is the BOE, $v ^ { \prime }$ is also the unique solution. Finally, since $v ^ { \prime }$ is an affine transformation of $v ^ { * }$ , the relative relationships between the action values remain the same. Hence, the greedy optimal policy derived from $v ^ { \prime }$ is the same as that from $v ^ { * }$ : $\arg \operatorname* { m a x } _ { \pi \in \Pi } ( r _ { \pi } + \gamma P _ { \pi } v ^ { \prime } )$ is the same as $\arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v ^ { * } )$ .

Readers may refer to [9] for a further discussion on the conditions under which modifications to the reward values preserve the optimal policy.

# Avoiding meaningless detours

In the reward setting, the agent receives a reward of $r _ { \mathrm { o t h e r } } ~ = ~ 0$ for every movement step (unless it enters a forbidden area or the target area or attempts to go beyond the boundary). Since a zero reward is not a punishment, would the optimal policy take meaningless detours before reaching the target? Should we set $r _ { \mathrm { o t h e r } }$ to be negative to encourage the agent to reach the target as quickly as possible?

![](images/1eafd557cbae2585127484359fb1c09ff6c9481433c64cfa8a6f3140c44f9d49.jpg)  
(a) Optimal policy

![](images/8f7471d604ffd40e68d119e60936d3cbb2b2a287887b67daff7d4cf860e61a6a.jpg)  
(b) Non-optimal policy   
Figure 3.5: Examples illustrating that optimal policies do not take meaningless detours due to the discount rate.

Consider the examples in Figure 3.5, where the bottom-right cell is the target area

to reach. The two policies here are the same except for state $s _ { 2 }$ . By the policy in Figure 3.5(a), the agent moves downward at $s _ { 2 }$ and the resulting trajectory is $s _ { 2 }  s _ { 4 }$ . By the policy in Figure 3.5(b), the agent moves leftward and the resulting trajectory is $s _ { 2 }  s _ { 1 }  s _ { 3 }  s _ { 4 }$ .

It is notable that the second policy takes a detour before reaching the target area. If we merely consider the immediate rewards, taking this detour does not matter because no negative immediate rewards will be obtained. However, if we consider the discounted return, then this detour matters. In particular, for the first policy, the discounted return is

$$
\mathrm {r e t u r n} = 1 + \gamma 1 + \gamma^ {2} 1 + \dots = 1 / (1 - \gamma) = 1 0.
$$

As a comparison, the discounted return for the second policy is

$$
\mathrm {r e t u r n} = 0 + \gamma 0 + \gamma^ {2} 1 + \gamma^ {3} 1 + \dots = \gamma^ {2} / (1 - \gamma) = 8. 1.
$$

It is clear that the shorter the trajectory is, the greater the return is. Therefore, although the immediate reward of every step does not encourage the agent to approach the target as quickly as possible, the discount rate does encourage it to do so.

A misunderstanding that beginners may have is that adding a negative reward (e.g., $- 1 )$ on top of the rewards obtained for every movement is necessary to encourage the agent to reach the target as quickly as possible. This is a misunderstanding because adding the same reward on top of all rewards is an affine transformation, which preserves the optimal policy. Moreover, optimal policies do not take meaningless detours due to the discount rate, even though a detour may not receive any immediate negative rewards.

# 3.6 Summary

The core concepts in this chapter include optimal policies and optimal state values. In particular, a policy is optimal if its state values are greater than or equal to those of any other policy. The state values of an optimal policy are the optimal state values. The BOE is the core tool for analyzing optimal policies and optimal state values. This equation is a nonlinear equation with a nice contraction property. We can apply the contraction mapping theorem to analyze this equation. It was shown that the solutions of the BOE correspond to the optimal state value and optimal policy. This is the reason why we need to study the BOE.

The contents of this chapter are important for thoroughly understanding many fundamental ideas of reinforcement learning. For example, Theorem 3.3 suggests an iterative algorithm for solving the BOE. This algorithm is exactly the value iteration algorithm that will be introduced in Chapter 4. A further discussion about the BOE can be found in [2].

# 3.7 Q&A

Q: What is the definition of optimal policies?

A: A policy is optimal if its corresponding state values are greater than or equal to any other policy.

It should be noted that this specific definition of optimality is valid only for tabular reinforcement learning algorithms. When the values or policies are approximated by functions, different metrics must be used to define optimal policies. This will become clearer in Chapters 8 and 9.

 Q: Why is the Bellman optimality equation important?

A: It is important because it characterizes both optimal policies and optimal state values. Solving this equation yields an optimal policy and the corresponding optimal state value.

 Q: Is the Bellman optimality equation a Bellman equation?

A: Yes. The Bellman optimality equation is a special Bellman equation whose corresponding policy is optimal.

 Q: Is the solution of the Bellman optimality equation unique?

A: The Bellman optimality equation has two unknown variables. The first unknown variable is a value, and the second is a policy. The value solution, which is the optimal state value, is unique. The policy solution, which is an optimal policy, may not be unique.

 Q: What is the key property of the Bellman optimality equation for analyzing its solution?

A: The key property is that the right-hand side of the Bellman optimality equation is a contraction mapping. As a result, we can apply the contraction mapping theorem to analyze its solution.

Q: Do optimal policies exist?

A: Yes. Optimal policies always exist according to the analysis of the BOE.

 Q: Are optimal policies unique?

A: No. There may exist multiple or infinite optimal policies that have the same optimal state values.

Q: Are optimal policies stochastic or deterministic?

A: An optimal policy can be either deterministic or stochastic. A nice fact is that there always exist deterministic greedy optimal policies.

 Q: How to obtain an optimal policy?

A: Solving the BOE using the iterative algorithm suggested by Theorem 3.3 yields an optimal policy. The detailed implementation of this iterative algorithm will be given in Chapter 4. Notably, all the reinforcement learning algorithms introduced in this book aim to obtain optimal policies under different settings.

 Q: What is the general impact on the optimal policies if we reduce the value of the discount rate?

A: The optimal policy becomes more short-sighted when we reduce the discount rate. That is, the agent does not dare to take risks even though it may obtain greater cumulative rewards afterward.

Q: What happens if we set the discount rate to zero?

A: The resulting optimal policy would become extremely short-sighted. The agent would take the action with the greatest immediate reward, even though that action is not good in the long run.

Q: If we increase all the rewards by the same amount, will the optimal state value change? Will the optimal policy change?

A: Increasing all the rewards by the same amount is an affine transformation of the rewards, which would not affect the optimal policies. However, the optimal state value would increase, as shown in (3.8).

> Q: If we hope that the optimal policy can avoid meaningless detours before reaching the target, should we add a negative reward to every step so that the agent reaches the target as quickly as possible?

A: First, introducing an additional negative reward to every step is an affine transformation of the rewards, which does not change the optimal policy. Second, the discount rate can automatically encourage the agent to reach the target as quickly as possible. This is because meaningless detours would increase the trajectory length and reduce the discounted return.

# Chapter 4

# Value Iteration and Policy Iteration

![](images/277c7a7dc7bae451cd0ee7beaed26787f93c7480338ae9acd510eecd939a2048.jpg)  
Figure 4.1: Where we are in this book.

With the preparation in the previous chapters, we are now ready to present the first algorithms that can find optimal policies. This chapter introduces three algorithms that are closely related to each other. The first is the value iteration algorithm, which is exactly the algorithm suggested by the contraction mapping theorem for solving the Bellman optimality equation as discussed in the last chapter. We focus more on the implementation details of this algorithm in the present chapter. The second is the policy iteration algorithm, whose idea is widely used in reinforcement learning algorithms. The third is the truncated policy iteration algorithm, which is a unified algorithm that includes the value iteration and policy iteration algorithms as special cases.

The algorithms introduced in this chapter are called dynamic programming algorithms [10,11], which require the system model. These algorithms are important foundations of the model-free reinforcement learning algorithms introduced in the subsequent chapters. For example, the Monte Carlo algorithms introduced in Chapter 5 can be immediately obtained by extending the policy iteration algorithm introduced in this chapter.

# 4.1 Value iteration

This section introduces the value iteration algorithm. It is exactly the algorithm suggested by the contraction mapping theorem for solving the Bellman optimality equation, as introduced in the last chapter (Theorem 3.3). In particular, the algorithm is

$$
v _ {k + 1} = \max _ {\pi \in \Pi} (r _ {\pi} + \gamma P _ {\pi} v _ {k}), \quad k = 0, 1, 2, \ldots
$$

It is guaranteed by Theorem 3.3 that $v _ { k }$ and $\pi _ { k }$ converge to the optimal state value and an optimal policy as $k  \infty$ , respectively.

This algorithm is iterative and has two steps in every iteration.

The first step in every iteration is a policy update step. Mathematically, it aims to find a policy that can solve the following optimization problem:

$$
\pi_ {k + 1} = \arg \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v _ {k}),
$$

where $v _ { k }$ is obtained in the previous iteration.

 The second step is called a value update step. Mathematically, it calculates a new value $v _ { k + 1 }$ by

$$
v _ {k + 1} = r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {k}, \tag {4.1}
$$

where $v _ { k + 1 }$ will be used in the next iteration.

The value iteration algorithm introduced above is in a matrix-vector form. To implement this algorithm, we need to further examine its elementwise form. While the matrix-vector form is useful for understanding the core idea of the algorithm, the elementwise form is necessary for explaining the implementation details.

# 4.1.1 Elementwise form and implementation

Consider the time step $k$ and a state $s$ .

 First, the elementwise form of the policy update step $\pi _ { k + 1 } = \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { k } )$ is

$$
\pi_ {k + 1} (s) = \arg \max _ {\pi} \sum_ {a} \pi (a | s) \underbrace {\left(\sum_ {r} p (r | s , a) r + \gamma \sum_ {s ^ {\prime}} p (s ^ {\prime} | s , a) v _ {k} (s ^ {\prime})\right)} _ {q _ {k} (s, a)}, \quad s \in \mathcal {S}.
$$

We showed in Section 3.3.1 that the optimal policy that can solve the above optimization problem is

$$
\pi_ {k + 1} (a | s) = \left\{ \begin{array}{l l} 1, & a = a _ {k} ^ {*} (s), \\ 0, & a \neq a _ {k} ^ {*} (s), \end{array} \right. \tag {4.2}
$$

where $a _ { k } ^ { * } ( s ) = \arg \operatorname* { m a x } _ { a } q _ { k } ( s , a )$ . If $a _ { k } ^ { * } ( s ) = \arg \operatorname* { m a x } _ { a } q _ { k } ( s , a )$ has multiple solutions, we can select any of them without affecting the convergence of the algorithm. Since the new policy $\pi _ { k + 1 }$ selects the action with the greatest $q _ { k } ( s , a )$ , such a policy is called greedy.

 Second, the elementwise form of the value update step $v _ { k + 1 } = r _ { \pi _ { k + 1 } } + \gamma P _ { \pi _ { k + 1 } } v _ { k }$ i s

$$
v _ {k + 1} (s) = \sum_ {a} \pi_ {k + 1} (a | s) \underbrace {\left(\sum_ {r} p (r | s , a) r + \gamma \sum_ {s ^ {\prime}} p (s ^ {\prime} | s , a) v _ {k} (s ^ {\prime})\right)} _ {q _ {k} (s, a)}, \quad s \in \mathcal {S}.
$$

Substituting (4.2) into the above equation gives

$$
v _ {k + 1} (s) = \max  _ {a} q _ {k} (s, a).
$$

In summary, the above steps can be illustrated as

$$
v _ {k} (s) \rightarrow q _ {k} (s, a) \rightarrow \text {n e w g r e e d y p o l i c y} \pi_ {k + 1} (s) \rightarrow \text {n e w v a l u e} v _ {k + 1} (s) = \max  _ {a} q _ {k} (s, a)
$$

The implementation details are summarized in Algorithm 4.1.

One problem that may be confusing is whether $v _ { k }$ in (4.1) is a state value. The answer is no. Although $v _ { k }$ eventually converges to the optimal state value, it is not ensured to satisfy the Bellman equation of any policy. For example, it does not satisfy $v _ { k } = r _ { \pi _ { k + 1 } } + \gamma P _ { \pi _ { k + 1 } } v _ { k }$ or $\boldsymbol { v } _ { k } = \boldsymbol { r } _ { \pi _ { k } } + \gamma \boldsymbol { P } _ { \pi _ { k } } \boldsymbol { v } _ { k }$ in general. It is merely an intermediate value generated by the algorithm. In addition, since $v _ { k }$ is not a state value, $q _ { k }$ is not an action value.

# 4.1.2 Illustrative examples

We next present an example to illustrate the step-by-step implementation of the value iteration algorithm. This example is a two-by-two grid with one forbidden area (Fig-

# Algorithm 4.1: Value iteration algorithm

Initialization: The probability models $p ( r | s , a )$ and $p ( s ^ { \prime } | s , a )$ for all $( s , a )$ are known. Initial guess $v _ { 0 }$ .

Goal: Search for the optimal state value and an optimal policy for solving the Bellman optimality equation.

While $v _ { k }$ has not converged in the sense that $\| v _ { k } - v _ { k - 1 } \|$ is greater than a predefined small threshold, for the kth iteration, do

For every state $s \in S$ , do

For every action $a \in { \mathcal { A } } ( s )$ , do

$$
\mathbf {q} - \text {v a l u e}: q _ {k} (s, a) = \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {k} \left(s ^ {\prime}\right)
$$

Maximum action value: $a _ { k } ^ { * } ( s ) = \arg \operatorname* { m a x } _ { a } q _ { k } ( s , a )$

Policy update: $\pi _ { k + 1 } ( a | s ) = 1$ if $a = a _ { k } ^ { * }$ , and $\pi _ { k + 1 } ( a | s ) = 0$ otherwise

Value update: $v _ { k + 1 } ( s ) = \operatorname* { m a x } _ { a } q _ { k } ( s , a )$

Table 4.1: The expression of $q ( s , a )$ for the example as shown in Figure 4.2.   

<table><tr><td>q-table</td><td>a1</td><td>a2</td><td>a3</td><td>a4</td><td>a5</td></tr><tr><td>s1</td><td>-1 + γv(s1)</td><td>-1 + γv(s2)</td><td>0 + γv(s3)</td><td>-1 + γv(s1)</td><td>0 + γv(s1)</td></tr><tr><td>s2</td><td>-1 + γv(s2)</td><td>-1 + γv(s2)</td><td>1 + γv(s4)</td><td>0 + γv(s1)</td><td>-1 + γv(s2)</td></tr><tr><td>s3</td><td>0 + γv(s1)</td><td>1 + γv(s4)</td><td>-1 + γv(s3)</td><td>-1 + γv(s3)</td><td>0 + γv(s3)</td></tr><tr><td>s4</td><td>-1 + γv(s2)</td><td>-1 + γv(s4)</td><td>-1 + γv(s4)</td><td>0 + γv(s3)</td><td>1 + γv(s4)</td></tr></table>

ure 4.2). The target area is $s _ { 4 }$ . The reward settings are $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1$ and $r _ { \mathrm { t a r g e t } } = 1$ . The discount rate is $\gamma = 0 . 9$ .

![](images/15f15cc6d1ae249cd1fb5eb7dee81f1aed8214ac771a7d6d50c7af5f4cef19bf.jpg)

![](images/bf2fdb9b59e7154e0d4942778f610c38b3af480a768d00ff35140619921e96d1.jpg)

![](images/d93bdf799ccc6202d15216c23145ba1d1c78bb30fa14e211cef9c24561aeb00e.jpg)  
Figure 4.2: An example for demonstrating the implementation of the value iteration algorithm.

The expression of the q-value for each state-action pair is shown in Table 4.1.

$$
\diamond k = 0:
$$

Without loss of generality, select the initial values as $v _ { 0 } ( s _ { 1 } ) = v _ { 0 } ( s _ { 2 } ) = v _ { 0 } ( s _ { 3 } ) =$ $v _ { 0 } ( s _ { 4 } ) = 0$ .

$q$ -value calculation: Substituting $v _ { 0 } ( s _ { i } )$ into Table 4.1 gives the q-values shown in Table 4.2.

Table 4.2: The value of $\boldsymbol { q } ( s , a )$ at $k = 0$   

<table><tr><td>q-table</td><td>a1</td><td>a2</td><td>a3</td><td>a4</td><td>a5</td></tr><tr><td>s1</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td></tr><tr><td>s2</td><td>-1</td><td>-1</td><td>1</td><td>0</td><td>-1</td></tr><tr><td>s3</td><td>0</td><td>1</td><td>-1</td><td>-1</td><td>0</td></tr><tr><td>s4</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>1</td></tr></table>

Table 4.3: The value of $\boldsymbol { q } ( s , a )$ at $k = 1$ .   

<table><tr><td>q-table</td><td>a1</td><td>a2</td><td>a3</td><td>a4</td><td>a5</td></tr><tr><td>s1</td><td>-1 + γ0</td><td>-1 + γ1</td><td>0 + γ1</td><td>-1 + γ0</td><td>0 + γ0</td></tr><tr><td>s2</td><td>-1 + γ1</td><td>-1 + γ1</td><td>1 + γ1</td><td>0 + γ0</td><td>-1 + γ1</td></tr><tr><td>s3</td><td>0 + γ0</td><td>1 + γ1</td><td>-1 + γ1</td><td>-1 + γ1</td><td>0 + γ1</td></tr><tr><td>s4</td><td>-1 + γ1</td><td>-1 + γ1</td><td>-1 + γ1</td><td>0 + γ1</td><td>1 + γ1</td></tr></table>

Policy update: $\pi _ { 1 }$ is obtained by selecting the actions with the greatest q-values for every state:

$$
\pi_ {1} (a _ {5} | s _ {1}) = 1, \quad \pi_ {1} (a _ {3} | s _ {2}) = 1, \quad \pi_ {1} (a _ {2} | s _ {3}) = 1, \quad \pi_ {1} (a _ {5} | s _ {4}) = 1.
$$

This policy is visualized in Figure 4.2 (the middle subfigure). It is clear that this policy is not optimal because it selects to stay still at $s _ { 1 }$ . Notably, the q-values for $( s _ { 1 } , a _ { 5 } )$ and $( s _ { 1 } , a _ { 3 } )$ are actually the same, and we can randomly select either action.

Value update: $v _ { 1 }$ is obtained by updating the v-value to the greatest q-value for each state:

$$
v _ {1} (s _ {1}) = 0, \quad v _ {1} (s _ {2}) = 1, \quad v _ {1} (s _ {3}) = 1, \quad v _ {1} (s _ {4}) = 1.
$$

$k = 1$ :

$q$ -value calculation: Substituting $v _ { 1 } ( s _ { i } )$ into Table 4.1 yields the q-values shown in Table 4.3.

Policy update: $\pi _ { 2 }$ is obtained by selecting the greatest q-values:

$$
\pi_ {2} (a _ {3} | s _ {1}) = 1, \quad \pi_ {2} (a _ {3} | s _ {2}) = 1, \quad \pi_ {2} (a _ {2} | s _ {3}) = 1, \quad \pi_ {2} (a _ {5} | s _ {4}) = 1.
$$

This policy is visualized in Figure 4.2 (the right subfigure).

Value update: $v _ { 2 }$ is obtained by updating the v-value to the greatest q-value for each state:

$$
v _ {2} (s _ {1}) = \gamma 1, \quad v _ {2} (s _ {2}) = 1 + \gamma 1, \quad v _ {2} (s _ {3}) = 1 + \gamma 1, \quad v _ {2} (s _ {4}) = 1 + \gamma 1.
$$

 $k = 2 , 3 , 4 , \dots .$

It is notable that policy $\pi _ { 2 }$ , as illustrated in Figure 4.2, is already optimal. Therefore, we

# 4.2. Policy iteration

only need to run two iterations to obtain an optimal policy in this simple example. For more complex examples, we need to run more iterations until the value of $v _ { k }$ converges (e.g., until $\| v _ { k + 1 } - v _ { k } \|$ is smaller than a pre-specified threshold).

# 4.2 Policy iteration

This section presents another important algorithm: policy iteration. Unlike value iteration, policy iteration is not for directly solving the Bellman optimality equation. However, it has an intimate relationship with value iteration, as shown later. Moreover, the idea of policy iteration is very important since it is widely utilized in reinforcement learning algorithms.

# 4.2.1 Algorithm analysis

Policy iteration is an iterative algorithm. Each iteration has two steps.

 The first is a policy evaluation step. As its name suggests, this step evaluates a given policy by calculating the corresponding state value. That is to solve the following Bellman equation:

$$
v _ {\pi_ {k}} = r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}}, \tag {4.3}
$$

where $\pi _ { k }$ is the policy obtained in the last iteration and ${ \boldsymbol { v } } _ { \pi _ { k } }$ is the state value to be calculated. The values of $r _ { \pi _ { k } }$ and $P _ { \pi _ { k } }$ can be obtained from the system model.

The second is a policy improvement step. As its name suggests, this step is used to improve the policy. In particular, once ${ \boldsymbol { v } } _ { \pi _ { k } }$ has been calculated in the first step, a new policy $\pi _ { k + 1 }$ can be obtained as

$$
\pi_ {k + 1} = \arg \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v _ {\pi_ {k}}).
$$

Three questions naturally follow the above description of the algorithm.

 In the policy evaluation step, how to solve the state value $v _ { \pi _ { k } }$ ?   
In the policy improvement step, why is the new policy $\pi _ { k + 1 }$ better than $\pi _ { k }$ ?   
$\diamond$ Why can this algorithm finally converge to an optimal policy?

We next answer these questions one by one.

# In the policy evaluation step, how to calculate $v _ { \pi _ { k } }$ ?

We introduced two methods in Chapter 2 for solving the Bellman equation in (4.3). We next revisit the two methods briefly. The first method is a closed-form solution:

# 4.2. Policy iteration

$v _ { \pi _ { k } } = ( I - \gamma P _ { \pi _ { k } } ) ^ { - 1 } r _ { \pi _ { k } }$ . This closed-form solution is useful for theoretical analysis purposes, but it is inefficient to implement since it requires other numerical algorithms to compute the matrix inverse. The second method is an iterative algorithm that can be easily implemented:

$$
v _ {\pi_ {k}} ^ {(j + 1)} = r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}} ^ {(j)}, \quad j = 0, 1, 2, \dots \tag {4.4}
$$

where $v _ { \pi _ { k } } ^ { ( j ) }$ denotes the $j$ th estimate of ${ \boldsymbol { v } } _ { \pi _ { k } }$ . Starting from any initial guess vπk , $v _ { \pi _ { k } } ^ { ( 0 ) }$ (0) it is ensured that $v _ { \pi _ { k } } ^ { ( j ) }  v _ { \pi _ { k } }$ as $j \to \infty$ . Details can be found in Section 2.7.

Interestingly, policy iteration is an iterative algorithm with another iterative algorithm (4.4) embedded in the policy evaluation step. In theory, this embedded iterative algorithm requires an infinite number of steps (that is, $j \to \infty$ ) to converge to the true state value $v _ { \pi _ { k } }$ . This is, however, impossible to realize. In practice, the iterative process terminates when a certain criterion is satisfied. For example, the termination criterion can be that $\| v _ { \pi _ { k } } ^ { ( j + 1 ) } - v _ { \pi _ { k } } ^ { ( j ) } \|$ is less than a prespecified threshold or that $j$ exceeds a prespecified value. If we do not run an infinite number of iterations, we can only obtain an imprecise value of $v _ { \pi _ { k } }$ , which will be used in the subsequent policy improvement step. Would this cause problems? The answer is no. The reason will become clear when we introduce the truncated policy iteration algorithm later in Section 4.3.

In the policy improvement step, why is $\pi _ { k + 1 }$ better than $\pi _ { k }$ ?

The policy improvement step can improve the given policy, as shown below.

Lemma 4.1 (Policy improvement). If $\pi _ { k + 1 } = \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { \pi _ { k } } )$ , then $v _ { \pi _ { k + 1 } } \geq v _ { \pi _ { k } }$

Here, $v _ { \pi _ { k + 1 } } \geq v _ { \pi _ { k } }$ means that $v _ { \pi _ { k + 1 } } ( s ) \geq v _ { \pi _ { k } } ( s )$ for all $s$ . The proof of this lemma is given in Box 4.1.

# Box 4.1: Proof of Lemma 4.1

Since $v _ { \pi _ { k + 1 } }$ and $v _ { \pi _ { k } }$ are state values, they satisfy the Bellman equations:

$$
v _ {\pi_ {k + 1}} = r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k + 1}},
$$

$$
v _ {\pi_ {k}} = r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}}.
$$

Since $\pi _ { k + 1 } = \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { \pi _ { k } } )$ , we know that

$$
r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k}} \geq r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}}.
$$

It then follows that

$$
\begin{array}{l} v _ {\pi_ {k}} - v _ {\pi_ {k + 1}} = \left(r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}}\right) - \left(r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k + 1}}\right) \\ \leq \left(r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k}}\right) - \left(r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k + 1}}\right) \\ \leq \gamma P _ {\pi_ {k + 1}} \left(v _ {\pi_ {k}} - v _ {\pi_ {k + 1}}\right). \\ \end{array}
$$

Therefore,

$$
\begin{array}{l} v _ {\pi_ {k}} - v _ {\pi_ {k + 1}} \leq \gamma^ {2} P _ {\pi_ {k + 1}} ^ {2} (v _ {\pi_ {k}} - v _ {\pi_ {k + 1}}) \leq \ldots \leq \gamma^ {n} P _ {\pi_ {k + 1}} ^ {n} (v _ {\pi_ {k}} - v _ {\pi_ {k + 1}}) \\ \leq \lim  _ {n \rightarrow \infty} \gamma^ {n} P _ {\pi_ {k + 1}} ^ {n} (v _ {\pi_ {k}} - v _ {\pi_ {k + 1}}) = 0. \\ \end{array}
$$

The limit is due to the facts that $\gamma ^ { \pi } \to 0$ as $n  \infty$ a n d P n $P _ { \pi _ { k + 1 } } ^ { n }$ is a nonnegative πk+1 stochastic matrix for any $n$ . Here, a stochastic matrix refers to a nonnegative matrix whose row sums are equal to one for all rows.

# Why can the policy iteration algorithm eventually find an optimal policy?

The policy iteration algorithm generates two sequences. The first is a sequence of policies: $\left\{ \pi _ { 0 } , \pi _ { 1 } , \ldots , \pi _ { k } , \ldots \right\}$ . The second is a sequence of state values: $\{ v _ { \pi _ { 0 } } , v _ { \pi _ { 1 } } , \ldots , v _ { \pi _ { k } } , \ldots \}$ . Suppose that $v ^ { * }$ is the optimal state value. Then, $v _ { \pi _ { k } } \le v ^ { * }$ for all $k$ . Since the policies are continuously improved according to Lemma 4.1, we know that

$$
v _ {\pi_ {0}} \leq v _ {\pi_ {1}} \leq v _ {\pi_ {2}} \leq \dots \leq v _ {\pi_ {k}} \leq \dots \leq v ^ {*}.
$$

Since ${ \boldsymbol { v } } _ { \pi _ { k } }$ is nondecreasing and always bounded from above by $v ^ { * }$ , it follows from the monotone convergence theorem [12] (Appendix C) that $v _ { \pi _ { k } }$ converges to a constant value, denoted as $v _ { \infty }$ , when $k  \infty$ . The following analysis shows that $v _ { \infty } = v ^ { * }$ .

Theorem 4.1 (Convergence of policy iteration). The state value sequence $\{ v _ { \pi _ { k } } \} _ { k = 0 } ^ { \infty }$ generated by the policy iteration algorithm converges to the optimal state value $v ^ { * }$ . As a result, the policy sequence $\{ \pi _ { k } \} _ { k = 0 } ^ { \infty }$ converges to an optimal policy.

The proof of this theorem is given in Box 4.2. The proof not only shows the convergence of the policy iteration algorithm but also reveals the relationship between the policy iteration and value iteration algorithms. Loosely speaking, if both algorithms start from the same initial guess, policy iteration will converge faster than value iteration due to the additional iterations embedded in the policy evaluation step. This point will become clearer when we introduce the truncated policy iteration algorithm in Section 4.3.

# Box 4.2: Proof of Theorem 4.1

The idea of the proof is to show that the policy iteration algorithm converges faster than the value iteration algorithm.

In particular, to prove the convergence of $\{ v _ { \pi _ { k } } \} _ { k = 0 } ^ { \infty }$ , we introduce another sequence $\{ v _ { k } \} _ { k = 0 } ^ { \infty }$ generated by

$$
v _ {k + 1} = f (v _ {k}) = \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v _ {k}).
$$

This iterative algorithm is exactly the value iteration algorithm. We already know that $v _ { k }$ converges to $v ^ { * }$ when given any initial value $v _ { 0 }$ .

For $k = 0$ , we can always find a $v _ { 0 }$ such that $v _ { \pi _ { 0 } } \geq v _ { 0 }$ for any $\pi _ { 0 }$

We next show that $v _ { k } \le v _ { \pi _ { k } } \le v ^ { * }$ for all $k$ by induction.

For $k \geq 0$ , suppose that $v _ { \pi _ { k } } \geq v _ { k }$

For $k + 1$ , we have

$$
\begin{array}{l} v _ {\pi_ {k + 1}} - v _ {k + 1} = \left(r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k + 1}}\right) - \max  _ {\pi} \left(r _ {\pi} + \gamma P _ {\pi} v _ {k}\right) \\ \geq \left(r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k}}\right) - \max  _ {\pi} \left(r _ {\pi} + \gamma P _ {\pi} v _ {k}\right) \\ (b e c a u s e v _ {\pi_ {k + 1}} \geq v _ {\pi_ {k}} b y L e m m a 4. 1 a n d P _ {\pi_ {k + 1}} \geq 0) \\ = \left(r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {\pi_ {k}}\right) - \left(r _ {\pi_ {k} ^ {\prime}} + \gamma P _ {\pi_ {k} ^ {\prime}} v _ {k}\right) \\ \left(\text {s u p p o s e} \pi_ {k} ^ {\prime} = \arg \max  _ {\pi} \left(r _ {\pi} + \gamma P _ {\pi} v _ {k}\right)\right) \\ \geq \left(r _ {\pi_ {k} ^ {\prime}} + \gamma P _ {\pi_ {k} ^ {\prime}} v _ {\pi_ {k}}\right) - \left(r _ {\pi_ {k} ^ {\prime}} + \gamma P _ {\pi_ {k} ^ {\prime}} v _ {k}\right) \\ \left(\text {b e c a u s e} \pi_ {k + 1} = \arg \max  _ {\pi} \left(r _ {\pi} + \gamma P _ {\pi} v _ {\pi_ {k}}\right)\right) \\ = \gamma P _ {\pi_ {k} ^ {\prime}} \left(v _ {\pi_ {k}} - v _ {k}\right). \\ \end{array}
$$

Since $v _ { \pi _ { k } } \mathrm { ~ - ~ } v _ { k } \geq 0$ and $P _ { \pi _ { k } ^ { \prime } }$ is nonnegative, we have $P _ { \pi _ { k } ^ { \prime } } ( v _ { \pi _ { k } } - v _ { k } ) \ge 0$ and hence $v _ { \pi _ { k + 1 } } - v _ { k + 1 } \geq 0$ .

Therefore, we can show by induction that $v _ { k } \le v _ { \pi _ { k } } \le v ^ { * }$ for any $k \geq 0$ . Since $v _ { k }$ converges to $v ^ { * }$ , $v _ { \pi _ { k } }$ also converges to $v ^ { * }$ .

# 4.2.2 Elementwise form and implementation

To implement the policy iteration algorithm, we need to study its elementwise form.

 First, the policy evaluation step solves $v _ { \pi _ { k } }$ from $\boldsymbol { v } _ { \pi _ { k } } = \boldsymbol { r } _ { \pi _ { k } } + \gamma P _ { \pi _ { k } } \boldsymbol { v } _ { \pi _ { k } }$ by using the

# Algorithm 4.2: Policy iteration algorithm

Initialization: The system model, $p ( r | s , a )$ and $p ( s ^ { \prime } | s , a )$ for all $( s , a )$ , is known. Initial guess $\pi _ { 0 }$ .

Goal: Search for the optimal state value and an optimal policy.

While $v _ { \pi _ { k } }$ has not converged, for the $k$ th iteration, do

Policy evaluation:

Initialization: an arbitrary initial guess vπk $v _ { \pi _ { k } } ^ { ( 0 ) }$

While vπk $v _ { \pi _ { k } } ^ { ( j ) }$ has not converged, for the $j$ th iteration, do

For every state $s \in S$ , do

$$
v _ {\pi_ {k}} ^ {(j + 1)} (s) = \sum_ {a} \pi_ {k} (a | s) \left[ \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi_ {k}} ^ {(j)} \left(s ^ {\prime}\right) \right]
$$

Policy improvement:

For every state $s \in S$ , do

For every action $a \in { \mathcal { A } }$ , do

$$
q _ {\pi_ {k}} (s, a) = \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi_ {k}} \left(s ^ {\prime}\right)
$$

$a _ { k } ^ { * } ( s ) = \arg \operatorname* { m a x } _ { a } q _ { \pi _ { k } } ( s , a )$

$$
\pi_ {k + 1} (a | s) = 1 \text {i f} a = a _ {k} ^ {*}, \text {a n d} \pi_ {k + 1} (a | s) = 0 \text {o t h e r w i s e}
$$

iterative algorithm in (4.4). The elementwise form of this algorithm is

$$
v _ {\pi_ {k}} ^ {(j + 1)} (s) = \sum_ {a} \pi_ {k} (a | s) \left(\sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi_ {k}} ^ {(j)} \left(s ^ {\prime}\right)\right), \quad s \in \mathcal {S},
$$

where $j = 0 , 1 , 2 , \ldots$

 Second, the policy improvement step solves $\pi _ { k + 1 } = \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { \pi _ { k } } )$ . The elementwise form of this equation is

$$
\pi_ {k + 1} (s) = \arg \max _ {\pi} \sum_ {a} \pi (a | s) \underbrace {\left(\sum_ {r} p (r | s , a) r + \gamma \sum_ {s ^ {\prime}} p (s ^ {\prime} | s , a) v _ {\pi_ {k}} (s ^ {\prime})\right)} _ {q _ {\pi_ {k}} (s, a)}, \quad s \in \mathcal {S},
$$

where $q _ { \pi _ { k } } ( s , a )$ is the action value under policy $\pi _ { k }$ . Let $a _ { k } ^ { * } ( s ) = \arg \operatorname* { m a x } _ { a } q _ { \pi _ { k } } ( s , a )$ . Then, the greedy optimal policy is

$$
\pi_ {k + 1} (a | s) = \left\{ \begin{array}{l l} 1, & a = a _ {k} ^ {*} (s), \\ 0, & a \neq a _ {k} ^ {*} (s). \end{array} \right.
$$

The implementation details are summarized in Algorithm 4.2.

# 4.2.3 Illustrative examples

# A simple example

Consider a simple example shown in Figure 4.3. There are two states with three possible actions: $\mathcal { A } = \{ a _ { \ell } , a _ { 0 } , a _ { r } \}$ . The three actions represent moving leftward, staying unchanged, and moving rightward. The reward settings are $r _ { \mathrm { b o u n d a r y } } = - 1$ and $r _ { \mathrm { t a r g e t } } = 1$ . The discount rate is $\gamma = 0 . 9$ .

![](images/39937ddd35b42c600844df506138317949dc8dc57f8abe88399fe47df223fecd.jpg)  
(a)

![](images/7a29c211b9267d791a097b3c08a28964b5dc3a199f8fe74bf5345147972abcd4.jpg)  
(b)   
Figure 4.3: An example for illustrating the implementation of the policy iteration algorithm.

We next present the implementation of the policy iteration algorithm in a step-by-step manner. When $k = 0$ , we start with the initial policy shown in Figure 4.3(a). This policy is not good because it does not move toward the target area. We next show how to apply the policy iteration algorithm to obtain an optimal policy.

 First, in the policy evaluation step, we need to solve the Bellman equation:

$$
v _ {\pi_ {0}} (s _ {1}) = - 1 + \gamma v _ {\pi_ {0}} (s _ {1}),
$$

$$
v _ {\pi_ {0}} (s _ {2}) = 0 + \gamma v _ {\pi_ {0}} (s _ {1}).
$$

Since the equation is simple, it can be manually solved that

$$
v _ {\pi_ {0}} (s _ {1}) = - 1 0, \quad v _ {\pi_ {0}} (s _ {2}) = - 9.
$$

In practice, the equation can be solved by the iterative algorithm in (4.4). For example, select the initial state values as $v _ { \pi _ { 0 } } ^ { ( 0 ) } ( s _ { 1 } ) = v _ { \pi _ { 0 } } ^ { ( 0 ) } ( s _ { 2 } ) = 0$ . It follows from (4.4) that

$$
\left\{ \begin{array}{l l} v _ {\pi_ {0}} ^ {(1)} (s _ {1}) = - 1 + \gamma v _ {\pi_ {0}} ^ {(0)} (s _ {1}) = - 1, \\ v _ {\pi_ {0}} ^ {(1)} (s _ {2}) = 0 + \gamma v _ {\pi_ {0}} ^ {(0)} (s _ {1}) = 0, \end{array} \right.
$$

$$
\left\{ \begin{array}{l} v _ {\pi_ {0}} ^ {(2)} (s _ {1}) = - 1 + \gamma v _ {\pi_ {0}} ^ {(1)} (s _ {1}) = - 1. 9, \\ v _ {\pi_ {0}} ^ {(2)} (s _ {2}) = 0 + \gamma v _ {\pi_ {0}} ^ {(1)} (s _ {1}) = - 0. 9, \end{array} \right.
$$

$$
\left\{ \begin{array}{l} v _ {\pi_ {0}} ^ {(3)} (s _ {1}) = - 1 + \gamma v _ {\pi_ {0}} ^ {(2)} (s _ {1}) = - 2. 7 1, \\ v _ {\pi_ {0}} ^ {(3)} (s _ {2}) = 0 + \gamma v _ {\pi_ {0}} ^ {(2)} (s _ {1}) = - 1. 7 1, \end{array} \right.
$$

With more iterations, we can see the trend: $v _ { \pi _ { 0 } } ^ { ( j ) } ( s _ { 1 } ) \to v _ { \pi _ { 0 } } ( s _ { 1 } ) = - 1 0$ and $v _ { \pi _ { 0 } } ^ { ( j ) } ( s _ { 2 } ) $ $v _ { \pi _ { 0 } } ( s _ { 2 } ) = - 9$ as $j$ increases.

Second, in the policy improvement step, the key is to calculate $q _ { \pi _ { 0 } } ( s , a )$ for each state-action pair. The following q-table can be used to demonstrate such a process:

Table 4.4: The expression of $q _ { \pi _ { k } } ( s , a )$ for the example in Figure 4.3.   

<table><tr><td>qπk(s,a)</td><td>aℓ</td><td>a0</td><td>ar</td></tr><tr><td>s1</td><td>-1 + γvπk(s1)</td><td>0 + γvπk(s1)</td><td>1 + γvπk(s2)</td></tr><tr><td>s2</td><td>0 + γvπk(s1)</td><td>1 + γvπk(s2)</td><td>-1 + γvπk(s2)</td></tr></table>

Substituting $v _ { \pi _ { 0 } } ( s _ { 1 } ) = - 1 0 , v _ { \pi _ { 0 } } ( s _ { 2 } ) = - 9$ obtained in the previous policy evaluation step into Table 4.4 yields Table 4.5.

Table 4.5: The value of $q _ { \pi _ { k } } ( s , a )$ when $k = 0$ .   

<table><tr><td>qπ0(s,a)</td><td>aℓ</td><td>a0</td><td>ar</td></tr><tr><td>s1</td><td>-10</td><td>-9</td><td>-7.1</td></tr><tr><td>s2</td><td>-9</td><td>-7.1</td><td>-9.1</td></tr></table>

By seeking the greatest value of $q _ { \pi _ { 0 } }$ , the improved policy $\pi _ { 1 }$ can be obtained as

$$
\pi_ {1} (a _ {r} | s _ {1}) = 1, \quad \pi_ {1} (a _ {0} | s _ {2}) = 1.
$$

This policy is illustrated in Figure 4.3(b). It is clear that this policy is optimal.

The above process shows that a single iteration is sufficient for finding the optimal policy in this simple example. More iterations are required for more complex examples.

# A more complicated example

We next demonstrate the policy iteration algorithm using a more complicated example shown in Figure 4.4. The reward settings are $r _ { \mathrm { b o u n d a r y } } = - 1$ , $r _ { \mathrm { f o r b i d d e n } } = - 1 0$ , and $r _ { \mathrm { t a r g e t } } =$ 1. The discount rate is $\gamma = 0 . 9$ . The policy iteration algorithm can converge to the optimal policy (Figure 4.4(h)) when starting from a random initial policy (Figure 4.4(a)).

Two interesting phenomena are observed during the iteration process.

First, if we observe how the policy evolves, an interesting pattern is that the states that are close to the target area find the optimal policies earlier than those far away. Only if the close states can find trajectories to the target first, can the farther states find trajectories passing through the close states to reach the target.   
Second, the spatial distribution of the state values exhibits an interesting pattern: the states that are located closer to the target have greater state values. The reason for this pattern is that an agent starting from a farther state must travel for many steps to obtain a positive reward. Such rewards would be severely discounted and hence relatively small.

![](images/beeec6f16514dc31394835e4ac012e5293429d4199bf1673d0b531353f8f838a.jpg)  
(a) $\pi _ { 0 }$ and $v _ { \pi _ { 0 } }$

![](images/8975da66b72d761391747f0ec0ee24c04ea2895486ad0a255480b4d6c0b28212.jpg)

![](images/5214937365c183e468974633b6df5c86ce2307407ed45e6fea50b704f2a28f54.jpg)

![](images/45b8cdbbad7287286eb3643ef87330bc31af872ec86c39f9acf5addbecc87193.jpg)  
(b) $\pi _ { 1 }$ and $v _ { \pi _ { 1 } }$

![](images/f78565aedec6c0be0f7c4dd7c0c7143dea98f9b2b5c9fad0a0e9432a08947acf.jpg)  
(c) $\pi _ { 2 }$ and $v _ { \pi _ { 2 } }$

![](images/5e4cc3eb6c59a282e128535f9a8fa16997cded37cee642ea0ce6fc729f4a0076.jpg)

![](images/88c219ab741df1d1643d97abb414d2cc628a979bde6e8cb7490277e1caa77338.jpg)

![](images/1790e7097edf12d421dd82b82daf5083c944c59bf43be7c9d4ae99e54ddcbf35.jpg)  
(d) $\pi _ { 3 }$ and $v _ { \pi _ { 3 } }$

![](images/a97468b9eaa6a76d0ed8023f8bb2527cebe900d4ea5db74f2013116d0e08ec09.jpg)  
(e) $\pi _ { 4 }$ and $v _ { \pi _ { 4 } }$

![](images/1e8523d50ce3e1145c08aa9ef72dbb1147c2c8c94934ff99b6c8e4808f5e4341.jpg)

![](images/05b61c19aab328e63d6c6169ce14d05674e4c65955aa43f924b423d51336caef.jpg)

![](images/69d562c8bbed153ab19b290128155d846f827e91ee672ee9e264c41f469b7392.jpg)  
(f) $\pi _ { 5 }$ and ${ \boldsymbol { v } } _ { \pi _ { 5 } }$

![](images/dcab50b1aa857cb76b0fcc66b50e3c0c19076ea73d0ad67e044018009fff2f47.jpg)  
(g) $\pi _ { 9 }$ and ${ \boldsymbol { v } } _ { \pi _ { 9 } }$

![](images/470b9777daab8e8354190c25222f62f5b2d21a9e28d526b4c4adbf17a0d3a62b.jpg)

![](images/408810d7704284eb1d64c4adada369a52f2a58b69b1cd782c5d06e9c644c5c38.jpg)

![](images/3bed573de49c395380b8a55b3d163a8a95b2a3036da7f047bfba17776b04dac7.jpg)  
(h) $\pi _ { 1 0 }$ and $v _ { \pi _ { 1 0 } }$   
Figure 4.4: The evolution processes of the policies generated by the policy iteration algorithm.

# 4.3 Truncated policy iteration

We next introduce a more general algorithm called truncated policy iteration. We will see that the value iteration and policy iteration algorithms are two special cases of the truncated policy iteration algorithm.

# 4.3.1 Comparing value iteration and policy iteration

First of all, we compare the value iteration and policy iteration algorithms by listing their steps as follows.

 Policy iteration: Select an arbitrary initial policy $\pi _ { 0 }$ . In the $k$ th iteration, do the following two steps.

- Step 1: Policy evaluation (PE). Given $\pi _ { k }$ , solve $v _ { \pi _ { k } }$ from

$$
v _ {\pi_ {k}} = r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}}.
$$

- Step 2: Policy improvement (PI). Given $v _ { \pi _ { k } }$ , solve $\pi _ { k + 1 }$ from

$$
\pi_ {k + 1} = \arg \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v _ {\pi_ {k}}).
$$

Value iteration: Select an arbitrary initial value $v _ { 0 }$ . In the $k$ th iteration, do the following two steps.

- Step 1: Policy update (PU). Given $v _ { k }$ , solve $\pi _ { k + 1 }$ from

$$
\pi_ {k + 1} = \arg \max _ {\pi} (r _ {\pi} + \gamma P _ {\pi} v _ {k}).
$$

- Step 2: Value update (VU). Given $\pi _ { k + 1 }$ , solve $v _ { k + 1 }$ from

$$
v _ {k + 1} = r _ {\pi_ {k + 1}} + \gamma P _ {\pi_ {k + 1}} v _ {k}.
$$

The above steps of the two algorithms can be illustrated as

→vπ0 P I π vπ1 P I π2 vπ2

It can be seen that the procedures of the two algorithms are very similar.

We examine their value steps more closely to see the difference between the two algorithms. In particular, let both algorithms start from the same initial condition: $v _ { 0 } ~ = ~ v _ { \pi _ { 0 } }$ . The procedures of the two algorithms are listed in Table 4.6. In the first three steps, the two algorithms generate the same results since $v _ { 0 } = v _ { \pi _ { 0 } }$ . They become

Table 4.6: A comparison between the implementation steps of policy iteration and value iteration.   

<table><tr><td></td><td>Policy iteration algorithm</td><td>Value iteration algorithm</td><td>Comments</td></tr><tr><td>1) Policy:</td><td>π0</td><td>N/A</td><td></td></tr><tr><td>2) Value:</td><td>vπ0 = rπ0 + γPπ0 vπ0</td><td>v0 ÷ vπ0</td><td></td></tr><tr><td>3) Policy:</td><td>π1 = arg maxπ(rπ + γPπ vπ0)</td><td>π1 = arg maxπ(rπ + γPπ v0)</td><td>The two policies are the same</td></tr><tr><td>4) Value:</td><td>vπ1 = rπ1 + γPπ1 vπ1</td><td>v1 = rπ1 + γPπ1 v0</td><td>vπ1 ≥ v1 since vπ1 ≥ vπ0</td></tr><tr><td>5) Policy:</td><td>π2 = arg maxπ(rπ + γPπ vπ1)</td><td>π2&#x27; = arg maxπ(rπ + γPπ v1)</td><td></td></tr><tr><td>:</td><td>:</td><td>:</td><td>:</td></tr></table>

different in the fourth step. During the fourth step, the value iteration algorithm executes $v _ { 1 } = r _ { \pi _ { 1 } } + \gamma P _ { \pi _ { 1 } } v _ { 0 }$ , which is a one-step calculation, whereas the policy iteration algorithm solves $v _ { \pi _ { 1 } } = r _ { \pi _ { 1 } } + \gamma P _ { \pi _ { 1 } } v _ { \pi _ { 1 } }$ , which requires an infinite number of iterations. If we explicitly write out the iterative process for solving $v _ { \pi _ { 1 } } = r _ { \pi _ { 1 } } + \gamma P _ { \pi _ { 1 } } v _ { \pi _ { 1 } }$ in the fourth step, everything becomes clear. By letting $v _ { \pi _ { 1 } } ^ { ( 0 ) } = v _ { 0 }$ , we have

$$
\begin{array}{r l} & v _ {\pi_ {1}} ^ {(0)} = v _ {0} \\ \mathrm {v a l u e i t e r a t i o n} \leftarrow v _ {1} \longleftarrow & v _ {\pi_ {1}} ^ {(1)} = r _ {\pi_ {1}} + \gamma P _ {\pi_ {1}} v _ {\pi_ {1}} ^ {(0)} \\ & v _ {\pi_ {1}} ^ {(2)} = r _ {\pi_ {1}} + \gamma P _ {\pi_ {1}} v _ {\pi_ {1}} ^ {(1)} \\ & \vdots \\ \mathrm {t r u n c a t e d p o l i c y i t e r a t i o n} \leftarrow \bar {v} _ {1} \longleftarrow & v _ {\pi_ {1}} ^ {(j)} = r _ {\pi_ {1}} + \gamma P _ {\pi_ {1}} v _ {\pi_ {1}} ^ {(j - 1)} \\ & \vdots \\ \mathrm {p o l i c y i t e r a t i o n} \leftarrow v _ {\pi_ {1}} \longleftarrow & v _ {\pi_ {1}} ^ {(\infty)} = r _ {\pi_ {1}} + \gamma P _ {\pi_ {1}} v _ {\pi_ {1}} ^ {(\infty)} \end{array}
$$

The following observations can be obtained from the above process.

If the iteration is run only once, then $v _ { \pi _ { 1 } } ^ { ( 1 ) }$ is actually $v _ { 1 }$ , as calculated in the value iteration algorithm.   
If the iteration is run an infinite number of times, then $v _ { \pi _ { 1 } } ^ { ( \infty ) }$ is actually $\scriptstyle v _ { \pi _ { 1 } }$ , as calculated in the policy iteration algorithm.   
 If the iteration is run a finite number of times (denoted as $j _ { \mathrm { t r u n c a t e } }$ ), then such an algorithm is called truncated policy iteration. It is called truncated because the remaining iterations from jtruncate to $\infty$ are truncated.

As a result, the value iteration and policy iteration algorithms can be viewed as two extreme cases of the truncated policy iteration algorithm: value iteration terminates

Algorithm 4.3: Truncated policy iteration algorithm   
Initialization: The probability models $p(r|s,a)$ and $p(s'|s,a)$ for all $(s,a)$ are known. Initial guess $\pi_0$ .  
Goal: Search for the optimal state value and an optimal policy.  
While $v_k$ has not converged, for the $k$ th iteration, do  
Policy evaluation:  
Initialization: select the initial guess as $v_k^{(0)} = v_{k-1}$ . The maximum number of iterations is set as $j_{\text{truncated}}$ .  
While $j < j_{\text{truncated}}$ , do  
For every state $s \in S$ , do $v_k^{(j+1)}(s) = \sum_a \pi_k(a|s) \left[ \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k^{(j)}(s') \right]$ Set $v_k = v_k^{(j_{\text{truncated}})}$ Policy improvement:  
For every state $s \in S$ , do  
For every action $a \in \mathcal{A}(s)$ , do $q_k(s,a) = \sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_k(s')$ $a_k^*(s) = \arg \max_a q_k(s,a)$ $\pi_{k+1}(a|s) = 1$ if $a = a_k^*$ , and $\pi_{k+1}(a|s) = 0$ otherwise

at $j _ { \mathrm { t r u n c a t e } } = 1$ , and policy iteration terminates at $j _ { \mathrm { t r u n c a t e } } = \infty$ . It should be noted that, although the above comparison is illustrative, it is based on the condition that $v _ { \pi _ { 1 } } ^ { ( 0 ) } = v _ { 0 } = v _ { \pi _ { 0 } }$ . The two algorithms cannot be directly compared without this condition.

# 4.3.2 Truncated policy iteration algorithm

In a nutshell, the truncated policy iteration algorithm is the same as the policy iteration algorithm except that it merely runs a finite number of iterations in the policy evaluation step. Its implementation details are summarized in Algorithm 4.3. It is notable that $v _ { k }$ and $v _ { k } ^ { ( j ) }$ in the algorithm are not state values. Instead, they are approximations of the true state values because only a finite number of iterations are executed in the policy evaluation step.

If $v _ { k }$ does not equal ${ \boldsymbol { v } } _ { \pi _ { k } }$ , will the algorithm still be able to find optimal policies? The answer is yes. Intuitively, truncated policy iteration is in between value iteration and policy iteration. On the one hand, it converges faster than the value iteration algorithm because it computes more than one iteration during the policy evaluation step. On the other hand, it converges slower than the policy iteration algorithm because it only computes a finite number of iterations. This intuition is illustrated in Figure 4.5. Such intuition is also supported by the following analysis.

Proposition 4.1 (Value improvement). Consider the iterative algorithm in the policy

![](images/2081f682c9877d7a886cd84383b059775953ffa2adabcb3b8d1c704473bf498e.jpg)  
Figure 4.5: An illustration of the relationships between the value iteration, policy iteration, and truncated policy iteration algorithms.

evaluation step:

$$
v _ {\pi_ {k}} ^ {(j + 1)} = r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}} ^ {(j)}, \quad j = 0, 1, 2, \ldots
$$

If the initial guess is selected as $v _ { \pi _ { k } } ^ { ( 0 ) } = v _ { \pi _ { k - 1 } }$ , it holds that

$$
v _ {\pi_ {k}} ^ {(j + 1)} \geq v _ {\pi_ {k}} ^ {(j)}
$$

for $j = 0 , 1 , 2 , \ldots$ .

# Box 4.3: Proof of Proposition 4.1

First, since $v _ { \pi _ { k } } ^ { ( j ) } = r _ { \pi _ { k } } + \gamma P _ { \pi _ { k } } v _ { \pi _ { k } } ^ { ( j - 1 ) }$ ) and v(j+πk $\begin{array} { r } { v _ { \pi _ { k } } ^ { ( j + 1 ) } = r _ { \pi _ { k } } + \gamma P _ { \pi _ { k } } v _ { \pi _ { k } } ^ { ( j ) } } \end{array}$ , we have

$$
v _ {\pi_ {k}} ^ {(j + 1)} - v _ {\pi_ {k}} ^ {(j)} = \gamma P _ {\pi_ {k}} (v _ {\pi_ {k}} ^ {(j)} - v _ {\pi_ {k}} ^ {(j - 1)}) = \dots = \gamma^ {j} P _ {\pi_ {k}} ^ {j} (v _ {\pi_ {k}} ^ {(1)} - v _ {\pi_ {k}} ^ {(0)}). \tag {4.5}
$$

Second, since $v _ { \pi _ { k } } ^ { ( 0 ) } = v _ { \pi _ { k - 1 } }$ vπ , we have

$$
v _ {\pi_ {k}} ^ {(1)} = r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k}} ^ {(0)} = r _ {\pi_ {k}} + \gamma P _ {\pi_ {k}} v _ {\pi_ {k - 1}} \geq r _ {\pi_ {k - 1}} + \gamma P _ {\pi_ {k - 1}} v _ {\pi_ {k - 1}} = v _ {\pi_ {k - 1}} = v _ {\pi_ {k}} ^ {(0)},
$$

where the inequality is due to $\pi _ { k } = \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { \pi _ { k - 1 } } )$ . Substituting $v _ { \pi _ { k } } ^ { ( 1 ) } \geq v _ { \pi _ { k } } ^ { ( 0 ) }$ vπ into (4.5) yields $v _ { \pi _ { k } } ^ { ( j + 1 ) } \geq v _ { \pi _ { k } } ^ { ( j ) }$ vπk . (j)

Notably, Proposition 4.1 requires the assumption that $v _ { \pi _ { k } } ^ { ( 0 ) } = v _ { \pi _ { k - 1 } }$ . However, $v _ { \pi _ { k - 1 } }$ is unavailable in practice, and only $v _ { k - 1 }$ is available. Nevertheless, Proposition 4.1 still sheds light on the convergence of the truncated policy iteration algorithm. A more in-depth discussion of this topic can be found in [2, Section 6.5].

Up to now, the advantages of truncated policy iteration are clear. Compared to the

policy iteration algorithm, the truncated one merely requires a finite number of iterations in the policy evaluation step and hence is more computationally efficient. Compared to value iteration, the truncated policy iteration algorithm can speed up its convergence rate by running for a few more iterations in the policy evaluation step.

# 4.4 Summary

This chapter introduced three algorithms that can be used to find optimal policies.

Value iteration: The value iteration algorithm is the same as the algorithm suggested by the contraction mapping theorem for solving the Bellman optimality equation. It can be decomposed into two steps: value update and policy update.   
 Policy iteration: The policy iteration algorithm is slightly more complicated than the value iteration algorithm. It also contains two steps: policy evaluation and policy improvement.   
Truncated policy iteration: The value iteration and policy iteration algorithms can be viewed as two extreme cases of the truncated policy iteration algorithm.

A common property of the three algorithms is that every iteration has two steps. One step is to update the value, and the other step is to update the policy. The idea of interaction between value and policy updates widely exists in reinforcement learning algorithms. This idea is also called generalized policy iteration [3].

Finally, the algorithms introduced in this chapter require the system model. Starting in Chapter 5, we will study model-free reinforcement learning algorithms. We will see that the model-free can be obtained by extending the algorithms introduced in this chapter.

# 4.5 Q&A

 Q: Is the value iteration algorithm guaranteed to find optimal policies?

A: Yes. This is because value iteration is exactly the algorithm suggested by the contraction mapping theorem for solving the Bellman optimality equation in the last chapter. The convergence of this algorithm is guaranteed by the contraction mapping theorem.

Q: Are the intermediate values generated by the value iteration algorithm state values?

A: No. These values are not guaranteed to satisfy the Bellman equation of any policy.

 Q: What steps are included in the policy iteration algorithm?

A: Each iteration of the policy iteration algorithm contains two steps: policy evaluation and policy improvement. In the policy evaluation step, the algorithm aims to solve the Bellman equation to obtain the state value of the current policy. In the

policy improvement step, the algorithm aims to update the policy so that the newly generated policy has greater state values.

 Q: Is another iterative algorithm embedded in the policy iteration algorithm?

A: Yes. In the policy evaluation step of the policy iteration algorithm, an iterative algorithm is required to solve the Bellman equation of the current policy.

 Q: Are the intermediate values generated by the policy iteration algorithm state values?

A: Yes. This is because these values are the solutions of the Bellman equation of the current policy.

 Q: Is the policy iteration algorithm guaranteed to find optimal policies?

A: Yes. We have presented a rigorous proof of its convergence in this chapter.

Q: What is the relationship between the truncated policy iteration and policy iteration algorithms?

A: As its name suggests, the truncated policy iteration algorithm can be obtained from the policy iteration algorithm by simply executing a finite number of iterations during the policy evaluation step.

 Q: What is the relationship between truncated policy iteration and value iteration?

A: Value iteration can be viewed as an extreme case of truncated policy iteration, where a single iteration is run during the policy evaluation step.

 Q: Are the intermediate values generated by the truncated policy iteration algorithm state values?

A: No. Only if we run an infinite number of iterations in the policy evaluation step, can we obtain true state values. If we run a finite number of iterations, we can only obtain approximates of the true state values.

Q: How many iterations should we run in the policy evaluation step of the truncated policy iteration algorithm?

A: The general guideline is to run a few iterations but not too many. The use of a few iterations in the policy evaluation step can speed up the overall convergence rate, but running too many iterations would not significantly speed up the convergence rate.

 Q: What is generalized policy iteration?

A: Generalized policy iteration is not a specific algorithm. Instead, it refers to the general idea of the interaction between value and policy updates. This idea is rooted in the policy iteration algorithm. Most of the reinforcement learning algorithms introduced in this book fall into the scope of generalized policy iteration.

Q: What are model-based and model-free reinforcement learning?

A: Although the algorithms introduced in this chapter can find optimal policies, they are usually called dynamic programming algorithms rather than reinforcement learning algorithms because they require the system model. Reinforcement learning algorithms can be classified into two categories: model-based and model-free. Here, “model-based” does not refer to the requirement of the system model. Instead, modelbased reinforcement learning uses data to estimate the system model and uses this model during the learning process. By contrast, model-free reinforcement learning does not involve model estimation during the learning process. More information about model-based reinforcement learning can be found in [13–16].

# Chapter 5

# Monte Carlo Methods

![](images/4961397cdc15c1b783b02b30c1c9caf50655dfbda0b35580c84d8dab74c3a72c.jpg)  
Figure 5.1: Where we are in this book.

In the previous chapter, we introduced algorithms that can find optimal policies based on the system model. In this chapter, we start introducing model-free reinforcement learning algorithms that do not presume system models.

While this is the first time we introduce model-free algorithms in this book, we must fill a knowledge gap: how can we find optimal policies without models? The philosophy is simple: If we do not have a model, we must have some data. If we do not have data, we must have a model. If we have neither, then we are not able to find optimal policies. The “data” in reinforcement learning usually refers to the agent’s interaction experiences with the environment.

To demonstrate how to learn from data rather than a model, we start this chapter by introducing the mean estimation problem, where the expected value of a random variable is estimated from some samples. Understanding this problem is crucial for understanding the fundamental idea of learning from data.

Then, we introduce three algorithms based on Monte Carlo (MC) methods. These algorithms can learn optimal policies from experience samples. The first and simplest algorithm is called MC Basic, which can be readily obtained by modifying the policy iteration algorithm introduced in the last chapter. Understanding this algorithm is important for grasping the fundamental idea of MC-based reinforcement learning. By extending this algorithm, we further introduce another two algorithms that are more complicated but more efficient.

# 5.1 Motivating example: Mean estimation

We next introduce the mean estimation problem to demonstrate how to learn from data rather than a model. We will see that mean estimation can be achieved based on Monte Carlo methods, which refer to a broad class of techniques that use stochastic samples to solve estimation problems. The reader may wonder why we care about the mean estimation problem. It is simply because state and action values are both defined as the means of returns. Estimating a state or action value is actually a mean estimation problem.

Consider a random variable $X$ that can take values from a finite set of real numbers denoted as $\mathcal { X }$ . Suppose that our task is to calculate the mean or expected value of $X$ : $\mathbb { E } { \lvert X \rvert }$ . Two approaches can be used to calculate $\mathbb { E } { \lvert X \rvert }$ .

$\diamond$ The first approach is model-based. Here, the model refers to the probability distribution of $X$ . If the model is known, then the mean can be directly calculated based on the definition of the expected value:

$$
\mathbb {E} [ X ] = \sum_ {x \in \mathcal {X}} p (x) x.
$$

In this book, we use the terms expected value, mean, and average interchangeably.

The second approach is model-free. When the probability distribution (i.e., the model) of $X$ is unknown, suppose that we have some samples $\{ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \}$ of $X$ . Then, the mean can be approximated as

$$
\mathbb {E} [ X ] \approx \bar {x} = \frac {1}{n} \sum_ {j = 1} ^ {n} x _ {j}.
$$

When $n$ is small, this approximation may not be accurate. However, as $n$ increases, the approximation becomes increasingly accurate. When $n \to \infty$ , we have $\bar { x } \to \mathbb { E } [ X ]$ .

This is guaranteed by the law of large numbers: the average of a large number of samples is close to the expected value. The law of large numbers is introduced in Box 5.1.

The following example illustrates the two approaches described above. Consider a coin flipping game. Let random variable $X$ denote which side is showing when the coin lands. $X$ has two possible values: $X = 1$ when the head is showing, and $X = - 1$ when the tail is showing. Suppose that the true probability distribution (i.e., the model) of $X$ is

$$
p (X = 1) = 0. 5, \quad p (X = - 1) = 0. 5.
$$

If the probability distribution is known in advance, we can directly calculate the mean as

$$
\mathbb {E} [ X ] = 0. 5 \cdot 1 + 0. 5 \cdot (- 1) = 0.
$$

If the probability distribution is unknown, then we can flip the coin many times and record the sampling results $\{ x _ { i } \} _ { i = 1 } ^ { n }$ . By calculating the average of the samples, we can obtain an estimate of the mean. As shown in Figure 5.2, the estimated mean becomes increasingly accurate as the number of samples increases.

![](images/33d28c9341cf8740835c2df3aae8fdb2f725c1ac8874fb620478a1a77d5a401e.jpg)  
Figure 5.2: An example for demonstrating the law of large numbers. Here, the samples are drawn from $\{ + 1 , - 1 \}$ following a uniform distribution. The average of the samples gradually converges to zero, which is the true expected value, as the number of samples increases.

It is worth mentioning that the samples used for mean estimation must be independent and identically distributed (i.i.d. or iid). Otherwise, if the sampling values correlate, it may be impossible to correctly estimate the expected value. An extreme case is that all the sampling values are the same as the first one, whatever the first one is. In this case, the average of the samples is always equal to the first sample, no matter how many samples we use.

# Box 5.1: Law of large numbers

For a random variable $X$ , suppose that $\{ x _ { i } \} _ { i = 1 } ^ { n }$ are some i.i.d. samples. Let $\bar { x } =$ $\textstyle { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } x _ { i }$ be the average of the samples. Then,

$$
\mathbb {E} [ \bar {x} ] = \mathbb {E} [ X ],
$$

$$
\operatorname {v a r} [ \bar {x} ] = \frac {1}{n} \operatorname {v a r} [ X ].
$$

The above two equations indicate that $x$ is an unbiased estimate of $\mathbb { E } { \lvert X \rvert }$ , and its variance decreases to zero as $n$ increases to infinity.

The proof is given below.

First, $\begin{array} { r } { \mathbb { E } [ \bar { x } ] = \mathbb { E } \big \lfloor \sum _ { i = 1 } ^ { n } x _ { i } / n \big \rfloor = \sum _ { i = 1 } ^ { n } \mathbb { E } [ x _ { i } ] / n = \mathbb { E } [ X ] } \end{array}$ , where the last equability is due to the fact that the samples are identically distributed (that is, $\mathbb { E } | x _ { i } | = \mathbb { E } | X |$ ).

Second, $\begin{array} { r c l } { \operatorname { v a r } ( \bar { x } ) } & { = } & { \operatorname { v a r } \left[ \sum _ { i = 1 } ^ { n } x _ { i } / n \right] ~ = ~ \sum _ { i = 1 } ^ { n } \operatorname { v a r } [ x _ { i } ] / n ^ { 2 } ~ = ~ ( n \cdot \operatorname { v a r } [ X ] ) / n ^ { 2 } ~ = ~ \operatorname { v a r } [ X ] / n ^ { 2 } ~ , } \end{array}$ $\operatorname { v a r } [ X ] / n$ , where the second equality is due to the fact that the samples are independent, and the third equability is a result of the samples being identically distributed (that is, $\operatorname { v a r } [ x _ { i } ] = \operatorname { v a r } [ X ] ,$ ).

# 5.2 MC Basic: The simplest MC-based algorithm

This section introduces the first and the simplest MC-based reinforcement learning algorithm. This algorithm is obtained by replacing the model-based policy evaluation step in the policy iteration algorithm introduced in Section 4.2 with a model-free MC estimation step.

# 5.2.1 Converting policy iteration to be model-free

There are two steps in every iteration of the policy iteration algorithm (see Section 4.2). The first step is policy evaluation, which aims to compute $v _ { \pi _ { k } }$ by solving $v _ { \pi _ { k } } = r _ { \pi _ { k } } +$ $\gamma { P _ { \pi _ { k } } } { v _ { \pi _ { k } } }$ . The second step is policy improvement, which aims to compute the greedy policy $\pi _ { k + 1 } = \arg \operatorname* { m a x } _ { \pi } ( r _ { \pi } + \gamma P _ { \pi } v _ { \pi _ { k } } )$ . The elementwise form of the policy improvement step is

$$
\begin{array}{l} \pi_ {k + 1} (s) = \arg \max  _ {\pi} \sum_ {a} \pi (a | s) \left[ \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi_ {k}} \left(s ^ {\prime}\right) \right] \\ = \arg \max  _ {\pi} \sum_ {a} \pi (a | s) q _ {\pi_ {k}} (s, a), \quad s \in \mathcal {S}. \\ \end{array}
$$

It must be noted that the action values lie in the core of these two steps. Specifically. in the first step, the state values are calculated for the purpose of calculating the action

values. In the second step, the new policy is generated based on the calculated action values. Let us reconsider how we can calculate the action values. Two approaches are available.

 The first is a model-based approach. This is the approach adopted by the policy iteration algorithm. In particular, we can first calculate the state value $v _ { \pi _ { k } }$ by solving the Bellman equation. Then, we can calculate the action values by using

$$
q _ {\pi_ {k}} (s, a) = \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi_ {k}} \left(s ^ {\prime}\right). \tag {5.1}
$$

This approach requires the system model $\{ p ( r | s , a ) , p ( s ^ { \prime } | s , a ) \}$ to be known.

 The second is a model-free approach. Recall that the definition of an action value is

$$
\begin{array}{l} q _ {\pi_ {k}} (s, a) = \mathbb {E} [ G _ {t} | S _ {t} = s, A _ {t} = a ] \\ = \mathbb {E} \left[ R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \dots \mid S _ {t} = s, A _ {t} = a \right], \\ \end{array}
$$

which is the expected return obtained when starting from $( s , a )$ . Since $q _ { \pi _ { k } } ( s , a )$ is an expectation, it can be estimated by MC methods as demonstrated in Section 5.1. To do that, starting from $( s , a )$ , the agent can interact with the environment by following policy $\pi _ { k }$ and then obtain a certain number of episodes. Suppose that there are $n$ episodes and that the return of the $i$ th episode is $g _ { \pi _ { k } } ^ { ( i ) } ( s , a )$ . Then, $q _ { \pi _ { k } } ( s , a )$ can be approximated as

$$
q _ {\pi_ {k}} (s, a) = \mathbb {E} [ G _ {t} | S _ {t} = s, A _ {t} = a ] \approx \frac {1}{n} \sum_ {i = 1} ^ {n} g _ {\pi_ {k}} ^ {(i)} (s, a). \tag {5.2}
$$

We already know that, if the number of episodes $n$ is sufficiently large, the approximation will be sufficiently accurate according to the law of large numbers.

The fundamental idea of MC-based reinforcement learning is to use a model-free method for estimating action values, as shown in (5.2), to replace the model-based method in the policy iteration algorithm.

# 5.2.2 The MC Basic algorithm

We are now ready to present the first MC-based reinforcement learning algorithm. Starting from an initial policy $\pi _ { 0 }$ , the algorithm has two steps in the $k$ th iteration ( $k =$ $0 , 1 , 2 , \ldots )$ ).

 Step 1: Policy evaluation. This step is used to estimate $q _ { \pi _ { k } } ( s , a )$ for all $( s , a )$ . Specifically, for every $( s , a )$ , we collect sufficiently many episodes and use the average of the returns, denoted as $q _ { k } ( s , a )$ , to approximate $q _ { \pi _ { k } } ( s , a )$ .

Algorithm 5.1: MC Basic (a model-free variant of policy iteration)   
Initialization: Initial guess $\pi_0$ Goal: Search for an optimal policy.   
For the $k$ th iteration $(k = 0,1,2,\ldots)$ ,do For every state $s\in S$ , do For every action $a\in \mathcal{A}(s)$ , do Collect sufficiently many episodes starting from $(s,a)$ by following $\pi_{k}$ Policy evaluation: $q_{\pi_k}(s,a)\approx q_k(s,a) =$ the average return of all the episodes starting from $(s,a)$ Policy improvement: $a_{k}^{*}(s) = \arg \max_{a}q_{k}(s,a)$ $\pi_{k + 1}(a|s) = 1$ if $a = a_k^*$ , and $\pi_{k + 1}(a|s) = 0$ otherwise

 Step 2: Policy improvement. This step solves $\begin{array} { r } { \pi _ { k + 1 } ( s ) = \arg \operatorname* { m a x } _ { \pi } \sum _ { a } \pi ( a | s ) q _ { k } ( s , a ) } \end{array}$ for all $s \in S$ . The greedy optimal policy is $\pi _ { k + 1 } ( a _ { k } ^ { * } | s ) = 1$ where $a _ { k } ^ { * } = \arg \operatorname* { m a x } _ { a } q _ { k } ( s , a )$ .

This is the simplest MC-based reinforcement learning algorithm, which is called $M C$ Basic in this book. The pseudocode of the MC Basic algorithm is given in Algorithm 5.1. As can be seen, it is very similar to the policy iteration algorithm. The only difference is that it calculates action values directly from experience samples, whereas policy iteration calculates state values first and then calculates the action values based on the system model. It should be noted that the model-free algorithm directly estimates action values. Otherwise, if it estimates state values instead, we still need to calculate action values from these state values using the system model, as shown in (5.1).

Since policy iteration is convergent, MC Basic is also convergent when given sufficient samples. That is, for every $( s , a )$ , suppose that there are sufficiently many episodes starting from $( s , a )$ . Then, the average of the returns of these episodes can accurately approximate the action value of $( s , a )$ . In practice, we usually do not have sufficient episodes for every $( s , a )$ . As a result, the approximation of the action values may not be accurate. Nevertheless, the algorithm usually can still work. This is similar to the truncated policy iteration algorithm, where the action values are neither accurately calculated.

Finally, MC Basic is too simple to be practical due to its low sample efficiency. The reason why we introduce this algorithm is to let readers grasp the core idea of MCbased reinforcement learning. It is important to understand this algorithm well before studying more complex algorithms introduced later in this chapter. We will see that more complex and sample-efficient algorithms can be readily obtained by extending the MC Basic algorithm.

# 5.2.3 Illustrative examples

A simple example: A step-by-step implementation

![](images/e0c3438403440a0b043a88f5e3fb10ad4a60e29e46f8b31d9abaaace60a1bd5f.jpg)  
Figure 5.3: An example for illustrating the MC Basic algorithm.

We next use an example to demonstrate the implementation details of the MC Basic algorithm. The reward settings are $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1$ and $r _ { \mathrm { t a r g e t } } = 1$ . The discount rate is $\gamma = 0 . 9$ . The initial policy $\pi _ { 0 }$ is shown in Figure 5.3. This initial policy is not optimal for $s _ { 1 }$ or $s _ { 3 }$ .

While all the action values should be calculated, we merely present those of $s _ { 1 }$ due to space limitations. At $s _ { 1 }$ , there are five possible actions. For each action, we need to collect many episodes that are sufficiently long to effectively approximate the action value. However, since this example is deterministic in terms of both the policy and model, running multiple times would generate the same trajectory. As a result, the estimation of each action value merely requires a single episode.

Following $\pi _ { 0 }$ , we can obtain the following episodes by respectively starting from $( s _ { 1 } , a _ { 1 } )$ , $( s _ { 1 } , a _ { 2 } )$ , . . . , $( s _ { 1 } , a _ { 5 } )$ .

Starting from $( s _ { 1 } , a _ { 1 } )$ , the episode is $s _ { 1 } \ { \stackrel { a 1 } { \longrightarrow } } s _ { 1 } \ { \stackrel { a 1 } { \longrightarrow } } \ s _ { 1 } \ { \stackrel { a 1 } { \longrightarrow } } \ \ldots .$ . The action value equals the discounted return of the episode:

$$
q _ {\pi_ {0}} (s _ {1}, a _ {1}) = - 1 + \gamma (- 1) + \gamma^ {2} (- 1) + \dots = \frac {- 1}{1 - \gamma}.
$$

 Starting from $( s _ { 1 } , a _ { 2 } )$ , the episode is $s _ { 1 } \ { \stackrel { u 2 } { \longrightarrow } } s _ { 2 } \ { \stackrel { u 3 } { \longrightarrow } } \ s _ { 5 } \ { \stackrel { u 3 } { \longrightarrow } } \ \dots .$ . The action value equals the discounted return of the episode:

$$
q _ {\pi_ {0}} (s _ {1}, a _ {2}) = 0 + \gamma 0 + \gamma^ {2} 0 + \gamma^ {3} (1) + \gamma^ {4} (1) + \dots = \frac {\gamma^ {3}}{1 - \gamma}.
$$

 Starting from $( s _ { 1 } , a _ { 3 } )$ , the episode is $s _ { 1 } \ { \xrightarrow { a _ { 3 } } } s _ { 4 } \ { \xrightarrow { a _ { 2 } } } \ s _ { 5 } \ { \xrightarrow { a _ { 3 } } } \ . \ . \ .$ . The action value equals

the discounted return of the episode:

$$
q _ {\pi_ {0}} (s _ {1}, a _ {3}) = 0 + \gamma 0 + \gamma^ {2} 0 + \gamma^ {3} (1) + \gamma^ {4} (1) + \dots = \frac {\gamma^ {3}}{1 - \gamma}.
$$

Starting from $( s _ { 1 } , a _ { 4 } )$ , the episode is $s _ { 1 } \ { \stackrel { u 4 } { \longrightarrow } } s _ { 1 } \ { \stackrel { u 1 } { \longrightarrow } } \ s _ { 1 } \ { \stackrel { u 1 } { \longrightarrow } } \ \dots$ . The action value equals the discounted return of the episode:

$$
q _ {\pi_ {0}} (s _ {1}, a _ {4}) = - 1 + \gamma (- 1) + \gamma^ {2} (- 1) + \dots = \frac {- 1}{1 - \gamma}.
$$

Starting from $( s _ { 1 } , a _ { 5 } )$ , the episode is $s _ { 1 } \ { \xrightarrow { a _ { 5 } } } s _ { 1 } \ { \xrightarrow { a _ { 1 } } } \ s _ { 1 } \ { \xrightarrow { a _ { 1 } } } \ . \ . \ .$ . The action value equals the discounted return of the episode:

$$
q _ {\pi_ {0}} (s _ {1}, a _ {5}) = 0 + \gamma (- 1) + \gamma^ {2} (- 1) + \dots = \frac {- \gamma}{1 - \gamma}.
$$

By comparing the five action values, we see that

$$
q _ {\pi_ {0}} (s _ {1}, a _ {2}) = q _ {\pi_ {0}} (s _ {1}, a _ {3}) = \frac {\gamma^ {3}}{1 - \gamma} > 0
$$

are the maximum values. As a result, the new policy can be obtained as

$$
\pi_ {1} (a _ {2} | s _ {1}) = 1 \quad \text {o r} \quad \pi_ {1} (a _ {3} | s _ {1}) = 1.
$$

It is intuitive that the improved policy, which takes either $a _ { 2 }$ or $a _ { 3 }$ at $s _ { 1 }$ , is optimal. Therefore, we can successfully obtain an optimal policy by using merely one iteration for this simple example. In this simple example, the initial policy is already optimal for all the states except $s _ { 1 }$ and $s _ { 3 }$ . Therefore, the policy can become optimal after merely a single iteration. When the policy is nonoptimal for other states, more iterations are needed.

# A comprehensive example: Episode length and sparse rewards

We next discuss some interesting properties of the MC Basic algorithm by examining a more comprehensive example. The example is a 5-by-5 grid world (Figure 5.4). The reward settings are $r _ { \mathrm { b o u n d a r y } } = - 1$ , $r _ { \mathrm { f o r b i d d e n } } = - 1 0$ , and $r _ { \mathrm { t a r g e t } } = 1$ . The discount rate is $\gamma = 0 . 9$ .

First, we demonstrate that the episode length greatly impacts the final optimal policies. In particular, Figure 5.4 shows the final results generated by the MC Basic algorithm with different episode lengths. When the length of each episode is too short, neither the policy nor the value estimate is optimal (see Figures 5.4(a)-(d)). In the extreme case where the episode length is one, only the states that are adjacent to the target have

![](images/ea9d0afdaeba69b9349533509d4924e7bcd1a1875db90fc642f2f64214c5c239.jpg)  
(a) Final value and policy with episode length=1

![](images/bc8de7aaabf3b8d62870a0a30906f6308e10b8067460abe9c67091f6d7b328d0.jpg)  
(b) Final value and policy with episode length=2

![](images/39f6c7397fc4b93806c8363de95c5d8a8665cc85f1534a4ff73b0e4868da8b4e.jpg)

![](images/b5605fb4afa3fb83f165a8dcf246ce02ae0be6c236e8fbc5a51e151f2edc70b3.jpg)  
(c) Final value and policy with episode length=3

![](images/02a80b8d32671ab7c2a4ea1191160dcf5a6a6382d76e5beb61d28636907d4ab5.jpg)  
(d) Final value and policy with episode length=4

![](images/0099a3f09464fc8a62bde59c359bccad2c0b60fcda1b1523a426542c0a402d9b.jpg)

![](images/dc45cf43985cdedd10bdf481d089a72175ed535ae78086a2fc4f8a0a81e701b8.jpg)  
(e) Final value and policy with episode length=14

![](images/7fdafd61e1485f2082e1a5dd0ac173cce4c72fa08bc3467a3f130ce19af860d6.jpg)  
(f) Final value and policy with episode length=15

![](images/c3f188ed1a67a88520b68876d0bddca60706ff3ac99c44984cc79b21423195a4.jpg)

![](images/7b0b6eca04168cc8ce5ff39ea0caa0b6250230c2699dbc5c236d15d4a2e44dc9.jpg)  
(g) Final value and policy with episode length=30

![](images/28959e8470d449dbcec88e003e32bbda3416c249bd717ba734a2300498d81f6e.jpg)  
(h) Final value and policy with episode length=100   
Figure 5.4: The policies and state values obtained by the MC Basic algorithm when given different episode lengths. Only if the length of each episode is sufficiently long, can the state values be accurately estimated.

nonzero values, and all the other states have zero values since each episode is too short to reach the target or get positive rewards (see Figure 5.4(a)). As the episode length increases, the policy and value estimates gradually approach the optimal ones (see Figure 5.4(h)).

As the episode length increases, an interesting spatial pattern emerges. That is, the states that are closer to the target possess nonzero values earlier than those that are farther away. The reason for this phenomenon is as follows. Starting from a state, the agent must travel at least a certain number of steps to reach the target state and then receive positive rewards. If the length of an episode is less than the minimum desired number of steps, it is certain that the return is zero, and so is the estimated state value. In this example, the episode length must be no less than 15, which is the minimum number of steps required to reach the target when starting from the bottom-left state.

While the above analysis suggests that each episode must be sufficiently long, the episodes are not necessarily infinitely long. As shown in Figure 5.4(g), when the length is 30, the algorithm can find an optimal policy, although the value estimate is not yet optimal.

The above analysis is related to an important reward design problem, sparse reward, which refers to the scenario in which no positive rewards can be obtained unless the target is reached. The sparse reward setting requires long episodes that can reach the target. This requirement is challenging to satisfy when the state space is large. As a result, the sparse reward problem downgrades the learning efficiency. One simple technique for solving this problem is to design nonsparse rewards. For instance, in the above grid world example, we can redesign the reward setting so that the agent can obtain a small positive reward when reaching the states near the target. In this way, an “attractive field” can be formed around the target so that the agent can find the target more easily. More information about sparse reward problems can be found in [17–19].

# 5.3 MC Exploring Starts

We next extend the MC Basic algorithm to obtain another MC-based reinforcement learning algorithm that is slightly more complicated but more sample-efficient.

# 5.3.1 Utilizing samples more efficiently

An important aspect of MC-based reinforcement learning is how to use samples more efficiently. Specifically, suppose that we have an episode of samples obtained by following a policy $\pi$ :

$$
s _ {1} \xrightarrow {a _ {2}} s _ {2} \xrightarrow {a _ {4}} s _ {1} \xrightarrow {a _ {2}} s _ {2} \xrightarrow {a _ {3}} s _ {5} \xrightarrow {a _ {1}} \dots \tag {5.3}
$$

where the subscripts refer to the state or action indexes rather than time steps. Every time a state-action pair appears in an episode, it is called a visit of that state-action pair. Different strategies can be employed to utilize the visits.

The first and simplest strategy is to use the initial visit. That is, an episode is only used to estimate the action value of the initial state-action pair that the episode starts from. For the example in (5.3), the initial-visit strategy merely estimates the action value of $( s _ { 1 } , a _ { 2 } )$ . The MC Basic algorithm utilizes the initial-visit strategy. However, this strategy is not sample-efficient because the episode also visits many other state-action pairs such as $( s _ { 2 } , a _ { 4 } )$ , $\left( { { s _ { 2 } } , { a _ { 3 } } } \right)$ , and $( s _ { 5 } , a _ { 1 } )$ . These visits can also be used to estimate the corresponding action values. In particular, we can decompose the episode in (5.3) into multiple subepisodes:

$$
\begin{array}{l} s _ {1} \xrightarrow {a _ {2}} s _ {2} \xrightarrow {a _ {4}} s _ {1} \xrightarrow {a _ {2}} s _ {2} \xrightarrow {a _ {3}} s _ {5} \xrightarrow {a _ {1}} \ldots [ \mathrm {o r i g i n a l e p i s o d e} ] \\ s _ {2} \xrightarrow {a _ {4}} s _ {1} \xrightarrow {a _ {2}} s _ {2} \xrightarrow {a _ {3}} s _ {5} \xrightarrow {a _ {1}} \ldots [ \mathrm {s u b e p i s o d e s t a r t i n g f r o m} (s _ {2}, a _ {4}) ] \\ s _ {1} \xrightarrow {a _ {2}} s _ {2} \xrightarrow {a _ {3}} s _ {5} \xrightarrow {a _ {1}} \dots \quad [ \mathrm {s u b e p i s o d e s t a r t i n g f r o m} (s _ {1}, a _ {2}) ] \\ s _ {2} \xrightarrow {a _ {3}} s _ {5} \xrightarrow {a _ {1}} \ldots [ \mathrm {s u b e p i s o d e s t a r t i n g f r o m} (s _ {2}, a _ {3}) ] \\ s _ {5} \xrightarrow {a _ {1}} \ldots [ \mathrm {s u b e p i s o d e s t a r t i n g f r o m} (s _ {5}, a _ {1}) ] \\ \end{array}
$$

The trajectory generated after the visit of a state-action pair can be viewed as a new episode. These new episodes can be used to estimate more action values. In this way, the samples in the episode can be utilized more efficiently.

Moreover, a state-action pair may be visited multiple times in an episode. For example, $( s _ { 1 } , a _ { 2 } )$ is visited twice in the episode in (5.3). If we only count the first-time visit, this is called a first-visit strategy. If we count every visit of a state-action pair, such a strategy is called every-visit [20].

In terms of sample usage efficiency, the every-visit strategy is the best. If an episode is sufficiently long such that it can visit all the state-action pairs many times, then this single episode may be sufficient for estimating all the action values using the every-visit strategy. However, the samples obtained by the every-visit strategy are correlated because the trajectory starting from the second visit is merely a subset of the trajectory starting from the first visit. Nevertheless, the correlation would not be strong if the two visits are far away from each other in the trajectory.

# 5.3.2 Updating policies more efficiently

Another aspect of MC-based reinforcement learning is when to update the policy. Two strategies are available.

 The first strategy is, in the policy evaluation step, to collect all the episodes starting from the same state-action pair and then approximate the action value using the average return of these episodes. This strategy is adopted in the MC Basic algorithm.

Algorithm 5.2: MC Exploring Starts (an efficient variant of MC Basic)   
Initialization: Initial policy $\pi_0(a|s)$ and initial value $q(s,a)$ for all $(s,a)$ . Returns $(s,a) = 0$ and $\mathrm{Num}(s,a) = 0$ for all $(s,a)$ .  
Goal: Search for an optimal policy.  
For each episode, do  
Episode generation: Select a starting state-action pair $(s_0,a_0)$ and ensure that all pairs can be possibly selected (this is the exploring-starts condition). Following the current policy, generate an episode of length $T$ : $s_0,a_0,r_1,\ldots ,s_{T - 1},a_{T - 1},r_T$ . Initialization for each episode: $g\gets 0$ For each step of the episode, $t = T - 1,T - 2,\dots ,0$ do $g\gets \gamma g + r_{t + 1}$ Returns $(s_t,a_t)\gets \mathrm{Returns}(s_t,a_t) + g$ $\mathrm{Num}(s_t,a_t)\gets \mathrm{Num}(s_t,a_t) + 1$ Policy evaluation: $q(s_{t},a_{t})\gets \mathrm{Returns}(s_{t},a_{t}) / \mathrm{Num}(s_{t},a_{t})$ Policy improvement: $\pi (a|s_t) = 1$ if $a = \arg \max_a q(s_t,a)$ and $\pi (a|s_t) = 0$ otherwise

The drawback of this strategy is that the agent must wait until all the episodes have been collected before the estimate can be updated.

The second strategy, which can overcome this drawback, is to use the return of a single episode to approximate the corresponding action value. In this way, we can immediately obtain a rough estimate when we receive an episode. Then, the policy can be improved in an episode-by-episode fashion.

Since the return of a single episode cannot accurately approximate the corresponding action value, one may wonder whether the second strategy is good. In fact, this strategy falls into the scope of generalized policy iteration introduced in the last chapter. That is, we can still update the policy even if the value estimate is not sufficiently accurate.

# 5.3.3 Algorithm description

We can use the techniques introduced in Sections 5.3.1 and 5.3.2 to enhance the efficiency of the MC Basic algorithm. Then, a new algorithm called MC Exploring Starts can be obtained.

The details of MC Exploring Starts are given in Algorithm 5.2. This algorithm uses the every-visit strategy. Interestingly, when calculating the discounted return obtained by starting from each state-action pair, the procedure starts from the ending states and travels back to the starting state. Such techniques can make the algorithm more efficient, but it also makes the algorithm more complex. This is why the MC Basic algorithm,

which is free of such techniques, is introduced first to reveal the core idea of MC-based reinforcement learning.

The exploring starts condition requires sufficiently many episodes starting from every state-action pair. Only if every state-action pair is well explored, can we accurately estimate their action values (according to the law of large numbers) and hence successfully find optimal policies. Otherwise, if an action is not well explored, its action value may be inaccurately estimated, and this action may not be selected by the policy even though it is indeed the best action. Both MC Basic and MC Exploring Starts require this condition. However, this condition is difficult to meet in many applications, especially those involving physical interactions with environments. Can we remove the exploring starts requirement? The answer is yes, as shown in the next section.

# 5.4 MC $\epsilon$ -Greedy: Learning without exploring starts

We next extend the MC Exploring Starts algorithm by removing the exploring starts condition. This condition actually requires that every state-action pair can be visited sufficiently many times, which can also be achieved based on soft policies.

# 5.4.1 $\epsilon$ -greedy policies

A policy is soft if it has a positive probability of taking any action at any state. Consider an extreme case in which we only have a single episode. With a soft policy, a single episode that is sufficiently long can visit every state-action pair many times (see the examples in Figure 5.8). Thus, we do not need to generate a large number of episodes starting from different state-action pairs, and then the exploring starts requirement can be removed.

One type of common soft policies is $\epsilon$ -greedy policies. An $\epsilon$ -greedy policy is a stochastic policy that has a higher chance of choosing the greedy action and the same nonzero probability of taking any other action. Here, the greedy action refers to the action with the greatest action value. In particular, suppose that $\epsilon \in \mathsf { \Gamma }  ( 0 , 1 ]$ . The corresponding $\epsilon$ -greedy policy has the following form:

$$
\pi (a | s) = \left\{ \begin{array}{l l} 1 - \frac {\epsilon}{| \mathcal {A} (s) |} (| \mathcal {A} (s) | - 1), & \mathrm {f o r t h e g r e e d y a c t i o n ,} \\ \frac {\epsilon}{| \mathcal {A} (s) |}, & \mathrm {f o r t h e o t h e r | \mathcal {A} (s) | - 1 a c t i o n s ,} \end{array} \right.
$$

where $| { \mathcal { A } } ( s ) |$ denotes the number of actions associated with $s$ .

When $\epsilon = 0$ , $\epsilon$ -greedy becomes greedy. When $\epsilon = 1$ , the probability of taking any action equals 1|A(s)| . $\frac { 1 } { | \mathcal { A } ( s ) | }$ 1

The probability of taking the greedy action is always greater than that of taking any

other action because

$$
1 - \frac {\epsilon}{| \mathcal {A} (s) |} (| \mathcal {A} (s) | - 1) = 1 - \epsilon + \frac {\epsilon}{| \mathcal {A} (s) |} \geq \frac {\epsilon}{| \mathcal {A} (s) |}
$$

for any $\epsilon \in \lfloor 0 , 1 \rfloor$

While an $\epsilon$ -greedy policy is stochastic, how can we select an action by following such a policy? We can first generate a random number $x$ in $\lfloor 0 , 1 \rfloor$ by following a uniform distribution. If $x \geq \epsilon$ , then we select the greedy action. If $x < \epsilon$ , then we randomly select an action in $\boldsymbol { \mathcal { A } } ( \boldsymbol { s } )$ with the probability of $\frac { 1 } { | \mathcal { A } ( s ) | }$ (we may select the greedy action again). In this way, the total probability of selecting the greedy action is $\begin{array} { r } { 1 - \epsilon + \frac { \epsilon } { | A ( s ) | } } \end{array}$ , and the probability of selecting any other action is |A(s)| . $\frac { \epsilon } { | \mathcal { A } ( s ) | }$

# 5.4.2 Algorithm description

To integrate $\epsilon$ -greedy policies into MC learning, we only need to change the policy improvement step from greedy to $\epsilon$ -greedy.

In particular, the policy improvement step in MC Basic or MC Exploring Starts aims to solve

$$
\pi_ {k + 1} (s) = \arg \max  _ {\pi \in \Pi} \sum_ {a} \pi (a | s) q _ {\pi_ {k}} (s, a), \tag {5.4}
$$

where $\Pi$ denotes the set of all possible policies. We know that the solution of (5.4) is a greedy policy:

$$
\pi_ {k + 1} (a | s) = \left\{ \begin{array}{l l} 1, & a = a _ {k} ^ {*}, \\ 0, & a \neq a _ {k} ^ {*}, \end{array} \right.
$$

where $a _ { k } ^ { * } = \arg \operatorname* { m a x } _ { a } q _ { \pi _ { k } } ( s , a )$

Now, the policy improvement step is changed to solve

$$
\pi_ {k + 1} (s) = \arg \max  _ {\pi \in \Pi_ {\epsilon}} \sum_ {a} \pi (a | s) q _ {\pi_ {k}} (s, a), \tag {5.5}
$$

where $\Pi _ { \epsilon }$ denotes the set of all -greedy policies with a given value of $\epsilon$ . In this way, we force the policy to be $\epsilon$ -greedy. The solution of (5.5) is

$$
\pi_ {k + 1} (a | s) = \left\{ \begin{array}{l l} 1 - \frac {| \mathcal {A} (s) | - 1}{| \mathcal {A} (s) |} \epsilon , & a = a _ {k} ^ {*}, \\ \frac {1}{| \mathcal {A} (s) |} \epsilon , & a \neq a _ {k} ^ {*}, \end{array} \right.
$$

where $a _ { k } ^ { * } = \arg \operatorname* { m a x } _ { a } q _ { \pi _ { k } } ( s , a )$ . With the above change, we obtain another algorithm called MC $\epsilon$ -Greedy. The details of this algorithm are given in Algorithm 5.3. Here, the every-visit strategy is employed to better utilize the samples.

Algorithm 5.3: MC -Greedy (a variant of MC Exploring Starts)   
Initialization: Initial policy $\pi_0(a|s)$ and initial value $q(s,a)$ for all $(s,a)$ . Returns $(s,a) = 0$ and $\mathrm{Num}(s,a) = 0$ for all $(s,a)$ . $\epsilon \in (0,1]$ Goal: Search for an optimal policy.  
For each episode, do  
Episode generation: Select a starting state-action pair $(s_0,a_0)$ (the exploring starts condition is not required). Following the current policy, generate an episode of length $T$ : $s_0,a_0,r_1,\ldots ,s_{T - 1},a_{T - 1},r_T$ . Initialization for each episode: $g\gets 0$ For each step of the episode, $t = T - 1,T - 2,\dots ,0$ do $g\gets \gamma g + r_{t + 1}$ Returns $(s_t,a_t)\gets \mathrm{Returns}(s_t,a_t) + g$ $\mathrm{Num}(s_t,a_t)\gets \mathrm{Num}(s_t,a_t) + 1$ Policy evaluation: $q(s_{t},a_{t})\gets \mathrm{Returns}(s_{t},a_{t}) / \mathrm{Num}(s_{t},a_{t})$ Policy improvement:  
Let $a^* = \arg \max_a q(s_t,a)$ and $\pi (a|s_t) = \left\{ \begin{array}{ll}1 - \frac{|A(s_t)| - 1}{|A(s_t)|}\epsilon , & a = a^*\\ \frac{1}{|A(s_t)|}\epsilon , & a\neq a^* \end{array} \right.$

If greedy policies are replaced by $\epsilon$ -greedy policies in the policy improvement step, can we still guarantee to obtain optimal policies? The answer is both yes and no. By yes, we mean that, when given sufficient samples, the algorithm can converge to an $\epsilon$ -greedy policy that is optimal in the set $\Pi _ { \epsilon }$ . By no, we mean that the policy is merely optimal in $\Pi _ { \epsilon }$ but may not be optimal in $\Pi$ . However, if $\epsilon$ is sufficiently small, the optimal policies in $\Pi _ { \epsilon }$ are close to those in $\Pi$ .

# 5.4.3 Illustrative examples

Consider the grid world example shown in Figure 5.5. The aim is to find the optimal policy for every state. A single episode with one million steps is generated in every iteration of the MC $\epsilon$ -Greedy algorithm. Here, we deliberately consider the extreme case with merely one single episode. We set $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1$ , $r _ { \mathrm { t a r g e t } } = 1$ , and $\gamma = 0 . 9$ .

The initial policy is a uniform policy that has the same probability 0.2 of taking any action, as shown in Figure 5.5. The optimal $\epsilon$ -greedy policy with $\epsilon = 0 . 5$ can be obtained after two iterations. Although each iteration merely uses a single episode, the policy gradually improves because all the state-action pairs can be visited and hence their values can be accurately estimated.

![](images/b47b1b6c14f623c5b5ba65b541ca0fc7ab9d53a18d11b929e47e34fc2368dff5.jpg)  
(a) Initial policy

![](images/7e62d36cb1ab90a880f3af7f78c99f31390557e4e5f54d40f89eefa25c0bc40e.jpg)  
(b) After the first iteration

![](images/a9d8864c3af53cdabd65ec6c9f08f80dfd3d9c818be22b0f1fa88276a76e60b8.jpg)  
(c) After the second iteration   
Figure 5.5: The evolution process of the MC $\epsilon$ -Greedy algorithm based on single episodes.

# 5.5 Exploration and exploitation of -greedy policies

Exploration and exploitation constitute a fundamental tradeoff in reinforcement learning. Here, exploration means that the policy can possibly take as many actions as possible. In this way, all the actions can be visited and evaluated well. Exploitation means that the improved policy should take the greedy action that has the greatest action value. However, since the action values obtained at the current moment may not be accurate due to insufficient exploration, we should keep exploring while conducting exploitation to avoid missing optimal actions.

$\epsilon$ -greedy policies provide one way to balance exploration and exploitation. On the one hand, an $\epsilon$ -greedy policy has a higher probability of taking the greedy action so that it can exploit the estimated values. On the other hand, the $\epsilon$ -greedy policy also has a chance to take other actions so that it can keep exploring. $\epsilon$ -greedy policies are used not only in MC-based reinforcement learning but also in other reinforcement learning algorithms such as temporal-difference learning as introduced in Chapter 7.

Exploitation is related to optimality because optimal policies should be greedy. The fundamental idea of $\epsilon$ -greedy policies is to enhance exploration by sacrificing optimality/exploitation. If we would like to enhance exploitation and optimality, we need to reduce the value of $\epsilon$ . However, if we would like to enhance exploration, we need to increase the value of $\epsilon$ .

We next discuss this tradeoff based on some interesting examples. The reinforcement learning task here is a 5-by-5 grid world. The reward settings are $r _ { \mathrm { b o u n d a r y } } = - 1$ , $r _ { \mathrm { f o r b i d d e n } } = - 1 0$ , and $r _ { \mathrm { t a r g e t } } = 1$ . The discount rate is $\gamma = 0 . 9$ .

# Optimality of $\epsilon$ -greedy policies

We next show that the optimality of $\epsilon$ -greedy policies becomes worse when $\epsilon$ increases.

First, a greedy optimal policy and the corresponding optimal state values are shown in Figure 5.6(a). The state values of some consistent $\epsilon$ -greedy policies are shown in

![](images/17380c8f01cdbd124031e3acbf6c6cdb2cb407c8ba85aabb5a7ada6279c1c1f1.jpg)

![](images/586a7f36ea584cb99d8f4f31e3c346a4dc3f050fff1384499bdf69de1e52d51e.jpg)  
(a) A given $\epsilon$ -greedy policy and its state values: $\epsilon = 0$

![](images/0563d68b4ffa172a38b8f009aff42d7b06b139b4ce8adebe11f91e7d045fe42f.jpg)

![](images/b8878215da9f60671a56665534b248be349059371b351a0907c920ca3ec21160.jpg)  
(b) A given $\epsilon$ -greedy policy and its state values: $\epsilon = 0 . 1$

![](images/176df405087149134b2865bdf3694b2d2b5dc9034fa9955949321bcc22a1f343.jpg)

![](images/90ab50c2a54cbc21120bf80680d55e18a7a51e9e255ea179654ef5843a3e8989.jpg)  
(c) A given $\epsilon$ -greedy policy and its state values: $\epsilon = 0 . 2$

![](images/ff21d52161ad0e7cd95811056ce8edc6556b76152626aabdbd59242a1be098ef.jpg)

![](images/3a3b8095e315c38c0f4b5176d346916f55433d814f507f87477e297885f96eaa.jpg)  
(d) A given $\epsilon$ -greedy policy and its state values: $\epsilon = 0 . 5$   
Figure 5.6: The state values of some $\epsilon$ -greedy policies. These $\epsilon$ -greedy policies are consistent with each other in the sense that the actions with the greatest probabilities are the same. It can be seen that, when the value of $\epsilon$ increases, the state values of the $\epsilon$ -greedy policies decrease and hence their optimality becomes worse.

![](images/cb3b11447c2c840b82079408f5253aa7e94d1dd3a0fcfc03e0b74d517f4bbba6.jpg)

![](images/43292511dee51b060c3a02efce3f6cbbfd06e8783dd64a68038e8854e8423ce1.jpg)  
(a) The optimal $\epsilon$ -greedy policy and its state values: $\epsilon = 0$

![](images/86edf7ae8193b9f62c7898d4ac37357c53fbbb97139f00a445a354c8f865f973.jpg)

![](images/8e966308ce7321745523695057415ac93a8eab3693c46a6179f10a1665718362.jpg)  
(b) The optimal $\epsilon$ -greedy policy and its state values: $\epsilon = 0 . 1$

![](images/755b98a0602f2f0c185d92fa8679143ee05bf62721156b3d1707f2d7a1e3a864.jpg)

![](images/aafd5a98b45cb77f74bfcc857560749adb7f0431978a41af39a0303492c84cd4.jpg)  
(c) The optimal $\epsilon$ -greedy policy and its state values: $\epsilon = 0 . 2$

![](images/666f462b3bd5cb69b28d1c1e5ccd2b69ad9b7616cbaf77ff710f8534970eb44b.jpg)

![](images/50f9e5846bde1d6b957776b6c6715c5e1024474bfa9e81da20d6ec870ee08b8b.jpg)  
(d) The optimal $\epsilon$ -greedy policy and its state values: $\epsilon = 0 . 5$   
Figure 5.7: The optimal $\epsilon$ -greedy policies and their corresponding state values under different values of $\epsilon$ . Here, these $\epsilon$ -greedy policies are optimal among all $\epsilon$ -greedy ones (with the same value of $\epsilon$ ). It can be seen that, when the value of $\epsilon$ increases, the optimal $\epsilon$ -greedy policies are no longer consistent with the optimal one as in (a).

Figures 5.6(b)-(d). Here, two $\epsilon$ -greedy policies are consistent if the actions with the greatest probabilities in the policies are the same.

As the value of $\epsilon$ increases, the state values of the $\epsilon$ -greedy policies decrease, indicating that the optimality of these $\epsilon$ -greedy policies becomes worse. Notably, the value of the target state becomes the smallest when $\epsilon$ is as large as 0.5. This is because, when $\epsilon$ is large, the agent starting from the target area may enter the surrounding forbidden areas and hence receive negative rewards with a higher probability.

 Second, Figure 5.7 shows the optimal $\epsilon$ -greedy policies (they are optimal in $\Pi _ { \epsilon }$ ). When $\epsilon = 0$ , the policy is greedy and optimal among all policies. When $\epsilon$ is as small as 0.1, the optimal $\epsilon$ -greedy policy is consistent with the optimal greedy one. However, when $\epsilon$ increases to, for example, 0.2, the obtained $\epsilon$ -greedy policies are not consistent with the optimal greedy one. Therefore, if we want to obtain $\epsilon$ -greedy policies that are consistent with the optimal greedy ones, the value of $\epsilon$ should be sufficiently small.

Why are the $\epsilon$ -greedy policies inconsistent with the optimal greedy one when $\epsilon$ i s large? We can answer this question by considering the target state. In the greedy case, the optimal policy at the target state is to stay still to gain positive rewards. However, when $\epsilon$ is large, there is a high chance of entering the forbidden areas and receiving negative rewards. Therefore, the optimal policy at the target state in this case is to escape instead of staying still.

# Exploration abilities of $\epsilon$ -greedy policies

We next illustrate that the exploration ability of an $\epsilon$ -greedy policy is strong when $\epsilon$ i s large.

First, consider an $\epsilon$ -greedy policy with $\epsilon = 1$ (see Figure 5.5(a)). In this case, the exploration ability of the $\epsilon$ -greedy policy is strong since it has a 0.2 probability of taking any action at any state. Starting from $( s _ { 1 } , a _ { 1 } )$ , an episode generated by the $\epsilon$ -policy is given in Figures 5.8(a)-(c). It can be seen that this single episode can visit all the stateaction pairs many times when the episode is sufficiently long due to the strong exploration ability of the policy. Moreover, the numbers of times that all the state-action pairs are visited are almost even, as shown in Figure 5.8(d).

Second, consider an $\epsilon$ -policy with $\epsilon = 0 . 5$ (see Figure 5.6(d)). In this case, the $\epsilon$ -greedy policy has a weaker exploration ability than the case of $\epsilon = 1$ . Starting from $( s _ { 1 } , a _ { 1 } )$ , an episode generated by the $\epsilon$ -policy is given in Figures 5.8(e)-(g). Although every action can still be visited when the episode is sufficiently long, the distribution of the number of visits may be extremely uneven. For example, given an episode with one million steps, some actions are visited more than 250,000 times, while most actions are visited merely hundreds or even tens of times, as shown in Figure 5.8(h).

The above examples demonstrate that the exploration abilities of $\epsilon$ -greedy policies decrease when $\epsilon$ decreases. One useful technique is to initially set $\epsilon$ to be large to enhance

![](images/1258cec4c49938b077ed21ac55b6074b0d6bc8f508da39daec28f48fa2f66a89.jpg)  
(a) $\epsilon = 1$ , trajectory of 100 steps

![](images/ff2f52ddab23f7c0ed194db1030754080a2b29c5f8fbed1ff441b91e4a13ad5c.jpg)  
(e) $\epsilon = 0 . 5$ , trajectory of 100 steps

![](images/fb61772fb4d1db4241328c5be30abec674e5285d8da6c25f4998614fd13791ab.jpg)  
(b) $\epsilon = 1$ , trajectory of 1,000 steps

![](images/427818f9e735c407b35e09330c97213fbd303dc0fbef84abbb43f45b637dc706.jpg)  
(f) $\epsilon = 0 . 5$ , trajectory of 1,000 steps

![](images/c806e37d557b8f75e1fd2bea4197635f9d7604a83c6ab01279eedd1afbb6a443.jpg)  
(c) $\epsilon = 1$ , trajectory of 10,000 steps

![](images/07cff234e6f2248b2bbff24e314143dc88acfa3f06acc39bdeff1eda2196930d.jpg)  
(g) $\epsilon = 0 . 5$ , trajectory of 10,000 steps

![](images/669f48906b7b4314c4d998c4eec52cbb75627a07c3fd3c87cc2e53192fc47064.jpg)  
(d) $\epsilon = 1$ , number of times each action is visited within 1 million steps

![](images/8e84798e7d4ea5eaab4eeee1852dd8d6903a00b3157b64f7dcd94b8458065820.jpg)  
(h) $\epsilon = 0 . 5$ , number of times each action is visited within 1 million steps   
Figure 5.8: Exploration abilities of $\epsilon$ -greedy policies with different values of .

exploration and gradually reduce it to ensure the optimality of the final policy [21–23].

# 5.6 Summary

The algorithms in this chapter are the first model-free reinforcement learning algorithms ever introduced in this book. We first introduced the idea of MC estimation by examining an important mean estimation problem. Then, three MC-based algorithms were introduced.

 MC Basic: This is the simplest MC-based reinforcement learning algorithm. This algorithm is obtained by replacing the model-based policy evaluation step in the policy iteration algorithm with a model-free MC-based estimation component. Given sufficient samples, it is guaranteed that this algorithm can converge to optimal policies and optimal state values.   
MC Exploring Starts: This algorithm is a variant of MC Basic. It can be obtained from the MC Basic algorithm using the first-visit or every-visit strategy to use samples more efficiently.   
MC $\epsilon$ -Greedy: This algorithm is a variant of MC Exploring Starts. Specifically, in the policy improvement step, it searches for the best $\epsilon$ -greedy policies instead of greedy policies. In this way, the exploration ability of the policy is enhanced and hence the condition of exploring starts can be removed.

Finally, a tradeoff between exploration and exploitation was introduced by examining the properties of $\epsilon$ -greedy policies. As the value of $\epsilon$ increases, the exploration ability of $\epsilon$ -greedy policies increases, and the exploitation of greedy actions decreases. On the other hand, if the value of $\epsilon$ decreases, we can better exploit the greedy actions, but the exploration ability is compromised.

# 5.7 Q&A

Q: What is Monte Carlo estimation?

A: Monte Carlo estimation refers to a broad class of techniques that use stochastic samples to solve approximation problems.

Q: What is the mean estimation problem?

A: The mean estimation problem refers to calculating the expected value of a random variable based on stochastic samples.

 Q: How to solve the mean estimation problem?

A: There are two approaches: model-based and model-free. In particular, if the probability distribution of a random variable is known, the expected value can be calculated

based on its definition. If the probability distribution is unknown, we can use Monte Carlo estimation to approximate the expected value. Such an approximation is accurate when the number of samples is large.

 Q: Why is the mean estimation problem important for reinforcement learning?

A: Both state and action values are defined as expected values of returns. Hence, estimating state or action values is essentially a mean estimation problem.

 Q: What is the core idea of model-free MC-based reinforcement learning?

A: The core idea is to convert the policy iteration algorithm to a model-free one. In particular, while the policy iteration algorithm aims to calculate values based on the system model, MC-based reinforcement learning replaces the model-based policy evaluation step in the policy iteration algorithm with a model-free MC-based policy evaluation step.

 Q: What are initial-visit, first-visit, and every-visit strategies?

A: They are different strategies for utilizing the samples in an episode. An episode may visit many state-action pairs. The initial-visit strategy uses the entire episode to estimate the action value of the initial state-action pair. The every-visit and first-visit strategies can better utilize the given samples. If the rest of the episode is used to estimate the action value of a state-action pair every time it is visited, such a strategy is called every-visit. If we only count the first time a state-action pair is visited in the episode, such a strategy is called first-visit.

Q: What is exploring starts? Why is it important?

A: Exploring starts requires an infinite number of (or sufficiently many) episodes to be generated when starting from every state-action pair. In theory, the exploring starts condition is necessary to find optimal policies. That is, only if every action value is well explored, can we accurately evaluate all the actions and then correctly select the optimal ones.

 Q: What is the idea used to avoid exploring starts?

A: The fundamental idea is to make policies soft. Soft policies are stochastic, enabling an episode to visit many state-action pairs. In this way, we do not need a large number of episodes starting from every state-action pair.

 Q: Can an $\epsilon$ -greedy policy be optimal?

A: The answer is both yes and no. By yes, we mean that, if given sufficient samples, the MC $\epsilon$ -Greedy algorithm can converge to an optimal $\epsilon$ -greedy policy. By no, we mean that the converged policy is merely optimal among all $\epsilon$ -greedy policies (with the same value of $\epsilon$ ).

Q: Is it possible to use one episode to visit all state-action pairs?

A: Yes, it is possible. If the policy is soft (e.g., $\epsilon$ -greedy) and the episode is sufficiently long.

 Q: What is the relationship between MC Basic, MC Exploring Starts, and MC - Greedy?

A: MC Basic is the simplest MC-based reinforcement learning algorithm. It is important because it reveals the fundamental idea of model-free MC-based reinforcement learning. MC Exploring Starts is a variant of MC Basic that adjusts the sample usage strategy. Furthermore, MC $\epsilon$ -Greedy is a variant of MC Exploring Starts that removes the exploring starts requirement. Therefore, while the basic idea is simple, complication appears when we want to achieve better performance. It is important to split the core idea from the complications that may be distracting for beginners.

# Chapter 6

# Stochastic Approximation

![](images/3a16c160fed29f422a0418fcaebecf23dc72c6501f24ba6013f79d7fcf1008dd.jpg)  
Figure 6.1: Where we are in this book.

Chapter 5 introduced the first class of model-free reinforcement learning algorithms based on Monte Carlo estimation. In the next chapter (Chapter 7), we will introduce another class of model-free reinforcement learning algorithms: temporal-difference learning. However, before proceeding to the next chapter, we need to press the pause button to better prepare ourselves. This is because temporal-difference algorithms are very different from the algorithms that we have studied so far. Many readers who see the temporaldifference algorithms for the first time often wonder how these algorithms were designed in the first place and why they can work effectively. In fact, there is a knowledge gap between the previous and subsequent chapters: the algorithms we have studied so far are

non-incremental, but the algorithms that we will study in the subsequent chapters are incremental.

We use the present chapter to fill this knowledge gap by introducing the basics of stochastic approximation. Although this chapter does not introduce any specific reinforcement learning algorithms, it lays the necessary foundations for studying subsequent chapters. We will see in Chapter 7 that the temporal-difference algorithms can be viewed as special stochastic approximation algorithms. The well-known stochastic gradient descent algorithms widely used in machine learning are also introduced in the present chapter.

# 6.1 Motivating example: Mean estimation

We next demonstrate how to convert a non-incremental algorithm to an incremental one by examining the mean estimation problem.

Consider a random variable $X$ that takes values from a finite set $\mathcal { X }$ . Our goal is to estimate $\mathbb { E } { \lvert X \rvert }$ . Suppose that we have a sequence of i.i.d. samples $\{ x _ { i } \} _ { i = 1 } ^ { n }$ . The expected value of $X$ can be approximated by

$$
\mathbb {E} [ X ] \approx \bar {x} \doteq \frac {1}{n} \sum_ {i = 1} ^ {n} x _ {i}. \tag {6.1}
$$

The approximation in (6.1) is the basic idea of Monte Carlo estimation, as introduced in Chapter 5. We know that ${ \bar { x } } \to \mathbb { E } | X |$ as $n \to \infty$ according to the law of large numbers.

We next show that two methods can be used to calculate $x$ in (6.1). The first nonincremental method collects all the samples first and then calculates the average. The drawback of such a method is that, if the number of samples is large, we may have to wait for a long time until all of the samples are collected. The second method can avoid this drawback because it calculates the average in an incremental manner. Specifically, suppose that

$$
w _ {k + 1} \doteq \frac {1}{k} \sum_ {i = 1} ^ {k} x _ {i}, \quad k = 1, 2, \ldots
$$

and hence

$$
w _ {k} = \frac {1}{k - 1} \sum_ {i = 1} ^ {k - 1} x _ {i}, \quad k = 2, 3, \ldots .
$$

Then, $w _ { k + 1 }$ can be expressed in terms of $w _ { k }$ as

$$
w _ {k + 1} = \frac {1}{k} \sum_ {i = 1} ^ {k} x _ {i} = \frac {1}{k} \left(\sum_ {i = 1} ^ {k - 1} x _ {i} + x _ {k}\right) = \frac {1}{k} ((k - 1) w _ {k} + x _ {k}) = w _ {k} - \frac {1}{k} (w _ {k} - x _ {k}).
$$

Therefore, we obtain the following incremental algorithm:

$$
w _ {k + 1} = w _ {k} - \frac {1}{k} (w _ {k} - x _ {k}). \tag {6.2}
$$

This algorithm can be used to calculate the mean $x$ in an incremental manner. It can be verified that

$$
w _ {1} = x _ {1},
$$

$$
w _ {2} = w _ {1} - \frac {1}{1} (w _ {1} - x _ {1}) = x _ {1},
$$

$$
w _ {3} = w _ {2} - \frac {1}{2} (w _ {2} - x _ {2}) = x _ {1} - \frac {1}{2} (x _ {1} - x _ {2}) = \frac {1}{2} (x _ {1} + x _ {2}),
$$

$$
w _ {4} = w _ {3} - \frac {1}{3} (w _ {3} - x _ {3}) = \frac {1}{3} (x _ {1} + x _ {2} + x _ {3}),
$$

$$
w _ {k + 1} = \frac {1}{k} \sum_ {i = 1} ^ {k} x _ {i}. \tag {6.3}
$$

The advantage of (6.2) is that the average can be immediately calculated every time we receive a sample. This average can be used to approximate $x$ and hence $\mathbb { E } { \lvert X \rvert }$ . Notably, the approximation may not be accurate at the beginning due to insufficient samples. However, it is better than nothing. As more samples are obtained, the estimation accuracy can be gradually improved according to the law of large numbers. In addition, one can also define wk+1 = 11+k Pk+1i=1 xi and wk = 1k Pki=1 xi. Doing so would not make $\begin{array} { r } { w _ { k + 1 } = \frac { 1 } { 1 + k } \sum _ { i = 1 } ^ { k + 1 } x _ { i } } \end{array}$ 1k+1 $\begin{array} { r } { \boldsymbol { w _ { k } } \ : = \ : \frac { 1 } { k } \sum _ { i = 1 } ^ { k } \boldsymbol { x _ { i } } } \end{array}$ any significant difference. In this case, the corresponding iterative algorithm is $w _ { k + 1 } =$ $\begin{array} { r } { w _ { k } - \frac { 1 } { 1 + k } ( w _ { k } - x _ { k + 1 } ) } \end{array}$ .

Furthermore, consider an algorithm with a more general expression:

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} (w _ {k} - x _ {k}). \tag {6.4}
$$

This algorithm is important and frequently used in this chapter. It is the same as (6.2) except that the coefficient $1 / k$ is replaced by $\alpha _ { k } > 0$ . Since the expression of $\alpha _ { k }$ is not given, we are not able to obtain the explicit expression of $w _ { k }$ as in (6.3). However, we will show in the next section that, if $\left\{ \alpha _ { k } \right\}$ satisfies some mild conditions, $w _ { k } \to \mathbb { E } [ X ]$ as $k  \infty$ . In Chapter 7, we will see that temporal-difference algorithms have similar (but more complex) expressions.

# 6.2 Robbins-Monro algorithm

Stochastic approximation refers to a broad class of stochastic iterative algorithms for solving root-finding or optimization problems [24]. Compared to many other root-finding

algorithms such as gradient-based ones, stochastic approximation is powerful in the sense that it does not require the expression of the objective function or its derivative.

The Robbins-Monro (RM) algorithm is a pioneering work in the field of stochastic approximation [24–27]. The famous stochastic gradient descent algorithm is a special form of the RM algorithm, as shown in Section 6.4. We next introduce the details of the RM algorithm.

Suppose that we would like to find the root of the equation

$$
g (w) = 0,
$$

where $w \in \mathbb R$ is the unknown variable and $g : \mathbb { R }  \mathbb { R }$ is a function. Many problems can be formulated as root-finding problems. For example, if $J ( w )$ is an objective function to be optimized, this optimization problem can be converted to solving $g ( w ) \doteq \nabla _ { w } J ( w ) = 0$ . In addition, an equation such as $g ( w ) = c$ , where $c$ is a constant, can also be converted to the above equation by rewriting $g ( w ) - c$ as a new function.

If the expression of $g$ or its derivative is known, there are many numerical algorithms that can be used. However, the problem we are facing is that the expression of the function $g$ is unknown. For example, the function may be represented by an artificial neural network whose structure and parameters are unknown. Moreover, we can only obtain a noisy observation of $g ( w )$ :

$$
\tilde {g} (w, \eta) = g (w) + \eta ,
$$

where $\eta \in \mathbb { R }$ is the observation error, which may or may not be Gaussian. In summary, it is a black-box system where only the input $w$ and the noisy output $\widetilde g ( w , \eta )$ are known (see Figure 6.2). Our aim is to solve $g ( w ) = 0$ using $w$ and $\tilde { g }$ .

![](images/eb26ce47c052697734ad19d4d44794e88b9971683bd8910aecac3e99412309f0.jpg)  
Figure 6.2: An illustration of the problem of solving $g ( w ) = 0$ from $w$ and $\tilde { g }$ .

The RM algorithm that can solve $g ( w ) = 0$ is

$$
w _ {k + 1} = w _ {k} - a _ {k} \tilde {g} \left(w _ {k}, \eta_ {k}\right), \quad k = 1, 2, 3, \dots \tag {6.5}
$$

where $w _ { k }$ is the $k$ th estimate of the root, $\tilde { g } ( w _ { k } , \eta _ { k } )$ is the $k$ th noisy observation, and $a _ { k }$ is a positive coefficient. As can be seen, the RM algorithm does not require any information about the function. It only requires the input and output.

![](images/5c649b0785e9b09322ed0122425243102cfdd22531cb2da1ed2ecc2cd884f0e3.jpg)

![](images/63b0209c058cab90295214fb6e461a3984f26a23196db41a76aed6b9356b5a8e.jpg)  
Figure 6.3: An illustrative example of the RM algorithm.

To illustrate the RM algorithm, consider an example in which $g ( w ) = w ^ { 3 } - 5$ . The true root is $5 ^ { 1 / 3 } \approx 1 . 7 1$ . Now, suppose that we can only observe the input $w$ and the output $\tilde { g } ( w ) = g ( w ) + \eta$ , where $\eta$ is i.i.d. and obeys a standard normal distribution with a zero mean and a standard deviation of 1. The initial guess is $w _ { 1 } = 0$ , and the coefficient is $a _ { k } = 1 / k$ . The evolution process of $w _ { k }$ is shown in Figure 6.3. Even though the observation is corrupted by noise $\eta _ { k }$ , the estimate $w _ { k }$ can still converge to the true root. Note that the initial guess $w _ { 1 }$ must be properly selected to ensure convergence for the specific function of $g ( w ) = w ^ { 3 } - 5$ . In the following subsection, we present the conditions under which the RM algorithm converges for any initial guesses.

# 6.2.1 Convergence properties

Why can the RM algorithm in (6.5) find the root of $g ( w ) = 0$ ? We next illustrate the idea with an example and then provide a rigorous convergence analysis.

Consider the example shown in Figure 6.4. In this example, $g ( w ) = \operatorname { t a n h } ( w - 1 )$ . The true root of $g ( w ) = 0$ is $w ^ { * } = 1$ . We apply the RM algorithm with $w _ { 1 } = 3$ and $a _ { k } = 1 / k$ . To better illustrate the reason for convergence, we simply set $\eta _ { k } \equiv 0$ , and consequently, $\tilde { g } ( w _ { k } , \eta _ { k } ) = g ( w _ { k } )$ . The RM algorithm in this case is $w _ { k + 1 } = w _ { k } - a _ { k } g ( w _ { k } )$ . The resulting $\{ w _ { k } \}$ generated by the RM algorithm is shown in Figure 6.4. It can be seen that $w _ { k }$ converges to the true root $w ^ { * } = 1$ .

This simple example can illustrate why the RM algorithm converges.

When $w _ { k } > w ^ { * }$ , we have $g ( w _ { k } ) > 0$ . Then, $w _ { k + 1 } = w _ { k } - a _ { k } g ( w _ { k } ) < w _ { k }$ . If $a _ { k } g ( w _ { k } )$ is sufficiently small, we have $w ^ { \ast } < w _ { k + 1 } < w _ { k }$ . As a result, $w _ { k + 1 }$ is closer to $w ^ { * }$ than $w _ { k }$ .   
 When $w _ { k } < w ^ { * }$ , we have $g ( w _ { k } ) < 0$ . Then, $w _ { k + 1 } = w _ { k } - a _ { k } g ( w _ { k } ) > w _ { k }$ . If $| a _ { k } g ( w _ { k } ) |$ i s sufficiently small, we have $w ^ { \ast } > w _ { k + 1 } > w _ { k }$ . As a result, $w _ { k + 1 }$ is closer to $w ^ { * }$ than $w _ { k }$ .

In either case, $w _ { k + 1 }$ is closer to $w ^ { * }$ than $w _ { k }$ . Therefore, it is intuitive that $w _ { k }$ converges to $w ^ { * }$ .

![](images/be284f6bb041179ce377791b2dc6833b1cbbda8089d9bb072e0943676742d622.jpg)  
Figure 6.4: An example for illustrating the convergence of the RM algorithm.

The above example is simple since the observation error is assumed to be zero. It would be nontrivial to analyze the convergence in the presence of stochastic observation errors. A rigorous convergence result is given below.

Theorem 6.1 (Robbins-Monro theorem). In the Robbins-Monro algorithm in (6.5), if

(a) $0 < c _ { 1 } \leq \nabla _ { w } g ( w ) \leq c _ { 2 }$ for all $w$   
$\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } = \infty$ and $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } < \infty$   
(c) $\mathbb { E } [ \eta _ { k } | \mathcal { H } _ { k } ] = 0$ and $\mathbb { E } [ \eta _ { k } ^ { 2 } \vert \mathcal { H } _ { k } ] < \infty$

where $\mathcal { H } _ { k } = \{ w _ { k } , w _ { k - 1 } , . . . \}$ , then $w _ { k }$ almost surely converges to the root $w ^ { * }$ satisfying $g ( w ^ { * } ) = 0$ .

We postpone the proof of this theorem to Section 6.3.3. This theorem relies on the notion of almost sure convergence, which is introduced in Appendix B.

The three conditions in Theorem 6.1 are explained as follows.

In the first condition, $0 < c _ { 1 } \le \nabla _ { w } g ( w )$ indicates that $g ( w )$ is a monotonically increasing function. This condition ensures that the root of $g ( w ) = 0$ exists and is unique. If $g ( w )$ is monotonically decreasing, we can simply treat $- g ( w )$ as a new function that is monotonically increasing.

As an application, we can formulate an optimization problem in which the objective function is $J ( w )$ as a root-finding problem: $g ( w ) \doteq \nabla _ { w } J ( w ) = 0$ . In this case, the condition that $g ( w )$ is monotonically increasing indicates that $J ( w )$ is convex, which is a commonly adopted assumption in optimization problems.

The inequality $\nabla _ { w } g ( w ) \le c _ { 2 }$ indicates that the gradient of $g ( w )$ is bounded from above. For example, $g ( w ) = \operatorname { t a n h } ( w - 1 )$ satisfies this condition, but $g ( w ) = w ^ { 3 } - 5$ does not.

$\diamond$ The second condition about $\{ a _ { k } \}$ is interesting. We often see conditions like this in reinforcement learning algorithms. In particular, the condition $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } < \infty$ means that $\scriptstyle \operatorname* { l i m } _ { n \to \infty } \sum _ { k = 1 } ^ { n } a _ { k } ^ { 2 }$ is bounded from above. It requires that $a _ { k }$ converges to zero as $k  \infty$ . The condition $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } = \infty$ means that $\scriptstyle \operatorname* { l i m } _ { n \to \infty } \sum _ { k = 1 } ^ { n } a _ { k }$ is infinitely large. It requires that $a _ { k }$ should not converge to zero too fast. These conditions have interesting properties, which will be analyzed in detail shortly.   
The third condition is mild. It does not require the observation error $\eta _ { k }$ to be Gaussian. An important special case is that $\{ \eta _ { k } \}$ is an i.i.d. stochastic sequence satisfying $\mathbb { E } [ \eta _ { k } ] = 0$ and $\mathbb { E } [ \eta _ { k } ^ { 2 } ] < \infty$ . In this case, the third condition is valid because $\eta _ { k }$ i s independent of $\mathcal { H } _ { k }$ and hence we have $\mathbb { E } [ \eta _ { k } | \mathcal { H } _ { k } ] = \mathbb { E } [ \eta _ { k } ] = 0$ and $\mathbb { E } [ \eta _ { k } ^ { 2 } | \mathcal { H } _ { k } ] = \mathbb { E } [ \eta _ { k } ^ { 2 } ]$ .

We next examine the second condition about the coefficients $\{ a _ { k } \}$ more closely.

 Why is the second condition important for the convergence of the RM algorithm?

This question can naturally be answered when we present a rigorous proof of the above theorem later. Here, we would like to provide some insightful intuition.

First, $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } < \infty$ indicates that $a _ { k } \to 0$ as $k  \infty$ . Why is this condition important? Suppose that the observation $\tilde { g } ( w _ { k } , \eta _ { k } )$ is always bounded. Since

$$
w _ {k + 1} - w _ {k} = - a _ {k} \tilde {g} (w _ {k}, \eta_ {k}),
$$

if $a _ { k } \to 0$ , then $a _ { k } \tilde { g } ( w _ { k } , \eta _ { k } ) \to 0$ and hence $w _ { k + 1 } - w _ { k } \to 0$ , indicating that $w _ { k + 1 }$ and $w _ { k }$ approach each other when $k  \infty$ . Otherwise, if $a _ { k }$ does not converge, then $w _ { k }$ may still fluctuate when $k  \infty$ .

Second, $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } = \infty$ indicates that $a _ { k }$ should not converge to zero too fast. Why is this condition important? Summarizing both sides of the equations of $w _ { 2 } - w _ { 1 } =$ $- a _ { 1 } \tilde { g } ( w _ { 1 } , \eta _ { 1 } )$ , $w _ { 3 } - w _ { 2 } = - a _ { 2 } \tilde { g } ( w _ { 2 } , \eta _ { 2 } )$ , $w _ { 4 } - w _ { 3 } = - a _ { 3 } \tilde { g } ( w _ { 3 } , \eta _ { 3 } )$ , . . . gives

$$
w _ {1} - w _ {\infty} = \sum_ {k = 1} ^ {\infty} a _ {k} \tilde {g} (w _ {k}, \eta_ {k}).
$$

If $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } < \infty$ , then $\textstyle { \big | } \sum _ { k = 1 } ^ { \infty } a _ { k } { \tilde { g } } { \big ( } w _ { k } , \eta _ { k } { \big ) } { \big | }$ is also bounded. Let $b$ denote the finite upper bound such that

$$
\left| w _ {1} - w _ {\infty} \right| = \left| \sum_ {k = 1} ^ {\infty} a _ {k} \tilde {g} \left(w _ {k}, \eta_ {k}\right) \right| \leq b. \tag {6.6}
$$

If the initial guess $w _ { 1 }$ is selected far away from $w ^ { * }$ so that $| w _ { 1 } - w ^ { * } | > b$ , then it is impossible to have $w _ { \infty } = w ^ { * }$ according to (6.6). This suggests that the RM algorithm cannot find the true solution $w ^ { * }$ in this case. Therefore, the condition $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } = \infty$ is necessary to ensure convergency given an arbitrary initial guess.

 What kinds of sequences satisfy $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } = \infty$ and $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } < \infty$

One typical sequence is

$$
a _ {k} = \frac {1}{k}.
$$

On the one hand, it holds that

$$
\lim  _ {n \rightarrow \infty} \left(\sum_ {k = 1} ^ {n} \frac {1}{k} - \ln n\right) = \kappa ,
$$

where $\kappa \approx 0 . 5 7 7$ is called the Euler-Mascheroni constant (or Euler’s constant) [28]. Since $\ln n  \infty$ as $n \to \infty$ , we have

$$
\sum_ {k = 1} ^ {\infty} \frac {1}{k} = \infty .
$$

In fact, $\begin{array} { r } { H _ { n } = \sum _ { k = 1 } ^ { n } \frac { 1 } { k } } \end{array}$ is called the harmonic number in number theory [29]. On the other hand, it holds that

$$
\sum_ {k = 1} ^ {\infty} \frac {1}{k ^ {2}} = \frac {\pi^ {2}}{6} <   \infty .
$$

Finding the value of $\scriptstyle \sum _ { k = 1 } ^ { \infty } { \frac { 1 } { k ^ { 2 } } }$ is known as the Basel problem [30].

In summary, the sequence $\{ a _ { k } = 1 / k \}$ satisfies the second condition in Theorem 6.1. Notably, a slight modification, such as $a _ { k } \ = \ 1 / ( k + 1 )$ or $a _ { k } ~ = ~ c _ { k } / k$ where $c _ { k }$ is bounded, also preserves this condition.

In the RM algorithm, $a _ { k }$ is often selected as a sufficiently small constant in many applications. Although the second condition is not satisfied anymore in this case because $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } = \infty$ rather than $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } < \infty$ , the algorithm can still converge in a certain sense [24, Section 1.5]. In addition, $g ( x ) = x ^ { 3 } - 5$ in the example shown in Figure 6.3 does not satisfy the first condition, but the RM algorithm can still find the root if the initial guess is adequately (not arbitrarily) selected.

# 6.2.2 Application to mean estimation

We next apply the Robbins-Monro theorem to analyze the mean estimation problem, which has been discussed in Section 6.1. Recall that

$$
w _ {k + 1} = w _ {k} + \alpha_ {k} \left(x _ {k} - w _ {k}\right)
$$

is the mean estimation algorithm in (6.4). When $\alpha _ { k } = 1 / k$ , we can obtain the analytical expression of $w _ { k + 1 }$ as $w _ { k + 1 } = 1 / k \textstyle \sum _ { i = 1 } ^ { k } x _ { i }$ . However, we would not be able to obtain an analytical expression when given general values of $\alpha _ { k }$ . In this case, the convergence analysis is nontrivial. We can show that the algorithm in this case is a special RM

algorithm and hence its convergence naturally follows.

In particular, define a function as

$$
g (w) \doteq w - \mathbb {E} [ X ].
$$

The original problem is to obtain the value of $\mathbb { E } { \lvert X \rvert }$ . This problem is formulated as a root-finding problem to solve $g ( w ) = 0$ . Given a value of $w$ , the noisy observation that we can obtain is $\tilde { g } \doteq w - x$ , where $x$ is a sample of $X$ . Note that $\tilde { g }$ can be written as

$$
\begin{array}{l} \tilde {g} (w, \eta) = w - x \\ = w - x + \mathbb {E} [ X ] - \mathbb {E} [ X ] \\ = (w - \mathbb {E} [ X ]) + (\mathbb {E} [ X ] - x) \dot {=} g (w) + \eta , \\ \end{array}
$$

where $\eta \doteq \mathbb { E } \lfloor X \rfloor - x$ .

The RM algorithm for solving this problem is

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \tilde {g} \left(w _ {k}, \eta_ {k}\right) = w _ {k} - \alpha_ {k} \left(w _ {k} - x _ {k}\right),
$$

which is exactly the algorithm in (6.4). As a result, it is guaranteed by Theorem 6.1 that $w _ { k }$ converges to $\mathbb { E } { \lvert X \rvert }$ almost surely if $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } = \infty$ , $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } ^ { 2 } < \infty$ , and $\{ x _ { k } \}$ is i.i.d. It is worth mentioning that the convergence property does not rely on any assumption regarding the distribution of $X$ .

# 6.3 Dvoretzky’s convergence theorem

Until now, the convergence of the RM algorithm has not yet been proven. To do that, we next introduce Dvoretzky’s theorem [31, 32], which is a classic result in the field of stochastic approximation. This theorem can be used to analyze the convergence of the RM algorithm and many reinforcement learning algorithms.

This section is slightly mathematically intensive. Readers who are interested in the convergence analyses of stochastic algorithms are recommended to study this section. Otherwise, this section can be skipped.

Theorem 6.2 (Dvoretzky’s theorem). Consider a stochastic process

$$
\Delta_ {k + 1} = (1 - \alpha_ {k}) \Delta_ {k} + \beta_ {k} \eta_ {k},
$$

where $\{ \alpha _ { k } \} _ { k = 1 } ^ { \infty } , \{ \beta _ { k } \} _ { k = 1 } ^ { \infty } , \{ \eta _ { k } \} _ { k = 1 } ^ { \infty }$ are stochastic sequences. Here $\alpha _ { k } \geq 0 , \beta _ { k } \geq 0$ for all $k$ . Then, $\Delta _ { k }$ converges to zero almost surely if the following conditions are satisfied:

(a) $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } = \infty$ , $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } ^ { 2 } < \infty$ , and $\textstyle \sum _ { k = 1 } ^ { \infty } \beta _ { k } ^ { 2 } < \infty$ uniformly almost surely;   
(b) $\mathbb { E } [ \eta _ { k } \vert \mathcal { H } _ { k } ] = 0$ and $\mathbb { E } [ \eta _ { k } ^ { 2 } \vert \mathcal { H } _ { k } ] \le C$ almost surely;

where $\mathcal { H } _ { k } = \{ \Delta _ { k } , \Delta _ { k - 1 } , \ldots , \eta _ { k - 1 } , \ldots , \alpha _ { k - 1 } , \ldots , \beta _ { k - 1 } , \ldots \} .$

Before presenting the proof of this theorem, we first clarify some issues.

 In the RM algorithm, the coefficient sequence $\left\{ \alpha _ { k } \right\}$ is deterministic. However, Dvoretzky’s theorem allows $\{ \alpha _ { k } \} , \{ \beta _ { k } \}$ to be random variables that depend on $\mathcal { H } _ { k }$ . Thus, it is more useful in cases where $\alpha _ { k }$ or $\beta _ { k }$ is a function of $\Delta _ { k }$ .   
> In the first condition, it is stated as “uniformly almost surely”. This is because $\alpha _ { k }$ and $\beta _ { k }$ may be random variables and hence the definition of their limits must be in the stochastic sense. In the second condition, it is also stated as “almost surely”. This is because $\mathcal { H } _ { k }$ is a sequence of random variables rather than specific values. As a result, $\mathbb { E } [ \eta _ { k } \vert \mathcal { H } _ { k } ]$ and $\mathbb { E } [ \eta _ { k } ^ { 2 } | \mathcal { H } _ { k } ]$ are random variables. The definition of the conditional expectation in this case is in the “almost sure” sense (Appendix B).   
The statement of Theorem 6.2 is slightly different from [32] in the sense that Theorem 6.2 does not require $\textstyle \sum _ { k = 1 } ^ { \infty } \beta _ { k } = \infty$ in the first condition. When $\textstyle \sum _ { k = 1 } ^ { \infty } \beta _ { k } < \infty$ , especially in the extreme case where $\beta _ { k } = 0$ for all $k$ , the sequence can still converge.

# 6.3.1 Proof of Dvoretzky’s theorem

The original proof of Dvoretzky’s theorem was given in 1956 [31]. There are also other proofs. We next present a proof based on quasimartingales. With the convergence results of quasimartingales, the proof of Dvoretzky’s theorem is straightforward. More information about quasimartingales can be found in Appendix C.

Proof of Dvoretzky’s theorem. Let $h _ { k } \doteq \Delta _ { k } ^ { 2 }$ . Then,

$$
\begin{array}{l} h _ {k + 1} - h _ {k} = \Delta_ {k + 1} ^ {2} - \Delta_ {k} ^ {2} \\ = (\Delta_ {k + 1} - \Delta_ {k}) (\Delta_ {k + 1} + \Delta_ {k}) \\ = (- \alpha_ {k} \Delta_ {k} + \beta_ {k} \eta_ {k}) [ (2 - \alpha_ {k}) \Delta_ {k} + \beta_ {k} \eta_ {k} ] \\ = - \alpha_ {k} (2 - \alpha_ {k}) \Delta_ {k} ^ {2} + \beta_ {k} ^ {2} \eta_ {k} ^ {2} + 2 (1 - \alpha_ {k}) \beta_ {k} \eta_ {k} \Delta_ {k}. \\ \end{array}
$$

Taking expectations on both sides of the above equation yields

$$
\mathbb {E} \left[ h _ {k + 1} - h _ {k} \mid \mathcal {H} _ {k} \right] = \mathbb {E} \left[ - \alpha_ {k} \left(2 - \alpha_ {k}\right) \Delta_ {k} ^ {2} \mid \mathcal {H} _ {k} \right] + \mathbb {E} \left[ \beta_ {k} ^ {2} \eta_ {k} ^ {2} \mid \mathcal {H} _ {k} \right] + \mathbb {E} \left[ 2 \left(1 - \alpha_ {k}\right) \beta_ {k} \eta_ {k} \Delta_ {k} \mid \mathcal {H} _ {k} \right]. \tag {6.7}
$$

First, since $\Delta _ { k }$ is included and hence determined by $\mathcal { H } _ { k }$ , it can be taken out from the expectation (see property (e) in Lemma B.1). Second, consider the simple case

where $\alpha _ { k } , \beta _ { k }$ is determined by $\mathcal { H } _ { k }$ . This case is valid when, for example, $\left\{ \alpha _ { k } \right\}$ and $\{ \beta _ { k } \}$ are functions of $\Delta _ { k }$ or deterministic sequences. Then, they can also be taken out of the expectation. Therefore, (6.7) becomes

$$
\mathbb {E} [ h _ {k + 1} - h _ {k} | \mathcal {H} _ {k} ] = - \alpha_ {k} (2 - \alpha_ {k}) \Delta_ {k} ^ {2} + \beta_ {k} ^ {2} \mathbb {E} [ \eta_ {k} ^ {2} | \mathcal {H} _ {k} ] + 2 (1 - \alpha_ {k}) \beta_ {k} \Delta_ {k} \mathbb {E} [ \eta_ {k} | \mathcal {H} _ {k} ]. \quad (6. 8)
$$

For the first term, since $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } ^ { 2 } < \infty$ implies $\alpha _ { k }  0$ almost surely, there exists a finite $n$ such that $\alpha _ { k } \leq 1$ almost surely for all $k \geq n$ . Without loss of generality, we next merely consider the case of $\alpha _ { k } \leq 1$ . Then, $- \alpha _ { k } ( 2 - \alpha _ { k } ) \Delta _ { k } ^ { 2 } \le 0$ . For the second term, we have $\beta _ { k } ^ { 2 } \mathbb { E } [ \eta _ { k } ^ { 2 } | \mathcal { H } _ { k } ] \le \beta _ { k } ^ { 2 } C$ as assumed. The third term equals zero because $\mathbb { E } [ \eta _ { k } \vert \mathcal { H } _ { k } ] = 0$ as assumed. Therefore, (6.8) becomes

$$
\mathbb {E} \left[ h _ {k + 1} - h _ {k} \mid \mathcal {H} _ {k} \right] = - \alpha_ {k} \left(2 - \alpha_ {k}\right) \Delta_ {k} ^ {2} + \beta_ {k} ^ {2} \mathbb {E} \left[ \eta_ {k} ^ {2} \mid \mathcal {H} _ {k} \right] \leq \beta_ {k} ^ {2} C, \tag {6.9}
$$

and hence

$$
\sum_ {k = 1} ^ {\infty} \mathbb {E} [ h _ {k + 1} - h _ {k} | \mathcal {H} _ {k} ] \leq \sum_ {k = 1} ^ {\infty} \beta_ {k} ^ {2} C <   \infty .
$$

The last inequality is due to the condition $\textstyle \sum _ { k = 1 } ^ { \infty } \beta _ { k } ^ { 2 } < \infty$ . Then, based on the quasimartingale convergence theorem in Appendix C, we conclude that $h _ { k }$ converges almost surely.

We next determine what value $\Delta _ { k }$ converges to. It follows from (6.9) that

$$
\sum_ {k = 1} ^ {\infty} \alpha_ {k} (2 - \alpha_ {k}) \Delta_ {k} ^ {2} = \sum_ {k = 1} ^ {\infty} \beta_ {k} ^ {2} \mathbb {E} [ \eta_ {k} ^ {2} | \mathcal {H} _ {k} ] - \sum_ {k = 1} ^ {\infty} \mathbb {E} [ h _ {k + 1} - h _ {k} | \mathcal {H} _ {k} ].
$$

The first term on the right-hand side is bounded as assumed. The second term is also bounded because $h _ { k }$ converges and hence $h _ { k + 1 } \ : - \ : h _ { k }$ is summable. Thus, $\begin{array} { r } { \sum _ { k = 1 } ^ { \infty } \alpha _ { k } ( 2 - \alpha _ { k } ) \Delta _ { k } ^ { 2 } } \end{array}$ on the left-hand side is also bounded. Since we consider the case of $\alpha _ { k } \leq 1$ , we have

$$
\infty > \sum_ {k = 1} ^ {\infty} \alpha_ {k} (2 - \alpha_ {k}) \Delta_ {k} ^ {2} \geq \sum_ {k = 1} ^ {\infty} \alpha_ {k} \Delta_ {k} ^ {2} \geq 0.
$$

Therefore, $\scriptstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } \Delta _ { k } ^ { 2 }$ is bounded. Since $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } = \infty$ , we must have $\Delta _ { k } \  \ 0$ almost surely.

# 6.3.2 Application to mean estimation

While the mean estimation algorithm, $w _ { k + 1 } = w _ { k } + \alpha _ { k } ( x _ { k } - w _ { k } )$ , has been analyzed using the RM theorem, we next show that its convergence can also be directly proven by Dvoretzky’s theorem.

Proof. Let $w ^ { * } = \mathbb { E } | X |$ . The mean estimation algorithm $w _ { k + 1 } = w _ { k } + \alpha _ { k } ( x _ { k } - w _ { k } )$ can be rewritten as

$$
w _ {k + 1} - w ^ {*} = w _ {k} - w ^ {*} + \alpha_ {k} \left(x _ {k} - w ^ {*} + w ^ {*} - w _ {k}\right).
$$

Let $\Delta \doteq w - w ^ { * }$ . Then, we have

$$
\begin{array}{l} \Delta_ {k + 1} = \Delta_ {k} + \alpha_ {k} \left(x _ {k} - w ^ {*} - \Delta_ {k}\right) \\ = (1 - \alpha_ {k}) \Delta_ {k} + \alpha_ {k} \underbrace {\left(x _ {k} - w ^ {*}\right)} _ {\eta_ {k}}. \\ \end{array}
$$

Since $\{ x _ { k } \}$ is i.i.d., we have $\mathbb { E } [ x _ { k } | \mathcal { H } _ { k } ] = \mathbb { E } [ x _ { k } ] = w ^ { * }$ . As a result, $\mathbb { E } [ \eta _ { k } | \mathcal { H } _ { k } ] = \mathbb { E } [ x _ { k } -$ $w ^ { * } | \mathcal { H } _ { k } ] = 0$ and $\mathbb { E } [ \eta _ { k } ^ { 2 } | \mathcal { H } _ { k } ] = \mathbb { E } [ x _ { k } ^ { 2 } | \mathcal { H } _ { k } ] - ( w ^ { * } ) ^ { 2 } = \mathbb { E } [ x _ { k } ^ { 2 } ] - ( w ^ { * } ) ^ { 2 }$ are bounded if the variance of $x _ { k }$ is finite. Following Dvoretzky’s theorem, we conclude that $\Delta _ { k }$ converges to zero and hence $w _ { k }$ converges to $w ^ { * } = \mathbb { E } | X |$ almost surely.

# 6.3.3 Application to the Robbins-Monro theorem

We are now ready to prove the Robbins-Monro theorem using Dvoretzky’s theorem.

Proof of the Robbins-Monro theorem. The RM algorithm aims to find the root of $g ( w ) = 0$ . Suppose that the root is $w ^ { * }$ such that $g ( w ^ { * } ) = 0$ . The RM algorithm is

$$
\begin{array}{l} w _ {k + 1} = w _ {k} - a _ {k} \tilde {g} \left(w _ {k}, \eta_ {k}\right) \\ = w _ {k} - a _ {k} \left[ g \left(w _ {k}\right) + \eta_ {k} \right]. \\ \end{array}
$$

Then, we have

$$
w _ {k + 1} - w ^ {*} = w _ {k} - w ^ {*} - a _ {k} \left[ g \left(w _ {k}\right) - g \left(w ^ {*}\right) + \eta_ {k} \right].
$$

Due to the mean value theorem [7, 8], we have $g ( w _ { k } ) - g ( w ^ { * } ) = \nabla _ { w } g ( w _ { k } ^ { \prime } ) ( w _ { k } - w ^ { * } )$

where $w _ { k } ^ { \prime } \in [ w _ { k } , w ^ { * } ]$ . Let $\Delta _ { k } \doteq w _ { k } - w ^ { * }$ . The above equation becomes

$$
\begin{array}{l} \Delta_ {k + 1} = \Delta_ {k} - a _ {k} \left[ \nabla_ {w} g \left(w _ {k} ^ {\prime}\right) \left(w _ {k} - w ^ {*}\right) + \eta_ {k} \right] \\ = \Delta_ {k} - a _ {k} \nabla_ {w} g (w _ {k} ^ {\prime}) \Delta_ {k} + a _ {k} (- \eta_ {k}) \\ = [ 1 - \underbrace {a _ {k} \nabla_ {w} g (w _ {k} ^ {\prime})} _ {\alpha_ {k}} ] \Delta_ {k} + a _ {k} (- \eta_ {k}). \\ \end{array}
$$

Note that $\nabla _ { w } g ( w )$ is bounded as $0 < c _ { 1 } \leq \nabla _ { w } g ( w ) \leq c _ { 2 }$ as assumed. Since $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } =$ $\infty$ and $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } < \infty$ as assumed, we know $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } = \infty$ and $\textstyle \sum _ { k = 1 } ^ { \infty } \alpha _ { k } ^ { 2 } < \infty$ . Thus, all the conditions in Dvoretzky’s theorem are satisfied and hence $\Delta _ { k }$ converges to zero almost surely.

The proof of the RM theorem demonstrates the power of Dvoretzky’s theorem. In particular, $\alpha _ { k }$ in the proof is a stochastic sequence depending on $w _ { k }$ rather than a deterministic sequence. In this case, Dvoretzky’s theorem is still applicable.

# 6.3.4 An extension of Dvoretzky’s theorem

We next extend Dvoretzky’s theorem to a more general theorem that can handle multiple variables. This general theorem, proposed by [32], can be used to analyze the convergence of stochastic iterative algorithms such as Q-learning.

Theorem 6.3. Consider a finite set $\boldsymbol { S }$ of real numbers. For the stochastic process

$$
\Delta_ {k + 1} (s) = (1 - \alpha_ {k} (s)) \Delta_ {k} (s) + \beta_ {k} (s) \eta_ {k} (s),
$$

it holds that $\Delta _ { k } ( s )$ converges to zero almost surely for every $s \in S$ if the following conditions are satisfied for $s \in S$ :

(a) $\begin{array} { l l l } { \sum _ { k } \alpha _ { k } ( s ) } & { = } & { \infty } \end{array}$ , $\begin{array} { r } { \sum _ { k } \alpha _ { k } ^ { 2 } ( s ) ~ < ~ \infty } \end{array}$ , $\begin{array} { r } { \sum _ { k } \beta _ { k } ^ { 2 } ( s ) ~ < ~ \infty } \end{array}$ , and $\mathbb { E } [ \beta _ { k } ( s ) | \mathcal { H } _ { k } ] \le $ $\mathbb { E } [ \alpha _ { k } ( s ) | \mathcal { H } _ { k } ]$ uniformly almost surely;   
(b) $\| \mathbb { E } [ \eta _ { k } ( s ) \vert \mathcal { H } _ { k } ] \| _ { \infty } \leq \gamma \| \Delta _ { k } \| _ { \infty }$ , where $\gamma \in ( 0 , 1 )$ ;   
(c) $\mathrm { v a r } [ \eta _ { k } ( s ) | \mathcal { H } _ { k } ] \leq C ( 1 + \| \Delta _ { k } ( s ) \| _ { \infty } ) ^ { 2 }$ , where $C$ is a constant.

Here, $\mathcal { H } _ { k } = \{ \Delta _ { k } , \Delta _ { k - 1 } , \ldots , \eta _ { k - 1 } , \ldots , \alpha _ { k - 1 } , \ldots , \beta _ { k - 1 } , \ldots \}$ represents the historical information. The term $\| \cdot \| _ { \infty }$ refers to the maximum norm.

Proof. As an extension, this theorem can be proven based on Dvoretzky’s theorem. Details can be found in [32] and are omitted here.

Some remarks about this theorem are given below.

> We first clarify some notations in the theorem. The variable $s$ can be viewed as an index. In the context of reinforcement learning, it indicates a state or a state-action pair. The maximum norm $\| \cdot \| _ { \infty }$ is defined over a set. It is similar but different from the $L ^ { \infty }$ norm of vectors. In particular, $\| \mathbb { E } [ \eta _ { k } ( s ) \vert \mathcal { H } _ { k } ] \| _ { \infty } \doteq$ $\mathrm { m a x } _ { s \in \mathcal { S } } | \mathbb { E } [ \eta _ { k } ( s ) | \mathcal { H } _ { k } ] |$ and $\lVert \Delta _ { k } ( s ) \rVert _ { \infty } \doteq \operatorname* { m a x } _ { s \in \mathcal { S } } \left| \Delta _ { k } ( s ) \right|$ .   
This theorem is more general than Dvoretzky’s theorem. First, it can handle the case of multiple variables due to the maximum norm operations. This is important for a reinforcement learning problem where there are multiple states. Second, while Dvoretzky’s theorem requires $\mathbb { E } [ \eta _ { k } ( s ) | \mathcal { H } _ { k } ] = 0$ and $\mathrm { v a r } [ \eta _ { k } ( s ) | \mathcal { H } _ { k } ] \le C$ , this theorem only requires that the expectation and variance are bounded by the error $\Delta _ { k }$ .   
> It should be noted that the convergence of $\Delta ( s )$ for all $s \in S$ requires that the conditions are valid for every $s \in S$ . Therefore, when applying this theorem to prove the convergence of reinforcement learning algorithms, we need to show that the conditions are valid for every state (or state-action pair).

# 6.4 Stochastic gradient descent

This section introduces stochastic gradient descent (SGD) algorithms, which are widely used in the field of machine learning. We will see that SGD is a special RM algorithm, and the mean estimation algorithm is a special SGD algorithm.

Consider the following optimization problem:

$$
\min  _ {w} J (w) = \mathbb {E} [ f (w, X) ], \tag {6.10}
$$

where $w$ is the parameter to be optimized, and $X$ is a random variable. The expectation is calculated with respect to $X$ . Here, $w$ and $X$ can be either scalars or vectors. The function $f ( \cdot )$ is a scalar.

A straightforward method for solving (6.10) is gradient descent. In particular, the gradient of $\mathbb { E } [ f ( w , X ) ]$ is $\nabla _ { w } \mathbb { E } [ f ( w , X ) ] = \mathbb { E } [ \nabla _ { w } f ( w , X ) ]$ . Then, the gradient descent algorithm is

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} J (w _ {k}) = w _ {k} - \alpha_ {k} \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ]. \qquad \qquad (6. 1 1)
$$

This gradient descent algorithm can find the optimal solution $w ^ { * }$ under some mild conditions such as the convexity of $f$ . Preliminaries about gradient descent algorithms can be found in Appendix D.

The gradient descent algorithm requires the expected value $\mathbb { E } [ \nabla _ { w } f ( w _ { k } , X ) ]$ . One way to obtain the expected value is based on the probability distribution of $X$ . The

distribution is, however, often unknown in practice. Another way is to collect a large number of i.i.d. samples $\{ x _ { i } \} _ { i = 1 } ^ { n }$ of $X$ so that the expected value can be approximated as

$$
\mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ] \approx \frac {1}{n} \sum_ {i = 1} ^ {n} \nabla_ {w} f (w _ {k}, x _ {i}).
$$

Then, (6.11) becomes

$$
w _ {k + 1} = w _ {k} - \frac {\alpha_ {k}}{n} \sum_ {i = 1} ^ {n} \nabla_ {w} f \left(w _ {k}, x _ {i}\right). \tag {6.12}
$$

One problem of the algorithm in (6.12) is that it requires all the samples in each iteration. In practice, if the samples are collected one by one, then it is favorable to update $w$ every time a sample is collected. To that end, we can use the following algorithm:

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} f \left(w _ {k}, x _ {k}\right), \tag {6.13}
$$

where $x _ { k }$ is the sample collected at time step $k$ . This is the well-known stochastic gradient descent algorithm. This algorithm is called “stochastic” because it relies on stochastic samples $\{ x _ { k } \}$ .

Compared to the gradient descent algorithm in (6.11), SGD replaces the true gradient $\mathbb { E } [ \nabla _ { w } f ( w , X ) ]$ with the stochastic gradient $\nabla _ { w } f ( w _ { k } , x _ { k } )$ . Since $\nabla _ { w } f ( w _ { k } , x _ { k } ) \neq$ $\mathbb { E } [ \nabla _ { w } f ( w , X ) ]$ , can such a replacement still ensure $w _ { k } \to w ^ { * }$ as $k  \infty$ ? The answer is yes. We next present an intuitive explanation and postpone the rigorous proof of the convergence to Section 6.4.5.

In particular, since

$$
\begin{array}{l} \nabla_ {w} f (w _ {k}, x _ {k}) = \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ] + (\nabla_ {w} f (w _ {k}, x _ {k}) - \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ]) \\ \stackrel {\cdot} {=} \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ] + \eta_ {k}, \\ \end{array}
$$

the SGD algorithm in (6.13) can be rewritten as

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ] - \alpha_ {k} \eta_ {k}.
$$

Therefore, the SGD algorithm is the same as the regular gradient descent algorithm except that it has a perturbation term $\alpha _ { k } { \prime } / k$ . Since $\{ x _ { k } \}$ is i.i.d., we have $\mathbb { E } _ { x _ { k } } [ \nabla _ { w } f ( w _ { k } , x _ { k } ) ] =$ $\mathbb { E } _ { X } \lvert \nabla _ { w } f ( w _ { k } , X ) \rvert$ . As a result,

$$
\mathbb {E} [ \eta_ {k} ] = \mathbb {E} \Big [ \nabla_ {w} f (w _ {k}, x _ {k}) - \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ] \Big ] = \mathbb {E} _ {x _ {k}} [ \nabla_ {w} f (w _ {k}, x _ {k}) ] - \mathbb {E} _ {X} [ \nabla_ {w} f (w _ {k}, X) ] = 0.
$$

Therefore, the perturbation term $\eta _ { k }$ has a zero mean, which intuitively suggests that it may not jeopardize the convergence property. A rigorous proof of the convergence of

SGD is given in Section 6.4.5.

# 6.4.1 Application to mean estimation

We next apply SGD to analyze the mean estimation problem and show that the mean estimation algorithm in (6.4) is a special SGD algorithm. To that end, we formulate the mean estimation problem as an optimization problem:

$$
\min  _ {w} J (w) = \mathbb {E} \left[ \frac {1}{2} \| w - X \| ^ {2} \right] \doteq \mathbb {E} [ f (w, X) ], \tag {6.14}
$$

where $f ( w , X ) = \| w - X \| ^ { 2 } / 2$ and the gradient is $\nabla _ { w } f ( w , X ) = w - X$ . It can be verified that the optimal solution is $w ^ { * } = \mathbb { E } | X |$ by solving $\nabla _ { w } J ( w ) = 0$ . Therefore, this optimization problem is equivalent to the mean estimation problem.

> The gradient descent algorithm for solving (6.14) is

$$
\begin{array}{l} w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} J (w _ {k}) \\ = w _ {k} - \alpha_ {k} \mathbb {E} \left[ \nabla_ {w} f \left(w _ {k}, X\right) \right] \\ = w _ {k} - \alpha_ {k} \mathbb {E} [ w _ {k} - X ]. \\ \end{array}
$$

This gradient descent algorithm is not applicable since $\mathbb { E } | w _ { k } - X |$ or $\mathbb { E } { \lvert X \rvert }$ on the right-hand side is unknown (in fact, it is what we need to solve).

 The SGD algorithm for solving (6.14) is

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} f (w _ {k}, x _ {k}) = w _ {k} - \alpha_ {k} (w _ {k} - x _ {k}),
$$

where $x _ { k }$ is a sample obtained at time step $k$ . Notably, this SGD algorithm is the same as the iterative mean estimation algorithm in (6.4). Therefore, (6.4) is an SGD algorithm designed specifically for solving the mean estimation problem.

# 6.4.2 Convergence pattern of SGD

The idea of the SGD algorithm is to replace the true gradient with a stochastic gradient. However, since the stochastic gradient is random, one may ask whether the convergence speed of SGD is slow or random. Fortunately, SGD can converge efficiently in general. An interesting convergence pattern is that it behaves similarly to the regular gradient descent algorithm when the estimate $w _ { k }$ is far from the optimal solution $w ^ { * }$ . Only when $w _ { k }$ is close to $w ^ { * }$ , does the convergence of SGD exhibit more randomness.

An analysis of this pattern and an illustrative example are given below.

Analysis: The relative error between the stochastic and true gradients is

$$
\delta_ {k} \doteq \frac {| \nabla_ {w} f (w _ {k} , x _ {k}) - \mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ] |}{| \mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ] |}.
$$

For the sake of simplicity, we consider the case where $w$ and $\nabla _ { w } f ( w , x )$ are both scalars. Since $w ^ { * }$ is the optimal solution, it holds that $\mathbb { E } [ \nabla _ { w } f ( w ^ { * } , X ) ] = 0$ . Then, the relative error can be rewritten as

$$
\delta_ {k} = \frac {| \nabla_ {w} f (w _ {k} , x _ {k}) - \mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ] |}{| \mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ] - \mathbb {E} [ \nabla_ {w} f (w ^ {*} , X) ] |} = \frac {| \nabla_ {w} f (w _ {k} , x _ {k}) - \mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ] |}{| \mathbb {E} [ \nabla_ {w} ^ {2} f (\tilde {w} _ {k} , X) (w _ {k} - w ^ {*}) ] |}, (6. 1 5)
$$

where the last equality is due to the mean value theorem [7, 8] and $\tilde { w } _ { k } \in [ w _ { k } , w ^ { * } ]$ . Suppose that $f$ is strictly convex such that $\nabla _ { w } ^ { 2 } f \ge c > 0$ for all $w , X$ . Then, the denominator in (6.15) becomes

$$
\begin{array}{l} \left| \mathbb {E} [ \nabla_ {w} ^ {2} f (\tilde {w} _ {k}, X) (w _ {k} - w ^ {*}) ] \right| = \left| \mathbb {E} [ \nabla_ {w} ^ {2} f (\tilde {w} _ {k}, X) ] \right| | (w _ {k} - w ^ {*}) | \\ \geq c | w _ {k} - w ^ {*} |. \\ \end{array}
$$

Substituting the above inequality into (6.15) yields

$$
\delta_ {k} \leq \frac {\left| \overbrace {\nabla_ {w} f (w _ {k} , x _ {k})} ^ {\text {s t o c h a s t i c g r a d i e n t}} - \overbrace {\mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ]} ^ {\text {t r u e g r a d i e n t}} \right|}{\underbrace {c \left| w _ {k} - w ^ {*} \right|} ^ {\left. \right.}}.
$$

| {z }distance to the optimal solution

The above inequality suggests an interesting convergence pattern of SGD: the relative error $\delta _ { k }$ is inversely proportional to $| w _ { k } - w ^ { * } |$ . As a result, when $| w _ { k } - w ^ { * } |$ is large, $\delta _ { k }$ is small. In this case, the SGD algorithm behaves like the gradient descent algorithm and hence $w _ { k }$ quickly converges to $w ^ { * }$ . When $w _ { k }$ is close to $w ^ { * }$ , the relative error $\delta _ { k }$ may be large, and the convergence exhibits more randomness.

Example: A good example for demonstrating the above analysis is the mean estimation problem. Consider the mean estimation problem in (6.14). When $w$ and $X$ are both scalar, we have $f ( w , X ) = | w - X | ^ { 2 } / 2$ and hence

$$
\nabla_ {w} f (w, x _ {k}) = w - x _ {k},
$$

$$
\mathbb {E} \left[ \nabla_ {w} f (w, x _ {k}) \right] = w - \mathbb {E} [ X ] = w - w ^ {*}.
$$

Thus, the relative error is

$$
\delta_ {k} = \frac {| \nabla_ {w} f (w _ {k} , x _ {k}) - \mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ] |}{| \mathbb {E} [ \nabla_ {w} f (w _ {k} , X) ] |} = \frac {| (w _ {k} - x _ {k}) - (w _ {k} - \mathbb {E} [ X ]) |}{| w _ {k} - w ^ {*} |} = \frac {| \mathbb {E} [ X ] - x _ {k} |}{| w _ {k} - w ^ {*} |}.
$$

The expression of the relative error clearly shows that $\delta _ { k }$ is inversely proportional to

![](images/e2cb7443782d33ec64d0bebb411968730b56189e668b25f425205b32238b3ee2.jpg)

![](images/2c9c911c417088b8c93670598ac941dd2d0775bff3f2bbfbcea5af09a0cdb8a1.jpg)  
Figure 6.5: An example for demonstrating stochastic and mini-batch gradient descent algorithms. The distribution of $X \in \mathbb { R } ^ { 2 }$ is uniform in the square area centered at the origin with a side length as 20. The mean is $\mathbb { E } [ X ] = 0$ . The mean estimation is based on 100 i.i.d. samples.

$| w _ { k } - w ^ { * } |$ . As a result, when $w _ { k }$ is far from $w ^ { * }$ , the relative error is small, and SGD behaves like gradient descent. In addition, since $\delta _ { k }$ is proportional to $| \mathbb { E } | X | - x _ { k } |$ , the mean of $\delta _ { k }$ is proportional to the variance of $X$ .

The simulation results are shown in Figure 6.5. Here, $X \in \mathbb { R } ^ { 2 }$ represents a random position in the plane. Its distribution is uniform in the square area centered at the origin and $\mathbb { E } | X | = 0$ . The mean estimation is based on 100 i.i.d. samples. Although the initial guess of the mean is far away from the true value, it can be seen that the SGD estimate quickly approaches the neighborhood of the origin. When the estimate is close to the origin, the convergence process exhibits certain randomness.

# 6.4.3 A deterministic formulation of SGD

The formulation of SGD in (6.13) involves random variables. One may often encounter a deterministic formulation of SGD without involving any random variables.

In particular, consider a set of real numbers $\{ x _ { i } \} _ { i = 1 } ^ { n }$ , where $x _ { i }$ does not have to be a sample of any random variable. The optimization problem to be solved is to minimize the average:

$$
\min _ {w} J (w) = \frac {1}{n} \sum_ {i = 1} ^ {n} f (w, x _ {i}),
$$

where $f ( w , x _ { i } )$ is a parameterized function, and $w$ is the parameter to be optimized. The gradient descent algorithm for solving this problem is

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} J (w _ {k}) = w _ {k} - \alpha_ {k} \frac {1}{n} \sum_ {i = 1} ^ {n} \nabla_ {w} f (w _ {k}, x _ {i}).
$$

Suppose that the set $\{ x _ { i } \} _ { i = 1 } ^ { n }$ is large and we can only fetch a single number each time.

In this case, it is favorable to update $w _ { k }$ in an incremental manner:

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} f (w _ {k}, x _ {k}). \tag {6.16}
$$

It must be noted that $x _ { k }$ here is the number fetched at time step $k$ instead of the $k$ th element in the set $\{ x _ { i } \} _ { i = 1 } ^ { n }$ .

The algorithm in (6.16) is very similar to SGD, but its problem formulation is subtly different because it does not involve any random variables or expected values. Then, many questions arise. For example, is this algorithm SGD? How should we use the finite set of numbers $\{ x _ { i } \} _ { i = 1 } ^ { n }$ ? Should we sort these numbers in a certain order and then use them one by one, or should we randomly sample a number from the set?

A quick answer to the above questions is that, although no random variables are involved in the above formulation, we can convert the deterministic formulation to the stochastic formulation by introducing a random variable. In particular, let $X$ be a random variable defined on the set $\{ x _ { i } \} _ { i = 1 } ^ { n }$ . Suppose that its probability distribution is uniform such that $p ( X = x _ { i } ) = 1 / n$ . Then, the deterministic optimization problem becomes a stochastic one:

$$
\min _ {w} J (w) = \frac {1}{n} \sum_ {i = 1} ^ {n} f (w, x _ {i}) = \mathbb {E} [ f (w, X) ].
$$

The last equality in the above equation is strict instead of approximate. Therefore, the algorithm in (6.16) is SGD, and the estimate converges if $x _ { k }$ is uniformly and independently sampled from $\{ x _ { i } \} _ { i = 1 } ^ { n }$ . Note that $x _ { k }$ may repeatedly take the same number in $\{ x _ { i } \} _ { i = 1 } ^ { n }$ since it is sampled randomly.

# 6.4.4 BGD, SGD, and mini-batch GD

While SGD uses a single sample in every iteration, we next introduce mini-batch gradient descent (MBGD), which uses a few more samples in every iteration. When all samples are used in every iteration, the algorithm is called batch gradient descent (BGD).

In particular, suppose that we would like to find the optimal solution that can minimize $J ( w ) = \mathbb { E } [ f ( w , X ) ]$ given a set of random samples $\{ x _ { i } \} _ { i = 1 } ^ { n }$ of $X$ . The BGD, SGD, and MBGD algorithms for solving this problem are, respectively,

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \frac {1}{n} \sum_ {i = 1} ^ {n} \nabla_ {w} f (w _ {k}, x _ {i}), \quad (\text {B G D})
$$

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \frac {1}{m} \sum_ {j \in \mathcal {I} _ {k}} \nabla_ {w} f (w _ {k}, x _ {j}), \quad (\text {M B G D})
$$

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} f (w _ {k}, x _ {k}). \quad \mathrm {(S G D)}
$$

In the BGD algorithm, all the samples are used in every iteration. When $n$ is large, $\textstyle ( 1 / n ) \sum _ { i = 1 } ^ { n } \nabla _ { w } f ( w _ { k } , x _ { i } )$ is close to the true gradient $\mathbb { E } [ \nabla _ { w } f ( w _ { k } , X ) ]$ . In the MBGD al-

gorithm, $\mathcal { T } _ { k }$ is a subset of $\{ 1 , \ldots , n \}$ obtained at time $k$ . The size of the set is $| \mathcal { T } _ { k } | = m$ . The samples in $\mathcal { T } _ { k }$ are also assumed to be i.i.d. In the SGD algorithm, $x _ { k }$ is randomly sampled from $\{ x _ { i } \} _ { i = 1 } ^ { n }$ at time $k$ .

MBGD can be viewed as an intermediate version between SGD and BGD. Compared to SGD, MBGD has less randomness because it uses more samples instead of just one as in SGD. Compared to BGD, MBGD does not require using all the samples in every iteration, making it more flexible. If $m = 1$ , then MBGD becomes SGD. However, if $m = n$ , MBGD may not become BGD. This is because MBGD uses $n$ randomly fetched samples, whereas BGD uses all $n$ numbers. These $n$ randomly fetched samples may contain the same number multiple times and hence may not cover all $n$ numbers in $\{ x _ { i } \} _ { i = 1 } ^ { n }$ .

The convergence speed of MBGD is faster than that of SGD in general. This is because SGD uses $\nabla _ { w } f ( w _ { k } , x _ { k } )$ to approximate the true gradient, whereas MBGD uses $\begin{array} { r } { ( 1 / m ) \sum _ { j \in \mathbb { Z } _ { k } } \nabla _ { w } f ( w _ { k } , x _ { j } ) } \end{array}$ , which is closer to the true gradient because the randomness is averaged out. The convergence of the MBGD algorithm can be proven similarly to the SGD case.

A good example for demonstrating the above analysis is the mean estimation problem. In particular, given some numbers $\{ x _ { i } \} _ { i = 1 } ^ { n }$ , our goal is to calculate the mean $\textstyle { \bar { x } } = \sum _ { i = 1 } ^ { n } x _ { i } / n$ . This problem can be equivalently stated as the following optimization problem:

$$
\min _ {w} J (w) = \frac {1}{2 n} \sum_ {i = 1} ^ {n} \| w - x _ {i} \| ^ {2},
$$

whose optimal solution is $w ^ { * } = \bar { x }$ . The three algorithms for solving this problem are, respectively,

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \frac {1}{n} \sum_ {i = 1} ^ {n} (w _ {k} - x _ {i}) = w _ {k} - \alpha_ {k} (w _ {k} - \bar {x}), \qquad (\mathrm {B G D})
$$

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \frac {1}{m} \sum_ {j \in \mathcal {I} _ {k}} (w _ {k} - x _ {j}) = w _ {k} - \alpha_ {k} \left(w _ {k} - \bar {x} _ {k} ^ {(m)}\right), \qquad (\mathrm {M B G D})
$$

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} (w _ {k} - x _ {k}), \qquad (\mathrm {S G D})
$$

where $\begin{array} { r } { \bar { x } _ { k } ^ { ( m ) } = \sum _ { j \in \mathcal { T } _ { k } } { x _ { j } / m } } \end{array}$ . Furthermore, if $\alpha _ { k } = 1 / k$ , the above equations can be solved

as follows:

$$
w _ {k + 1} = \frac {1}{k} \sum_ {j = 1} ^ {k} \bar {x} = \bar {x}, \qquad (\mathrm {B G D})
$$

$$
w _ {k + 1} = \frac {1}{k} \sum_ {j = 1} ^ {k} \bar {x} _ {j} ^ {(m)}, \quad (\mathrm {M B G D})
$$

$$
w _ {k + 1} = \frac {1}{k} \sum_ {j = 1} ^ {k} x _ {j}. \qquad (\mathrm {S G D})
$$

The derivation of the above equations is similar to that of (6.3) and is omitted here. It can be seen that the estimate given by BGD at each step is exactly the optimal solution $w ^ { * } = \bar { x }$ . MBGD converges to the mean faster than SGD because $\bar { x } _ { k } ^ { ( m ) }$ is already an average.

A simulation example is given in Figure 6.5 to demonstrate the convergence of MBGD. Let $\alpha _ { k } = 1 / k$ . It is shown that all MBGD algorithms with different mini-batch sizes can converge to the mean. The case with $m = 5 0$ converges the fastest, while SGD with $m = 1$ is the slowest. This is consistent with the above analysis. Nevertheless, the convergence rate of SGD is still fast, especially when $w _ { k }$ is far from $w ^ { * }$ .

# 6.4.5 Convergence of SGD

The rigorous proof of the convergence of SGD is given as follows.

Theorem 6.4 (Convergence of SGD). For the SGD algorithm in (6.13), if the following conditions are satisfied, then $w _ { k }$ converges to the root of $\nabla _ { \boldsymbol { w } } \mathbb { E } [ f ( \boldsymbol { w } , \boldsymbol { X } ) ] = 0$ almost surely.

(a) $0 < c _ { 1 } \leq \nabla _ { w } ^ { 2 } f ( w , X ) \leq c _ { 2 }$ ;   
$\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } = \infty$ $\textstyle \sum _ { k = 1 } ^ { \infty } a _ { k } ^ { 2 } < \infty$   
(c) $\{ x _ { k } \} _ { k = 1 } ^ { \infty }$ are i.i.d.

The three conditions in Theorem 6.4 are discussed below.

Condition (a) is about the convexity of $f$ . It requires the curvature of $f$ to be bounded from above and below. Here, $w$ is a scalar, and so is $\nabla _ { w } ^ { 2 } f ( w , X )$ . This condition can be generalized to the vector case. When $w$ is a vector, $\nabla _ { w } ^ { 2 } f ( w , X )$ is the well-known Hessian matrix.   
Condition (b) is similar to that of the RM algorithm. In fact, the SGD algorithm is a special RM algorithm (as shown in the proof in Box 6.1). In practice, $a _ { k }$ is often selected as a sufficiently small constant. Although condition (b) is not satisfied in this case, the algorithm can still converge in a certain sense [24, Section 1.5].   
 Condition (c) is a common requirement.

# Box 6.1: Proof of Theorem 6.4

We next show that the SGD algorithm is a special RM algorithm. Then, the convergence of SGD naturally follows from the RM theorem.

The problem to be solved by SGD is to minimize $J ( w ) = \mathbb { E } [ f ( w , X ) ]$ . This problem can be converted to a root-finding problem. That is, finding the root of $\nabla _ { w } J ( w ) = \operatorname { \mathbb { E } } [ \nabla _ { w } f ( w , X ) ] = 0$ . Let

$$
g (w) = \nabla_ {w} J (w) = \mathbb {E} [ \nabla_ {w} f (w, X) ].
$$

Then, SGD aims to find the root of $g ( w ) = 0$ . This is exactly the problem solved by the RM algorithm. The quantity that we can measure is $\tilde { g } = \nabla _ { w } f ( w , x )$ , where $x$ i s a sample of $X$ . Note that $\tilde { g }$ can be rewritten as

$$
\begin{array}{l} \tilde {g} (w, \eta) = \nabla_ {w} f (w, x) \\ = \mathbb {E} [ \nabla_ {w} f (w, X) ] + \underbrace {\nabla_ {w} f (w , x) - \mathbb {E} [ \nabla_ {w} f (w , X) ]} _ {\eta}. \\ \end{array}
$$

Then, the RM algorithm for solving $g ( w ) = 0$ is

$$
w _ {k + 1} = w _ {k} - a _ {k} \tilde {g} (w _ {k}, \eta_ {k}) = w _ {k} - a _ {k} \nabla_ {w} f (w _ {k}, x _ {k}),
$$

which is the same as the SGD algorithm in (6.13). As a result, the SGD algorithm is a special RM algorithm. We next show that the three conditions in Theorem 6.1 are satisfied. Then, the convergence of SGD naturally follows from Theorem 6.1.

Since $\nabla _ { w } g ( w ) ~ = ~ \nabla _ { w } \mathbb { E } [ \nabla _ { w } f ( w , X ) ] ~ = ~ \mathbb { E } [ \nabla _ { w } ^ { 2 } f ( w , X ) ]$ , it follows from $c _ { 1 } \leq$ $\nabla _ { w } ^ { 2 } f ( w , X ) \leq c _ { 2 }$ that $c _ { 1 } \leq \nabla _ { w } g ( w ) \leq c _ { 2 }$ . Thus, the first condition in Theorem 6.1 is satisfied.   
The second condition in Theorem 6.1 is the same as the second condition in this theorem.   
The third condition in Theorem 6.1 requires $\mathbb { E } [ \eta _ { k } | \mathcal { H } _ { k } ] = 0$ and $\mathbb { E } [ \eta _ { k } ^ { 2 } \vert \mathcal { H } _ { k } ] < \infty$ . Since $\{ x _ { k } \}$ is i.i.d., we have $\mathbb { E } _ { x _ { k } } [ \nabla _ { w } f ( w , x _ { k } ) ] = \mathbb { E } [ \nabla _ { w } f ( w , X ) ]$ for all $k$ . Therefore,

$$
\mathbb {E} [ \eta_ {k} | \mathcal {H} _ {k} ] = \mathbb {E} [ \nabla_ {w} f (w _ {k}, x _ {k}) - \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ] | \mathcal {H} _ {k} ].
$$

Since $\mathcal { H } _ { k } = \{ w _ { k } , w _ { k - 1 } , . . . \}$ and $x _ { k }$ is independent of $\mathcal { H } _ { k }$ , the first term on the right-hand side becomes $\mathbb { E } [ \nabla _ { w } f ( w _ { k } , x _ { k } ) | \mathcal { H } _ { k } ] \ = \ \mathbb { E } _ { x _ { k } } [ \nabla _ { w } f ( w _ { k } , x _ { k } ) ]$ . The second term becomes $\mathbb { E } [ \mathbb { E } [ \nabla _ { w } f ( w _ { k } , X ) ] | \mathcal { H } _ { k } ] = \mathbb { E } [ \nabla _ { w } f ( w _ { k } , X ) ]$ because $\mathbb { E } [ \nabla _ { w } f ( w _ { k } , X ) ]$ is

a function of $w _ { k }$ . Therefore,

$$
\mathbb {E} [ \eta_ {k} | \mathcal {H} _ {k} ] = \mathbb {E} _ {x _ {k}} [ \nabla_ {w} f (w _ {k}, x _ {k}) ] - \mathbb {E} [ \nabla_ {w} f (w _ {k}, X) ] = 0.
$$

Similarly, it can be proven that $\mathbb { E } [ \eta _ { k } ^ { 2 } \vert \mathcal { H } _ { k } ] < \infty$ if $| \nabla _ { w } f ( w , x ) | < \infty$ for all $w$ given any $x$ .

Since the three conditions in Theorem 6.1 are satisfied, the convergence of the SGD algorithm follows.

# 6.5 Summary

Instead of introducing new reinforcement learning algorithms, this chapter introduced the preliminaries of stochastic approximation such as the RM and SGD algorithms. Compared to many other root-finding algorithms, the RM algorithm does not require the expression of the objective function or its derivative. It has been shown that the SGD algorithm is a special RM algorithm. Moreover, an important problem frequently discussed throughout this chapter is mean estimation. The mean estimation algorithm (6.4) is the first stochastic iterative algorithm we have ever introduced in this book. We showed that it is a special SGD algorithm. We will see in Chapter 7 that temporal-difference learning algorithms have similar expressions. Finally, the name “stochastic approximation” was first used by Robbins and Monro in 1951 [25]. More information about stochastic approximation can be found in [24].

# 6.6 Q&A

 Q: What is stochastic approximation?

A: Stochastic approximation refers to a broad class of stochastic iterative algorithms for solving root-finding or optimization problems.

 Q: Why do we need to study stochastic approximation?

A: This is because the temporal-difference reinforcement learning algorithms that will be introduced in Chapter 7 can be viewed as stochastic approximation algorithms. With the knowledge introduced in this chapter, we can be better prepared, and it will not be abrupt for us to see these algorithms for the first time.

 Q: Why do we frequently discuss the mean estimation problem in this chapter?

A: This is because the state and action values are defined as the means of random variables. The temporal-difference learning algorithms introduced in Chapter 7 are similar to stochastic approximation algorithms for mean estimation.

Q: What is the advantage of the RM algorithm over other root-finding algorithms?

A: Compared to many other root-finding algorithms, the RM algorithm is powerful in the sense that it does not require the expression of the objective function or its derivative. As a result, it is a black-box technique that only requires the input and output of the objective function. The famous SGD algorithm is a special form of the RM algorithm.

Q: What is the basic idea of SGD?

A: SGD aims to solve optimization problems involving random variables. When the probability distributions of the given random variables are not known, SGD can solve the optimization problems merely by using samples. Mathematically, the SGD algorithm can be obtained by replacing the true gradient expressed as an expectation in the gradient descent algorithm with a stochastic gradient.

 Q: Can SGD converge quickly?

A: SGD has an interesting convergence pattern. That is, if the estimate is far from the optimal solution, then the convergence process is fast. When the estimate is close to the solution, the randomness of the stochastic gradient becomes influential, and the convergence rate decreases.

Q: What is MBGD? What are its advantages over SGD and BGD?

A: MBGD can be viewed as an intermediate version between SGD and BGD. Compared to SGD, it has less randomness because it uses more samples instead of just one as in SGD. Compared to BGD, it does not require the use of all the samples, making it more flexible.

# Chapter 7

# Temporal-Difference Methods

![](images/22317d18b60e61190ad0eeabd9abc7da1dfc8d380b6a6212dea7f0001254f8b7.jpg)  
Figure 7.1: Where we are in this book.

This chapter introduces temporal-difference (TD) methods for reinforcement learning. Similar to Monte Carlo (MC) learning, TD learning is also model-free, but it has some advantages due to its incremental form. With the preparation in Chapter 6, readers will not feel alarmed when seeing TD learning algorithms. In fact, TD learning algorithms can be viewed as special stochastic algorithms for solving the Bellman or Bellman optimality equations.

Since this chapter introduces quite a few TD algorithms, we first overview these algorithms and clarify the relationships between them.

Section 7.1 introduces the most basic TD algorithm, which can estimate the state

values of a given policy. It is important to understand this basic algorithm first before studying the other TD algorithms.

Section 7.2 introduces the Sarsa algorithm, which can estimate the action values of a given policy. This algorithm can be combined with a policy improvement step to find optimal policies. The Sarsa algorithm can be easily obtained from the TD algorithm in Section 7.1 by replacing state value estimation with action value estimation.   
 Section 7.3 introduces the $n$ -step Sarsa algorithm, which is a generalization of the Sarsa algorithm. It will be shown that Sarsa and MC learning are two special cases of $n$ -step Sarsa.   
Section 7.4 introduces the Q-learning algorithm, which is one of the most classic reinforcement learning algorithms. While the other TD algorithms aim to solve the Bellman equation of a given policy, Q-learning aims to directly solve the Bellman optimality equation to obtain optimal policies.   
 Section 7.5 compares the TD algorithms introduced in this chapter and provides a unified point of view.

# 7.1 TD learning of state values

TD learning often refers to a broad class of reinforcement learning algorithms. For example, all the algorithms introduced in this chapter fall into the scope of TD learning. However, TD learning in this section specifically refers to a classic algorithm for estimating state values.

# 7.1.1 Algorithm description

Given a policy $\pi$ , our goal is to estimate $v _ { \pi } ( s )$ for all $s \in S$ . Suppose that we have some experience samples $( s _ { 0 } , r _ { 1 } , s _ { 1 } , \ldots , s _ { t } , r _ { t + 1 } , s _ { t + 1 } , \ldots )$ generated following $\pi$ . Here, $t$ denotes the time step. The following TD algorithm can estimate the state values using these samples:

$$
v _ {t + 1} (s _ {t}) = v _ {t} (s _ {t}) - \alpha_ {t} (s _ {t}) \left[ v _ {t} (s _ {t}) - \left(r _ {t + 1} + \gamma v _ {t} (s _ {t + 1})\right) \right], \tag {7.1}
$$

$$
v _ {t + 1} (s) = v _ {t} (s), \quad \text {f o r a l l} s \neq s _ {t}, \tag {7.2}
$$

where $t = 0 , 1 , 2 , \ldots .$ Here, $v _ { t } ( s _ { t } )$ is the estimate of $v _ { \pi } ( s _ { t } )$ at time $t$ ; $\alpha _ { t } ( s _ { t } )$ is the learning rate for $s _ { t }$ at time $t$ .

It should be noted that, at time $t$ , only the value of the visited state $s _ { t }$ is updated. The values of the unvisited states $s \neq s _ { t }$ remain unchanged as shown in (7.2). Equation (7.2) is often omitted for simplicity, but it should be kept in mind because the algorithm would be mathematically incomplete without this equation.

Readers who see the TD learning algorithm for the first time may wonder why it is designed like this. In fact, it can be viewed as a special stochastic approximation algorithm for solving the Bellman equation. To see that, first recall that the definition of the state value is

$$
v _ {\pi} (s) = \mathbb {E} \left[ R _ {t + 1} + \gamma G _ {t + 1} \mid S _ {t} = s \right], \quad s \in \mathcal {S}. \tag {7.3}
$$

We can rewrite (7.3) as

$$
v _ {\pi} (s) = \mathbb {E} \left[ R _ {t + 1} + \gamma v _ {\pi} \left(S _ {t + 1}\right) \mid S _ {t} = s \right], \quad s \in \mathcal {S}. \tag {7.4}
$$

That is because $\begin{array} { r } { \mathbb { E } [ G _ { t + 1 } | S _ { t } = s ] = \sum _ { a } \pi ( a | s ) \sum _ { s ^ { \prime } } p ( s ^ { \prime } | s , a ) v _ { \pi } ( s ^ { \prime } ) = \mathbb { E } [ v _ { \pi } ( S _ { t + 1 } ) | S _ { t } = s ] } \end{array}$ . Equation (7.4) is another expression of the Bellman equation. It is sometimes called the Bellman expectation equation.

The TD algorithm can be derived by applying the Robbins-Monro algorithm (Chapter 6) to solve the Bellman equation in (7.4). Interested readers can check the details in Box 7.1.

# Box 7.1: Derivation of the TD algorithm

We next show that the TD algorithm in (7.1) can be obtained by applying the Robbins-Monro algorithm to solve (7.4).

For state $s _ { t }$ , we define a function as

$$
g (v _ {\pi} (s _ {t})) \doteq v _ {\pi} (s _ {t}) - \mathbb {E} \big [ R _ {t + 1} + \gamma v _ {\pi} (S _ {t + 1}) | S _ {t} = s _ {t} \big ].
$$

Then, (7.4) is equivalent to

$$
g (v _ {\pi} (s _ {t})) = 0.
$$

Our goal is to solve the above equation to obtain $v _ { \pi } ( s _ { t } )$ using the Robbins-Monro algorithm. Since we can obtain $r _ { t + 1 }$ and $s _ { t + 1 }$ , which are the samples of $R _ { t + 1 }$ and $S _ { t + 1 }$ , the noisy observation of $g ( v _ { \pi } ( s _ { t } ) )$ that we can obtain is

$$
\begin{array}{l} \tilde {g} \left(v _ {\pi} \left(s _ {t}\right)\right) = v _ {\pi} \left(s _ {t}\right) - \left[ r _ {t + 1} + \gamma v _ {\pi} \left(s _ {t + 1}\right) \right] \\ = \underbrace {\left(v _ {\pi} (s _ {t}) - \mathbb {E} \left[ R _ {t + 1} + \gamma v _ {\pi} (S _ {t + 1}) | S _ {t} = s _ {t} \right]\right)} _ {g (v _ {\pi} (s _ {t}))} \\ + \underbrace {\left(\mathbb {E} \left[ R _ {t + 1} + \gamma v _ {\pi} (S _ {t + 1}) | S _ {t} = s _ {t} \right] - \left[ r _ {t + 1} + \gamma v _ {\pi} (s _ {t + 1}) \right]\right)} _ {\eta}. \\ \end{array}
$$

Therefore, the Robbins-Monro algorithm (Section 6.2) for solving $g ( v _ { \pi } ( s _ { t } ) ) = 0$ is

$$
\begin{array}{l} v _ {t + 1} (s _ {t}) = v _ {t} (s _ {t}) - \alpha_ {t} (s _ {t}) \tilde {g} (v _ {t} (s _ {t})) \\ = v _ {t} \left(s _ {t}\right) - \alpha_ {t} \left(s _ {t}\right) \left(v _ {t} \left(s _ {t}\right) - \left[ r _ {t + 1} + \gamma v _ {\pi} \left(s _ {t + 1}\right) \right]\right), \tag {7.5} \\ \end{array}
$$

where $v _ { t } ( s _ { t } )$ is the estimate of $v _ { \pi } ( s _ { t } )$ at time $t$ , and $\alpha _ { t } ( s _ { t } )$ is the learning rate.

The algorithm in (7.5) has a similar expression to that of the TD algorithm in (7.1). The only difference is that the right-hand side of (7.5) contains $v _ { \pi } ( s _ { t + 1 } )$ , whereas (7.1) contains $v _ { t } ( s _ { t + 1 } )$ . That is because (7.5) is designed to merely estimate the state value of $s _ { t }$ by assuming that the state values of the other states are already known. If we would like to estimate the state values of all the states, then $v _ { \pi } ( s _ { t + 1 } )$ on the right-hand side should be replaced with $v _ { t } ( s _ { t + 1 } )$ . Then, (7.5) is exactly the same as (7.1). However, can such a replacement still ensure convergence? The answer is yes, and it will be proven later in Theorem 7.1.

# 7.1.2 Property analysis

Some important properties of the TD algorithm are discussed as follows.

First, we examine the expression of the TD algorithm more closely. In particular, (7.1) can be described as

$$
\underbrace {v _ {t + 1} \left(s _ {t}\right)} _ {\text {n e w e s t i m a t e}} = \underbrace {v _ {t} \left(s _ {t}\right)} _ {\text {c u r r e n t e s t i m a t e}} - \alpha_ {t} \left(s _ {t}\right) \left[ \overbrace {v _ {t} \left(s _ {t}\right) - \left(\underbrace {r _ {t + 1} + \gamma v _ {t} \left(s _ {t + 1}\right)} _ {\text {T D t a r g e t} \bar {v} _ {t}}\right)} ^ {\text {T D e r r o r} \delta_ {t}} \right], \tag {7.6}
$$

where

$$
\bar {v} _ {t} \doteq r _ {t + 1} + \gamma v _ {t} (s _ {t + 1})
$$

is called the TD target and

$$
\delta_ {t} \doteq v (s _ {t}) - \bar {v} _ {t} = v _ {t} (s _ {t}) - \left(r _ {t + 1} + \gamma v _ {t} (s _ {t + 1})\right)
$$

is called the TD error. It can be seen that the new estimate $v _ { t + 1 } ( s _ { t } )$ is a combination of the current estimate $v _ { t } ( s _ { t } )$ and the TD error $\delta _ { t }$ .

$\diamond$ Why is $v _ { t }$ called the TD target?

This is because $\bar { v } _ { t }$ is the target value that the algorithm attempts to drive $v ( s _ { t } )$ to. To see that, subtracting $\bar { v } _ { t }$ from both sides of (7.6) gives

$$
\begin{array}{l} v _ {t + 1} (s _ {t}) - \bar {v} _ {t} = [ v _ {t} (s _ {t}) - \bar {v} _ {t} ] - \alpha_ {t} (s _ {t}) [ v _ {t} (s _ {t}) - \bar {v} _ {t} ] \\ = \left[ 1 - \alpha_ {t} \left(s _ {t}\right) \right] \left[ v _ {t} \left(s _ {t}\right) - \bar {v} _ {t} \right]. \\ \end{array}
$$

Taking the absolute values of both sides of the above equation gives

$$
| v _ {t + 1} (s _ {t}) - \bar {v} _ {t} | = | 1 - \alpha_ {t} (s _ {t}) | | v _ {t} (s _ {t}) - \bar {v} _ {t} |.
$$

Since $\alpha _ { t } ( s _ { t } )$ is a small positive number, we have $0 < 1 - \alpha _ { t } ( s _ { t } ) < 1$ . It then follows that

$$
\left| v _ {t + 1} (s _ {t}) - \bar {v} _ {t} \right| <   \left| v _ {t} (s _ {t}) - \bar {v} _ {t} \right|.
$$

The above inequality is important because it indicates that the new value $\boldsymbol { v } _ { t + 1 } ( s _ { t } )$ is closer to $v _ { t }$ than the old value $v _ { t } ( s _ { t } )$ . Therefore, this algorithm mathematically drives $v _ { t } ( s _ { t } )$ toward $v _ { t }$ . This is why $v _ { t }$ is called the TD target.

What is the interpretation of the TD error?

First, this error is called temporal-difference because $\delta _ { t } = v _ { t } ( s _ { t } ) - ( r _ { t + 1 } + \gamma v _ { t } ( s _ { t + 1 } ) )$ reflects the discrepancy between two time steps $t$ and $t + 1$ . Second, the TD error is zero in the expectation sense when the state value estimate is accurate. To see that, when ${ \boldsymbol { v } } _ { t } = { \boldsymbol { v } } _ { \pi }$ , the expected value of the TD error is

$$
\begin{array}{l} \mathbb {E} [ \delta_ {t} | S _ {t} = s _ {t} ] = \mathbb {E} \big [ v _ {\pi} (S _ {t}) - (R _ {t + 1} + \gamma v _ {\pi} (S _ {t + 1})) | S _ {t} = s _ {t} \big ] \\ = v _ {\pi} (s _ {t}) - \mathbb {E} \left[ R _ {t + 1} + \gamma v _ {\pi} (S _ {t + 1}) | S _ {t} = s _ {t} \right] \\ = 0. \quad (\text {d u e t o} (7. 3)) \\ \end{array}
$$

Therefore, the TD error reflects not only the discrepancy between two time steps but also, more importantly, the discrepancy between the estimate $v _ { t }$ and the true state value $v _ { \pi }$ .

On a more abstract level, the TD error can be interpreted as the innovation, which indicates new information obtained from the experience sample $( s _ { t } , r _ { t + 1 } , s _ { t + 1 } )$ . The fundamental idea of TD learning is to correct our current estimate of the state value based on the newly obtained information. Innovation is fundamental in many estimation problems such as Kalman filtering [33, 34].

Second, the TD algorithm in (7.1) can only estimate the state values of a given policy. To find optimal policies, we still need to further calculate the action values and then conduct policy improvement. This will be introduced in Section 7.2. Nevertheless, the TD algorithm introduced in this section is very basic and important for understanding the other algorithms in this chapter.

Third, while both TD learning and MC learning are model-free, what are their advantages and disadvantages? The answers are summarized in Table 7.1.

Table 7.1: A comparison between TD learning and MC learning.   

<table><tr><td>TD learning</td><td>MC learning</td></tr><tr><td>Incremental: TD learning is incremental. It can update the state/action values immediately after receiving an experience sample.</td><td>Non-incremental: MC learning is non-incremental. It must wait until an episode has been completely collected. That is because it must calculate the discounted return of the episode.</td></tr><tr><td>Continuing tasks: Since TD learning is incremental, it can handle both episodic and continuing tasks. Continuing tasks may not have terminal states.</td><td>Episodic tasks: Since MC learning is non-incremental, it can only handle episodic tasks where the episodes terminate after a finite number of steps.</td></tr><tr><td>Bootstrapping: TD learning bootstraps because the update of a state/action value relies on the previous estimate of this value. As a result, TD learning requires an initial guess of the values.</td><td>Non-bootstrapping: MC is not bootstrapping because it can directly estimate state/action values without initial guesses.</td></tr><tr><td>Low estimation variance: The estimation variance of TD is lower than that of M-C because it involves fewer random variables. For instance, to estimate an action value qπ(st, at), Sarsa merely requires the samples of three random variables: Rt+1, St+1, At+1.</td><td>High estimation variance: The estimation variance of MC is higher since many random variables are involved. For example, to estimate qπ(st, at), we need samples of Rt+1 + γ Rt+2 + γ2 Rt+3 + . . . Suppose that the length of each episode is L. Assume that each state has the same number of actions as |A|. Then, there are |A|L possible episodes following a soft policy. If we merely use a few episodes to estimate, it is not surprising that the estimation variance is high.</td></tr></table>

# 7.1.3 Convergence analysis

The convergence analysis of the TD algorithm in (7.1) is given below.

Theorem 7.1 (Convergence of TD learning). Given a policy $\pi$ , by the TD algorithm in (7.1), $\boldsymbol { v } _ { t } ( s )$ converges almost surely to $v _ { \pi } ( s )$ as $t \to \infty$ for all $s \in S$ if $\textstyle \sum _ { t } \alpha _ { t } ( s ) = \infty$ and $\textstyle \sum _ { t } \alpha _ { t } ^ { 2 } ( s ) < \infty$ for all $s \in S$ .

Some remarks about $\alpha _ { t }$ are given below. First, the condition of $\begin{array} { r } { \sum _ { t } \alpha _ { t } ( s ) = \infty } \end{array}$ and $\textstyle \sum _ { t } \alpha _ { t } ^ { 2 } ( s ) < \infty$ must be valid for all $s \in S$ . Note that, at time $t$ , $\alpha _ { t } ( s ) > 0$ if $s$ is being visited and $\alpha _ { t } ( s ) = 0$ otherwise. The condition $\begin{array} { r } { \sum _ { t } \alpha _ { t } ( s ) = \infty } \end{array}$ requires the state $s$ to be visited an infinite (or sufficiently many) number of times. This requires either the condition of exploring starts or an exploratory policy so that every state-action pair can possibly be visited many times. Second, the learning rate $\alpha _ { t }$ is often selected as a small

positive constant in practice. In this case, the condition that $\textstyle \sum _ { t } \alpha _ { t } ^ { 2 } ( s ) < \infty$ is no longer valid. When $\alpha$ is constant, it can still be shown that the algorithm converges in the sense of expectation [24, Section 1.5].

# Box 7.2: Proof of Theorem 7.1

We prove the convergence based on Theorem 6.3 in Chapter 6. To do that, we need first to construct a stochastic process as that in Theorem 6.3. Consider an arbitrary state $s \in S$ . At time $t$ , it follows from the TD algorithm in (7.1) that

$$
v _ {t + 1} (s) = v _ {t} (s) - \alpha_ {t} (s) \Big (v _ {t} (s) - (r _ {t + 1} + \gamma v _ {t} (s _ {t + 1})) \Big), \quad \mathrm {i f} s = s _ {t}, \qquad (7. 7)
$$

or

$$
v _ {t + 1} (s) = v _ {t} (s), \quad \text {i f} s \neq s _ {t}. \tag {7.8}
$$

The estimation error is defined as

$$
\Delta_ {t} (s) \doteq v _ {t} (s) - v _ {\pi} (s),
$$

where $v _ { \pi } ( s )$ is the state value of $s$ under policy $\pi$ . Deducting $v _ { \pi } ( s )$ from both sides of (7.7) gives

$$
\begin{array}{l} \Delta_ {t + 1} (s) = (1 - \alpha_ {t} (s)) \Delta_ {t} (s) + \alpha_ {t} (s) (\underbrace {r _ {t + 1} + \gamma v _ {t} (s _ {t + 1}) - v _ {\pi} (s)} _ {\eta_ {t} (s)}) \\ = (1 - \alpha_ {t} (s)) \Delta_ {t} (s) + \alpha_ {t} (s) \eta_ {t} (s), \quad s = s _ {t}. \tag {7.9} \\ \end{array}
$$

Deducting $v _ { \pi } ( s )$ from both sides of (7.8) gives

$$
\Delta_ {t + 1} (s) = \Delta_ {t} (s) = (1 - \alpha_ {t} (s)) \Delta_ {t} (s) + \alpha_ {t} (s) \eta_ {t} (s), \qquad s \neq s _ {t},
$$

whose expression is the same as that of (7.9) except that $\alpha _ { t } ( s ) = 0$ and $\eta _ { t } ( s ) = 0$ . Therefore, regardless of whether $s = s _ { t }$ , we obtain the following unified expression:

$$
\Delta_ {t + 1} (s) = (1 - \alpha_ {t} (s)) \Delta_ {t} (s) + \alpha_ {t} (s) \eta_ {t} (s).
$$

This is the process in Theorem 6.3. Our goal is to show that the three conditions in Theorem 6.3 are satisfied and hence the process converges.

The first condition is valid as assumed in Theorem 7.1. We next show that the second condition is valid. That is, $\| \mathbb { E } [ \eta _ { t } ( s ) \vert \mathcal { H } _ { t } ] \| _ { \infty } \leq \gamma \| \Delta _ { t } ( s ) \| _ { \infty }$ for all $s \in S$ . Here, $\mathcal { H } _ { t }$ represents the historical information (see the definition in Theorem 6.3). Due to the Markovian property, $\eta _ { t } ( s ) = r _ { t + 1 } + \gamma v _ { t } ( s _ { t + 1 } ) - v _ { \pi } ( s )$ or $\eta _ { t } ( s ) = 0$ does not depend

on the historical information once $s$ is given. As a result, we have $\mathbb { E } [ \eta _ { t } ( s ) \vert \mathcal { H } _ { t } ] =$ $\mathbb { E } [ \eta _ { t } ( s ) ]$ . For $s \neq s _ { t }$ , we have $\eta _ { t } ( s ) = 0$ . Then, it is trivial to see that

$$
\left| \mathbb {E} \left[ \eta_ {t} (s) \right] \right| = 0 \leq \gamma \| \Delta_ {t} (s) \| _ {\infty}. \tag {7.10}
$$

For $s = s _ { t }$ , we have

$$
\begin{array}{l} \mathbb {E} \left[ \eta_ {t} (s) \right] = \mathbb {E} \left[ \eta_ {t} \left(s _ {t}\right) \right] \\ = \mathbb {E} \left[ r _ {t + 1} + \gamma v _ {t} \left(s _ {t + 1}\right) - v _ {\pi} \left(s _ {t}\right) \mid s _ {t} \right] \\ = \mathbb {E} \left[ r _ {t + 1} + \gamma v _ {t} \left(s _ {t + 1}\right) \mid s _ {t} \right] - v _ {\pi} \left(s _ {t}\right). \\ \end{array}
$$

Since $v _ { \pi } ( s _ { t } ) = \mathbb { E } [ r _ { t + 1 } + \gamma v _ { \pi } ( s _ { t + 1 } ) | s _ { t } ]$ , the above equation implies that

$$
\begin{array}{l} \mathbb {E} \left[ \eta_ {t} (s) \right] = \gamma \mathbb {E} \left[ v _ {t} \left(s _ {t + 1}\right) - v _ {\pi} \left(s _ {t + 1}\right) \mid s _ {t} \right] \\ = \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} \mid s _ {t}\right) \left[ v _ {t} \left(s ^ {\prime}\right) - v _ {\pi} \left(s ^ {\prime}\right) \right]. \\ \end{array}
$$

It follows that

$$
\begin{array}{l} | \mathbb {E} [ \eta_ {t} (s) ] | = \gamma \left| \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s _ {t}) [ v _ {t} (s ^ {\prime}) - v _ {\pi} (s ^ {\prime}) ] \right| \\ \leq \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s _ {t}) \max _ {s ^ {\prime} \in \mathcal {S}} | v _ {t} (s ^ {\prime}) - v _ {\pi} (s ^ {\prime}) | \\ = \gamma \max _ {s ^ {\prime} \in \mathcal {S}} \left| v _ {t} \left(s ^ {\prime}\right) - v _ {\pi} \left(s ^ {\prime}\right) \right| \\ = \gamma \| v _ {t} \left(s ^ {\prime}\right) - v _ {\pi} \left(s ^ {\prime}\right) \| _ {\infty} \\ = \gamma \| \Delta_ {t} (s) \| _ {\infty}. \tag {7.11} \\ \end{array}
$$

Therefore, at time $t$ , we know from (7.10) and (7.11) that $| \mathbb { E } [ \eta _ { t } ( s ) ] | \leq \gamma \| \Delta _ { t } ( s ) \| _ { \infty }$ for all $s \in S$ regardless of whether $s = s _ { t }$ . Thus,

$$
\| \mathbb {E} [ \eta_ {t} (s) ] \| _ {\infty} \leq \gamma \| \Delta_ {t} (s) \| _ {\infty},
$$

which is the second condition in Theorem 6.3. Finally, regarding the third condition, we have v $\mathrm { r r } [ \eta _ { t } ( s ) \vert \mathcal { H } _ { t } ] = \mathrm { v a r } [ r _ { t + 1 } + \gamma v _ { t } ( s _ { t + 1 } ) - v _ { \pi } ( s _ { t } ) \vert s _ { t } ] = \mathrm { v a r } [ r _ { t + 1 } + \gamma v _ { t } ( s _ { t + 1 } ) \vert s _ { t } ]$ for $s = s _ { t }$ and $\mathrm { v a r } [ \eta _ { t } ( s ) | \mathcal { H } _ { t } ] = 0$ for $s \neq s _ { t }$ . Since $r _ { t + 1 }$ is bounded, the third condition can be proven without difficulty.

The above proof is inspired by [32].

# 7.2 TD learning of action values: Sarsa

The TD algorithm introduced in Section 7.1 can only estimate state values. This section introduces another TD algorithm called Sarsa that can directly estimate action values. Estimating action values is important because it can be combined with a policy improvement step to learn optimal policies.

# 7.2.1 Algorithm description

Given a policy $\pi$ , our goal is to estimate the action values. Suppose that we have some experience samples generated following $\pi$ : $( s _ { 0 } , a _ { 0 } , r _ { 1 } , s _ { 1 } , a _ { 1 } , \ldots , s _ { t } , a _ { t } , r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } , \ldots )$ . We can use the following Sarsa algorithm to estimate the action values:

$$
q _ {t + 1} \left(s _ {t}, a _ {t}\right) = q _ {t} \left(s _ {t}, a _ {t}\right) - \alpha_ {t} \left(s _ {t}, a _ {t}\right) \left[ q _ {t} \left(s _ {t}, a _ {t}\right) - \left(r _ {t + 1} + \gamma q _ {t} \left(s _ {t + 1}, a _ {t + 1}\right)\right) \right], \tag {7.12}
$$

$$
q _ {t + 1} (s, a) = q _ {t} (s, a), \quad \text {f o r a l l} (s, a) \neq (s _ {t}, a _ {t}),
$$

where $t = 0 , 1 , 2 , \ldots$ and $\alpha _ { t } ( s _ { t } , a _ { t } )$ is the learning rate. Here, $q _ { t } ( s _ { t } , a _ { t } )$ is the estimate of $q _ { \pi } ( s _ { t } , a _ { t } )$ . At time $t$ , only the q-value of $( s _ { t } , a _ { t } )$ is updated, whereas the q-values of the others remain the same.

Some important properties of the Sarsa algorithm are discussed as follows.

Why is this algorithm called “Sarsa”? That is because each iteration of the algorithm requires $( s _ { t } , a _ { t } , r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } )$ . Sarsa is an abbreviation for state-action-reward-stateaction. The Sarsa algorithm was first proposed in [35] and its name was coined by [3].   
Why is Sarsa designed in this way? One may have noticed that Sarsa is similar to the TD algorithm in (7.1). In fact, Sarsa can be easily obtained from the TD algorithm by replacing state value estimation with action value estimation.   
What does Sarsa do mathematically? Similar to the TD algorithm in (7.1), Sarsa is a stochastic approximation algorithm for solving the Bellman equation of a given policy:

$$
q _ {\pi} (s, a) = \mathbb {E} \left[ R + \gamma q _ {\pi} \left(S ^ {\prime}, A ^ {\prime}\right) | s, a \right], \quad \text {f o r a l l} (s, a). \tag {7.13}
$$

Equation (7.13) is the Bellman equation expressed in terms of action values. A proof is given in Box 7.3.

# Box 7.3: Showing that (7.13) is the Bellman equation

As introduced in Section 2.8.2, the Bellman equation expressed in terms of action values is

$$
\begin{array}{l} q _ {\pi} (s, a) = \sum_ {r} r p (r | s, a) + \gamma \sum_ {s ^ {\prime}} \sum_ {a ^ {\prime}} q _ {\pi} (s ^ {\prime}, a ^ {\prime}) p (s ^ {\prime} | s, a) \pi (a ^ {\prime} | s ^ {\prime}) \\ = \sum_ {r} r p (r | s, a) + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) \sum_ {a ^ {\prime}} q _ {\pi} \left(s ^ {\prime}, a ^ {\prime}\right) \pi \left(a ^ {\prime} \mid s ^ {\prime}\right). \tag {7.14} \\ \end{array}
$$

This equation establishes the relationships among the action values. Since

$$
\begin{array}{l} p (s ^ {\prime}, a ^ {\prime} | s, a) = p (s ^ {\prime} | s, a) p (a ^ {\prime} | s ^ {\prime}, s, a) \\ = p (s ^ {\prime} | s, a) p (a ^ {\prime} | s ^ {\prime}) \quad (\mathrm {d u e t o c o n d i t i o n a l i n d e p e n d e n c e}) \\ \stackrel {.} {=} p (s ^ {\prime} | s, a) \pi (a ^ {\prime} | s ^ {\prime}), \\ \end{array}
$$

(7.14) can be rewritten as

$$
q _ {\pi} (s, a) = \sum_ {r} r p (r | s, a) + \gamma \sum_ {s ^ {\prime}} \sum_ {a ^ {\prime}} q _ {\pi} \left(s ^ {\prime}, a ^ {\prime}\right) p \left(s ^ {\prime}, a ^ {\prime} \mid s, a\right).
$$

By the definition of the expected value, the above equation is equivalent to (7.13). Hence, (7.13) is the Bellman equation.

 Is Sarsa convergent? Since Sarsa is the action-value version of the TD algorithm in (7.1), the convergence result is similar to Theorem 7.1 and given below.

Theorem 7.2 (Convergence of Sarsa). Given a policy $\pi$ , by the Sarsa algorithm in (7.12), $q _ { t } ( s , a )$ converges almost surely to the action value $q _ { \pi } ( s , a )$ as $t \to \infty$ for all $( s , a )$ if $\textstyle \sum _ { t } \alpha _ { t } ( s , a ) = \infty$ and $\textstyle \sum _ { t } \alpha _ { t } ^ { 2 } ( s , a ) < \infty$ for all $( s , a )$ .

The proof is similar to that of Theorem 7.1 and is omitted here. The condition of $\textstyle \sum _ { t } \alpha _ { t } ( s , a ) = \infty$ and $\textstyle \sum _ { t } \alpha _ { t } ^ { 2 } ( s , a ) < \infty$ should be valid for all $( s , a )$ . In particular, $\textstyle \sum _ { t } \alpha _ { t } ( s , a ) = \infty$ requires that every state-action pair must be visited an infinite (or sufficiently many) number of times. At time $t$ , if $( s , a ) = ( s _ { t } , a _ { t } )$ , then $\alpha _ { t } ( s , a ) > 0$ ; otherwise, $\alpha _ { t } ( s , a ) = 0$ .

# 7.2.2 Optimal policy learning via Sarsa

The Sarsa algorithm in (7.12) can only estimate the action values of a given policy. To find optimal policies, we can combine it with a policy improvement step. The combination is also often called Sarsa, and its implementation procedure is given in Algorithm 7.1.

# Algorithm 7.1: Optimal policy learning by Sarsa

Initialization: $\alpha _ { t } ( s , a ) = \alpha > 0$ for all $( s , a )$ and all $t$ . $\epsilon \in ( 0 , 1 )$ . Initial $q _ { 0 } ( s , a )$ for all $( s , a )$ . Initial $\epsilon { \cdot }$ -greedy policy $\pi _ { 0 }$ derived from $q _ { 0 }$ .

Goal: Learn an optimal policy that can lead the agent to the target state from an initial state $s _ { 0 }$ .

For each episode, do

Generate $a _ { 0 }$ at $s _ { 0 }$ following $\pi _ { 0 } ( s _ { 0 } )$

If $s _ { t }$ $( t = 0 , 1 , 2 , \ldots )$ is not the target state, do

Collect an experience sample $( r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } )$ given $( s _ { t } , a _ { t } )$ : generate $r _ { t + 1 } , s _ { t + 1 }$ by interacting with the environment; generate $a _ { t + 1 }$ following $\pi _ { t } ( s _ { t + 1 } )$ .

Update $q \mathrm { . }$ -value for $( s _ { t } , a _ { t } )$ :

$$
q _ {t + 1} \left(s _ {t}, a _ {t}\right) = q _ {t} \left(s _ {t}, a _ {t}\right) - \alpha_ {t} \left(s _ {t}, a _ {t}\right) \left[ q _ {t} \left(s _ {t}, a _ {t}\right) - \left(r _ {t + 1} + \gamma q _ {t} \left(s _ {t + 1}, a _ {t + 1}\right)\right) \right]
$$

Update policy for $s _ { t }$

$$
\begin{array}{l} \pi_ {t + 1} (a | s _ {t}) = 1 - \frac {\epsilon}{| \mathcal {A} (s _ {t}) |} (| \mathcal {A} (s _ {t}) | - 1) \mathrm {i f} a = \arg \max _ {a} q _ {t + 1} (s _ {t}, a) \\ \pi_ {t + 1} (a | s _ {t}) = \frac {\epsilon}{| \mathcal {A} (s _ {t}) |} o t h e r w i s e \\ s _ {t} \leftarrow s _ {t + 1}, a _ {t} \leftarrow a _ {t + 1} \\ \end{array}
$$

As shown in Algorithm 7.1, each iteration has two steps. The first step is to update the q-value of the visited state-action pair. The second step is to update the policy to an $\epsilon$ -greedy one. The q-value update step only updates the single state-action pair visited at time $t$ . Afterward, the policy of $s _ { t }$ is immediately updated. Therefore, we do not evaluate a given policy sufficiently well before updating the policy. This is based on the idea of generalized policy iteration. Moreover, after the policy is updated, the policy is immediately used to generate the next experience sample. The policy here is $\epsilon$ -greedy so that it is exploratory.

A simulation example is shown in Figure 7.2 to demonstrate the Sarsa algorithm. Unlike all the tasks we have seen in this book, the task here aims to find an optimal path from a specific starting state to a target state. It does not aim to find the optimal policies for all states. This task is often encountered in practice where the starting state (e.g., home) and the target state (e.g., workplace) are fixed, and we only need to find an optimal path connecting them. This task is relatively simple because we only need to explore the states that are close to the path and do not need to explore all the states. However, if we do not explore all the states, the final path may be locally optimal rather than globally optimal.

The simulation setup and simulation results are discussed below.

Simulation setup: In this example, all the episodes start from the top-left state and terminate at the target state. The reward settings are $r _ { \mathrm { t a r g e t } } = 0$ , $r _ { \mathrm { f o r b i d d e n } } = r _ { \mathrm { b o u n d a r y } } =$ $- 1 0$ , and $r _ { \mathrm { o t h e r } } = - 1$ . Moreover, $\alpha _ { t } ( s , a ) = 0 . 1$ for all $t$ and $\epsilon = 0 . 1$ . The initial guesses of the action values are $q _ { 0 } ( s , a ) = 0$ for all $( s , a )$ . The initial policy has a

![](images/2ccebc939bf15851a89584ca343b935dd4167be52d798b1737f52c791f19a7d1.jpg)

![](images/e5a05ef5b3016f7706967983bd32e30c94b93e46d7648e157e7ee269f9a50d23.jpg)  
Figure 7.2: An example for demonstrating Sarsa. All the episodes start from the top-left state and terminate when reaching the target state (the blue cell). The goal is to find an optimal path from the starting state to the target state. The reward settings are $r _ { \mathrm { t a r g e t } } = 0$ , $r _ { \mathrm { f o r b i d d e n } } = r _ { \mathrm { b o u n d a r y } } = - 1 0$ , and $r _ { \mathrm { o t h e r } } = - 1$ . The learning rate is $\alpha = 0 . 1$ and the value of $\epsilon$ is 0.1. The left figure shows the final policy obtained by the algorithm. The right figures show the total reward and length of every episode.

uniform distribution: $\pi _ { 0 } ( a | s ) = 0 . 2$ for all $s , a$ .

Learned policy: The left figure in Figure 7.2 shows the final policy learned by Sarsa. As can be seen, this policy can successfully lead to the target state from the starting state. However, the policies of some other states may not be optimal. That is because the other states are not well explored.   
Total reward of each episode: The top-right subfigure in Figure 7.2 shows the total reward of each episode. Here, the total reward is the non-discounted sum of all immediate rewards. As can be seen, the total reward of each episode increases gradually. That is because the initial policy is not good and hence negative rewards are frequently obtained. As the policy becomes better, the total reward increases.   
Length of each episode: The bottom-right subfigure in Figure 7.2 shows that the length of each episode drops gradually. That is because the initial policy is not good and may take many detours before reaching the target. As the policy becomes better, the length of the trajectory becomes shorter. Notably, the length of an episode may increase abruptly (e.g., the 460th episode) and the corresponding total reward also drops sharply. That is because the policy is $\epsilon$ -greedy, and there is a chance for it to take non-optimal actions. One way to resolve this problem is to use decaying $\epsilon$ whose value converges to zero gradually.

Finally, Sarsa also has some variants such as Expected Sarsa. Interested readers may check Box 7.4.

# Box 7.4: Expected Sarsa

Given a policy $\pi$ , its action values can be evaluated by Expected Sarsa, which is a variant of Sarsa. The Expected Sarsa algorithm is

$$
q _ {t + 1} (s _ {t}, a _ {t}) = q _ {t} (s _ {t}, a _ {t}) - \alpha_ {t} (s _ {t}, a _ {t}) \Big [ q _ {t} (s _ {t}, a _ {t}) - (r _ {t + 1} + \gamma \mathbb {E} [ q _ {t} (s _ {t + 1}, A) ]) \Big ],
$$

$$
q _ {t + 1} (s, a) = q _ {t} (s, a), \quad \mathrm {f o r a l l} (s, a) \neq (s _ {t}, a _ {t}),
$$

where

$$
\mathbb {E} \left[ q _ {t} \left(s _ {t + 1}, A\right) \right] = \sum_ {a} \pi_ {t} (a \mid s _ {t + 1}) q _ {t} \left(s _ {t + 1}, a\right) \doteq v _ {t} \left(s _ {t + 1}\right)
$$

is the expected value of $q _ { t } ( s _ { t + 1 } , a )$ under policy $\pi _ { t }$ . The expression of the Expected Sarsa algorithm is very similar to that of Sarsa. They are different only in terms of their TD targets. In particular, the TD target in Expected Sarsa is $r _ { t + 1 } + \gamma \mathbb { E } [ q _ { t } ( s _ { t + 1 } , A ) ]$ , while that of Sarsa is $r _ { t + 1 } + \gamma q _ { t } ( s _ { t + 1 } , a _ { t + 1 } )$ . Since the algorithm involves an expected value, it is called Expected Sarsa. Although calculating the expected value may increase the computational complexity slightly, it is beneficial in the sense that it reduces the estimation variances because it reduces the random variables in Sarsa from $\left\{ s _ { t } , a _ { t } , r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } \right\}$ to $\left\{ s _ { t } , a _ { t } , r _ { t + 1 } , s _ { t + 1 } \right\}$ .

Similar to the TD learning algorithm in (7.1), Expected Sarsa can be viewed as a stochastic approximation algorithm for solving the following equation:

$$
q _ {\pi} (s, a) = \mathbb {E} \left[ R _ {t + 1} + \gamma \mathbb {E} \left[ q _ {\pi} \left(S _ {t + 1}, A _ {t + 1}\right) \mid S _ {t + 1} \right] \mid S _ {t} = s, A _ {t} = a \right], \quad \text {f o r a l l} s, a. \tag {7.15}
$$

The above equation may look strange at first glance. In fact, it is another expression of the Bellman equation. To see that, substituting

$$
\mathbb {E} \left[ q _ {\pi} \left(S _ {t + 1}, A _ {t + 1}\right) \mid S _ {t + 1} \right] = \sum_ {A ^ {\prime}} q _ {\pi} \left(S _ {t + 1}, A ^ {\prime}\right) \pi \left(A ^ {\prime} \mid S _ {t + 1}\right) = v _ {\pi} \left(S _ {t + 1}\right)
$$

into (7.15) gives

$$
q _ {\pi} (s, a) = \mathbb {E} \left[ R _ {t + 1} + \gamma v _ {\pi} (S _ {t + 1}) | S _ {t} = s, A _ {t} = a \right],
$$

which is clearly the Bellman equation.

The implementation of Expected Sarsa is similar to that of Sarsa. More details can be found in [3, 36, 37].

# 7.3 TD learning of action values: $n$ -step Sarsa

This section introduces $n$ -step Sarsa, an extension of Sarsa. We will see that Sarsa and MC learning are two extreme cases of $n$ -step Sarsa.

Recall that the definition of the action value is

$$
q _ {\pi} (s, a) = \mathbb {E} [ G _ {t} | S _ {t} = s, A _ {t} = a ], \tag {7.16}
$$

where $G _ { t }$ is the discounted return satisfying

$$
G _ {t} = R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \ldots .
$$

In fact, $G _ { t }$ can also be decomposed into different forms:

$$
\mathrm {S a r s a} \longleftarrow G _ {t} ^ {(1)} = R _ {t + 1} + \gamma q _ {\pi} (S _ {t + 1}, A _ {t + 1}),
$$

$$
G _ {t} ^ {(2)} = R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} q _ {\pi} (S _ {t + 2}, A _ {t + 2}),
$$

$$
n \text {- s t e p} G _ {t} ^ {(n)} = R _ {t + 1} + \gamma R _ {t + 2} + \dots + \gamma^ {n} q _ {\pi} (S _ {t + n}, A _ {t + n}),
$$

$$
\mathrm {M C} \longleftarrow G _ {t} ^ {(\infty)} = R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \gamma^ {3} R _ {t + 4} \dots
$$

It should be noted that $G _ { t } = G _ { t } ^ { ( 1 ) } = G _ { t } ^ { ( 2 ) } = G _ { t } ^ { ( n ) } = G _ { t } ^ { ( \infty ) }$ , where the superscripts merely indicate the different decomposition structures of $G _ { t }$ .

Substituting different decompositions of $G _ { t } ^ { ( n ) }$ into $q _ { \pi } ( s , a )$ in (7.16) results in different algorithms.

$\diamond$ When $n = 1$ , we have

$$
q _ {\pi} (s, a) = \mathbb {E} [ G _ {t} ^ {(1)} | s, a ] = \mathbb {E} [ R _ {t + 1} + \gamma q _ {\pi} (S _ {t + 1}, A _ {t + 1}) | s, a ].
$$

The corresponding stochastic approximation algorithm for solving this equation is

$$
q _ {t + 1} (s _ {t}, a _ {t}) = q _ {t} (s _ {t}, a _ {t}) - \alpha_ {t} (s _ {t}, a _ {t}) \Big [ q _ {t} (s _ {t}, a _ {t}) - (r _ {t + 1} + \gamma q _ {t} (s _ {t + 1}, a _ {t + 1})) \Big ],
$$

which is the Sarsa algorithm in (7.12).

 When $n = \infty$ , we have

$$
q _ {\pi} (s, a) = \mathbb {E} [ G _ {t} ^ {(\infty)} | s, a ] = \mathbb {E} [ R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \ldots | s, a ].
$$

The corresponding algorithm for solving this equation is

$$
q _ {t + 1} (s _ {t}, a _ {t}) = g _ {t} \doteq r _ {t + 1} + \gamma r _ {t + 2} + \gamma^ {2} r _ {t + 3} + \ldots ,
$$

where $g _ { t }$ is a sample of $G _ { t }$ . In fact, this is the MC learning algorithm, which approximates the action value of $( s _ { t } , a _ { t } )$ using the discounted return of an episode starting from $( s _ { t } , a _ { t } )$ .

 For a general value of $n$ , we have

$$
q _ {\pi} (s, a) = \mathbb {E} [ G _ {t} ^ {(n)} | s, a ] = \mathbb {E} [ R _ {t + 1} + \gamma R _ {t + 2} + \dots + \gamma^ {n} q _ {\pi} (S _ {t + n}, A _ {t + n}) | s, a ].
$$

The corresponding algorithm for solving the above equation is

$$
\begin{array}{l} q _ {t + 1} \left(s _ {t}, a _ {t}\right) = q _ {t} \left(s _ {t}, a _ {t}\right) \\ \left. - \alpha_ {t} \left(s _ {t}, a _ {t}\right) \left[ q _ {t} \left(s _ {t}, a _ {t}\right) - \left(r _ {t + 1} + \gamma r _ {t + 2} + \dots + \gamma^ {n} q _ {t} \left(s _ {t + n}, a _ {t + n}\right)\right) \right]. \right. \tag {7.17} \\ \end{array}
$$

This algorithm is called $n$ -step Sarsa.

In summary, $n$ -step Sarsa is a more general algorithm because it becomes the (onestep) Sarsa algorithm when $n = 1$ and the MC learning algorithm when $\boldsymbol { \mathit { n } } = \infty$ (by setting $\alpha _ { t } = 1$ ).

To implement the $n$ -step Sarsa algorithm in (7.17), we need the experience samples $( s _ { t } , a _ { t } , r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } , \dots , r _ { t + n } , s _ { t + n } , a _ { t + n } )$ . Since $( r _ { t + n } , s _ { t + n } , a _ { t + n } )$ has not been collected at time $t$ , we have to wait until time $t + n$ to update the q-value of $( s _ { t } , a _ { t } )$ . To that end, (7.17) can be rewritten as

$$
\begin{array}{l} q _ {t + n} \left(s _ {t}, a _ {t}\right) = q _ {t + n - 1} \left(s _ {t}, a _ {t}\right) \\ - \alpha_ {t + n - 1} (s _ {t}, a _ {t}) \Big [ q _ {t + n - 1} (s _ {t}, a _ {t}) - \big (r _ {t + 1} + \gamma r _ {t + 2} + \dots + \gamma^ {n} q _ {t + n - 1} (s _ {t + n}, a _ {t + n}) \big) \Big ], \\ \end{array}
$$

where $q _ { t + n } ( s _ { t } , a _ { t } )$ is the estimate of $q _ { \pi } ( s _ { t } , a _ { t } )$ at time $t + n$ .

Since $n$ -step Sarsa includes Sarsa and MC learning as two extreme cases, it is not surprising that the performance of $n$ -step Sarsa is between that of Sarsa and MC learning. In particular, if $n$ is selected as a large number, $n$ -step Sarsa is close to MC learning: the estimate has a relatively high variance but a small bias. If $n$ is selected to be small, $n$ -step Sarsa is close to Sarsa: the estimate has a relatively large bias but a low variance. Finally, the $n$ -step Sarsa algorithm presented here is merely used for policy evaluation. It must be combined with a policy improvement step to learn optimal policies. The implementation is similar to that of Sarsa and is omitted here. Interested readers may check [3, Chapter 7] for a detailed analysis of multi-step TD learning.

# 7.4 TD learning of optimal action values: Q-learning

In this section, we introduce the Q-learning algorithm, one of the most classic reinforcement learning algorithms [38,39]. Recall that Sarsa can only estimate the action values of a given policy, and it must be combined with a policy improvement step to find optimal policies. By contrast, Q-learning can directly estimate optimal action values and find optimal policies.

# 7.4.1 Algorithm description

The Q-learning algorithm is

$$
q _ {t + 1} (s _ {t}, a _ {t}) = q _ {t} (s _ {t}, a _ {t}) - \alpha_ {t} (s _ {t}, a _ {t}) \left[ q _ {t} (s _ {t}, a _ {t}) - \left(r _ {t + 1} + \gamma \max  _ {a \in \mathcal {A} (s _ {t + 1})} q _ {t} (s _ {t + 1}, a)\right) \right], \tag {7.18}
$$

$$
q _ {t + 1} (s, a) = q _ {t} (s, a), \quad \mathrm {f o r a l l} (s, a) \neq (s _ {t}, a _ {t}),
$$

where $t = 0 , 1 , 2 , \ldots .$ . Here, $q _ { t } ( s _ { t } , a _ { t } )$ is the estimate of the optimal action value of $( s _ { t } , a _ { t } )$ and $\alpha _ { t } ( s _ { t } , a _ { t } )$ is the learning rate for $( s _ { t } , a _ { t } )$ .

The expression of Q-learning is similar to that of Sarsa. They are different only in terms of their TD targets: the TD target of Q-learning is $r _ { t + 1 } + \gamma \operatorname* { m a x } _ { a } q _ { t } ( s _ { t + 1 } , a )$ , whereas that of Sarsa is $r _ { t + 1 } + \gamma q _ { t } ( s _ { t + 1 } , a _ { t + 1 } )$ . Moreover, given $( s _ { t } , a _ { t } )$ , Sarsa requires $( r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } )$ in every iteration, whereas Q-learning merely requires $( r _ { t + 1 } , s _ { t + 1 } )$ .

Why is Q-learning designed as the expression in (7.18), and what does it do mathematically? Q-learning is a stochastic approximation algorithm for solving the following equation:

$$
q (s, a) = \mathbb {E} \left[ R _ {t + 1} + \gamma \max  _ {a} q \left(S _ {t + 1}, a\right) \mid S _ {t} = s, A _ {t} = a \right]. \tag {7.19}
$$

This is the Bellman optimality equation expressed in terms of action values. The proof is given in Box 7.5. The convergence analysis of Q-learning is similar to Theorem 7.1 and omitted here. More information can be found in [32, 39].

# Box 7.5: Showing that (7.19) is the Bellman optimality equation

By the definition of expectation, (7.19) can be rewritten as

$$
q (s, a) = \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) \max _ {a \in \mathcal {A} (s ^ {\prime})} q (s ^ {\prime}, a).
$$

Taking the maximum of both sides of the equation gives

$$
\max _ {a \in \mathcal {A} (s)} q (s, a) = \max _ {a \in \mathcal {A} (s)} \left[ \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) \max _ {a \in \mathcal {A} (s ^ {\prime})} q (s ^ {\prime}, a) \right].
$$

By denoting $v ( s ) \doteq \operatorname* { m a x } _ { a \in { \mathcal { A } } ( s ) } q ( s , a )$ , we can rewrite the above equation as

$$
\begin{array}{l} v (s) = \max _ {a \in \mathcal {A} (s)} \left[ \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) v (s ^ {\prime}) \right] \\ = \max _ {\pi} \sum_ {a \in \mathcal {A} (s)} \pi (a | s) \left[ \sum_ {r} p (r | s, a) r + \gamma \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) v (s ^ {\prime}) \right], \\ \end{array}
$$

which is clearly the Bellman optimality equation in terms of state values as introduced in Chapter 3.

# 7.4.2 Off-policy vs on-policy

We next introduce two important concepts: on-policy learning and off-policy learning. What makes Q-learning slightly special compared to the other TD algorithms is that Q-learning is off-policy while the others are on-policy.

Two policies exist in any reinforcement learning task: a behavior policy and a target policy. The behavior policy is the one used to generate experience samples. The target policy is the one that is constantly updated to converge to an optimal policy. When the behavior policy is the same as the target policy, such a learning process is called on-policy. Otherwise, when they are different, the learning process is called off-policy.

The advantage of off-policy learning is that it can learn optimal policies based on the experience samples generated by other policies, which may be, for example, a policy executed by a human operator. As an important case, the behavior policy can be selected to be exploratory. For example, if we would like to estimate the action values of all stateaction pairs, we must generate episodes visiting every state-action pair sufficiently many times. Although Sarsa uses $\epsilon$ -greedy policies to maintain certain exploration abilities, the value of $\epsilon$ is usually small and hence the exploration ability is limited. By contrast, if we can use a policy with a strong exploration ability to generate episodes and then use off-policy learning to learn optimal policies, the learning efficiency would be significantly increased.

To determine if an algorithm is on-policy or off-policy, we can examine two aspects. The first is the mathematical problem that the algorithm aims to solve. The second is the experience samples required by the algorithm.

$\diamond$ Sarsa is on-policy.

The reason is as follows. Sarsa has two steps in every iteration. The first step is to evaluate a policy $\pi$ by solving its Bellman equation. To do that, we need samples generated by $\pi$ . Therefore, $\pi$ is the behavior policy. The second step is to obtain an improved policy based on the estimated values of $\pi$ . As a result, $\pi$ is the target policy that is constantly updated and eventually converges to an optimal policy. Therefore, the behavior policy and the target policy are the same.

From another point of view, we can examine the samples required by the algorithm. The samples required by Sarsa in every iteration include $( s _ { t } , a _ { t } , r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } )$ . How these samples are generated is illustrated below:

$$
s _ {t} \xrightarrow {\pi_ {b}} a _ {t} \xrightarrow {\mathrm {m o d e l}} r _ {t + 1}, s _ {t + 1} \xrightarrow {\pi_ {b}} a _ {t + 1}
$$

As can be seen, the behavior policy $\pi _ { b }$ is the one that generates $a _ { t }$ at $s _ { t }$ and $a _ { t + 1 }$ at $s _ { t + 1 }$ . The Sarsa algorithm aims to estimate the action value of $( s _ { t } , a _ { t } )$ of a policy denoted as $\pi _ { T }$ , which is the target policy because it is improved in every iteration based on the estimated values. In fact, $\pi _ { T }$ is the same as $\pi _ { b }$ because the evaluation of $\pi _ { T }$ relies on the samples $( r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } )$ , where $a _ { t + 1 }$ is generated following $\pi _ { b }$ . In other words, the policy that Sarsa evaluates is the policy used to generate samples.

Q-learning is off-policy.

The fundamental reason is that Q-learning is an algorithm for solving the Bellman optimality equation, whereas Sarsa is for solving the Bellman equation of a given policy. While solving the Bellman equation can evaluate the associated policy, solving the Bellman optimality equation can directly generate the optimal values and optimal policies.

In particular, the samples required by Q-learning in every iteration is $\left( { { s _ { t } } , { a _ { t } } , { r _ { t + 1 } } , { s _ { t + 1 } } } \right)$ . How these samples are generated is illustrated below:

$$
s _ {t} \xrightarrow {\pi_ {b}} a _ {t} \xrightarrow {\text {m o d e l}} r _ {t + 1}, s _ {t + 1}
$$

As can be seen, the behavior policy $\pi _ { b }$ is the one that generates $a _ { t }$ at $s _ { t }$ . The Q-learning algorithm aims to estimate the optimal action value of $( s _ { t } , a _ { t } )$ . This estimation process relies on the samples $( r _ { t + 1 } , s _ { t + 1 } )$ . The process of generating $( r _ { t + 1 } , s _ { t + 1 } )$ does not involve $\pi _ { b }$ because it is governed by the system model (or by interacting with the environment). Therefore, the estimation of the optimal action value of $( s _ { t } , a _ { t } )$ does not involve $\pi _ { b }$ and we can use any $\pi _ { b }$ to generate $a _ { t }$ at $s _ { t }$ . Moreover, the target policy $\pi _ { T }$ here is the greedy policy obtained based on the estimated optimal values (Algorithm 7.3). The behavior policy does not have to be the same as $\pi _ { T }$ .

MC learning is on-policy. The reason is similar to that of Sarsa. The target policy to be evaluated and improved is the same as the behavior policy that generates samples.

Another concept that may be confused with on-policy/off-policy is online/offline. Online learning refers to the case where the agent updates the values and policies while interacting with the environment. Offline learning refers to the case where the agent updates the values and policies using pre-collected experience data without interacting with the environment. If an algorithm is on-policy, then it can be implemented in an online fashion, but cannot use pre-collected data generated by other policies. If an algorithm is off-policy, then it can be implemented in either an online or offline fashion.

# Algorithm 7.2: Optimal policy learning via Q-learning (on-policy version)

Initialization: $\alpha _ { t } ( s , a ) = \alpha > 0$ for all $( s , a )$ and all $t$ . $\epsilon \in ( 0 , 1 )$ . Initial $q _ { 0 } ( s , a )$ for all $( s , a )$ . Initial $\epsilon { \cdot }$ -greedy policy $\pi _ { 0 }$ derived from $q _ { 0 }$ .

Goal: Learn an optimal path that can lead the agent to the target state from an initial state $s _ { 0 }$ .

For each episode, do

If $s _ { t }$ $( t = 0 , 1 , 2 , \ldots )$ is not the target state, do

Collect the experience sample $( a _ { t } , r _ { t + 1 } , s _ { t + 1 } )$ given $s _ { t }$ : generate $a _ { t }$ following $\pi _ { t } ( s _ { t } )$ ; generate $r _ { t + 1 } , s _ { t + 1 }$ by interacting with the environment.

Update $q \mathrm { . }$ -value for $( s _ { t } , a _ { t } )$ :

$$
q _ {t + 1} \left(s _ {t}, a _ {t}\right) = q _ {t} \left(s _ {t}, a _ {t}\right) - \alpha_ {t} \left(s _ {t}, a _ {t}\right) \left[ q _ {t} \left(s _ {t}, a _ {t}\right) - \left(r _ {t + 1} + \gamma \max  _ {a} q _ {t} \left(s _ {t + 1}, a\right)\right) \right]
$$

Update policy for $s _ { t }$

$$
\pi_ {t + 1} (a | s _ {t}) = 1 - \frac {\epsilon}{| \mathcal {A} (s _ {t}) |} \left(| \mathcal {A} (s _ {t}) | - 1\right) \text {i f} a = \arg \max  _ {a} q _ {t + 1} (s _ {t}, a)
$$

$$
\pi_ {t + 1} (a | s _ {t}) = \frac {\epsilon}{| \mathcal {A} (s _ {t}) |} o t h e r w i s e
$$

# Algorithm 7.3: Optimal policy learning via Q-learning (off-policy version)

Initialization: Initial guess $q _ { 0 } ( s , a )$ for all $( s , a )$ . Behavior policy $\pi _ { b } ( a | s )$ for all $( s , a )$ . $\alpha _ { t } ( s , a ) = \alpha > 0$ for all $( s , a )$ and all $t$ .

Goal: Learn an optimal target policy $\pi _ { T }$ for all states from the experience samples generated by $\pi _ { b }$ .

For each episode $\left\{ s _ { 0 } , a _ { 0 } , r _ { 1 } , s _ { 1 } , a _ { 1 } , r _ { 2 } , . . . \right\}$ generated by $\pi _ { b }$ , do

For each step $t = 0 , 1 , 2 , \ldots$ . of the episode, do

Update $q \mathrm { . }$ -value for $( s _ { t } , a _ { t } )$ :

$$
q _ {t + 1} (s _ {t}, a _ {t}) = q _ {t} (s _ {t}, a _ {t}) - \alpha_ {t} (s _ {t}, a _ {t}) \left[ q _ {t} (s _ {t}, a _ {t}) - \left(r _ {t + 1} + \gamma \max  _ {a} q _ {t} (s _ {t + 1}, a)\right) \right]
$$

Update target policy for $s _ { t }$

$$
\pi_ {T, t + 1} (a \mid s _ {t}) = 1 \text {i f} a = \arg \max  _ {a} q _ {t + 1} \left(s _ {t}, a\right)
$$

$$
\pi_ {T, t + 1} (a | s _ {t}) = 0 \text {o t h e r w i s e}
$$

![](images/a8e931bf946c786c133e258c019d28e2bcb7a6b5b60acd2b46c3db89553eecf7.jpg)

![](images/74209b625136ac503a110e0bee8cf66c75caafe09083ee47bd958d3934d3b52c.jpg)  
Figure 7.3: An example for demonstrating Q-learning. All the episodes start from the top-left state and terminate after reaching the target state. The aim is to find an optimal path from the starting state to the target state. The reward settings are $r _ { \mathrm { t a r g e t } } = 0$ , $r _ { \mathrm { f o r b i d d e n } } = r _ { \mathrm { b o u n d a r y } } = - 1 0$ , and $r _ { \mathrm { o t h e r } } = - 1$ . The learning rate is $\alpha = 0 . 1$ and the value of $\epsilon$ is 0.1. The left figure shows the final policy obtained by the algorithm. The right figure shows the total reward and length of every episode.

# 7.4.3 Implementation

Since Q-learning is off-policy, it can be implemented in either an on-policy or off-policy fashion.

The on-policy version of Q-learning is shown in Algorithm 7.2. This implementation is similar to the Sarsa one in Algorithm 7.1. Here, the behavior policy is the same as the target policy, which is an $\epsilon$ -greedy policy.

The off-policy version is shown in Algorithm 7.3. The behavior policy $\pi _ { b }$ can be any policy as long as it can generate sufficient experience samples. It is usually favorable when $\pi _ { b }$ is exploratory. Here, the target policy $\pi _ { T }$ is greedy rather than $\epsilon$ -greedy since it is not used to generate samples and hence is not required to be exploratory. Moreover, the off-policy version of Q-learning presented here is implemented offline: all the experience samples are collected first and then processed.

# 7.4.4 Illustrative examples

We next present examples to demonstrate Q-learning.

The first example is shown in Figure 7.3. It demonstrates on-policy Q-learning. The goal here is to find an optimal path from a starting state to the target state. The setup is given in the caption of Figure 7.3. As can be seen, Q-learning can eventually find an optimal path. During the learning process, the length of each episode decreases, whereas the total reward of each episode increases.

The second set of examples is shown in Figure 7.4 and Figure 7.5. They demonstrate off-policy Q-learning. The goal here is to find an optimal policy for all the states. The reward setting is $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1$ , and $r _ { \mathrm { t a r g e t } } = 1$ . The discount rate is $\gamma = 0 . 9$ . The learning rate is $\alpha = 0 . 1$ .

Ground truth: To verify the effectiveness of Q-learning, we first need to know the ground truth of the optimal policies and optimal state values. Here, the ground truth is obtained by the model-based policy iteration algorithm. The ground truth is given in Figures 7.4(a) and (b).   
Experience samples: The behavior policy has a uniform distribution: the probability of taking any action at any state is 0.2 (Figure 7.4(c)). A single episode with 100,000 steps is generated (Figure 7.4(d)). Due to the good exploration ability of the behavior policy, the episode visits every state-action pair many times.   
Learned results: Based on the episode generated by the behavior policy, the final target policy learned by Q-learning is shown in Figure 7.4(e). This policy is optimal because the estimated state value error (root-mean-square error) converges to zero as shown in Figure 7.4(f). In addition, one may notice that the learned optimal policy is not exactly the same as that in Figure 7.4(a). In fact, there exist multiple optimal policies that have the same optimal state values.   
Different initial values: Since Q-learning bootstraps, the performance of the algorithm depends on the initial guess for the action values. As shown in Figure 7.4(g), when the initial guess is close to the true value, the estimate converges within approximately 10,000 steps. Otherwise, the convergence requires more steps (Figure 7.4(h)). Nevertheless, these figures demonstrate that Q-learning can still converge rapidly even though the initial value is not accurate.   
Different behavior policies: When the behavior policy is not exploratory, the learning performance drops significantly. For example, consider the behavior policies shown in Figure 7.5. They are $\epsilon$ -greedy policies with $\epsilon = 0 . 5$ or 0.1 (the uniform policy in Figure 7.4(c) can be viewed as $\epsilon$ -greedy with $\epsilon = 1$ ). It is shown that, when $\epsilon$ decreases from 1 to 0.5 and then to 0.1, the learning speed drops significantly. That is because the exploration ability of the policy is weak and hence the experience samples are insufficient.

# 7.5 A unified viewpoint

Up to now, we have introduced different TD algorithms such as Sarsa, $n$ -step Sarsa, and Q-learning. In this section, we introduce a unified framework to accommodate all these algorithms and MC learning.

In particular, the TD algorithms (for action value estimation) can be expressed in a unified expression:

$$
q _ {t + 1} (s _ {t}, a _ {t}) = q _ {t} (s _ {t}, a _ {t}) - \alpha_ {t} (s _ {t}, a _ {t}) [ q _ {t} (s _ {t}, a _ {t}) - \bar {q} _ {t} ], \tag {7.20}
$$

![](images/fc9df40216523148b2d167feb93014afb4f1d177b8befd1c5668c4322a843d14.jpg)  
(a) Optimal policy

![](images/3abc4232ccff9181479baf30be2a496218a120f59ca6f231b158bbbf88c90ad0.jpg)  
(b) Optimal state value

![](images/d6f499242e3478e08bee258f66dedba3ebdda0fc156f5156fb8df6dd09029393.jpg)  
(c) Behavior policy

![](images/83bfae8f002fc2ed3bd274e4d239e5177e90fe12e94b8c238e228c07876131bc.jpg)  
(d) Generated episode

![](images/291709cb4b1e3fc24997a7c947776318c3757f562dde738104f2fbf30198a911.jpg)  
(e) Learned policy

![](images/d64ee142c782f92ecb8ac9a529effc46d9cf255f83f9d65203526197bf575264.jpg)  
(f) State value error when $q _ { 0 } ( s , a ) = 0$

![](images/c308521fbd81890239f92250b170d2282bb4b5d3defcd2b0841d3b15ef792e42.jpg)  
(g) State value error when $q _ { 0 } ( s , a ) = 1 0$

![](images/02a39304a3454b72eb3429e8674f62f39292c87fe395f739e8573a7c37026c31.jpg)  
(h) State value error when $q _ { 0 } ( s , a ) = 1 0 0$   
Figure 7.4: Examples for demonstrating off-policy learning via Q-learning. The optimal policy and optimal state values are shown in (a) and (b), respectively. The behavior policy and the generated episode are shown in (c) and (d), respectively. The estimated policy and the estimation error evolution are shown in (e) and (f), respectively. The cases with different initial values are shown in (g) and (h).

![](images/ec6e74d81947357ebd7135fa7d382a6bd7379bd710514b4446973aa547cd770c.jpg)

![](images/5c4583827a71dd47c01eb7a23f4fae6d57c1b7203a99d63bc1dcd66da1ac8ec8.jpg)

![](images/2a16759528abdcba233f99bd104843616a10300f03fc6cc869189b1ad3a2d589.jpg)  
(a)  = 0.5

![](images/622878c7476ce1abe6985a2ff15c7d147f2790df8fc904ecb275b97397b50167.jpg)

![](images/814b436ebe34554a5cfd1922a5e8725b2bd29fb66a9804ecf19a573c00316272.jpg)

![](images/33273ae861967636529b5b9f07e86f00b2e0b6fd614b621d1a51c962dfa37bbb.jpg)  
(b) $\epsilon = 0 . 1$

![](images/ef182fc5609e735809fd51386421af997f96c212b4e8c3766e5d05eaf084f27a.jpg)

![](images/4dd1965a3cef5e5a19eae42b8bd523ca0e2cbccd5f81e9525d5c7c39fcd629dc.jpg)

![](images/e835722f953032aedb04bc37358ad4985315c6589fdd184a34d0540816b60737.jpg)  
(c)  = 0.1   
Figure 7.5: The performance of Q-learning drops when the behavior policy is not exploratory. The figures in the left column show the behavior policies. The figures in the middle column show the generated episodes following the corresponding behavior policies. The episode in each example has 100,000 steps. The figures in the right column show the evolution of the root-mean-square error of the estimated state values.

<table><tr><td>Algorithm</td><td>Expression of the TD target q̅t in (7.20)</td></tr><tr><td>Sarsa</td><td>q̅t = rt+1 + γqt(st+1, at+1)</td></tr><tr><td>n-step Sarsa</td><td>q̅t = rt+1 + γrt+2 + ··· + γnqt(st+n, at+n)</td></tr><tr><td>Q-learning</td><td>q̅t = rt+1 + γmaxaqt(st+1, a)</td></tr><tr><td>Monte Carlo</td><td>q̅t = rt+1 + γrt+2 + γ2rt+3 + ···</td></tr></table>

Table 7.2: A unified point of view of TD algorithms. Here, BE and BOE denote the Bellman equation and Bellman optimality equation, respectively.   

<table><tr><td>Algorithm</td><td>Equation to be solved</td></tr><tr><td>Sarsa</td><td>BE: qπ(s, a) = E[ Rt+1 + γqπ(St+1, At+1)|St = s, At = a]</td></tr><tr><td>n-step Sarsa</td><td>BE: qπ(s, a) = E[Rt+1 + γRt+2 + ··· + γnqπ(St+n, At+n)|St = s, At = a]</td></tr><tr><td>Q-learning</td><td>BOE: q(s, a) = E[ Rt+1 + γmaxa q(St+1, a)|St = s, At = a]</td></tr><tr><td>Monte Carlo</td><td>BE: qπ(s, a) = E[Rt+1 + γRt+2 + γ2Rt+3 + ··· |St = s, At = a]</td></tr></table>

where $q _ { t }$ is the TD target. Different TD algorithms have different $q _ { t }$ . See Table 7.2 for a summary. The MC learning algorithm can be viewed as a special case of (7.20): we can set $\alpha _ { t } ( s _ { t } , a _ { t } ) = 1$ and then (7.20) becomes $q _ { t + 1 } ( s _ { t } , a _ { t } ) = \bar { q } _ { t }$ .

Algorithm (7.20) can be viewed as a stochastic approximation algorithm for solving a unified equation: $q ( s , a ) \ = \ \mathbb { E } [ { \bar { q } } _ { t } | s , a ]$ . This equation has different expressions with different $q _ { t }$ . These expressions are summarized in Table 7.2. As can be seen, all of the algorithms aim to solve the Bellman equation except Q-learning, which aims to solve the Bellman optimality equation.

# 7.6 Summary

This chapter introduced an important class of reinforcement learning algorithms called TD learning. The specific algorithms that we introduced include Sarsa, $n$ -step Sarsa, and Q-learning. All these algorithms can be viewed as stochastic approximation algorithms for solving Bellman or Bellman optimality equations.

The TD algorithms introduced in this chapter, except Q-learning, are used to evaluate a given policy. That is to estimate a given policy’s state/action values from some experience samples. Together with policy improvement, they can be used to learn optimal policies. Moreover, these algorithms are on-policy: the target policy is used as the behavior policy to generate experience samples.

Q-learning is slightly special compared to the other TD algorithms in the sense that it is off-policy. The target policy can be different from the behavior policy in Q-learning. The fundamental reason why Q-learning is off-policy is that Q-learning aims to solve the Bellman optimality equation rather than the Bellman equation of a given policy.

It is worth mentioning that there are some methods that can convert an on-policy algorithm to be off-policy. Importance sampling is a widely used one [3, 40] and will be introduced in Chapter 10. Finally, there are some variants and extensions of the TD algorithms introduced in this chapter [41–45]. For example, the TD(λ) method provides a more general and unified framework for TD learning. More information can be found in [3, 20, 46].

# 7.7 Q&A

 Q: What does the term “TD” in TD learning mean?

A: Every TD algorithm has a TD error, which represents the discrepancy between the new sample and the current estimate. Since this discrepancy is calculated between different time steps, it is called temporal-difference.

 Q: What does the term “learning” in TD learning mean?

A: From a mathematical point of view, “learning” simply means “estimation”. That is to estimate state/action values from some samples and then obtain policies based on the estimated values.

 Q: While Sarsa can estimate the action values of a given policy, how can it be used to learn optimal policies?

A: To obtain an optimal policy, the value estimation process should interact with the policy improvement process. That is, after a value is updated, the corresponding policy should be updated. Then, the updated policy generates new samples that can be used to estimate values again. This is the idea of generalized policy iteration.

 Q: Why does Sarsa update policies to be $\epsilon$ -greedy?

A: That is because the policy is also used to generate samples for value estimation. Hence, it should be exploratory to generate sufficient experience samples.

 Q: While Theorems 7.1 and 7.2 require that the learning rate $\alpha _ { t }$ converges to zero gradually, why is it often set to be a small constant in practice?

A: The fundamental reason is that the policy to be evaluated keeps changing (or called nonstationary). In particular, a TD learning algorithm like Sarsa aims to estimate the action values of a given policy. If the policy is fixed, using a decaying learning rate is acceptable. However, in the optimal policy learning process, the policy that Sarsa aims to evaluate keeps changing after every iteration. We need a constant learning rate in this case; otherwise, a decaying learning rate may be too small to effectively evaluate policies. Although a drawback of constant learning rates is that the value estimate may fluctuate eventually, the fluctuation is neglectable as long as the constant learning rate is sufficiently small.

Q: Should we learn the optimal policies for all states or a subset of the states?

A: It depends on the task. One may notice that some tasks considered in this chapter (e.g., Figure 7.2) do not require finding the optimal policies for all states. Instead, they only need to find an optimal path from a given starting state to the target state. Such tasks are not demanding in terms of data because the agent does not need to visit every state-action pair sufficiently many times. It, however, must be noted that the obtained path is not guaranteed to be optimal. That is because better paths may be missed if not all state-action pairs are well explored. Nevertheless, given sufficient data, we can still find a good or locally optimal path.

 Q: Why is Q-learning off-policy while all the other TD algorithms in this chapter are on-policy?

A: The fundamental reason is that Q-learning aims to solve the Bellman optimality equation, whereas the other TD algorithms aim to solve the Bellman equation of a given policy. Details can be found in Section 7.4.2.

Q: Why does the off-policy version of Q-learning update policies to be greedy instead of $\epsilon$ -greedy?

A: That is because the target policy is not required to generate experience samples. Hence, it is not required to be exploratory.

# Chapter 8

# Value Function Methods

![](images/5198225c2f81010375183cc92d3c199544f5ada863a3c7c32c1e2f581cd3bbee.jpg)  
Figure 8.1: Where we are in this book.

In this chapter, we continue to study temporal-difference learning algorithms. However, a different method is used to represent state/action values. So far in this book, state/action values have been represented by tables. The tabular method is straightforward to understand, but it is inefficient for handling large state or action spaces. To solve this problem, this chapter introduces the value function method, which has become a standard way to represent values. It is also where artificial neural networks are incorporated into reinforcement learning as function approximators. The idea of value function can also be extended to policy function, as introduced in Chapter 9.

![](images/f3666dd0dcced9a2c84a47f88e8945e0d23542ec5c599bfea4613a926ea55f5d.jpg)  
Figure 8.2: An illustration of the function approximation method. The x-axis and y-axis correspond to $s$ and $\hat { v } ( s )$ , respectively.

# 8.1 Value representation: From table to function

We next use an example to demonstrate the difference between the tabular and function approximation methods.

Suppose that there are $n$ states $\{ s _ { i } \} _ { i = 1 } ^ { n }$ , whose state values are $\{ v _ { \pi } ( s _ { i } ) \} _ { i = 1 } ^ { n }$ . Here, $\pi$ is a given policy. Let $\{ \hat { v } ( s _ { i } ) \} _ { i = 1 } ^ { n }$ denote the estimates of the true state values. If we use the tabular method, the estimated values can be maintained in the following table. This table can be stored in memory as an array or a vector. To retrieve or update any value, we can directly read or rewrite the corresponding entry in the table.

<table><tr><td>State</td><td>s1</td><td>s2</td><td>···</td><td>sn</td></tr><tr><td>Estimated value</td><td>ˆv(s1)</td><td>ˆv(s2)</td><td>···</td><td>ˆv(sn)</td></tr></table>

We next show that the values in the above table can be approximated by a function. In particular, $\{ ( s _ { i } , \hat { v } ( s _ { i } ) ) \} _ { i = 1 } ^ { n }$ are shown as $n$ points in Figure 8.2. These points can be fitted or approximated by a curve. The simplest curve is a straight line, which can be described as

$$
\hat {v} (s, w) = a s + b = \underbrace {[ s , 1 ]} _ {\phi^ {T} (s)} \underbrace {\left[ \begin{array}{l} a \\ b \end{array} \right]} _ {w} = \phi^ {T} (s) w. \tag {8.1}
$$

Here, ${ \hat { v } } ( s , w )$ is a function for approximating $v _ { \pi } ( s )$ . It is determined jointly by the state $s$ and the parameter vector $w \in \mathbb { R } ^ { 2 }$ . ${ \hat { v } } ( s , w )$ is sometimes written as $\hat { v } _ { w } ( s )$ . Here, $\phi ( s ) \in \mathbb { R } ^ { 2 }$ is called the feature vector of $s$ .

The first notable difference between the tabular and function approximation methods concerns how they retrieve and update a value.

How to retrieve a value: When the values are represented by a table, if we want to retrieve a value, we can directly read the corresponding entry in the table. However,

when the values are represented by a function, it becomes slightly more complicated to retrieve a value. In particular, we need to input the state index $s$ into the function and calculate the function value (Figure 8.3). For the example in (8.1), we first need to calculate the feature vector $\phi ( s )$ and then calculate $\phi ^ { T } ( s ) w$ . If the function is an artificial neural network, a forward propagation from the input to the output is needed.

![](images/4c6b7fa82c0f4922b77fac616878850a10602fdea952a84cba296e957c3ffdf3.jpg)  
Figure 8.3: An illustration of the process for retrieving the value of $s$ when using the function approximation method.

The function approximation method is more efficient in terms of storage due to the way in which the state values are retrieved. Specifically, while the tabular method needs to store $n$ values, we now only need to store a lower dimensional parameter vector $w$ . Thus, the storage efficiency can be significantly improved. Such a benefit is, however, not free. It comes with a cost: the state values may not be accurately represented by the function. For example, a straight line is not able to accurately fit the points in Figure 8.2. That is why this method is called approximation. From a fundamental point of view, some information will certainly be lost when we use a lowdimensional vector to represent a high-dimensional dataset. Therefore, the function approximation method enhances storage efficiency by sacrificing accuracy.

 How to update a value: When the values are represented by a table, if we want to update one value, we can directly rewrite the corresponding entry in the table. However, when the values are represented by a function, the way to update a value is completely different. Specifically, we must update $w$ to change the values indirectly. How to update $w$ to find optimal state values will be addressed in detail later.

Thanks to the way in which the state values are updated, the function approximation method has another merit: its generalization ability is stronger than that of the tabular method. The reason is as follows. When using the tabular method, we can update a value if the corresponding state is visited in an episode. The values of the states that have not been visited cannot be updated. However, when using the function approximation method, we need to update $w$ to update the value of a state. The update of $w$ also affects the values of some other states even though these states have not been visited. Therefore, the experience sample for one state can generalize to help estimate the values of some other states.

The above analysis is illustrated in Figure 8.4, where there are three states $\{ s _ { 1 } , s _ { 2 } , s _ { 3 } \}$ .

Suppose that we have an experience sample for $s _ { 3 }$ and would like to update $\hat { v } ( s _ { 3 } )$ . When using the tabular method, we can only update $\hat { v } ( s _ { 3 } )$ without changing $\hat { v } ( s _ { 1 } )$ or $\hat { v } ( s _ { 2 } )$ , as shown in Figure 8.4(a). When using the function approximation method, updating $w$ not only can update $\hat { v } ( s _ { 3 } )$ but also would change $\hat { v } ( s _ { 1 } )$ and $\hat { v } ( s _ { 2 } )$ , as shown in Figure 8.4(b). Therefore, the experience sample of $s _ { 3 }$ can help update the values of its neighboring states.

![](images/210c708436573989edb8cbbdad503d22bc436d0af2e0282907ebe557b3b9e87a.jpg)  
(a) Tabular method: when $\hat { v } ( s _ { 3 } )$ is updated, the other values remain the same.

Figure 8.4: An illustration of how to update the value of a state.   
![](images/7b747652eedc9e91476e659c127f93254095781ef2ed510a326d1616d8d01cc6.jpg)  
(b) Function approximation method: when we update $\hat { v } ( s _ { 3 } )$ by changing $w$ , the values of the neighboring states are also changed.

We can use more complex functions that have stronger approximation abilities than straight lines. For example, consider a second-order polynomial:

$$
\hat {v} (s, w) = a s ^ {2} + b s + c = \underbrace {\left[ s ^ {2} , s , 1 \right]} _ {\phi^ {T} (s)} \underbrace {\left[ \begin{array}{l} a \\ b \\ c \end{array} \right]} _ {w} = \phi^ {T} (s) w. \tag {8.2}
$$

We can use even higher-order polynomial curves to fit the points. As the order of the curve increases, the approximation accuracy can be improved, but the dimension of the parameter vector also increases, requiring more storage and computational resources.

Note that ${ \hat { v } } ( s , w )$ in either (8.1) or (8.2) is linear in $w$ (though it may be nonlinear in $s$ ). This type of method is called linear function approximation, which is the simplest function approximation method. To realize linear function approximation, we need to select an appropriate feature vector $\phi ( s )$ . That is, we must decide, for example, whether we should use a first-order straight line or a second-order curve to fit the points. The selection of appropriate feature vectors is nontrivial. It requires prior knowledge of the given task: the better we understand the task, the better the feature vectors we can select. For instance, if we know that the points in Figure 8.2 are approximately located on a

straight line, we can use a straight line to fit the points. However, such prior knowledge is usually unknown in practice. If we do not have any prior knowledge, a popular solution is to use artificial neural networks as nonlinear function approximations.

Another important problem is how to find the optimal parameter vector. If we know $\{ v _ { \pi } ( s _ { i } ) \} _ { i = 1 } ^ { n }$ , this is a least-squares problem. The optimal parameter can be obtained by optimizing the following objective function:

$$
\begin{array}{l} J _ {1} = \sum_ {i = 1} ^ {n} \left(\hat {v} (s _ {i}, w) - v _ {\pi} (s _ {i})\right) ^ {2} = \sum_ {i = 1} ^ {n} \left(\phi^ {T} (s _ {i}) w - v _ {\pi} (s _ {i})\right) ^ {2} \\ = \left\| \left[ \begin{array}{c} \phi^ {T} (s _ {1}) \\ \vdots \\ \phi^ {T} (s _ {n}) \end{array} \right] w - \left[ \begin{array}{c} v _ {\pi} (s _ {1}) \\ \vdots \\ v _ {\pi} (s _ {n}) \end{array} \right] \right\| ^ {2} \doteq \| \Phi w - v _ {\pi} \| ^ {2}, \\ \end{array}
$$

where

$$
\Phi \doteq \left[ \begin{array}{c} \phi^ {T} (s _ {1}) \\ \vdots \\ \phi^ {T} (s _ {n}) \end{array} \right] \in \mathbb {R} ^ {n \times 2}, \qquad v _ {\pi} \doteq \left[ \begin{array}{c} v _ {\pi} (s _ {1}) \\ \vdots \\ v _ {\pi} (s _ {n}) \end{array} \right] \in \mathbb {R} ^ {n}.
$$

It can be verified that the optimal solution to this least-squares problem is

$$
w ^ {*} = \left(\Phi^ {T} \Phi\right) ^ {- 1} \Phi v _ {\pi}.
$$

More information about least-squares problems can be found in [47, Section 3.3] and [48, Section 5.14].

The curve-fitting example presented in this section illustrates the basic idea of value function approximation. This idea will be formally introduced in the next section.

# 8.2 TD learning of state values based on function approximation

In this section, we show how to integrate the function approximation method into TD learning to estimate the state values of a given policy. This algorithm will be extended to learn action values and optimal policies in Section 8.3.

This section contains quite a few subsections and many coherent contents. It is better for us to review the contents first before diving into the details.

The function approximation method is formulated as an optimization problem. The objective function of this problem is introduced in Section 8.2.1. The TD learning algorithm for optimizing this objective function is introduced in Section 8.2.2.

To apply the TD learning algorithm, we need to select appropriate feature vectors. Section 8.2.3 discusses this problem.   
Examples are given in Section 8.2.4 to demonstrate the TD algorithm and the impacts of different feature vectors.   
A theoretical analysis of the TD algorithm is given in Section 8.2.5. This subsection is mathematically intensive. Readers may read it selectively based on their interests.

# 8.2.1 Objective function

Let $v _ { \pi } ( s )$ and ${ \hat { v } } ( s , w )$ be the true state value and approximated state value of $s \in S$ , respectively. The problem to be solved is to find an optimal $w$ so that ${ \hat { v } } ( s , w )$ can best approximate $v _ { \pi } ( s )$ for every $s$ . In particular, the objective function is

$$
J (w) = \mathbb {E} \left[ \left(v _ {\pi} (S) - \hat {v} (S, w)\right) ^ {2} \right], \tag {8.3}
$$

where the expectation is calculated with respect to the random variable $S \in S$ . While $S$ is a random variable, what is its probability distribution? This question is important for understanding this objective function. There are several ways to define the probability distribution of $S$ .

The first way is to use a uniform distribution. That is to treat all the states as equally important by setting the probability of each state to $1 / n$ . In this case, the objective function in (8.3) becomes

$$
J (w) = \frac {1}{n} \sum_ {s \in \mathcal {S}} \left(v _ {\pi} (s) - \hat {v} (s, w)\right) ^ {2}, \tag {8.4}
$$

which is the average value of the approximation errors of all the states. However, this way does not consider the real dynamics of the Markov process under the given policy. Since some states may be rarely visited by a policy, it may be unreasonable to treat all the states as equally important.

The second way, which is the focus of this chapter, is to use the stationary distribution. The stationary distribution describes the long-term behavior of a Markov decision process. More specifically, after the agent executes a given policy for a sufficiently long period, the probability of the agent being located at any state can be described by this stationary distribution. Interested readers may see the details in Box 8.1.

Let $\{ d _ { \pi } ( s ) \} _ { s \in { \cal S } }$ denote the stationary distribution of the Markov process under policy $\pi$ . That is, the probability for the agent visiting $s$ after a long period of time is $d _ { \pi } ( s )$ . By definition, $\begin{array} { r } { \sum _ { s \in \mathcal { S } } d _ { \pi } ( s ) = 1 } \end{array}$ . Then, the objective function in (8.3) can be rewritten

as

$$
J (w) = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) (v _ {\pi} (s) - \hat {v} (s, w)) ^ {2}, \tag {8.5}
$$

which is a weighted average of the approximation errors. The states that have higher probabilities of being visited are given greater weights.

It is notable that the value of $d _ { \pi } ( s )$ is nontrivial to obtain because it requires knowing the state transition probability matrix $P _ { \pi }$ (see Box 8.1). Fortunately, we do not need to calculate the specific value of $d _ { \pi } ( s )$ to minimize this objective function as shown in the next subsection. In addition, it was assumed that the number of states was finite when we introduced (8.4) and (8.5). When the state space is continuous, we can replace the summations with integrals.

# Box 8.1: Stationary distribution of a Markov decision process

The key tool for analyzing stationary distribution is $P _ { \pi } \in \mathbb { R } ^ { n \times n }$ , which is the probability transition matrix under the given policy $\pi$ . If the states are indexed as $s _ { 1 } , \ldots , s _ { n }$ , then $\lfloor P _ { \pi } \rfloor _ { i j }$ is defined as the probability for the agent moving from $s _ { i }$ to $s _ { j }$ . The definition of $P _ { \pi }$ can be found in Section 2.6.

 Interpretation of $P _ { \pi } ^ { k }$ $\mathit { \Delta } ^ { ' } k = 1 , 2 , 3 , . . . . .$ ).

First of all, it is necessary to examine the interpretation of the entries in $P _ { \pi } ^ { k }$ . The probability of the agent transitioning from $s _ { i }$ to $s _ { j }$ using exactly $k$ steps is denoted as

$$
p _ {i j} ^ {(k)} = \operatorname * {P r} (S _ {t _ {k}} = j | S _ {t _ {0}} = i),
$$

where $t _ { 0 }$ and $t _ { k }$ are the initial and $k$ th time steps, respectively. First, by the definition of $P _ { \pi }$ , we have

$$
[ P _ {\pi} ] _ {i j} = p _ {i j} ^ {(1)},
$$

which means that $\lfloor P _ { \pi } \rfloor _ { i j }$ is the probability of transitioning from $s _ { i }$ to $s _ { j }$ using $a$ single step. Second, consider $P _ { \pi } ^ { 2 }$ . It can be verified that

$$
[ P _ {\pi} ^ {2} ] _ {i j} = [ P _ {\pi} P _ {\pi} ] _ {i j} = \sum_ {q = 1} ^ {n} [ P _ {\pi} ] _ {i q} [ P _ {\pi} ] _ {q j}.
$$

Since $[ P _ { \pi } ] _ { i q } [ P _ { \pi } ] _ { q j }$ is the joint probability of transitioning from $s _ { i }$ to $s _ { q }$ and then from $s _ { q }$ to $s _ { j }$ , we know that $[ P _ { \pi } ^ { 2 } ] _ { i j }$ is the probability of transitioning from $s _ { i }$ to $s _ { j }$

using exactly two steps. That is

$$
[ P _ {\pi} ^ {2} ] _ {i j} = p _ {i j} ^ {(2)}.
$$

Similarly, we know that

$$
[ P _ {\pi} ^ {k} ] _ {i j} = p _ {i j} ^ {(k)},
$$

which means that $[ P _ { \pi } ^ { k } ] _ { i j }$ is the probability of transitioning from $s _ { i }$ to $s _ { j }$ using exactly $k$ steps.

 Definition of stationary distributions.

Let $d _ { 0 } \in \mathbb { R } ^ { n }$ be a vector representing the probability distribution of the states at the initial time step. For example, if $s$ is always selected as the starting state, then $d _ { 0 } ( s ) = 1$ and the other entries of $d _ { 0 }$ are 0. Let $d _ { k } \in \mathbb { R } ^ { n }$ be the vector representing the probability distribution obtained after exactly $k$ steps starting from $d _ { 0 }$ . Then, we have

$$
d _ {k} (s _ {i}) = \sum_ {j = 1} ^ {n} d _ {0} (s _ {j}) [ P _ {\pi} ^ {k} ] _ {j i}, \quad i = 1, 2, \dots . \tag {8.6}
$$

This equation indicates that the probability of the agent visiting $s _ { i }$ at step $k$ equals the sum of the probabilities of the agent transitioning from $\{ s _ { j } \} _ { j = 1 } ^ { n }$ to $s _ { i }$ using exactly $k$ steps. The matrix-vector form of (8.6) is

$$
d _ {k} ^ {T} = d _ {0} ^ {T} P _ {\pi} ^ {k}. \tag {8.7}
$$

When we consider the long-term behavior of the Markov process, it holds under certain conditions that

$$
\lim  _ {k \rightarrow \infty} P _ {\pi} ^ {k} = \mathbf {1} _ {n} d _ {\pi} ^ {T}, \tag {8.8}
$$

where $\mathbf { 1 } _ { n } \ = \ [ 1 , \ldots , 1 ] ^ { T } \ \in \ \mathbb { R } ^ { n }$ and ${ \bf 1 } _ { n } d _ { \pi } ^ { I ^ { \prime } }$ is a constant matrix with all its rows equal to $d _ { \pi } ^ { I ^ { \prime } }$ . The conditions under which (8.8) is valid will be discussed later. Substituting (8.8) into (8.7) yields

$$
\lim  _ {k \rightarrow \infty} d _ {k} ^ {T} = d _ {0} ^ {T} \lim  _ {k \rightarrow \infty} P _ {\pi} ^ {k} = d _ {0} ^ {T} \mathbf {1} _ {n} d _ {\pi} ^ {T} = d _ {\pi} ^ {T}, \tag {8.9}
$$

where the last equality is valid because $d _ { 0 } ^ { I } \mathbf { 1 } _ { n } = 1$ .

Equation (8.9) means that the state distribution $d _ { k }$ converges to a constant value $d _ { \pi }$ , which is called the limiting distribution. The limiting distribution depends

on the system model and the policy $\pi$ . Interestingly, it is independent of the initial distribution $d _ { 0 }$ . That is, regardless of which state the agent starts from, the probability distribution of the agent after a sufficiently long period can always be described by the limiting distribution.

The value of $d _ { \pi }$ can be calculated in the following way. Taking the limit of both sides of $d _ { k } ^ { T } = d _ { k - 1 } ^ { T } P _ { \pi }$ gives $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } d _ { k } ^ { I ^ { \prime } } = \operatorname* { l i m } _ { k \to \infty } d _ { k - 1 } ^ { I ^ { \prime } } P _ { \pi } } \end{array}$ and hence

$$
d _ {\pi} ^ {T} = d _ {\pi} ^ {T} P _ {\pi}. \tag {8.10}
$$

As a result, $d _ { \pi }$ is the left eigenvector of $P _ { \pi }$ associated with the eigenvalue 1. The solution of (8.10) is called the stationary distribution. It holds that $\begin{array} { r } { \sum _ { s \in \mathcal { S } } d _ { \pi } ( s ) = } \end{array}$ 1 and $d _ { \pi } ( s ) > 0$ for all $s \in S$ . The reason why $d _ { \pi } ( s ) > 0$ (not $d _ { \pi } ( s ) \geq 0$ ) will be explained later.

Conditions for the uniqueness of stationary distributions.

The solution $d _ { \pi }$ of (8.10) is usually called a stationary distribution, whereas the distribution $d _ { \pi }$ in (8.9) is usually called the limiting distribution. Note that (8.9) implies (8.10), but the converse may not be true. A general class of Markov processes that have unique stationary (or limiting) distributions is irreducible (or regular ) Markov processes. Some necessary definitions are given below. More details can be found in [49, Chapter IV].

State $s _ { j }$ is said to be accessible from state $s _ { i }$ if there exists a finite integer $k$ so that $[ P _ { \pi } ] _ { i j } ^ { k } > 0$ , which means that the agent starting from $s _ { i }$ can possibly reach $s _ { j }$ after a finite number of transitions.   
If two states $s _ { i }$ and $s _ { j }$ are mutually accessible, then the two states are said to communicate.   
A Markov process is called irreducible if all of its states communicate with each other. In other words, the agent starting from an arbitrary state can possibly reach any other state within a finite number of steps. Mathematically, it indicates that, for any $s _ { i }$ and $s _ { j }$ , there exists $k \geq 1$ such that $[ { P _ { \pi } ^ { k } } ] _ { i j } > 0$ (the value of $k$ may vary for different $i , j$ ).   
- A Markov process is called regular if there exists $k \geq 1$ such that $[ { P _ { \pi } ^ { k } } ] _ { i j } > 0$ for all $i , j$ . Equivalently, there exists $k \geq 1$ such that $P _ { \pi } ^ { k } > 0$ , where $>$ i s elementwise. As a result, it is possible that every state is reachable from any other state within at most $k$ steps. A regular Markov process is also irreducible, but the converse is not true. However, if a Markov process is irreducible and there exists $i$ such that $[ { \cal P } _ { \pi } ] _ { i i } > 0$ , then it is also regular. Moreover, if $P _ { \pi } ^ { k } > 0$ , then $P _ { \pi } ^ { k ^ { \prime } } > 0$ for any $k ^ { \prime } \geq k$ since $P _ { \pi } \geq 0$ . It then follows from (8.9) that $d _ { \pi } ( s ) > 0$ for every $s$ .

 Policies that may lead to unique stationary distributions.

Once the policy is given, a Markov decision process becomes a Markov process, whose long-term behavior is jointly determined by the given policy and the system model. Then, an important question is what kind of policies can lead to regular Markov processes? In general, the answer is exploratory policies such as $\epsilon$ -greedy policies. That is because an exploratory policy has a positive probability of taking any action at any state. As a result, the states can communicate with each other when the system model allows them to do so.

> An example is given in Figure 8.5 to illustrate stationary distributions. The policy in this example is $\epsilon$ -greedy with $\epsilon = 0 . 5$ . The states are indexed as $s _ { 1 } , s _ { 2 } , s _ { 3 } , s _ { 4 }$ , which correspond to the top-left, top-right, bottom-left, and bottom-right cells in the grid, respectively.

We compare two methods to calculate the stationary distributions. The first method is to solve (8.10) to get the theoretical value of $d _ { \pi }$ . The second method is to estimate $d _ { \pi }$ numerically: we start from an arbitrary initial state and generate a sufficiently long episode by following the given policy. Then, $d _ { \pi }$ can be estimated by the ratio between the number of times each state is visited in the episode and the total length of the episode. The estimation result is more accurate when the episode is longer. We next compare the theoretical and estimated results.

![](images/86fdef829b51983ebadd174a7f5a74fd97cb19f56cd96089a4392c1e61ca65db.jpg)

![](images/9a3f8bb34c03593ab0b02f91a34bf46a97ecb7ff03b3da2b5aa869326dc67491.jpg)  
Figure 8.5: Long-term behavior of an $\epsilon$ -greedy policy with $\epsilon = 0 . 5$ . The asterisks in the right figure represent the theoretical values of the elements of $d _ { \pi }$ .

Theoretical value of $d _ { \pi }$ : It can be verified that the Markov process induced by the policy is both irreducible and regular. That is due to the following reasons. First, since all the states communicate, the resulting Markov process is irreducible. Second, since every state can transition to itself, the resulting

Markov process is regular. It can be seen from Figure 8.5 that

$$
P _ {\pi} ^ {T} = \left[ \begin{array}{c c c c} 0. 3 & 0. 1 & 0. 1 & 0 \\ 0. 1 & 0. 3 & 0 & 0. 1 \\ 0. 6 & 0 & 0. 3 & 0. 1 \\ 0 & 0. 6 & 0. 6 & 0. 8 \end{array} \right].
$$

The eigenvalues of $P _ { \pi } ^ { I }$ can be calculated as $\{ - 0 . 0 4 4 9 , 0 . 3 , 0 . 4 4 4 9 , 1 \}$ . The unit-length (right) eigenvector of $P _ { \pi } ^ { I }$ corresponding to the eigenvalue 1 is $[ 0 . 0 4 6 3 , 0 . 1 4 5 5 , 0 . 1 7 8 5 , 0 . 9 7 2 0 ] ^ { T }$ . After scaling this vector so that the sum of all its elements is equal to 1, we obtain the theoretical value of $d _ { \pi }$ as follows:

$$
d _ {\pi} = \left[ \begin{array}{l} 0. 0 3 4 5 \\ 0. 1 0 8 4 \\ 0. 1 3 3 0 \\ 0. 7 2 4 1 \end{array} \right].
$$

The $i$ th element of $d _ { \pi }$ corresponds to the probability of the agent visiting $s _ { i }$ in the long run.

Estimated value of $d _ { \pi }$ : We next verify the above theoretical value of $d _ { \pi }$ by executing the policy for sufficiently many steps in the simulation. Specifically, we select $s _ { 1 }$ as the starting state and run 1,000 steps by following the policy. The proportion of the visits of each state during the process is shown in Figure 8.5. It can be seen that the proportions converge to the theoretical value of $d _ { \pi }$ after hundreds of steps.

# 8.2.2 Optimization algorithms

To minimize the objective function $J ( w )$ in (8.3), we can use the gradient descent algorithm:

$$
w _ {k + 1} = w _ {k} - \alpha_ {k} \nabla_ {w} J (w _ {k}),
$$

where

$$
\begin{array}{l} \nabla_ {w} J (w _ {k}) = \nabla_ {w} \mathbb {E} \left[ \left(v _ {\pi} (S) - \hat {v} (S, w _ {k})\right) ^ {2} \right] \\ = \mathbb {E} \left[ \nabla_ {w} \left(v _ {\pi} (S) - \hat {v} (S, w _ {k})\right) ^ {2} \right] \\ = 2 \mathbb {E} \left[ \left(v _ {\pi} (S) - \hat {v} (S, w _ {k})\right) \left(- \nabla_ {w} \hat {v} (S, w _ {k})\right) \right] \\ = - 2 \mathbb {E} \left[ \left(v _ {\pi} (S) - \hat {v} (S, w _ {k})\right) \nabla_ {w} \hat {v} (S, w _ {k}) \right]. \\ \end{array}
$$

Therefore, the gradient descent algorithm is

$$
w _ {k + 1} = w _ {k} + 2 \alpha_ {k} \mathbb {E} [ (v _ {\pi} (S) - \hat {v} (S, w _ {k})) \nabla_ {w} \hat {v} (S, w _ {k}) ], \tag {8.11}
$$

where the coefficient 2 before $\alpha _ { k }$ can be merged into $\alpha _ { k }$ without loss of generality. The algorithm in (8.11) requires calculating the expectation. In the spirit of stochastic gradient descent, we can replace the true gradient with a stochastic gradient. Then, (8.11) becomes

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \left(v _ {\pi} (s _ {t}) - \hat {v} \left(s _ {t}, w _ {t}\right)\right) \nabla_ {w} \hat {v} \left(s _ {t}, w _ {t}\right), \tag {8.12}
$$

where $s _ { t }$ is a sample of $S$ at time $t$ .

Notably, (8.12) is not implementable because it requires the true state value $v _ { \pi }$ , which is unknown and must be estimated. We can replace $v _ { \pi } ( s _ { t } )$ with an approximation to make the algorithm implementable. The following two methods can be used to do so.

 Monte Carlo method: Suppose that we have an episode $( s _ { 0 } , r _ { 1 } , s _ { 1 } , r _ { 2 } , \dots )$ . Let $g _ { t }$ be the discounted return starting from $s _ { t }$ . Then, $g _ { t }$ can be used as an approximation of $v _ { \pi } ( s _ { t } )$ . The algorithm in (8.12) becomes

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \big (g _ {t} - \hat {v} (s _ {t}, w _ {t}) \big) \nabla_ {w} \hat {v} (s _ {t}, w _ {t}).
$$

This is the algorithm of Monte Carlo learning with function approximation.

 Temporal-difference method: In the spirit of TD learning, $r _ { t + 1 } + \gamma \hat { v } ( s _ { t + 1 } , w _ { t } )$ can be used as an approximation of $v _ { \pi } ( s _ { t } )$ . The algorithm in (8.12) becomes

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \left[ r _ {t + 1} + \gamma \hat {v} \left(s _ {t + 1}, w _ {t}\right) - \hat {v} \left(s _ {t}, w _ {t}\right) \right] \nabla_ {w} \hat {v} \left(s _ {t}, w _ {t}\right). \tag {8.13}
$$

This is the algorithm of TD learning with function approximation. This algorithm is summarized in Algorithm 8.1.

Understanding the TD algorithm in (8.13) is important for studying the other algorithms in this chapter. Notably, (8.13) can only learn the state values of a given policy. It will be extended to algorithms that can learn action values in Sections 8.3.1 and 8.3.2.

# 8.2.3 Selection of function approximators

To apply the TD algorithm in (8.13), we need to select appropriate ${ \hat { v } } ( s , w )$ . There are two ways to do that. The first is to use an artificial neural network as a nonlinear function approximator. The input of the neural network is the state, the output is ${ \hat { v } } ( s , w )$ , and the network parameter is $w$ . The second is to simply use a linear function:

$$
\hat {v} (s, w) = \phi^ {T} (s) w,
$$

# Algorithm 8.1: TD learning of state values with function approximation

Initialization: A function ${ \hat { v } } ( s , w )$ that is differentiable in $w$ . Initial parameter $w _ { 0 }$ . Goal: Learn the true state values of a given policy $\pi$ .

For each episode $\{ ( s _ { t } , r _ { t + 1 } , s _ { t + 1 } ) \} _ { t }$ generated by $\pi$ , do For each sample $( s _ { t } , r _ { t + 1 } , s _ { t + 1 } )$ , do In the general case, wt+1 = wt+αt [rt+1 + γvˆ(st+1, wt) − vˆ(st, wt)] ∇wvˆ(st, wt) In the linear case, $w _ { t + 1 } = w _ { t } + \alpha _ { t } \big [ r _ { t + 1 } + \gamma \phi ^ { T } ( s _ { t + 1 } ) w _ { t } - \phi ^ { T } ( s _ { t } ) w _ { t } \big ] \phi ( s _ { t } )$

where $\phi ( s ) \in \mathbb { R } ^ { m }$ is the feature vector of $s$ . The lengths of $\phi ( s )$ and $w$ are equal to $m$ , which is usually much smaller than the number of states. In the linear case, the gradient is

$$
\nabla_ {w} \hat {v} (s, w) = \phi (s),
$$

Substituting which into (8.13) yields

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \left[ r _ {t + 1} + \gamma \phi^ {T} \left(s _ {t + 1}\right) w _ {t} - \phi^ {T} \left(s _ {t}\right) w _ {t} \right] \phi \left(s _ {t}\right). \tag {8.14}
$$

This is the algorithm of TD learning with linear function approximation. We call it TD-Linear for short.

The linear case is much better understood in theory than the nonlinear case. However, its approximation ability is limited. It is also nontrivial to select appropriate feature vectors for complex tasks. By contrast, artificial neural networks can approximate values as black-box universal nonlinear approximators, which are more friendly to use.

Nevertheless, it is still meaningful to study the linear case. A better understanding of the linear case can help readers better grasp the idea of the function approximation method. Moreover, the linear case is sufficient for solving the simple grid world tasks considered in this book. More importantly, the linear case is still powerful in the sense that the tabular method can be viewed as a special linear case. More information can be found in Box 8.2.

# Box 8.2: Tabular TD learning is a special case of TD-Linear

We next show that the tabular TD algorithm in (7.1) in Chapter 7 is a special case of the TD-Linear algorithm in (8.14).

Consider the following special feature vector for any $s \in S$ :

$$
\phi (s) = e _ {s} \in \mathbb {R} ^ {n},
$$

where $e _ { s }$ is the vector with the entry corresponding to $s$ equal to 1 and the other

entries equal to 0. In this case,

$$
\hat {v} (s, w) = e _ {s} ^ {T} w = w (s),
$$

where $w ( s )$ is the entry in $w$ that corresponds to $s$ . Substituting the above equation into (8.14) yields

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \big (r _ {t + 1} + \gamma w _ {t} (s _ {t + 1}) - w _ {t} (s _ {t}) \big) e _ {s _ {t}}.
$$

The above equation merely updates the entry $w _ { t } ( s _ { t } )$ due to the definition of $e _ { s _ { t } }$ Motivated by this, multiplying $e _ { s _ { t } } ^ { T }$ on both sides of the equation yields

$$
w _ {t + 1} (s _ {t}) = w _ {t} (s _ {t}) + \alpha_ {t} \big (r _ {t + 1} + \gamma w _ {t} (s _ {t + 1}) - w _ {t} (s _ {t}) \big),
$$

which is exactly the tabular TD algorithm in (7.1).

In summary, by selecting the feature vector as $\phi ( s ) = e _ { s }$ , the TD-Linear algorithm becomes the tabular TD algorithm.

# 8.2.4 Illustrative examples

We next present some examples for demonstrating how to use the TD-Linear algorithm in (8.14) to estimate the state values of a given policy. In the meantime, we demonstrate how to select feature vectors.

![](images/7018ec2d78269b3299613d8f31b2599f09fdf893fb6bc93a4f2274db6113be16.jpg)

![](images/a93b73f4a1454c05d97a4bed2e8a99c73312cd7a832a36aed7a0ee091c90d9b2.jpg)  
(b)

![](images/f2c84771bf7ce3593c0a9b7bb504877748a5e49c1f7e223a2e7fe9a1566204b1.jpg)  
(c)   
Figure 8.6: (a) The policy to be evaluated. (b) The true state values are represented as a table. (c) The true state values are represented as a 3D surface.

The grid world example is shown in Figure 8.6. The given policy takes any action at a state with a probability of 0.2. Our goal is to estimate the state values under this policy. There are 25 state values in total. The true state values are shown in Figure 8.6(b). The true state values are visualized as a three-dimensional surface in Figure 8.6(c).

We next show that we can use fewer than 25 parameters to approximate these state values. The simulation setup is as follows. Five hundred episodes are generated by the given policy. Each episode has 500 steps and starts from a randomly selected state-action pair following a uniform distribution. In addition, in each simulation trial, the parameter vector $w$ is randomly initialized such that each element is drawn from a standard normal distribution with a zero mean and a standard deviation of 1. We set $r _ { \mathrm { f o r b i d d e n } } = r _ { \mathrm { b o u n d a r y } } =$ $- 1$ , $r _ { \mathrm { t a r g e t } } = 1$ , and $\gamma = 0 . 9$ .

To implement the TD-Linear algorithm, we need to select the feature vector $\phi ( s )$ first. There are different ways to do that as shown below.

 The first type of feature vector is based on polynomials. In the grid world example, a state $s$ corresponds to a 2D location. Let $x$ and $y$ denote the column and row indexes of $s$ , respectively. To avoid numerical issues, we normalize $x$ and $y$ so that their values are within the interval of $[ - 1 , + 1 ]$ . With a slight abuse of notation, the normalized values are also represented by $x$ and $y$ . Then, the simplest feature vector is

$$
\phi (s) = \left[ \begin{array}{l} x \\ y \end{array} \right] \in \mathbb {R} ^ {2}.
$$

In this case, we have

$$
\hat {v} (s, w) = \phi^ {T} (s) w = [ x, y ] \left[ \begin{array}{l} w _ {1} \\ w _ {2} \end{array} \right] = w _ {1} x + w _ {2} y.
$$

When $w$ is given, $\hat { v } ( s , w ) = w _ { 1 } x + w _ { 2 } y$ represents a 2D plane that passes through the origin. Since the surface of the state values may not pass through the origin, we need to introduce a bias to the 2D plane to better approximate the state values. To do that, we consider the following 3D feature vector:

$$
\phi (s) = \left[ \begin{array}{l} 1 \\ x \\ y \end{array} \right] \in \mathbb {R} ^ {3}. \tag {8.15}
$$

In this case, the approximated state value is

$$
\hat {v} (s, w) = \phi^ {T} (s) w = [ 1, x, y ] \left[ \begin{array}{l} w _ {1} \\ w _ {2} \\ w _ {3} \end{array} \right] = w _ {1} + w _ {2} x + w _ {3} y.
$$

When $w$ is given, ${ \hat { v } } ( s , w )$ corresponds to a plane that may not pass through the origin. Notably, $\phi ( s )$ can also be defined as $\phi ( s ) = [ x , y , 1 ] ^ { \scriptscriptstyle T }$ , where the order of the elements does not matter.

The estimation result when we use the feature vector in (8.15) is shown in Fig-

ure 8.7(a). It can be seen that the estimated state values form a 2D plane. Although the estimation error converges as more episodes are used, the error cannot decrease to zero due to the limited approximation ability of a 2D plane.

To enhance the approximation ability, we can increase the dimension of the feature vector. To that end, consider

$$
\phi (s) = [ 1, x, y, x ^ {2}, y ^ {2}, x y ] ^ {T} \in \mathbb {R} ^ {6}. \tag {8.16}
$$

In this case, $\hat { v } ( s , w ) = \phi ^ { T } ( s ) w = w _ { 1 } + w _ { 2 } x + w _ { 3 } y + w _ { 4 } x ^ { 2 } + w _ { 5 } y ^ { 2 } + w _ { 6 } x y$ , which corresponds to a quadratic 3D surface. We can further increase the dimension of the feature vector:

$$
\phi (s) = \left[ 1, x, y, x ^ {2}, y ^ {2}, x y, x ^ {3}, y ^ {3}, x ^ {2} y, x y ^ {2} \right] ^ {T} \in \mathbb {R} ^ {1 0}. \tag {8.17}
$$

The estimation results when we use the feature vectors in (8.16) and (8.17) are shown in Figures 8.7(b)-(c). As can be seen, the longer the feature vector is, the more accurately the state values can be approximated. However, in all three cases, the estimation error cannot converge to zero because these linear approximators still have limited approximation abilities.

![](images/acb2d2672dc51341fe37040fe25efb0c252d2fa4cb3702dda47ffaf9c9edc97a.jpg)

![](images/2f668d4cf81a721b8ba857ef0de789e720c9289736c5c4bc386a7949b21def19.jpg)

![](images/213296dd587aeeb3a944f2b2654712a764d075ec41b3ddd08c592fa44f7196e0.jpg)

![](images/7f1c2c4b4835c62f364594eed803c3e300fe64f4a91a4cf0ffafe616f6aed91e.jpg)  
(a) φ(s)  R3

![](images/cf4cd91e58e50b2c717bda5ee51f352428808f993ad884f77dfa5d040b06f5a2.jpg)  
$( \mathrm { b } ) \ \phi ( s ) \in \mathbb { R } ^ { 6 }$

![](images/63ee8e1fe27ae513419805e3ab6349e6633ff73571e3ea447c386a68e06d3e01.jpg)  
(c) φ(s) R10   
Figure 8.7: TD-Linear estimation results obtained with the polynomial features in (8.15), (8.16), and (8.17).

In addition to polynomial feature vectors, many other types of features are available such as Fourier basis and tile coding [3, Chapter 9]. First, the values of $x$ and $y$ of

![](images/1d479d6d708ec03726409bfab57b8b51bf3cd82a6e6eae4cdc6969f16aabe694.jpg)

![](images/2da1076b55954572be60cd8bb09aec7269c517b17f8f23fd8c3bbce6facb313e.jpg)

![](images/201687b7fd7f4dca7c23027a5b3a2849bf55d68a2aa32c347688c94e27003cf3.jpg)

![](images/0edf1d2b438c50b12907d426b9ecb8b1a7640dc707290ace9282039e918ed918.jpg)  
(a) $q = 1$ and $\phi ( s ) \in \mathbb { R } ^ { 4 }$

![](images/b03ac3a8a6b93b0361b4795a50adf5ab93b8eb4eda255ac7b3b7f330fcad5a5a.jpg)  
(b) q = 2 and $\phi ( s ) \in \mathbb { R } ^ { 9 }$

![](images/9f7adb78337211d0d825afff5188b14a502eec105fca1583ec264f15af750c2e.jpg)  
(c) $q = 3$ and $\phi ( s ) \in \mathbb { R } ^ { 1 6 }$   
Figure 8.8: TD-Linear estimation results obtained with the Fourier features in (8.18).

each state are normalized to the interval of $[ 0 , 1 ]$ . The resulting feature vector is

$$
\phi (s) = \left[ \begin{array}{c} \vdots \\ \cos (\pi (c _ {1} x + c _ {2} y)) \\ \vdots \end{array} \right] \in \mathbb {R} ^ {(q + 1) ^ {2}}, \tag {8.18}
$$

where $\pi$ denotes the circumference ratio, which is $3 . 1 4 1 5 \ldots$ , instead of a policy. Here, $c _ { 1 }$ or $c _ { 2 }$ can be set as any integers in $\{ 0 , 1 , \ldots , q \}$ , where $q$ is a user-specified integer. As a result, there are $( q + 1 ) ^ { 2 }$ possible values for the pair $\left( c _ { 1 } , c _ { 2 } \right)$ to take. Hence, the dimension of $\phi ( s )$ is $( q + 1 ) ^ { 2 }$ . For example, in the case of $q = 1$ , the feature vector is

$$
\phi (s) = \left[ \begin{array}{c} \cos (\pi (0 x + 0 y)) \\ \cos (\pi (0 x + 1 y)) \\ \cos (\pi (1 x + 0 y)) \\ \cos (\pi (1 x + 1 y)) \end{array} \right] = \left[ \begin{array}{c} 1 \\ \cos (\pi y) \\ \cos (\pi x) \\ \cos (\pi (x + y)) \end{array} \right] \in \mathbb {R} ^ {4}.
$$

The estimation results obtained when we use the Fourier features with $q = 1 , 2 , 3$ are shown in Figure 8.8. The dimensions of the feature vectors in the three cases are $4 , 9 , 1 6$ , respectively. As can be seen, the higher the dimension of the feature vector is, the more accurately the state values can be approximated.

# 8.2.5 Theoretical analysis

Thus far, we have finished describing the story of TD learning with function approximation. This story started from the objective function in (8.3). To optimize this objective

function, we introduced the stochastic algorithm in (8.12). Later, the true value function in the algorithm, which was unknown, was replaced by an approximation, leading to the TD algorithm in (8.13). Although this story is helpful for understanding the basic idea of value function approximation, it is not mathematically rigorous. For example, the algorithm in (8.13) actually does not minimize the objective function in (8.3).

We next present a theoretical analysis of the TD algorithm in (8.13) to reveal why the algorithm works effectively and what mathematical problems it solves. Since general nonlinear approximators are difficult to analyze, this part only considers the linear case. Readers are advised to read selectively based on their interests since this part is mathematically intensive.

# Convergence analysis

To study the convergence property of (8.13), we first consider the following deterministic algorithm:

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \mathbb {E} \left[ \left(r _ {t + 1} + \gamma \phi^ {T} \left(s _ {t + 1}\right) w _ {t} - \phi^ {T} \left(s _ {t}\right) w _ {t}\right) \phi \left(s _ {t}\right) \right], \tag {8.19}
$$

where the expectation is calculated with respect to the random variables $s _ { t } , s _ { t + 1 } , r _ { t + 1 }$ . The distribution of $s _ { t }$ is assumed to be the stationary distribution $d _ { \pi }$ . The algorithm in (8.19) is deterministic because the random variables $s _ { t } , s _ { t + 1 } , r _ { t + 1 }$ all disappear after calculating the expectation.

Why would we consider this deterministic algorithm? First, the convergence of this deterministic algorithm is easier (though nontrivial) to analyze. Second and more importantly, the convergence of this deterministic algorithm implies the convergence of the stochastic TD algorithm in (8.13). That is because (8.13) can be viewed as a stochastic gradient descent (SGD) implementation of (8.19). Therefore, we only need to study the convergence property of the deterministic algorithm.

Although the expression of (8.19) may look complex at first glance, it can be greatly simplified. To do that, define

$$
\Phi = \left[ \begin{array}{c} \vdots \\ \phi^ {T} (s) \\ \vdots \end{array} \right] \in \mathbb {R} ^ {n \times m}, \quad D = \left[ \begin{array}{c c c} \ddots & & \\ & d _ {\pi} (s) & \\ & & \ddots \end{array} \right] \in \mathbb {R} ^ {n \times n}, \tag {8.20}
$$

where $\Phi$ is the matrix containing all the feature vectors, and $\boldsymbol { D }$ is a diagonal matrix with the stationary distribution in its diagonal entries. The two matrices will be frequently used.

Lemma 8.1. The expectation in (8.19) can be rewritten as

$$
\mathbb {E} \Big [ \big (r _ {t + 1} + \gamma \phi^ {T} (s _ {t + 1}) w _ {t} - \phi^ {T} (s _ {t}) w _ {t} \big) \phi (s _ {t}) \Big ] = b - A w _ {t},
$$

where

$$
A \doteq \Phi^ {T} D (I - \gamma P _ {\pi}) \Phi \in \mathbb {R} ^ {m \times m},
$$

$$
b \doteq \Phi^ {T} D r _ {\pi} \in \mathbb {R} ^ {m}. \tag {8.21}
$$

Here, $P _ { \pi } , r _ { \pi }$ are the two terms in the Bellman equation $\boldsymbol { v } _ { \pi } = \boldsymbol { r } _ { \pi } + \gamma \boldsymbol { P } _ { \pi } \boldsymbol { v } _ { \pi }$ , and $I$ is the identity matrix with appropriate dimensions.

The proof is given in Box 8.3. With the expression in Lemma 8.1, the deterministic algorithm in (8.19) can be rewritten as

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} (b - A w _ {t}), \tag {8.22}
$$

which is a simple deterministic process. Its convergence is analyzed below.

First, what is the converged value of $w _ { t }$ ? Hypothetically, if $w _ { t }$ converges to a constant value $w ^ { * }$ as $t \to \infty$ , then (8.22) implies $w ^ { * } = w ^ { * } + \alpha _ { \infty } ( b - A w ^ { * } )$ , which suggests that $b - A w ^ { * } = 0$ and hence

$$
w ^ {*} = A ^ {- 1} b.
$$

Several remarks about this converged value are given below.

 Is $A$ invertible? The answer is yes. In fact, $A$ is not only invertible but also positive definite. That is, for any nonzero vector $x$ with appropriate dimensions, $x ^ { T } A x > 0$ . The proof is given in Box 8.4.   
What is the interpretation of $w ^ { * } = A ^ { - 1 } b ^ { \epsilon }$ ? It is actually the optimal solution for minimizing the projected Bellman error. The details will be introduced in Section 8.2.5.   
The tabular method is a special case. One interesting result is that, when the dimensionality of $w$ equals $n = | S |$ and $\phi ( s ) = [ 0 , \ldots , 1 , \ldots , 0 ] ^ { \scriptscriptstyle T }$ , where the entry corresponding to $s$ is 1, we have

$$
w ^ {*} = A ^ {- 1} b = v _ {\pi}. \tag {8.23}
$$

This equation indicates that the parameter vector to be learned is actually the true state value. This conclusion is consistent with the fact that the tabular TD algorithm is a special case of the TD-Linear algorithm, as introduced in Box 8.2. The proof of (8.23) is given below. It can be verified that $\Phi = I$ in this case and hence $A =$ $\Phi ^ { I } D ( I - \gamma P _ { \pi } ) \Phi \ = \ D ( I - \gamma P _ { \pi } )$ and $b = \Phi ^ { T } D r _ { \pi } = D r _ { \pi }$ . Thus, $w ^ { * } = A ^ { - 1 } b =$ $( I - \gamma P _ { \pi } ) ^ { - 1 } D ^ { - 1 } D r _ { \pi } = ( I - \gamma P _ { \pi } ) ^ { - 1 } r _ { \pi } = v _ { \pi }$ .

Second, we prove that $w _ { t }$ in (8.22) converges to $w ^ { * } = A ^ { - 1 } b$ as $t \to \infty$ . Since (8.22) is a simple deterministic process, it can be proven in many ways. We present two proofs as follows.

 Proof 1: Define the convergence error as $\delta _ { t } \doteq w _ { t } - w ^ { * }$ . We only need to show that $\delta _ { t }$ converges to zero. To do that, substituting $\boldsymbol { w } _ { t } = \boldsymbol { \delta } _ { t } + \boldsymbol { w } ^ { * }$ into (8.22) gives

$$
\delta_ {t + 1} = \delta_ {t} - \alpha_ {t} A \delta_ {t} = (I - \alpha_ {t} A) \delta_ {t}.
$$

It then follows that

$$
\delta_ {t + 1} = (I - \alpha_ {t} A) \dots (I - \alpha_ {0} A) \delta_ {0}.
$$

Consider the simple case where $\alpha _ { t } = \alpha$ for all $t$ . Then, we have

$$
\| \delta_ {t + 1} \| _ {2} \leq \| I - \alpha A \| _ {2} ^ {t + 1} \| \delta_ {0} \| _ {2}.
$$

When $\alpha > 0$ is sufficiently small, we have that $\| I - \alpha A \| _ { 2 } < 1$ and hence $\delta _ { t } \to 0$ as $t \to \infty$ . The reason why $\| I - \alpha A \| _ { 2 } < 1$ holds is that $A$ is positive definite and hence $x ^ { T } ( I - \alpha A ) x < 1$ for any $x$ .

Proof 2: Consider $g ( w ) \doteq b - A w$ . Since $w ^ { * }$ is the root of $g ( w ) = 0$ , the task is actually a root-finding problem. The algorithm in (8.22) is actually a Robbins-Monro (RM) algorithm. Although the original RM algorithm was designed for stochastic processes, it can also be applied to deterministic cases. The convergence of RM algorithms can shed light on the convergence of $w _ { t + 1 } = w _ { t } + \alpha _ { t } ( b - A w _ { t } )$ . That is, $w _ { t }$ converges to $w ^ { * }$ when $\textstyle \sum _ { t } \alpha _ { t } = \infty$ and $\textstyle \sum _ { t } \alpha _ { t } ^ { 2 } < \infty$ .

# Box 8.3: Proof of Lemma 8.1

By using the law of total expectation, we have

$$
\begin{array}{l} \mathbb {E} \left[ r _ {t + 1} \phi (s _ {t}) + \phi (s _ {t}) \left(\gamma \phi^ {T} (s _ {t + 1}) - \phi^ {T} (s _ {t})\right) w _ {t} \right] \\ = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \mathbb {E} \left[ r _ {t + 1} \phi \left(s _ {t}\right) + \phi \left(s _ {t}\right) \left(\gamma \phi^ {T} \left(s _ {t + 1}\right) - \phi^ {T} \left(s _ {t}\right)\right) w _ {t} \mid s _ {t} = s \right] \\ = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \mathbb {E} \left[ r _ {t + 1} \phi \left(s _ {t}\right) \mid s _ {t} = s \right] + \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \mathbb {E} \left[ \phi \left(s _ {t}\right) \left(\gamma \phi^ {T} \left(s _ {t + 1}\right) - \phi^ {T} \left(s _ {t}\right)\right) w _ {t} \mid s _ {t} = s \right]. \tag {8.24} \\ \end{array}
$$

Here, $s _ { t }$ is assumed to obey the stationary distribution $d _ { \pi }$

First, consider the first term in (8.24). Note that

$$
\mathbb {E} \Big [ r _ {t + 1} \phi (s _ {t}) | s _ {t} = s \Big ] = \phi (s) \mathbb {E} \Big [ r _ {t + 1} | s _ {t} = s \Big ] = \phi (s) r _ {\pi} (s),
$$

where $\begin{array} { r } { r _ { \pi } ( s ) = \sum _ { a } \pi ( a \vert s ) \sum _ { r } r p ( r \vert s , a ) } \end{array}$ . Then, the first term in (8.24) can be rewritten

$$
\sum_ {s \in \mathcal {S}} d _ {\pi} (s) \mathbb {E} \left[ r _ {t + 1} \phi \left(s _ {t}\right) \mid s _ {t} = s \right] = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \phi (s) r _ {\pi} (s) = \Phi^ {T} D r _ {\pi}, \tag {8.25}
$$

where $r _ { \pi } = [ \cdot \cdot \cdot , r _ { \pi } ( s ) , \cdot \cdot \cdot ] ^ { \scriptscriptstyle T } \in \mathbb { R } ^ { n }$ .

Second, consider the second term in (8.24). Since

$$
\begin{array}{l} \mathbb {E} \left[ \phi (s _ {t}) \left(\gamma \phi^ {T} (s _ {t + 1}) - \phi^ {T} (s _ {t})\right) w _ {t} \mid s _ {t} = s \right] \\ = - \mathbb {E} \left[ \phi (s _ {t}) \phi^ {T} (s _ {t}) w _ {t} \mid s _ {t} = s \right] + \mathbb {E} \left[ \gamma \phi (s _ {t}) \phi^ {T} (s _ {t + 1}) w _ {t} \mid s _ {t} = s \right] \\ = - \phi (s) \phi^ {T} (s) w _ {t} + \gamma \phi (s) \mathbb {E} \left[ \phi^ {T} \left(s _ {t + 1}\right) \mid s _ {t} = s \right] w _ {t} \\ = - \phi (s) \phi^ {T} (s) w _ {t} + \gamma \phi (s) \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s) \phi^ {T} (s ^ {\prime}) w _ {t}, \\ \end{array}
$$

the second term in (8.24) becomes

$$
\begin{array}{l} \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \mathbb {E} \left[ \phi (s _ {t}) \left(\gamma \phi^ {T} (s _ {t + 1}) - \phi^ {T} (s _ {t})\right) w _ {t} \mid s _ {t} = s \right] \\ = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \left[ - \phi (s) \phi^ {T} (s) w _ {t} + \gamma \phi (s) \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s) \phi^ {T} (s ^ {\prime}) w _ {t} \right] \\ = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \phi (s) \left[ - \phi (s) + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} | s\right) \phi \left(s ^ {\prime}\right) \right] ^ {T} w _ {t} \\ = \Phi^ {T} D (- \Phi + \gamma P _ {\pi} \Phi) w _ {t} \\ = - \Phi^ {T} D (I - \gamma P _ {\pi}) \Phi w _ {t}. \tag {8.26} \\ \end{array}
$$

Combining (8.25) and (8.26) gives

$$
\begin{array}{l} \mathbb {E} \left[ \left(r _ {t + 1} + \gamma \phi^ {T} (s _ {t + 1}) w _ {t} - \phi^ {T} (s _ {t}) w _ {t}\right) \phi (s _ {t}) \right] = \Phi^ {T} D r _ {\pi} - \Phi^ {T} D (I - \gamma P _ {\pi}) \Phi w _ {t} \\ \dot {=} b - A w _ {t}, \tag {8.27} \\ \end{array}
$$

where $b \doteq \Phi ^ { T } D r _ { \pi }$ and $A \doteq \Phi ^ { T } D ( I - \gamma P _ { \pi } ) \Phi$

# Box 8.4: Proving that $A = \Phi ^ { T } D ( I - \gamma P _ { \pi } ) \Phi$ is invertible and positive definite.

The matrix $A$ is positive definite if $x ^ { T } A x > 0$ for any nonzero vector $x$ with appropriate dimensions. If $A$ is positive (or negative) definite, it is denoted as $A \succ 0$ (or $A \prec 0$ ). Here, $\succ$ and $\prec$ should be differentiated from $>$ and $<$ <, which indicate elementwise comparisons. Note that $A$ may not be symmetric. Although positive

definite matrices often refer to symmetric matrices, nonsymmetric ones can also be positive definite.

We next prove that $A \succ 0$ and hence $A$ is invertible. The idea for proving $A \succ 0$ is to show that

$$
D (I - \gamma P _ {\pi}) \doteq M \succ 0. \tag {8.28}
$$

It is clear that $M \succ 0$ implies $A = \Phi ^ { T } M \Phi \succ 0$ since $\Phi$ is a tall matrix with full column rank (suppose that the feature vectors are selected to be linearly independent). Note that

$$
M = \frac {M + M ^ {T}}{2} + \frac {M - M ^ {T}}{2}.
$$

Since $M - M ^ { T }$ is skew-symmetric and hence $x ^ { T } ( M - M ^ { T } ) x = 0$ for any $x$ , we know that $M \succ 0$ if and only if $M + M ^ { T } \succ 0$ . To show $M + M ^ { T } \succ 0$ , we apply the fact that strictly diagonal dominant matrices are positive definite [4].

First, it holds that

$$
(M + M ^ {T}) \mathbf {1} _ {n} > 0, \tag {8.29}
$$

where $\mathbf { 1 } _ { n } = [ 1 , \ldots , 1 ] ^ { T } \in \mathbb { R } ^ { n }$ . The proof of (8.29) is given below. Since $P _ { \pi } \mathbf { 1 } _ { n } = \mathbf { 1 } _ { n }$ , we have $M { \bf 1 } _ { n } = D ( I - \gamma P _ { \pi } ) { \bf 1 } _ { n } = D ( { \bf 1 } _ { n } - \gamma { \bf 1 } _ { n } ) = ( 1 - \gamma ) d _ { \pi }$ . Moreover, $M ^ { T } \mathbf { 1 } _ { n } =$ $( I - \gamma P _ { \pi } ^ { T } ) D { \bf 1 } _ { n } = ( I - \gamma P _ { \pi } ^ { T } ) d _ { \pi } = ( 1 - \gamma ) d _ { \pi }$ , where the last equality is valid because $P _ { \pi } ^ { I ^ { \prime } } d _ { \pi } = d _ { \pi }$ . In summary, we have

$$
(M + M ^ {T}) \mathbf {1} _ {n} = 2 (1 - \gamma) d _ {\pi}.
$$

Since all the entries of $d _ { \pi }$ are positive (see Box 8.1), we have $( M + M ^ { T } ) \mathbf { 1 } _ { n } > 0$ . Second, the elementwise form of (8.29) is

$$
\sum_ {j = 1} ^ {n} [ M + M ^ {T} ] _ {i j} > 0, \qquad i = 1, \ldots , n,
$$

which can be further written as

$$
[ M + M ^ {T} ] _ {i i} + \sum_ {j \neq i} [ M + M ^ {T} ] _ {i j} > 0.
$$

It can be verified according to the expression of $M$ in (8.28) that the diagonal entries of $M$ are positive and the off-diagonal entries of $M$ are nonpositive. Therefore, the

above inequality can be rewritten as

$$
\left| \left[ M + M ^ {T} \right] _ {i i} \right| > \sum_ {j \neq i} \left| \left[ M + M ^ {T} \right] _ {i j} \right|.
$$

The above inequality indicates that the absolute value of the $i$ th diagonal entry in $M + M ^ { T }$ is greater than the sum of the absolute values of the off-diagonal entries in the same row. Thus, $M + M ^ { T }$ is strictly diagonal dominant and the proof is complete.

# TD learning minimizes the projected Bellman error

While we have shown that the TD-Linear algorithm converges to $w ^ { * } = A ^ { - 1 } b$ , we next show that $w ^ { * }$ is the optimal solution that minimizes the projected Bellman error. To do that, we review three objective functions.

 The first objective function is

$$
J _ {E} (w) = \mathbb {E} [ (v _ {\pi} (S) - \hat {v} (S, w)) ^ {2} ],
$$

which has been introduced in (8.3). By the definition of expectation, $J _ { E } ( w )$ can be reexpressed in a matrix-vector form as

$$
J _ {E} (w) = \| \hat {v} (w) - v _ {\pi} \| _ {D} ^ {2},
$$

where $v _ { \pi }$ is the true state value vector and $\hat { v } ( w )$ is the approximated one. Here, $\| \cdot \| _ { D } ^ { 2 }$ is a weighted norm: $\| x \| _ { D } ^ { 2 } = x ^ { T } D x = \| D ^ { 1 / 2 } x \| _ { 2 } ^ { 2 }$ , where $D$ is given in (8.20).

This is the simplest objective function that we can imagine when talking about function approximation. However, it relies on the true state, which is unknown. To obtain an implementable algorithm, we must consider other objective functions such as the Bellman error and projected Bellman error [50–54].

The second objective function is the Bellman error. In particular, since $v _ { \pi }$ satisfies the Bellman equation $v _ { \pi } = r _ { \pi } + \gamma P _ { \pi } v _ { \pi }$ , it is expected that the estimated value $\hat { v } ( w )$ should also satisfy this equation to the greatest extent possible. Thus, the Bellman error is

$$
J _ {B E} (w) = \| \hat {v} (w) - (r _ {\pi} + \gamma P _ {\pi} \hat {v} (w)) \| _ {D} ^ {2} \doteq \| \hat {v} (w) - T _ {\pi} (\hat {v} (w)) \| _ {D} ^ {2}. \tag {8.30}
$$

Here, $T _ { \pi } ( \cdot )$ is the Bellman operator. In particular, for any vector $x \in \mathbb { R } ^ { n }$ , the Bellman operator is defined as

$$
T _ {\pi} (x) \doteq r _ {\pi} + \gamma P _ {\pi} x.
$$

Minimizing the Bellman error is a standard least-squares problem. The details of the solution are omitted here.

Third, it is notable that $J _ { B E } ( w )$ in (8.30) may not be minimized to zero due to the limited approximation ability of the approximator. By contrast, an objective function that can be minimized to zero is the projected Bellman error :

$$
J _ {P B E} (w) = \| \hat {v} (w) - M T _ {\pi} (\hat {v} (w)) \| _ {D} ^ {2},
$$

where $M \in \mathbb { R } ^ { n \times n }$ is the orthogonal projection matrix that geometrically projects any vector onto the space of all approximations.

In fact, the TD learning algorithm in (8.13) aims to minimize the projected Bellman error $J _ { P B E }$ rather than $J _ { E }$ or $J _ { B E }$ . The reason is as follows. For the sake of simplicity, consider the linear case where $\hat { v } ( w ) = \Phi w$ . Here, $\Phi$ is defined in (8.20). The range space of $\Phi$ is the set of all possible linear approximations. Then,

$$
M = \Phi \left(\Phi^ {T} D \Phi\right) ^ {- 1} \Phi^ {T} D \in \mathbb {R} ^ {n \times n}
$$

is the projection matrix that geometrically projects any vector onto the range space $\Phi$ . Since $\hat { v } ( w )$ is in the range space of $\Phi$ , we can always find a value of $w$ that can minimize $J _ { P B E } ( w )$ to zero. It can be proven that the solution minimizing $J _ { P B E } ( w )$ is $w ^ { * } = A ^ { - 1 } b$ . That is

$$
w ^ {*} = A ^ {- 1} b = \arg \min _ {w} J _ {P B E} (w),
$$

The proof is given in Box 8.5.

# Box 8.5: Showing that $w ^ { * } = A ^ { - 1 } b$ minimizes $J _ { P B E } ( w )$

We next show that $w ^ { * } = A ^ { - 1 } b$ is the optimal solution that minimizes $J _ { P B E } ( w )$ . Since $J _ { P B E } ( w ) = 0 \Leftrightarrow \hat { v } ( w ) - M T _ { \pi } ( \hat { v } ( w ) ) = 0$ , we only need to study the root of

$$
\hat {v} (w) = M T _ {\pi} (\hat {v} (w)).
$$

In the linear case, substituting $\hat { v } ( w ) = \Phi w$ and the expression of $M$ into the above equation gives

$$
\Phi w = \Phi (\Phi^ {T} D \Phi) ^ {- 1} \Phi^ {T} D (r _ {\pi} + \gamma P _ {\pi} \Phi w). \tag {8.31}
$$

Since $\Phi$ has full column rank, we have $\Phi x = \Phi y \Leftrightarrow x = y$ for any $x , y$ . Therefore, (8.31) implies

$$
\begin{array}{l} w = \left(\Phi^ {T} D \Phi\right) ^ {- 1} \Phi^ {T} D \left(r _ {\pi} + \gamma P _ {\pi} \Phi w\right) \\ \Longleftrightarrow \Phi^ {T} D (r _ {\pi} + \gamma P _ {\pi} \Phi w) = (\Phi^ {T} D \Phi) w \\ \Longleftrightarrow \Phi^ {T} D r _ {\pi} + \gamma \Phi^ {T} D P _ {\pi} \Phi w = (\Phi^ {T} D \Phi) w \\ \Longleftrightarrow \Phi^ {T} D r _ {\pi} = \Phi^ {T} D (I - \gamma P _ {\pi}) \Phi w \\ \Longleftrightarrow w = \left(\Phi^ {T} D (I - \gamma P _ {\pi}) \Phi\right) ^ {- 1} \Phi^ {T} D r _ {\pi} = A ^ {- 1} b, \\ \end{array}
$$

where $A , b$ are given in (8.21). Therefore, $w ^ { * } = A ^ { - 1 } b$ is the optimal solution that minimizes $J _ { P B E } ( w )$ .

Since the TD algorithm aims to minimize $J _ { P B E }$ rather than $J _ { E }$ , it is natural to ask how close the estimated value $\hat { v } ( w )$ is to the true state value $v _ { \pi }$ . In the linear case, the estimated value that minimizes the projected Bellman error is $\hat { v } ( w ^ { * } ) = \Phi w ^ { * }$ . Its deviation from the true state value $v _ { \pi }$ satisfies

$$
\left\| \hat {v} \left(w ^ {*}\right) - v _ {\pi} \right\| _ {D} = \left\| \Phi w ^ {*} - v _ {\pi} \right\| _ {D} \leq \frac {1}{1 - \gamma} \min  _ {w} \| \hat {v} (w) - v _ {\pi} \| _ {D} = \frac {1}{1 - \gamma} \min  _ {w} \sqrt {J _ {E} (w)}. \tag {8.32}
$$

The proof of this inequality is given in Box 8.6. Inequality (8.32) indicates that the discrepancy between $\Phi w ^ { * }$ and $v _ { \pi }$ is bounded from above by the minimum value of $J _ { E } ( w )$ . However, this bound is loose, especially when $\gamma$ is close to one. It is thus mainly of theoretical value.

# Box 8.6: Proof of the error bound in (8.32)

Note that

$$
\begin{array}{l} \left\| \Phi w ^ {*} - v _ {\pi} \right\| _ {D} = \left\| \Phi w ^ {*} - M v _ {\pi} + M v _ {\pi} - v _ {\pi} \right\| _ {D} \\ \leq \left\| \Phi w ^ {*} - M v _ {\pi} \right\| _ {D} + \left\| M v _ {\pi} - v _ {\pi} \right\| _ {D} \\ = \left\| M T _ {\pi} \left(\Phi w ^ {*}\right) - M T _ {\pi} \left(v _ {\pi}\right) \right\| _ {D} + \left\| M v _ {\pi} - v _ {\pi} \right\| _ {D}, \tag {8.33} \\ \end{array}
$$

where the last equality is due to $\Phi w ^ { * } = M T _ { \pi } ( \Phi w ^ { * } )$ and $v _ { \pi } = T _ { \pi } ( v _ { \pi } )$ . Substituting

$$
M T _ {\pi} \left(\Phi w ^ {*}\right) - M T _ {\pi} \left(v _ {\pi}\right) = M \left(r _ {\pi} + \gamma P _ {\pi} \Phi w ^ {*}\right) - M \left(r _ {\pi} + \gamma P _ {\pi} v _ {\pi}\right) = \gamma M P _ {\pi} \left(\Phi w ^ {*} - v _ {\pi}\right)
$$

into (8.33) yields

$$
\begin{array}{l} \left\| \Phi w ^ {*} - v _ {\pi} \right\| _ {D} \leq \left\| \gamma M P _ {\pi} \left(\Phi w ^ {*} - v _ {\pi}\right) \right\| _ {D} + \left\| M v _ {\pi} - v _ {\pi} \right\| _ {D} \\ \leq \gamma \| M \| _ {D} \| P _ {\pi} \left(\Phi w ^ {*} - v _ {\pi}\right) \| _ {D} + \| M v _ {\pi} - v _ {\pi} \| _ {D} \\ = \gamma \| P _ {\pi} \left(\Phi w ^ {*} - v _ {\pi}\right) \| _ {D} + \| M v _ {\pi} - v _ {\pi} \| _ {D} \quad \mathrm {(b e c a u s e} \| M \| _ {D} = 1) \\ \leq \gamma \| \Phi w ^ {*} - v _ {\pi} \| _ {D} + \| M v _ {\pi} - v _ {\pi} \| _ {D}. \quad \mathrm {(b e c a u s e} \| P _ {\pi} x \| _ {D} \leq \| x \| _ {D} \mathrm {f o r a l l} x) \\ \end{array}
$$

The proof of $\| M \| _ { D } = 1$ and $\| P _ { \pi } x \| _ { D } \leq \| x \| _ { D }$ are postponed to the end of the box. Recognizing the above inequality gives

$$
\begin{array}{l} \left\| \Phi w ^ {*} - v _ {\pi} \right\| _ {D} \leq \frac {1}{1 - \gamma} \left\| M v _ {\pi} - v _ {\pi} \right\| _ {D} \\ = \frac {1}{1 - \gamma} \min _ {w} \| \hat {v} (w) - v _ {\pi} \| _ {D}, \\ \end{array}
$$

where the last equality is because $\| M v _ { \pi } - v _ { \pi } \| _ { D }$ is the error between $v _ { \pi }$ and its orthogonal projection into the space of all possible approximations. Therefore, it is the minimum value of the error between $v _ { \pi }$ and any $\hat { v } ( w )$ .

We next prove some useful facts, which have already been used in the above proof.

> Properties of matrix weighted norms. By definition, $\| x \| _ { D } = \sqrt { x ^ { T } D x } = \| D ^ { 1 / 2 } x \| _ { 2 }$ . The induced matrix norm is $\begin{array} { r } { \| A \| _ { D } = \operatorname* { m a x } _ { x \neq 0 } \| A x \| _ { D } / \| x \| _ { D } = \| D ^ { 1 / 2 } A D ^ { - 1 / 2 } \| _ { 2 } } \end{array}$ . For matrices $A , B$ with appropriate dimensions, we have $\| A B x \| _ { D } \leq \| A \| _ { D } \| B \| _ { D } \| x \| _ { D }$ . To see that, $\| A B x \| _ { D } \ = \ \| D ^ { 1 / 2 } A B x \| _ { 2 } \ = \ \| D ^ { 1 / 2 } A D ^ { - 1 / 2 } D ^ { 1 / 2 } B D ^ { - 1 / 2 } D ^ { 1 / 2 } x \| _ { 2 } \ \leq \quad \quad$ $\| D ^ { 1 / 2 } A D ^ { - 1 / 2 } \| _ { 2 } \| D ^ { 1 / 2 } B D ^ { - 1 / 2 } \| _ { 2 } \| D ^ { 1 / 2 } x \| _ { 2 } = \| A \| _ { D } \| B \| _ { D } \| x \| _ { D } .$   
Proof of $\| M \| _ { D } = 1$ . This is valid because $\| M \| _ { D } ~ = ~ \| \Phi ( \Phi ^ { T } D \Phi ) ^ { - 1 } \Phi ^ { T } D \| _ { D } ~ = ~$ $\| D ^ { 1 / 2 } \Phi ( \Phi ^ { T } D \Phi ) ^ { - 1 } \Phi ^ { T } D D ^ { - 1 / 2 } \| _ { 2 } = 1$ , where the last equality is valid due to the fact that the matrix in the $L _ { 2 }$ -norm is an orthogonal projection matrix and the $L _ { \mathrm { 2 } }$ -norm of any orthogonal projection matrix is equal to one.   
 Proof of $\| P _ { \pi } x \| _ { D } \leq \| x \| _ { D }$ for any $x \in \mathbb { R } ^ { n }$ . First,

$$
\| P _ {\pi} x \| _ {D} ^ {2} = x ^ {T} P _ {\pi} ^ {T} D P _ {\pi} x = \sum_ {i, j} x _ {i} [ P _ {\pi} ^ {T} D P _ {\pi} ] _ {i j} x _ {j} = \sum_ {i, j} x _ {i} \left(\sum_ {k} [ P _ {\pi} ^ {T} ] _ {i k} [ D ] _ {k k} [ P _ {\pi} ] _ {k j}\right) x _ {j}.
$$

Reorganizing the above equation gives

$$
\begin{array}{l} \left\| P _ {\pi} x \right\| _ {D} ^ {2} = \sum_ {k} [ D ] _ {k k} \Big (\sum_ {i} [ P _ {\pi} ] _ {k i} x _ {i} \Big) ^ {2} \\ \leq \sum_ {k} [ D ] _ {k k} \left(\sum_ {i} \left[ P _ {\pi} \right] _ {k i} x _ {i} ^ {2}\right) \quad (\text {d u e t o J e s s e n ' s i n e q u a l i t y [ 5 5 , 5 6 ]}) \\ = \sum_ {i} \left(\sum_ {k} [ D ] _ {k k} [ P _ {\pi} ] _ {k i}\right) x _ {i} ^ {2} \\ = \sum_ {i} [ D ] _ {i i} x _ {i} ^ {2} \quad \left(\text {d u e t o} d _ {\pi} ^ {T} P _ {\pi} = d _ {\pi} ^ {T}\right) \\ = \left\| x \right\| _ {D} ^ {2}. \\ \end{array}
$$

# Least-squares TD

We next introduce an algorithm called least-squares TD (LSTD) [57]. Like the TD-Linear algorithm, LSTD aims to minimize the projected Bellman error. However, it has some advantages over the TD-Linear algorithm.

Recall that the optimal parameter for minimizing the projected Bellman error is $w ^ { * } = A ^ { - 1 } b$ , where $A = \Phi ^ { I } D ( I - \gamma P _ { \pi } ) \Phi$ and $b = \Phi ^ { I ^ { \prime } } D r _ { \pi }$ . In fact, it follows from (8.27) that $A$ and $b$ can also be written as

$$
A = \mathbb {E} \left[ \phi (s _ {t}) \left(\phi (s _ {t}) - \gamma \phi (s _ {t + 1})\right) ^ {T} \right],
$$

$$
b = \mathbb {E} \Big [ r _ {t + 1} \phi (s _ {t}) \Big ].
$$

The above two equations show that $A$ and $b$ are expectations of $s _ { t } , s _ { t + 1 } , r _ { t + 1 }$ . The idea of LSTD is simple: if we can use random samples to directly obtain the estimates of $A$ and $b$ , which are denoted as $\hat { A }$ and $\hat { b }$ , then the optimal parameter can be directly estimated as $w ^ { * } \approx \hat { A } ^ { - 1 } \hat { b }$ .

In particular, suppose that $( s _ { 0 } , r _ { 1 } , s _ { 1 } , \ldots , s _ { t } , r _ { t + 1 } , s _ { t + 1 } , \ldots )$ is a trajectory obtained by following a given policy $\pi$ . Let $\hat { A } _ { t }$ and $\hat { b } _ { t }$ be the estimates of $A$ and $b$ at time $t$ , respectively. They are calculated as the averages of the samples:

$$
\hat {A} _ {t} = \sum_ {k = 0} ^ {t - 1} \phi (s _ {k}) \left(\phi (s _ {k}) - \gamma \phi \left(s _ {k + 1}\right)\right) ^ {T},
$$

$$
\hat {b} _ {t} = \sum_ {k = 0} ^ {t - 1} r _ {k + 1} \phi \left(s _ {k}\right). \tag {8.34}
$$

Then, the estimated parameter is

$$
w _ {t} = \hat {A} _ {t} ^ {- 1} \hat {b} _ {t}.
$$

The reader may wonder if a coefficient of $1 / t$ is missing on the right-hand side of (8.34). In fact, it is omitted for the sake of simplicity since the value of $w _ { t }$ remains the same when it is omitted. Since $\hat { A } _ { t }$ may not be invertible especially when $t$ is small, $\hat { A } _ { t }$ is usually biased by a small constant matrix $\sigma { \boldsymbol { I } }$ , where $I$ is the identity matrix and $\sigma$ is a small positive number.

The advantage of LSTD is that it uses experience samples more efficiently and converges faster than the TD method. That is because this algorithm is specifically designed based on the knowledge of the optimal solution’s expression. The better we understand a problem, the better algorithms we can design.

The disadvantages of LSTD are as follows. First, it can only estimate state values. By contrast, the TD algorithm can be extended to estimate action values as shown in the next section. Moreover, while the TD algorithm allows nonlinear approximators, LSTD does not. That is because this algorithm is specifically designed based on the expression of $w ^ { * }$ . Second, the computational cost of LSTD is higher than that of TD since LSTD updates an $m \times m$ matrix in each update step, whereas TD updates an $m$ -dimensional vector. More importantly, in every step, LSTD needs to compute the inverse of $\hat { A } _ { t }$ , whose computational complexity is $O ( m ^ { 3 } )$ . The common method for resolving this problem is to directly update the inverse of ${ \hat { A } } _ { t }$ rather than updating $\hat { A } _ { t }$ . In particular, $\hat { A } _ { t + 1 }$ can be calculated recursively as follows:

$$
\begin{array}{l} \hat {A} _ {t + 1} = \sum_ {k = 0} ^ {t} \phi (s _ {k}) \left(\phi (s _ {k}) - \gamma \phi (s _ {k + 1})\right) ^ {T} \\ = \sum_ {k = 0} ^ {t - 1} \phi (s _ {k}) \left(\phi (s _ {k}) - \gamma \phi (s _ {k + 1})\right) ^ {T} + \phi (s _ {t}) \left(\phi (s _ {t}) - \gamma \phi (s _ {t + 1})\right) ^ {T} \\ = \hat {A} _ {t} + \phi (s _ {t}) \big (\phi (s _ {t}) - \gamma \phi (s _ {t + 1}) \big) ^ {T}. \\ \end{array}
$$

The above expression decomposes $\hat { A } _ { t + 1 }$ into the sum of two matrices. Its inverse can be calculated as [58]

$$
\begin{array}{l} \hat {A} _ {t + 1} ^ {- 1} = \left(\hat {A} _ {t} + \phi (s _ {t}) \big (\phi (s _ {t}) - \gamma \phi (s _ {t + 1}) \big) ^ {T}\right) ^ {- 1} \\ = \hat {A} _ {t} ^ {- 1} + \frac {\hat {A} _ {t} ^ {- 1} \phi (s _ {t}) (\phi (s _ {t}) - \gamma \phi (s _ {t + 1})) ^ {T} \hat {A} _ {t} ^ {- 1}}{1 + (\phi (s _ {t}) - \gamma \phi (s _ {t + 1})) ^ {T} \hat {A} _ {t} ^ {- 1} \phi (s _ {t})}. \\ \end{array}
$$

Therefore, we can directly store and update $\hat { A } _ { t } ^ { - 1 }$ to avoid the need to calculate the matrix inverse. This recursive algorithm does not require a step size. However, it requires setting the initial value of $\hat { A } _ { 0 } ^ { - 1 }$ . The initial value of such a recursive algorithm can be selected as $\hat { A } _ { 0 } ^ { - 1 } = \sigma I$ , where $\sigma$ is a positive number. A good tutorial on the recursive least-squares approach can be found in [59].

# 8.3 TD learning of action values based on function approximation

While Section 8.2 introduced the problem of state value estimation, the present section introduces how to estimate action values. The tabular Sarsa and tabular Q-learning algorithms are extended to the case of value function approximation. Readers will see that the extension is straightforward.

# 8.3.1 Sarsa with function approximation

The Sarsa algorithm with function approximation can be readily obtained from (8.13) by replacing the state values with action values. In particular, suppose that $q _ { \pi } ( s , a )$ is approximated by ${ \hat { q } } ( s , a , w )$ . Replacing ${ \hat { v } } ( s , w )$ in (8.13) by ${ \hat { q } } ( s , a , w )$ gives

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \left[ r _ {t + 1} + \gamma \hat {q} \left(s _ {t + 1}, a _ {t + 1}, w _ {t}\right) - \hat {q} \left(s _ {t}, a _ {t}, w _ {t}\right) \right] \nabla_ {w} \hat {q} \left(s _ {t}, a _ {t}, w _ {t}\right). \tag {8.35}
$$

The analysis of (8.35) is similar to that of (8.13) and is omitted here. When linear functions are used, we have

$$
\hat {q} (s, a, w) = \phi^ {T} (s, a) w,
$$

where $\phi ( s , a )$ is a feature vector. In this case, $\nabla _ { w } \widehat { q } ( s , a , w ) = \phi ( s , a )$ .

The value estimation step in (8.35) can be combined with a policy improvement step to learn optimal policies. The procedure is summarized in Algorithm 8.2. It should be noted that accurately estimating the action values of a given policy requires (8.35) to be run sufficiently many times. However, (8.35) is executed only once before switching to the policy improvement step. This is similar to the tabular Sarsa algorithm. Moreover, the implementation in Algorithm 8.2 aims to solve the task of finding a good path to the target state from a prespecified starting state. As a result, it cannot find the optimal policy for every state. However, if sufficient experience data are available, the implementation process can be easily adapted to find optimal policies for every state.

An illustrative example is shown in Figure 8.9. In this example, the task is to find a good policy that can lead the agent to the target when starting from the top-left state. Both the total reward and the length of each episode gradually converge to steady values. In this example, the linear feature vector is selected as the Fourier function of order 5. The expression of a Fourier feature vector is given in (8.18).

![](images/d1499640765c1cdb1b9563c71c4aaf8cf6abae938f30b1fff609760d9210f70e.jpg)

![](images/9a9d65ff5c96217bc8c8cc31700a0264a53e7f60aa0f7b8e9191b5b4ac4a8bc4.jpg)

![](images/2bf71fd0eae2df6bcd4710c1786a48d29d0ca1facfbc1bf73c61ea8734547bcd.jpg)  
Figure 8.9: Sarsa with linear function approximation. Here, $\gamma = 0 . 9$ , $\epsilon = 0 . 1$ , $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } =$ $- 1 0$ , $r _ { \mathrm { t a r g e t } } = 1$ , and $\alpha = 0 . 0 0 1$ .

# Algorithm 8.2: Sarsa with function approximation

Initialization: Initial parameter $w _ { 0 }$ . Initial policy $\pi _ { 0 }$ . $\alpha _ { t } = \alpha > 0$ for all $t$ . $\epsilon \in ( 0 , 1 )$

Goal: Learn an optimal policy that can lead the agent to the target state from an initial state $s _ { 0 }$ .

For each episode, do

Generate $a _ { 0 }$ at $s _ { 0 }$ following $\pi _ { 0 } ( s _ { 0 } )$

If $s _ { t }$ $( t = 0 , 1 , 2 , \ldots )$ is not the target state, do

Collect the experience sample $( r _ { t + 1 } , s _ { t + 1 } , a _ { t + 1 } )$ given $( s _ { t } , a _ { t } )$ : generate $r _ { t + 1 } , s _ { t + 1 }$

by interacting with the environment; generate $a _ { t + 1 }$ following $\pi _ { t } ( s _ { t + 1 } )$

Update $q \mathrm { . }$ -value:

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \left[ r _ {t + 1} + \gamma \hat {q} \left(s _ {t + 1}, a _ {t + 1}, w _ {t}\right) - \hat {q} \left(s _ {t}, a _ {t}, w _ {t}\right) \right] \nabla_ {w} \hat {q} \left(s _ {t}, a _ {t}, w _ {t}\right)
$$

Update policy:

$$
\pi_ {t + 1} (a | s _ {t}) = 1 - \frac {\varepsilon}{| \mathcal {A} (s _ {t}) |} (| \mathcal {A} (s _ {t}) | - 1) \text {i f} a = \arg \max  _ {a \in \mathcal {A} (s _ {t})} \hat {q} (s _ {t}, a, w _ {t + 1})
$$

$$
\pi_ {t + 1} (a | s _ {t}) = \frac {\epsilon}{| \mathcal {A} (s _ {t}) |} o t h e r w i s e
$$

$$
s _ {t} \leftarrow s _ {t + 1}, a _ {t} \leftarrow a _ {t + 1}
$$

# 8.3.2 Q-learning with function approximation

Tabular Q-learning can also be extended to the case of function approximation. The update rule is

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \left[ r _ {t + 1} + \gamma \max  _ {a \in \mathcal {A} \left(s _ {t + 1}\right)} \hat {q} \left(s _ {t + 1}, a, w _ {t}\right) - \hat {q} \left(s _ {t}, a _ {t}, w _ {t}\right) \right] \nabla_ {w} \hat {q} \left(s _ {t}, a _ {t}, w _ {t}\right). \tag {8.36}
$$

The above update rule is similar to (8.35) except that $\hat { q } ( s _ { t + 1 } , a _ { t + 1 } , w _ { t } )$ in (8.35) is replaced with $\operatorname* { m a x } _ { a \in \mathcal { A } ( s _ { t + 1 } ) } \hat { q } \big ( s _ { t + 1 } , a , w _ { t } \big )$ .

Similar to the tabular case, (8.36) can be implemented in either an on-policy or off-policy fashion. An on-policy version is given in Algorithm 8.3. An example for demonstrating the on-policy version is shown in Figure 8.10. In this example, the task is to find a good policy that can lead the agent to the target state from the top-left state.

Algorithm 8.3: Q-learning with function approximation (on-policy version)

Initialization: Initial parameter $w _ { 0 }$ . Initial policy $\pi _ { 0 }$ . $\alpha _ { t } = \alpha > 0$ for all $t$ . $\epsilon \in ( 0 , 1 )$ .

Goal: Learn an optimal path that can lead the agent to the target state from an initial state $s _ { 0 }$ .

For each episode, do

If $s _ { t }$ $( t = 0 , 1 , 2 , \ldots )$ is not the target state, do

Collect the experience sample $( a _ { t } , r _ { t + 1 } , s _ { t + 1 } )$ given $s _ { t }$ : generate $a _ { t }$ following $\pi _ { t } ( s _ { t } )$ ; generate $r _ { t + 1 } , s _ { t + 1 }$ by interacting with the environment.

Update $q$ -value:

$$
w _ {t + 1} = w _ {t} + \alpha_ {t} \left[ r _ {t + 1} + \gamma \max  _ {a \in \mathcal {A} (s _ {t + 1})} \hat {q} (s _ {t + 1}, a, w _ {t}) - \hat {q} (s _ {t}, a _ {t}, w _ {t}) \right] \nabla_ {w} \hat {q} (s _ {t}, a _ {t}, w _ {t})
$$

Update policy:

$$
\pi_ {t + 1} (a | s _ {t}) = 1 - \frac {\varepsilon}{| \mathcal {A} (s _ {t}) |} (| \mathcal {A} (s _ {t}) | - 1) \mathrm {i f} a = \arg \max _ {a \in \mathcal {A} (s _ {t})} \hat {q} (s _ {t}, a, w _ {t + 1})
$$

$$
\pi_ {t + 1} (a | s _ {t}) = \frac {\varepsilon}{| \mathcal {A} (s _ {t}) |} o t h e r w i s e
$$

As can be seen, Q-learning with linear function approximation can successfully learn an optimal policy. Here, linear Fourier basis functions of order five are used. The off-policy version will be demonstrated when we introduce deep Q-learning in Section 8.4.

![](images/253f2783d87b32e8aa4788e117c0bc7d731ee40083f027265972be8ad5f5776e.jpg)

![](images/0f4e80ca2dc3a28329e3b49265a665e650b2631b1bcc5da24f25095566df6f78.jpg)

![](images/20ae30935e508f318a150d103cb89d8e37769aa87f9d361dbf4b8655e14ec3ce.jpg)  
Figure 8.10: Q-learning with linear function approximation. Here, $\gamma ~ = ~ 0 . 9$ , $\epsilon \ : = \ : 0 . 1$ , rboundary = $r _ { \mathrm { f o r b i d d e n } } = - 1 0$ , $r _ { \mathrm { t a r g e t } } = 1$ , and $\alpha = 0 . 0 0 1$ .

One may notice in Algorithm 8.2 and Algorithm 8.3 that, although the values are represented as functions, the policy $\pi ( a | s )$ is still represented as a table. Thus, it still assumes finite numbers of states and actions. In Chapter 9, we will see that the policies can be represented as functions so that continuous state and action spaces can be handled.

# 8.4 Deep Q-learning

We can integrate deep neural networks into Q-learning to obtain an approach called deep Q-learning or deep Q-network (DQN) [22, 60, 61]. Deep Q-learning is one of the

# 8.4. Deep Q-learning

earliest and most successful deep reinforcement learning algorithms. Notably, the neural networks do not have to be deep. For simple tasks such as our grid world examples, shallow networks with one or two hidden layers may be sufficient.

Deep Q-learning can be viewed as an extension of the algorithm in (8.36). However, its mathematical formulation and implementation techniques are substantially different and deserve special attention.

# 8.4.1 Algorithm description

Mathematically, deep Q-learning aims to minimize the following objective function:

$$
J = \mathbb {E} \left[ \left(R + \gamma \max _ {a \in \mathcal {A} (S ^ {\prime})} \hat {q} (S ^ {\prime}, a, w) - \hat {q} (S, A, w)\right) ^ {2} \right], \tag {8.37}
$$

where $( S , A , R , S ^ { \prime } )$ are random variables that denote a state, an action, the immediate reward, and the next state, respectively. This objective function can be viewed as the squared Bellman optimality error. That is because

$$
q (s, a) = \mathbb {E} \left[ R _ {t + 1} + \gamma \max _ {a \in \mathcal {A} (S _ {t + 1})} q (S _ {t + 1}, a) \Big | S _ {t} = s, A _ {t} = a \right], \quad \mathrm {f o r a l l} s, a
$$

is the Bellman optimality equation (the proof is given in Box 7.5). Therefore, $R +$ $\gamma \operatorname* { m a x } _ { a \in { \mathcal { A } } ( S ^ { \prime } ) } { \hat { q } } ( S ^ { \prime } , a , w ) - { \hat { q } } ( S , A , w )$ should equal zero in the expectation sense when ${ \hat { q } } ( S , A , w )$ can accurately approximate the optimal action values.

To minimize the objective function in (8.37), we can use the gradient descent algorithm. To that end, we need to calculate the gradient of $J$ with respect to $w$ . It is noted that the parameter $w$ appears not only in ${ \hat { q } } ( S , A , w )$ but also in $\begin{array} { r } { y \doteq R \mathrm { + } \gamma \operatorname* { m a x } _ { a \in \mathcal { A } ( S ^ { \prime } ) } \hat { q } ( S ^ { \prime } , a , w ) } \end{array}$ . As a result, it is nontrivial to calculate the gradient. For the sake of simplicity, it is assumed that the value of $w$ in $y$ is fixed (for a short period of time) so that the calculation of the gradient becomes much easier. In particular, we introduce two networks: one is a main network representing ${ \hat { q } } ( s , a , w )$ and the other is a target network $\hat { q } ( s , a , w _ { T } )$ . The objective function in this case becomes

$$
J = \mathbb {E} \left[ \left(R + \gamma \max _ {a \in \mathcal {A} (S ^ {\prime})} \hat {q} (S ^ {\prime}, a, w _ {T}) - \hat {q} (S, A, w)\right) ^ {2} \right],
$$

where $w _ { T }$ is the target network’s parameter. When $w _ { T }$ is fixed, the gradient of $J$ is

$$
\nabla_ {w} J = - \mathbb {E} \left[ \left(R + \gamma \max _ {a \in \mathcal {A} (S ^ {\prime})} \hat {q} (S ^ {\prime}, a, w _ {T}) - \hat {q} (S, A, w)\right) \nabla_ {w} \hat {q} (S, A, w) \right], \tag {8.38}
$$

where some constant coefficients are omitted without loss of generality.

To use the gradient in (8.38) to minimize the objective function, we need to pay

attention to the following techniques.

The first technique is to use two networks, a main network and a target network, as mentioned when we calculate the gradient in (8.38). The implementation details are explained below. Let $w$ and $w _ { T }$ denote the parameters of the main and target networks, respectively. They are initially set to the same value.

In every iteration, we draw a mini-batch of samples $\{ ( s , a , r , s ^ { \prime } ) \}$ from the replay buffer (the replay buffer will be explained soon). The inputs of the main network are $s$ and $a$ . The output $\boldsymbol { y } = \boldsymbol { \hat { q } } ( s , a , w )$ is the estimated q-value. The target value of the output is $\begin{array} { r } { y _ { T } \doteq r + \gamma \operatorname* { m a x } _ { a \in \mathcal { A } ( s ^ { \prime } ) } \hat { q } ( s ^ { \prime } , a , w _ { T } ) } \end{array}$ . The main network is updated to minimize the TD error (also called the loss function) $\sum ( y - y _ { T } ) ^ { 2 }$ over the samples $\{ ( s , a , y _ { T } ) \}$ .

Updating $w$ in the main network does not explicitly use the gradient in (8.38). Instead, it relies on the existing software tools for training neural networks. As a result, we need a mini-batch of samples to train a network instead of using a single sample to update the main network based on (8.38). This is one notable difference between deep and nondeep reinforcement learning algorithms.

The main network is updated in every iteration. By contrast, the target network is set to be the same as the main network every certain number of iterations to satisfy the assumption that $w _ { T }$ is fixed when calculating the gradient in (8.38).

> The second technique is experience replay [22,60,62]. That is, after we have collected some experience samples, we do not use these samples in the order they were collected. Instead, we store them in a dataset called the replay buffer. In particular, let $( s , a , r , s ^ { \prime } )$ be an experience sample and $B \doteq \{ ( s , a , r , s ^ { \prime } ) \}$ be the replay buffer. Every time we update the main network, we can draw a mini-batch of experience samples from the replay buffer. The draw of samples, or called experience replay, should follow a uniform distribution.

Why is experience replay necessary in deep Q-learning, and why must the replay follow a uniform distribution? The answer lies in the objective function in (8.37). In particular, to well define the objective function, we must specify the probability distributions for $S , A , R , S ^ { \prime }$ . The distributions of $R$ and $S ^ { \prime }$ are determined by the system model once $( S , A )$ is given. The simplest way to describe the distribution of the state-action pair $( S , A )$ is to assume it to be uniformly distributed.

However, the state-action samples may not be uniformly distributed in practice since they are generated as a sample sequence according to the behavior policy. It is necessary to break the correlation between the samples in the sequence to satisfy the assumption of uniform distribution. To do this, we can use the experience replay technique by uniformly drawing samples from the replay buffer. This is the mathematical reason why experience replay is necessary and why experience replay must follow a uniform distribution. A benefit of random sampling is that each experience sample

# Algorithm 8.3: Deep Q-learning (off-policy version)

Initialization: A main network and a target network with the same initial parameter.

Goal: Learn an optimal target network to approximate the optimal action values from the experience samples generated by a given behavior policy $\pi _ { b }$ .

Store the experience samples generated by $\pi _ { b }$ in a replay buffer $\boldsymbol { B } = \{ ( s , a , r , s ^ { \prime } ) \}$

For each iteration, do

Uniformly draw a mini-batch of samples from $\boldsymbol { B }$

For each sample $( s , a , r , s ^ { \prime } )$ , calculate the target value as $\begin{array} { r c l } { y _ { T } } & { = } & { r _ { \mathrm { ~ } } + } \end{array}$ $\gamma \operatorname* { m a x } _ { a \in \mathcal { A } ( s ^ { \prime } ) } \hat { q } \big ( s ^ { \prime } , a , w _ { T } \big )$ , where $w _ { T }$ is the parameter of the target network

Update the main network to minimize $( y _ { T } - \hat { q } ( s , a , w ) ) ^ { 2 }$ using the mini-batch of samples

Set $w _ { T } = w$ every $C$ iterations

may be used multiple times, which can increase the data efficiency. This is especially important when we have a limited amount of data.

The implementation procedure of deep Q-learning is summarized in Algorithm 8.3. This implementation is off-policy. It can also be adapted to become on-policy if needed.

# 8.4.2 Illustrative examples

An example is given in Figure 8.11 to demonstrate Algorithm 8.3. This example aims to learn the optimal action values for every state-action pair. Once the optimal action values are obtained, the optimal greedy policy can be obtained immediately.

A single episode is generated by the behavior policy shown in Figure 8.11(a). This behavior policy is exploratory in the sense that it has the same probability of taking any action at any state. The episode has only 1,000 steps as shown in Figure 8.11(b). Although there are only 1,000 steps, almost all the state action pairs are visited in this episode due to the strong exploration ability of the behavior policy. The replay buffer is a set of 1,000 experience samples. The mini-batch size is 100, meaning that we uniformly draw 100 samples from the replay buffer every time we acquire samples.

The main and target networks have the same structure: a neural network with one hidden layer of 100 neurons (the numbers of layers and neurons can be tuned). The neural network has three inputs and one output. The first two inputs are the normalized row and column indexes of a state. The third input is the normalized action index. Here, “normalization” means converting a value to the interval of [0,1]. The output of the network is the estimated value. The reason why we design the inputs as the row and column of a state rather than a state index is that we know that a state corresponds to a two-dimensional location in the grid. The more information about the state we use when designing the network, the better the network can perform. Moreover, the neural

![](images/5d8eedd004a87f8c33ae5d1a127144341255acebb95164374a8c7823a8c2eaf0.jpg)  
(a) The behavior policy.

![](images/14d85b796208a577203b3c3f607d80cf770efc8525aef661af4a2a0025ac6a86.jpg)  
(b) An episode with 1,000 steps.

![](images/561ea36a42041aa93846335cfd95685bb81b5d69017ddf3b4faa6ffd106938d2.jpg)  
(c) The final learned policy.

![](images/83b676b26a735b8c3a223d8bf1305faf9348202ddd035bd3050cb2458ee88582.jpg)  
(d) The loss function converges to zero.

![](images/fc90415dc1db1088681917ac847aee2ddff3a8de946ef3ef83e3fb08f16e907a.jpg)  
(e) The value error converges to zero.   
Figure 8.11: Optimal policy learning via deep Q-learning. Here, $\gamma = 0 . 9$ , $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1 0$ , and $r _ { \mathrm { t a r g e t } } = 1$ . The batch size is 100.

network can also be designed in other ways. For example, it can have two inputs and five outputs, where the two inputs are the normalized row and column of a state and the outputs are the five estimated action values for the input state [22].

As shown in Figure 8.11(d), the loss function, defined as the average squared TD error of each mini-batch, converges to zero, meaning that the network can fit the training samples well. As shown in Figure 8.11(e), the state value estimation error also converges to zero, indicating that the estimates of the optimal action values become sufficiently accurate. Then, the corresponding greedy policy is optimal.

This example demonstrates the high efficiency of deep Q-learning. In particular, a short episode of 1,000 steps is sufficient for obtaining an optimal policy here. By contrast, an episode with 100,000 steps is required by tabular Q-learning, as shown in Figure 7.4. One reason for the high efficiency is that the function approximation method has a strong generalization ability. Another reason is that the experience samples can be repeatedly used.

We next deliberately challenge the deep Q-learning algorithm by considering a scenario with fewer experience samples. Figure 8.12 shows an example of an episode with merely 100 steps. In this example, although the network can still be well-trained in the sense

![](images/0d6b69b8aee3ff73beec5dcdb89a9a72c02e3b50912f7e602dc1b55bceeaff9e.jpg)  
(a) The behavior policy.

![](images/f0a5910ac908e68ec5bdd0c32b5dac61bc513480bc6ace010a0c1ccda5588e15.jpg)  
(b) An episode with 100 steps.

![](images/a2be14bc1a548a205a351618fe072acd806d2cae2960462cbba2644f8e626648.jpg)  
(c) The final learned policy.

![](images/b52bb8efc244f6de0a8bbcab181a22713925178c1f1b3a1b77ce6615d4b7ce32.jpg)  
(d) The loss function converges to zero.

![](images/831eda0747cd871984a94d568042d102acd5c097b930fa647667bc23bbd57de7.jpg)  
(e) The value error does not converge to zero.   
Figure 8.12: Optimal policy learning via deep Q-learning. Here, $\gamma = 0 . 9$ , $r _ { \mathrm { b o u n d a r y } } = r _ { \mathrm { f o r b i d d e n } } = - 1 0$ , and $r _ { \mathrm { t a r g e t } } = 1$ . The batch size is 50.

that the loss function converges to zero, the state estimation error cannot converge to zero. That means the network can properly fit the given experience samples, but the experience samples are too few to accurately estimate the optimal action values.

# 8.5 Summary

This chapter continued introducing TD learning algorithms. However, it switches from the tabular method to the function approximation method. The key to understanding the function approximation method is to know that it is an optimization problem. The simplest objective function is the squared error between the true state values and the estimated values. There are also other objective functions such as the Bellman error and the projected Bellman error. We have shown that the TD-Linear algorithm actually minimizes the projected Bellman error. Several optimization algorithms such as Sarsa and Q-learning with value approximation have been introduced.

One reason why the value function approximation method is important is that it allows artificial neural networks to be integrated with reinforcement learning. For example, deep Q-learning is one of the most successful deep reinforcement learning algorithms.

Although neural networks have been widely used as nonlinear function approximators, this chapter provides a comprehensive introduction to the linear function case. Fully understanding the linear case is important for better understanding the nonlinear case. Interested readers may refer to [63] for a thorough analysis of TD learning algorithms with function approximation. A more theoretical discussion on deep Q-learning can be found in [61].

An important concept named stationary distribution is introduced in this chapter. The stationary distribution plays an important role in defining an appropriate objective function in the value function approximation method. It also plays a key role in Chapter 9 when we use functions to approximate policies. An excellent introduction to this topic can be found in [49, Chapter IV]. The contents of this chapter heavily rely on matrix analysis. Some results are used without explanation. Excellent references regarding matrix analysis and linear algebra can be found in [4, 48].

# 8.6 Q&A

 Q: What is the difference between the tabular and function approximation methods?

A: One important difference is how a value is updated and retrieved.

How to retrieve a value: When the values are represented by a table, if we would like to retrieve a value, we can directly read the corresponding entry in the table. However, when the values are represented by a function, we need to input the state index $s$ into the function and calculate the function value. If the function is an artificial neural network, a forward prorogation process from the input to the output is needed.

How to update a value: When the values are represented by a table, if we would like to update one value, we can directly rewrite the corresponding entry in the table. However, when the values are represented by a function, we must update the function parameter to change the values indirectly.

$\diamond$ Q: What are the advantages of the function approximation method over the tabular method?

A: Due to the way state values are retrieved, the function approximation method is more efficient in storage. In particular, while the tabular method needs to store $| S |$ values, the function approximation method only needs to store a parameter vector whose dimension is usually much less than $| S |$ .

Due to the way in which state values are updated, the function approximation method has another merit: its generalization ability is stronger than that of the tabular method. The reason is as follows. With the tabular method, updating one state value would not change the other state values. However, with the function approximation method, updating the function parameter affects the values of many states.

Therefore, the experience sample for one state can generalize to help estimate the values of other states.

 Q: Can we unify the tabular and the function approximation methods?

A: Yes. The tabular method can be viewed as a special case of the function approximation method. The related details can be found in Box 8.2.

 Q: What is the stationary distribution and why is it important?

A: The stationary distribution describes the long-term behavior of a Markov decision process. More specifically, after the agent executes a given policy for a sufficiently long period, the probability of the agent visiting a state can be described by this stationary distribution. More information can be found in Box 8.1.

The reason why this concept emerges in this chapter is that it is necessary for defining a valid objective function. In particular, the objective function involves the probability distribution of the states, which is usually selected as the stationary distribution. The stationary distribution is important not only for the value approximation method but also for the policy gradient method, which will be introduced in Chapter 9.

 Q: What are the advantages and disadvantages of the linear function approximation method?

A: Linear function approximation is the simplest case whose theoretical properties can be thoroughly analyzed. However, the approximation ability of this method is limited. It is also nontrivial to select appropriate feature vectors for complex tasks. By contrast, artificial neural networks can be used to approximate values as black-box universal nonlinear approximators, which are more friendly to use. Nevertheless, it is still meaningful to study the linear case to better grasp the idea of the function approximation method. Moreover, the linear case is powerful in the sense that the tabular method can be viewed as a special linear case (Box 8.2).

 Q: Why does deep Q-learning require experience replay?

A: The reason lies in the objective function in (8.37). In particular, to well define the objective function, we must specify the probability distributions of $S , A , R , S ^ { \prime }$ . The distributions of $R$ and $S ^ { \prime }$ are determined by the system model once $( S , A )$ is given. The simplest way to describe the distribution of the state-action pair $( S , A )$ is to assume it to be uniformly distributed. However, the state-action samples may not be uniformly distributed in practice since they are generated as a sequence by the behavior policy. It is necessary to break the correlation between the samples in the sequence to satisfy the assumption of uniform distribution. To do this, we can use the experience replay technique by uniformly drawing samples from the replay buffer. A benefit of experience replay is that each experience sample may be used multiple times, which can increase the data efficiency.

 Q: Can tabular Q-learning use experience replay?

A: Although tabular Q-learning does not require experience replay, it can also use experience relay without encountering problems. That is because Q-learning has no requirements about how the samples are obtained due to its off-policy attribute. One benefit of using experience replay is that the samples can be used repeatedly and hence more efficiently.

Q: Why does deep Q-learning require two networks?

A: The fundamental reason is to simplify the calculation of the gradient of (8.37). Since $w$ appears not only in ${ \hat { q } } ( S , A , w )$ but also in $\begin{array} { r } { R + \gamma \operatorname* { m a x } _ { a \in \mathcal { A } ( S ^ { \prime } ) } \hat { q } ( S ^ { \prime } , a , w ) } \end{array}$ , it is nontrivial to calculate the gradient with respect to $w$ . On the one hand, if we fix $w$ in $\begin{array} { r } { R + \gamma \operatorname* { m a x } _ { a \in \mathcal { A } ( S ^ { \prime } ) } \hat { q } ( S ^ { \prime } , a , w ) } \end{array}$ , the gradient can be easily calculated as shown in (8.38). This gradient suggests that two networks should be maintained. The main network’s parameter is updated in every iteration. The target network’s parameter is fixed within a certain period. On the other hand, the target network’s parameter cannot be fixed forever. It should be updated every certain number of iterations.

Q: When an artificial neural network is used as a nonlinear function approximator, how should we update its parameter?

A: It must be noted that we should not directly update the parameter vector by using, for example, (8.36). Instead, we should follow the network training procedure to update the parameter. This procedure can be realized based on neural network training toolkits, which are currently mature and widely available.

# Chapter 9

# Policy Gradient Methods

![](images/974d5dff808cbf4f316a4df54d1ff9946dab6781e45fb94c5808ba8b0afcce8c.jpg)  
Figure 9.1: Where we are in this book.

The idea of function approximation can be applied not only to represent state/action values, as introduced in Chapter 8, but also to represent policies, as introduced in this chapter. So far in this book, policies have been represented by tables: the action probabilities of all states are stored in a table (e.g., Table 9.1). In this chapter, we show that policies can be represented by parameterized functions denoted as $\pi ( a | s , \theta )$ , where $\theta \in \mathbb { R } ^ { m }$ is a parameter vector. It can also be written in other forms such as $\pi _ { \boldsymbol { \theta } } ( a | \boldsymbol { s } )$ , $\pi _ { \boldsymbol { \theta } } ( \boldsymbol { a } , \boldsymbol { s } )$ , or $\pi ( \boldsymbol { a } , \boldsymbol { s } , \boldsymbol { \theta } )$ .

When policies are represented as functions, optimal policies can be obtained by optimizing certain scalar metrics. Such a method is called policy gradient. The policy

Table 9.1: A tabular representation of a policy. There are nine states and five actions for each state.   

<table><tr><td></td><td>a1</td><td>a2</td><td>a3</td><td>a4</td><td>a5</td></tr><tr><td>s1</td><td>π(a1|s1)</td><td>π(a2|s1)</td><td>π(a3|s1)</td><td>π(a4|s1)</td><td>π(a5|s1)</td></tr><tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr><tr><td>s9</td><td>π(a1|s9)</td><td>π(a2|s9)</td><td>π(a3|s9)</td><td>π(a4|s9)</td><td>π(a5|s9)</td></tr></table>

![](images/9aff41e1cd69a581a76d600214a1293a4416e3ca974610e5b2955dfd524acaa3.jpg)  
(a)

![](images/0b1fa65b0e5bcbf6f410da86c0f56215e26b46b1ad4ceeb289188823cb027f29.jpg)  
(b)   
Figure 9.2: Function representations of policies. The functions may have different structures.

gradient method is a big step forward in this book because it is policy-based. By contrast, all the previous chapters in this book discuss value-based methods. The advantages of the policy gradient method are numerous. For example, it is more efficient for handling large state/action spaces. It has stronger generalization abilities and hence is more efficient in terms of sample usage.

# 9.1 Policy representation: From table to function

When the representation of a policy is switched from a table to a function, it is necessary to clarify the difference between the two representation methods.

First, how to define optimal policies? When represented as a table, a policy is defined as optimal if it can maximize every state value. When represented by a function, a policy is defined as optimal if it can maximize certain scalar metrics.   
Second, how to update a policy? When represented by a table, a policy can be updated by directly changing the entries in the table. When represented by a parameterized function, a policy can no longer be updated in this way. Instead, it can only be updated by changing the parameter $\theta$ .   
Third, how to retrieve the probability of an action? In the tabular case, the probability of an action can be directly obtained by looking up the corresponding entry in the table. In the case of function representation, we need to input $( s , a )$ into the function to calculate its probability (see Figure 9.2(a)). Depending on the structure of the function, we can also input a state and then output the probabilities of all actions (see Figure 9.2(b)).

The basic idea of the policy gradient method is summarized below. Suppose that $J ( \theta )$ is a scalar metric. Optimal policies can be obtained by optimizing this metric via the gradient-based algorithm:

$$
\theta_ {t + 1} = \theta_ {t} + \alpha \nabla_ {\theta} J (\theta_ {t}),
$$

where $\nabla _ { \boldsymbol { \theta } } J$ is the gradient of $J$ with respect to $\theta$ , $t$ is the time step, and $\alpha$ is the optimization rate.

With this basic idea, we will answer the following three questions in the remainder of this chapter.

What metrics should be used? (Section 9.2).   
 How to calculate the gradients of the metrics? (Section 9.3)   
 How to use experience samples to calculate the gradients? (Section 9.4)

# 9.2 Metrics for defining optimal policies

If a policy is represented by a function, there are two types of metrics for defining optimal policies. One is based on state values and the other is based on immediate rewards.

# Metric 1: Average state value

The first metric is the average state value or simply called the average value. It is defined as

$$
\bar {v} _ {\pi} = \sum_ {s \in \mathcal {S}} d (s) v _ {\pi} (s),
$$

where $d ( s )$ is the weight of state $s$ . It satisfies $d ( s ) \geq 0$ for any $s \in S$ and $\begin{array} { r } { \sum _ { s \in \mathcal { S } } d ( s ) = 1 } \end{array}$ . Therefore, we can interpret $d ( s )$ as a probability distribution of $s$ . Then, the metric can be written as

$$
\bar {v} _ {\pi} = \mathbb {E} _ {S \sim d} [ v _ {\pi} (S) ].
$$

How to select the distribution $d$ ? This is an important question. There are two cases.

The first and simplest case is that $d$ is independent of the policy $\pi$ . In this case, we specifically denote $d$ as $d _ { 0 }$ and $v _ { \pi }$ as $\bar { v } _ { \pi } ^ { 0 }$ to indicate that the distribution is independent of the policy. One case is to treat all the states equally important and select $d _ { 0 } ( s ) =$ $1 / | S |$ . Another case is when we are only interested in a specific state $s _ { 0 }$ (e.g., the agent always starts from $s _ { 0 }$ ). In this case, we can design

$$
d _ {0} (s _ {0}) = 1, \quad d _ {0} (s \neq s _ {0}) = 0.
$$

The second case is that $d$ is dependent on the policy $\pi$ . In this case, it is common to select $d$ as $d _ { \pi }$ , which is the stationary distribution under $\pi$ . One basic property of $d _ { \pi }$ is that it satisfies

$$
d _ {\pi} ^ {T} P _ {\pi} = d _ {\pi} ^ {T},
$$

where $P _ { \pi }$ is the state transition probability matrix. More information about the stationary distribution can be found in Box 8.1.

The interpretation of selecting $d _ { \pi }$ is as follows. The stationary distribution reflects the long-term behavior of a Markov decision process under a given policy. If one state is frequently visited in the long term, it is more important and deserves a higher weight; if a state is rarely visited, then its importance is low and deserves a lower weight.

As its name suggests, $v _ { \pi }$ is a weighted average of the state values. Different values of $\theta$ lead to different values of $v _ { \pi }$ . Our ultimate goal is to find an optimal policy (or equivalently an optimal $\theta$ ) to maximize $v _ { \pi }$ .

We next introduce another two important equivalent expressions of $v _ { \pi }$ .

Suppose that an agent collects rewards $\{ R _ { t + 1 } \} _ { t = 0 } ^ { \infty }$ by following a given policy $\pi ( \theta )$ . Readers may often see the following metric in the literature:

$$
J (\theta) = \lim  _ {n \rightarrow \infty} \mathbb {E} \left[ \sum_ {t = 0} ^ {n} \gamma^ {t} R _ {t + 1} \right] = \mathbb {E} \left[ \sum_ {t = 0} ^ {\infty} \gamma^ {t} R _ {t + 1} \right]. \tag {9.1}
$$

This metric may be nontrivial to interpret at first glance. In fact, it is equal to $v _ { \pi }$ . To see that, we have

$$
\begin{array}{l} \mathbb {E} \left[ \sum_ {t = 0} ^ {\infty} \gamma^ {t} R _ {t + 1} \right] = \sum_ {s \in \mathcal {S}} d (s) \mathbb {E} \left[ \sum_ {t = 0} ^ {\infty} \gamma^ {t} R _ {t + 1} | S _ {0} = s \right] \\ = \sum_ {s \in \mathcal {S}} d (s) v _ {\pi} (s) \\ = \bar {v} _ {\pi}. \\ \end{array}
$$

The first equality in the above equation is due to the law of total expectation. The second equality is by the definition of state values.

 The metric $v _ { \pi }$ can also be rewritten as the inner product of two vectors. In particular, let

$$
v _ {\pi} = [ \dots , v _ {\pi} (s), \dots ] ^ {T} \in \mathbb {R} ^ {| \mathcal {S} |},
$$

$$
d = [ \ldots , d (s), \ldots ] ^ {T} \in \mathbb {R} ^ {| \mathcal {S} |}.
$$

Then, we have

$$
\bar {v} _ {\pi} = d ^ {T} v _ {\pi}.
$$

This expression will be useful when we analyze its gradient.

# Metric 2: Average reward

The second metric is the average one-step reward or simply called the average reward [2, 64, 65]. In particular, it is defined as

$$
\begin{array}{l} \bar {r} _ {\pi} \doteq \sum_ {s \in \mathcal {S}} d _ {\pi} (s) r _ {\pi} (s) \\ = \mathbb {E} _ {S \sim d _ {\pi}} [ r _ {\pi} (S) ], \tag {9.2} \\ \end{array}
$$

where $d _ { \pi }$ is the stationary distribution and

$$
r _ {\pi} (s) \doteq \sum_ {a \in \mathcal {A}} \pi (a | s, \theta) r (s, a) = \mathbb {E} _ {A \sim \pi (s, \theta)} [ r (s, A) | s ] \tag {9.3}
$$

is the expectation of the immediate rewards. Here, $\begin{array} { r } { r ( s , a ) \dot { = } \mathbb { E } [ R | s , a ] = \sum _ { r } r p ( r | s , a ) } \end{array}$ . We next present another two important equivalent expressions of $r _ { \pi }$ .

 Suppose that the agent collects rewards $\{ R _ { t + 1 } \} _ { t = 0 } ^ { \infty }$ by following a given policy $\pi ( \theta )$ . A common metric that readers may often see in the literature is

$$
J (\theta) = \lim  _ {n \rightarrow \infty} \frac {1}{n} \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} \right]. \tag {9.4}
$$

It may seem nontrivial to interpret this metric at first glance. In fact, it is equal to $r _ { \pi }$ :

$$
\lim  _ {n \rightarrow \infty} \frac {1}{n} \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} \right] = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) r _ {\pi} (s) = \bar {r} _ {\pi}. \tag {9.5}
$$

The proof of (9.5) is given in Box 9.1.

The average reward $r _ { \pi }$ in (9.2) can also be written as the inner product of two vectors. In particular, let

$$
r _ {\pi} = [ \dots , r _ {\pi} (s), \dots ] ^ {T} \in \mathbb {R} ^ {| \mathcal {S} |},
$$

$$
d _ {\pi} = [ \dots , d _ {\pi} (s), \dots ] ^ {T} \in \mathbb {R} ^ {| \mathcal {S} |},
$$

where $r _ { \pi } ( s )$ is defined in (9.3). Then, it is clear that

$$
\bar {r} _ {\pi} = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) r _ {\pi} (s) = d _ {\pi} ^ {T} r _ {\pi}.
$$

This expression will be useful when we derive its gradient.

# Box 9.1: Proof of (9.5)

Step 1: We first prove that the following equation is valid for any starting state $s _ { 0 } \in S$ :

$$
\bar {r} _ {\pi} = \lim  _ {n \rightarrow \infty} \frac {1}{n} \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} | S _ {0} = s _ {0} \right]. \tag {9.6}
$$

To do that, we notice

$$
\begin{array}{l} \lim _ {n \to \infty} \frac {1}{n} \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} | S _ {0} = s _ {0} \right] = \lim _ {n \to \infty} \frac {1}{n} \sum_ {t = 0} ^ {n - 1} \mathbb {E} \left[ R _ {t + 1} | S _ {0} = s _ {0} \right] \\ = \lim  _ {t \rightarrow \infty} \mathbb {E} \left[ R _ {t + 1} \mid S _ {0} = s _ {0} \right], \tag {9.7} \\ \end{array}
$$

where the last equality is due to the property of the Cesaro mean (also called the Cesaro summation). In particular, if $\{ a _ { k } \} _ { k = 1 } ^ { \infty }$ is a convergent sequence such that $\scriptstyle \operatorname* { l i m } _ { k \to \infty } a _ { k }$ exists, then $\textstyle { \bigl \{ 1 / n \sum _ { k = 1 } ^ { n } a _ { k } \bigr \} _ { n = 1 } ^ { \infty } }$ is also a convergent sequence such that $\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } 1 / n \sum _ { k = 1 } ^ { n } a _ { k } = \operatorname* { l i m } _ { k \to \infty } a _ { k } } \end{array}$ .

We next examine $\mathbb { E } \left[ R _ { t + 1 } | S _ { 0 } = s _ { 0 } \right]$ in (9.7) more closely. By the law of total expectation, we have

$$
\begin{array}{l} \mathbb {E} \left[ R _ {t + 1} | S _ {0} = s _ {0} \right] = \sum_ {s \in \mathcal {S}} \mathbb {E} \left[ R _ {t + 1} | S _ {t} = s, S _ {0} = s _ {0} \right] p ^ {(t)} (s | s _ {0}) \\ = \sum_ {s \in \mathcal {S}} \mathbb {E} \left[ R _ {t + 1} \mid S _ {t} = s \right] p ^ {(t)} (s \mid s _ {0}) \\ = \sum_ {s \in \mathcal {S}} r _ {\pi} (s) p ^ {(t)} (s | s _ {0}), \\ \end{array}
$$

where $p ^ { ( t ) } ( s | s _ { 0 } )$ denotes the probability of transitioning from $s _ { 0 }$ to $s$ using exactly $t$ steps. The second equality in the above equation is due to the Markov memoryless property: the reward obtained at the next time step depends only on the current state rather than the previous ones.

Note that

$$
\lim _ {t \to \infty} p ^ {(t)} (s | s _ {0}) = d _ {\pi} (s)
$$

by the definition of the stationary distribution. As a result, the starting state $s _ { 0 }$ does not matter. Then, we have

$$
\lim _ {t \to \infty} \mathbb {E} \left[ R _ {t + 1} | S _ {0} = s _ {0} \right] = \lim _ {t \to \infty} \sum_ {s \in \mathcal {S}} r _ {\pi} (s) p ^ {(t)} (s | s _ {0}) = \sum_ {s \in \mathcal {S}} r _ {\pi} (s) d _ {\pi} (s) = \bar {r} _ {\pi}.
$$

Substituting the above equation into (9.7) gives (9.6).

Step 2: Consider an arbitrary state distribution $d$ . By the law of total expectation, we have

$$
\begin{array}{l} \lim _ {n \to \infty} \frac {1}{n} \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} \right] = \lim _ {n \to \infty} \frac {1}{n} \sum_ {s \in \mathcal {S}} d (s) \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} | S _ {0} = s \right] \\ = \sum_ {s \in \mathcal {S}} d (s) \lim _ {n \to \infty} \frac {1}{n} \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} | S _ {0} = s \right]. \\ \end{array}
$$

Since (9.6) is valid for any starting state, substituting (9.6) into the above equation yields

$$
\lim _ {n \to \infty} \frac {1}{n} \mathbb {E} \left[ \sum_ {t = 0} ^ {n - 1} R _ {t + 1} \right] = \sum_ {s \in \mathcal {S}} d (s) \bar {r} _ {\pi} = \bar {r} _ {\pi}.
$$

The proof is complete.

# Some remarks

Table 9.2: Summary of the different but equivalent expressions of $\bar { v } _ { \pi }$ and $\bar { r } _ { \pi }$ .   

<table><tr><td>Metric</td><td>Expression 1</td><td>Expression 2</td><td>Expression 3</td></tr><tr><td>vπ</td><td>∑s∈Sd(s)vπ(s)</td><td>ESS~d[vπ(S)]</td><td>limn→∞E[∑t=0nγtRt+1]</td></tr><tr><td>rπ</td><td>∑s∈Sdπ(s)rπ(s)</td><td>ESS~dπ[rπ(S)]</td><td>limn→∞1/nE[∑t=0n-1Rt+1]</td></tr></table>

Up to now, we have introduced two types of metrics: $v _ { \pi }$ and $r _ { \pi }$ . Each metric has several different but equivalent expressions. They are summarized in Table 9.2. We sometimes use $v _ { \pi }$ to specifically refer to the case where the state distribution is the stationary distribution $d _ { \pi }$ and use $\bar { v } _ { \pi } ^ { 0 }$ to refer to the case where $d _ { 0 }$ is independent of $\pi$ . Some remarks about the metrics are given below.

All these metrics are functions of $\pi$ . Since $\pi$ is parameterized by $\theta$ , these metrics are functions of $\theta$ . In other words, different values of $\theta$ can generate different metric

values. Therefore, we can search for the optimal values of $\theta$ to maximize these metrics. This is the basic idea of policy gradient methods.

 The two metrics $v _ { \pi }$ and $r _ { \pi }$ are equivalent in the discounted case where $\gamma < 1$ . In particular, it can be shown that

$$
\bar {r} _ {\pi} = (1 - \gamma) \bar {v} _ {\pi}.
$$

The above equation indicates that these two metrics can be simultaneously maximized. The proof of this equation is given later in Lemma 9.1.

# 9.3 Gradients of the metrics

Given the metrics introduced in the last section, we can use gradient-based methods to maximize them. To do that, we need to first calculate the gradients of these metrics. The most important theoretical result in this chapter is the following theorem.

Theorem 9.1 (Policy gradient theorem). The gradient of $J ( \theta )$ is

$$
\nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} \eta (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a), \tag {9.8}
$$

where η is a state distribution and $\nabla _ { \boldsymbol { \theta } } \pi$ is the gradient of π with respect to θ. Moreover, (9.8) has a compact form expressed in terms of expectation:

$$
\nabla_ {\theta} J (\theta) = \mathbb {E} _ {S \sim \eta , A \sim \pi (S, \theta)} \Big [ \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \Big ], \tag {9.9}
$$

where ln is the natural logarithm.

Some important remarks about Theorem 9.1 are given below.

It should be noted that Theorem 9.1 is a summary of the results in Theorem 9.2, Theorem 9.3, and Theorem 9.5. These three theorems address different scenarios involving different metrics and discounted/undiscounted cases. The gradients in these scenarios all have similar expressions and hence are summarized in Theorem 9.1. The specific expressions of $J ( \theta )$ and $\eta$ are not given in Theorem 9.1 and can be found in Theorem 9.2, Theorem 9.3, and Theorem 9.5. In particular, $J ( \theta )$ could be $\bar { v } _ { \pi } ^ { 0 }$ , $v _ { \pi }$ , or $r _ { \pi }$ . The equality in (9.8) may become a strict equality or an approximation. The distribution $\eta$ also varies in different scenarios.

The derivation of the gradients is the most complicated part of the policy gradient method. For many readers, it is sufficient to be familiar with the result in Theorem 9.1 without knowing the proof. The derivation details presented in the rest of this section are mathematically intensive. Readers are suggested to study selectively based on their interests.

 The expression in (9.9) is more favorable than (9.8) because it is expressed as an expectation. We will show in Section 9.4 that this true gradient can be approximated by a stochastic gradient.

Why can (9.8) be expressed as (9.9)? The proof is given below. By the definition of expectation, (9.8) can be rewritten as

$$
\begin{array}{l} \nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} \eta (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) \\ = \mathbb {E} _ {S \sim \eta} \left[ \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | S, \theta) q _ {\pi} (S, a) \right]. \tag {9.10} \\ \end{array}
$$

Furthermore, the gradient of $\ln \pi ( a | s , \theta )$ is

$$
\nabla_ {\theta} \ln \pi (a | s, \theta) = \frac {\nabla_ {\theta} \pi (a | s , \theta)}{\pi (a | s , \theta)}.
$$

It follows that

$$
\nabla_ {\theta} \pi (a | s, \theta) = \pi (a | s, \theta) \nabla_ {\theta} \ln \pi (a | s, \theta). \tag {9.11}
$$

Substituting (9.11) into (9.10) gives

$$
\begin{array}{l} \nabla_ {\theta} J (\theta) = \mathbb {E} \left[ \sum_ {a \in \mathcal {A}} \pi (a | S, \theta) \nabla_ {\theta} \ln \pi (a | S, \theta) q _ {\pi} (S, a) \right] \\ = \mathbb {E} _ {S \sim \eta , A \sim \pi (S, \theta)} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \right]. \\ \end{array}
$$

 It is notable that $\pi ( a | s , \theta )$ must be positive for all $( s , a )$ to ensure that $\ln \pi ( a | s , \theta )$ is valid. This can be achieved by using softmax functions:

$$
\pi (a | s, \theta) = \frac {e ^ {h (s , a , \theta)}}{\sum_ {a ^ {\prime} \in \mathcal {A}} e ^ {h (s , a ^ {\prime} , \theta)}}, \quad a \in \mathcal {A}, \tag {9.12}
$$

where $h ( s , a , \theta )$ is a function indicating the preference for selecting $a$ at $s$ . The policy in (9.12) satisfies $\pi ( a | s , \theta ) \in ( 0 , 1 )$ and $\textstyle \sum _ { a \in { \mathcal { A } } } \pi ( a | s , \theta ) = 1$ for any $s \in S$ . This policy can be realized by a neural network. The input of the network is $s$ . The output layer is a softmax layer so that the network outputs $\pi ( a | s , \theta )$ for all $a$ and the sum of the outputs is equal to 1. See Figure 9.2(b) for an illustration.

Since $\pi ( a | s , \theta ) > 0$ for all $a$ , the policy is stochastic and hence exploratory. The policy does not directly tell which action to take. Instead, the action should be generated according to the probability distribution of the policy.

# 9.3.1 Derivation of the gradients in the discounted case

We next derive the gradients of the metrics in the discounted case where $\gamma \in ( 0 , 1 )$ . The state value and action value in the discounted case are defined as

$$
v _ {\pi} (s) = \mathbb {E} [ R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \dots | S _ {t} = s ],
$$

$$
q _ {\pi} (s, a) = \mathbb {E} \left[ R _ {t + 1} + \gamma R _ {t + 2} + \gamma^ {2} R _ {t + 3} + \dots \mid S _ {t} = s, A _ {t} = a \right].
$$

It holds that $\begin{array} { r } { v _ { \pi } ( s ) = \sum _ { a \in \mathcal { A } } \pi ( a | s , \theta ) q _ { \pi } ( s , a ) } \end{array}$ and the state value satisfies the Bellman equation.

First, we show that $\bar { v } _ { \pi } ( \theta )$ and $\bar { r } _ { \pi } ( \theta )$ are equivalent metrics.

Lemma 9.1 (Equivalence between $\bar { v } _ { \pi } ( \theta )$ and $\bar { r } _ { \pi } ( \theta )$ ). In the discounted case where $\gamma \in$ $( 0 , 1 )$ , it holds that

$$
\bar {r} _ {\pi} = (1 - \gamma) \bar {v} _ {\pi}. \tag {9.13}
$$

Proof. Note that $\bar { v } _ { \pi } ( \theta ) = d _ { \pi } ^ { I ^ { \prime } } v _ { \pi }$ and $\bar { r } _ { \pi } ( \theta ) = d _ { \pi } ^ { I } r _ { \pi }$ , where $v _ { \pi }$ and $r _ { \pi }$ satisfy the Bellman equation $v _ { \pi } = r _ { \pi } + \gamma P _ { \pi } v _ { \pi }$ . Multiplying $d _ { \pi } ^ { I ^ { \prime } }$ on both sides of the Bellman equation yields

$$
\bar {v} _ {\pi} = \bar {r} _ {\pi} + \gamma d _ {\pi} ^ {T} P _ {\pi} v _ {\pi} = \bar {r} _ {\pi} + \gamma d _ {\pi} ^ {T} v _ {\pi} = \bar {r} _ {\pi} + \gamma \bar {v} _ {\pi},
$$

which implies (9.13).

![](images/2f612f37c10a0d8ae9b30c4742efa402dd41e33f378bb6813d460865a50ef551.jpg)

Second, the following lemma gives the gradient of $v _ { \pi } ( s )$ for any $s$ .

Lemma 9.2 (Gradient of $v _ { \pi } ( s )$ ). In the discounted case, it holds for any $s \in S$ that

$$
\nabla_ {\theta} v _ {\pi} (s) = \sum_ {s ^ {\prime} \in \mathcal {S}} \Pr_ {\pi} \left(s ^ {\prime} \mid s\right) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi \left(a \mid s ^ {\prime}, \theta\right) q _ {\pi} \left(s ^ {\prime}, a\right), \tag {9.14}
$$

where

$$
\operatorname * {P r} _ {\pi} (s ^ {\prime} | s) \doteq \sum_ {k = 0} ^ {\infty} \gamma^ {k} [ P _ {\pi} ^ {k} ] _ {s s ^ {\prime}} = \left[ (I _ {n} - \gamma P _ {\pi}) ^ {- 1} \right] _ {s s ^ {\prime}}
$$

is the discounted total probability of transitioning from s to $s ^ { \prime }$ under policy $\pi$ . Here, $[ \cdot ] _ { s s ^ { \prime } }$ denotes the entry in the sth row and s0th column, and $[ P _ { \pi } ^ { k } ] _ { s s ^ { \prime } }$ is the probability of transitioning from s to $s ^ { \prime }$ using exactly $k$ steps under $\pi$ .

# Box 9.2: Proof of Lemma 9.2

First, for any $s \in S$ , it holds that

$$
\begin{array}{l} \nabla_ {\theta} v _ {\pi} (s) = \nabla_ {\theta} \left[ \sum_ {a \in \mathcal {A}} \pi (a | s, \theta) q _ {\pi} (s, a) \right] \\ = \sum_ {a \in \mathcal {A}} \left[ \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) + \pi (a | s, \theta) \nabla_ {\theta} q _ {\pi} (s, a) \right], \tag {9.15} \\ \end{array}
$$

where $q _ { \pi } ( s , a )$ is the action value given by

$$
q _ {\pi} (s, a) = r (s, a) + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) v _ {\pi} (s ^ {\prime}).
$$

Since $\begin{array} { r } { r ( s , a ) = \sum _ { r } r p ( r | s , a ) } \end{array}$ is independent of $\theta$ , we have

$$
\nabla_ {\theta} q _ {\pi} (s, a) = 0 + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) \nabla_ {\theta} v _ {\pi} (s ^ {\prime}).
$$

Substituting this result into (9.15) yields

$$
\begin{array}{l} \nabla_ {\theta} v _ {\pi} (s) = \sum_ {a \in \mathcal {A}} \left[ \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) + \pi (a | s, \theta) \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) \nabla_ {\theta} v _ {\pi} (s ^ {\prime}) \right] \\ = \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) + \gamma \sum_ {a \in \mathcal {A}} \pi (a | s, \theta) \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} \mid s, a\right) \nabla_ {\theta} v _ {\pi} \left(s ^ {\prime}\right). \tag {9.16} \\ \end{array}
$$

It is notable that $\nabla _ { \boldsymbol { \theta } } v _ { \pi }$ appears on both sides of the above equation. One way to calculate it is to use the unrolling technique [64]. Here, we use another way based on the matrix-vector form, which we believe is more straightforward to understand. In particular, let

$$
u (s) \doteq \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a).
$$

Since

$$
\sum_ {a \in \mathcal {A}} \pi (a | s, \theta) \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) \nabla_ {\theta} v _ {\pi} (s ^ {\prime}) = \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s) \nabla_ {\theta} v _ {\pi} (s ^ {\prime}) = \sum_ {s ^ {\prime} \in \mathcal {S}} [ P _ {\pi} ] _ {s s ^ {\prime}} \nabla_ {\theta} v _ {\pi} (s ^ {\prime}),
$$

equation (9.16) can be written in matrix-vector form as

$$
\underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\pi} (s) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\pi} \in \mathbb {R} ^ {m n}} = \underbrace {\left[ \begin{array}{c} \vdots \\ u (s) \\ \vdots \end{array} \right]} _ {u \in \mathbb {R} ^ {m n}} + \gamma (P _ {\pi} \otimes I _ {m}) \underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\pi} (s ^ {\prime}) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\pi} \in \mathbb {R} ^ {m n}},
$$

which can be written concisely as

$$
\nabla_ {\theta} v _ {\pi} = u + \gamma (P _ {\pi} \otimes I _ {m}) \nabla_ {\theta} v _ {\pi}.
$$

Here, $n = | S |$ , and $m$ is the dimension of the parameter vector $\theta$ . The reason that the Kronecker product $\otimes$ emerges in the equation is that $\nabla _ { \theta } v _ { \pi } ( s )$ is a vector. The above equation is a linear equation of $\nabla _ { \boldsymbol { \theta } } v _ { \pi }$ , which can be solved as

$$
\begin{array}{l} \nabla_ {\theta} v _ {\pi} = \left(I _ {n m} - \gamma P _ {\pi} \otimes I _ {m}\right) ^ {- 1} u \\ = \left(I _ {n} \otimes I _ {m} - \gamma P _ {\pi} \otimes I _ {m}\right) ^ {- 1} u \\ = \left[ \left(I _ {n} - \gamma P _ {\pi}\right) ^ {- 1} \otimes I _ {m} \right] u. \tag {9.17} \\ \end{array}
$$

For any state $s$ , it follows from (9.17) that

$$
\begin{array}{l} \nabla_ {\theta} v _ {\pi} (s) = \sum_ {s ^ {\prime} \in \mathcal {S}} \left[ \left(I _ {n} - \gamma P _ {\pi}\right) ^ {- 1} \right] _ {s s ^ {\prime}} u (s ^ {\prime}) \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} \left[ \left(I _ {n} - \gamma P _ {\pi}\right) ^ {- 1} \right] _ {s s ^ {\prime}} \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s ^ {\prime}, \theta) q _ {\pi} \left(s ^ {\prime}, a\right). \tag {9.18} \\ \end{array}
$$

The quantity $[ ( I _ { n } - \gamma P _ { \pi } ) ^ { - 1 } ] _ { s s ^ { \prime } }$ has a clear probabilistic interpretation. In particular, since $( I _ { n } - \gamma P _ { \pi } ) ^ { - 1 } = I + \gamma P _ { \pi } + \gamma ^ { 2 } P _ { \pi } ^ { 2 } + \cdot \cdot \cdot$ , we have

$$
\left[ \left(I _ {n} - \gamma P _ {\pi}\right) ^ {- 1} \right] _ {s s ^ {\prime}} = \left[ I \right] _ {s s ^ {\prime}} + \gamma \left[ P _ {\pi} \right] _ {s s ^ {\prime}} + \gamma^ {2} \left[ P _ {\pi} ^ {2} \right] _ {s s ^ {\prime}} + \dots = \sum_ {k = 0} ^ {\infty} \gamma^ {k} \left[ P _ {\pi} ^ {k} \right] _ {s s ^ {\prime}}.
$$

Note that $[ P _ { \pi } ^ { k } ] _ { s s ^ { \prime } }$ is the probability of transitioning from $s$ to $s ^ { \prime }$ using exactly $k$ steps (see Box 8.1). Therefore, $[ ( I _ { n } - \gamma P _ { \pi } ) ^ { - 1 } ] _ { s s ^ { \prime } }$ is the discounted total probability of transitioning from $s$ to $s ^ { \prime }$ using any number of steps. By denoting $\left[ ( I _ { n } - \gamma P _ { \pi } ) ^ { - 1 } \right] _ { s s ^ { \prime } } \doteq$ $\operatorname* { P r } _ { \pi } ( s ^ { \prime } | s )$ , equation (9.18) becomes (9.14).

With the results in Lemma 9.2, we are ready to derive the gradient of $\bar { v } _ { \pi } ^ { 0 }$ .

Theorem 9.2 (Gradient of $\bar { v } _ { \pi } ^ { 0 }$ in the discounted case). In the discounted case where $\gamma \in ( 0 , 1 )$ , the gradient of $\bar { v } _ { \pi } ^ { 0 } = d _ { 0 } ^ { I ^ { \prime } } v _ { \pi }$ is

$$
\nabla_ {\theta} \bar {v} _ {\pi} ^ {0} = \mathbb {E} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \right],
$$

where $S \sim \rho _ { \pi }$ and $A \sim \pi ( S , \theta )$ . Here, the state distribution $\rho _ { \pi }$ is

$$
\rho_ {\pi} (s) = \sum_ {s ^ {\prime} \in \mathcal {S}} d _ {0} \left(s ^ {\prime}\right) \Pr_ {\pi} \left(s \mid s ^ {\prime}\right), \quad s \in \mathcal {S}, \tag {9.19}
$$

where $\begin{array} { r } { \operatorname* { P r } _ { \pi } ( s | s ^ { \prime } ) = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } [ P _ { \pi } ^ { k } ] _ { s ^ { \prime } s } = [ ( I - \gamma P _ { \pi } ) ^ { - 1 } ] _ { s ^ { \prime } s } } \end{array}$ is the discounted total probability of

transitioning from $s ^ { \prime }$ to s under policy $\pi$ .

# Box 9.3: Proof of Theorem 9.2

Since $d _ { 0 } ( s )$ is independent of $\pi$ , we have

$$
\nabla_ {\theta} \bar {v} _ {\pi} ^ {0} = \nabla_ {\theta} \sum_ {s \in \mathcal {S}} d _ {0} (s) v _ {\pi} (s) = \sum_ {s \in \mathcal {S}} d _ {0} (s) \nabla_ {\theta} v _ {\pi} (s).
$$

Substituting the expression of $\nabla _ { \theta } v _ { \pi } ( s )$ given in Lemma 9.2 into the above equation yields

$$
\begin{array}{l} \nabla_ {\theta} \bar {v} _ {\pi} ^ {0} = \sum_ {s \in \mathcal {S}} d _ {0} (s) \nabla_ {\theta} v _ {\pi} (s) = \sum_ {s \in \mathcal {S}} d _ {0} (s) \sum_ {s ^ {\prime} \in \mathcal {S}} \operatorname * {P r} _ {\pi} (s ^ {\prime} | s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s ^ {\prime}, \theta) q _ {\pi} (s ^ {\prime}, a) \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} \left(\sum_ {s \in \mathcal {S}} d _ {0} (s) \Pr_ {\pi} (s ^ {\prime} | s)\right) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s ^ {\prime}, \theta) q _ {\pi} (s ^ {\prime}, a) \\ \dot {=} \sum_ {s ^ {\prime} \in \mathcal {S}} \rho_ {\pi} (s ^ {\prime}) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s ^ {\prime}, \theta) q _ {\pi} (s ^ {\prime}, a) \\ = \sum_ {s \in \mathcal {S}} \rho_ {\pi} (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) \qquad (\text {c h a n g e} s ^ {\prime} \text {t o} s) \\ = \sum_ {s \in \mathcal {S}} \rho_ {\pi} (s) \sum_ {a \in \mathcal {A}} \pi (a | s, \theta) \nabla_ {\theta} \ln \pi (a | s, \theta) q _ {\pi} (s, a) \\ = \mathbb {E} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \right], \\ \end{array}
$$

where $S \sim \rho _ { \pi }$ and $A \sim \pi ( S , \theta )$ . The proof is complete.

With Lemma 9.1 and Lemma 9.2, we can derive the gradients of $\bar { r } _ { \pi }$ and $v _ { \pi }$ .

Theorem 9.3 (Gradients of $r _ { \pi }$ and $v _ { \pi }$ in the discounted case). In the discounted case where $\gamma \in ( 0 , 1 )$ , the gradients of $r _ { \pi }$ and $v _ { \pi }$ are

$$
\begin{array}{l} \nabla_ {\theta} \bar {r} _ {\pi} = (1 - \gamma) \nabla_ {\theta} \bar {v} _ {\pi} \approx \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) \\ = \mathbb {E} \big [ \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \big ], \\ \end{array}
$$

where $S \sim d _ { \pi }$ and $A \sim \pi ( S , \theta )$ . Here, the approximation is more accurate when $\gamma$ is closer to 1.

# Box 9.4: Proof of Theorem 9.3

It follows from the definition of $v _ { \pi }$ that

$$
\begin{array}{l} \nabla_ {\theta} \bar {v} _ {\pi} = \nabla_ {\theta} \sum_ {s \in \mathcal {S}} d _ {\pi} (s) v _ {\pi} (s) \\ = \sum_ {s \in S} \nabla_ {\theta} d _ {\pi} (s) v _ {\pi} (s) + \sum_ {s \in S} d _ {\pi} (s) \nabla_ {\theta} v _ {\pi} (s). \tag {9.20} \\ \end{array}
$$

This equation contains two terms. On the one hand, substituting the expression of $\nabla _ { \boldsymbol { \theta } } v _ { \pi }$ given in (9.17) into the second term gives

$$
\begin{array}{l} \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \nabla_ {\theta} v _ {\pi} (s) = (d _ {\pi} ^ {T} \otimes I _ {m}) \nabla_ {\theta} v _ {\pi} \\ = \left(d _ {\pi} ^ {T} \otimes I _ {m}\right) \left[ \left(I _ {n} - \gamma P _ {\pi}\right) ^ {- 1} \otimes I _ {m} \right] u \\ = \left[ d _ {\pi} ^ {T} \left(I _ {n} - \gamma P _ {\pi}\right) ^ {- 1} \right] \otimes I _ {m} u. \tag {9.21} \\ \end{array}
$$

It is noted that

$$
d _ {\pi} ^ {T} (I _ {n} - \gamma P _ {\pi}) ^ {- 1} = \frac {1}{1 - \gamma} d _ {\pi} ^ {T},
$$

which can be easily verified by multiplying $\left( I _ { n } - \gamma P _ { \pi } \right)$ on both sides of the equation. Therefore, (9.21) becomes

$$
\begin{array}{l} \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \nabla_ {\theta} v _ {\pi} (s) = \frac {1}{1 - \gamma} d _ {\pi} ^ {T} \otimes I _ {m} u \\ = \frac {1}{1 - \gamma} \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a). \\ \end{array}
$$

On the other hand, the first term of (9.20) involves $\nabla _ { \theta } d _ { \pi }$ . However, since the second term contains $\frac { 1 } { 1 - \gamma }$ , the second term becomes dominant, and the first term becomes negligible when $\gamma  1$ . Therefore,

$$
\nabla_ {\theta} \bar {v} _ {\pi} \approx \frac {1}{1 - \gamma} \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a).
$$

Furthermore, it follows from $\bar { r } _ { \pi } = ( 1 - \gamma ) \bar { v } _ { \pi }$ that

$$
\begin{array}{l} \nabla_ {\theta} \bar {r} _ {\pi} = (1 - \gamma) \nabla_ {\theta} \bar {v} _ {\pi} \approx \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) \\ = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \sum_ {a \in \mathcal {A}} \pi (a | s, \theta) \nabla_ {\theta} \ln \pi (a | s, \theta) q _ {\pi} (s, a) \\ = \mathbb {E} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \right]. \\ \end{array}
$$

The approximation in the above equation requires that the first term does not go to infinity when $\gamma \to 1$ . More information can be found in [66, Section 4].

# 9.3.2 Derivation of the gradients in the undiscounted case

We next show how to calculate the gradients of the metrics in the undiscounted case where $\gamma = 1$ . Readers may wonder why we suddenly start considering the undiscounted case while we have only considered the discounted case so far in this book. In fact, the definition of the average reward $r _ { \pi }$ is valid for both discounted and undiscounted cases. While the gradient of $r _ { \pi }$ in the discounted case is an approximation, we will see that its gradient in the undiscounted case is more elegant.

# State values and the Poisson equation

In the undiscounted case, it is necessary to redefine state and action values. Since the undiscounted sum of the rewards, $\mathbb { E } [ R _ { t + 1 } + R _ { t + 2 } + R _ { t + 3 } + \dots | S _ { t } = s ]$ , may diverge, the state and action values are defined in a special way [64]:

$$
v _ {\pi} (s) \doteq \mathbb {E} \left[ \left(R _ {t + 1} - \bar {r} _ {\pi}\right) + \left(R _ {t + 2} - \bar {r} _ {\pi}\right) + \left(R _ {t + 3} - \bar {r} _ {\pi}\right) + \dots \mid S _ {t} = s \right],
$$

$$
q _ {\pi} (s, a) \doteq \mathbb {E} \big [ (R _ {t + 1} - \bar {r} _ {\pi}) + (R _ {t + 2} - \bar {r} _ {\pi}) + (R _ {t + 3} - \bar {r} _ {\pi}) + \ldots | S _ {t} = s, A _ {t} = a \big ],
$$

where $r _ { \pi }$ is the average reward, which is determined when $\pi$ is given. There are different names for $v _ { \pi } ( s )$ in the literature such as the differential reward [65] or bias [2, Section 8.2.1]. It can be verified that the state value defined above satisfies the following Bellman-like equation:

$$
v _ {\pi} (s) = \sum_ {a} \pi (a | s, \theta) \left[ \sum_ {r} p (r | s, a) (r - \bar {r} _ {\pi}) + \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) v _ {\pi} (s ^ {\prime}) \right]. \tag {9.22}
$$

Since $\begin{array} { r } { v _ { \pi } ( s ) \ = \ \sum _ { a \in \mathcal { A } } \pi ( a | s , \theta ) q _ { \pi } ( s , a ) } \end{array}$ , it holds that $\begin{array} { r } { q _ { \pi } ( s , a ) \ = \ \sum _ { r } p ( r | s , a ) ( r - \bar { r } _ { \pi } ) \ + \ } \end{array}$ $\begin{array} { r l } { \sum _ { s ^ { \prime } } p ( s ^ { \prime } | s , a ) v _ { \pi } ( s ^ { \prime } ) } \end{array}$ . The matrix-vector form of (9.22) is

$$
v _ {\pi} = r _ {\pi} - \bar {r} _ {\pi} \mathbf {1} _ {n} + P _ {\pi} v _ {\pi}, \tag {9.23}
$$

where $\mathbf { 1 } _ { n } = [ 1 , \ldots , 1 ] ^ { T } \in \mathbb { R } ^ { n }$ . Equation (9.23) is similar to the Bellman equation and it has a specific name called the Poisson equation [65, 67].

How to solve $v _ { \pi }$ from the Poisson equation? The answer is given in the following theorem.

Theorem 9.4 (Solution of the Poisson equation). Let

$$
v _ {\pi} ^ {*} = \left(I _ {n} - P _ {\pi} + \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) ^ {- 1} r _ {\pi}. \tag {9.24}
$$

Then, $v _ { \pi } ^ { \ast }$ is a solution of the Poisson equation in (9.23). Moreover, any solution of the Poisson equation has the following form:

$$
v _ {\pi} = v _ {\pi} ^ {*} + c \mathbf {1} _ {n},
$$

where $c \in \mathbb { R }$ .

This theorem indicates that the solution of the Poisson equation may not be unique.

# Box 9.5: Proof of Theorem 9.4

We prove using three steps.

 Step 1: Show that $v _ { \pi } ^ { * }$ in (9.24) is a solution of the Poisson equation.

For the sake of simplicity, let

$$
A \doteq I _ {n} - P _ {\pi} + \mathbf {1} _ {n} d _ {\pi} ^ {T}.
$$

Then, $v _ { \pi } ^ { \ast } = A ^ { - 1 } r _ { \pi }$ . The fact that $A$ is invertible will be proven in Step 3. Substituting $v _ { \pi } ^ { \ast } = A ^ { - 1 } r _ { \pi }$ into (9.23) gives

$$
A ^ {- 1} r _ {\pi} = r _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T} r _ {\pi} + P _ {\pi} A ^ {- 1} r _ {\pi}.
$$

This equation is valid as proven below. Recognizing this equation gives $( - A ^ { - 1 } +$ $I _ { n } - { \bf 1 } _ { n } d _ { \pi } ^ { I ^ { \prime } } + P _ { \pi } A ^ { - 1 } ) r _ { \pi } = 0$ , and consequently,

$$
\left(- I _ {n} + A - \mathbf {1} _ {n} d _ {\pi} ^ {T} A + P _ {\pi}\right) A ^ {- 1} r _ {\pi} = 0.
$$

The term in the brackets in the above equation is zero because $- I _ { n } + A - { \bf 1 } _ { n } d _ { \pi } ^ { I ^ { \prime } } A +$ $P _ { \pi } = - I _ { n } + ( I _ { n } - P _ { \pi } + { \bf 1 } _ { n } d _ { \pi } ^ { T } ) - { \bf 1 } _ { n } d _ { \pi } ^ { T } ( I _ { n } - P _ { \pi } + { \bf 1 } _ { n } d _ { \pi } ^ { T } ) + P _ { \pi } = 0$ . Therefore, $v _ { \pi } ^ { \ast }$ in (9.24) is a solution.

Step 2: General expression of the solutions.

Substituting $\bar { r } _ { \pi } = d _ { \pi } ^ { I ^ { \prime } } r _ { \pi }$ into (9.23) gives

$$
v _ {\pi} = r _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T} r _ {\pi} + P _ {\pi} v _ {\pi} \tag {9.25}
$$

and consequently

$$
(I _ {n} - P _ {\pi}) v _ {\pi} = \left(I _ {n} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) r _ {\pi}. \tag {9.26}
$$

It is noted that $I _ { n } - { \cal P } _ { \pi }$ is singular because $( I _ { n } - P _ { \pi } ) \mathbf { 1 } _ { n } = 0$ for any $\pi$ . Therefore, the solution of (9.26) is not unique: if $v _ { \pi } ^ { \ast }$ is a solution, then $v _ { \pi } ^ { * } + x$ is also a solution for any $x \in \mathrm { N u l l } ( I _ { n } - P _ { \pi } )$ . When $P _ { \pi }$ is irreducible, $\mathrm { N u l l } ( I _ { n } - P _ { \pi } ) = \mathrm { s p a n } \{ { \bf 1 } _ { n } \}$ . Then, any solution of the Poisson equation has the expression $v _ { \pi } ^ { * } + c { \bf l } _ { n }$ where $c \in \mathbb { R }$ .

 Step 3: Show that $A = I _ { n } - P _ { \pi } + { \bf 1 } _ { n } d _ { \pi } ^ { I ^ { \prime } }$ is invertible.

Since $v _ { \pi } ^ { \ast }$ involves $A ^ { - 1 }$ , it is necessary to show that $A$ is invertible. The analysis is summarized in the following lemma.

Lemma 9.3. The matrix ${ \cal I } _ { n } - { \cal P } _ { \pi } + { \bf 1 } _ { n } d _ { \pi } ^ { I }$ is invertible and its inverse is

$$
\left[ I _ {n} - \left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) \right] ^ {- 1} = \sum_ {k = 1} ^ {\infty} \left(P _ {\pi} ^ {k} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) + I _ {n}.
$$

Proof. First of all, we state some preliminary facts without proof. Let $\rho ( M )$ be the spectral radius of a matrix $M$ . Then, $I - M$ is invertible if $\rho ( M ) < 1$ . Moreover, $\rho ( M ) < 1$ if and only if $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } M ^ { k } = 0 } \end{array}$ .

Based on the above facts, we next show that $\begin{array} { r } { \operatorname* { l i m } _ { k  \infty } ( P _ { \pi } - { \bf 1 } _ { n } d _ { \pi } ^ { I } ) ^ { k }  0 } \end{array}$ , and then the invertibility of $I _ { n } - ( P _ { \pi } - \mathbf { 1 } _ { n } d _ { \pi } ^ { I ^ { \prime } } )$ immediately follows. To do that, we notice that

$$
\left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) ^ {k} = P _ {\pi} ^ {k} - \mathbf {1} _ {n} d _ {\pi} ^ {T}, \quad k \geq 1, \tag {9.27}
$$

which can be proven by induction. For instance, when $k = 1$ , the equation is valid. When $k = 2$ , we have

$$
\begin{array}{l} \left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) ^ {2} = \left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) \left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) \\ = P _ {\pi} ^ {2} - P _ {\pi} \mathbf {1} _ {n} d _ {\pi} ^ {T} - \mathbf {1} _ {n} d _ {\pi} ^ {T} P _ {\pi} + \mathbf {1} _ {n} d _ {\pi} ^ {T} \mathbf {1} _ {n} d _ {\pi} ^ {T} \\ \mathbf {\Sigma} = P _ {\pi} ^ {2} - \mathbf {1} _ {n} d _ {\pi} ^ {T}, \\ \end{array}
$$

where the last equality is due to $P _ { \pi } \mathbf { 1 } _ { n } = \mathbf { 1 } _ { n }$ , $d _ { \pi } ^ { \prime } P _ { \pi } = d _ { \pi } ^ { \prime }$ , and $d _ { \pi } ^ { I ^ { \prime } } \mathbf { 1 } _ { n } = 1$ . The case of $k \geq 3$ can be proven similarly.

Since $d _ { \pi }$ is the stationary distribution of the state, it holds that $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } P _ { \pi } ^ { k } = d _ { \pi } ^ { I ^ { \prime } } \mathbf { 1 } _ { n } } \end{array}$ (see Box 8.1). Therefore, (9.27) implies that

$$
\lim _ {k \to \infty} (P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}) ^ {k} = \lim _ {k \to \infty} P _ {\pi} ^ {k} - d _ {\pi} ^ {T} \mathbf {1} _ {n} = 0.
$$

As a result, $\rho ( P _ { \pi } { - } \mathbf { 1 } _ { n } d _ { \pi } ^ { I } ) < 1$ and hence $I _ { n } - ( P _ { \pi } - { \bf 1 } _ { n } d _ { \pi } ^ { I } )$ is invertible. Furthermore,

the inverse of this matrix is given by

$$
\begin{array}{l} \left(I _ {n} - \left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right)\right) ^ {- 1} = \sum_ {k = 0} ^ {\infty} \left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) ^ {k} \\ = I _ {n} + \sum_ {k = 1} ^ {\infty} \left(P _ {\pi} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) ^ {k} \\ = I _ {n} + \sum_ {k = 1} ^ {\infty} (P _ {\pi} ^ {k} - \mathbf {1} _ {n} d _ {\pi} ^ {T}) \\ = \sum_ {k = 0} ^ {\infty} \left(P _ {\pi} ^ {k} - \mathbf {1} _ {n} d _ {\pi} ^ {T}\right) + \mathbf {1} _ {n} d _ {\pi} ^ {T}. \\ \end{array}
$$

The proof is complete.

The proof of Lemma 9.3 is inspired by [66]. However, the result $( I _ { n } - P _ { \pi } +$ $\begin{array} { r } { \mathbf { 1 } _ { n } d _ { \pi } ^ { I } ) ^ { - 1 } = \sum _ { k = 0 } ^ { \infty } ( P _ { \pi } ^ { k } - \mathbf { 1 } _ { n } d _ { \pi } ^ { I ^ { \prime } } ) } \end{array}$ given in [66] (the statement above equation (16) in [66]) is inaccurate because ${ \textstyle \sum _ { k = 0 } ^ { \infty } } ( P _ { \pi } ^ { k } - \mathbf { 1 } _ { n } d _ { \pi } ^ { T } )$ is singular since $\scriptstyle \sum _ { k = 0 } ^ { \infty } ( P _ { \pi } ^ { k } \ -$ $\mathbf { 1 } _ { n } d _ { \pi } ^ { I ^ { \prime } } ) \mathbf { 1 } _ { n } = 0$ . Lemma 9.3 corrects this inaccuracy.

# Derivation of gradients

Although the value of $v _ { \pi }$ is not unique in the undiscounted case, as shown in Theorem 9.4, the value of $r _ { \pi }$ is unique. In particular, it follows from the Poisson equation that

$$
\begin{array}{l} \bar {r} _ {\pi} \mathbf {1} _ {n} = r _ {\pi} + (P _ {\pi} - I _ {n}) v _ {\pi} \\ = r _ {\pi} + \left(P _ {\pi} - I _ {n}\right) \left(v _ {\pi} ^ {*} + c \mathbf {1} _ {n}\right) \\ = r _ {\pi} + \left(P _ {\pi} - I _ {n}\right) v _ {\pi} ^ {*}. \\ \end{array}
$$

Notably, the undetermined value $c$ is canceled and hence $r _ { \pi }$ is unique. Therefore, we can calculate the gradient of $r _ { \pi }$ in the undiscounted case. In addition, since $v _ { \pi }$ is not unique, $v _ { \pi }$ is not unique either. We do not study the gradient of $v _ { \pi }$ in the undiscounted case. For interested readers, it is worth mentioning that we can add more constraints to uniquely solve $v _ { \pi }$ from the Poisson equation. For example, by assuming that a recurrent state exists, the state value of this recurrent state can be determined [65, Section II], and hence $c$ can be determined. There are also other ways to uniquely determine $v _ { \pi }$ . See, for example, equations (8.6.5)-(8.6.7) in [2].

The gradient of $r _ { \pi }$ in the undiscounted case is given below.

Theorem 9.5 (Gradient of $r _ { \pi }$ in the undiscounted case). In the undiscounted case, the

gradient of the average reward $\bar { r } _ { \pi }$ is

$$
\begin{array}{l} \nabla_ {\theta} \bar {r} _ {\pi} = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) \\ = \mathbb {E} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \right], \tag {9.28} \\ \end{array}
$$

where $S \sim d _ { \pi }$ and $A \sim \pi ( S , \theta )$ .

Compared to the discounted case shown in Theorem 9.3, the gradient of $\bar { r } _ { \pi }$ in the undiscounted case is more elegant in the sense that (9.28) is strictly valid and $S$ obeys the stationary distribution.

# Box 9.6: Proof of Theorem 9.5

First of all, it follows from $\begin{array} { r } { v _ { \pi } ( s ) = \sum _ { a \in \mathcal { A } } \pi ( a | s , \theta ) q _ { \pi } ( s , a ) } \end{array}$ that

$$
\begin{array}{l} \nabla_ {\theta} v _ {\pi} (s) = \nabla_ {\theta} \left[ \sum_ {a \in \mathcal {A}} \pi (a | s, \theta) q _ {\pi} (s, a) \right] \\ = \sum_ {a \in \mathcal {A}} \left[ \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) + \pi (a | s, \theta) \nabla_ {\theta} q _ {\pi} (s, a) \right], \tag {9.29} \\ \end{array}
$$

where $q _ { \pi } ( s , a )$ is the action value satisfying

$$
\begin{array}{l} q _ {\pi} (s, a) = \sum_ {r} p (r | s, a) (r - \bar {r} _ {\pi}) + \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) v _ {\pi} (s ^ {\prime}) \\ = r (s, a) - \bar {r} _ {\pi} + \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {\pi} \left(s ^ {\prime}\right). \\ \end{array}
$$

Since $\begin{array} { r } { r ( s , a ) = \sum _ { r } r p ( r | s , a ) } \end{array}$ is independent of $\theta$ , we have

$$
\nabla_ {\theta} q _ {\pi} (s, a) = 0 - \nabla_ {\theta} \bar {r} _ {\pi} + \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) \nabla_ {\theta} v _ {\pi} (s ^ {\prime}).
$$

Substituting this result into (9.29) yields

$$
\begin{array}{l} \nabla_ {\theta} v _ {\pi} (s) = \sum_ {a \in \mathcal {A}} \left[ \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) + \pi (a | s, \theta) \left(- \nabla_ {\theta} \bar {r} _ {\pi} + \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) \nabla_ {\theta} v _ {\pi} (s ^ {\prime})\right) \right] \\ = \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) - \nabla_ {\theta} \bar {r} _ {\pi} + \sum_ {a \in \mathcal {A}} \pi (a | s, \theta) \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} \mid s, a\right) \nabla_ {\theta} v _ {\pi} \left(s ^ {\prime}\right). \tag {9.30} \\ \end{array}
$$

Let

$$
u (s) \doteq \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a).
$$

Since $\begin{array} { r l r } { \sum _ { a \in \mathcal { A } } \pi ( a | s , \theta ) \sum _ { s ^ { \prime } \in \mathcal { S } } p ( s ^ { \prime } | s , a ) \nabla _ { \theta } v _ { \pi } ( s ^ { \prime } ) } & { = } & { \sum _ { s ^ { \prime } \in \mathcal { S } } p ( s ^ { \prime } | s ) \nabla _ { \theta } v _ { \pi } ( s ^ { \prime } ) , } \end{array}$ $\begin{array} { r } { \sum _ { s ^ { \prime } \in \mathcal { S } } p ( s ^ { \prime } | s ) \nabla _ { \theta } v _ { \pi } ( s ^ { \prime } ) } \end{array}$ equation (9.30) can be written in matrix-vector form as

$$
\underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\pi} (s) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\pi} \in \mathbb {R} ^ {m n}} = \underbrace {\left[ \begin{array}{c} \vdots \\ u (s) \\ \vdots \end{array} \right]} _ {u \in \mathbb {R} ^ {m n}} - \mathbf {1} _ {n} \otimes \nabla_ {\theta} \bar {r} _ {\pi} + (P _ {\pi} \otimes I _ {m}) \underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\pi} (s ^ {\prime}) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\pi} \in \mathbb {R} ^ {m n}},
$$

where $n = | S |$ , $m$ is the dimension of $\theta$ , and $\otimes$ is the Kronecker product. The above equation can be written concisely as

$$
\nabla_ {\theta} v _ {\pi} = u - \mathbf {1} _ {n} \otimes \nabla_ {\theta} \bar {r} _ {\pi} + (P _ {\pi} \otimes I _ {m}) \nabla_ {\theta} v _ {\pi},
$$

and hence

$$
\mathbf {1} _ {n} \otimes \nabla_ {\theta} \bar {r} _ {\pi} = u + (P _ {\pi} \otimes I _ {m}) \nabla_ {\theta} v _ {\pi} - \nabla_ {\theta} v _ {\pi}.
$$

Multiplying $d _ { \pi } ^ { I ^ { \prime } } \otimes I _ { m }$ on both sides of the above equation gives

$$
\begin{array}{l} \left(d _ {\pi} ^ {T} \mathbf {1} _ {n}\right) \otimes \nabla_ {\theta} \bar {r} _ {\pi} = d _ {\pi} ^ {T} \otimes I _ {m} u + \left(d _ {\pi} ^ {T} P _ {\pi}\right) \otimes I _ {m} \nabla_ {\theta} v _ {\pi} - d _ {\pi} ^ {T} \otimes I _ {m} \nabla_ {\theta} v _ {\pi} \\ = d _ {\pi} ^ {T} \otimes I _ {m} u, \\ \end{array}
$$

which implies

$$
\begin{array}{l} \nabla_ {\theta} \bar {r} _ {\pi} = d _ {\pi} ^ {T} \otimes I _ {m} u \\ = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) u (s) \\ = \sum_ {s \in \mathcal {S}} d _ {\pi} (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a). \\ \end{array}
$$

# 9.4 Monte Carlo policy gradient (REINFORCE)

With the gradient presented in Theorem 9.1, we next show how to use the gradient-based method to optimize the metrics to obtain optimal policies.

The gradient-ascent algorithm for maximizing $J ( \theta )$ is

$$
\begin{array}{l} \theta_ {t + 1} = \theta_ {t} + \alpha \nabla_ {\theta} J (\theta_ {t}) \\ = \theta_ {t} + \alpha \mathbb {E} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) q _ {\pi} (S, A) \right], \tag {9.31} \\ \end{array}
$$

where $\alpha > 0$ is a constant learning rate. Since the true gradient in (9.31) is unknown, we

can replace the true gradient with a stochastic gradient to obtain the following algorithm:

$$
\theta_ {t + 1} = \theta_ {t} + \alpha \nabla_ {\theta} \ln \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right) q _ {t} \left(s _ {t}, a _ {t}\right), \tag {9.32}
$$

where $q _ { t } ( s _ { t } , a _ { t } )$ is an approximation of $q _ { \pi } ( s _ { t } , a _ { t } )$ . If $q _ { t } ( s _ { t } , a _ { t } )$ is obtained by Monte Carlo estimation, the algorithm is called REINFORCE [68] or Monte Carlo policy gradient, which is one of earliest and simplest policy gradient algorithms.

The algorithm in (9.32) is important since many other policy gradient algorithms can be obtained by extending it. We next examine the interpretation of (9.32) more closely. Since $\begin{array} { r } { \nabla _ { \theta } \ln \pi ( a _ { t } | s _ { t } , \theta _ { t } ) = \frac { \nabla _ { \theta } \pi ( a _ { t } | s _ { t } , \theta _ { t } ) } { \pi ( a _ { t } | s _ { t } , \theta _ { t } ) } } \end{array}$ we can rewrite (9.32) as

$$
\theta_ {t + 1} = \theta_ {t} + \alpha \underbrace {\left(\frac {q _ {t} (s _ {t} , a _ {t})}{\pi (a _ {t} | s _ {t} , \theta_ {t})}\right)} _ {\beta_ {t}} \nabla_ {\theta} \pi (a _ {t} | s _ {t}, \theta_ {t}),
$$

which can be further written concisely as

$$
\theta_ {t + 1} = \theta_ {t} + \alpha \beta_ {t} \nabla_ {\theta} \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right). \tag {9.33}
$$

Two important interpretations can be seen from this equation.

First, since (9.33) is a simple gradient-ascent algorithm, the following observations can be obtained.

- If $\beta _ { t } \geq 0$ , the probability of choosing $( s _ { t } , a _ { t } )$ is enhanced. That is

$$
\pi \left(a _ {t} \mid s _ {t}, \theta_ {t + 1}\right) \geq \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right).
$$

The greater $\beta _ { t }$ is, the stronger the enhancement is.

- If $\beta _ { t } < 0$ , the probability of choosing $( s _ { t } , a _ { t } )$ decreases. That is

$$
\pi \left(a _ {t} \mid s _ {t}, \theta_ {t + 1}\right) <   \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right).
$$

The above observations can be proven as follows. When $\theta _ { t + 1 } - \theta _ { t }$ is sufficiently small, it follows from the Taylor expansion that

$$
\begin{array}{l} \pi (a _ {t} | s _ {t}, \theta_ {t + 1}) \approx \pi (a _ {t} | s _ {t}, \theta_ {t}) + (\nabla_ {\theta} \pi (a _ {t} | s _ {t}, \theta_ {t})) ^ {T} (\theta_ {t + 1} - \theta_ {t}) \\ = \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right) + \alpha \beta_ {t} \left(\nabla_ {\theta} \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right)\right) ^ {T} \left(\nabla_ {\theta} \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right)\right) \quad (\text {s u b s t i t u t i n g (9 . 3 3)}) \\ = \pi (a _ {t} | s _ {t}, \theta_ {t}) + \alpha \beta_ {t} \| \nabla_ {\theta} \pi (a _ {t} | s _ {t}, \theta_ {t}) \| _ {2} ^ {2}. \\ \end{array}
$$

It is clear that $\pi ( a _ { t } | s _ { t } , \theta _ { t + 1 } ) \geq \pi ( a _ { t } | s _ { t } , \theta _ { t } )$ when $\beta _ { t } \geq 0$ and $\pi ( a _ { t } | s _ { t } , \theta _ { t + 1 } ) < \pi ( a _ { t } | s _ { t } , \theta _ { t } )$ when $\beta _ { t } < 0$ .

 Second, the algorithm can strike a balance between exploration and exploitation to a

# Algorithm 9.1: Policy Gradient by Monte Carlo (REINFORCE)

Initialization: Initial parameter $\theta$ ; $\gamma \in ( 0 , 1 )$ ; $\alpha > 0$ .

Goal: Learn an optimal policy for maximizing $J ( \theta )$ .

For each episode, do

Generate an episode $\left\{ s _ { 0 } , a _ { 0 } , r _ { 1 } , \ldots , s _ { T - 1 } , a _ { T - 1 } , r _ { T } \right\}$ following $\pi ( \theta )$

For $t = 0 , 1 , \dots , T - 1$

$\begin{array} { r } { q _ { t } ( s _ { t } , a _ { t } ) = \sum _ { k = t + 1 } ^ { T } \gamma ^ { k - t - 1 } r _ { k } } \end{array}$

Policy update: $\theta \gets \theta + \alpha \nabla _ { \theta } \ln \pi ( a _ { t } | s _ { t } , \theta ) q _ { t } ( s _ { t } , a _ { t } )$

certain extent due to the expression of

$$
\beta_ {t} = \frac {q _ {t} (s _ {t} , a _ {t})}{\pi (a _ {t} | s _ {t} , \theta_ {t})}.
$$

On the one hand, $\beta _ { t }$ is proportional to $q _ { t } ( s _ { t } , a _ { t } )$ . As a result, if the action value of $( s _ { t } , a _ { t } )$ is large, then $\pi ( a _ { t } | s _ { t } , \theta _ { t } )$ is enhanced so that the probability of selecting $a _ { t }$ increases. Therefore, the algorithm attempts to exploit actions with greater values. One the other hand, $\beta _ { t }$ is inversely proportional to $\pi ( a _ { t } | s _ { t } , \theta _ { t } )$ when $q _ { t } ( s _ { t } , a _ { t } ) > 0$ . As a result, if the probability of selecting $a _ { t }$ is small, then $\pi ( a _ { t } | s _ { t } , \theta _ { t } )$ is enhanced so that the probability of selecting $a _ { t }$ increases. Therefore, the algorithm attempts to explore actions with low probabilities.

Moreover, since (9.32) uses samples to approximate the true gradient in (9.31), it is important to understand how the samples should be obtained.

$\diamond$ How to sample $S$ ? $S$ in the true gradient $\mathbb { E } [ \nabla _ { \theta } \ln \pi ( A | S , \theta _ { t } ) q _ { \pi } ( S , A ) ]$ should obey the distribution $\eta$ which is either the stationary distribution $d _ { \pi }$ or the discounted total probability distribution $\rho _ { \pi }$ in (9.19). Either $d _ { \pi }$ or $\rho _ { \pi }$ represents the long-term behavior exhibited under $\pi$ .   
How to sample $A$ ? $A$ in $\mathbb { E } [ \nabla _ { \theta } \ln \pi ( A | S , \theta _ { t } ) q _ { \pi } ( S , A ) ]$ should obey the distribution of $\pi ( A | S , \theta )$ . The ideal way to sample $A$ is to select $a _ { t }$ following $\pi ( { a } | s _ { t } , \theta _ { t } )$ . Therefore, the policy gradient algorithm is on-policy.

Unfortunately, the ideal ways for sampling $S$ and $A$ are not strictly followed in practice due to their low efficiency of sample usage. A more sample-efficient implementation of (9.32) is given in Algorithm 9.1. In this implementation, an episode is first generated by following $\pi ( \theta )$ . Then, $\theta$ is updated multiple times using every experience sample in the episode.

# 9.5 Summary

This chapter introduced the policy gradient method, which is the foundation of many modern reinforcement learning algorithms. Policy gradient methods are policy-based. It is a big step forward in this book because all the methods in the previous chapters are value-based. The basic idea of the policy gradient method is simple. That is to select an appropriate scalar metric and then optimize it via a gradient-ascent algorithm.

The most complicated part of the policy gradient method is the derivation of the gradients of the metrics. That is because we have to distinguish various scenarios with different metrics and discounted/undiscounted cases. Fortunately, the expressions of the gradients in different scenarios are similar. Hence, we summarized the expressions in Theorem 9.1, which is the most important theoretical result in this chapter. For many readers, it is sufficient to be aware of this theorem. Its proof is nontrivial, and it is not required for all readers to study.

The policy gradient algorithm in (9.32) must be properly understood since it is the foundation of many advanced policy gradient algorithms. In the next chapter, this algorithm will be extended to another important policy gradient method called actor-critic.

# 9.6 Q&A

Q: What is the basic idea of the policy gradient method?

A: The basic idea is simple. That is to define an appropriate scalar metric, derive its gradient, and then use gradient-ascent methods to optimize the metric. The most important theoretical result regarding this method is the policy gradient given in Theorem 9.1.

 Q: What is the most complicated part of the policy gradient method?

A: The basic idea of the policy gradient method is simple. However, the derivation procedure of the gradients is quite complicated. That is because we have to distinguish numerous different scenarios. The mathematical derivation procedure in each scenario is nontrivial. It is sufficient for many readers to be familiar with the result in Theorem 9.1 without knowing the proof.

 Q: What metrics should be used in the policy gradient method?

A: We introduced three common metrics in this chapter: $v _ { \pi }$ , $\bar { v } _ { \pi } ^ { 0 }$ , and $r _ { \pi }$ . Since they all lead to similar policy gradients, they all can be adopted in the policy gradient method. More importantly, the expressions in (9.1) and (9.4) are often encountered in the literature.

Q: Why is a natural logarithm function contained in the policy gradient?

A: A natural logarithm function is introduced to express the gradient as an expected value. In this way, we can approximate the true gradient with a stochastic one.

 Q: Why do we need to study undiscounted cases when deriving the policy gradient?

A: The definition of the average reward $r _ { \pi }$ is valid for both discounted and undiscounted cases. While the gradient of $\bar { r } _ { \pi }$ in the discounted case is an approximation, its gradient in the undiscounted case is more elegant.

 Q: What does the policy gradient algorithm in (9.32) do mathematically?

A: To better understand this algorithm, readers are recommended to examine its concise expression in (9.33), which clearly shows that it is a gradient-ascent algorithm for updating the value of $\pi ( a _ { t } | s _ { t } , \theta _ { t } )$ . That is, when a sample $( s _ { t } , a _ { t } )$ is available, the policy can be updated so that $\pi ( a _ { t } | s _ { t } , \theta _ { t + 1 } ) \geq \pi ( a _ { t } | s _ { t } , \theta _ { t } )$ or $\pi ( a _ { t } | s _ { t } , \theta _ { t + 1 } ) < \pi ( a _ { t } | s _ { t } , \theta _ { t } )$ depending on the coefficients.

# Chapter 10

# Actor-Critic Methods

![](images/6f27785b4d75c56a2e9ef0a40481e2e169e522b6c763bcb6ee9850df94c33364.jpg)  
Figure 10.1: Where we are in this book.

This chapter introduces actor-critic methods. From one point of view, “actor-critic” refers to a structure that incorporates both policy-based and value-based methods. Here, an “actor” refers to a policy update step. The reason that it is called an actor is that the actions are taken by following the policy. Here, an “critic” refers to a value update step. It is called a critic because it criticizes the actor by evaluating its corresponding values. From another point of view, actor-critic methods are still policy gradient algorithms. They can be obtained by extending the policy gradient algorithm introduced in Chapter 9. It is important for the reader to well understand the contents of Chapters 8 and 9 before studying this chapter.

# 10.1 The simplest actor-critic algorithm (QAC)

This section introduces the simplest actor-critic algorithm. This algorithm can be easily obtained by extending the policy gradient algorithm in (9.32).

Recall that the idea of the policy gradient method is to search for an optimal policy by maximizing a scalar metric $J ( \theta )$ . The gradient-ascent algorithm for maximizing $J ( \theta )$ is

$$
\begin{array}{l} \theta_ {t + 1} = \theta_ {t} + \alpha \nabla_ {\theta} J (\theta_ {t}) \\ = \theta_ {t} + \alpha \mathbb {E} _ {S \sim \eta , A \sim \pi} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) q _ {\pi} (S, A) \right], \tag {10.1} \\ \end{array}
$$

where $\eta$ is a distribution of the states (see Theorem 9.1 for more information). Since the true gradient is unknown, we can use a stochastic gradient to approximate it:

$$
\theta_ {t + 1} = \theta_ {t} + \alpha \nabla_ {\theta} \ln \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right) q _ {t} \left(s _ {t}, a _ {t}\right). \tag {10.2}
$$

This is the algorithm given in (9.32).

Equation (10.2) is important because it clearly shows how policy-based and valuebased methods can be combined. On the one hand, it is a policy-based algorithm since it directly updates the policy parameter. On the other hand, this equation requires knowing $q _ { t } ( s _ { t } , a _ { t } )$ , which is an estimate of the action value $q _ { \pi } ( s _ { t } , a _ { t } )$ . As a result, another valuebased algorithm is required to generate $q _ { t } ( s _ { t } , a _ { t } )$ . So far, we have studied two ways to estimate action values in this book. The first is based on Monte Carlo learning and the second is temporal-difference (TD) learning.

> If $q _ { t } ( s _ { t } , a _ { t } )$ is estimated by Monte Carlo learning, the corresponding algorithm is called REINFORCE or Monte Carlo policy gradient, which has already been introduced in Chapter 9.   
If $q _ { t } ( s _ { t } , a _ { t } )$ is estimated by TD learning, the corresponding algorithms are usually called actor-critic. Therefore, actor-critic methods can be obtained by incorporating TD-based value estimation into policy gradient methods.

The procedure of the simplest actor-critic algorithm is summarized in Algorithm 10.1. The critic corresponds to the value update step via the Sarsa algorithm presented in (8.35). The action values are represented by a parameterized function $q ( s , a , w )$ . The actor corresponds to the policy update step in (10.2). This actor-citric algorithm is sometimes called $Q$ actor-critic (QAC). Although it is simple, QAC reveals the core idea of actor-critic methods. It can be extended to generate many advanced ones as shown in the rest of this chapter.

# Algorithm 10.1: The simplest actor-critic algorithm (QAC)

Initialization: A policy function $\pi ( a | s , \theta _ { 0 } )$ where $\theta _ { 0 }$ is the initial parameter. A value function $q ( s , a , w _ { 0 } )$ where $w _ { 0 }$ is the initial parameter. $\alpha _ { w } , \alpha _ { \theta } > 0$ .

Goal: Learn an optimal policy to maximize $J ( \theta )$ .

At time step $t$ in each episode, do

Generate $a _ { t }$ following $\pi ( a | s _ { t } , \theta _ { t } )$ , observe $r _ { t + 1 } , s _ { t + 1 }$ , and then generate $a _ { t + 1 }$ following $\pi ( a | s _ { t + 1 } , \theta _ { t } )$ .

Actor (policy update):

$$
\theta_ {t + 1} = \theta_ {t} + \alpha_ {\theta} \nabla_ {\theta} \ln \pi \left(a _ {t} \mid s _ {t}, \theta_ {t}\right) q \left(s _ {t}, a _ {t}, w _ {t}\right)
$$

Critic (value update):

$$
w _ {t + 1} = w _ {t} + \alpha_ {w} \big [ r _ {t + 1} + \gamma q (s _ {t + 1}, a _ {t + 1}, w _ {t}) - q (s _ {t}, a _ {t}, w _ {t}) \big ] \nabla_ {w} q (s _ {t}, a _ {t}, w _ {t})
$$

# 10.2 Advantage actor-critic (A2C)

We now introduce the algorithm of advantage actor-critic. The core idea of this algorithm is to introduce a baseline to reduce estimation variance.

# 10.2.1 Baseline invariance

One interesting property of the policy gradient is that it is invariant to an additional baseline. That is

$$
\mathbb {E} _ {S \sim \eta , A \sim \pi} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) q _ {\pi} (S, A) \right] = \mathbb {E} _ {S \sim \eta , A \sim \pi} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) \left(q _ {\pi} (S, A) - b (S)\right) \right], \tag {10.3}
$$

where the additional baseline $b ( S )$ is a scalar function of $S$ . We next answer two questions about the baseline.

 First, why is (10.3) valid?

Equation (10.3) holds if and only if

$$
\mathbb {E} _ {S \sim \eta , A \sim \pi} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) b (S) \right] = 0.
$$

This equation is valid because

$$
\begin{array}{l} \mathbb {E} _ {S \sim \eta , A \sim \pi} \Big [ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) b (S) \Big ] = \sum_ {s \in \mathcal {S}} \eta (s) \sum_ {a \in \mathcal {A}} \pi (a | s, \theta_ {t}) \nabla_ {\theta} \ln \pi (a | s, \theta_ {t}) b (s) \\ = \sum_ {s \in \mathcal {S}} \eta (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta_ {t}) b (s) \\ = \sum_ {s \in \mathcal {S}} \eta (s) b (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta_ {t}) \\ = \sum_ {s \in \mathcal {S}} \eta (s) b (s) \nabla_ {\theta} \sum_ {a \in \mathcal {A}} \pi (a | s, \theta_ {t}) \\ = \sum_ {s \in \mathcal {S}} \eta (s) b (s) \nabla_ {\theta} 1 = 0. \\ \end{array}
$$

Second, why is the baseline useful?

The baseline is useful because it can reduce the approximation variance when we use samples to approximate the true gradient. In particular, let

$$
X (S, A) \doteq \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) [ q _ {\pi} (S, A) - b (S) ]. \tag {10.4}
$$

Then, the true gradient is $\mathbb { E } [ X ( S , A ) ]$ . Since we need to use a stochastic sample $x$ t o approximate $\mathbb { E } { \lvert X \rvert }$ , it would be favorable if the variance $\operatorname { v a r } ( X )$ is small. For example, if $\operatorname { v a r } ( X )$ is close to zero, then any sample $x$ can accurately approximate $\mathbb { E } { \lvert X \rvert }$ . On the contrary, if $\operatorname { v a r } ( X )$ is large, the value of a sample may be far from $\mathbb { E } { \lvert X \rvert }$ .

Although $\mathbb { E } [ X ]$ is invariant to the baseline, the variance $\operatorname { v a r } ( X )$ is not. Our goal is to design a good baseline to minimize $\operatorname { v a r } ( X )$ . In the algorithms of REINFORCE and QAC, we set $b = 0$ , which is not guaranteed to be a good baseline.

In fact, the optimal baseline that minimizes $\operatorname { v a r } ( X )$ is

$$
b ^ {*} (s) = \frac {\mathbb {E} _ {A \sim \pi} \left[ \| \nabla_ {\theta} \ln \pi (A | s , \theta_ {t}) \| ^ {2} q _ {\pi} (s , A) \right]}{\mathbb {E} _ {A \sim \pi} \left[ \| \nabla_ {\theta} \ln \pi (A | s , \theta_ {t}) \| ^ {2} \right]}, \quad s \in \mathcal {S}. \tag {10.5}
$$

The proof is given in Box 10.1.

Although the baseline in (10.5) is optimal, it is too complex to be useful in practice. If the weight $\| \nabla _ { \theta } \ln \pi ( A | s , \theta _ { t } ) \| ^ { 2 }$ is removed from (10.5), we can obtain a suboptimal baseline that has a concise expression:

$$
b ^ {\dagger} (s) = \mathbb {E} _ {A \sim \pi} [ q _ {\pi} (s, A) ] = v _ {\pi} (s), \quad s \in \mathcal {S}.
$$

Interestingly, this suboptimal baseline is the state value.

# Box 10.1: Showing that $b ^ { * } ( s )$ in (10.5) is the optimal baseline

Let ${ \bar { x } } { \dot { = } } \operatorname { \mathbb { E } } [ X ]$ , which is invariant for any $b ( s )$ . If $X$ is a vector, its variance is a matrix. It is common to select the trace of $\operatorname { v a r } ( X )$ as a scalar objective function for optimization:

$$
\begin{array}{l} \operatorname {t r} [ \operatorname {v a r} (X) ] = \operatorname {t r} \mathbb {E} [ (X - \bar {x}) (X - \bar {x}) ^ {T} ] \\ = \operatorname {t r} \mathbb {E} \left[ X X ^ {T} - \bar {x} X ^ {T} - X \bar {x} ^ {T} + \bar {x} \bar {x} ^ {T} \right] \\ = \mathbb {E} \left[ X ^ {T} X - X ^ {T} \bar {x} - \bar {x} ^ {T} X + \bar {x} ^ {T} \bar {x} \right] \\ = \mathbb {E} \left[ X ^ {T} X \right] - \bar {x} ^ {T} \bar {x}. \tag {10.6} \\ \end{array}
$$

When deriving the above equation, we use the trace property $\operatorname { t r } ( A B ) \ = \operatorname { t r } ( B A )$ for any squared matrices $A , B$ with appropriate dimensions. Since $x$ is invariant, equation (10.6) suggests that we only need to minimize $\mathbb { E } [ X ^ { T } X ]$ . With $X$ defined in (10.4), we have

$$
\begin{array}{l} \mathbb {E} \left[ X ^ {T} X \right] = \mathbb {E} \left[ \left(\nabla_ {\theta} \ln \pi\right) ^ {T} \left(\nabla_ {\theta} \ln \pi\right) \left(q _ {\pi} (S, A) - b (S)\right) ^ {2} \right] \\ = \mathbb {E} \left[ \| \nabla_ {\theta} \ln \pi \| ^ {2} (q _ {\pi} (S, A) - b (S)) ^ {2} \right], \\ \end{array}
$$

where $\pi ( A | S , \theta )$ is written as $\pi$ for short. Since $S \sim \eta$ and $A \sim \pi$ , the above equation can be rewritten as

$$
\mathbb {E} [ X ^ {T} X ] = \sum_ {s \in \mathcal {S}} \eta (s) \mathbb {E} _ {A \sim \pi} \left[ \| \nabla_ {\theta} \ln \pi \| ^ {2} (q _ {\pi} (s, A) - b (s)) ^ {2} \right].
$$

To ensure $\nabla _ { b } \mathbb { E } [ X ^ { T } X ] = 0$ , $b ( s )$ for any $s \in S$ should satisfy

$$
\mathbb {E} _ {A \sim \pi} \big [ \| \nabla_ {\theta} \ln \pi \| ^ {2} (b (s) - q _ {\pi} (s, A)) \big ] = 0, \qquad s \in \mathcal {S}.
$$

The above equation can be easily solved to obtain the optimal baseline:

$$
b ^ {*} (s) = \frac {\mathbb {E} _ {A \sim \pi} [ \| \nabla_ {\theta} \ln \pi \| ^ {2} q _ {\pi} (s , A) ]}{\mathbb {E} _ {A \sim \pi} [ \| \nabla_ {\theta} \ln \pi \| ^ {2} ]}, \qquad s \in \mathcal {S}.
$$

More discussions on optimal baselines in policy gradient methods can be found in [69, 70].

# 10.2.2 Algorithm description

When $b ( s ) = v _ { \pi } ( s )$ , the gradient-ascent algorithm in (10.1) becomes

$$
\begin{array}{l} \theta_ {t + 1} = \theta_ {t} + \alpha \mathbb {E} \Big [ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) [ q _ {\pi} (S, A) - v _ {\pi} (S) ] \Big ] \\ \dot {=} \theta_ {t} + \alpha \mathbb {E} \left[ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) \delta_ {\pi} (S, A) \right]. \tag {10.7} \\ \end{array}
$$

Here,

$$
\delta_ {\pi} (S, A) \doteq q _ {\pi} (S, A) - v _ {\pi} (S)
$$

is called the advantage function, which reflects the advantage of one action over the others. More specifically, note that $\begin{array} { r } { v _ { \pi } ( s ) = \sum _ { a \in \mathcal { A } } \pi ( a | s ) q _ { \pi } ( s , a ) } \end{array}$ is the mean of the action values. If $\delta _ { \pi } ( s , a ) > 0$ , it means that the corresponding action has a greater value than the mean value.

The stochastic version of (10.7) is

$$
\begin{array}{l} \theta_ {t + 1} = \theta_ {t} + \alpha \nabla_ {\theta} \ln \pi (a _ {t} | s _ {t}, \theta_ {t}) [ q _ {t} (s _ {t}, a _ {t}) - v _ {t} (s _ {t}) ] \\ = \theta_ {t} + \alpha \nabla_ {\theta} \ln \pi (a _ {t} | s _ {t}, \theta_ {t}) \delta_ {t} (s _ {t}, a _ {t}), \tag {10.8} \\ \end{array}
$$

where $s _ { t } , a _ { t }$ are samples of $S , A$ at time $t$ . Here, $q _ { t } ( s _ { t } , a _ { t } )$ and $v _ { t } ( s _ { t } )$ are approximations of $q _ { \pi ( \theta _ { t } ) } ( s _ { t } , a _ { t } )$ and $v _ { \pi ( \theta _ { t } ) } ( s _ { t } )$ , respectively. The algorithm in (10.8) updates the policy based on the relative value of $q _ { t }$ with respect to $v _ { t }$ rather than the absolute value of $q _ { t }$ . This is intuitively reasonable because, when we attempt to select an action at a state, we only care about which action has the greatest value relative to the others.

If $q _ { t } ( s _ { t } , a _ { t } )$ and $v _ { t } ( s _ { t } )$ are estimated by Monte Carlo learning, the algorithm in (10.8) is called REINFORCE with a baseline. If $q _ { t } ( s _ { t } , a _ { t } )$ and $v _ { t } ( s _ { t } )$ are estimated by TD learning, the algorithm is usually called advantage actor-critic (A2C). The implementation of A2C is summarized in Algorithm 10.2. It should be noted that the advantage function in this implementation is approximated by the TD error:

$$
q _ {t} (s _ {t}, a _ {t}) - v _ {t} (s _ {t}) \approx r _ {t + 1} + \gamma v _ {t} (s _ {t + 1}) - v _ {t} (s _ {t}).
$$

This approximation is reasonable because

$$
q _ {\pi} (s _ {t}, a _ {t}) - v _ {\pi} (s _ {t}) = \mathbb {E} \Big [ R _ {t + 1} + \gamma v _ {\pi} (S _ {t + 1}) - v _ {\pi} (S _ {t}) | S _ {t} = s _ {t}, A _ {t} = a _ {t} \Big ],
$$

which is valid due to the definition of $q _ { \pi } ( s _ { t } , a _ { t } )$ . One merit of using the TD error is that we only need to use a single neural network to represent $v _ { \pi } ( s )$ . Otherwise, if $\delta _ { t } =$ $q _ { t } ( s _ { t } , a _ { t } ) - v _ { t } ( s _ { t } )$ , we need to maintain two networks to represent $v _ { \pi } ( s )$ and $q _ { \pi } ( s , a )$ , respectively. When we use the TD error, the algorithm may also be called TD actorcritic. In addition, it is notable that the policy $\pi ( \theta _ { t } )$ is stochastic and hence exploratory. Therefore, it can be directly used to generate experience samples without relying on

Algorithm 10.2: Advantage actor-critic (A2C) or TD actor-critic   
Initialization: A policy function $\pi (a|s,\theta_0)$ where $\theta_0$ is the initial parameter. A value function $v(s,w_0)$ where $w_{0}$ is the initial parameter. $\alpha_w,\alpha_\theta >0$ Goal: Learn an optimal policy to maximize $J(\theta)$ At time step $t$ in each episode, do Generate $a_{t}$ following $\pi (a|s_t,\theta_t)$ and then observe $r_{t + 1},s_{t + 1}$ Advantage (TD error): $\delta_t = r_{t + 1} + \gamma v(s_{t + 1},w_t) - v(s_t,w_t)$ Actor (policy update): $\theta_{t + 1} = \theta_t + \alpha_\theta \delta_t\nabla_\theta \ln \pi (a_t|s_t,\theta_t)$ Critic (value update): $w_{t + 1} = w_{t} + \alpha_{w}\delta_{t}\nabla_{w}v(s_{t},w_{t})$

techniques such as $\varepsilon$ -greedy. There are some variants of A2C such as asynchronous advantage actor-critic (A3C). Interested readers may check [71, 72].

# 10.3 Off-policy actor-critic

The policy gradient methods that we have studied so far, including REINFORCE, QAC, and A2C, are all on-policy. The reason for this can be seen from the expression of the true gradient:

$$
\nabla_ {\theta} J (\theta) = \mathbb {E} _ {S \sim \eta , A \sim \pi} \Big [ \nabla_ {\theta} \ln \pi (A | S, \theta_ {t}) (q _ {\pi} (S, A) - v _ {\pi} (S)) \Big ].
$$

To use samples to approximate this true gradient, we must generate the action samples by following $\pi ( \theta )$ . Hence, $\pi ( \theta )$ is the behavior policy. Since $\pi ( \theta )$ is also the target policy that we aim to improve, the policy gradient methods are on-policy.

In the case that we already have some samples generated by a given behavior policy, the policy gradient methods can still be applied to utilize these samples. To do that, we can employ a technique called importance sampling. It is worth mentioning that the importance sampling technique is not restricted to the field of reinforcement learning. It is a general technique for estimating expected values defined over one probability distribution using some samples drawn from another distribution.

# 10.3.1 Importance sampling

We next introduce the importance sampling technique. Consider a random variable $X \in \mathcal { X }$ . Suppose that $p _ { 0 } ( X )$ is a probability distribution. Our goal is to estimate $\mathbb { E } _ { X \sim p _ { 0 } } | X |$ . Suppose that we have some i.i.d. samples $\{ x _ { i } \} _ { i = 1 } ^ { n }$ .

$\diamond$ First, if the samples $\{ x _ { i } \} _ { i = 1 } ^ { n }$ are generated by following $p _ { 0 }$ , then the average value $\textstyle { \bar { x } } = { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } x _ { i }$ can be used to approximate $\mathbb { E } _ { X \sim p _ { 0 } } | X |$ because $x$ is an unbiased estimate of $\mathbb { E } _ { X \sim p _ { 0 } } [ X ]$ and the estimation variance converges to zero as $n \to \infty$ (see the law of large numbers in Box 5.1 for more information).   
 Second, consider a new scenario where the samples $\{ x _ { i } \} _ { i = 1 } ^ { n }$ are not generated by $p _ { 0 }$ . Instead, they are generated by another distribution $p _ { 1 }$ . Can we still use these samples to approximate $\mathbb { E } _ { X \sim p _ { 0 } } | X |$ ? The answer is yes. However, we can no longer use $\textstyle { \bar { x } } = { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } x _ { i }$ to approximate $\mathbb { E } _ { X \sim p _ { 0 } } [ X ]$ since $\bar { x } \approx \mathbb { E } _ { X \sim p _ { 1 } } | X |$ rather than $\mathbb { E } _ { X \sim p _ { 0 } } [ X ]$ .

In the second scenario, $\mathbb { E } _ { X \sim p _ { 0 } } [ X ]$ can be approximated based on the importance sampling technique. In particular, $\mathbb { E } _ { X \sim p _ { 0 } } | X |$ satisfies

$$
\mathbb {E} _ {X \sim p _ {0}} [ X ] = \sum_ {x \in \mathcal {X}} p _ {0} (x) x = \sum_ {x \in \mathcal {X}} p _ {1} (x) \underbrace {\frac {p _ {0} (x)}{p _ {1} (x)}} _ {f (x)} x = \mathbb {E} _ {X \sim p _ {1}} [ f (X) ]. \tag {10.9}
$$

Thus, estimating $\mathbb { E } _ { X \sim p _ { 0 } } [ X ]$ becomes the problem of estimating $\mathbb { E } _ { X \sim p _ { 1 } } [ f ( X ) ]$ . Let

$$
\bar {f} \doteq \frac {1}{n} \sum_ {i = 1} ^ {n} f (x _ {i}).
$$

Since $f$ can effectively approximate $\mathbb { E } _ { X \sim p _ { 1 } } [ f ( X ) ]$ , it then follows from (10.9) that

$$
\mathbb {E} _ {X \sim p _ {0}} [ X ] = \mathbb {E} _ {X \sim p _ {1}} [ f (X) ] \approx \bar {f} = \frac {1}{n} \sum_ {i = 1} ^ {n} f \left(x _ {i}\right) = \frac {1}{n} \sum_ {i = 1} ^ {n} \underbrace {\frac {p _ {0} \left(x _ {i}\right)}{p _ {1} \left(x _ {i}\right)}} _ {\text {i m p o r t a n c e}} x _ {i}. \tag {10.10}
$$

Equation (10.10) suggests that $\mathbb { E } _ { X \sim p _ { 0 } } | X |$ can be approximated by a weighted average of $x _ { i }$ . Here, $\frac { p _ { 0 } ( x _ { i } ) } { p _ { 1 } ( x _ { i } ) }$ is called the importance weight. When $p _ { 1 } = p _ { 0 }$ , the importance weight is $1$ and $f$ becomes $x$ . When $p _ { 0 } ( x _ { i } ) \geq p _ { 1 } ( x _ { i } )$ , $x _ { i }$ can be sampled more frequently by $p _ { 0 }$ but less frequently by $p _ { 1 }$ . In this case, the importance weight, which is greater than one, emphasizes the importance of this sample.

Some readers may ask the following question: while $p _ { 0 } ( x )$ is required in (10.10), why do we not directly calculate $\mathbb { E } _ { X \sim p _ { 0 } } [ X ]$ using its definition $\begin{array} { r } { \mathbb { E } _ { X \sim p _ { 0 } } [ X ] = \sum _ { x \in \mathcal { X } } p _ { 0 } ( x ) x \ ? } \end{array}$ The answer is as follows. To use the definition, we need to know either the analytical expression of $p _ { 0 }$ or the value of $p _ { 0 } ( x )$ for every $x \in \mathcal { X }$ . However, it is difficult to obtain the analytical expression of $p _ { 0 }$ when the distribution is represented by, for example, a neural network. It is also difficult to obtain the value of $p _ { 0 } ( x )$ for every $x \in \mathcal { X }$ when $\mathcal { X }$ is large. By contrast, (10.10) merely requires the values of $p _ { 0 } ( x _ { i } )$ for some samples and is much easier to implement in practice.

# An illustrative example

We next present an example to demonstrate the importance sampling technique. Consider $X \in \mathcal { X } \doteq \{ + 1 , - 1 \}$ . Suppose that $p _ { 0 }$ is a probability distribution satisfying

$$
p _ {0} (X = + 1) = 0. 5, \quad p _ {0} (X = - 1) = 0. 5.
$$

The expectation of $X$ over $p _ { 0 }$ is

$$
\mathbb {E} _ {X \sim p _ {0}} [ X ] = (+ 1) \cdot 0. 5 + (- 1) \cdot 0. 5 = 0.
$$

Suppose that $p _ { 1 }$ is another distribution satisfying

$$
p _ {1} (X = + 1) = 0. 8, \quad p _ {1} (X = - 1) = 0. 2.
$$

The expectation of $X$ over $p _ { 1 }$ i s

$$
\mathbb {E} _ {X \sim p _ {1}} [ X ] = (+ 1) \cdot 0. 8 + (- 1) \cdot 0. 2 = 0. 6.
$$

Suppose that we have some samples $\{ x _ { i } \}$ drawn over $p _ { 1 }$ . Our goal is to estimate $\mathbb { E } _ { X \sim p _ { 0 } } | X |$ using these samples. As shown in Figure 10.2, there are more samples of $+ 1$ than $^ { - 1 }$ . That is because $p _ { 1 } ( X = + 1 ) = 0 . 8 > p _ { 1 } ( X = - 1 ) = 0 . 2$ . If we directly calculate the average value $\scriptstyle \sum _ { i = 1 } ^ { n } x _ { i } / n$ of the samples, this value converges to $\mathbb { E } _ { X \sim p _ { 1 } } | X | = 0 . 6$ (see the dotted line in Figure 10.2). By contrast, if we calculate the weighted average value as in (10.10), this value can successfully converge to $\mathbb { E } _ { X \sim p _ { 0 } } \lfloor X \rfloor = 0$ (see the solid line in Figure 10.2).

![](images/a07c1bcec11332afb5d96a7908a5491c3693e666373b5f75dbb1a65763cdb547.jpg)  
Figure 10.2: An example for demonstrating the importance sampling technique. Here, $X \in \{ + 1 , - 1 \}$ and $p _ { 0 } ( X = + 1 ) = p _ { 0 } ( X = - 1 ) = 0 . 5$ . The samples are generated according to $p _ { 1 }$ where $p _ { 1 } ( X = + 1 ) = 0 . 8$ and $p _ { 1 } ( X = - 1 ) = 0 . 2$ . The average of the samples converges to $E _ { X \sim p _ { 1 } } | X | = 0 . 6$ , but the weighted average calculated by the importance sampling technique in (10.10) converges to $E _ { X \sim p _ { 0 } } [ X ] = 0$ .

Finally, the distribution $p _ { 1 }$ , which is used to generate samples, must satisfy that $p _ { 1 } ( x ) \neq 0$ when $p _ { 0 } ( x ) \neq 0$ . If $p _ { 1 } ( x ) = 0$ while $p _ { 0 } ( x ) \neq 0$ , the estimation result may be problematic. For example, if

$$
p _ {1} (X = + 1) = 1, \quad p _ {1} (X = - 1) = 0,
$$

then the samples generated by $p _ { 1 }$ are all positive: $\{ x _ { i } \} = \{ + 1 , + 1 , . ~ . ~ . ~ , + 1 \}$ . These samples cannot be used to correctly estimate $\mathbb { E } _ { X \sim p _ { 0 } } [ X ] = 0$ because

$$
\frac {1}{n} \sum_ {i = 1} ^ {n} \frac {p _ {0} (x _ {i})}{p _ {1} (x _ {i})} x _ {i} = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {p _ {0} (+ 1)}{p _ {1} (+ 1)} 1 = \frac {1}{n} \sum_ {i = 1} ^ {n} \frac {0 . 5}{1} 1 \equiv 0. 5,
$$

no matter how large $n$ is.

# 10.3.2 The off-policy policy gradient theorem

With the importance sampling technique, we are ready to present the off-policy policy gradient theorem. Suppose that $\beta$ is a behavior policy. Our goal is to use the samples generated by $\beta$ to learn a target policy $\pi$ that can maximize the following metric:

$$
J (\theta) = \sum_ {s \in \mathcal {S}} d _ {\beta} (s) v _ {\pi} (s) = \mathbb {E} _ {S \sim d _ {\beta}} [ v _ {\pi} (S) ],
$$

where $d _ { \beta }$ is the stationary distribution under policy $\beta$ and $v _ { \pi }$ is the state value under policy $\pi$ . The gradient of this metric is given in the following theorem.

Theorem 10.1 (Off-policy policy gradient theorem). In the discounted case where $\gamma \in$ $( 0 , 1 )$ , the gradient of $J ( \theta )$ is

$$
\nabla_ {\theta} J (\theta) = \mathbb {E} _ {S \sim \rho , A \sim \beta} \left[ \underbrace {\frac {\pi (A | S , \theta)}{\beta (A | S)}} _ {\text {i m p o r t a n c e}} \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \right], \tag {10.11}
$$

where the state distribution ρ is

$$
\rho (s) \doteq \sum_ {s ^ {\prime} \in \mathcal {S}} d _ {\beta} (s ^ {\prime}) \Pr_ {\pi} (s | s ^ {\prime}), \qquad s \in \mathcal {S},
$$

where $\begin{array} { r } { \operatorname* { P r } _ { \pi } ( s | s ^ { \prime } ) = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } [ P _ { \pi } ^ { k } ] _ { s ^ { \prime } s } = [ ( I - \gamma P _ { \pi } ) ^ { - 1 } ] _ { s ^ { \prime } s } } \end{array}$ is the discounted total probability of transitioning from $s ^ { \prime }$ to $s$ under policy $\pi$ .

The gradient in (10.11) is similar to that in the on-policy case in Theorem 9.1, but there are two differences. The first difference is the importance weight. The second difference is that $A \sim \beta$ instead of $A \sim \pi$ . Therefore, we can use the action samples

generated by following $\beta$ to approximate the true gradient. The proof of the theorem is given in Box 10.2.

# Box 10.2: Proof of Theorem 10.1

Since $d _ { \beta }$ is independent of $\theta$ , the gradient of $J ( \theta )$ satisfies

$$
\nabla_ {\theta} J (\theta) = \nabla_ {\theta} \sum_ {s \in \mathcal {S}} d _ {\beta} (s) v _ {\pi} (s) = \sum_ {s \in \mathcal {S}} d _ {\beta} (s) \nabla_ {\theta} v _ {\pi} (s). \tag {10.12}
$$

According to Lemma 9.2, the expression of $\nabla _ { \theta } v _ { \pi } ( s )$ is

$$
\nabla_ {\theta} v _ {\pi} (s) = \sum_ {s ^ {\prime} \in \mathcal {S}} \Pr_ {\pi} \left(s ^ {\prime} \mid s\right) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi \left(a \mid s ^ {\prime}, \theta\right) q _ {\pi} \left(s ^ {\prime}, a\right), \tag {10.13}
$$

where $\begin{array} { r } { \operatorname* { P r } _ { \pi } ( s ^ { \prime } | s ) \ \dot { = } \ \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } \big [ P _ { \pi } ^ { k } \big ] _ { s s ^ { \prime } } \ = \ \big [ ( I _ { n } - \gamma P _ { \pi } ) ^ { - 1 } \big ] _ { s s ^ { \prime } } } \end{array}$ . Substituting (10.13) into (10.12) yields

$$
\begin{array}{l} \nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} d _ {\beta} (s) \nabla_ {\theta} v _ {\pi} (s) = \sum_ {s \in \mathcal {S}} d _ {\beta} (s) \sum_ {s ^ {\prime} \in \mathcal {S}} \Pr_ {\pi} (s ^ {\prime} | s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s ^ {\prime}, \theta) q _ {\pi} (s ^ {\prime}, a) \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} \left(\sum_ {s \in \mathcal {S}} d _ {\beta} (s) \Pr_ {\pi} \left(s ^ {\prime} \mid s\right)\right) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a \mid s ^ {\prime}, \theta) q _ {\pi} \left(s ^ {\prime}, a\right) \\ \dot {=} \sum_ {s ^ {\prime} \in \mathcal {S}} \rho (s ^ {\prime}) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s ^ {\prime}, \theta) q _ {\pi} (s ^ {\prime}, a) \\ = \sum_ {s \in \mathcal {S}} \rho (s) \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | s, \theta) q _ {\pi} (s, a) \quad (\text {c h a n g e} s ^ {\prime} \text {t o} s) \\ = \mathbb {E} _ {S \sim \rho} \left[ \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | S, \theta) q _ {\pi} (S, a) \right]. \\ \end{array}
$$

By using the importance sampling technique, the above equation can be further rewritten as

$$
\begin{array}{l} \mathbb {E} _ {S \sim \rho} \left[ \sum_ {a \in \mathcal {A}} \nabla_ {\theta} \pi (a | S, \theta) q _ {\pi} (S, a) \right] = \mathbb {E} _ {S \sim \rho} \left[ \sum_ {a \in \mathcal {A}} \beta (a | S) \frac {\pi (a | S , \theta)}{\beta (a | S)} \frac {\nabla_ {\theta} \pi (a | S , \theta)}{\pi (a | S , \theta)} q _ {\pi} (S, a) \right] \\ = \mathbb {E} _ {S \sim \rho} \left[ \sum_ {a \in \mathcal {A}} \beta (a | S) \frac {\pi (a | S , \theta)}{\beta (a | S)} \nabla_ {\theta} \ln \pi (a | S, \theta) q _ {\pi} (S, a) \right] \\ = \mathbb {E} _ {S \sim \rho , A \sim \beta} \left[ \frac {\pi (A | S , \theta)}{\beta (A | S)} \nabla_ {\theta} \ln \pi (A | S, \theta) q _ {\pi} (S, A) \right]. \\ \end{array}
$$

The proof is complete. The above proof is similar to that of Theorem 9.1.

# 10.3.3 Algorithm description

Based on the off-policy policy gradient theorem, we are ready to present the off-policy actor-critic algorithm. Since the off-policy case is very similar to the on-policy case, we merely present some key steps.

First, the off-policy policy gradient is invariant to any additional baseline $b ( s )$ . In particular, we have

$$
\nabla_ {\theta} J (\theta) = \mathbb {E} _ {S \sim \rho , A \sim \beta} \left[ \frac {\pi (A | S , \theta)}{\beta (A | S)} \nabla_ {\theta} \ln \pi (A | S, \theta) \big (q _ {\pi} (S, A) - b (S) \big) \right],
$$

because $\begin{array} { r } { \mathbb { E } \left[ \frac { \pi ( A | S , \theta ) } { \beta ( A | S ) } \nabla _ { \theta } \ln \pi ( A | S , \theta ) b ( S ) \right] = 0 } \end{array}$ hπ(A|S,θ)β(A|S) ∇θ ln π(A|S, θ)b(S)i = 0. To reduce the estimation variance, we can select the baseline as $b ( S ) = v _ { \pi } ( S )$ and obtain

$$
\nabla_ {\theta} J (\theta) = \mathbb {E} \left[ \frac {\pi (A | S , \theta)}{\beta (A | S)} \nabla_ {\theta} \ln \pi (A | S, \theta) \big (q _ {\pi} (S, A) - v _ {\pi} (S) \big) \right].
$$

The corresponding stochastic gradient-ascent algorithm is

$$
\theta_ {t + 1} = \theta_ {t} + \alpha_ {\theta} \frac {\pi (a _ {t} | s _ {t} , \theta_ {t})}{\beta (a _ {t} | s _ {t})} \nabla_ {\theta} \ln \pi (a _ {t} | s _ {t}, \theta_ {t}) \big (q _ {t} (s _ {t}, a _ {t}) - v _ {t} (s _ {t}) \big),
$$

where $\alpha _ { \theta } > 0$ . Similar to the on-policy case, the advantage function $q _ { t } ( s , a ) - v _ { t } ( s )$ can be replaced by the TD error. That is

$$
q _ {t} (s _ {t}, a _ {t}) - v _ {t} (s _ {t}) \approx r _ {t + 1} + \gamma v _ {t} (s _ {t + 1}) - v _ {t} (s _ {t}) \doteq \delta_ {t} (s _ {t}, a _ {t}).
$$

Then, the algorithm becomes

$$
\theta_ {t + 1} = \theta_ {t} + \alpha_ {\theta} \frac {\pi (a _ {t} | s _ {t} , \theta)}{\beta (a _ {t} | s _ {t})} \nabla_ {\theta} \ln \pi (a _ {t} | s _ {t}, \theta) \delta_ {t} (s _ {t}, a _ {t}).
$$

The implementation of the off-policy actor-critic algorithm is summarized in Algorithm 10.3. As can be seen, the algorithm is the same as the advantage actor-critic algorithm except that an additional importance weight is included in both the critic and the actor. It must be noted that, in addition to the actor, the critic is also converted from on-policy to off-policy by the importance sampling technique. In fact, importance sampling is a general technique that can be applied to both policy-based and value-based algorithms. Finally, Algorithm 10.3 can be extended in various ways to incorporate more techniques such as eligibility traces [73].

# Algorithm 10.3: Off-policy actor-critic based on importance sampling

Initialization: A given behavior policy $\beta ( a | s )$ . A target policy $\pi ( a | s , \theta _ { 0 } )$ where $\theta _ { 0 }$ is the initial parameter. A value function $v ( s , w _ { 0 } )$ where $w _ { 0 }$ is the initial parameter. $\alpha _ { w } , \alpha _ { \theta } > 0$ .

Goal: Learn an optimal policy to maximize $J ( \theta )$ .

At time step $t$ in each episode, do

Generate $a _ { t }$ following $\beta ( s _ { t } )$ and then observe $r _ { t + 1 } , s _ { t + 1 }$

Advantage (TD error):

$$
\delta_ {t} = r _ {t + 1} + \gamma v (s _ {t + 1}, w _ {t}) - v (s _ {t}, w _ {t})
$$

Actor (policy update):

$$
\theta_ {t + 1} = \theta_ {t} + \alpha_ {\theta} \frac {\pi (a _ {t} | s _ {t} , \theta_ {t})}{\beta (a _ {t} | s _ {t})} \delta_ {t} \nabla_ {\theta} \ln \pi (a _ {t} | s _ {t}, \theta_ {t})
$$

Critic (value update):

$$
w _ {t + 1} = w _ {t} + \alpha_ {w} \frac {\pi (a _ {t} | s _ {t} , \theta_ {t})}{\beta (a _ {t} | s _ {t})} \delta_ {t} \nabla_ {w} v (s _ {t}, w _ {t})
$$

# 10.4 Deterministic actor-critic

Up to now, the policies used in the policy gradient methods are all stochastic since it is required that $\pi ( a | s , \theta ) > 0$ for every $( s , a )$ . This section shows that deterministic policies can also be used in policy gradient methods. Here, “deterministic” indicates that, for any state, a single action is given a probability of one and all the other actions are given probabilities of zero. It is important to study the deterministic case since it is naturally off-policy and can effectively handle continuous action spaces.

We have been using $\pi ( a | s , \theta )$ to denote a general policy, which can be either stochastic or deterministic. In this section, we use

$$
a = \mu (s, \theta)
$$

to specifically denote a deterministic policy. Different from $\pi$ which gives the probability of an action, $\mu$ directly gives the action since it is a mapping from $\boldsymbol { S }$ to $\mathcal { A }$ . This deterministic policy can be represented by, for example, a neural network with $s$ as its input, $a$ as its output, and $\theta$ as its parameter. For the sake of simplicity, we often write $\mu ( s , \theta )$ as $\mu ( s )$ for short.

# 10.4.1 The deterministic policy gradient theorem

The policy gradient theorem introduced in the last chapter is only valid for stochastic policies. When we require the policy to be deterministic, a new policy gradient theorem must be derived.

Theorem 10.2 (Deterministic policy gradient theorem). The gradient of $J ( \theta )$ is

$$
\begin{array}{l} \nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} \eta (s) \nabla_ {\theta} \mu (s) \big (\nabla_ {a} q _ {\mu} (s, a) \big) | _ {a = \mu (s)} \\ = \mathbb {E} _ {S \sim \eta} \left[ \nabla_ {\theta} \mu (S) \left(\nabla_ {a} q _ {\mu} (S, a)\right) | _ {a = \mu (S)} \right], \tag {10.14} \\ \end{array}
$$

where $\eta$ is a distribution of the states.

Theorem 10.2 is a summary of the results presented in Theorem 10.3 and Theorem 10.4 since the gradients in the two theorems have similar expressions. The specific expressions of $J ( \theta )$ and $\eta$ can be found in Theorems 10.3 and 10.4.

Unlike the stochastic case, the gradient in the deterministic case shown in (10.14) does not involve the action random variable $A$ . As a result, when we use samples to approximate the true gradient, it is not required to sample actions. Therefore, the deterministic policy gradient method is off-policy. In addition, some readers may wonder why $\left( \nabla _ { a } q _ { \mu } ( S , a ) \right) | _ { a = \mu ( S ) }$ cannot be written as $\nabla _ { a } q _ { \mu } ( S , \mu ( S ) )$ , which seems more concise. That is simply because, if we do that, it is unclear how $q _ { \mu } ( S , \mu ( S ) )$ is a function of $a$ . A concise yet less confusing expression may be $\nabla _ { a } q _ { \mu } ( S , a = \mu ( S ) )$ .

In the rest of this subsection, we present the derivation details of Theorem 10.2. In particular, we derive the gradients of two common metrics: the first is the average value and the second is the average reward. Since these two metrics have been discussed in detail in Section 9.2, we sometimes use their properties without proof. For most readers, it is sufficient to be familiar with Theorem 10.2 without knowing its derivation details. Interested readers can selectively examine the details in the remainder of this section.

# Metric 1: Average value

We first derive the gradient of the average value:

$$
J (\theta) = \mathbb {E} [ v _ {\mu} (s) ] = \sum_ {s \in \mathcal {S}} d _ {0} (s) v _ {\mu} (s), \tag {10.15}
$$

where $d _ { 0 }$ is the probability distribution of the states. Here, $d _ { 0 }$ is selected to be independent of $\mu$ for simplicity. There are two special yet important cases of selecting $d _ { 0 }$ . The first case is that $d _ { 0 } ( s _ { 0 } ) = 1$ and $d _ { 0 } ( s \ne s _ { 0 } ) = 0$ , where $s _ { 0 }$ is a specific state of interest. In this case, the policy aims to maximize the discounted return that can be obtained when starting from $s _ { 0 }$ . The second case is that $d _ { 0 }$ is the distribution of a given behavior policy that is different from the target policy.

To calculate the gradient of $J ( \theta )$ , we need to first calculate the gradient of $v _ { \mu } ( s )$ for any $s \in S$ . Consider the discounted case where $\gamma \in ( 0 , 1 )$ .

Lemma 10.1 (Gradient of $v _ { \mu } ( s )$ ). In the discounted case, it holds for any $s \in S$ that

$$
\nabla_ {\theta} v _ {\mu} (s) = \sum_ {s ^ {\prime} \in \mathcal {S}} \Pr_ {\mu} \left(s ^ {\prime} \mid s\right) \nabla_ {\theta} \mu \left(s ^ {\prime}\right) \left(\nabla_ {a} q _ {\mu} \left(s ^ {\prime}, a\right)\right) | _ {a = \mu \left(s ^ {\prime}\right)}, \tag {10.16}
$$

where

$$
\operatorname * {P r} _ {\mu} (s ^ {\prime} | s) \doteq \sum_ {k = 0} ^ {\infty} \gamma^ {k} [ P _ {\mu} ^ {k} ] _ {s s ^ {\prime}} = \left[ (I - \gamma P _ {\mu}) ^ {- 1} \right] _ {s s ^ {\prime}}
$$

is the discounted total probability of transitioning from s to $s ^ { \prime }$ under policy µ. Here, $[ \cdot ] _ { s s ^ { \prime } }$ denotes the entry in the sth row and s0th column of a matrix.

# Box 10.3: Proof of Lemma 10.1

Since the policy is deterministic, we have

$$
v _ {\mu} (s) = q _ {\mu} (s, \mu (s)).
$$

Since both $q _ { \mu }$ and $\mu$ are functions of $\theta$ , we have

$$
\nabla_ {\theta} v _ {\mu} (s) = \nabla_ {\theta} q _ {\mu} (s, \mu (s)) = \big (\nabla_ {\theta} q _ {\mu} (s, a) \big) | _ {a = \mu (s)} + \nabla_ {\theta} \mu (s) \big (\nabla_ {a} q _ {\mu} (s, a) \big) | _ {a = \mu (s)}. \tag {10.17}
$$

By the definition of action values, for any given $( s , a )$ , we have

$$
q _ {\mu} (s, a) = r (s, a) + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) v _ {\mu} (s ^ {\prime}),
$$

where $\begin{array} { r } { r ( s , a ) = \sum _ { r } r p ( r | s , a ) } \end{array}$ . Since $r ( s , a )$ is independent of $\mu$ , we have

$$
\nabla_ {\theta} q _ {\mu} (s, a) = 0 + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) \nabla_ {\theta} v _ {\mu} (s ^ {\prime}).
$$

Substituting the above equation into (10.17) yields

$$
\nabla_ {\theta} v _ {\mu} (s) = \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, \mu (s)) \nabla_ {\theta} v _ {\mu} (s ^ {\prime}) + \underbrace {\nabla_ {\theta} \mu (s) \left(\nabla_ {a} q _ {\mu} (s , a)\right) | _ {a = \mu (s)}} _ {u (s)}, \quad s \in \mathcal {S}.
$$

Since the above equation is valid for all $s \in S$ , we can combine these equations to obtain a matrix-vector form:

$$
\underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\mu} (s) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\mu} \in \mathbb {R} ^ {m n}} = \underbrace {\left[ \begin{array}{c} \vdots \\ u (s) \\ \vdots \end{array} \right]} _ {u \in \mathbb {R} ^ {m n}} + \gamma (P _ {\mu} \otimes I _ {m}) \underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\mu} (s ^ {\prime}) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\mu} \in \mathbb {R} ^ {m n}},
$$

where $n = | S |$ , $m$ is the dimensionality of $\theta$ , $P _ { \mu }$ is the state transition matrix with $[ P _ { \mu } ] _ { s s ^ { \prime } } = p ( s ^ { \prime } | s , \mu ( s ) )$ , and $\otimes$ is the Kronecker product. The above matrix-vector form can be written concisely as

$$
\nabla_ {\theta} v _ {\mu} = u + \gamma (P _ {\mu} \otimes I _ {m}) \nabla_ {\theta} v _ {\mu},
$$

which is a linear equation of $\nabla _ { \boldsymbol { \theta } } v _ { \mu }$ . Then, $\nabla _ { \boldsymbol { \theta } } v _ { \mu }$ can be solved as

$$
\begin{array}{l} \nabla_ {\theta} v _ {\mu} = \left(I _ {m n} - \gamma P _ {\mu} \otimes I _ {m}\right) ^ {- 1} u \\ = \left(I _ {n} \otimes I _ {m} - \gamma P _ {\mu} \otimes I _ {m}\right) ^ {- 1} u \\ = \left[ \left(I _ {n} - \gamma P _ {\mu}\right) ^ {- 1} \otimes I _ {m} \right] u. \tag {10.18} \\ \end{array}
$$

The elementwise form of (10.18) is

$$
\begin{array}{l} \nabla_ {\theta} v _ {\mu} (s) = \sum_ {s ^ {\prime} \in \mathcal {S}} \left[ (I - \gamma P _ {\mu}) ^ {- 1} \right] _ {s s ^ {\prime}} u (s ^ {\prime}) \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} \left[ (I - \gamma P _ {\mu}) ^ {- 1} \right] _ {s s ^ {\prime}} \left[ \nabla_ {\theta} \mu \left(s ^ {\prime}\right) \left(\nabla_ {a} q _ {\mu} \left(s ^ {\prime}, a\right)\right) | _ {a = \mu \left(s ^ {\prime}\right)} \right]. \tag {10.19} \\ \end{array}
$$

The quantity $\left[ ( I - \gamma P _ { \mu } ) ^ { - 1 } \right] _ { s s ^ { \prime } }$ has a clear probabilistic interpretation. Since $( I -$ $\gamma P _ { \mu } ) ^ { - 1 } = I + \gamma P _ { \mu } + \gamma ^ { 2 } P _ { \mu } ^ { 2 } + \cdot \cdot \cdot$ , we have

$$
\left[ (I - \gamma P _ {\mu}) ^ {- 1} \right] _ {s s ^ {\prime}} = [ I ] _ {s s ^ {\prime}} + \gamma [ P _ {\mu} ] _ {s s ^ {\prime}} + \gamma^ {2} [ P _ {\mu} ^ {2} ] _ {s s ^ {\prime}} + \dots = \sum_ {k = 0} ^ {\infty} \gamma^ {k} [ P _ {\mu} ^ {k} ] _ {s s ^ {\prime}}.
$$

Note that $[ P _ { \mu } ^ { k } ] _ { s s ^ { \prime } }$ is the probability of transitioning from $s$ to $s ^ { \prime }$ using exactly $k$ steps (see Box 8.1 for more information). Therefore, $\left[ ( I - \gamma P _ { \mu } ) ^ { - 1 } \right] _ { s s ^ { \prime } }$ is the discounted total probability of transitioning from $s$ to $s ^ { \prime }$ using any number of steps. By denoting $[ ( I - \gamma P _ { \mu } ) ^ { - 1 } ] _ { s s ^ { \prime } } \doteq \operatorname* { P r } _ { \mu } ( s ^ { \prime } | s )$ , equation (10.19) leads to (10.16).

With the preparation in Lemma 10.1, we are ready to derive the gradient of $J ( \theta )$ .

Theorem 10.3 (Deterministic policy gradient theorem in the discounted case). In the

discounted case where $\gamma \in ( 0 , 1 )$ , the gradient of $J ( \theta )$ in (10.15) is

$$
\begin{array}{l} \nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} \rho_ {\mu} (s) \nabla_ {\theta} \mu (s) \left(\nabla_ {a} q _ {\mu} (s, a)\right) | _ {a = \mu (s)} \\ = \mathbb {E} _ {S \sim \rho_ {\mu}} \left[ \nabla_ {\theta} \mu (S) \left(\nabla_ {a} q _ {\mu} (S, a)\right) | _ {a = \mu (S)} \right], \\ \end{array}
$$

where the state distribution $\rho _ { \mu }$ is

$$
\rho_ {\mu} (s) = \sum_ {s ^ {\prime} \in \mathcal {S}} d _ {0} (s ^ {\prime}) \mathrm {P r} _ {\mu} (s | s ^ {\prime}), \qquad s \in \mathcal {S}.
$$

Here, $\begin{array} { r } { \operatorname* { P r } _ { \boldsymbol { \mu } } ( s | s ^ { \prime } ) = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } [ P _ { \boldsymbol { \mu } } ^ { k } ] _ { s ^ { \prime } s } = [ ( I - \gamma P _ { \boldsymbol { \mu } } ) ^ { - 1 } ] _ { s ^ { \prime } s } } \end{array}$ is the discounted total probability of transitioning from $s ^ { \prime }$ to s under policy $\mu$ .

# Box 10.4: Proof of Theorem 10.3

Since $d _ { 0 }$ is independent of $\mu$ , we have

$$
\nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} d _ {0} (s) \nabla_ {\theta} v _ {\mu} (s).
$$

Substituting the expression of $\nabla _ { \theta } v _ { \mu } ( s )$ given by Lemma 10.1 into the above equation yields

$$
\begin{array}{l} \nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} d _ {0} (s) \nabla_ {\theta} v _ {\mu} (s) \\ = \sum_ {s \in \mathcal {S}} d _ {0} (s) \sum_ {s ^ {\prime} \in \mathcal {S}} \Pr_ {\mu} \left(s ^ {\prime} | s\right) \nabla_ {\theta} \mu \left(s ^ {\prime}\right) \left(\nabla_ {a} q _ {\mu} \left(s ^ {\prime}, a\right)\right) \mid_ {a = \mu \left(s ^ {\prime}\right)} \\ = \sum_ {s ^ {\prime} \in \mathcal {S}} \left(\sum_ {s \in \mathcal {S}} d _ {0} (s) \Pr_ {\mu} \left(s ^ {\prime} \mid s\right)\right) \nabla_ {\theta} \mu \left(s ^ {\prime}\right) \left(\nabla_ {a} q _ {\mu} \left(s ^ {\prime}, a\right)\right) | _ {a = \mu \left(s ^ {\prime}\right)} \\ \dot {=} \sum_ {s ^ {\prime} \in \mathcal {S}} \rho_ {\mu} (s ^ {\prime}) \nabla_ {\theta} \mu (s ^ {\prime}) \left(\nabla_ {a} q _ {\mu} (s ^ {\prime}, a)\right) | _ {a = \mu (s ^ {\prime})} \\ = \sum_ {s \in \mathcal {S}} \rho_ {\mu} (s) \nabla_ {\theta} \mu (s) \left(\nabla_ {a} q _ {\mu} (s, a)\right) | _ {a = \mu (s)} \quad (\text {c h a n g e} s ^ {\prime} \text {t o} s) \\ = \mathbb {E} _ {S \sim \rho_ {\mu}} \left[ \nabla_ {\theta} \mu (S) \left(\nabla_ {a} q _ {\mu} (S, a)\right) | _ {a = \mu (S)} \right]. \\ \end{array}
$$

The proof is complete. The above proof is consistent with the proof of Theorem 1 in [74]. Here, we consider the case in which the states and actions are finite. When they are continuous, the proof is similar, but the summations should be replaced by integrals [74].

# Metric 2: Average reward

We next derive the gradient of the average reward:

$$
\begin{array}{l} J (\theta) = \bar {r} _ {\mu} = \sum_ {s \in \mathcal {S}} d _ {\mu} (s) r _ {\mu} (s) \\ = \mathbb {E} _ {S \sim d _ {\mu}} \left[ r _ {\mu} (S) \right], \tag {10.20} \\ \end{array}
$$

where

$$
r _ {\mu} (s) = \mathbb {E} [ R | s, a = \mu (s) ] = \sum_ {r} r p (r | s, a = \mu (s))
$$

is the expectation of the immediate rewards. More information about this metric can be found in Section 9.2.

The gradient of $J ( \theta )$ is given in the following theorem.

Theorem 10.4 (Deterministic policy gradient theorem in the undiscounted case). In the undiscounted case, the gradient of $J ( \theta )$ in (10.20) is

$$
\begin{array}{l} \nabla_ {\theta} J (\theta) = \sum_ {s \in \mathcal {S}} d _ {\mu} (s) \nabla_ {\theta} \mu (s) \left(\nabla_ {a} q _ {\mu} (s, a)\right) | _ {a = \mu (s)} \\ = \mathbb {E} _ {S \sim d _ {\mu}} \left[ \nabla_ {\theta} \mu (S) \left(\nabla_ {a} q _ {\mu} (S, a)\right) | _ {a = \mu (S)} \right], \\ \end{array}
$$

where $d _ { \mu }$ is the stationary distribution of the states under policy µ.

# Box 10.5: Proof of Theorem 10.4

Since the policy is deterministic, we have

$$
v _ {\mu} (s) = q _ {\mu} (s, \mu (s)).
$$

Since both $q _ { \mu }$ and $\mu$ are functions of $\theta$ , we have

$$
\nabla_ {\theta} v _ {\mu} (s) = \nabla_ {\theta} q _ {\mu} (s, \mu (s)) = (\nabla_ {\theta} q _ {\mu} (s, a)) | _ {a = \mu (s)} + \nabla_ {\theta} \mu (s) (\nabla_ {a} q _ {\mu} (s, a)) | _ {a = \mu (s)}. \tag {10.21}
$$

In the undiscounted case, it follows from the definition of action value (Section 9.3.2) that

$$
\begin{array}{l} q _ {\mu} (s, a) = \mathbb {E} \left[ R _ {t + 1} - \bar {r} _ {\mu} + v _ {\mu} \left(S _ {t + 1}\right) \mid s, a \right] \\ = \sum_ {r} p (r | s, a) (r - \bar {r} _ {\mu}) + \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) v _ {\mu} \left(s ^ {\prime}\right) \\ = r (s, a) - \bar {r} _ {\mu} + \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) v _ {\mu} (s ^ {\prime}). \\ \end{array}
$$

Since $\begin{array} { r } { r ( s , a ) = \sum _ { r } r p ( r | s , a ) } \end{array}$ is independent of $\theta$ , we have

$$
\nabla_ {\theta} q _ {\mu} (s, a) = 0 - \nabla_ {\theta} \bar {r} _ {\mu} + \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, a) \nabla_ {\theta} v _ {\mu} (s ^ {\prime}).
$$

Substituting the above equation into (10.21) gives

$$
\nabla_ {\theta} v _ {\mu} (s) = - \nabla_ {\theta} \bar {r} _ {\mu} + \sum_ {s ^ {\prime}} p (s ^ {\prime} | s, \mu (s)) \nabla_ {\theta} v _ {\mu} (s ^ {\prime}) + \underbrace {\nabla_ {\theta} \mu (s) \left(\nabla_ {a} q _ {\mu} (s , a)\right) | _ {a = \mu (s)}} _ {u (s)}, \quad s \in \mathcal {S}.
$$

While the above equation is valid for all $s \in S$ , we can combine these equations to obtain a matrix-vector form:

$$
\underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\mu} (s) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\mu} \in \mathbb {R} ^ {m n}} = - \mathbf {1} _ {n} \otimes \nabla_ {\theta} \bar {r} _ {\mu} + (P _ {\mu} \otimes I _ {m}) \underbrace {\left[ \begin{array}{c} \vdots \\ \nabla_ {\theta} v _ {\mu} (s ^ {\prime}) \\ \vdots \end{array} \right]} _ {\nabla_ {\theta} v _ {\mu} \in \mathbb {R} ^ {m n}} + \underbrace {\left[ \begin{array}{c} \vdots \\ u (s) \\ \vdots \end{array} \right]} _ {u \in \mathbb {R} ^ {m n}},
$$

where $n \ = \ | { \cal S } |$ , $m$ is the dimension of $\theta$ , $P _ { \mu }$ is the state transition matrix with $[ P _ { \mu } ] _ { s s ^ { \prime } } = p ( s ^ { \prime } | s , \mu ( s ) )$ , and $\otimes$ is the Kronecker product. The above matrix-vector form can be written concisely as

$$
\nabla_ {\theta} v _ {\mu} = u - \mathbf {1} _ {n} \otimes \nabla_ {\theta} \bar {r} _ {\mu} + (P _ {\mu} \otimes I _ {m}) \nabla_ {\theta} v _ {\mu},
$$

and hence

$$
\mathbf {1} _ {n} \otimes \nabla_ {\theta} \bar {r} _ {\mu} = u + \left(P _ {\mu} \otimes I _ {m}\right) \nabla_ {\theta} v _ {\mu} - \nabla_ {\theta} v _ {\mu}. \tag {10.22}
$$

Since $d _ { \mu }$ is the stationary distribution, we have $d _ { \mu } ^ { I ^ { \prime } } P _ { \mu } = d _ { \mu } ^ { I ^ { \prime } }$ . Multiplying $d _ { \mu } ^ { I ^ { \prime } } \otimes I _ { m }$ on both sides of (10.22) gives

$$
\begin{array}{l} \left(d _ {\mu} ^ {T} \mathbf {1} _ {n}\right) \otimes \nabla_ {\theta} \bar {r} _ {\mu} = d _ {\mu} ^ {T} \otimes I _ {m} u + \left(d _ {\mu} ^ {T} P _ {\mu}\right) \otimes I _ {m} \nabla_ {\theta} v _ {\mu} - d _ {\mu} ^ {T} \otimes I _ {m} \nabla_ {\theta} v _ {\mu} \\ = d _ {\mu} ^ {T} \otimes I _ {m} u + d _ {\mu} ^ {T} \otimes I _ {m} \nabla_ {\theta} v _ {\mu} - d _ {\mu} ^ {T} \otimes I _ {m} \nabla_ {\theta} v _ {\mu} \\ = d _ {\mu} ^ {T} \otimes I _ {m} u. \\ \end{array}
$$

Since $d _ { \mu } ^ { I ^ { \prime } } \mathbf { 1 } _ { n } = 1$ , the above equations become

$$
\begin{array}{l} \nabla_ {\theta} \bar {r} _ {\mu} = d _ {\mu} ^ {T} \otimes I _ {m} u \\ = \sum_ {s \in \mathcal {S}} d _ {\mu} (s) u (s) \\ = \sum_ {s \in \mathcal {S}} d _ {\mu} (s) \nabla_ {\theta} \mu (s) \left(\nabla_ {a} q _ {\mu} (s, a)\right) | _ {a = \mu (s)} \\ = \mathbb {E} _ {S \sim d _ {\mu}} \left[ \nabla_ {\theta} \mu (S) \left(\nabla_ {a} q _ {\mu} (S, a)\right) | _ {a = \mu (S)} \right]. \\ \end{array}
$$

The proof is complete.

# 10.4.2 Algorithm description

Based on the gradient given in Theorem 10.2, we can apply the gradient-ascent algorithm to maximize $J ( \theta )$ :

$$
\theta_ {t + 1} = \theta_ {t} + \alpha_ {\theta} \mathbb {E} _ {S \sim \eta} \left[ \nabla_ {\theta} \mu (S) \big (\nabla_ {a} q _ {\mu} (S, a) \big) | _ {a = \mu (S)} \right].
$$

The corresponding stochastic gradient-ascent algorithm is

$$
\theta_ {t + 1} = \theta_ {t} + \alpha_ {\theta} \nabla_ {\theta} \mu (s _ {t}) \big (\nabla_ {a} q _ {\mu} (s _ {t}, a) \big) | _ {a = \mu (s _ {t})}.
$$

The implementation is summarized in Algorithm 10.4. It should be noted that this algorithm is off-policy since the behavior policy $\beta$ may be different from $\mu$ . First, the actor is off-policy. We already explained the reason when presenting Theorem 10.2. Second, the critic is also off-policy. Special attention must be paid to why the critic is off-policy but does not require the importance sampling technique. In particular, the experience sample required by the critic is $( s _ { t } , a _ { t } , r _ { t + 1 } , s _ { t + 1 } , \tilde { a } _ { t + 1 } )$ , where $\tilde { a } _ { t + 1 } = \mu ( s _ { t + 1 } )$ . The generation of this experience sample involves two policies. The first is the policy for generating $a _ { t }$ at $s _ { t }$ , and the second is the policy for generating $\tilde { a } _ { t + 1 }$ at $s _ { t + 1 }$ . The first policy that generates $a _ { t }$ is the behavior policy since $a _ { t }$ is used to interact with the environment. The second policy must be $\mu$ because it is the policy that the critic aims to evaluate. Hence, $\mu$ is the target policy. It should be noted that $\tilde { a } _ { t + 1 }$ is not used to interact with the environment in the next time step. Hence, $\mu$ is not the behavior policy. Therefore, the critic is off-policy.

How to select the function $q ( s , a , w )$ ? The original research work [74] that proposed the deterministic policy gradient method adopted linear functions: $q ( s , a , w ) = \phi ^ { T } ( s , a ) w$ where $\phi ( s , a )$ is the feature vector. It is currently popular to represent $q ( s , a , w )$ using neural networks, as suggested in the deep deterministic policy gradient (DDPG) method [75].

# Algorithm 10.4: Deterministic policy gradient or deterministic actor-critic

Initialization: A given behavior policy $\beta ( a | s )$ . A deterministic target policy $\mu ( s , \theta _ { 0 } )$ where $\theta _ { 0 }$ is the initial parameter. A value function $q ( s , a , w _ { 0 } )$ where $w _ { 0 }$ is the initial parameter. $\alpha _ { w } , \alpha _ { \theta } > 0$ .

Goal: Learn an optimal policy to maximize $J ( \theta )$ .

At time step $t$ in each episode, do

Generate $a _ { t }$ following $\beta$ and then observe $r _ { t + 1 } , s _ { t + 1 }$

TD error:

$$
\delta_ {t} = r _ {t + 1} + \gamma q (s _ {t + 1}, \mu (s _ {t + 1}, \theta_ {t}), w _ {t}) - q (s _ {t}, a _ {t}, w _ {t})
$$

Actor (policy update):

$$
\theta_ {t + 1} = \theta_ {t} + \alpha_ {\theta} \nabla_ {\theta} \mu (s _ {t}, \theta_ {t}) \left(\nabla_ {a} q (s _ {t}, a, w _ {t})\right) | _ {a = \mu (s _ {t})}
$$

Critic (value update):

$$
w _ {t + 1} = w _ {t} + \alpha_ {w} \delta_ {t} \nabla_ {w} q \left(s _ {t}, a _ {t}, w _ {t}\right)
$$

How to select the behavior policy $\beta$ ? It can be any exploratory policy. It can also be a stochastic policy obtained by adding noise to $\mu$ [75]. In this case, $\mu$ is also the behavior policy and hence this way is an on-policy implementation.

# 10.5 Summary

In this chapter, we introduced actor-critic methods. The contents are summarized as follows.

 Section 10.1 introduced the simplest actor-critic algorithm called QAC. This algorithm is similar to the policy gradient algorithm, REINFORCE, introduced in the last chapter. The only difference is that the q-value estimation in QAC relies on TD learning while REINFORCE relies on Monte Carlo estimation.   
Section 10.2 extended QAC to advantage actor-critic. It was shown that the policy gradient is invariant to any additional baseline. It was then shown that an optimal baseline could help reduce the estimation variance.   
Section 10.3 further extended the advantage actor-critic algorithm to the off-policy case. To do that, we introduced an important technique called importance sampling.   
 Finally, while all the previously presented policy gradient algorithms rely on stochastic policies, we showed in Section 10.4 that the policy can be forced to be deterministic. The corresponding gradient was derived, and the deterministic policy gradient algorithm was introduced.

Policy gradient and actor-critic methods are widely used in modern reinforcement learning. There exist a large number of advanced algorithms in the literature such as SAC [76,77], TRPO [78], PPO [79], and TD3 [80]. In addition, the single-agent case can

also be extended to the case of multi-agent reinforcement learning [81–85]. Experience samples can also be used to fit system models to achieve model-based reinforcement learning [15, 86, 87]. Distributional reinforcement learning provides a fundamentally different perspective from the conventional one [88, 89]. The relationships between reinforcement learning and control theory have been discussed in [90–95]. This book is not able to cover all these topics. Hopefully, the foundations laid by this book can help readers better study them in the future.

# 10.6 Q&A

 Q: What is the relationship between actor-critic and policy gradient methods?

A: Actor-critic methods are actually policy gradient methods. Sometimes, we use them interchangeably. It is required to estimate action values in any policy gradient algorithm. When the action values are estimated using temporal-difference learning with value function approximation, such a policy gradient algorithm is called actorcritic. The name “actor-critic” highlights its algorithmic structure that combines the components of policy update and value update. This structure is also the fundamental structure used in all reinforcement learning algorithms.

 Q: Why is it important to introduce additional baselines to actor-critic methods?

A: Since the policy gradient is invariant to any additional baseline, we can utilize the baseline to reduce estimation variance. The resulting algorithm is called advantage actor-critic.

 Q: Can importance sampling be used in value-based algorithms other than policybased ones?

A: The answer is yes. That is because importance sampling is a general technique for estimating the expectation of a random variable over one distribution using some samples drawn from another distribution. The reason why this technique is useful in reinforcement learning is that the many problems in reinforcement learning are to estimate expectations. For example, in value-based methods, the action or state values are defined as expectations. In the policy gradient method, the true gradient is also an expectation. As a result, importance sampling can be applied in both valuebased and policy-based algorithms. In fact, it has been applied in the value-based component of Algorithm 10.3.

 Q: Why is the deterministic policy gradient method off-policy?

A: The true gradient in the deterministic case does not involve the action random variable. As a result, when we use samples to approximate the true gradient, it is not required to sample actions and hence any policy can be used. Therefore, the deterministic policy gradient method is off-policy.

# Appendix A

# Preliminaries for Probability Theory

Reinforcement learning heavily relies on probability theory. We next summarize some concepts and results frequently used in this book.

Random variable: The term “variable” indicates that a random variable can take values from a set of numbers. The term “random” indicates that taking a value must follow a probability distribution.

A random variable is usually denoted by a capital letter. Its value is usually denoted by a lowercase letter. For example, $X$ is a random variable, and $x$ is a value that $X$ can take.

This book mainly considers the case where a random variable can only take a finite number of values. A random variable can be a scalar or a vector.

Like normal variables, random variables have normal mathematical operations such as summation, product, and absolute value. For example, if $X , Y$ are two random variables, we can calculate $X + Y$ , $X + 1$ , and $X Y$ .

A stochastic sequence is a sequence of random variables.

One scenario we often encounter is collecting a stochastic sampling sequence $\{ x _ { i } \} _ { i = 1 } ^ { n }$ of a random variable $X$ . For example, consider the task of tossing a die $n$ times. Let $x _ { i }$ be a random variable representing the value obtained for the $i$ th toss. Then, $\{ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \}$ is a stochastic process.

It may be confusing to beginners why $x _ { i }$ is a random variable instead of a deterministic value. In fact, if the sampling sequence is {1,6,3,5,...}, then this sequence is not a stochastic sequence because all the elements are already determined. However, if we use a variable $x _ { i }$ to represent the values that can possibly be sampled, it is a random variable since $x _ { i }$ can take any value in $\{ 1 , \ldots , 6 \}$ . Although $x _ { i }$ is a lowercase letter, it still represents a random variable.

 Probability: The notation $p ( X = x )$ or $p _ { X } ( x )$ describes the probability of the random variable $X$ taking the value $x$ . When the context is clear, $p ( X = x )$ is often written as $p ( x )$ for short.

 Joint probability: The notation $p ( X = x , Y = y )$ or $p ( x , y )$ describes the probability of the random variable $X$ taking the value $x$ and $Y$ taking the value $y$ . One useful identity is as follows:

$$
\sum_ {y} p (x, y) = p (x).
$$

 Conditional probability: The notation $p ( X = x | A = a )$ describes the probability of the random variable $X$ taking the value $x$ given that the random variable $A$ has already taken the value $a$ . We often write $p ( X = x | A = a )$ as $p ( x | a )$ for short.

It holds that

$$
p (x, a) = p (x | a) p (a)
$$

and

$$
p (x | a) = \frac {p (x , a)}{p (a)}.
$$

Since $\begin{array} { r } { p ( x ) = \sum _ { a } p ( x , a ) } \end{array}$ , we have

$$
p (x) = \sum_ {a} p (x, a) = \sum_ {a} p (x | a) p (a),
$$

which is called the law of total probability.

 Independence: Two random variables are independent if the sampling value of one random variable does not affect the other. Mathematically, $X$ and $Y$ are independent if

$$
p (x, y) = p (x) p (y).
$$

Since $p ( x , y ) = p ( x | y ) p ( y )$ , the above equation implies

$$
p (x | y) = p (x).
$$

 Conditional independence: Let $X , A , B$ be three random variables. $X$ is said to be conditionally independent of $A$ given $B$ if

$$
p (X = x \mid A = a, B = b) = p (X = x \mid B = b).
$$

In the context of reinforcement learning, consider three consecutive states: $s _ { t } , s _ { t + 1 } , s _ { t + 2 }$ . Since they are obtained consecutively, $s _ { t + 2 }$ is dependent on $s _ { t + 1 }$ and also $s _ { t }$ . However, if $s _ { t + 1 }$ is already given, then $s _ { t + 2 }$ is conditionally independent of $s _ { t }$ . That is

$$
p \left(s _ {t + 2} \mid s _ {t + 1}, s _ {t}\right) = p \left(s _ {t + 2} \mid s _ {t + 1}\right).
$$

This is also the memoryless property of Markov processes.

 Law of total probability: The law of total probability was already mentioned when we

introduced the concept of conditional probability. Due to its importance, we list it again below:

$$
p (x) = \sum_ {y} p (x, y)
$$

and

$$
p (x | a) = \sum_ {y} p (x, y | a).
$$

Chain rule of conditional probability and joint probability. By the definition of conditional probability, we have

$$
p (a, b) = p (a | b) p (b).
$$

This can be extended to

$$
p (a, b, c) = p (a | b, c) p (b, c) = p (a | b, c) p (b | c) p (c),
$$

and hence $p ( a , b , c ) / p ( c ) \ = \ p ( a , b | c ) \ = \ p ( a | b , c ) p ( b | c )$ . The fact that $p ( a , b | c ) ~ =$ $p ( a | b , c ) p ( b | c )$ implies the following property:

$$
p (x | a) = \sum_ {b} p (x, b | a) = \sum_ {b} p (x | b, a) p (b | a).
$$

Expectation/expected value/mean: Suppose that $X$ is a random variable and the probability of taking the value $x$ is $p ( x )$ . The expectation, expected value, or mean of $X$ is defined as

$$
\mathbb {E} [ X ] = \sum_ {x} p (x) x.
$$

The linearity property of expectation is

$$
\mathbb {E} [ X + Y ] = \mathbb {E} [ X ] + \mathbb {E} [ Y ],
$$

$$
\mathbb {E} [ a X ] = a \mathbb {E} [ X ].
$$

The second equation above can be trivially proven by definition. The first equation is proven below:

$$
\begin{array}{l} \mathbb {E} [ X + Y ] = \sum_ {x} \sum_ {y} (x + y) p (X = x, Y = y) \\ = \sum_ {x} x \sum_ {y} p (x, y) + \sum_ {y} y \sum_ {x} p (x, y) \\ = \sum_ {x} x p (x) + \sum_ {y} y p (y) \\ = \mathbb {E} [ X ] + \mathbb {E} [ Y ]. \\ \end{array}
$$

Due to the linearity of expectation, we have the following useful fact:

$$
\mathbb {E} \left[ \sum_ {i} a _ {i} X _ {i} \right] = \sum_ {i} a _ {i} \mathbb {E} [ X _ {i} ].
$$

Similarly, it can be proven that

$$
\mathbb {E} [ A X ] = A \mathbb {E} [ X ],
$$

where $A \in \mathbb { R } ^ { n \times n }$ is a deterministic matrix and $X \in \mathbb { R } ^ { n }$ is a random vector.

 Conditional expectation: The definition of conditional expectation is

$$
\mathbb {E} [ X | A = a ] = \sum_ {x} x p (x | a).
$$

Similar to the law of total probability, we have the law of total expectation:

$$
\mathbb {E} [ X ] = \sum_ {a} \mathbb {E} [ X | A = a ] p (a).
$$

The proof is as follows. By the definition of expectation, it holds that

$$
\begin{array}{l} \sum_ {a} \mathbb {E} [ X | A = a ] p (a) = \sum_ {a} \left[ \sum_ {x} p (x | a) x \right] p (a) \\ = \sum_ {x} \sum_ {a} p (x | a) p (a) x \\ = \sum_ {x} \left[ \sum_ {a} p (x | a) p (a) \right] x \\ = \sum_ {x} p (x) x \\ = \mathbb {E} [ X ]. \\ \end{array}
$$

The law of total expectation is frequently used in reinforcement learning.

Similarly, conditional expectation satisfies

$$
\mathbb {E} [ X | A = a ] = \sum_ {b} \mathbb {E} [ X | A = a, B = b ] p (b | a).
$$

This equation is useful in the derivation of the Bellman equation. A hint of its proof is the chain rule: $p ( x | a , b ) p ( b | a ) = p ( x , b | a )$ .

Finally, it is worth noting that $\mathbb { E } | X | A = a ]$ is different from $\mathbb { E } | X | A |$ . The former is a value, whereas the latter is a random variable. In fact, $\mathbb { E } [ X | A ]$ is a function of the random variable $A$ . We need rigorous probability theory to define $\mathbb { E } [ X | A ]$ .

 Gradient of expectation: Let $f ( X , \beta )$ be a scalar function of a random variable $X$ and a deterministic parameter vector $\beta$ . Then,

$$
\nabla_ {\beta} \mathbb {E} [ f (X, \beta) ] = \mathbb {E} [ \nabla_ {\beta} f (X, \beta) ].
$$

Proof: Since $\begin{array} { r } { \mathbb { E } [ f ( X , \beta ) ] = \sum _ { x } f ( x , a ) p ( x ) } \end{array}$ , we have $\begin{array} { r } { \nabla _ { \beta } \mathbb { E } [ f ( X , \beta ) ] = \nabla _ { \beta } \sum _ { x } f ( x , a ) p ( x ) = } \end{array}$ $\begin{array} { r } { \sum _ { x } \nabla _ { \beta } f ( x , a ) p ( x ) = \mathbb { E } [ \nabla _ { \beta } f ( X , \beta ) ] } \end{array}$ .

Variance, covariance, covariance matrix : For a single random variable $X$ , its variance is defined as $\operatorname { v a r } ( X ) = \mathbb { E } [ ( X - { \bar { X } } ) ^ { 2 } ]$ , where ${ \bar { X } } = \mathbb { E } [ X ]$ . For two random variables $X , Y$ , their covariance is defined as $\operatorname { c o v } ( X , Y ) = \mathbb { E } [ ( X - { \bar { X } } ) ( Y - { \bar { Y } } ) ]$ . For a random vector $X = [ X _ { 1 } , \ldots , X _ { n } ] ^ { \scriptscriptstyle T }$ , the covariance matrix of $X$ is defined as $\operatorname { v a r } ( X ) \doteq \Sigma = \mathbb { E } [ ( X -$ $\bar { X } ) ( X - \bar { X } ) ^ { T } ] \in \mathbb { R } ^ { n \times n }$ . The $i j$ th entry of $\Sigma$ is $[ \Sigma ] _ { i j } = \mathbb { E } [ [ X - X ] _ { i } [ X - X ] _ { j } ] = \mathbb { E } [ ( X _ { i } -$ $\bar { X } _ { i } ) ( X _ { j } - \bar { X } _ { j } ) ] = \operatorname { c o v } ( X _ { i } , X _ { j } )$ . One trivial property is $\operatorname { v a r } ( a ) = 0$ if $a$ is deterministic. Moreover, it can be verified that $\operatorname { v a r } ( A X + a ) = \operatorname { v a r } ( A X ) = A \operatorname { v a r } ( X ) A ^ { T } = A \Sigma A ^ { T }$ .

Some useful facts are summarized below.

- Fact: $\operatorname { \mathbb { E } } [ ( X - { \bar { X } } ) ( Y - { \bar { Y } } ) ] = \operatorname { \mathbb { E } } [ X Y ] - { \bar { X } } { \bar { Y } } = \operatorname { \mathbb { E } } [ X Y ] - \operatorname { \mathbb { E } } [ X ] \operatorname { \mathbb { E } } [ Y ] .$ .

Proof: $\operatorname { \mathbb { E } } [ ( X - { \bar { X } } ) ( Y - { \bar { Y } } ) ] = \operatorname { \mathbb { E } } [ X Y - X { \bar { Y } } - { \bar { X } } Y + { \bar { X } } { \bar { Y } } ] = \operatorname { \mathbb { E } } [ X Y ]$ ] − E[X ]Y¯ −

$X \mathbb { E } [ Y ] + X Y = \mathbb { E } [ X Y ] - \mathbb { E } [ X ] \mathbb { E } [ Y ] - \mathbb { E } [ X ] \mathbb { E } [ Y ] + \mathbb { E } [ X ] \mathbb { E } [ Y ] = \mathbb { E } [ X Y ] - \mathbb { H } [ X Y ] .$ E[X ]E[Y ].

- Fact: $\mathbb { E } [ X Y ] = \mathbb { E } [ X ] \mathbb { E } [ Y ]$ if $X , Y$ are independent.

Proof: $\begin{array} { r } { \mathbb { E } [ X Y ] = \sum _ { x } \sum _ { y } p ( x , y ) x y = \sum _ { x } \sum _ { y } p ( x ) p ( y ) x y = \sum _ { x } p ( x ) x \sum _ { y } p ( y ) y = } \end{array}$ $\mathbb { E } [ X ] \mathbb { E } [ Y ]$ .

- Fact: $\operatorname { c o v } ( X , Y ) = 0$ if $X , Y$ are independent.

Proof: When $X , Y$ are independent, $\operatorname { c o v } ( X , Y ) = \mathbb { E } [ X Y ] - \mathbb { E } [ X ] \mathbb { E } [ Y ] = \mathbb { E } [ X ] \mathbb { E } [ Y ] -$ $\mathbb { E } [ X ] \mathbb { E } [ Y ] = 0$ .

# Appendix B

# Measure-Theoretic Probability Theory

We now briefly introduce measure-theoretic probability theory, which is also called rigorous probability theory. We only present basic notions and results. Comprehensive introductions can be found in [96–98]. Moreover, measure-theoretic probability theory requires some basic knowledge of measure theory, which is not covered here. Interested readers may refer to [99].

The reader may wonder if it is necessary to understand measure-theoretic probability theory before studying reinforcement learning. The answer is yes if the reader is interested in rigorously analyzing the convergence of stochastic sequences. For example, we often encounter the notion of almost sure convergence in Chapter 6 and Chapter 7. This notion is taken from measure-theoretic probability theory. If the reader is not interested in the convergence of stochastic sequences, it is okay to skip this part.

# Probability triples

A probability triple is fundamental for establishing measure-theoretic probability theory. It is also called a probability space or probability measure space. A probability triple consists of three ingredients.

$\diamond \mathrm { ~ \Large ~ \mathfrak ~ { ~ \mathfrak ~ { ~ Q ~ } ~ } ~ }$ : This is a set called the sample space (or outcome space). Any element (or point) in $\Omega$ , denoted as $\omega$ , is called an outcome. This set contains all the possible outcomes of a random sampling process.

Example: When playing a game of dice, we have six possible outcomes $\{ 1 , 2 , 3 , 4 , 5 , 6 \}$ . Hence, $\Omega = \{ 1 , 2 , 3 , 4 , 5 , 6 \}$ .

$\mathcal { F }$ : This is a set called the event space. In particular, it is a $\sigma$ -algebra (or $\sigma$ -field) of $\Omega$ . The definition of a $\sigma$ -algebra is given in Box B.1. An element in $\mathcal { F }$ , denoted as $A$ , is called an event. An elementary event refers to a single outcome in the sample space. An event may be an elementary event or a combination of multiple elementary events.

Example: Consider the game of dice. An example of an elementary event is “the number you get is $i ^ { \ ' }$ , where $i \in \{ 1 , \ldots , 6 \}$ . An example of a nonelementary event is “the number you get is greater than $3 ^ { \circ }$ . We care about such an event in practice because, for example, we can win the game if this event occurs. This event is mathematically expressed as $A = \{ \omega \in \Omega : \omega > 3 \}$ . Since $\Omega = \{ 1 , 2 , 3 , 4 , 5 , 6 \}$ in this case, we have $A = \{ 4 , 5 , 6 \}$ .

$\mathbb { P }$ : This is a probability measure, which is a mapping from $\mathcal { F }$ to $\lfloor 0 , 1 \rfloor$ . Any $A \in { \mathcal { F } }$ is a set that contains some points in $\Omega$ . Then, $\mathbb { P } ( A )$ is the measure of this set.

Example: If $A = \Omega$ , which contains all $\omega$ values, then $\mathbb { P } ( A ) = 1$ ; if $A = \emptyset$ , then $\mathbb { P } ( A ) = 0$ . In the game of dice, consider the event “the number you get is greater than 3”. In this case, $A = \{ \omega \in \Omega : \omega > 3 \}$ , and $\Omega = \{ 1 , 2 , 3 , 4 , 5 , 6 \}$ . Then, we have $A = \{ 4 , 5 , 6 \}$ and hence $\mathbb { P } ( A ) = 1 / 2$ . That is, the probability of us rolling a number greater than 3 is $1 / 2$ .

# Box B.1: Definition of a $\sigma$ -algebra

An algebra of $\Omega$ is a set of some subsets of $\Omega$ that satisfy certain conditions. A $\sigma$ -algebra is a specific and important type of algebra. In particular, denote $\mathcal { F }$ as a $\sigma$ -algebra. Then, it must satisfy the following conditions.

$\diamond$ $\mathcal { F }$ contains $\emptyset$ and $\Omega$ ;   
 $\mathcal { F }$ is closed under complements;   
$\diamond$ $\mathcal { F }$ is closed under countable unions and intersections.

The $\sigma$ -algebras of a given $\Omega$ are not unique. $\mathcal { F }$ may contain all the subsets of $\Omega$ , and it may also merely contain some of them as long as it satisfies the above three conditions (see the examples below). Moreover, the three conditions are not independent. For example, if $\mathcal { F }$ contains $\Omega$ and is closed under complements, then it naturally contains $\emptyset$ . More information can be found in [96–98].

Example: When playing the dice game, we have $\Omega \ = \ \{ 1 , 2 , 3 , 4 , 5 , 6 \}$ . Then, $\mathcal { F } = \{ \Omega , \emptyset , \{ 1 , 2 , 3 \} , \{ 4 , 5 , 6 \} \}$ is a $\sigma$ -algebra. The above three conditions can be easily verified. There are also other $\sigma$ -algebras such as $\{ \Omega , \emptyset , \{ 1 , 2 , 3 , 4 , 5 \} , \{ 6 \} \}$ . Moreover, for any $\Omega$ with finite elements, the collection of all the subsets of $\Omega$ is a $\sigma$ -algebra.

# Random variables

Based on the notion of probability triples, we can formally define random variables. They are called variables, but they are actually functions that map from $\Omega$ to $\mathbb { R }$ . In particular,

a random variable assigns each outcome in $\Omega$ a numerical value, and hence it is a function: $X ( \omega ) : \Omega  \mathbb { R }$ .

Not all mappings from $\Omega$ to $\mathbb { R }$ are random variables. The formal definition of a random variable is as follows. A function $X : \Omega  \mathbb { R }$ is a random variable if

$$
A = \{\omega \in \Omega | X (\omega) \leq x \} \in \mathcal {F}
$$

for all $x \in \mathbb { R }$ . This definition indicates that $X$ is a random variable only if $X ( \omega ) \leq x$ i s an event in $\mathcal { F }$ . More information can be found in [96, Section 3.1].

# Expectation of random variables

The definition of the expectation of general random variables is sophisticated. Here, we only consider the special yet important case of simple random variables. In particular, a random variable is simple if $X ( \omega )$ only takes a finite number of values. Let $\mathcal { X }$ be the set of all the possible values that $X$ can take. A simple random variable is a function: $X ( w ) : \Omega  \mathcal { X }$ . It can be defined in a closed form as

$$
X (\omega) \doteq \sum_ {x \in \mathcal {X}} x \mathbb {1} _ {A _ {x}} (\omega),
$$

where

$$
A _ {x} = \{\omega \in \Omega | X (\omega) = x \} \doteq X ^ {- 1} (x)
$$

and

$$
\mathbb {1} _ {A _ {x}} (\omega) \doteq \left\{ \begin{array}{l l} 1, & \omega \in A _ {x}, \\ 0, & \text {o t h e r w i s e .} \end{array} \right. \tag {B.1}
$$

Here, $\mathbb { 1 } _ { A _ { x } } ( \omega )$ is an indicator function $\mathbb { 1 } _ { A _ { x } } ( \omega ) : \Omega  \{ 0 , 1 \}$ . If $\omega$ is mapped to $x$ , the indicator function equals one; otherwise, it equals zero. It is possible that multiple $\omega$ ’s in $\Omega$ map to the same value in $\mathcal { X }$ , but a single $\omega$ cannot be mapped to multiple values in $\mathcal { X }$ .

With the above preparation, the expectation of a simple random variable is defined as

$$
\mathbb {E} [ X ] \doteq \sum_ {x \in \mathcal {X}} x \mathbb {P} \left(A _ {x}\right), \tag {B.2}
$$

where

$$
A _ {x} = \{\omega \in \Omega | X (\omega) = x \}.
$$

The definition in (B.2) is similar to but more formal than the definition of expectation in the nonmeasure-theoretic case: $\begin{array} { r } { \mathbb { E } [ X ] = \sum _ { x \in \mathcal { X } } x p ( x ) } \end{array}$ .

As a demonstrative example, we next calculate the expectation of the indicator func-

tion in (B.1). It is notable that the indicator function is also a random variable that maps $\Omega$ to $\{ 0 , 1 \}$ [96, Proposition 3.1.5]. As a result, we can calculate its expectation. In particular, consider the indicator function $\mathbb { 1 } _ { A }$ where $A$ denotes any event. We have

$$
\mathbb {E} [ \mathbb {1} _ {A} ] = \mathbb {P} (A).
$$

To prove that, we have

$$
\begin{array}{l} \mathbb {E} [ \mathbb {1} _ {A} ] = \sum_ {z \in \{0, 1 \}} z \mathbb {P} (\mathbb {1} _ {A} = z) \\ = 0 \cdot \mathbb {P} \left(\mathbb {1} _ {A} = 0\right) + 1 \cdot \mathbb {P} \left(\mathbb {1} _ {A} = 1\right) \\ = \mathbb {P} \left(\mathbb {1} _ {A} = 1\right) \\ = \mathbb {P} (A). \\ \end{array}
$$

More properties of indicator functions can be found in [100, Chapter 24].

# Conditional expectation as a random variable

While the expectation in (B.2) maps random variables to a specific value, we next introduce a conditional expectation that maps random variables to another random variable.

Suppose that $X , Y , Z$ are all random variables. Consider three cases. First, a conditional expectation like $\mathbb { E } [ X | Y = 2 ]$ or $\mathbb { E } [ X | Y = 5 ]$ is specific number. Second, $\mathbb { E } [ X | Y = y ]$ , where $y$ is a variable, is a function of $y$ . Third, $\mathbb { E } | X | Y |$ , where $Y$ is a random variable, is a function of $Y$ and hence also a random variable. Since $\mathbb { E } | X | Y |$ is also a random variable, we can calculate, for example, its expectation.

We next examine the third case closely since it frequently emerges in the convergence analyses of stochastic sequences. The rigorous definition is not covered here and can be found in [96, Chapter 13]. We merely present some useful properties [101].

Lemma B.1 (Basic properties). Let $X , Y , Z$ be random variables. The following properties hold.

(a) $\mathbb { E } [ a | Y ] = a$ , where a is a given number.   
(b) ${ \mathbb E } [ a X + b Z | Y ] = a { \mathbb E } [ X | Y ] + b { \mathbb E } [ Z | Y ] .$   
(c) $\mathbb { E } [ X | Y ] = \mathbb { E } [ X ]$ if $X , Y$ are independent.   
(d) $\mathbb { E } [ X f ( Y ) | Y ] = f ( Y ) \mathbb { E } [ X | Y ] .$   
(e) $\mathbb { E } [ f ( Y ) | Y ] = f ( Y )$ .   
(f ) $\mathbb { E } [ X | Y , f ( Y ) ] = \mathbb { E } [ X | Y ] .$ .   
(g) If $X \geq 0$ , then $\mathbb { E } [ X | Y ] \ge 0$ .   
$( h )$ If $X \geq Z$ , then $\mathbb { E } [ X | Y ] \ge \mathbb { E } [ Z | Y ]$ .

Proof. We only prove some properties. The others can be proven similarly.

To prove $\mathbb { E } [ a | Y ] = a$ as in (a), we can show that $\mathbb { E } [ a | Y = y ] = a$ is valid for any $y$ that $Y$ can possibly take. This is clearly true, and the proof is complete.

To prove the property in (d), we can show that ${ \mathbb E } [ X f ( Y ) | Y = y ] = f ( Y = y ) { \mathbb E } [ X | Y =$ $y ]$ for any $y$ . This is valid because $\begin{array} { r } { \mathbb { E } [ X f ( Y ) | Y = y ] = \sum _ { x } x f ( y ) p ( x | y ) = f ( y ) \sum _ { x } x p ( x | y ) = } \end{array}$ $f ( y ) \mathbb { E } [ X | Y = y ]$ . □

Since $\mathbb { E } [ X | Y ]$ is a random variable, we can calculate its expectation. The related properties are presented below. These properties are useful for analyzing the convergence of stochastic sequences.

Lemma B.2. Let $X , Y , Z$ be random variables. The following properties hold.

(a) $\mathbb { E } \lfloor \mathbb { E } [ X | Y ] \rfloor = \mathbb { E } [ X ]$ .   
(b) $\mathbb { E } \big [ \mathbb { E } [ X | Y , Z ] \big ] = \mathbb { E } [ X ]$ .   
(c) $\mathbb { E } \big [ \mathbb { E } [ X | Y ] | Y \big ] = \mathbb { E } [ X | Y ] .$

Proof. To prove the property in (a), we need to show that $\mathbb { E } \big [ \mathbb { E } [ X | Y = y ] \big ] = \mathbb { E } [ X ]$ for any $y$ that $Y$ can possibly take. To that end, considering that $\mathbb { E } [ X | Y ]$ is a function of $Y$ , we denote it as $f ( Y ) = \mathbb { E } [ X | Y ]$ . Then,

$$
\begin{array}{l} \mathbb {E} \bigl [ \mathbb {E} [ X | Y ] \bigr ] = \mathbb {E} \bigl [ f (Y) \bigr ] = \sum_ {y} f (Y = y) p (y) \\ = \sum_ {y} \mathbb {E} [ X | Y = y ] p (y) \\ = \sum_ {y} \left(\sum_ {x} x p (x | y)\right) p (y) \\ = \sum_ {x} x \sum_ {y} p (x | y) p (y) \\ = \sum_ {x} x \sum_ {y} p (x, y) \\ = \sum_ {x} x p (x) \\ = \mathbb {E} [ X ]. \\ \end{array}
$$

The proof of the property in (b) is similar. In particular, we have

$$
\mathbb {E} \left[ \mathbb {E} [ X | Y, Z ] \right] = \sum_ {y, z} \mathbb {E} [ X | y, z ] p (y, z) = \sum_ {y, z} \sum_ {x} x p (x | y, z) p (y, z) = \sum_ {x} x p (x) = \mathbb {E} [ X ].
$$

The proof of the property in (c) follows immediately from property (e) in Lemma B.1. That is because $\mathbb { E } | X | Y |$ is a function of $Y$ . We denote this function as $f ( Y )$ . It then follows that $\operatorname { \mathbb { E } } [ \operatorname { \mathbb { E } } [ X | Y ] | Y ] = \operatorname { \mathbb { E } } [ f ( Y ) | Y ] = f ( Y ) = \operatorname { \mathbb { E } } [ X | Y ]$ . □

# Definitions of stochastic convergence

One main reason why we care about measure-theoretic probability theory is that it can rigorously describe the convergence properties of stochastic sequences.

Consider the stochastic sequence $\{ X _ { k } \} \doteq \{ X _ { 1 } , X _ { 2 } , \ldots , X _ { k } , \ldots \}$ . Each element in this sequence is a random variable defined on a triple $( \Omega , \mathcal { F } , \mathbb { P } )$ . When we say $\{ X _ { k } \}$ converges to a random variable $X$ , we should be careful since there are different types of convergence as shown below.

Sure convergence:

Definition: $\{ X _ { k } \}$ converges surely (or everywhere or pointwise) to $X$ if

$$
\lim _ {k \to \infty} X _ {k} (\omega) = X (\omega), \quad \mathrm {f o r a l l} \omega \in \Omega .
$$

It means that $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } X _ { k } ( \omega ) = X ( \omega ) } \end{array}$ is valid for all points in $\Omega$ . This definition can be equivalently stated as

$$
A = \Omega \quad \text {w h e r e} \quad A = \left\{\omega \in \Omega : \lim  _ {k \to \infty} X _ {k} (\omega) = X (\omega) \right\}.
$$

 Almost sure convergence:

Definition: $\{ X _ { k } \}$ converges almost surely (or almost everywhere or with probability 1 or w.p.1 ) to $X$ if

$$
\mathbb {P} (A) = 1 \quad \text {w h e r e} \quad A = \left\{\omega \in \Omega : \lim  _ {k \rightarrow \infty} X _ {k} (\omega) = X (\omega) \right\}. \tag {B.3}
$$

It means that $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } X _ { k } ( \omega ) = X ( \omega ) } \end{array}$ is valid for almost all points in $\Omega$ . The points, for which this limit is invalid, form a set of zero measure. For the sake of simplicity, (B.3) is often written as

$$
\mathbb {P} \left(\lim  _ {k \to \infty} X _ {k} = X\right) = 1.
$$

Almost sure convergence can be denoted as $X _ { k } \xrightarrow { a . s . } X$

Convergence in probability:

Definition: $\{ X _ { k } \}$ converges in probability to $X$ if for any $\epsilon > 0$ ,

$$
\lim  _ {k \rightarrow \infty} \mathbb {P} (A _ {k}) = 0 \quad \text {w h e r e} \quad A _ {k} = \left\{\omega \in \Omega : | X _ {k} (\omega) - X (\omega) | > \epsilon \right\}. \tag {B.4}
$$

For simplicity, (B.4) can be written as

$$
\lim _ {k \to \infty} \mathbb {P} (| X _ {k} - X | > \epsilon) = 0.
$$

The difference between convergence in probability and (almost) sure convergence is as follows. Both sure convergence and almost sure convergence first evaluate the convergence of every point in $\Omega$ and then check the measure of these points that converge. By contrast, convergence in probability first checks the points that satisfy $| X _ { k } - X | > \epsilon$ and then evaluates if the measure will converge to zero as $k  \infty$ .

Convergence in mean:

Definition: $\{ X _ { k } \}$ converges in the $r$ -th mean (or in the $L ^ { r }$ norm) to $X$ if

$$
\lim _ {k \to \infty} \mathbb {E} [ | X _ {k} - X | ^ {r} ] = 0.
$$

The most frequently used cases are $r = 1$ and $r \ = \ 2$ . It is worth mentioning that convergence in mean is not equivalent to $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \mathbb { E } | X _ { k } - X | = 0 } \end{array}$ or $\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \mathbb { E } [ X _ { k } ] = } \end{array}$ $\mathbb { E } { \lvert X \rvert }$ , which indicates that $\mathbb { E } [ X _ { k } ]$ converges but the variance may not.

Convergence in distribution:

Definition: The cumulative distribution function of $X _ { k }$ is defined as $\mathbb { P } ( X _ { k } \le a )$ where $a \in \mathbb { R }$ . Then, $\{ X _ { k } \}$ converges to $X$ in distribution if the cumulative distribution function converges:

$$
\lim _ {k \to \infty} \mathbb {P} (X _ {k} \leq a) = \mathbb {P} (X \leq a), \quad \mathrm {f o r a l l} a \in \mathbb {R}.
$$

A compact expression is

$$
\lim _ {k \to \infty} \mathbb {P} (A _ {k}) = \mathbb {P} (A),
$$

where

$$
A _ {k} \doteq \left\{\omega \in \Omega : X _ {k} (\omega) \leq a \right\}, A \doteq \left\{\omega \in \Omega : X (\omega) \leq a \right\}.
$$

The relationships between the above types of convergence are given below:

almost sure convergence $\Rightarrow$ convergence in probability $\Rightarrow$ convergence in distribution convergence in mean $\Rightarrow$ convergence in probability $\Rightarrow$ convergence in distribution

Almost sure convergence and convergence in mean do not imply each other. More information can be found in [102].

# Appendix C

# Convergence of Sequences

We next introduce some results about the convergence of deterministic and stochastic sequences. These results are useful for analyzing the convergence of reinforcement learning algorithms such as those in Chapters 6 and 7.

We first consider deterministic sequences and then stochastic sequences.

# C.1 Convergence of deterministic sequences

# Convergence of monotonic sequences

Consider a sequence $\{ x _ { k } \} \doteq \{ x _ { 1 } , x _ { 2 } , \ldots , x _ { k } , \ldots \}$ where $x _ { k } \in \mathbb { R }$ . Suppose that this sequence is deterministic in the sense that $x _ { k }$ is not a random variable.

One of the most well-known convergence results is that a nonincreasing sequence with a lower bound converges. The following is a formal statement of this result.

Theorem C.1 (Convergence of monotonic sequences). If the sequence $\{ x _ { k } \}$ is nonincreasing and bounded from below:

> Nonincreasing: $x _ { k + 1 } \leq x _ { k }$ for all $k$ ;   
$\diamond$ Lower bound: $x _ { k } \geq \alpha$ for all $k$ ;

then $x _ { k }$ converges to a limit, which is the infimum of $\{ x _ { k } \}$ , as $k  \infty$ .

Similarly, if $\{ x _ { k } \}$ is nondecreasing and bounded from above, then the sequence is convergent.

# Convergence of nonmonotonic sequences

We next analyze the convergence of nonmonotonic sequences.

To analyze the convergence of nonmonotonic sequences, we introduce the following useful operator [103]. For any $z \in \mathbb { R }$ , define

$$
z ^ {+} \doteq \left\{ \begin{array}{l l} z, & \text {i f} z \geq 0, \\ 0, & \text {i f} z <   0, \end{array} \right.
$$

$$
z ^ {-} \doteq \left\{ \begin{array}{l l} z, & \text {i f} z \leq 0, \\ 0, & \text {i f} z > 0. \end{array} \right.
$$

It is obvious that $z ^ { + } \geq 0$ and $z ^ { - } \leq 0$ for any $z$ . Moreover, it holds that

$$
z = z ^ {+} + z ^ {-}
$$

for all $z \in \mathbb { R }$ .

To analyze the convergence of $\{ x _ { k } \}$ , we rewrite $x _ { k }$ as

$$
\begin{array}{l} x _ {k} = x _ {k} - x _ {k - 1} + x _ {k - 1} - x _ {k - 2} + \dots - x _ {2} + x _ {2} - x _ {1} + x _ {1} \\ = \sum_ {i = 1} ^ {k - 1} (x _ {i + 1} - x _ {i}) + x _ {1} \\ \stackrel {\cdot} {=} S _ {k} + x _ {1}, \tag {C.1} \\ \end{array}
$$

where $\begin{array} { r } { S _ { k } \doteq \sum _ { i = 1 } ^ { k - 1 } ( x _ { i + 1 } - x _ { i } ) } \end{array}$ . Note that $S _ { k }$ can be decomposed as

$$
S _ {k} = \sum_ {i = 1} ^ {k - 1} \left(x _ {i + 1} - x _ {i}\right) = S _ {k} ^ {+} + S _ {k} ^ {-},
$$

where

$$
S _ {k} ^ {+} = \sum_ {i = 1} ^ {k - 1} (x _ {i + 1} - x _ {i}) ^ {+} \geq 0, \qquad S _ {k} ^ {-} = \sum_ {i = 1} ^ {k - 1} (x _ {i + 1} - x _ {i}) ^ {-} \leq 0.
$$

Some useful properties of $S _ { k } ^ { + }$ and $S _ { k } ^ { - }$ are given below.

 $\{ S _ { k } ^ { + } \geq 0 \}$ is a nondecreasing sequence since $S _ { k + 1 } ^ { + } \geq S _ { k } ^ { + }$ for all $k$   
 $\{ S _ { k } ^ { - } \le 0 \}$ is a nonincreasing sequence since $S _ { k + 1 } ^ { - } \le S _ { k } ^ { - }$ for all $k$   
$\diamond$ If $S _ { k } ^ { + }$ is bounded from above, then $S _ { k } ^ { - }$ is bounded from below. This is because $S _ { k } ^ { - } \ge - S _ { k } ^ { + } - x _ { 1 }$ due to the fact that $S _ { k } ^ { - } + S _ { k } ^ { + } + x _ { 1 } = x _ { k } \ge 0$ .

With the above preparation, we can show the following result.

Theorem C.2 (Convergence of nonmonotonic sequences). For any nonnegative sequence

$$
\left\{x _ {k} \geq 0 \right\}, i f
$$

$$
\sum_ {k = 1} ^ {\infty} \left(x _ {k + 1} - x _ {k}\right) ^ {+} <   \infty , \tag {C.2}
$$

then $\{ x _ { k } \}$ converges as $k  \infty$

Proof. First, the condition $\begin{array} { r } { \sum _ { k = 1 } ^ { \infty } ( x _ { k + 1 } - x _ { k } ) ^ { + } < \infty } \end{array}$ indicates that $\begin{array} { r } { S _ { k } ^ { + } = \sum _ { i = 1 } ^ { k - 1 } ( x _ { i + 1 } - } \end{array}$ $x _ { i } ) ^ { + }$ is bounded from above for all $k$ . Since $\{ S _ { k } ^ { + } \}$ is nondecreasing, the convergence of $\{ S _ { k } ^ { + } \}$ immediately follows from Theorem C.1. Suppose that $S _ { k } ^ { + }$ converges to $S _ { * } ^ { + }$ .

Second, the boundedness of $S _ { k } ^ { + }$ implies that $S _ { k } ^ { - }$ is bounded from below since $S _ { k } ^ { - } \ge - S _ { k } ^ { + } - x _ { 1 }$ . Since $\{ S _ { k } ^ { - } \}$ is nonincreasing, the convergence of $\{ S _ { k } ^ { - } \}$ immediately follows from Theorem C.1. Suppose that $S _ { k } ^ { - }$ converges to $S _ { * } ^ { - }$ .

Finally, since $x _ { k } = S _ { k } ^ { + } + S _ { k } ^ { - } + x _ { 1 }$ , as shown in (C.1), the convergence of $S _ { k } ^ { + }$ a n d $S _ { k } ^ { - }$ implies that $\{ x _ { k } \}$ converges to $S _ { * } ^ { + } + S _ { * } ^ { - } + x _ { 1 }$ .

Theorem C.2 is more general than Theorem C.1 because it allows $x _ { k }$ to increase as long as the increase is damped as in (C.2). In the monotonic case, Theorem C.2 still applies. In particular, if $x _ { k + 1 } \leq x _ { k }$ , then $\begin{array} { r } { \sum _ { k = 1 } ^ { \infty } ( x _ { k + 1 } - x _ { k } ) ^ { + } = 0 } \end{array}$ . In this case, (C.2) is still satisfied and the convergence follows.

We next consider a special yet importance case. Suppose that $\{ x _ { k } \geq 0 \}$ is a nonnegative sequence satisfying

$$
x _ {k + 1} \leq x _ {k} + \eta_ {k}.
$$

When $\eta _ { k } = 0$ , we have $x _ { k + 1 } \leq x _ { k }$ , meaning that the sequence is monotonic. When $\eta _ { k } \geq 0$ , the sequence is not monotonic because $x _ { k + 1 }$ may be greater than $x _ { k }$ . Nevertheless, we can still ensure the convergence of the sequence under some mild conditions. The following result is an immediate corollary of Theorem C.2.

Corollary C.1. For any nonnegative sequence $\{ x _ { k } \geq 0 \}$ , if

$$
x _ {k + 1} \leq x _ {k} + \eta_ {k}
$$

and $\{ \eta _ { k } \ge 0 \}$ satisfies

$$
\sum_ {k = 1} ^ {\infty} \eta_ {k} <   \infty ,
$$

then $\{ x _ { k } \ge 0 \}$ converges.

Proof. Since $x _ { k + 1 } \leq x _ { k } + \eta _ { k }$ , we have $( x _ { k + 1 } - x _ { k } ) ^ { + } \leq \eta _ { k }$ for all $k$ . Then, we have

$$
\sum_ {k = 1} ^ {\infty} (x _ {k + 1} - x _ {k}) ^ {+} \leq \sum_ {k = 1} ^ {\infty} \eta_ {k} <   \infty .
$$

As a result, (C.2) is satisfied and the convergence follows from Theorem C.2.

# C.2 Convergence of stochastic sequences

We now consider stochastic sequences. While various definitions of stochastic sequences have been given in Appendix B, how to determine the convergence of a given stochastic sequence has not yet been discussed. We next present an important class of stochastic sequences called martingales. If a sequence can be classified as a martingale (or one of its variants), then the convergence of the sequence immediately follows.

# Convergence of martingale sequences

 Definition: A stochastic sequence $\{ X _ { k } \} _ { k = 1 } ^ { \infty }$ is called a martingale if $\mathbb { E } [ | X _ { k } | ] < \infty$ and

$$
\mathbb {E} \left[ X _ {k + 1} \mid X _ {1}, \dots , X _ {k} \right] = X _ {k} \tag {C.3}
$$

almost surely for all $k$ .

Here, $\mathbb { E } [ X _ { k + 1 } | X _ { 1 } , \dots , X _ { k } ]$ is a random variable rather than a deterministic value. The term “almost surely” in the second condition is due to the definition of such expectations. In addition, $\mathbb { E } [ X _ { k + 1 } | X _ { 1 } , \dots , X _ { k } ]$ is often written as $\mathbb { E } [ X _ { k + 1 } | \mathcal { H } _ { k } ]$ for short where ${ \mathcal { H } } _ { k } = \{ X _ { 1 } , . . . , X _ { k } \}$ represents the “history” of the sequence. $\mathcal { H } _ { k }$ has a specific name called a filtration. More information can be found in [96, Chapter 14] and [104].

 Example: An example that can demonstrate martingales is random walk, which is a stochastic process describing the position of a point that moves randomly. Specifically, let $X _ { k }$ denote the position of the point at time step $k$ . Starting from $X _ { k }$ , the expectation of the next position $X _ { k + 1 }$ equals $X _ { k }$ if the mean of the one-step displacement is zero. In this case, we have $\mathbb { E } [ X _ { k + 1 } | X _ { 1 } , \dots , X _ { k } ] = X _ { k }$ and hence $\{ X _ { k } \}$ is a martingale.

A basic property of martingales is that

$$
\mathbb {E} [ X _ {k + 1} ] = \mathbb {E} [ X _ {k} ]
$$

for all $k$ and hence

$$
\mathbb {E} \left[ X _ {k} \right] = \mathbb {E} \left[ X _ {k - 1} \right] = \dots = \mathbb {E} \left[ X _ {2} \right] = \mathbb {E} \left[ X _ {1} \right].
$$

This result can be obtained by calculating the expectation on both sides of (C.3) based on property (b) in Lemma B.2.

While the expectation of a martingale is constant, we next extend martingales to submartingales and supermartingales, whose expectations vary monotonically.

$\diamond$ Definition: A stochastic sequence $\{ X _ { k } \}$ is called a submartingale if it satisfies $\mathbb { E } [ | X _ { k } | ] <$ $\infty$ and

$$
\mathbb {E} \left[ X _ {k + 1} \mid X _ {1}, \dots , X _ {k} \right] \geq X _ {k} \tag {C.4}
$$

for all $k$ .

Taking the expectation on both sides of (C.4) yields $\mathbb { E } [ X _ { k + 1 } ] \geq \mathbb { E } [ X _ { k } ]$ . In particular, the left-hand side leads to $\mathbb { E } [ \mathbb { E } [ X _ { k + 1 } | X _ { 1 } , \dots , X _ { k } ] ] = \mathbb { E } [ X _ { k + 1 } ]$ due to property (b) in Lemma B.2. By induction, we have

$$
\mathbb {E} [ X _ {k} ] \geq \mathbb {E} [ X _ {k - 1} ] \geq \dots \geq \mathbb {E} [ X _ {2} ] \geq \mathbb {E} [ X _ {1} ].
$$

Therefore, the expectation of a submartingale is nondecreasing.

It may be worth mentioning that, for two random variables $X$ and $Y$ , $X \le Y$ means $X ( \omega ) \leq Y ( \omega )$ for all $\omega \in \Omega$ . It does not mean the maximum of $X$ is less than the minimum of $Y$ .

 Definition: A stochastic sequence $\{ X _ { k } \}$ is called a supermartingale if it satisfies $\mathbb { E } [ | X _ { k } | ] < \infty$ and

$$
\mathbb {E} \left[ X _ {k + 1} \mid X _ {1}, \dots , X _ {k} \right] \leq X _ {k} \tag {C.5}
$$

for all $k$ .

Taking expectation on both sides of (C.5) gives $\mathbb { E } [ X _ { k + 1 } ] \le \mathbb { E } [ X _ { k } ]$ . By induction, we have

$$
\mathbb {E} [ X _ {k} ] \leq \mathbb {E} [ X _ {k - 1} ] \leq \dots \leq \mathbb {E} [ X _ {2} ] \leq \mathbb {E} [ X _ {1} ].
$$

Therefore, the expectation of a supmartingale is nonincreasing.

The names “submartingale” and “supmartingale” are standard, but it may not be easy for beginners to distinguish them. Some tricks can be employed to do so. For example, since “supermartingale” has a letter “p” that points down, its expectation decreases; since submartingale has a letter “b” that points up, its expectation increases [104].

A supermartingale or submartingale is comparable to a deterministic monotonic sequence. While the convergence result for monotonic sequences has been given in Theorem C.1, we provide a similar convergence result for martingales as follows.

Theorem C.3 (Martingale convergence theorem). If $\{ X _ { k } \}$ is a submartingale (or supermartingale), then there is a finite random variable $X$ such that $X _ { k } \to X$ almost surely.

The proof is omitted. A comprehensive introduction to martingales can be found in [96, Chapter 14] and [104].

# Convergence of quasimartingale sequences

We next introduce quasimartingales, which can be viewed as a generalization of martingales since their expectations are not monotonic. They are comparable to nonmonotonic deterministic sequences. The rigorous definition and convergence results of quasimartingales are nontrivial. We merely list some useful results.

The event $A _ { k }$ is defined as $A _ { k } \doteq \{ \omega \in \Omega : \mathbb { E } | X _ { k + 1 } - X _ { k } | \mathcal { H } _ { k } | \ge 0 \}$ , where $\mathcal { H } _ { k } \ =$ $\{ X _ { 1 } , \ldots , X _ { k } \}$ . Intuitively, $A _ { k }$ indicates that $X _ { k + 1 }$ is greater than $X _ { k }$ in expectation. Let $\mathbb { 1 } _ { A _ { k } }$ be an indicator function:

$$
\mathbb {1} _ {A _ {k}} = \left\{ \begin{array}{l l} 1, & \mathbb {E} [ X _ {k + 1} - X _ {k} | \mathcal {H} _ {k} ] \geq 0, \\ 0, & \mathbb {E} [ X _ {k + 1} - X _ {k} | \mathcal {H} _ {k} ] <   0. \end{array} \right.
$$

The indicator function has a property that

$$
1 = \mathbb {1} _ {A} + \mathbb {1} _ {A ^ {c}}
$$

for any event $A$ where $A ^ { c }$ denotes the complementary event of $A$ . As a result, it holds for any random variable that

$$
X = \mathbb {1} _ {A} X + \mathbb {1} _ {A ^ {c}} X.
$$

Although quasimartingales do not have monotonic expectations, their convergence is still ensured under some mild conditions as shown below.

Theorem C.4 (Quasimartingale convergence theorem). For a nonnegative stochastic sequence $\{ X _ { k } \ge 0 \}$ , if

$$
\sum_ {k = 1} ^ {\infty} \mathbb {E} [ (X _ {k + 1} - X _ {k}) \mathbb {1} _ {A _ {k}} ] <   \infty ,
$$

then $\begin{array} { r } { \sum _ { k = 1 } ^ { \infty } \mathbb { E } [ ( X _ { k + 1 } - X _ { k } ) \mathbb { 1 } _ { A _ { k } ^ { c } } ] > - \infty } \end{array}$ and there is a finite random variable such that $X _ { k } \to X$ almost surely as $k  \infty$ .

Theorem C.4 can be viewed as an analogy of Theorem C.2, which is for nonmonotonic deterministic sequences. The proof of this theorem can be found in [105, Proposition 9.5]. Note that $X _ { k }$ here is required to be nonnegative. As a result, the boundedness of $\begin{array} { r l } { ~ } & { { } \sum _ { k = 1 } ^ { \infty } \mathbb { E } [ ( X _ { k + 1 } - X _ { k } ) \mathbb { 1 } _ { A _ { k } } ] } \end{array}$ implies the boundedness of $\begin{array} { r l } { ~ } & { { } \sum _ { k = 1 } ^ { \infty } \mathbb { E } [ ( X _ { k + 1 } - X _ { k } ) \mathbb { 1 } _ { A _ { k } ^ { c } } ] } \end{array}$ .

# Summary and comparison

We finally summarize and compare the results for deterministic and stochastic sequences.

Deterministic sequences:

Monotonic sequences: As shown in Theorem C.1, if a sequence is monotonic and bounded, then it converges.   
Nonmonotonic sequences: As shown in Theorem C.2, given a nonnegative sequence, even if it is nonmonotonic, it can still converge as long as its variation is damped in the sense that $\begin{array} { r } { \sum _ { k = 1 } ^ { \infty } ( x _ { k + 1 } - x _ { k } ) ^ { + } < \infty } \end{array}$ .

 Stochastic sequences:

Supermartingale/submartingale sequences: As shown in Theorem C.3, the expectation of a supermartingale or submartingale is monotonic. If a sequence is a supermartingale or submartingale, then the sequence converges almost surely.   
Quasimartingale sequences: As shown in Theorem C.4, even if a sequence’s expectation is nonmonotonic, it can still converge as long as its variation is damped in the sense that $\begin{array} { r } { \sum _ { k = 1 } ^ { \infty } \mathbb { E } [ ( X _ { k + 1 } - X _ { k } ) \mathbf { 1 } _ { \mathbb { E } [ X _ { k + 1 } - X _ { k } | \mathcal { H } _ { k } ] > 0 } ] < \infty } \end{array}$ .

The above properties are summarized in Table C.1.

Table C.1: Summary of the monotonicity of different variants of martingales.   

<table><tr><td>Variants of martingales</td><td>Monotonicity of E[Xk]</td></tr><tr><td>Martingale</td><td>Constant: E[Xk+1] = E[Xk]</td></tr><tr><td>Submartingale</td><td>Increasing: E[Xk+1] ≥ E[Xk]</td></tr><tr><td>Supermartingale</td><td>Decreasing: E[Xk+1] ≤ E[Xk]</td></tr><tr><td>Quasimartingale</td><td>Non-monotonic</td></tr></table>

# Appendix D

# Preliminaries for Gradient Descent

We next present some preliminaries for the gradient descent method, which is one of the most frequently used optimization methods. The gradient descent method is also the foundation for the stochastic gradient descent method introduced in Chapter 6.

# Convexity

$\diamond$ Definitions:

Convex set: Suppose that $\mathcal { D }$ is a subset of $\mathbb { R } ^ { n }$ . This set is convex if $z \doteq c x + ( 1 -$ $c ) y \in { \mathcal { D } }$ for any $x , y \in { \mathcal { D } }$ and any $c \in \left[ 0 , 1 \right]$ .   
Convex function: Suppose $f : \mathcal { D }  \mathbb { R }$ where $\mathcal { D }$ is convex. Then, the function $f ( x )$ is convex if

$$
f (c x + (1 - x) y) \leq c f (x) + (1 - c) f (y)
$$

for any $x , y \in { \mathcal { D } }$ and $c \in [ 0 , 1 ]$ .

Convex conditions:

First-order condition: Consider a function $f : \mathcal { D }  \mathbb { R }$ where $\mathcal { D }$ is convex. Then, $f$ is convex if [106, 3.1.3]

$$
f (y) - f (x) \geq \nabla f (x) ^ {T} (y - x), \quad \text {f o r a l l} x, y \in \mathcal {D}. \tag {D.1}
$$

When $x$ is a scalar, $\nabla f ( x )$ represents the slope of the tangent line of $f ( x )$ at $x$ . The geometric interpretation of (D.1) is that the point $( y , f ( y ) )$ is always located above the tangent line.

- Second-order condition: Consider a function $f : \mathcal { D }  \mathbb { R }$ where $\mathcal { D }$ is convex. Then, $f$ is convex if

$$
\nabla^ {2} f (x) \succeq 0, \quad \text {f o r a l l} x \in \mathcal {D},
$$

where $\nabla ^ { 2 } f ( x )$ is the Hessian matrix.

 Degree of convexity:

Given a convex function, it is often of interest how strong its convexity is. The Hessian matrix is a useful tool for describing the degree of convexity. If $\nabla ^ { 2 } f ( x )$ is close to rank deficiency at a point, then the function is flat around that point and hence weakly convex. Otherwise, if the minimum singular value of $\nabla ^ { 2 } f ( x )$ is positive and large, the function is curly around that point and hence strongly convex. The degree of convexity influences the step size selection in gradient descent algorithms.

The lower and upper bounds of $\nabla ^ { 2 } f ( x )$ play an important role in characterizing the function convexity.

Lower bound of $\nabla ^ { 2 } f ( x )$ : A function is called strongly convex or strictly convex if $\nabla ^ { 2 } f ( x ) \succeq \ell I _ { n }$ , where $\ell > 0$ for all $x$ .   
Upper bound of $\nabla ^ { 2 } f ( x )$ : If $\nabla ^ { 2 } f ( x )$ is bounded from above so that $\nabla ^ { 2 } f ( x ) \preceq L I _ { n }$ , then the change in the first-order derivative $\nabla f ( x )$ cannot be arbitrarily fast; equivalently, the function cannot be arbitrarily convex at a point.

The upper bound can be implied by a Lipschitz condition of $\nabla f ( x )$ , as shown below.

Lemma D.1. Suppose that $f$ is a convex function. If $\nabla f ( x )$ is Lipschitz continuous with a constant L so that

$$
\| \nabla f (x) - \nabla f (y) \| \leq L \| x - y \|, \quad \text {f o r a l l} x, y,
$$

then $\nabla ^ { 2 } f ( x ) \preceq L I _ { n }$ for all x. Here, $\| \cdot \|$ denotes the Euclidean norm.

# Gradient descent algorithms

Consider the following optimization problem:

$$
\min  _ {x} f (x)
$$

where $x \in \mathcal { D } \subseteq \mathbb { R } ^ { n }$ and $f : \mathcal { D }  \mathbb { R }$ . The gradient descent algorithm is

$$
x _ {k + 1} = x _ {k} - \alpha_ {k} \nabla f (x _ {k}), \quad k = 0, 1, 2, \dots \tag {D.2}
$$

where $\alpha _ { k }$ is a positive coefficient that may be fixed or time-varying. Here, $\alpha _ { k }$ is called the step size or learning rate. Some remarks about (D.2) are given below.

 Direction of change: $\nabla f ( x _ { k } )$ is a vector that points in the direction along which $f ( x _ { k } )$ increases the fastest. Hence, the term $- \alpha _ { k } \nabla f ( x _ { k } )$ changes $x _ { k }$ in the direction along which $f ( x _ { k } )$ decreases the fastest.   
Magnitude of change: The magnitude of the change $- \alpha _ { k } \nabla f ( x _ { k } )$ is jointly determined by the step size $\alpha _ { k }$ and the magnitude of $\nabla f ( x _ { k } )$ .

- Magnitude of $\nabla f ( x _ { k } )$ :

When $x _ { k }$ is close to the optimum $x ^ { * }$ where $\nabla f ( x ^ { * } ) = 0$ , the magnitude $\| \nabla f ( x _ { k } ) \|$ is small. In this case, the update process of $x _ { k }$ is slow, which is reasonable because we do not want to update $x$ too aggressively and miss the optimum.

When $x _ { k }$ is far from the optimum, the magnitude of $\nabla f ( x _ { k } )$ may be large, and hence the update process of $x _ { k }$ is fast. This is also reasonable because we hope that the estimate can approach the optimum as quickly as possible.

- Step size $\alpha _ { k }$

If $\alpha _ { k }$ is small, the magnitude of $- \alpha _ { k } \nabla f ( x _ { k } )$ is small, and hence the convergence process is slow. If $\alpha _ { k }$ is too large, the update process of $x _ { k }$ is aggressive, which leads to either fast convergence or divergence.

How to select $\alpha _ { k }$ ? The selection of $\alpha _ { k }$ should depend on the degree of convexity of $f ( x _ { k } )$ . If the function is curly around the optimum (the degree of convexity is strong), then the step size $\alpha _ { k }$ should be small to guarantee convergence. If the function is flat around the optimum (the degree of convexity is weak), then the step size could be large so that $x _ { k }$ can quickly approach the optimum. The above intuition will be verified in the following convergence analysis.

# Convergence analysis

We next present a proof of the convergence of the gradient descent algorithm in (D.2). That is to show $x _ { k }$ converges to the optimum $x ^ { * }$ where $\nabla f ( x ^ { * } ) = 0$ . First of all, we make some assumptions.

 Assumption 1: $f ( x )$ is strongly convex such that

$$
\nabla^ {2} f (x) \succeq \ell I,
$$

where $\ell > 0$ .

Assumption 2: $\nabla f ( x )$ is Lipschitz continuous with a constant $L$ . This assumption implies the following inequality according to Lemma D.1:

$$
\nabla^ {2} f (x) \preceq L I _ {n}.
$$

The convergence proof is given below.

Proof. For any $x _ { k + 1 }$ and $x _ { k }$ , it follows from [106, Section 9.1.2] that

$$
f (x _ {k + 1}) = f (x _ {k}) + \nabla f (x _ {k}) ^ {T} (x _ {k + 1} - x _ {k}) + \frac {1}{2} (x _ {k + 1} - x _ {k}) ^ {T} \nabla^ {2} f (z _ {k}) (x _ {k + 1} - x _ {k}), \quad (\mathrm {D .} 3)
$$

where $z _ { k }$ is a convex combination of $x _ { k }$ and $x _ { k + 1 }$ . Since it is assumed that $\nabla ^ { 2 } f ( z _ { k } ) \preceq L I _ { n }$ , we have $\| \nabla ^ { 2 } f ( z _ { k } ) \| \leq L$ . (D.3) implies

$$
\begin{array}{l} f (x _ {k + 1}) \leq f (x _ {k}) + \nabla f (x _ {k}) ^ {T} (x _ {k + 1} - x _ {k}) + \frac {1}{2} \| \nabla^ {2} f (z _ {k}) \| \| x _ {k + 1} - x _ {k} \| ^ {2} \\ \leq f (x _ {k}) + \nabla f (x _ {k}) ^ {T} (x _ {k + 1} - x _ {k}) + \frac {L}{2} \| x _ {k + 1} - x _ {k} \| ^ {2}. \\ \end{array}
$$

Substituting $x _ { k + 1 } = x _ { k } - \alpha _ { k } \nabla f ( x _ { k } )$ into the above inequality yields

$$
\begin{array}{l} f (x _ {k + 1}) \leq f (x _ {k}) + \nabla f (x _ {k}) ^ {T} (- \alpha_ {k} \nabla f (x _ {k})) + \frac {L}{2} \| \alpha_ {k} \nabla f (x _ {k}) \| ^ {2} \\ = f (x _ {k}) - \alpha_ {k} \| \nabla f (x _ {k}) \| ^ {2} + \frac {\alpha_ {k} ^ {2} L}{2} \| \nabla f (x _ {k}) \| ^ {2} \\ = f \left(x _ {k}\right) - \underbrace {\alpha_ {k} \left(1 - \frac {\alpha_ {k} L}{2}\right)} _ {\eta_ {k}} \| \nabla f \left(x _ {k}\right) \| ^ {2}. \tag {D.4} \\ \end{array}
$$

We next show that if we select

$$
0 <   \alpha_ {k} <   \frac {2}{L}, \tag {D.5}
$$

then the sequence $\{ f ( x _ { k } ) \} _ { k = 1 } ^ { \infty }$ converges to $f ( x ^ { * } )$ where $\nabla f ( x ^ { * } ) = 0$ . First, (D.5) implies that $\eta _ { k } > 0$ . Then, (D.4) implies that $f ( x _ { k + 1 } ) \leq f ( x _ { k } )$ . Therefore, $\{ f ( x _ { k } ) \}$ is a nonincreasing sequence. Second, since $f ( x _ { k } )$ is always bounded from below by $f ( x ^ { * } )$ , we know that $\{ f ( x _ { k } ) \}$ converges as $k  \infty$ according to the monotone convergence theorem in Theorem C.1. Suppose that the limit of the sequence is $f ^ { * }$ . Then, taking the limit on both sides of (D.4) gives

$$
\begin{array}{l} \lim  _ {k \rightarrow \infty} f (x _ {k + 1}) \leq \lim  _ {k \rightarrow \infty} f (x _ {k}) - \lim  _ {k \rightarrow \infty} \eta_ {k} \| \nabla f (x _ {k}) \| ^ {2} \\ \Leftrightarrow f ^ {*} \leq f ^ {*} - \lim _ {k \rightarrow \infty} \eta_ {k} \| \nabla f (x _ {k}) \| ^ {2} \\ \Leftrightarrow 0 \leq - \lim  _ {k \rightarrow \infty} \eta_ {k} \| \nabla f (x _ {k}) \| ^ {2}. \\ \end{array}
$$

Since $\eta _ { k } \| \nabla f ( x _ { k } ) \| ^ { 2 } \ge 0$ , the above inequality implies that $\begin{array} { r } { \operatorname* { l i m } _ { k  \infty } \eta _ { k } \| \nabla f ( x _ { k } ) \| ^ { 2 } = 0 } \end{array}$ . As a result, $x$ converges to $x ^ { * }$ where $\nabla f ( x ^ { * } ) = 0$ . The proof is complete. The above proof is inspired by [107]. □

The inequality in (D.5) provides valuable insights into how $\alpha _ { k }$ should be selected. If the function is flat ( $L$ is small), the step size can be large; otherwise, if the function is strongly convex ( $L$ is large), then the step size must be sufficiently small to ensure convergence. There are also many other ways to prove the convergence such as the contraction mapping theorem [108, Lemma 3]. A comprehensive introduction to convex optimization can be found in [106].

# Bibliography

[1] M. Pinsky and S. Karlin, An introduction to stochastic modeling (3rd Edition). Academic Press, 1998.   
[2] M. L. Puterman, Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, 2014.   
[3] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (2nd Edition). MIT Press, 2018.   
[4] R. A. Horn and C. R. Johnson, Matrix analysis. Cambridge University Press, 2012.   
[5] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-dynamic programming. Athena Scientific, 1996.   
[6] H. K. Khalil, Nonlinear systems (3rd Edition). Patience Hall, 2002.   
[7] G. Strang, Calculus. Wellesley-Cambridge Press, 1991.   
[8] A. Besenyei, “A brief history of the mean value theorem,” 2012. Lecture notes.   
[9] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward transformations: Theory and application to reward shaping,” in International Conference on Machine Learning, vol. 99, pp. 278–287, 1999.   
[10] R. E. Bellman, Dynamic programming. Princeton University Press, 2010.   
[11] R. E. Bellman and S. E. Dreyfus, Applied dynamic programming. Princeton University Press, 2015.   
[12] J. Bibby, “Axiomatisations of the average and a further generalisation of monotonic sequences,” Glasgow Mathematical Journal, vol. 15, no. 1, pp. 63–65, 1974.   
[13] A. S. Polydoros and L. Nalpantidis, “Survey of model-based reinforcement learning: Applications on robotics,” Journal of Intelligent & Robotic Systems, vol. 86, no. 2, pp. 153–173, 2017.

[14] T. M. Moerland, J. Broekens, A. Plaat, and C. M. Jonker, “Model-based reinforcement learning: A survey,” Foundations and Trends in Machine Learning, vol. 16, no. 1, pp. 1–118, 2023.   
[15] F.-M. Luo, T. Xu, H. Lai, X.-H. Chen, W. Zhang, and Y. Yu, “A survey on modelbased reinforcement learning,” arXiv:2206.09328, 2022.   
[16] X. Wang, Z. Zhang, and W. Zhang, “Model-based multi-agent reinforcement learning: Recent progress and prospects,” arXiv:2203.10603, 2022.   
[17] M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess, and J. T. Springenberg, “Learning by playing solving sparse reward tasks from scratch,” in International Conference on Machine Learning, pp. 4344–4353, 2018.   
[18] J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, and S. Levine, “How to train your robot with deep reinforcement learning: Lessons we have learned,” The International Journal of Robotics Research, vol. 40, no. 4-5, pp. 698–721, 2021.   
[19] S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and P. Stone, “Curriculum learning for reinforcement learning domains: A framework and survey,” The Journal of Machine Learning Research, vol. 21, no. 1, pp. 7382–7431, 2020.   
[20] C. Szepesv´ari, Algorithms for reinforcement learning. Springer, 2010.   
[21] A. Maroti, “RBED: Reward based epsilon decay,” arXiv:1910.13701, 2019.   
[22] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015.   
[23] W. Dabney, G. Ostrovski, and A. Barreto, “Temporally-extended epsilon-greedy exploration,” arXiv:2006.01782, 2020.   
[24] H.-F. Chen, Stochastic approximation and its applications, vol. 64. Springer Science & Business Media, 2006.   
[25] H. Robbins and S. Monro, “A stochastic approximation method,” The Annals of Mathematical Statistics, pp. 400–407, 1951.   
[26] J. Venter, “An extension of the Robbins-Monro procedure,” The Annals of Mathematical Statistics, vol. 38, no. 1, pp. 181–190, 1967.

[27] D. Ruppert, “Efficient estimations from a slowly convergent Robbins-Monro process,” tech. rep., Cornell University Operations Research and Industrial Engineering, 1988.   
[28] J. Lagarias, “Euler’s constant: Euler’s work and modern developments,” Bulletin of the American Mathematical Society, vol. 50, no. 4, pp. 527–628, 2013.   
[29] J. H. Conway and R. Guy, The book of numbers. Springer Science & Business Media, 1998.   
[30] S. Ghosh, “The Basel problem,” arXiv:2010.03953, 2020.   
[31] A. Dvoretzky, “On stochastic approximation,” in The Third Berkeley Symposium on Mathematical Statistics and Probability, 1956.   
[32] T. Jaakkola, M. I. Jordan, and S. P. Singh, “On the convergence of stochastic iterative dynamic programming algorithms,” Neural Computation, vol. 6, no. 6, pp. 1185–1201, 1994.   
[33] T. Kailath, A. H. Sayed, and B. Hassibi, Linear estimation. Prentice Hall, 2000.   
[34] C. K. Chui and G. Chen, Kalman filtering. Springer, 2017.   
[35] G. A. Rummery and M. Niranjan, On-line Q-learning using connectionist systems. Technical Report, Cambridge University, 1994.   
[36] H. Van Seijen, H. Van Hasselt, S. Whiteson, and M. Wiering, “A theoretical and empirical analysis of Expected Sarsa,” in IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, pp. 177–184, 2009.   
[37] M. Ganger, E. Duryea, and W. Hu, “Double Sarsa and double expected Sarsa with shallow and deep learning,” Journal of Data Analysis and Information Processing, vol. 4, no. 4, pp. 159–176, 2016.   
[38] C. J. C. H. Watkins, Learning from delayed rewards. PhD thesis, King’s College, 1989.   
[39] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no. 3-4, pp. 279–292, 1992.   
[40] T. C. Hesterberg, Advances in importance sampling. PhD Thesis, Stanford University, 1988.   
[41] H. Hasselt, “Double Q-learning,” Advances in Neural Information Processing Systems, vol. 23, 2010.

[42] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double Q-learning,” in AAAI Conference on Artificial Intelligence, vol. 30, 2016.   
[43] C. Dann, G. Neumann, and J. Peters, “Policy evaluation with temporal differences: A survey and comparison,” Journal of Machine Learning Research, vol. 15, pp. 809– 883, 2014.   
[44] J. Clifton and E. Laber, “Q-learning: Theory and applications,” Annual Review of Statistics and Its Application, vol. 7, pp. 279–301, 2020.   
[45] B. Jang, M. Kim, G. Harerimana, and J. W. Kim, “Q-learning algorithms: A comprehensive classification and applications,” IEEE Access, vol. 7, pp. 133653– 133667, 2019.   
[46] R. S. Sutton, “Learning to predict by the methods of temporal differences,” Machine Learning, vol. 3, no. 1, pp. 9–44, 1988.   
[47] G. Strang, Linear algebra and its applications (4th Edition). Belmont, CA: Thomson, Brooks/Cole, 2006.   
[48] C. D. Meyer and I. Stewart, Matrix analysis and applied linear algebra. SIAM, 2023.   
[49] M. Pinsky and S. Karlin, An introduction to stochastic modeling. Academic Press, 2010.   
[50] M. G. Lagoudakis and R. Parr, “Least-squares policy iteration,” The Journal of Machine Learning Research, vol. 4, pp. 1107–1149, 2003.   
[51] R. Munos, “Error bounds for approximate policy iteration,” in International Conference on Machine Learning, vol. 3, pp. 560–567, 2003.   
[52] A. Geramifard, T. J. Walsh, S. Tellex, G. Chowdhary, N. Roy, and J. P. How, “A tutorial on linear function approximators for dynamic programming and reinforcement learning,” Foundations and Trends in Machine Learning, vol. 6, no. 4, pp. 375–451, 2013.   
[53] B. Scherrer, “Should one compute the temporal difference fix point or minimize the Bellman residual? the unified oblique projection view,” in International Conference on Machine Learning, 2010.   
[54] D. P. Bertsekas, Dynamic programming and optimal control: Approximate dynamic programming (Volume II). Athena Scientific, 2011.   
[55] S. Abramovich, G. Jameson, and G. Sinnamon, “Refining Jensen’s inequality,” Bulletin math´ematique de la Soci´et´e des Sciences Math´ematiques de Roumanie, pp. 3–14, 2004.

[56] S. S. Dragomir, “Some reverses of the Jensen inequality with applications,” Bulletin of the Australian Mathematical Society, vol. 87, no. 2, pp. 177–194, 2013.   
[57] S. J. Bradtke and A. G. Barto, “Linear least-squares algorithms for temporal difference learning,” Machine Learning, vol. 22, no. 1, pp. 33–57, 1996.   
[58] K. S. Miller, “On the inverse of the sum of matrices,” Mathematics Magazine, vol. 54, no. 2, pp. 67–72, 1981.   
[59] S. A. U. Islam and D. S. Bernstein, “Recursive least squares for real-time implementation,” IEEE Control Systems Magazine, vol. 39, no. 3, pp. 82–85, 2019.   
[60] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing Atari with deep reinforcement learning,” arXiv preprint arXiv:1312.5602, 2013.   
[61] J. Fan, Z. Wang, Y. Xie, and Z. Yang, “A theoretical analysis of deep Q-learning,” in Learning for Dynamics and Control, pp. 486–489, 2020.   
[62] L.-J. Lin, Reinforcement learning for robots using neural networks. 1992. Technical report.   
[63] J. N. Tsitsiklis and B. Van Roy, “An analysis of temporal-difference learning with function approximation,” IEEE Transactions on Automatic Control, vol. 42, no. 5, pp. 674–690, 1997.   
[64] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient methods for reinforcement learning with function approximation,” Advances in Neural Information Processing Systems, vol. 12, 1999.   
[65] P. Marbach and J. N. Tsitsiklis, “Simulation-based optimization of Markov reward processes,” IEEE Transactions on Automatic Control, vol. 46, no. 2, pp. 191–209, 2001.   
[66] J. Baxter and P. L. Bartlett, “Infinite-horizon policy-gradient estimation,” Journal of Artificial Intelligence Research, vol. 15, pp. 319–350, 2001.   
[67] X.-R. Cao, “A basic formula for online policy gradient algorithms,” IEEE Transactions on Automatic Control, vol. 50, no. 5, pp. 696–699, 2005.   
[68] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine Learning, vol. 8, no. 3, pp. 229–256, 1992.   
[69] J. Peters and S. Schaal, “Reinforcement learning of motor skills with policy gradients,” Neural Networks, vol. 21, no. 4, pp. 682–697, 2008.

[70] E. Greensmith, P. L. Bartlett, and J. Baxter, “Variance reduction techniques for gradient estimates in reinforcement learning,” Journal of Machine Learning Research, vol. 5, no. 9, 2004.   
[71] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in International Conference on Machine Learning, pp. 1928–1937, 2016.   
[72] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, “Reinforcement learning through asynchronous advantage actor-critic on a GPU,” arXiv:1611.06256, 2016.   
[73] T. Degris, M. White, and R. S. Sutton, “Off-policy actor-critic,” arXiv:1205.4839, 2012.   
[74] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, “Deterministic policy gradient algorithms,” in International Conference on Machine Learning, pp. 387–395, 2014.   
[75] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,” arXiv:1509.02971, 2015.   
[76] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,” in International Conference on Machine Learning, pp. 1861–1870, 2018.   
[77] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, and P. Abbeel, “Soft actor-critic algorithms and applications,” arXiv:1812.05905, 2018.   
[78] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International Conference on Machine Learning, pp. 1889–1897, 2015.   
[79] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.   
[80] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation error in actor-critic methods,” in International Conference on Machine Learning, pp. 1587– 1596, 2018.   
[81] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy gradients,” in AAAI Conference on Artificial Intelligence, vol. 32, 2018.

[82] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch, “Multiagent actor-critic for mixed cooperative-competitive environments,” Advances in Neural Information Processing Systems, vol. 30, 2017.   
[83] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean field multiagent reinforcement learning,” in International Conference on Machine Learning, pp. 5571–5580, 2018.   
[84] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al., “Grandmaster level in Star-Craft II using multi-agent reinforcement learning,” Nature, vol. 575, no. 7782, pp. 350–354, 2019.   
[85] Y. Yang and J. Wang, “An overview of multi-agent reinforcement learning from game theoretical perspective,” arXiv:2011.00583, 2020.   
[86] S. Levine and V. Koltun, “Guided policy search,” in International Conference on Machine Learning, pp. 1–9, 2013.   
[87] M. Janner, J. Fu, M. Zhang, and S. Levine, “When to trust your model: Modelbased policy optimization,” Advances in Neural Information Processing Systems, vol. 32, 2019.   
[88] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective on reinforcement learning,” in International Conference on Machine Learning, pp. 449– 458, 2017.   
[89] M. G. Bellemare, W. Dabney, and M. Rowland, Distributional Reinforcement Learning. MIT Press, 2023.   
[90] H. Zhang, D. Liu, Y. Luo, and D. Wang, Adaptive dynamic programming for control: algorithms and stability. Springer Science & Business Media, 2012.   
[91] F. L. Lewis, D. Vrabie, and K. G. Vamvoudakis, “Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers,” IEEE Control Systems Magazine, vol. 32, no. 6, pp. 76–105, 2012.   
[92] F. L. Lewis and D. Liu, Reinforcement learning and approximate dynamic programming for feedback control. John Wiley & Sons, 2013.   
[93] Z.-P. Jiang, T. Bian, and W. Gao, “Learning-based control: A tutorial and some recent results,” Foundations and Trends in Systems and Control, vol. 8, no. 3, pp. 176–284, 2020.   
[94] S. Meyn, Control systems and reinforcement learning. Cambridge University Press, 2022.

[95] S. E. Li, Reinforcement learning for sequential decision and optimal control. Springer, 2023.   
[96] J. S. Rosenthal, First look at rigorous probability theory (2nd Edition). World Scientific Publishing Company, 2006.   
[97] D. Pollard, A user’s guide to measure theoretic probability. Cambridge University Press, 2002.   
[98] P. J. Spreij, “Measure theoretic probability,” UvA Course Notes, 2012.   
[99] R. G. Bartle, The elements of integration and Lebesgue measure. John Wiley & Sons, 2014.   
[100] M. Taboga, Lectures on probability theory and mathematical statistics (2nd Edition). CreateSpace Independent Publishing Platform, 2012.   
[101] T. Kennedy, “Theory of probability,” 2007. Lecture notes.   
[102] A. W. Van der Vaart, Asymptotic statistics. Cambridge University Press, 2000.   
[103] L. Bottou, “Online learning and stochastic approximations,” Online Learning in Neural Networks, vol. 17, no. 9, p. 142, 1998.   
[104] D. Williams, Probability with martingales. Cambridge University Press, 1991.   
[105] M. M´etivier, Semimartingales: A course on stochastic processes. Walter de Gruyter, 1982.   
[106] S. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization. Cambridge University Press, 2004.   
[107] S. Bubeck et al., “Convex optimization: Algorithms and complexity,” Foundations and Trends in Machine Learning, vol. 8, no. 3-4, pp. 231–357, 2015.   
[108] A. Jung, “A fixed-point of view on gradient methods for big data,” Frontiers in Applied Mathematics and Statistics, vol. 3, p. 18, 2017.

# Symbols

In this book, a matrix or a random variable is represented by capital letters. A vector, a scalar, or a sample is represented by a lowercase letter. The mathematical symbols that are frequently used in this book are listed below.

<table><tr><td>=</td><td>equality</td></tr><tr><td>≈</td><td>approximation</td></tr><tr><td>÷</td><td>equality by definition</td></tr><tr><td>≥, &gt;, ≤, &lt;</td><td>elementwise comparison</td></tr><tr><td>∈</td><td>is an element of</td></tr><tr><td>||·||2</td><td>Euclidean norm of a vector or the corresponding in-
duced matrix norm</td></tr><tr><td>||·||∞</td><td>maximum norm of a vector or the corresponding in-
duced matrix norm</td></tr><tr><td>ln</td><td>natural logarithm</td></tr><tr><td>R</td><td>set of real numbers</td></tr><tr><td>Rn</td><td>set of n-dimensional real vectors</td></tr><tr><td>Rn×m</td><td>set of all n × m-dimensional real matrices</td></tr><tr><td>A ≥ 0 (A &gt; 0)</td><td>matrix A is positive semidefinite (definite)</td></tr><tr><td>A ≦ 0 (A &gt; 0)</td><td>matrix A is negative semidefinite (definite)</td></tr><tr><td>|x|</td><td>absolute value of real scalar x</td></tr><tr><td>|S|</td><td>number of elements in set S</td></tr><tr><td>∇xf(x)</td><td>gradient of scalar function f(x) with respect to vector x. It may be written as ∇f(x) for short.</td></tr><tr><td>[A]ij</td><td>element in theith row and jth column of matrix A</td></tr><tr><td>[x]i</td><td>ith element of vector x</td></tr><tr><td>X ~ p</td><td>p is the probability distribution of random variable X.</td></tr><tr><td>p(X=x), Pr(X=x)</td><td>probability of X=x. They are often written as p(x) or Pr(x) for short.</td></tr><tr><td>p(x|y)</td><td>conditional probability</td></tr><tr><td>E_{X~p}[X]</td><td>expectation or expected value of random variable X. It is often written as E[X] for short when the distribu-
bution of X is clear.</td></tr></table>

$\operatorname { v a r } ( X )$ variance of random variable $X$

arg maxx f(x) maximizer of function $f ( x )$

1n vector of all ones. It is often written as 1 for short when its dimension is clear.

$I _ { n }$ $n \times n$ -dimensional identity matrix. It is often written as $I$ for short when its dimensions are clear.

# Index

$\epsilon$ -greedy policy, 89

$n$ -step Sarsa, 138

action, 2

action space, 2

action value, 30

illustrative examples, 31

relationship to state value, 30

undiscounted case, 205

actor-critic, 216

advantage actor-critic, 217

deterministic actor-critic, 227

off-policy actor-critic, 221

QAC, 216

advantage actor-critic, 217

advantage function, 220

baseline invariance, 217

optimal baseline, 218

pseudocode, 221

agent, 12

Bellman equation, 20

closed-form solution, 27

elementwise expression, 21

equivalent expressions, 22

expression in action values, 32

illustrative examples, 22

iterative solution, 28

matrix-vector expression, 26

policy evaluation, 27

Bellman error, 173

Bellman expectation equation, 127

Bellman optimality equation, 38

contraction property, 44

elementwise expression, 38

matrix-vector expression, 40

optimal policy, 47

optimal state value, 47

solution and properties, 46

bootstrapping, 18

Cauchy sequence, 42

contraction mapping, 41

contraction mapping theorem, 42

deterministic actor-critic, 227

policy gradient theorem, 228

pseudocode, 235

deterministic policy gradient, 235

discount rate, 9

discounted return, 9

Dvoretzky’s convergence theorem, 109

environment, 12

episode, 10

episodic tasks, 10

expected Sarsa, 137

experience replay, 183

exploration and exploitation, 92

policy gradient, 212

feature vector, 152

fixed point, 41

grid world example, 1

importance sampling, 221

illustrative examples, 223

importance weight, 222

law of large numbers, 80

least-squares TD, 177

recursive least squares, 178

Markov decision process, 11

model and dynamics, 11

Markov process, 12

Markov property, 11

mean estimation, 78

incremental manner, 102

metrics for policy gradient

average reward, 195

average value, 193

equivalent expressions, 197

metrics for value function approximation

Bellman error, 173

projected Bellman error, 174

Monte Carlo methods, 78

MC $\epsilon$ -Greedy, 90

MC Basic, 81

MC Exploring Starts, 86

comparison with TD learning, 129

on-policy, 142

off-policy, 141

off-policy actor-critic, 221

importance sampling, 221

policy gradient theorem, 224

pseudocode, 226

on-policy, 141

online and offline, 130

optimal policy, 37

greedy is optimal, 47

impact of the discount rate, 49

impact of the reward values, 51

optimal state value, 37

Poisson equation, 205

policy, 4

function representation, 192

deterministic policy, 5

stochastic policy, 5

tabular representation, 6

policy evaluation

illustrative examples, 17

solving the Bellman equation, 27

policy gradient theorem, 198

deterministic case, 228

off-policy case, 224

policy iteration algorithm, 62

comparison with value iteration, 70

convergence analysis, 64

pseudocode, 66

projected Bellman error, 174

Q-learning (deep Q-learning), 182

experience replay, 183

illustrative examples, 184

main network, 182

pseudocode, 184

replay buffer, 183

target network, 182

Q-learning (function representation), 180

Q-learning (tabular representation), 140

illustrative examples, 144

pseudocode, 143

off-policy, 141

QAC, 216

REINFORCE, 210

replay buffer, 183

return, 8

reward, 6

Robbins-Monro algorithm, 103

application to mean estimation, 108

convergence analysis, 106

Sarsa (function representation), 179

Sarsa (tabular representation), 133

convergence analysis, 134

on-policy, 141

variant: $n$ -step Sarsa, 138

variant: expected Sarsa, 137

algorithm, 133

optimal policy learning, 135

state, 2

state space, 2

state transition, 3

state value, 19

function representation, 152

relationship to action value, 30

undiscounted case, 205

stationary distribution, 157

metrics for policy gradient, 193

metrics for value function approximation, 156

stochastic gradient descent, 114

application to mean estimation, 116

comparison with batch gradient descent, 119

convergence analysis, 121

convergence pattern, 116

deterministic formulation, 118

TD error, 128

TD target, 128

temporal-difference methods, 125

$n$ -step Sarsa, 138

Q-learning, 140

Sarsa, 133

TD learning of state values, 126

a unified viewpoint, 145

expected Sarsa, 137

value function approximation, 151

trajectory, 8

truncated policy iteration, 70

comparison with value iteration and policy iteration, 74

pseudocode, 72

value function approximation

Q-learning with function approximation, 180

Sarsa with function approximation, 179

TD learning of state values, 155

deep Q-learning, 182

function approximators, 162

illustrative examples, 164

least-squares TD, 177

linear function, 155

theoretical analysis, 167

value iteration algorithm, 58

comparison with policy iteration, 70

pseudocode, 60

# Undergraduate Fundamentals of Machine Learning

The initial version of this textbook was created by William J. Deuschle for his senior thesis, based on his notes of CS181 during the Spring of 2017. This textbook has since been maintained by the CS181 course staff with bug fixes from many CS181 students.

# Contents

# 1 Introduction to Machine Learning 1

1.1 What is Machine Learning? 1   
1.2 What Will This Book Teach Me? 1   
1.3 Our Machine Learning Framework 2   
1.4 This Book’s Notation . 3

Mathematical and Statistical Notation 3   
Textbook Specific Notation 3

# 2 Regression 4

2.1 Defining the Problem . 4   
2.2 Solution Options 4

2.2.1 K-Nearest-Neighbors . 5   
2.2.2 Neural Networks 5   
2.2.3 Random Forests 5   
2.2.4 Gradient Boosted Trees 5   
2.2.5 Turning to Linear Regression 5

2.3 Introduction to Linear Regression . 6   
2.4 Basic Setup 6

2.4.1 Merging of Bias . 7   
2.4.2 Visualization of Linear Regression 7

2.5 Finding the Best Fitting Line 8

2.5.1 Objective Functions and Loss 8   
2.5.2 Least Squares Loss . 9

2.6 Linear Regression Algorithms . . 10

2.6.1 Optimal Weights via Matrix Differentiation 10   
2.6.2 Bayesian Solution: Maximum Likelihood Estimation 11   
2.6.3 Alternate Interpretation: Linear Regression as Projection 13

2.7 Model Flexibility . . 13

2.7.1 Basis Functions . 13   
2.7.2 Regularization 16

2.7.3 Generalizing Regularization 19   
2.7.4 Bayesian Regularization 20

# 2.8 Choosing Between Models 22

2.8.1 Bias-Variance Tradeoff and Decomposition 22   
2.8.2 Cross-Validation 26   
2.8.3 Making a Model Choice 26   
2.8.4 Bayesian Model Averaging . . . 27

# 2.9 Linear Regression Extras 27

2.9.1 Predictive Distribution . 27

# 2.10 Conclusion 28

# 3 Classification 29

3.1 Defining the Problem . 29   
3.2 Solution Options 29   
3.3 Discriminant Functions 30

3.3.1 Basic Setup: Binary Linear Classification 30   
3.3.2 Multiple Classes 31   
3.3.3 Basis Changes in Classification 31

# 3.4 Numerical Parameter Optimization and Gradient Descent . 33

3.4.1 Gradient Descent . 35   
3.4.2 Batch Gradient Descent versus Stochastic Gradient Descent . . 36

# 3.5 Objectives for Decision Boundaries 36

3.5.1 0/1 Loss . . 36   
3.5.2 Least Squares Loss . 37   
3.5.3 Hinge Loss 38

# 3.6 Probabilistic Methods 39

3.6.1 Probabilistic Discriminative Models 39

Logistic Regression . 40   
Multi-Class Logistic Regression and Softmax 42

# 3.6.2 Probabilistic Generative Models 44

Classification in the Generative Setting 44   
MLE Solution . . 45   
Naive Bayes . . 47

# 3.7 Conclusion 49

# 4 Neural Networks 50

4.1 Motivation 50

4.1.1 Comparison to Other Methods 51   
4.1.2 Universal Function Approximation 51

# 4.2 Feed-Forward Networks 52

4.3 Neural Network Basics and Terminology 52

4.3.1 Adaptive Basis Functions 53

4.4 Network Training . 56

4.4.1 Objective Function . 56   
4.4.2 Optimizing Parameters 56   
4.4.3 Backpropagation 57   
4.4.4 Computing Derivatives Using Backpropagation . 57

4.5 Choosing a Network Structure 61

4.5.1 Cross Validation for Neural Networks 61   
4.5.2 Preventing Overfitting . . 62

Regularization 62   
Data Augmentation 62

4.6 Specialized Forms of Neural Networks 63

4.6.1 Convolutional Neural Networks (CNNs) . 63   
4.6.2 Recurrent Neural Networks (RNNs) 63   
4.6.3 Bayesian Neural Networks (BNNs) . . 64

# 5 Support Vector Machines 65

5.1 Motivation 65

5.1.1 Max Margin Methods 65   
5.1.2 Applications 66

5.2 Hard Margin Classifier for Linearly Separable Data . 67

5.2.1 Why the Hard Margin . . 67   
5.2.2 Deriving our Optimization Problem 67   
5.2.3 What is a Support Vector 69

5.3 Soft Margin Classifier 70

5.3.1 Why the Soft Margin? 70   
5.3.2 Updated Optimization Problem for Soft Margins 71   
5.3.3 Soft Margin Support Vectors 72

5.4 Conversion to Dual Form 72

5.4.1 Lagrange Multipliers . 73   
5.4.2 Deriving the Dual Formulation 74   
5.4.3 Making Predictions . 75   
5.4.4 Why is the Dual Formulation Helpful? . 76   
5.4.5 Kernel Composition 77

# 6 Clustering 78

6.1 Motivation 78   
6.1.1 Applications 79   
6.2 K-Means Clustering 79

6.2.1 Lloyd’s Algorithm 79   
6.2.2 Example of Lloyd’s . 81   
6.2.3 Number of Clusters 84   
6.2.4 Initialization and K-Means++ 84   
6.2.5 K-Medoids Alternative . 86

# 6.3 Hierarchical Agglomerative Clustering 86

6.3.1 HAC Algorithm 87   
6.3.2 Linkage Criterion . 89

Min-Linkage Criteria . 89   
Max-Linkage Criterion . 89   
Average-Linkage Criterion . 90   
Centroid-Linkage Criterion 90   
Different Linkage Criteria Produce Different Clusterings . 90

6.3.3 How HAC Differs from K-Means 91

# 7 Dimensionality Reduction 92

7.1 Motivation 92   
7.2 Applications . . 93   
7.3 Principal Component Analysis 93

7.3.1 Reconstruction Loss 94   
7.3.2 Minimizing Reconstruction Loss 96   
7.3.3 Multiple Principal Components . 97   
7.3.4 Identifying Directions of Maximal Variance in our Data 97   
7.3.5 Choosing the Optimal Number of Principal Components . . 98

7.4 Conclusion 101

# 8 Graphical Models 102

8.1 Motivation 102   
8.2 Directed Graphical Models (Bayesian Networks) 102

8.2.1 Joint Probability Distributions . 105   
8.2.2 Generative Models 105   
8.2.3 Generative Modeling vs. Discriminative Modeling . 107   
8.2.4 Understanding Complexity 108   
8.2.5 Independence and D-Separation . 109

8.3 Example: Naive Bayes . . 112   
8.4 Conclusion 112

# 9 Mixture Models 113

9.1 Motivation 113   
9.2 Applications . 115

9.3 Fitting a Model 115

9.3.1 Maximum Likelihood for Mixture Models 115   
9.3.2 Complete-Data Log Likelihood 116

9.4 Expectation-Maximization (EM) 116

9.4.1 Expectation Step . . 117   
9.4.2 Maximization Step . . 118   
9.4.3 Full EM Algorithm . 119

9.4.4 The Math of EM . 119

The Evidence Lower Bound (ELBO) . 120   
Optimization 121   
Correctness . 123   
Equivalence to Prior Formulation . . 123

9.4.5 Connection to K-Means Clustering . . 123   
9.4.6 Dice Example: Mixture of Multinomials 124

9.5 Gaussian Mixture Models (GMM) 126   
9.6 Admixture Models: Latent Dirichlet Allocation (LDA) . . 127

9.6.1 LDA for Topic Modeling . . 127   
9.6.2 Applying EM to LDA 128

9.7 Conclusion 130

# 10 Hidden Markov Models 131

10.1 Motivation 131   
10.2 Applications . . 132   
10.3 HMM Data, Model, and Parameterization 133

10.3.1 HMM Data 133   
10.3.2 HMM Model Assumptions . . 133   
10.3.3 HMM Parameterization 134

10.4 Inference in HMMs . 134

10.4.1 The Forward-Backward Algorithm . . . 135   
10.4.2 Using $\alpha$ ’s and $\beta$ ’s for Training and Inference 138

p(Seq) 138  
Prediction . 138   
Smoothing 138   
Transition . 139   
Filtering . . 139   
Best path 139

10.5 Using EM to Train a HMM 140

10.5.1 E-Step . . 140   
10.5.2 M-Step 141

10.6 Conclusion 141

# 11 Markov Decision Processes 142

11.1 Formal Definition of an MDP 143   
11.2 Finite Horizon Planning 144   
11.3 Infinite Horizon Planning 144

11.3.1 Value iteration . 145   
Bellman Consistency Equation and Bellman Optimality 145   
Bellman Operator . 147   
Value Iteration Algorithm . 147

11.3.2 Policy Iteration . 148

Policy Evaluation . . . 148

# 12 Reinforcement Learning 149

12.1 Motivation 149   
12.2 General Approaches to RL . 149   
12.3 Model-Free Learning . 150

12.3.1 SARSA and Q-Learning . . . 151

Convergence Conditions 152

12.3.2 Deep Q-Networks . . . 152   
12.3.3 Policy Learning . . . . 153

12.4 Model-Based Learning . . 154   
12.5 Conclusion 155

# Chapter 1

# Introduction to Machine Learning

# 1.1 What is Machine Learning?

There is a great deal of misunderstanding about what machine learning is, fueled by recent success and at times sensationalist media coverage. While its applications have been and will continue to be extraordinarily powerful under the right circumstances, it’s important to gain some sense of where and why the tools presented in this book will be applicable. Broadly, machine learning is the application of statistical, mathematical, and numerical techniques to derive some form of knowledge from data. This ‘knowledge’ may afford us some sort of summarization, visualization, grouping, or even predictive power over data sets.

With all that said, it’s important to emphasize the limitations of machine learning. It is not nor will it ever be a replacement for critical thought and methodical, procedural work in data science. Indeed, machine learning can be reasonably characterized a loose collection of disciplines and tools. Where the lines begin that separate machine learning from statistics or mathematics or probability theory or any other handful of fields that it draws on are not clear. So while this book is a synopsis of the basics of machine learning, it might be better understood as a collection of tools that can be applied to a specific subset of problems.

# 1.2 What Will This Book Teach Me?

The purpose of this book is to provide you the reader with the following: a framework with which to approach problems that machine learning learning might help solve. You will hopefully come away with a sense of the strengths and weaknesses of the tools presented within and, even more importantly, gain an understanding of how these tools are situated among problems of interest. To that end, we will aim to develop systems for thinking about the structure of the problems we work on. That way, as we continue to add new tools and techniques to our repertoire, we will always have a clear view of the context in which we can expect to use them. This will not only create a nice categorization of the different practices in machine learning, it will also help motivate why these techniques exist in the first place.

You will not be an expert in any individual ML concept after reading this text. Rather, you should come away with three different levels of understanding. First, you should gain a general contextual awareness of the different problem types that ML techniques may be used to solve. Second, you should come away with a practical awareness of how different ML techniques operate. This means that after you have successfully identified an appropriate ML technique for a given problem, you will also know how that method actually accomplishes the goal at hand. If you only

come away with these two levels of understanding, you will be off to a good start. The third level of understanding relates to having a derivational awareness of the algorithms and methods we will make use of. This level of understanding is not strictly necessary to successfully interact with existing machine learning capabilities, but it will be required if you desire to go further and deepen existing knowledge. Thus, we will be presenting derivations, but it will be secondary to a high level understanding of problem types and the practical intuition behind available solutions.

# 1.3 Our Machine Learning Framework

Let’s consider for the first time what we will call the Machine Learning Cube. The purpose of this cube is to describe the domain of problems we will encounter, and it will be a useful way to delineate the techniques we will apply to different types of problems. Understanding the different facets of the cube will aid you in understanding machine learning as a whole, and can even give you intuition about techniques that you have never encountered before. Let’s now describe the features of the cube.

Our cube has three axes. On the first axis we will put the output domain. The domain of our output can take on one of two forms: discrete or continuous. Discrete, or categorical data, is data that can only fall into one of $n$ specific classes. For example: male or female, integer values between 1 and 10, or different states in the U.S. are all examples of categorical data. Continuous data is that which falls on the real number line.

The second axis of the cube is reserved for the statistical nature of the machine learning technique in question. Specifically, it will fall into one of two broad categories: probabilistic or non-probabilistic techniques. Probabilistic techniques are those for which we incorporate our data using some form of statistical distribution or summary. In general, we are then able to discard some or all of our data once we have finished tuning our probabilistic model. In contrast, non-probabilistic techniques are those that use the data directly to perform some action. A very common and general example of this is comparing how close a new data point is to other points in your existing data set. Non-probabilistic techniques potentially make fewer assumptions, but they do require that you keep around some or all of your data. They are also potentially slower techniques at runtime because they may require touching all of the data in your dataset to perform some action. These are a very broad set of guidelines for the distinction between probabilistic and non-probabilistic techniques - you should expect to see some exceptions and even some techniques that fit into both of these categories to some degree. Having a sense for their general benefits and drawbacks is useful, and you will gain more intuition about the distinction as we begin to explore different techniques.

The third and final axis of the cube describes the type of training we will use. There are two major classes of machine learning techniques: supervised and unsupervised. In fact, these two classes of techniques are so important to describing the field of machine learning that we will roughly divide this textbook into two halves dedicated to techniques found within each of these categories. A supervised technique is one for which we get to observe a data set of both the inputs and the outputs ahead of time, to be used for training. For example, we might be given a data set about weather conditions and crop production over the years. Then, we could train a machine learning model that learns a relationship between the input data (weather) and output data (crop production). The implication here is that given new input data, we will be able to predict the unseen output data. An unsupervised technique is one for which we only get a data set of ‘inputs’ ahead of time. In fact, we don’t even need to consider these as inputs anymore: we can just consider them to be a set of data points that we wish to summarize or describe. Unsupervised techniques

revolve around clustering or otherwise describing our data.

We will see examples of all of these as we progress throughout the book, and you will gain an intuition for where different types of data and techniques fall in our cube. Eventually, given just the information in the cube for a new technique, you will have a solid idea of how that technique operates.

# 1.4 This Book’s Notation

The machine learning community uses a number of different conventions, and learning to decipher the different versions of those conventions is important to understanding work done in the field. For this book, we will try to stick to a standard notation that we define here in part. In addition to mathematical and statistical notation, we will also describe the conventions used in this book for explaining and breaking up different concepts.

# Mathematical and Statistical Notation

We will describe the dimensionality of variables when necessary, but generic variables will often be sufficient when explaining new techniques. Boldface variables (x) represent vectors, capital boldface characters ( $\mathbf { x }$ ) represent matrices, and standard typeface variables (x) describe scalars.

Statistical distributions will sometimes be described in terms of their probability density function (PDF), e.g. $\begin{array} { r } { Y \sim \frac { 1 } { \sigma \sqrt { 2 \pi } } e ^ { - ( x - \mu ) ^ { 2 } / 2 \sigma ^ { 2 } } } \end{array}$ e−(x−µ)2/2σ2. Alternatively, in the case of a well known probability 6 distribution, we will describe those in terms of their standard notation, e.g. $Y \sim { \mathcal { N } } ( \mu , \sigma ^ { 2 } )$

# Textbook Specific Notation

We have also introduced a few conventions to make consumption of this material easier. We have boxes dedicated to definitions, explaining techniques in the context of the ML Framework Cube, and for explaining common misconceptions:

Definition 1.4.1 (Definition Explanation): You will find definitions in these dark gray boxes.

Derivation 1.4.1 (Derivation Explanation): You will find derivations in these light gray boxes.

# ML Framework Cube: ML Framework Cube

You will find ML Framework Cube explanations in these blue wrapped boxes.

$\star$ You will find explanations for subtle or confusing concepts in these red wrapped boxes.

# Chapter 2

# Regression

A major component of machine learning, the one that most people associate with ML, is dedicated to making predictions about a target given some inputs, such as predicting how much money an individual will earn in their lifetime given their demographic information. In this chapter, w!e’re going to focus on the case where our prediction is a continuous, real number. When the target is a real number, we call this prediction procedure regression.

# 2.1 Defining the Problem

In order to understand regression, let’s start in a more natural place: what types of problems are we trying to solve? What exactly does it mean to make a prediction that is a continuous, real number? To build some intuition, here are a few examples of problems that regression could be used for:

1. Predicting a person’s height given the height of their parents.   
2. Predicting the amount of time someone will take to pay back a loan given their credit history.   
3. Predicting what time a package will arrive given current weather and traffic conditions.

Hopefully you are starting to see the pattern emerging here. Given some inputs, we need to produce a prediction for a continuous output. That is exactly the purpose of regression. Notice that regression isn’t any one technique in particular. It’s just a class of methods that helps us achieve our overall goal of predicting a continuous output.

Definition 2.1.1 (Regression): A class of techniques that seeks to make predictions about unknown continuous target variables given observed input variables.

# 2.2 Solution Options

Now that you understand the type of problems we are trying to solve with regression, we can start to think at a high level of the different ways we might arrive at a solution. Here is a short, nonexhaustive list of regression techniques with brief explanations:

# 2.2.1 K-Nearest-Neighbors

K-Nearest-Neighbors is an extremely intuitive, non-parametric technique for regression or classification. It works as follows in the regression case:

1. Identify the $K$ points in our data set that are closest to the new data point. ‘Closest’ is some measure of distance, usually Euclidean.   
2. Average the value of interest for those $K$ data points.   
3. Return that averaged value of interest: it is the prediction for our new data point.

$\star$ A non-parametric model simply means we don’t make any assumptions about the form of our data. We only need to use the data itself to make predictions.

# 2.2.2 Neural Networks

A neural network works by scaling and combining input variables many times. Furthermore, at each step of scaling and combining inputs, we typically apply some form of nonlinearity over our data values. The proof is beyond the scope of this textbook, but neural networks are known to be universal function approximators. This means that given enough scaling and combining steps, we can approximate any function to an arbitrarily high degree of accuracy using a neural network.

# 2.2.3 Random Forests

Random forests are what’s known as an ensemble method. This means we average the results of many smaller regressors known as decision trees to produce a prediction. These decision trees individually operate by partitioning our original data set with respect to the value of predictive interest using a subset of the features present in that data set. Each decision tree individually may not be a reliable regressor; by combining many of them we achieve a model with better performance.

# 2.2.4 Gradient Boosted Trees

Gradient boosted trees are another technique built on decision trees. Assume we start with a set of decision trees that together achieve a certain level of performance. Then, we iteratively add new trees to improve performance on the hardest examples in our data set, and reweight our new set of decision trees to produce the best level of performance possible.

# 2.2.5 Turning to Linear Regression

You’ve likely never even heard of some of these preceding techniques - that’s okay. The point is that we have a number of existing methods that take different approaches to solving regression problems. Each of these will have their own strengths and weaknesses that contribute to a decision about whether or not you might use them for a given regression problem. We obviously do not cover these methods in great detail here; what’s more important is the understanding that there are a variety of ways to go about solving regression problems. From here on out, we will take a deeper dive into linear regression. There are several reasons for which we are exploring this specific technique in greater detail. The two main reasons are that it has been around for a long time and thus is very well understood, and it also will introduce many concepts and terms that will be utilized extensively in other machine learning topics.

# 2.3 Introduction to Linear Regression

In this chapter, we’re specifically going to focus on linear regression, which means that our goal is to find some linear combination of the $x _ { 1 } , . . . , x _ { D }$ input values that predict our target $y$ .

Definition 2.3.1 (Linear Regression): Suppose we have an input $\mathbf { x } \in \mathbb { R } ^ { D }$ and a continuous target $y \in \mathbb { R }$ . Linear regression determines weights $w _ { i } \in \mathbb { R }$ that combine the values of $x _ { i }$ to produce $y$ :

$$
y = w _ {0} + w _ {1} x _ {1} + \dots + w _ {D} x _ {D} \tag {2.1}
$$

$\star$ Notice $\scriptstyle w _ { 0 }$ in the expression above, which doesn’t have a corresponding $x _ { 0 }$ value. This is known as the bias term. If you consider the definition of a line $y = m x + b$ , the bias term is corresponds to the intercept $b$ . It accounts for data that has a non-zero mean.

Let’s illustrate how linear regression works using an example, considering the case of 10 year old Sam. She is curious about how tall she will be when she grows up. She has a data set of parents’ heights and the final heights of their children. The inputs $\mathbf { x }$ are:

$$
x _ {1} = \text {h e i g h t o f m o t h e r (c m)}
$$

$$
x _ {2} = \text {h e i g h t o f f a t h e r (c m)}
$$

Using linear regression, she determines the weights w to be:

$$
\mathbf {w} = \left[ 3 4, 0. 3 9, 0. 3 3 \right]
$$

Sam’s mother is 165 cm tall and her father is 185 cm tall. Using the results of the linear regression solution, Sam solves for her expected height:

$$
\text {S a m ' s h e i g h t} = 3 4 + 0. 3 9 (1 6 5) + 0. 3 3 (1 8 5) = 1 5 9. 4 \mathrm {c m}
$$

# ML Framework Cube: Linear Regression

Let’s inspect the categories linear regression falls into for our ML framework cube. First, as we’ve already stated, linear regression deals with a continuous output domain. Second, our goal is to make predictions on future data points, and to construct something capable of making those predictions we first need a labeled data set of inputs and outputs. This makes linear regression a supervised technique. Third and finally, linear regression is non-probabilistic. Note that there also exist probabilistic interpretations of linear regression which we will discuss later in the chapter.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Continuous</td><td>Supervised</td><td>No</td></tr></table>

# 2.4 Basic Setup

The most basic form of linear regression is a simple weighted combination of the input variables $\mathbf { x }$ , which you will often see written as:

$$
f (\mathbf {x}, \mathbf {w}) = w _ {0} + w _ {1} x _ {1} + \dots + w _ {D} x _ {D} \tag {2.2}
$$

![](images/ebbda3b372e3ffc62a6d7e0527e23b4a9320ece9557ffdb4b010320909f245f1.jpg)  
Figure 2.1: Data set with clear trend.

# 2.4.1 Merging of Bias

We’re going to introduce a common notational trick here for making the bias term, $w _ { 0 }$ , easier to handle. At the moment $w _ { 0 }$ is unwieldy because it is not being multiplied by an $x _ { i }$ value. A simple way to make our bias term easier to handle is to simply introduce another variable, $x _ { 0 }$ , that is always 1 for every data point. For example, considering the case of Sam’s height from above, we have the height of her parents, x:

$$
\mathbf {x} = (1 6 5, 1 8 5)
$$

We now add a 1 in the first position of the data point to make it:

$$
\mathbf {x} ^ {\prime} = (1, 1 6 5, 1 8 5)
$$

We do this for every point in our data set. This bias trick lets us write:

$$
f (\mathbf {x}, \mathbf {w}) = \mathbf {w} ^ {\top} \mathbf {x} = w _ {0} x _ {0} + w _ {1} x _ {1} + \dots + w _ {D} x _ {D} \tag {2.3}
$$

This is more compact, easier to reason about, and makes properties of linear algebra nicer for the calculations we will be performing.

# 2.4.2 Visualization of Linear Regression

Let’s try to build some intuition about how linear regression works. Our algorithm is given a collection of data points: inputs $\mathbf { x }$ and corresponding targets y. Our goal is to find the best set of weights w such that given a new data point $\mathbf { x }$ , we can accurately predict the true target value $y$ . This is visualizable in the simple case where $\mathbf { x }$ is a 1-dimensional input variable, as in Figure 2.1.

Our eyes are naturally able to detect a very clear trend in this data. If we were given a new $\mathbf { x } ^ { * }$ data point, how would be predict its target value $y$ ? We would first fit a line to our data, as in Figure 2.2, and then find where on that line the new $\mathbf { x } ^ { * }$ value sits.

That is the entirety of linear regression! It fits the ‘best’ line to our data, and then uses that line to make predictions. In 1-D input space, this manifests itself as the simple problem seen above,

![](images/42e090b80a46446c7266020d32b9ff7fab1736cd022de176f1244e2624697a83.jpg)  
Figure 2.2: Data set with clear trend, best fitting line included.

where we need only find a single bias term $w _ { 0 }$ (which acts as the intercept of the line) and single weight $w _ { 1 }$ (which acts as the slope of the line). However, the same principle applies to higher dimensional data as well. We’re always fitting the hyperplane that best predicts the data.

$\star$ Although our input data points $\mathbf { x }$ can take on multiple dimensions, our output data $_ y$ is always a 1-dimensional real number when dealing with regression problems.

Now that we have some intuition for what linear regression is, a natural question arises: how do we find the optimal values for w? That is the remaining focus of this chapter.

# 2.5 Finding the Best Fitting Line

# 2.5.1 Objective Functions and Loss

Now that we’ve defined our model as a weighted combination of our input variables, we need some way to choose our value of w. To do this, we need an objective function.

Definition 2.5.1 (Objective Function): A function that measures the ‘goodness’ of a model. We can optimize this function to identify the best possible model for our data.

As the definition explains, the purpose of an objective function is to measure how good a specific model is. We can therefore optimize this function to find a good model. Note that in the case of linear regression, our ‘model’ is just a setting of our parameters w.

An objective function will sometimes be referred to as loss. Loss actually measures how bad a model is, and then our goal is to minimize it. It is common to think in terms of loss when discussing linear regression, and we incur loss when the hyperplane we fit is far away from our data.

So how do we compute the loss for a specific setting of w? To do this, we often use residuals.

Definition 2.5.2 (Residual): The residual is the difference between the target (y) and predicted $y ( \mathbf { x } , \mathbf { w } ) )$ value that a model produces:

$$
\text {r e s i d u a l} = \text {t a r g e t} - \text {p r e d i c t i o n} = y - f (\mathbf {x}, \mathbf {w}) = \boxed {y - \mathbf {w} ^ {\top} \mathbf {x}}
$$

Commonly, loss is a function of the residuals produced by a model. For example, you can imagine taking the absolute value of all of the residuals and adding those up to produce a measurement of loss. This is sometimes referred to as L1 Loss. Or, you might square all of the residuals and then add those up to produce loss, which is called L2 loss or least squares loss. You might also use some combination of L1 and L2 loss. For the most part, these are the two most common forms of loss you will see when discussing linear regression.

When minimized, these distinct measurements of loss will produce solutions for w that have different properties. For example, L2 loss is not robust to outliers due to the fact that we are squaring residuals. Furthermore, L2 loss will produce only a single solution while L1 loss can potentially have many equivalent solutions. Finally, L1 loss produces unstable solutions, meaning that for small changes in our data set, we may see large changes in our solution w.

Loss is a concept that we will come back to very frequently in the context of supervised machine learning methods. Before exploring exactly how we use loss to fit a line, let’s consider least squares loss in greater depth.

# 2.5.2 Least Squares Loss

As we mentioned above, there are different methods for computing loss. One of the most commonly used measurements is known as least squares or L2 loss. Least squares, as it is often abbreviated, says that the loss for a given data point is the square of the difference between the target and predicted values:

$$
\mathcal {L} _ {n} (\mathbf {w}) = \left(y - \mathbf {w} ^ {\top} \mathbf {x} _ {n}\right) ^ {2} \tag {2.4}
$$

$\star$ The notation ${ \mathcal { L } } _ { n } ( \mathbf { w } )$ is used to indicate the loss incurred by a model w for a single data point $\left( \mathbf { x } _ { n } , \mathbf { y } \right)$ . $\mathcal { L } ( \mathbf { w } )$ indicates the loss incurred for an entire data set by the model w. Be aware that this notation is sometimes inconsistent between different sources.

There is a satisfying statistical interpretation for using this loss function which we will explain later in this chapter, but for now it will suffice to discuss some of the properties of this loss function that make it desirable.

First, notice that it will always take on positive values. This is convenient because we can focus exclusively on minimizing our loss, and it also allows us to combine the loss incurred from different data points without worrying about them cancelling out.

A more subtle but enormously important property of this loss function is that we know a lot about how to efficiently optimize quadratic functions. This is not a textbook about optimization, but some quick and dirty intuition that we will take advantage of throughout this book is that we can easily and reliably take the derivative of quadratic functions because they are continuously differentiable. We also know that optima of a quadrative function will be located at points where the derivative of the function is equal to 0, as seen in Figure 2.3. In contrast, L1 loss is not continuously differentiable over the entirety of its domain.

![](images/e941124ef8d6e63ebba1c9e0a7e35cc3aa47cfe0d8d546c2f351ec2e81a64eee.jpg)  
Figure 2.3: Quadratic function with clear optimum at $x = 2$ , where the derivative of the function is 0.

# 2.6 Linear Regression Algorithms

Now that we have our least squares loss function, we can finally begin to fit a line to our data. We will walk through the derivation of how this is done, in the process revealing the algorithm used for linear regression.

# 2.6.1 Optimal Weights via Matrix Differentiation

Let $\mathbf { X }$ denote a $N \times D$ dimension matrix (the “design matrix”) where the row $n$ is $\mathbf { x } _ { n } ^ { \mid }$ , i.e., the features corresponding to the $n$ th example. Let $\mathbf { Y }$ denote a $N \times 1$ vector corresponding to the target values, where the $n$ th entry corresponds to $y _ { n }$ .

First, we can define the loss incurred by parameters w over our entire data set $\mathbf { X }$ as follows:

$$
\mathcal {L} (\mathbf {w}) = \frac {1}{2} \sum_ {n = 1} ^ {N} \left(y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}\right) ^ {2} \tag {2.5}
$$

$\star$ Note that we added a constant $\textstyle { \frac { 1 } { 2 } }$ to the beginning of our loss expression. This scales the loss, which will not change our final result for the optimal parameters. It has the benefit of making our calculations cleaner once we’ve taken the gradient of the loss.

We now want to solve for the values of w that minimize this expression.

Derivation 2.6.1 (Least Squares Optimal Weights Derivation): We find the optimal weights w∗ as follows:

Start by taking the gradient of the loss with respect to our parameter w:

$$
\nabla \mathcal {L} (\mathbf {w}) = \sum_ {n = 1} ^ {N} (y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}) (- \mathbf {x} _ {n})
$$

Setting this gradient to 0, and multiplying both sides by -1, we have:

$$
\sum_ {n = 1} ^ {N} y _ {n} \mathbf {x} _ {n} - \sum_ {n = 1} ^ {N} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n}\right) \mathbf {x} _ {n} = 0. \tag {2.6}
$$

Since $( \mathbf { w } ^ { \top } \mathbf { x } _ { n } ) \mathbf { x } _ { n } = ( \mathbf { x } _ { n } ^ { \top } \mathbf { w } ) \mathbf { x } _ { n } = \mathbf { x } _ { n } ( \mathbf { x } _ { n } ^ { \top } \mathbf { w } )$ , recognizing that for scalar $a$ and vector $\mathbf { v }$ we have $a ^ { \mid } = a$ and $a \mathbf { v } = \mathbf { v } a$ . Substituting, we want to solve for w such that

$$
\sum_ {n = 1} ^ {N} y _ {n} \mathbf {x} _ {n} - \sum_ {n = 1} ^ {N} \mathbf {x} _ {n} (\mathbf {x} _ {n} ^ {\top} \mathbf {w}) = 0.
$$

At this point, it is convenient to rewrite these summations as matrix operations, making use of design matrix X $( N \times D )$ and target values y $( N \times 1 )$ . We have

$$
\mathbf {X} ^ {\top} \mathbf {y} = \sum_ {n = 1} ^ {N} y _ {n} \mathbf {x} _ {n}, \quad \mathbf {X} ^ {\top} \mathbf {X} \mathbf {w} = \sum_ {n = 1} ^ {N} \mathbf {x} _ {n} \left(\mathbf {x} _ {n} ^ {\top} \mathbf {w}\right)
$$

Substituting, we have

$$
\mathbf {X} ^ {\top} \mathbf {y} - \mathbf {X} ^ {\top} \mathbf {X} \mathbf {w} = 0.
$$

Solving for the optimal weight vector, we have

$$
\mathbf {w} ^ {*} = \left(\mathbf {X} ^ {\top} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\top} \mathbf {y}. \tag {2.7}
$$

For this to be well defined we need $\mathbf { X }$ to have full column rank (features are not colinear) so that $\mathbf { X } ^ { \mid } \mathbf { X }$ is positive definite and the inverse exists.

The quantity $( \mathbf { X } ^ { \top } \mathbf { X } ) ^ { - 1 } \mathbf { X } ^ { \top }$ in Derivation 2.6.1 has a special name: the Moore-Penrose pseudo inverse. You can think of it as a generalization of a matrix inversion operation to a non-square matrix.

# 2.6.2 Bayesian Solution: Maximum Likelihood Estimation

We’ve thus far been discussing linear regression exclusively in terms of a loss function that helps us fit a set of weights to our data. In particular, we have been working with least squares, which has nice properties that make it a reasonable loss function.

In a very satisfying fashion, least squares also has a statistical foundation. In fact, you can recover the least squares loss function purely from a statistical derivation that we present here.

Consider our data set $\mathbf { D } = \{ ( \mathbf { x } _ { n } , y _ { n } ) \} _ { n = 1 } ^ { N }$ , $\mathbf { x } _ { n } \in \mathbb { R } ^ { m }$ , $y \in \mathbb R$ . Let’s imagine that our data was generated according to the following process:

$$
y _ {n} \sim \mathcal {N} (\mathbf {w} ^ {\top} \mathbf {x} _ {n}, \beta^ {- 1})
$$

Which can be written equivalently as:

$$
p \left(y _ {n} \mid \mathbf {x} _ {n}, \mathbf {w}, \beta\right) = \mathcal {N} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n}, \beta^ {- 1}\right) \tag {2.8}
$$

The interpretation of this is that our target value $y$ is generated according to a linear combination

of our inputs $\mathbf { x }$ , but there is also some noise in the data generating process described by the variance parameter $\beta ^ { - 1 }$ . It’s an acknowledgement that some noise exists naturally in our data.

$\star$ It’s common to write variance as an inverse term, such as $\beta ^ { - 1 }$ . The parameter $\beta$ is then known as the precision, which is sometimes easier to work with than the variance.

As before, we now ask the question: how do we solve for the optimal weights w? One approach we can take is to maximize the probability of observing our target data y. This technique is known as maximum likelihood estimation.

# Derivation 2.6.2 (Maximum Likelihood Estimation for Bayesian Linear Regression):

The likelihood of our data set is given by:

$$
p (\mathbf {y} | \mathbf {X}, \mathbf {w}, \beta) = \prod_ {n = 1} ^ {N} \mathcal {N} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n}, \beta^ {- 1}\right)
$$

We then take the logarithm of the likelihood, and since the logarithm is a strictly increasing, continuous function, this will not change our optimal weights w:

$$
\ln p (\mathbf {y} | \mathbf {X}, \mathbf {w}, \beta) = \sum_ {n = 1} ^ {N} \ln \mathcal {N} (\mathbf {w} ^ {\top} \mathbf {x} _ {n}, \beta^ {- 1})
$$

Using the density function of a univariate Gaussian:

$$
\begin{array}{l} \ln p (\mathbf {y} | \mathbf {X}, \mathbf {w}, \beta) = \sum_ {n = 1} ^ {N} \ln \frac {1}{\sqrt {2 \pi \beta^ {- 1}}} e ^ {- (y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}) ^ {2} / 2 \beta^ {- 1}} \\ = \frac {N}{2} \ln \beta - \frac {N}{2} \ln (2 \pi) - \frac {\beta}{2} \sum_ {n = 1} ^ {N} (y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}) ^ {2} \\ \end{array}
$$

Notice that this is a quadratic function in w, which means that we can solve for it by taking the derivative with respect to w, setting that expression to 0, and solving for w:

$$
\begin{array}{l} \frac {\partial \ln p (\mathbf {y} | \mathbf {X} , \mathbf {w} , \beta)}{\partial \mathbf {w}} = - \beta \sum_ {n = 1} ^ {N} (y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}) (- \mathbf {x} _ {n}) \\ \Leftrightarrow \sum_ {n = 1} ^ {N} y _ {n} \mathbf {x} _ {n} - \sum_ {n = 1} ^ {N} (\mathbf {w} ^ {\top} \mathbf {x} _ {n}) \mathbf {x} _ {n} = 0. \\ \end{array}
$$

Notice that this is exactly the same form as Equation 2.6. Solving for w as before, we have:

$$
\mathbf {w} ^ {*} = \left(\mathbf {X} ^ {\top} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\top} \mathbf {y} \tag {2.9}
$$

Notice that our final solution is exactly the same form as the solution in Equation 2.7, which we solved for by minimizing the least squares loss! The takeaway here is that minimizing a least squares loss function is equivalent to maximizing the probability under the assumption of a linear model with Gaussian noise.

# 2.6.3 Alternate Interpretation: Linear Regression as Projection

Another common interpretation of linear regression is that of a projection of our targets, y, onto the column space of our inputs $\mathbf { X }$ . This can be useful for building intuition.

We showed above that the quantity $( \mathbf { X } ^ { \top } \mathbf { X } ) ^ { - 1 } \mathbf { X } ^ { \top }$ can be thought of as the pseudoinverse for our inputs $\mathbf { X }$ . Let’s now consider the case where $\mathbf { X }$ is square and the pseudoinverse is equal to the true inverse: $\mathbf { X } ^ { - 1 } = ( \mathbf { X } ^ { \mid } \mathbf { X } ) ^ { - 1 } \mathbf { X } ^ { \mid }$ . We have for our optimal $\mathbf { w } ^ { * }$ :

$$
\mathbf {w} ^ {*} = \left(\mathbf {X} ^ {\top} \mathbf {X}\right) ^ {- 1} \mathbf {X} ^ {\top} \mathbf {y}
$$

which simplifies as

$$
\mathbf {w} ^ {*} = \mathbf {X} ^ {- 1} \mathbf {y}
$$

We can recover our target values y by multiplying either side by $\mathbf { X }$ :

$$
\mathbf {X} \mathbf {w} ^ {*} = \mathbf {X} \mathbf {X} ^ {- 1} \mathbf {y}
$$

$$
\mathbf {X w} ^ {*} = \mathbf {y}
$$

We were able to recover our targets y exactly because $\mathbf { X }$ is an invertible tranformation. However, in the general case where $\mathbf { X }$ is not invertible and we have to use the approximate pseudoinverse $( \mathbf { X } ^ { \top } \mathbf { X } ) ^ { - 1 } \mathbf { X } ^ { \top }$ , we instead recover $\hat { \mathbf { y } }$ :

$$
\mathbf {X} \mathbf {w} ^ {*} = \mathbf {X} (\mathbf {X} ^ {\top} \mathbf {X}) ^ {- 1} \mathbf {X} ^ {\top} \mathbf {y}
$$

$$
\mathbf {X} \mathbf {w} ^ {*} = \hat {\mathbf {y}}
$$

where $\hat { \mathbf { y } }$ can be thought of as the closest projection of y onto the column space of $\mathbf { X }$ .

Furthermore, this motivates the intuition that $\mathbf { w } ^ { * }$ is the set of coefficients that best transforms our input space $\mathbf { X }$ into our target values y.

# 2.7 Model Flexibility

Occasionally, linear regression will fail to recover a good solution for a data set. While this may be because our data doesn’t actually have predictive power, it might also just indicate that our data is provided in a format unsuitable for linear regression. This section explores that problem, in particular focusing on how we can manipulate the flexibility of our models to make them perform better.

# 2.7.1 Basis Functions

There are some situations where linear regression is not the best choice of model for our input data $\mathbf { X }$ . Because linear regression only scales and combines input variables, it is unable to apply more complex transformations to our data such as a sin or square root function. In those situations where we need to transform our input variable somehow prior to performing linear regression (which is known as moving our data into a new basis), we can apply a basis function.

Definition 2.7.1 (Basis Function): Typically denoted by the symbol $\phi ( \cdot )$ , a basis function is a transformation applied to an input data point $\mathbf { x }$ to move our data into a different input basis, which is another phrase for input domain.

![](images/5114c075a7ff9380eff90259ae8436bc031a979c24e1b42aeabfa835fa6db9e2.jpg)  
Figure 2.4: Data with no basis function applied.

For example, consider our original data point:

$$
\mathbf {x} = \left(x ^ {(1)}, x ^ {(2)}\right) ^ {\prime}
$$

We may choose our basis function $\phi ( \mathbf { x } )$ such that our transformed data point in its new basis is:

$$
\phi (\mathbf {x}) = (x ^ {(1)}, x ^ {(1) ^ {2}}, x ^ {(2)}, \sin (x ^ {(2)})) ^ {\prime}
$$

Using a basis function is so common that we will sometimes describe our input data points as $\phi = ( \phi ^ { ( 1 ) } , \phi ^ { ( 2 ) } , . . . , \phi ^ { ( D ) } ) ^ { \prime }$ .

$\star$ The notation $\mathbf { x } = ( x ^ { ( 1 ) } , x ^ { ( 2 ) } ) ^ { \prime }$ is a way to describe the dimensions of a single data point $\mathbf { x }$ . The term $\mathbf { x } ^ { ( 1 ) }$ is the first dimension of a data point $\mathbf { x }$ , while $\mathbf { x } _ { 1 }$ is the first data point in a data set.

Basis functions are very general - they could specify that we just keep our input data the same. As a result, it’s common to rewrite the least squares loss function from Equation 2.4 for linear regression in terms of the basis function applied to our input data:

$$
\mathcal {L} (\mathbf {w}) = \frac {1}{2} \sum_ {n = 1} ^ {N} \left(y _ {n} - \mathbf {w} ^ {\top} \phi_ {n}\right) ^ {2} \tag {2.10}
$$

To motivate why we might need basis functions for performing linear regression, let’s consider this graph of 1-dimensional inputs $\mathbf { X }$ along with their target outputs y, presented in Figure 2.4.

As we can see, we’re not going to be able to fit a good line to this data. The best we can hope to do is something like that of Figure 2.5.

However, if we just apply a simple basis function to our data, in this case the square root function, $\phi ( \mathbf { x } ) = ( \sqrt { x _ { 1 } } ) ^ { \prime }$ , we then have the red line in Figure 2.6. We now see that we can fit a very good line to our data, thanks to basis functions.

![](images/7f7711fcc643986e520c96997782a0bc3c18f2ecf5e06e99f40eb4e41b158a51.jpg)  
Figure 2.5: Data with no basis function applied, attempt to fit a line.

![](images/46a5309ee5f052cdcf39d082250d40e2a61ab7cc229118c2f08521d2f11f15e8.jpg)  
Figure 2.6: Data with square root basis function applied.

![](images/168020757293bde13af14d3786eb6bda4dd0f7b62b5b281f7bb3234301a53ca2.jpg)  
Figure 2.7: Data set with a clear trend and Gaussian noise.

Still, the logical question remains: how can I choose the appropriate basis function? This toy example had a very obviously good basis function, but in general with high-dimensional, messy input data, how do we choose the basis function we need?

The answer is that this is not an easy problem to solve. Often, you may have some domain specific knowledge that tells you to try a certain basis, such as if you’re working with chemical data and know that an important equation involves a certain function of one of your inputs. However, more often than not we won’t have this expert knowledge either. Later, in the chapter on neural networks, we will discuss methods for discovering the best basis functions for our data automatically.

# 2.7.2 Regularization

When we introduced the idea of basis functions above, you might have wondered why we didn’t just try adding many basis transformations to our input data to find a good transformation. For example, we might use this large basis function on a $D$ -dimensional data point z:

$$
\phi (\mathbf {z}) = (z ^ {(1)}, z ^ {(1) ^ {2}}, \dots , z ^ {(1) ^ {1 0 0}}, z ^ {(2)}, z ^ {(2) ^ {2}}, \dots , z ^ {(2) ^ {1 0 0}}, \dots , z ^ {(D)}, z ^ {(D) ^ {2}}, \dots , z ^ {(D) ^ {1 0 0}}) ^ {\prime}
$$

where you can see that we expand the dimensions of the data point to be 100 times its original size.

Let’s say we have an input data point x that is 1-dimensional, and we apply the basis function described above, so that after the transformation each data point is represented by 100 values. Say we have 100 data points on which to perform linear regression, and because our transformed input space has 100 values, we have 100 parameters to fit. In this case, with one parameter per data point, it’s possible for us to fit our regression line perfectly to our data so that we have no loss! But is this a desirable outcome? The answer is no, and we’ll provide a visual example to illustrate that.

Imagine Figure 2.7 is our data set. There is a very clear trend in this data, and you would likely draw a line that looks something like that of Figure 2.8 to fit it.

However, imagine we performed a large basis transformation like the one described above. If we do that, it’s possible for us to fit our line perfectly, threading every data point, like that in Figure 2.9.

![](images/3ef9a36c9681cc6247fb86c1547bc6af3b7322d76c7211d4731078a5c2114c06.jpg)  
Figure 2.8: Natural fit for this data set.

![](images/18794091b8a08ac44b66bf4571500ff5951fc4f0d506dcdc7f048df8b27aa416.jpg)  
Figure 2.9: Unnatural fit for this data set.

Let’s see how both of these would perform on new data points. With our first regression line, if we have a new data point $\mathbf { x } = ( 1 0 ) ^ { \prime }$ , we would predict a target value of 14.1, which most people would agree is a pretty good measurement. However, with the second regression line, we would predict a value of 9.5, which most people would agree does not describe the general trend in the data. So how can we handle this problem elegantly?

Examining our loss function, we see that right now we’re only penalizing predictions that are not correct in training. However, what we ultimately care about is doing well on new data points, not just our training set. This leads us to the idea of generalization.

Definition 2.7.2 (Generalization): Generalization is the ability of a model to perform well on new data points outside of the training set.

A convoluted line that matches the noise of our training set exactly isn’t going to generalize well to new data points that don’t look exactly like those found in our training set. If wish to avoid recovering a convoluted line as our solution, we should also penalize the total size of our weights w. The effect of this is to discourage many complex weight values that produce a messy regression line. By penalizing large weights, we favor simple regression lines like the one in Figure 2.8 that take advantage of only the most important basis functions.

The concept that we are introducing, penalizing large weights, is an example of what’s known as regularization, and it’s one that we will see come up often in different machine learning methods.

Definition 2.7.3 (Regularization): Applying penalties to parameters of a model.

There is obviously a tradeoff between how aggressively we regularize our weights and how tightly our solution fits to our data, and we will formalize this tradeoff in the next section. However, for now, we will simply introduce a regularization parameter $\lambda$ to our least squares loss function:

$$
\mathcal {L} (\mathbf {w}) = \frac {1}{2} \sum_ {n = 1} ^ {N} \left(y _ {n} - \mathbf {w} ^ {\top} \phi_ {n}\right) ^ {2} + \frac {\lambda}{2} \mathbf {w} ^ {\top} \mathbf {w} \tag {2.11}
$$

The effect of $\lambda$ is to penalize large weight parameters. The larger $\lambda$ is, the more we will favor simple solutions. In the limit $\scriptstyle \operatorname* { l i m } _ { \lambda \to \infty } { \mathcal { L } } ( \mathbf { w } )$ , we will drive all weights to 0, while with a nonexistant $\lambda = 0$ we will apply no regularization at all. Notice that we’re squaring our weight parameters - this is known as $L \mathcal { Q }$ norm regularization or ridge regression. While L2 norm regularization is very common, it is just one example of many ways we can perform regularization.

To build some intuition about the effect of this regularization parameter, examine Figure 2.10. Notice how larger values of $\lambda$ produce less complex lines, which is the result of applying more regularization. This is very nice for the problem we started with - needing a way to choose which basis functions we wanted to use. With regularization, we can select many basis functions, and then allow regularization to ‘prune’ the ones that aren’t meaningful (by driving their weight parameters to 0). While this doesn’t mean that we should use as many basis transformations as possible (there will be computational overhead for doing this), it does allow us to create a much more flexible linear regression model without creating a convoluted regression line.

![](images/e80da0d94269e0167f35b297edca3a37ab97a81d0fea3182f6d53cf69291e568.jpg)  
Figure 2.10: Effect of different regularization parameter values on final regression solution.

# 2.7.3 Generalizing Regularization

We’ve thus far only discussed one form of regularization: ridge regression. Remember that under ridge regression, the loss function takes the form:

$$
\mathcal {L} (\mathbf {w}) = \frac {1}{2} \sum_ {n = 1} ^ {N} (y _ {n} - \mathbf {w} ^ {\top} \boldsymbol {\phi} _ {n}) ^ {2} + \frac {\lambda}{2} \mathbf {w} ^ {\top} \mathbf {w},
$$

where the $( \lambda / 2 ) \mathbf { w } ^ { \top } \mathbf { w }$ term is for the regularization. We can generalize our type of regularization by writing it as:

$$
\mathcal {L} (\mathbf {w}) = \frac {1}{2} \sum_ {n = 1} ^ {N} (y _ {n} - \mathbf {w} ^ {\top} \boldsymbol {\phi} _ {n}) ^ {2} + \frac {\lambda}{2} | | \mathbf {w} | | _ {h} ^ {h}
$$

where $h$ determines the type of regularization we are using and thus the form of the optimal solution that we recover. For example, if $h = 2$ then we add $\lambda / 2$ times the square of the L2 norm. The three most commonly used forms of regularization are lasso, ridge, and elastic net.

# Ridge Regression

This is the case of $h = 2$ , which we’ve already discussed, but what type of solutions does it tend to recover? Ridge regression prevents any individual weight from growing too large, providing us with solutions that are generally moderate.

# Lasso Regression

Lasso regression is the case of $h = 1$ . Unlike ridge regression, lasso regression will drive some parameters $w _ { i }$ to zero if they aren’t informative for our final solution. Thus, lasso regression is good if you wish to recover a sparse solution that will allow you to throw out some of your basis functions. You can see the forms of ridge and lasso regression functions in Figure 2.11. If you think about how Lasso is L1 Norm (absolute value) and Ridge is L2 Norm (squared distance), you can think of those shapes as being the set of points $( w _ { 1 } , w _ { 2 } )$ for which the norm takes on a constant

![](images/0d14d8df5f1cfcb7508532bbb72bb2d9c096c54682fc69ccbcdf14f4d76eab8e.jpg)  
Figure 2.11: Form of the ridge (blue) and lasso (red) regression functions.

value.

# Elastic Net

Elastic net is a middle ground between ridge and lasso regression, which it achieves by using a linear combination of the previous two regularization terms. Depending on how heavily each regularization term is weighted, this can produce results on a spectrum between lasso and ridge regression.

# 2.7.4 Bayesian Regularization

We’ve seen regularization in the context of loss functions, where the goal is to penalize large weight values. How does the concept of regularization apply to Bayesian linear regression?

The answer is that we can interpret regularizing our weight parameters as adding a prior distribution over w. Note that this is a different conception of regularization than we saw in the previous section. In the Bayesian framework, we are averaging over different models specified by different values of w. Therefore, in this context regularization entails weighting models with smaller values of w more heavily.

Derivation 2.7.1 (Bayesian Regularization Derivation): Because we wish to shrink our weight values toward 0 (which is exactly what regularization does), we will select a Normal prior with mean 0 and variance ${ \bf S } _ { 0 } ^ { - 1 }$ :

$$
\mathbf {w} \sim \mathcal {N} (0, \mathbf {S} _ {0} ^ {- 1})
$$

Remember from Equation 2.8 that the distribution over our observed data is Normal as well, written here in terms of our entire data set:

$$
p (\mathbf {y} | \mathbf {X}, \mathbf {w}, \beta) = \mathcal {N} (\mathbf {X w}, \beta^ {- 1} \mathbf {I})
$$

We want to combine the likelihood and the prior to recover the posterior distribution of $\mathbf { w }$ , which follows directly from Bayes’ Theorem:

$$
\underbrace {p (\mathbf {w} | \mathbf {X} , \mathbf {y} , \beta)} _ {\text {p o s t e r i o r}} \propto \underbrace {p (\mathbf {y} | \mathbf {X} , \mathbf {w} , \beta)} _ {\text {l i k e l i h o o d}} \underbrace {p (\mathbf {w})} _ {\text {p r i o r}}
$$

We now wish to find the value of w that maximizes the posterior distribution. We can maximize the log of the posterior with respect to w, which simplifies the problem slightly:

$$
\ln p (\mathbf {w} | \mathbf {X}, \mathbf {y}, \beta) = \ln p (\mathbf {y} | \mathbf {X}, \mathbf {w}, \beta) + \ln p (\mathbf {w}) + C _ {1}
$$

where $C _ { 1 }$ is some constant free of w. Let’s handle $\ln p ( \mathbf { y } | \mathbf { X } , \mathbf { w } , \beta )$ first:

$$
\begin{array}{l} \ln p (\mathbf {y} | \mathbf {X}, \mathbf {w}, \beta) = \ln \prod_ {n = 1} ^ {N} \mathcal {N} (y _ {n} | \mathbf {w} ^ {\top} \mathbf {x} _ {n}, \beta^ {- 1}) \\ = \ln \prod_ {n = 1} ^ {N} \frac {1}{\sqrt {2 \pi \beta^ {- 1}}} \exp \left\{- \frac {\beta}{2} (y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}) ^ {2} \right\} \\ = C _ {2} - \frac {\beta}{2} \sum_ {n = 1} ^ {N} (y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}) ^ {2} + \frac {1}{\sqrt {2 \pi \beta^ {- 1}}} \\ \end{array}
$$

where $C _ { 2 }$ collects more constant terms that don’t depend on w. Let’s now handle $\ln p ( \mathbf { w } )$

$$
\begin{array}{l} \ln p (\mathbf {w}) = \ln \mathcal {N} (0, \mathbf {S} _ {0} ^ {- 1}) \\ = \ln \frac {1}{\left(\left| 2 \pi \mathbf {S} _ {0} ^ {- 1} \right|\right) ^ {\frac {1}{2}}} \exp \left\{- \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {S} _ {0} \mathbf {w} \right\} \\ = C _ {3} - \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {S} _ {0} \mathbf {w} \\ \end{array}
$$

combining the terms for $\ln p ( \mathbf { y } | \mathbf { X } , \mathbf { w } , \beta )$ and $\ln p ( \mathbf { w } )$ :

$$
\begin{array}{l} \ln p (\mathbf {w} | \mathbf {X}, \mathbf {y}, \beta) = - \frac {\beta}{2} \sum_ {n = 1} ^ {N} (y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}) ^ {2} - \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {S} _ {0} \mathbf {w} + C \\ = - \frac {1}{\beta} \left(\frac {1}{2} \sum_ {n = 1} ^ {N} \left(y _ {n} - \mathbf {w} ^ {\top} \mathbf {x} _ {n}\right) ^ {2} + \frac {1}{2 \beta} \mathbf {w} ^ {\top} \mathbf {S} _ {0} \mathbf {w}\right) + C \\ \end{array}
$$

Since $\beta > 0$ , notice that maximizing the posterior probability with respect to w is equivalent to minimizing the sum of squared errors $( y _ { n } - \mathbf { w } ^ { \top } \mathbf { x } _ { n } ) ^ { 2 }$ and the regularization term $\begin{array} { r } { \frac { 1 } { \beta } \mathbf { w } ^ { \top } \mathbf { S } _ { 0 } \mathbf { w } } \end{array}$ .

The interpretation of this in the case ${ \bf S } _ { 0 } = s { \bf I }$ is that adding a prior over the distribution of our weight parameters w and then maximizing the resulting posterior distribution is equivalent to ridge regression with $\begin{array} { r } { \lambda = \frac { s } { \beta } } \end{array}$ , as the regularization term simplifies to $\begin{array} { r } { \frac { s } { \beta } \mathbf { w } ^ { \top } \mathbf { S } _ { 0 } \mathbf { w } } \end{array}$ . Recall that both $s$ and $\beta$ are precisions. This states that if the precision on the observations is small, then the regularization term is relatively large, meaning our posterior leans more towards our prior. If the precision on our prior is small, then the regularization is small, meaning our posterior leans more towards the data.

# 2.8 Choosing Between Models

# 2.8.1 Bias-Variance Tradeoff and Decomposition

Now that you know about regularization, you might have some intuition for why we need to find a balance between complex and simple regression solutions. A complex solution, while it might fit all of our training data, may not generalize well to future data points. On the other hand, a line that is too simple might not vary enough to provide good predictions at all. This phenomenon is not unique to linear regression- it’s actually a very fundamental concept in machine learning that’s known as the bias-variance tradeoff.

Definition 2.8.1 (Bias-Variance Tradeoff): When constructing machine learning models, we have a choice somewhere on a spectrum between two extremes: fitting exactly to our training data or not varying in response to our training data at all. The first extreme, fitting all of our training data, is a situation of high variance, because our output changes heavily in reponse to our input data (see the red line in Figure 2.10). At the other extreme, a solution that doesn’t change in response to our training data at all is a situation of high bias (see the yellow line in Figure 2.10). This means our model heavily favors a specific form regardless of the training data, so our target outputs don’t fluctuate between distinct training sets.

Obviously a good solution will fall somewhere in between these two extremes of high variance and high bias. Indeed, we have techniques like regularization to help us balance the two extremes (improving generalization), and we have other techniques like cross-validation that help us determine when we have found a good balance (measuring generalization).

$\star$ In case you are not familiar with the terms bias and variance, we provide their statistical definitions here:

$$
\operatorname {b i a s} (\theta) = \operatorname {E} [ \theta ] - \theta
$$

$$
\operatorname {v a r i a n c e} (\theta) = \operatorname {E} \left[ \left(\theta - \operatorname {E} [ \theta ]\right) ^ {2} \right]
$$

Before we discuss how to effectively mediate between these opposing forces of error in our models, we will first show that the bias-variance tradeoff is not only conceptual but also has probabilistic underpinnings. Specifically, any loss that we incur over our training set using a given model can be described in terms of bias and variance, as we will demonstrate now.

Derivation 2.8.1 (Bias-Variance Decomposition): Let’s begin by asserting that we have a model $f ( \cdot )$ that makes a prediction of our target $y$ given input data point $\mathbf { x }$ . We wish to break down the squared error of $f$ into terms involving bias and variance.

Start with the expected squared error (MSE), where the expectation is taken with respect to both our data set $\mathbf { D }$ (variation in our modeling error comes from what data set we get), which is a random variable of $( \mathbf { x } , y )$ pairs sample from a distribution $F ^ { \prime }$ , and our conditional distribution $y | \mathbf x$ (there may be additional error because the data are noisy):

$$
M S E = \operatorname {E} _ {\mathbf {D}, y | x} [ (y - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ]
$$

where we use the notation $f _ { \mathbf { D } }$ to explicitly acknowledge the dependence of our fitted model $f$ on the dataset $\mathbf { D }$ . For reasons that will become clear in a few steps, add and subtract our target mean

$y$ , which is the true conditional mean given by $\bar { y } = \mathrm { E } _ { y | \mathbf { x } } [ y ]$ , inside of the squared term:

$$
M S E = \operatorname {E} _ {\mathbf {D}, y | x} [ (y - \bar {y} + \bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ]
$$

Group together the first two terms and the last two terms:

$$
M S E = \operatorname {E} _ {\mathbf {D}, y | x} [ ((y - \bar {y}) + (\bar {y} - f _ {\mathbf {D}} (\mathbf {x}))) ^ {2} ]
$$

Expanding this expression and using linearity of expectation:

$$
M S E = \operatorname {E} _ {\mathbf {D}, y \mid x} [ (y - \bar {y}) ^ {2} ] + \operatorname {E} _ {\mathbf {D}, y \mid x} [ (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] + 2 \operatorname {E} _ {\mathbf {D}, y \mid x} [ (y - \bar {y}) (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ] \tag {2.12}
$$

Let’s examine the last term, $2 \mathrm { E } [ ( y - { \bar { y } } ) ( { \bar { y } } - f _ { \mathbf { D } } ( \mathbf { x } ) ) ]$ . Notice that $\left( { \bar { y } } - f _ { \mathbf { D } } ( \mathbf { x } ) \right)$ does not depend on the conditional distribution $y | \mathbf x$ at all. Thus, we are able to move one of those expecations in, which makes this term:

$$
2 \mathrm {E} _ {\mathbf {D}, y | x} [ (y - \bar {y}) (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ] = 2 \mathrm {E} _ {\mathbf {D}} [ \mathrm {E} _ {y | \mathbf {x}} [ (y - \bar {y}) (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ]
$$

And note that:

$$
\operatorname {E} _ {y \mid \mathbf {x}} [ (y - \bar {y}) ] = 0
$$

Which eliminates this last term entirely:

$$
2 \mathrm {E} _ {\mathbf {D}, y | x} [ (y - \bar {y}) (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ] = 2 \mathrm {E} _ {\mathbf {D}} \left[ 0 \cdot (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) \right] = 0
$$

We can now write Equation 2.12 as:

$$
\begin{array}{l} M S E = \mathrm {E} _ {\mathbf {D}, y | x} [ (y - \bar {y}) ^ {2} ] + \mathrm {E} _ {\mathbf {D}, y | x} [ (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] \tag {2.13} \\ = \mathrm {E} _ {y | x} [ (y - \bar {y}) ^ {2} ] + \mathrm {E} _ {\mathbf {D}} [ (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] \\ \end{array}
$$

where we have removed expectations that do not apply (e.g. $y$ does not depend on the dataset $\mathbf { D }$ ). We now have two terms contributing to our squared error. We will put aside the first term $\mathrm { E } _ { y \mid x } [ ( y - { \bar { y } } ) ^ { 2 } ]$ , as this is unidentifiable noise in our data set. In other words, our data will randomly deviate from the mean in ways we cannot predict. On the other hand, we can work with the second term $\mathrm { E } _ { \bf D } [ ( \bar { y } - f _ { \bf D } ( { \bf x } ) ) ^ { 2 } ]$ as it involves our model function $f ( \cdot )$

As before, for reasons that will become clear in a few steps, let’s add and subtract our prediction mean $f ( \cdot ) = \mathrm { E } _ { \mathbf { D } } [ f _ { \mathbf { D } } ( \mathbf { x } ) ]$ , which is the expectation of our model function taken with respect to our random data set.

$$
\operatorname {E} _ {\mathbf {D}} [ (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] = \operatorname {E} _ {\mathbf {D}} [ (\bar {y} - \bar {f} (\mathbf {x}) + \bar {f} (\mathbf {x}) - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ]
$$

Expanding this squared term, we have:

$$
\operatorname {E} _ {\mathbf {D}} [ (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] = (\bar {y} - \bar {f} (\mathbf {x})) ^ {2} + \operatorname {E} _ {\mathbf {D}} [ (\bar {f} (\mathbf {x}) - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] + 2 \operatorname {E} _ {\mathbf {D}} [ (\bar {y} - \bar {f} (\mathbf {x})) (\bar {f} (\mathbf {x}) - f _ {\mathbf {D}} (\mathbf {x})) ]
$$

As before, the third term here is 0:

$$
2 \mathrm {E} _ {\mathbf {D}} [ (\bar {y} - \bar {f} (\mathbf {x})) (\bar {f} (\mathbf {x}) - f _ {\mathbf {D}} (\mathbf {x})) ] = 2 (\bar {y} - \bar {f} (\mathbf {x})) \mathrm {E} _ {\mathbf {D}} [ (\bar {f} (\mathbf {x}) - f _ {\mathbf {D}} (\mathbf {x})) ] = 2 (\bar {y} - \bar {f} (\mathbf {x})) (0) = 0
$$

Leaving us with these two terms:

$$
\operatorname {E} _ {\mathbf {D}} [ (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] = (\bar {y} - \bar {f} (\mathbf {x})) ^ {2} + \operatorname {E} _ {\mathbf {D}} [ (\bar {f} (\mathbf {x}) - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ]
$$

Notice the form of these two terms. The first one, $( \bar { y } - f ( \mathbf { x } ) ) ^ { 2 }$ , is the squared bias of our model, since it is the square of the average difference between our prediction and the true target value. The second one, $\mathrm { E } _ { \bf D } [ ( f ( { \bf x } ) - f _ { \bf D } ( { \bf x } ) ) ^ { 2 } ]$ , is the variance of our model, since it is the expected squared difference between our model and its average value. Thus:

$$
\operatorname {E} _ {\mathbf {D}} [ (\bar {y} - f _ {\mathbf {D}} (\mathbf {x})) ^ {2} ] = b i a s (f (\mathbf {x})) ^ {2} + v a r i a n c e (f (\mathbf {x}))
$$

Thus, our total squared error, plugging in to Equation 2.13 can be written as:

$$
M S E = \text {n o i s e} (\mathbf {x}) + \text {b i a s} (f (\mathbf {x})) ^ {2} + \text {v a r i a n c e} (f (\mathbf {x}))
$$

![](images/162dd1117f0dfa735541ea54e686963111a3b7fe009dd68fdf2d138a04227e03.jpg)  
Figure 2.12: Bias and variance both contribute to the overall error of our model.

The key takeaway of the bias-variance decomposition is that the controllable error in our model is given by the squared bias and variance. Holding our error constant, to decrease bias requires increasing the variance in our model, and vice-versa. In general, a graph of the source of error in our model might look something like Figure 2.12.

For a moment, consider what happens on the far left side of this graph. Our variance is very high, and our bias is very low. In effect, we’re fitting perfectly to all of the data in our data set. This is exactly why we introduced the idea of regularization from before - we’re fitting a very convoluted line that is able to pass through all of our data but which doesn’t generalize well to new data points. There is a name for this: overfitting.

Definition 2.8.2 (Overfitting): A phenomenon where we construct a convoluted model that is able to predict every point in our data set perfectly but which doesn’t generalize well to new data points.

The opposite idea, underfitting, is what happens at the far right of the graph: we have high bias and aren’t responding to the variation in our data set at all.

Definition 2.8.3 (Underfitting): A phenomenon where we construct a model that doesn’t respond to variation in our data.

So you can hopefully now see that the bias-variance tradeoff is important to managing the problem of overfitting and underfitting. Too much variance in our model and we’ll overfit to our data set. Too much bias and we won’t account for the trends in our data set at all.

In general, we would like to find a sweet spot of moderate bias and variance that produces minimal error. In the next section, we will explore how we find this sweet spot.

# 2.8.2 Cross-Validation

We’ve seen that in choosing a model, we incur error that can be described in terms of bias and variance. We’ve also seen that we can regulate the source of error through regularization, where heavier regularization increases the bias of our model. A natural question then is how do we know how much regularization to apply to achieve a good balance of bias and variance?

Another way to look at this is that we’ve traded the question of finding the optimal number of basis functions for finding the optimal value of the regularization parameter $\lambda$ , which is often an easier problem in most contexts.

One very general technique for finding the sweet spot of our regularization parameter, other hyperparameters, or even for choosing among entirely different models is known as cross-validation.

Definition 2.8.4 (Cross-Validation): A subsampling procedure used over a data set to tune hyperparameters and avoid over-fitting. Some portion of a data set (10-20% is common) is set aside, and training is performed on the remaining, larger portion of data. When training is complete, the smaller portion of data left out of training is used for testing. The larger portion of data is sometimes referred to as the training set, and the smaller portion is sometimes referred to as the validation set.

Cross-validation is often performed more than once for a given setting of hyperparameters to avoid a skewed set of validation data being selected by chance. In K-Folds cross-validation, you perform cross-validation $\mathbf { K }$ times, allocating $\scriptstyle { \frac { 1 } { \mathbf { K } } }$ of your data for the validation set at each iteration.

Let’s tie this back into finding a good regularization parameter. For a given value of $\lambda$ , we will incur a certain amount of error in our model. We can measure this error using cross-validation, where we train our model on the training set and compute the final error using the validation set. To find the optimal value for $\lambda$ , we perform cross-validation using different values of $\lambda$ , eventually settling on the value that produces the lowest final error. This will effectively trade off bias and variance, finding the value of $\lambda$ that minimizes the total error.

You might wonder why we need to perform cross-validation at all - why can’t we train on the entire data set and then compute the error over the entire data set as well?

The answer is again overfitting. If we train over the entire data set and then validate our results on the exact same data set, we are likely to choose a regularization parameter that encourages our model to conform to the exact variation in our data set instead of finding the generalizable trends. By training on one set of data, and then validating on a completely different set of data, we force our model to find good generalizations in our data set. This ultimately allows us to pick the regularization term $\lambda$ that finds the sweet spot between bias and variance, overfitting and underfitting.

# 2.8.3 Making a Model Choice

Now that we’re aware of overfitting, underfitting, and how those concepts relate to the bias-variance tradeoff, we still need to come back to the question of how we actually select a model. Intuitively, we are trying to find the middle ground between bias and variance: picking a model that fits our data but that is also general enough to perform well on yet unseen data. Furthermore, there is no such thing as the ‘right’ model choice. Instead, there are only model options that are either better or worse than others. To that end, it can be best to rely on the techniques presented above, specifically cross-validation, to make your model selection. Then, although you will not be able to make any sort of guarantee about your selection being the ‘best’ of all possible models, you can at

least have confidence your model achieved the best generalizability that could be proven through cross-validation.

# 2.8.4 Bayesian Model Averaging

We can also handle model selection using a Bayesian approach. This means we account for our uncertainty about the true model by averaging over the possible candidate models, weighting each model by our prior certainty that it is the one producing our data. If we have $M$ models indexed by $m = 1 , . . . , M$ , we can write the likelihood of observing our data set $\mathbf { X }$ as follows:

$$
p (\mathbf {X}) = \sum_ {m = 1} ^ {M} p (\mathbf {X} | m) p (m)
$$

where $p ( m )$ is our prior certainty for a given model and $p ( \mathbf { X } | m )$ is the likelihood of our data set given that model. The elegance of this approach is that we don’t have to pick any particular model, instead choosing to marginalize out our uncertainty.

# 2.9 Linear Regression Extras

With most of linear regression under our belt at this point, it’s useful to drill down on a few concepts to come to a deeper understanding of how we can use them in the context of linear regression and beyond.

# 2.9.1 Predictive Distribution

Remaining in the setting of Bayesian Linear Regression, we may wish to get a distribution over our weights w instead of a point estimator for it using maximum likelihood. As we saw in Section 2.7.4, we can introduce a prior distribution over w, then together with our observed data, we can produce a posterior distribution over w as desired.

Derivation 2.9.1 (Posterior Predictive Derivation): For the sake of simplicity and ease of use, we will select our prior over w to be a Normal distribution with mean $\pmb { \mu } _ { 0 }$ and variance $S _ { 0 } ^ { - 1 }$ :

$$
p (\mathbf {w}) = \mathcal {N} (\boldsymbol {\mu} _ {0}, \boldsymbol {S} _ {0} ^ {- 1})
$$

Remembering that the observed data is normally distributed, and accounting for Normal-Normal conjugacy, our posterior distribution will be Normal as well:

$$
p (\mathbf {w} | \mathbf {X}, \mathbf {y}, \beta) = \mathcal {N} (\boldsymbol {\mu} _ {N}, \boldsymbol {S} _ {N} ^ {- 1})
$$

where

$$
\boldsymbol {S} _ {N} = \left(\boldsymbol {S} _ {0} ^ {- 1} + \beta \mathbf {X} ^ {\top} \mathbf {X}\right) ^ {- 1}
$$

$$
\boldsymbol {\mu} _ {N} = \boldsymbol {S} _ {N} \left(\boldsymbol {S} _ {0} ^ {- 1} \boldsymbol {\mu} _ {0} + \beta \mathbf {X} \mathbf {y}\right)
$$

We now have a posterior distribution over w. However, usually this distribution is not what we care about. We’re actually interested in making a point prediction for the target $y ^ { * }$ given a new input $\mathbf { x } ^ { * }$ . How do we go from a posterior distribution over w to this prediction?

The answer is using what’s known as the posterior predictive over $y ^ { * }$ given by:

$$
\begin{array}{l} p \left(y ^ {*} | \mathbf {x} ^ {*}, \mathbf {X}, \mathbf {y}\right) = \int_ {\mathbf {w}} p \left(y ^ {*} | \mathbf {x} ^ {*}, \mathbf {w}\right) p (\mathbf {w} | \mathbf {X}, \mathbf {y}) d \mathbf {w} \tag {2.14} \\ = \int_ {\mathbf {w}} \mathcal {N} \left(y ^ {*} | \mathbf {w} ^ {\top} \mathbf {x} ^ {*}, \beta^ {- 1}\right) \mathcal {N} \left(\mathbf {w} \mid \boldsymbol {\mu} _ {N}, S _ {N} ^ {- 1}\right) d \mathbf {w} \\ \end{array}
$$

The idea here is to average the probability of $y ^ { * }$ over all the possible setting of w, weighting the probabilities by how likely each setting of w is according to its posterior distribution.

# 2.10 Conclusion

In this chapter, we looked at a specific tool for handling regression problems known as linear regression. We’ve seen linear regression described in terms of loss functions, probabilistic expressions, and geometric projections, which reflects the deep body of knowledge that we have around this very common technique.

We’ve also discussed many concepts in this chapter that will prove useful in other areas of machine learning, particularly for other supervised techniques: loss functions, regularization, bias and variance, over and underfitting, posterior distributions, maximum likelihood estimation, and cross-validation among others. Spending time to develop an understanding of these concepts now will pay off going forward.

It may or may not be obvious at this point that we are missing a technique for a very large class of problems: those where the solution is not just a continuous, real number. How do we handle situations where we need to make a choice between different discrete options? This is the question we will turn to in the next chapter.

# Chapter 3

# Classification

In the last chapter we explored ways of predicting a continuous, real-number target. In this chapter, we’re going to think about a different problem- one where our target output is discrete-valued. This type of problem, one where we make a prediction by choosing between finite class options, is known as classification.

# 3.1 Defining the Problem

As we did when studying regression, let’s begin by thinking about the type of problems we are trying to solve. Here are a few examples of classification tasks:

1. Predicting whether a given email is spam.   
2. Predicting the type of object in an image.   
3. Predicting whether a manufactured good is defective.

The point of classification is hopefully clear: we’re trying to identify the most appropriate class for an input data point.

Definition 3.1.1 (Classification): A set of problems that seeks to make predictions about unobserved target classes given observed input variables.

# 3.2 Solution Options

There are several different means by which we can solve classification problems. We’re going to discuss three in this chapter: discriminant functions, probabilistic discriminative models (e.g. logistic regression), and probabilistic generative models. Note that these are not the only methods for performing classification tasks, but they are similar enough that it makes sense to present and explore them together. Specifically, these techniques all use some linear combination of input variables to produce a class prediction. For that reason, we will refer to these techniques as generalized linear models.

# ML Framework Cube: Generalized Linear Models

Since we are using these techniques to perform classification, generalized linear models deal with a discrete output domain. Second, as with linear regression, our goal is to make predictions on future data points given an initial set of data to learn from. Thus, generalized linear models are supervised techniques. Finally, depending on the type of generalized linear model, they can be either probabilistic or non-probabilistic.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Discrete</td><td>Supervised</td><td>Yes / No</td></tr></table>

# 3.3 Discriminant Functions

Generalized linear models for classification come in several different flavors. The most straightforward method carries over very easily from linear regression: discriminant functions. As we will see, with discriminant functions we are linearly separating the input space into sections belonging to different target classes. We will explore this method first. One thing to keep in mind is that it’s generally easiest to initially learn these techniques in the case where we have only two target classes, but there is typically a generalization that allows us to handle the multi-class case as well.

As with linear regression, discriminant functions $h ( \mathbf { x } , \mathbf { w } )$ seek to find a weighted combination of our input variables to make a prediction about the target class:

$$
h (\mathbf {x}, \mathbf {w}) = w ^ {(0)} x ^ {(0)} + w ^ {(1)} x ^ {(1)} + \dots + w ^ {(D)} x ^ {(D)} \tag {3.1}
$$

where we are using the bias trick of appending $x ^ { ( 0 ) } = 1$ to all of our data points.

# 3.3.1 Basic Setup: Binary Linear Classification

The simplest use case for a discriminant function is when we only have two classes that we are trying to decide between. Let’s denote these two classes 1 and -1. Our discriminant function in Equation 3.1 will then predict class $\mathbf { 1 }$ if $h ( \mathbf { x } , \mathbf { w } ) \geq 0$ and class -1 if $h ( \mathbf { x } , \mathbf { w } ) < 0$ :

$$
\left\{ \begin{array}{l l} 1 & \text {i f} h (\mathbf {x}, \mathbf {w}) \geq 0 \\ - 1 & \text {i f} h (\mathbf {x}, \mathbf {w}) <   0 \end{array} \right.
$$

Geometrically, the linear separation between these two classes then looks like that of Figure 3.1. Notice the line where our prediction switches from class 1 to class -1. This is precisely where $h ( \mathbf { x } , \mathbf { w } ) = 0$ , and it is known as the decision boundary.

Definition 3.3.1 (Decision Boundary): The decision boundary is the line that divides the input space into different target classes. It is learned from an initial data set, and then the target class of new data points can be predicted based on where they fall relative to the decision boundary. At the decision boundary, the discriminant function takes on a value of 0.

![](images/3c18bfabd3b064e06c8b6e53e02850a15fe315b22e4f2ebda6de1922a1dc463b.jpg)  
Figure 3.1: Clear separation between classes.

# 3.3.2 Multiple Classes

Now consider the case that we have $K > 2$ classes $C _ { 1 } , C _ { 2 } , . . . , C _ { K }$ to choose between. One obvious approach we might try is to use $K$ different discriminant functions that each determine whether or not a given input is in that class $C _ { k }$ . This is known as a one-versus-all approach, and it doesn’t work properly because we end up with ambiguous regions as demonstrated in Figure 3.2. Intuitively, several of the discriminator functions could claim that a data point is a part of their class, which is an undesirable result.

Another obvious approach we might employ is to use $\binom { K } { 2 }$ discriminant functions that each determine whether a given point is more likely to be in class $C _ { j }$ or class $C _ { k }$ . This is known as a one-versus-one approach, and it also doesn’t work because we again end up with ambiguous regions as demonstrated in Figure 3.3.

Instead, we can avoid these ambiguities in the multi-class case by using $K$ different linear classifiers $h _ { k } ( \mathbf { x } , \mathbf { w } _ { k } )$ , and then assigning new data points to the class $C _ { k }$ for which $h _ { k } ( \mathbf { x } , \mathbf { w } _ { k } ) >$ $h _ { j } ( \mathbf { x } , \mathbf { w } _ { j } )$ for all $j \neq k$ . Then, similar to the two-class case, the decision boundaries are described by the surface along which $h _ { k } ( \mathbf { x } , \mathbf { w } _ { k } ) = h _ { j } ( \mathbf { x } , \mathbf { w } _ { j } )$ .

Now that we’ve explored the multi-class generalization, we can consider how to learn the weights w that define the optimal discriminant functions. However, prior to solving for w, we need to discuss how basis transformations apply to classification problems.

# 3.3.3 Basis Changes in Classification

We initially discussed basis changes in the context of linear regression, and they are equally important for classification tasks. For example, consider the data set in Figure 3.4.

It’s obviously not possible for us to use a linear classifier to separate this data set. However, if we apply a basis change by squaring one of the data points, we instead have Figure 3.5, which is

![](images/56d4ba92b7b58e8136c0530dd4654a624e938d49a2577e2311589cc3e7150b8c.jpg)  
Figure 3.2: Ambiguities arise from one-versus-all method.

![](images/f0a2f2555969d505ef271de5206ae9add68d33220ab3ce8999fe837e39c45edd.jpg)  
Figure 3.3: Ambiguities arise from one-versus-one method.

![](images/8ec55ce8024aee35ff417a9692e06ebe72f9b845b302d525117e24aca0fb9cbd.jpg)  
Figure 3.4: Data set without any basis functions applied, not linearly separable.

now linearly separable by a plane between the two classes. Applying a generic basis change $\phi ( \cdot )$ , we can write our generalized linear model as:

$$
h _ {k} (\mathbf {x}, \mathbf {w} _ {k}) = \mathbf {w} _ {k} ^ {\top} \phi (\mathbf {x}) = \mathbf {w} _ {k} ^ {\top} \phi \tag {3.2}
$$

For the sake of simplicity in the rest of this chapter, we will leave out any basis changes in our derivations, but you should recognize that they could be applied to any of our input data to make the problems more tractable.

$\star$ For an input matrix X, there is a matrix generalization of our basis transformed inputs: $\Phi = \phi ( \mathbf { X } )$ , where $\Phi$ i s known as the design matrix.

# 3.4 Numerical Parameter Optimization and Gradient Descent

Recall from the previous chapter on linear regression that when it comes to optimizing our model’s weight parameters w, the goal is to minimize our loss function $\mathcal L ( \mathbf w )$ (also called the objective function). We did this by taking the derivative of our objective function with respect to $\mathbf { w }$ , setting that expression equal to 0, and solving for w. We were previously able to perform that procedure with confidence because the least squares loss function was convex with respect to the weight parameters, which meant it had a global solution we could solve for directly. Thus, the point of minimization for the objective function would occur where $\nabla \mathcal { L } ( \mathbf { w } ) = 0$ .

Unfortunately, it’s not always the case that our objective function will be convex with respect to our weight parameters. In fact, in the next section, we will consider an objective function that is not convex, and as a result we will need a new way to optimize our parameters. Typically, when facing a non-convex objective function, we will need to resort to a numerical procedure.

![](images/eaae47a23029f5871d4269ea4e4d890b7c5285ec70139785b36d11451f686e74.jpg)  
Figure 3.5: Data set with basis functions applied, now linearly separable.

![](images/d873b57ea3b672db2adcb9977aeaa5115a1a541297ce4ae17e8c9a5e13fb39a8.jpg)  
Figure 3.6: Step Size in Gradient Descent.

$\star$ The terms numerical and analytical procedures come up very frequently in machine learning literature. An analytical solution typically utilizes a closed form equation that accepts your model and input data and returns a solution in the form of optimized model parameters. On the other hand, numerical solutions are those that require some sort of iteration to move toward an ever better solution, eventually stopping once the solution is deemed ‘good enough’. Analytical solutions are typically more desirable than numerical solutions due to computational efficiency and performance guarantees, but they often are not possible for complex problems due to non-convexity.

Gradient descent is one such numerical optimization technique.

# 3.4.1 Gradient Descent

Definition 3.4.1 (Gradient Descent): Gradient descent is a numerical, iterative optimization technique for finding the minimum of a function. It is often used to fit complex model parameters.

The high level idea behind gradient descent is as follows: to update our parameters, we take a small step in the opposite direction of the gradient of our objective function with respect to the weight parameters $\mathbf { w } ^ { ( t ) }$ . Notationally, this looks like the following:

$$
\mathbf {w} ^ {(t + 1)} = \mathbf {w} ^ {(t)} - \eta \nabla \mathcal {L} \left(\mathbf {w} ^ {(t)}\right) \tag {3.3}
$$

where $\mathbf { w } ^ { ( t ) }$ corresponds to the state of the parameters w at time $t$ , $\mathcal { L } ( \mathbf { w } ^ { ( t ) } )$ is the gradient of our objective function, and $\eta > 0$ is known as the learning rate. Note that the parameter values at time $t = 0$ given by $\mathbf { w } ^ { ( 0 ) }$ are often initialized randomly.

$\star$ In general, we want a learning rate that is large enough so that we make progress toward reaching a better solution, but not so large that we take a step that puts us in a worse place in the parameter space than we were at the previous step. Notice in Figure 3.6 that an appropriately small step size improves our objective function, while a large step size overshoots the update and leaves us in a worse position.

Why take a step in the opposite direction of the gradient of the objective function? You can think of the objective function as a hill, and the current state of our parameters $\mathbf { w } ^ { ( t ) }$ is our position on that hill. The gradient tells us the steepest direction of increase in the objective function (i.e. it specifies the direction that will make our model worse). Since we want to minimize the objective function, we choose to move away from the direction of the gradient, sending our model down the hill towards an area of lower error. We typically cease optimization when our updates become sufficiently small, indicating that we’ve reached a local minimum. Note that it’s a good idea to run gradient descent multiple times to settle on a final value for w, ideally initializing $\mathbf { w } ^ { ( 0 ) }$ to a different starting value each time, because we are optimizing a function with multiple local minima.

# 3.4.2 Batch Gradient Descent versus Stochastic Gradient Descent

There are different means by which we can compute the gradient of our objective function at each step. The first way, often called batch gradient descent, computes the gradient for our objective function at each step using the entire data set. In contrast, the technique known as stochastic gradient descent (also known as SGD) utilizes a subset of the data points at each step to compute the gradient, sometimes just a single data point. Stochastic gradient descent is typically a more popular technique for several reasons. First, the computation time is often significantly smaller as you don’t need to pass over the entire data set at each iteration. Furthermore, it’s less likely that you will get stuck in local minima while running SGD because a point in the parameter space that is a local minima for the entire data set combined is much less likely to be a local minima for each data point individually. Finally, SGD lends itself to being used for training online models (meaning models built on data points that are arriving at regular intervals) as the entirety of the data does not need to be present in order to train.

# 3.5 Objectives for Decision Boundaries

Now that we have a high-level understanding of what we’re trying to accomplish with discriminant functions as well as a grasp on gradient descent, we can consider how to solve for the decision boundaries that will dictate our classification decisions. Similar to linear regression, we first need to establish an objective function to optimize. We begin with a very simple objective function known as 0/1 loss.

# 3.5.1 0/1 Loss

Recall that a loss function penalizes mistakes made by our model. The idea behind the 0/1 loss function is very simple: if our model misclassifies a point, we incur a loss of 1, and if our model classifies it correctly, we incur no loss.

While this is a very intuitive loss function, it does not have a closed form solution like least squares does, and it is non-convex so it is not easily optimized. Intuitively, because we incur a loss of 0 or 1 for every prediction, we have no sense of ‘how good’ a given prediction was. For example, one prediction could be very close to correct, while another could be way off, but they would both receive an equivalent loss of 1. Formally, because this loss function is not differentiable, we cannot get gradient information with which to optimize our model parameters w. We will find a way around this in a moment when we discuss hinge loss, but before we get to that, let’s consider using least squares loss as we did for linear regression.

# 3.5.2 Least Squares Loss

We are already familiar with the least squares loss function from linear regression, and we can apply it again in this context to find the set of weights w that form the optimal decision boundary between target classes.

We first need to introduce the idea of one-hot encoding, which simply means that the class of a given data point is described by a vector with $K$ options that has a 1 in the position that corresponds to class $C _ { k }$ and 0s everywhere else (note that these classes aren’t usually 0-indexed). For example, class $C _ { 1 }$ of 4 classes would be represented by the vector:

$$
\left[ \begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array} \right] \tag {3.4}
$$

While class $C _ { 2 }$ would be represented by the vector:

$$
\left[ \begin{array}{l} 0 \\ 1 \\ 0 \\ 0 \end{array} \right] \tag {3.5}
$$

and so on. Now that we have the idea of one-hot encoding, we can describe our target classes for each data point in terms of a one-hot encoded vector, which can then be used in our training process for least squares.

Each class $C _ { k }$ gets its own linear function with a different set of weights $\mathbf { w } _ { k }$ :

$$
h _ {k} (\mathbf {x}, \mathbf {w} _ {k}) = \mathbf {w} _ {k} ^ {\top} \mathbf {x}
$$

We can combine the set of weights for each class into a matrix W, which gives us our linear classifier:

$$
h (\mathbf {x}, \mathbf {W}) = \mathbf {W} ^ {\top} \mathbf {x} \tag {3.6}
$$

where each row in the transposed weight matrix $\mathbf { W } ^ { \parallel }$ corresponds to the linear function of an individual class, and matrix W is $D \times K$ . We can use the results derived in the last chapter to find the solution for $\mathbf { W }$ that minimizes the least squares loss function. Assuming a data set of input data points $\mathbf { X }$ and one-hot encoded target vectors $\mathbf { Y }$ (where every row is a single target vector, so that $\mathbf { Y }$ is $N \times K$ ), the optimal solution for $\mathbf { W }$ is given by:

$$
\mathbf {W} ^ {*} = (\mathbf {X} ^ {\top} \mathbf {X}) ^ {- 1} \mathbf {X} ^ {\top} \mathbf {Y},
$$

which we can then use in our discriminant function $h ( \mathbf { x } , \mathbf { W } ^ { * } )$ to make predictions on new data points.

While least squares gives us an analytic solution for our discriminant function, it has significant limitations when used for classification. For one, least squares penalizes data points that are ‘too good’, meaning they fall too far on the correct side of the decision boundary. Furthermore, it is not robust to outliers, meaning the decision boundary significantly changes with the addition of just a few outlier data points, as seen in Figure 3.7.

We can help remedy the problems with least squares by using an alternative loss function for determining our weight parameters.

![](images/9c8b0dffcd6239b30f020c4905d1b952ad35168ab7bfa956df88fc59d69631c5.jpg)

![](images/d266233d58800c86dfcbd70955b4d15ba8121fbb2934236fb37a62423929af23.jpg)  
Figure 3.7: Outliers significantly impact our decision boundary.

![](images/4d24e0c3234aaf8b9c299250bcaeb045c74f8164c7aa97a7581d84b40959c5ed.jpg)  
Figure 3.8: Form of the ReLU function.

# 3.5.3 Hinge Loss

Recall that the problem with 0/1 loss was that we couldn’t use it optimize our model parameters w. It didn’t produce a closed form solution like least squares loss, and it wasn’t differentiable so we couldn’t take gradients.

The hinge loss function is a modification of the 0/1 loss function that both provides more finegrained information about the ‘goodness’ of a prediction and makes the loss function differentiable. To understand the hinge loss function, it’s first necessary to introduce the rectified linear activation unit, known as ReLU, seen in Figure 3.8.

$$
\operatorname {R e L U} (z) = \max  \{0, z \} \tag {3.7}
$$

We can use the form of this function to our advantage in constructing the hinge loss by recognizing that we wish to incur error when we’re wrong (which corresponds to $z \ > \ 0$ , the right side of the graph that is continuously increasing), and we wish to incur 0 error if we are correct (which corresponds to the left side of the graph where $z < 0$ ).

Remember from the previous section on least squares that in the two-class case, we classify a data point $\mathbf { x } ^ { * }$ as being from class 1 if $h ( \mathbf { x } ^ { * } , \mathbf { w } ) \geq 0$ , and class -1 otherwise. We can combine this

logic with ReLU by recognizing that $- h ( \mathbf { x } ^ { * } , \mathbf { w } ) y ^ { * } \geq 0$ when there is a classification error, where $y ^ { \ast }$ is the true class of data point $\mathbf { x } ^ { * }$ . This has exactly the properties we described above: we incur error when we misclassify, and otherwise we do not incur error.

We can then write the entirety of the hinge loss function:

$$
\begin{array}{l} \mathcal {L} (\mathbf {w}) = \sum_ {i = 1} ^ {N} \operatorname {R e L U} (- h \left(\mathbf {x} _ {i}, \mathbf {w}\right) y _ {i}) (3.8) \\ = - \sum_ {y _ {i} \neq \hat {y} _ {i}} ^ {N} h \left(\mathbf {x} _ {i}, \mathbf {w}\right) y _ {i} (3.9) \\ = - \sum_ {y _ {i} \neq \hat {y} _ {i}} ^ {N} \mathbf {w} ^ {\top} \mathbf {x} _ {i} y _ {i} (3.10) \\ \end{array}
$$

where $\hat { y } _ { i }$ is our class prediction and $y _ { i }$ is the true class value. Notice that misclassified examples contribute positive loss, as desired. We can take the gradient of this loss function, which will allow us to optimize it using stochastic gradient descent. The gradient of the loss with respect to our parameters w is as follows:

$$
\frac {\partial \mathcal {L} (\mathbf {w})}{\partial \mathbf {w}} = - \sum_ {y _ {i} \neq \hat {y} _ {i}} ^ {N} \mathbf {x} _ {i} y _ {i}
$$

and then our update equation from time $t$ to time $t + 1$ for a single misclassified example and with learning rate $\eta$ is given by:

$$
\mathbf {w} ^ {(t + 1)} = \mathbf {w} ^ {(t)} - \eta \frac {\partial \mathcal {L} (\mathbf {w})}{\partial \mathbf {w}} = \mathbf {w} ^ {(t)} + \eta \mathbf {x} _ {i} y _ {i}
$$

To sum up, the benefits of the hinge loss function are its differentiability (which allows us to optimize our weight parameters), the fact that it doesn’t penalize any correctly classified data points (unlike basic linear classification), and that it penalizes more heavily data points that are more poorly misclassified.

Using hinge loss with discriminant functions to solve classification tasks (and applying stochastic gradient descent to optimize the model parameters) is known as the perceptron algorithm. The perceptron algorithm guarantees that if there is separability between all of our data points and we run the algorithm for long enough, we will find a setting of parameters that perfectly separates our data set. The proof for this is beyond the scope of this textbook.

# 3.6 Probabilistic Methods

Unsurprisingly, we can also cast the problem of classification into a probabilistic context, which we now turn our attention to. Within this setting, we have a secondary choice to make between two distinct probabilistic approaches: discriminative or generative. We will explore both of these options.

# 3.6.1 Probabilistic Discriminative Models

Ultimately, our classification task can be summarized as follows: given a new data point $\pmb { x } ^ { * }$ , can we accurately predict the target class $y ^ { * }$ ?

Given this problem statement, it makes sense that we might try to model $p ( \boldsymbol { y } ^ { * } | \mathbf { x } ^ { * } )$ . In fact, modeling this conditional distribution directly is what’s known as probabilistic discriminative modeling.

Definition 3.6.1 (Probabilistic Discriminative Modeling): Probabilistic modeling is a classification technique whereby we choose to directly model the conditional class distribution in order to make classification predictions.

This means that we will start with the functional form of the generalized linear model described by Equation 3.2, convert this to a conditional distribution, and then optimize the parameters of the conditional distribution directly using a maximum likelihood procedure. From here, we will be able to make predictions on new data points $\mathbf { x } ^ { * }$ . The key feature of this procedure, which is known as discriminative training, is that it optimizes the parameters of a conditional distribution directly. We describe a specific, common example of this type of procedure called logistic regression in the next section.

# Logistic Regression

One problem we need to face in our discriminative modeling paradigm is that the results of our generalized linear model are not probabilities; they are simply real numbers. This is why in the previous paragraph we mentioned needing to convert our generalized linear model to a conditional distribution. That step boils down to somehow squashing the outputs of our generalized linear model onto the real numbers between 0 and 1, which will then correspond to probabilities. To do this, we will apply what is known as the logistic sigmoid function, $\sigma ( \cdot )$ .

Definition 3.6.2 (Logistic Sigmoid Function, $\sigma ( \cdot )$ ): The logistic sigmoid function is commonly used to compress the real number line down to values between 0 and 1. It is defined functionally as:

$$
\sigma (z) = \frac {1}{1 + \exp (- z)}
$$

As you can see in Figure 3.9 where the logistic sigmoid function is graphed, it squashes our output domain between 0 and 1 as desired for a probability.

$\star$ There is a more satisfying derivation for our use of the logistic sigmoid function in logistic regression, but understanding its squashing properties as motivation is sufficient for the purposes of this book.

Using the logistic sigmoid function, we now have a means of generating a probability that a new data point $\mathbf { x } ^ { * }$ is part of class $y ^ { \ast }$ . Because we are currently operating in the two-class case, which in this context will be denoted $C _ { 1 }$ and $C _ { 2 }$ , we’ll write the probability for each of these classes as:

$$
p \left(y ^ {*} = C _ {1} | \mathbf {x} ^ {*}\right) = \sigma \left(\mathbf {w} ^ {\top} \mathbf {x} ^ {*}\right)
$$

$$
p \left(y ^ {*} = C _ {2} | \mathbf {x} ^ {*}\right) = 1 - p \left(y ^ {*} = C _ {1} | \mathbf {x} ^ {*}\right)
$$

Now that we have such functions, we can apply the maximum likelihood procedure to determine the optimal parameters for our logistic regression model.

![](images/dc5d9d0b9e5964f4c3440352ba81ef61b06087de3b28d2afe6c78eaa87dc212d.jpg)  
Figure 3.9: Logistic Sigmoid Function.

For a data set $\{ \mathbf { x } _ { i } , y _ { i } \}$ where $i = 1 . . N$ and $y _ { i } \in \{ 0 , 1 \}$ , the likelihood for our setting of parameters w can be written as:

$$
p \left(\left\{y _ {i} \right\} _ {i = 1} ^ {N} \mid \mathbf {w}\right) = \prod_ {i = 1} ^ {N} \hat {y} _ {i} ^ {y _ {i}} \left\{1 - \hat {y} _ {i} \right\} ^ {1 - y _ {i}} \tag {3.11}
$$

where $\hat { y } _ { i } = p ( y _ { i } = C _ { 1 } | \mathbf { x } _ { i } ) = \sigma ( \mathbf { w } ^ { \top } \mathbf { x } _ { i } )$ .

In general, we would like to maximize this probability to find the optimal setting of our parameters. This is exactly what we intend to do, but with two further simplifications. First, we’re going to maximize the probability of the logarithm of the likelihood, as in Equation 3.12.

$$
\ln \left(p \left(\left\{y _ {i} \right\} _ {i = 1} ^ {N} \mid \mathbf {w}\right)\right) = \sum_ {i = 1} ^ {N} \left\{y _ {i} \ln \hat {y} _ {i} + \left(1 - y _ {i}\right) \ln \left(1 - \hat {y} _ {i}\right) \right\} \tag {3.12}
$$

As a monotonically increasing function, maximizing the logarithm of the likelihood (called the log likelihood) will result in the same optimal setting of parameters as if we had just optimized the likelihood directly. Furthermore, using the log likelihood has the nice effect of turning what is currently a product of terms from $1 . . N$ to a sum of terms from $1 . . N$ , which will make our calculations nicer.

Second, we will turn our log likelihood into an error function by taking the negative of our log likelihood expression. Now, instead of maximizing the log likelihood, we will be minimizing the error function, which will again find us the same setting of parameters.

$\star$ It’s worth rereading the above paragraph again to understand the pattern presented there, which we will see several times throughout this book. Instead of maximizing a likelihood function directly, it is often easier to define an error function using the negative log likelihood, which we can then minimize to find the optimal setting of parameters for our model.

After taking the negative logarithm of the likelihood function defined by Equation 3.11, we are left with the following term, known as the cross-entropy error function, which we will seek to

minimize:

$$
\operatorname {E} (\mathbf {w}) = - \ln p \left(\left\{y _ {i} \right\} \mid \mathbf {w}\right) = - \sum_ {i = 1} ^ {N} \left\{y _ {i} \ln \hat {y} _ {i} + \left(1 - y _ {i}\right) \ln \left(1 - \hat {y} _ {i}\right) \right\} \tag {3.13}
$$

where as before $\hat { y } _ { i } = p ( y _ { i } = C _ { 1 } | \mathbf { x } _ { i } ) = \sigma ( \mathbf { w } ^ { \top } \mathbf { x } _ { i } )$ . The cross-entropy error refers to the log likelhood of the labels conditioned on the examples. When used with the specific form of the logistic regression, this is also the logistic loss. Now, to solve for the optimal setting of parameters using a maximum likelihood approach as we’ve done previously, we start by taking the gradient of the cross-entropy error function with respect to w:

$$
\nabla \mathrm {E} (\mathbf {w}) = \sum_ {i = 1} ^ {N} \left(\hat {y} _ {i} - y _ {i}\right) \mathbf {x} _ {i} \tag {3.14}
$$

which we arrive at by recognizing that the derivative of the logistic sigmoid function can be written in terms of itself as:

$$
\frac {\partial \sigma (z)}{\partial z} = \sigma (z) (1 - \sigma (z))
$$

Let’s inspect the form of Equation 3.14 for a moment to understand its implications. First, it’s a summation over all of our data points, as we would expect. Then, for each data point, we are taking the difference between our predicted value $\hat { y } _ { i }$ and the actual value $y _ { i }$ , and multiplying that difference by the input vector $\mathbf { x } _ { i }$ .

While a closed form solution does not present itself here as it did in the case of linear regression due to the nonlinearity of the logistic sigmoid function, we can still optimize the parameters w of our model using an iterative procedure like gradient descent, where the objective function is defined by Equation 3.13.

# Multi-Class Logistic Regression and Softmax

As we saw when working with discriminant functions, we also need to account for multi-class problems, which are practically speaking more common than the simple two-class scenario.

In the logistic regression setting (which is a form of discriminative modeling, not to be confused with discriminant functions), we are now working with probabilities, which is why we introduced the ‘probability squashing’ sigmoidal function $\sigma ( \cdot )$ . Note that the sigmoidal function is also sometimes known as the sigmoidal activation function.

Similarly, in the multi-class logistic regression setting, we would like to also have a probability squashing function that generalizes beyond two classes. This generalization of the sigmoidal function is known as softmax.

Definition 3.6.3 (Softmax): Softmax is the multi-class generalization of the sigmoidal activation function. It accepts a vector of activations (inputs) and returns a vector of probabilities corresponding to those activations. It is defined as follows:

$$
\operatorname {s o f t m a x} _ {k} (\mathbf {z}) = \frac {\exp \left(z _ {k}\right)}{\sum_ {i = 1} ^ {K} \exp \left(z _ {i}\right)}, \text {f o r a l l} k
$$

Multi-class logistic regression uses softmax over a vector of activations to select the most likely target class for a new data point. It does this by applying softmax and then assigning the new data point to the class with the highest probability.

Example 3.1 (Softmax Example): Consider an example that has three classes: $C _ { 1 } , C _ { 2 } , C _ { 3 }$ . Let’s say we have an activation vector $\mathbf { z }$ for our new data point $\mathbf { x }$ that we wish to classify, given by:

$$
\mathbf {z} = \mathbf {W} ^ {\top} \mathbf {x} = \left[ \begin{array}{l} 4 \\ 1 \\ 7 \end{array} \right]
$$

where

$$
\mathbf {z} _ {j} = \mathbf {w} _ {j} ^ {\top} \mathbf {x}
$$

Then, using our definition of softmax, we have:

$$
\operatorname {s o f t m a x} (\mathbf {z}) = \left[ \begin{array}{l} 0. 0 4 7 \\ 0. 0 0 2 \\ 0. 9 5 0 \end{array} \right]
$$

And therefore, we would assign our new data point $\mathbf { x }$ to class $C _ { 3 }$ , which has the largest activation.

As in the two-class logistic regression case, we now need to solve for the parameters W of our model, also written as $\{ \mathbf { w } _ { j } \}$ . Assume we have an observed data set $\{ \mathbf { x } _ { i } , \mathbf { y } _ { i } \}$ for $i = 1 . . N$ where $\mathbf { y } _ { i }$ are one-hot encoded target vectors. We begin this process by writing the likelihood for our data, which is only slightly modified here to account for multiple classes:

$$
p \left(\left\{\mathbf {y} _ {i} \right\} _ {i = 1} ^ {N} \mid \mathbf {W}\right) = \prod_ {i = 1} ^ {N} \prod_ {j = 1} ^ {K} p \left(\mathbf {y} _ {i} = C _ {j} \mid \mathbf {x} _ {i}\right) ^ {y _ {i j}} = \prod_ {i = 1} ^ {N} \prod_ {j = 1} ^ {K} \hat {y} _ {i j} ^ {y _ {i j}} \tag {3.15}
$$

where $\hat { y } _ { i j } = \mathrm { s o f t m a x } _ { j } ( \mathbf { W } \mathbf { x } _ { i } )$

We can now take the negative logarithm to get the cross-entropy error function for the multiclass classification problem:

$$
\operatorname {E} (\mathbf {W}) = - \ln p \left(\left\{\mathbf {y} _ {i} \right\} _ {i = 1} ^ {N} \mid \mathbf {W}\right) = - \sum_ {i = 1} ^ {N} \sum_ {j = 1} ^ {K} y _ {i j} \ln \hat {y} _ {i j} \tag {3.16}
$$

As in the two-class case, we now take the gradient with respect to one of our weight parameter vectors $\mathbf { w } _ { j }$ :

$$
\nabla_ {\mathbf {w} _ {j}} \operatorname {E} (\mathbf {W}) = \sum_ {i = 1} ^ {N} \left(\hat {y} _ {i j} - y _ {i j}\right) \mathbf {x} _ {i} \tag {3.17}
$$

which we arrived at by recognizing that the derivative of the softmax function with respect to the input activations $z _ { j }$ can be written in terms of itself:

$$
\frac {\partial \mathrm {s o f t m a x} _ {k} (z)}{\partial z _ {j}} = \mathrm {s o f t m a x} _ {k} (z) (\mathrm {I} _ {k j} - \mathrm {s o f t m a x} _ {j} (z))
$$

where I is the identity matrix.

As in the two-class case, now that we have this gradient expression, we can use an iterative procedure like gradient descent to optimize our parameters W.

# 3.6.2 Probabilistic Generative Models

With the probabilistic discriminative modeling approach, we elected to directly model $p ( \boldsymbol { y } ^ { * } | \mathbf { x } ^ { * } )$ . However, there was an alternative option: we could have instead modeled the joint distribution of the class $y ^ { \ast }$ and the input data point $\mathbf { x } ^ { * }$ together as $p ( \boldsymbol { y } ^ { * } , \mathbf { x } ^ { * } )$ . This approach is what’s known as probabilistic generative modeling because we actually model the process by which the data was generated.

To model the data generating process in classification tasks generally acknowledges that a data point is produced by first selecting a class $y ^ { * }$ from a categorical class prior $p ( y ^ { \ast } )$ and then generating the data point $\mathbf { x } ^ { * }$ itself from the class-conditional distribution $p ( \mathbf { x } ^ { * } | \boldsymbol { y } ^ { * } )$ , the form of which is problem specific. This generative approach is a particularly good idea if we want to create more data (by sampling from the joint distribution) or if we have some sort of expert knowledge about how the data was generated, which can make our model more powerful than the discriminative approach.

$\star$ Notice that with probabilistic generative modeling, we choose a specific distribution for our class-conditional densities instead of simply using a generalized linear model combined with a sigmoid/softmax function as we did in the logistic regression setting. This highlights the difference between discriminative and generative modeling: in the generative setting, we are modeling the production of the data itself instead of simply optimizing the parameters of a more general model that predicts class membership directly.

# Classification in the Generative Setting

Now that we’re situated in the generative setting, we turn our attention to the actual problem of using our model to predict class membership of new data points $\mathbf { x } ^ { * }$ .

To perform classification, we will pick the class $C _ { k }$ that maximizes the probability of $\mathbf { x } ^ { * }$ being from that class as defined by $p ( y ^ { * } = C _ { k } | \mathbf { x } ^ { * } )$ . We can relate this conditional density to the joint density $p ( \boldsymbol { y } ^ { * } , \mathbf { x } ^ { * } )$ through Bayes’ Rule:

$$
p (y ^ {*} = C _ {k} | \mathbf {x} ^ {*}) = \frac {p (y ^ {*} , \mathbf {x} ^ {*})}{p (\mathbf {x} ^ {*})} = \frac {p (\mathbf {x} ^ {*} | y ^ {*} = C _ {k}) p (y ^ {*} = C _ {k})}{p (\mathbf {x} ^ {*})} \propto p (\mathbf {x} ^ {*} | y ^ {*} = C _ {k}) p (y ^ {*} = C _ {k})
$$

where $p ( \mathbf { x } ^ { * } )$ is a constant that can be ignored as it will be the same for every conditional probability $p ( y ^ { * } = C _ { k } | \mathbf { x } ^ { * } )$ .

Recall that the class prior $p ( y )$ will always be a categorical distribution (the multi-class generalization of the Bernoulli distribution), while the class-conditional distribution can be specified using prior knowledge of the problem domain. Once we have specified this class conditional distribution, we can solve for the parameters of both that model and the categorical distribution by optimizing the likelihood function. Let’s now derive that likelihood function.

# Derivation 3.6.1 (Probabilistic Generative Model Likelihood Function):

We’re going to derive the likelihood function for the parameters of our probabilistic generative model in the two-class setting, allowing that the multi-class generalization will be a straightforward exercise.

Let’s start by assuming a Gaussian conditional distribution for our data $p ( \mathbf { x } | y = C _ { k } )$ . Given a data set $\{ \mathbf { x } _ { i } , y _ { i } \}$ for $i = 1 . . N$ , where $y _ { i } = 1$ corresponds to class $C _ { 1 }$ and $y _ { i } = 0$ corresponds to class $C _ { 2 }$ , we can construct our maximum likelihood solution. Let’s first specify our class priors:

$$
\begin{array}{l} p (C _ {1}) = \pi \\ p (C _ {2}) = 1 - \pi \\ \end{array}
$$

For simplicity, we’ll assume a shared covariance matrix $\pmb { \Sigma }$ between our two classes. Then, for data points $\mathbf { x } _ { i }$ from class $C _ { 1 }$ , we have:

$$
p \left(\mathbf {x} _ {i}, C _ {1}\right) = p \left(C _ {1}\right) p \left(\mathbf {x} _ {i} \mid C _ {1}\right) = \pi \mathcal {N} \left(\mathbf {x} _ {i} \mid \boldsymbol {\mu} _ {1}, \boldsymbol {\Sigma}\right)
$$

And for data points $\mathbf { x } _ { i }$ from class $C _ { 2 }$ , we have:

$$
p (\mathbf {x} _ {i}, C _ {2}) = p (C _ {2}) p (\mathbf {x} _ {i} | C _ {2}) = (1 - \pi) \mathcal {N} (\mathbf {x} _ {i} | \boldsymbol {\mu} _ {2}, \boldsymbol {\Sigma})
$$

Using these two densities, we can construct our likelihood function:

$$
p (\pi , \boldsymbol {\mu} _ {1}, \boldsymbol {\mu} _ {2}, \boldsymbol {\Sigma}) = \prod_ {i = 1} ^ {N} \left(\pi \mathcal {N} (\mathbf {x} _ {i} | \boldsymbol {\mu} _ {1}, \boldsymbol {\Sigma})\right) ^ {y _ {i}} \left((1 - \pi) \mathcal {N} (\mathbf {x} _ {i} | \boldsymbol {\mu} _ {2}, \boldsymbol {\Sigma})\right) ^ {1 - y _ {i}}
$$

As usual, we will take the logarithm which is easier to work with:

$$
\ln p (\boldsymbol {\pi}, \boldsymbol {\mu} _ {1}, \boldsymbol {\mu} _ {2}, \boldsymbol {\Sigma}) = \sum_ {i = 1} ^ {N} y _ {i} \ln \left(\pi \mathcal {N} (\mathbf {x} _ {i} | \boldsymbol {\mu} _ {1}, \boldsymbol {\Sigma})\right) + (1 - y _ {i}) \ln \left((1 - \pi) \mathcal {N} (\mathbf {x} _ {i} | \boldsymbol {\mu} _ {2}, \boldsymbol {\Sigma})\right)
$$

Now that we have specified the log-likelihood function for our model, we can go about optimizing our model by maximizing this likelihood. One way to do this is with a straightforward maximimum likelihood estimation approach. We will optimize our parameters $\pi , \mu _ { 1 } , \mu _ { 2 }$ , and, $\pmb { \Sigma }$ separately, using the usual procedure of taking the derivative, setting equal to 0, and then solving for the parameter of interest. We write down this MLE solution in the following section.

# MLE Solution

# Solving for $\pi$

Beginning with $\pi$ , we’ll concern ourselves only with the terms that depend on $\pi$ which are:

$$
\sum_ {i = 1} ^ {N} y _ {i} \ln \pi + (1 - y _ {i}) \ln (1 - \pi)
$$

Taking the derivative with respect to $\pi$ , setting equal to 0, rearranging, we get:

$$
\pi = \frac {1}{N} \sum_ {i = 1} ^ {N} y _ {i} = \frac {N _ {1}}{N} = \frac {N _ {1}}{N _ {1} + N _ {2}}
$$

where $N _ { 1 }$ is the number of data points in our data set from class $C _ { 1 }$ , $N _ { 2 }$ is the number of data points from class $C _ { 2 }$ , and $N$ is just the total number of data points. This means that the maximum likelihood solution for $\pi$ is the fraction of points that are assigned to class $C _ { 1 }$ , a fairly intuitive solution and one that will be commonly seen when working with maximum likelihood calculations.

# Solving for $\pmb { \mu }$

Let’s now perform the maximization for $\pmb { \mu } _ { 1 }$ . Start by considering the terms from our log likelihood involving $\pmb { \mu } _ { 1 }$ :

$$
\sum_ {i = 1} ^ {N} y _ {i} \ln {\mathcal {N}} (\mathbf {x} _ {i} | \boldsymbol {\mu} _ {1}, \boldsymbol {\Sigma}) = - \frac {1}{2} \sum_ {i = 1} ^ {N} y _ {i} (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) ^ {\top} \boldsymbol {\Sigma} ^ {- 1} (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) + c
$$

where $c$ are constants not involving the $\pmb { \mu } _ { 1 }$ term. Taking the derivative with respect to $\pmb { \mu } _ { 1 }$ , setting equal to 0, and rearranging:

$$
\pmb {\mu} _ {1} = \frac {1}{N _ {1}} \sum_ {i = 1} ^ {N} y _ {i} \mathbf {x} _ {i}
$$

which is simply the average of all the data points $\mathbf { x } _ { i }$ assigned to class $C _ { 1 }$ , a very intuitive result. By the same derivation, the maximum likelihood solution for $\mu _ { 2 }$ is:

$$
\pmb {\mu} _ {2} = \frac {1}{N _ {2}} \sum_ {i = 1} ^ {N} (1 - y _ {i}) \mathbf {x} _ {i}
$$

# Solving for $\pmb { \Sigma }$

We can also the maximum likelihood solution for the shared covariance matrix $\pmb { \Sigma }$ . Start by considering the terms in our log likelihood expression involving $\pmb { \Sigma }$ :

$$
\begin{array}{l} - \frac {1}{2} \sum_ {i = 1} ^ {N} y _ {i} \ln | \boldsymbol {\Sigma} | - \frac {1}{2} \sum_ {i = 1} ^ {N} y _ {i} \left(\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}\right) ^ {\top} \boldsymbol {\Sigma} ^ {- 1} \left(\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}\right) - \frac {1}{2} \sum_ {i = 1} ^ {N} (1 - y _ {i}) \ln | \boldsymbol {\Sigma} | \\ - \frac {1}{2} \sum_ {i = 1} ^ {N} (1 - y _ {i}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) ^ {\top} \boldsymbol {\Sigma} ^ {- 1} (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) \\ \end{array}
$$

We can use the following “matrix cookbook formulas” to help with taking the derivative with respect to $\pmb { \Sigma }$ . Also, we adopt convention ${ \bf Z } ^ { - } : = ( { \bf Z } ^ { \mathrm { ~ \tiny ~ \backslash ~ - 1 ~ } }$ . The two helpful formulas are:

$$
\frac {\partial \mathbf {a} ^ {\top} \mathbf {Z} ^ {- 1} \mathbf {b}}{\partial \mathbf {Z}} = - \mathbf {Z} ^ {- \top} \mathbf {a b} ^ {\top} \mathbf {Z} ^ {- \top}
$$

$$
\frac {\partial \ln | \det (\mathbf {Z}) |}{\partial \mathbf {Z}} = \mathbf {Z} ^ {- \top}.
$$

Now, taking the derivative with respect to $\pmb { \Sigma }$ and collecting terms, we have:

$$
- \frac {1}{2} N \boldsymbol {\Sigma} ^ {- \top} + \frac {1}{2} \sum_ {i = 1} ^ {N} y _ {i} \boldsymbol {\Sigma} ^ {- \top} (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) ^ {\top} \boldsymbol {\Sigma} ^ {- \top} + \frac {1}{2} \sum_ {i = 1} ^ {N} (1 - y _ {i}) \boldsymbol {\Sigma} ^ {- \top} (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) ^ {\top} \boldsymbol {\Sigma} ^ {- \top}
$$

Setting this to zero and multiplying both sides by $\pmb { \Sigma } ^ { \top }$ from the left and right (with the effect of retaining $\pmb { \Sigma } ^ { \top }$ in only the first term, since $\pmb { \Sigma } ^ { \top } \pmb { \Sigma } ^ { - \top } = \pmb { \Sigma } ^ { \top } \left( \pmb { \Sigma } ^ { \top } \right) ^ { - 1 } = \mathbf { I }$ and $\Sigma ^ { - \top } \Sigma ^ { \top } = \left( \Sigma ^ { \top } \right) ^ { - 1 } \Sigma ^ { \top } =$ I), and multiplying by 2, we have

$$
N \boldsymbol {\Sigma} ^ {\top} - \sum_ {i = 1} ^ {N} \left(y _ {i} (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) ^ {\top} + (1 - y _ {i}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) ^ {\top}\right) = 0.
$$

Rearranging to solve for $\pmb { \Sigma }$ , and recognizing that covariance matrices are symmetric, and so $\pmb { \Sigma } ^ { \top } = \pmb { \Sigma }$ , we have:

$$
\boldsymbol {\Sigma} = \frac {1}{N} \sum_ {i = 1} ^ {N} \left(y _ {i} (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {1}) ^ {\top} + (1 - y _ {i}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) (\mathbf {x} _ {i} - \boldsymbol {\mu} _ {2}) ^ {\top}\right).
$$

This has the intuitive interpretation that the maximum likelihood solution for the shared covariance matrix is the weighted average of the two individual covariance matrices. Note that $( \mathbf { x } _ { i } - { \pmb \mu } _ { 1 } ) ( \mathbf { x } _ { i } - { \pmb \mu } _ { 1 } ) ^ { \top }$ is a matrix (the outer product of the two vectors). Also, $y _ { i }$ is a scalar, which means that each term is a sum of matrices. For any point $i$ , only one of two matrices inside will contribute due to the use of $y _ { i }$ and $( 1 - y _ { i } )$ .

It is relatively straightforward to extend these maximum likelihood derivations from their twoclass form to their more general, multi-class form.

# Naive Bayes

There exists a further simplification to probabilistic generative modeling in the context of classification known as Naive Bayes.

Definition 3.6.4 (Naive Bayes): Naive Bayes is a type of generative model for classification tasks. It imposes the simplifying rule that for a given class $C _ { k }$ , we assume that each feature of the data points $\mathbf { x }$ generated within that class are independent (hence the descriptor ‘naive’). This means that the conditional distribution $p ( \mathbf { x } | y = C _ { k } )$ can be written as:

$$
p (\mathbf {x} | y = C _ {k}) = \prod_ {i = 1} ^ {D} p (x _ {i} | y = C _ {k})
$$

where $D$ is the number of features in our data point $\mathbf { x }$ and $C _ { k }$ is the class. Note that Naive Bayes does not specify the form of the model $p ( x _ { i } | y = C _ { k } )$ ), this decision is left up to us.

This is obviously not a realistic simplification for all scenarios, but it can make our calculations easier and may actually hold true in certain cases. We can build more intuition for how Naive Bayes works through an example.

Example 3.2 (Naive Bayes Example): Suppose you are given a biased two-sided coin and two biased dice. The coin has probabilities as follows:

The dice have the numbers 1 through 6 on them, but they are biased differently. Die 1 has probabilities as follows:

1 : 40%   
2 : 20%   
3 : 10%   
4 : 10%   
5 : 10%   
6 : 10%

Die 2 has probabilities as follows:

Your friend is tasked with doing the following. First, they flip the coin. If it lands Heads, they select Die 1, otherwise they select Die 2. Then, they roll that die 10 times in a row, recording the results of the die rolls. After they have completed this, you get to observe the aggregated results from the die rolls. Using this information (and assuming you know the biases associated with the coin and dice), you must then classify which die the rolls came from. Assume your friend went through this procedure and produced the following counts:

Determine which die this roll count most likely came from.

# Solution:

This problem is situated in the Naive Bayes framework: for a given class (dictated by the coin flip), the outcomes within that class (each die roll) are independent. Making a classification in this situation is as simple as computing the probability that the selected die produced the given roll counts. Let’s start by computing the probability for Die 1:

$$
\begin{array}{l} p (\text {D i e} 1) = p (\text {C o i n F l i p} = \text {H e a d s}) * p (\text {R o l l C o u n t} = [ 3, 1, 2, 2, 2, 1, 1 ]) \\ \propto 0. 3 * (0. 4) ^ {3} * (0. 2) ^ {1} * (0. 1) ^ {2} * (0. 1) ^ {2} * (0. 1) ^ {1} * (0. 1) ^ {1} \\ \propto 3. 8 4 * 1 0 ^ {- 9} \\ \end{array}
$$

Notice that we don’t concern ourselves with the normalization constant for the probability of the roll count - this will not differ between the choice of dice and we can thus ignore it for simplicity. Now the probability for Die 2:

$$
\begin{array}{l} p (\text {D i e} 2) = p (\text {C o i n F l i p} = \text {T a i l s}) * p (\text {R o l l C o u n t} = [ 3, 1, 2, 2, 1, 1 ]) \\ \propto 0. 7 * (0. 2) ^ {3} * (0. 2) ^ {1} * (0. 1) ^ {2} * (0. 3) ^ {2} * (0. 1) ^ {1} * (0. 1) ^ {1} \\ \propto 1. 0 0 8 * 1 0 ^ {- 8} \\ \end{array}
$$

Therefore, we would classify this roll count as having come from Die 2.

Note that this problem asked us only to make a classification prediction after we already knew the parameters governing the coin flip and dice rolls. However, given a data set, we could have also used a maximum likelihood procedure under the Naive Bayes assumption to estimate the values of the parameters governing the probability of the coin flip and die rolls.

# 3.7 Conclusion

In this chapter, we looked at different objectives and techniques for solving classification problems, including discriminant functions, probabilistic discriminative models, and probabilistic generative models. In particular, we emphasized the distinction between two-class and multi-class problems as well as the philosophical differences between generative and discriminative modeling.

We also covered several topics that we will make use of in subsequent chapters, including sigmoid functions and softmax, maximum likelihood solutions, and further use of basis changes.

By now, you have a sound understanding of generative modeling and how it can be applied to classification tasks. In the next chapter, we will explore how generative modeling is applied to a still broader class of problems.

# Chapter 4

# Neural Networks

Despite how seemingly popular neural networks have become recently, they aren’t actually a novel technique. The first neural networks were described in the early 1940s, and the only reason they weren’t put into practice shortly thereafter was the fact that we didn’t yet have access to the large amounts of storage and compute that complex neural network require. Over the last two decades, and particularly with the advent of cloud computing, we now have more and more access to the cheap processing power and memory required to make neural networks a viable option for model building.

As we will come to see in this chapter, neural networks are an extraordinarily flexible class of models used to solve a variety of different problem types. In fact, this flexibility is both what makes them so widely applicable and yet so difficult to use properly. We will explore the applications, underlying theory, and training schemes behind neural networks.

# 4.1 Motivation

For problems that fall into the category of regression or classification, we’ve already discussed the utility of basis functions. Sometimes, a problem that is intractable with our raw input data will be readily solvable with basis-transformed data. We often select these basis changes using expert knowledge. For example, if we were working with a data set that related to chemical information, and there were certain equations that a chemist told us to be important for the particular problem we were trying to solve, we might include a variety of the transformations that are present in those equations.

However, imagine now that we have a data set with no accompanying expert information. More often than not, complex problem domains don’t come with a useful set of suggested transformations. How do we find useful basis functions in these situations? This is exactly the strength of neural networks - they identify the best basis for a data set!

Neural networks simultaneously solve for our model parameters and the best basis transformations. This makes them exceedingly flexible. Unfortunately, this flexibility is also the weakness of neural nets: while it enables us to solve difficult problems, it also creates a host of other complications. Chief among these complications is the fact that neural networks require a lot of computation to train. This is a result of the effective model space being so large - to explore it all takes time and resources. Furthermore, this flexibilty can cause rather severe overfitting if we are not careful.

In summary, neural networks identify good basis transformations for our data, and the strengths and weaknesses of neural networks stem from the same root cause: model flexibility. It will be our goal then to appropriately harness these properties to create useful models.

# 4.1.1 Comparison to Other Methods

In the previous two chapters, we explored two broad problem types: classification and regression, and it’s natural to wonder where neural networks fit in. The answer is that they are applicable to both. The flexibility of neural networks even extends to the types of problems they can be made to handle. Thus, the tasks that we’ve explored over the last two chapters, such as predicting heights in the regression case or object category in the classification case, can be performed by neural networks.

Given that neural networks are flexible enough to be used as models for either regression or classification tasks, this means that every time you’re faced with a problem that falls into one of these categories, you have a choice to make between the methods we’ve already covered or using a neural network. Before we’ve explored the specifics of neural networks, how can we discern at a high level when they will be a good choice for a specific problem?

One simple way to think about this is that if we never needed to use neural networks, we probably wouldn’t. In other words, if a problem can be solved effectively by one of the techniques we’ve already described for regression or classification (such as linear regression, discriminant functions, etc.), we would prefer to use those. The reason is that neural networks are often more memory and processor intensive than these other techniques, and they are much more complex to train and debug.

The flip side of this is that hard problems are often too complex or too hard to engineer features for to use a simple regression or classification technique. Indeed, even if you eventually think you will need to use a neural network to solve a given problem, it makes sense to try a simple technique first both to get a baseline of performance and because it may just happen to be good enough.

What is so special about neural networks that they can solve problems that the other techniques we’ve explored may not be able to? And why are they so expensive to train? These questions will be explored over the course of the chapter, and a good place to start is with the status of neural networks as universal function approximators.

# 4.1.2 Universal Function Approximation

The flexibility of neural networks is a well-established phenomenon. In fact, neural networks are what are known as universal function approximators. This means that with a large enough network, it is possible to approximate any function. The proof of this is beyond the scope of this textbook, but it provides some context for why flexibility is one of the key attributes of neural networks.

# ML Framework Cube: Neural Networks

As universal function approximators, neural networks can operate over discrete or continuous outputs. We primarily use neural networks to solve regression or classification problems, which involve training on data sets with example inputs and outputs, making this a supervised technique. Finally, while there exist probabilistic extensions for neural networks, they primarily operate in the non-probabilistic setting.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Continuous/Discrete</td><td>Supervised</td><td>No</td></tr></table>

![](images/b3b5940c6d742d3ba024cb2b2f6c7effa1ddcfe04c7aa767f80af2ca81185cb5.jpg)  
Figure 4.1: Simple Neural Network.

# 4.2 Feed-Forward Networks

The feed-forward neural network is the most basic setup for a neural network. Most of the logic behind neural networks can be explained using a feed-forward network, with additional bells and whistles typically added to form more complex networks. We will explore this basic neural network structure first.

# 4.3 Neural Network Basics and Terminology

Looking at Figure 4.1, we see that a feed-forward neural network is a series of connected layers that transform an input data point $\mathbf { x }$ into an output data point y. Each layer is composed of nodes, the small black circles. Each node in the input layer corresponds to one dimension of a single data point $\mathbf { x }$ (meaning the first node is $x _ { 1 }$ , the second node $x _ { 2 }$ , etc.). The same is true of the nodes in the output layer, which represent each dimension of y. For a binary classification problem there is a single output node, representing the predicted probability of the positive class (with $K$ outputs for multi-classification). For a regression problem there may one or more nodes, depending on the dimensions of the output. The nodes in the hidden layers correspond to activation functions.

Let’s zoom in on the first node in hidden layer 1, shown in Figure 4.2, to describe what happens at each node as we transform an input. For now we adopt simple notation and don’t worry about indexing by later. Looking at Figure 4.2, notice that every node in the input layer is connected to this first node in hidden layer 1. Each of the lines is a connection, that has a weight $w _ { d }$ associated with it. We multiply all the nodes in the input layer by their corresponding weight, and then add them all together to produce the activation $a$ , i..e, the input at this first node (we’ve included the bias term in the input vector):

$$
a = x _ {1} w _ {1} + x _ {2} w _ {2} + x _ {3} w _ {3} \tag {4.1}
$$

![](images/c0c48eb927203d26193988655f9d735df0f84e9b3346ecb0debc097c29157677.jpg)  
Figure 4.2: Zooming in on the inputs and the first node of the first layer.

$\star$ Every node in every layer has distinct weights associated with it.

This gives us the activation for the first node in the first hidden layer. Once we’ve done this for every node in the first hidden layer, we make a non-linear transform of these activations, and then move on to computing the activations for the second hidden layer (which require the outputs from the first hidden layer, as indicated by the network of connections). We keep pushing values through the network in this manner until we have our complete output layer, at which point we are finished.

We’ve skipped over some important details in this high-level overview, but with this general information about what a neural network looks like and the terminology associated with it, we can now dig into the details a little deeper.

# 4.3.1 Adaptive Basis Functions

As we mentioned in the introduction, the strength of neural networks is that we can learn an effective basis for our problem domain at the same time as we train the parameters of our model. In fact, learning this basis becomes just another part of our parameter training. Let’s make this notion of learning a basis more concrete.

Thinking back to our chapter on linear regression, we were training a model that made predictions using a functional form that looked like:

$$
y (\mathbf {x}, \mathbf {w}) = \mathbf {w} \phi^ {T} = \sum_ {d = 1} ^ {D} w _ {d} \phi_ {d}
$$

where $\phi = \phi ( \mathbf { x } )$ , $\phi$ is the basis transformation function, and $D$ is the dimensionality of the data point.

Typically with this linear regression setup, we are training our model to optimize the parameters w. With neural networks this is no different— we still train to learn those parameters. However, the difference in the neural network setting is that the basis transformation function $\phi$ is no longer fixed. Instead, the transformations are incorporated into the model parameters, and thus learned at the same time.

This leads to a different functional form for neural networks. A neural network with $M$ nodes in its first hidden layer performs $M$ linear combinations of an input data point $\mathbf { x }$ :

$$
a _ {j} ^ {(1)} = \sum_ {d = 1} ^ {D} w _ {j d} ^ {(1)} x _ {d} + w _ {j 0} ^ {(1)} \quad \forall j \in 1.. M \tag {4.2}
$$

Here, we use $a ^ { ( 1 ) }$ to denote the activation of a unit in layer 1 and notation $w ^ { ( 1 ) }$ denotes the weights used to determine the activations in layer 1. We also make the bias explicit. We will still use the bias trick in general, but we’ve left it out here to explicitly illustrate the bias term ${ w _ { j 0 } ^ { ( 1 ) } }$ Other than this, equation 4.2 describes what we’ve already seen in Figure 4.2. The only difference is that we index each node in the hidden layer (along with its weights) by $j$ .

The $M$ different values $a _ { j } ^ { ( 1 ) }$ are the activations. We transform these activations with a non-linear activation function $h ( \cdot )$ to give:

$$
z _ {j} ^ {(1)} = h \left(a _ {j}\right) \tag {4.3}
$$

$\star$ Note that we didn’t mention activation functions in the previous section only for the sake of simplicity. These non-linearities are crucial to the performance of neural networks because they allow for modeling of outcomes that vary non-linearly with their input variables.

These values $z _ { j } ^ { ( 1 ) }$ correspond to the outputs of the hidden units, each of which is associated with an activation function. Superscript (1) indicates they are the outputs of units in layer 1. A typical activation function is the sigmoid function, but other common choices are the tanh function and rectified linear unit $( R e L U )$ .

These output values $z _ { j } ^ { ( 1 ) }$ , for units $j \in \{ 1 , \ldots , M \}$ in layer 1, form the inputs to the next layer. The activation of unit $j ^ { \prime }$ in layer 2 depends on the outputs from layer 1 and the weights $w _ { j ^ { \prime } 0 } ^ { ( 2 ) } , w _ { j ^ { \prime } 1 } ^ { ( 2 ) } , \ldots , w _ { j ^ { \prime } M } ^ { ( 2 ) }$ that define the linear sum at the input of unit $j ^ { \prime }$ :

$$
a _ {j ^ {\prime}} ^ {(2)} = \sum_ {j = 1} ^ {M} w _ {j ^ {\prime} m} ^ {(2)} z _ {j} ^ {(1)} + w _ {j ^ {\prime} 0} ^ {(2)} \tag {4.4}
$$

We can connect many layers together in this way. They need not all have the same number of nodes but we will adopt $M$ for the number of nodes in each layer for convenience of exposition. Eventually, we will reach the output layer, and each output is denoted $y _ { k }$ , for $k \in \{ 1 , \ldots , K \}$ . The final activation function may be the sigmoid function, softmax function, or just linear (and thus no transform).

We can now examine a more complete diagram of a feed-forward neural network, shown in Figure 4.3. It may be helpful to reread the previous paragraphs and use the diagram to visualize how a neural network transforms its inputs. This is a single hidden layer, or two-layer, network. Here, we use $z$ to denote the output values of the units in the hidden layer.

![](images/6fee6b42b0332a78954b51cea5ae93551c12792c1f83ca0ef2a5fe25a5028fa8.jpg)  
Figure 4.3: Feed-Forward Neural Network.

$\star$ Different resources choose to count the number of layers in a neural net in different ways. We’ve elected to count each layer of non-input nodes, thus the two-layer network in Figure 4.3. However, some resources will choose to count every layer of nodes (three in this case) and still others count only the number of hidden layers (making this a one layer network).

Combining Figure 4.3 and our preceeding functional description, we can describe the operation performed by a two-layer neural network using a single functional transformation (with $m$ to index a unit in the hidden layer):

$$
y _ {k} (\mathbf {x}, \mathbf {w}) = \sigma \left(\sum_ {m = 1} ^ {M} w _ {k m} ^ {(2)} h \left(\sum_ {d = 1} ^ {D} w _ {m d} ^ {(1)} x _ {d} + w _ {m 0} ^ {(1)}\right) + w _ {k 0} ^ {(2)}\right) \tag {4.5}
$$

where we’ve elected to make the final activation function the sigmoid function $\sigma ( \cdot )$ , as is suitable for binary classification. We use $h$ to denote the non-linear activation function for a hidden unit. Written like this, a neural network is simply a non-linear function that transforms an input $\mathbf { x }$ into an output y that is controlled by our set of parameters w.

Furthermore, we see now why this basic variety of neural networks is a feed-forward neural network. We’re simply feeding our input $\mathbf { x }$ forward through the network from the first layer to the last layer. Assuming we have a fully trained network, we can make predictions on new input data points by propagating them through the network to generate output predictions (“the forward pass”).

We can also simplify this equation by utilizing the bias trick and appending an $x _ { 0 } = 1$ value to each of our data points such that:

$$
y _ {k} (\mathbf {x}, \mathbf {w}) = \sigma \Bigg (\sum_ {m = 1} ^ {M} w _ {k m} ^ {(2)} h \Bigg (\sum_ {d = 1} ^ {D} w _ {m d} ^ {(1)} x _ {d} \Bigg) \Bigg)
$$

Finally, it’s worth considering that while a neural network is a series of linear combinations, it is special because of the differentiable non-linearities applied at each of the hidden layers. Without

these non-linearities, the successive application of different network weights would be equivalent to a single large linear combination.

# 4.4 Network Training

Now that we understand the structure of a basic feed-forward neural network and how they can be used to make predictions, we turn our attention to the training process.

# 4.4.1 Objective Function

To train our network, it’s first necessary to establish an objective function. Remember that neural networks can be used to solve both regression and classification problems, which means that our choice of objective will depend on the type of problem and the properties we desire.

For the case of linear regression, a common objective function is the least squares loss:

$$
\mathcal {L} (\mathbf {w}) = \frac {1}{2} \sum_ {n = 1} ^ {N} \left(y \left(\mathbf {x} _ {n}, \mathbf {w}\right) - y _ {n}\right) ^ {2}, \tag {4.6}
$$

where $y _ { n }$ is the target value on example $n$ . Sometimes we will have a regression problem with multiple outputs, in which case the loss would also take the sum over these different target values.

For a binary classification problem, which we model through a single, sigmoid output activation unit, then negated log-likelihood (or cross-entropy) is the typical loss function:

$$
\mathcal {L} (\mathbf {w}) = - \sum_ {n = 1} ^ {N} \left(y _ {n} \ln \hat {y} _ {n} + (1 - y _ {n}) (\ln (1 - \hat {y} _ {n})\right) \tag {4.7}
$$

For a multiclass classification problem, produced by a softmax function in the output activation layer, we would use the negated log likelihood (cross entropy) loss:

$$
\mathcal {L} (\mathbf {w}) = - \sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {K} y _ {k n} \ln \left(\frac {\exp \left(a _ {k} (\mathbf {x} , \mathbf {w})\right)}{\sum_ {j = 1} ^ {K} \exp \left(a _ {j} (\mathbf {x} , \mathbf {w})\right)}\right) \tag {4.8}
$$

$\star$ Loss function and objective function all refer to the same concept: the function we optimize to train our model.

# 4.4.2 Optimizing Parameters

We want to find weight parameters w to minimize the objective function. The highly non-linear nature of neural networks means that this objective function will be non-convex with respect to our weight parameters. But still, we make use of stochastic gradient descent for optimization (refer to the previous chapter for a refresher).

In order to use gradient descent, we first need to figure out how to compute the gradient of our objective function with respect to our weights. That is the topic of the next section.

# 4.4.3 Backpropagation

Considering how our feed-forward neural network works, by propagating activations through our network to produce a final output, it’s not immediately clear how we can compute gradients for the weights that lie in the middle of our networ. There is an elegant solution to this, which comes from “sending errors backwards” through our network, in a process known as backpropagation.

# Definition 4.4.1 (Backpropagation):

Backpropagation is the procedure by which we pass errors backwards through a feed-forward neural network in order to compute gradients for the weight parameters of the network.

Backpropagation refers specifically to the portion of neural network training during which we compute the derivative of the objective function with respect to the weight parameters. This is done by propagating errors backwards through the network, hence the name.

$\star$ Note that we still need to update the value of the weight parameters after computing their derivatives. This is typically done using gradient descent or some variant of it.

We now explore the details of backpropagation in greater depth.

# 4.4.4 Computing Derivatives Using Backpropagation

Recall that the activation a(ℓ)j $a _ { j } ^ { ( \ell ) }$ for a node $j$ in layer $\ell$ of a neural network can be described by the equation:

$$
a _ {j} ^ {(\ell)} = \sum_ {m = 1} ^ {M} w _ {j m} ^ {(\ell)} z _ {m} ^ {(\ell - 1)}, \tag {4.9}
$$

with the weights in layer ℓ corresponding to node j denoted by w(ℓ)j1 , ..., w $M$ $\ell$ $j$ $w _ { j 1 } ^ { ( \ell ) } , . . . , w _ { j M } ^ { ( \ell ) }$ (e) $z _ { 1 } ^ { ( \ell - 1 ) } , . . . , z _ { M } ^ { ( \ell - 1 ) }$ . This activation is (ℓ−1) and transformed by an activation function $h ( \cdot )$ to give unit output $z _ { j } ^ { ( \ell ) }$ (ℓ) z

$$
z _ {j} ^ {(\ell)} = h \left(a _ {j} ^ {(\ell)}\right). \tag {4.10}
$$

Computing these values as we flow through the network constitutes the forward pass through our network.

We now wish to begin the process of computing derivatives of the objective function with respect to our weights. For the sake of simplicity, we’ll assume that the current setting of our parameters w generates a loss of $L$ for a single data point, as though we were performing stochastic gradient descent.

Let’s consider how we could compute the derivative of $L$ with respect to an individual weight in our network, $w _ { j m } ^ { ( \ell ) }$ (the $_ { \textrm { \textbf { \textit { H } } } }$ th weight for activation $j$ in layer $\ell$ ):

$$
\frac {\partial L}{\partial w _ {j m} ^ {(\ell)}}. \tag {4.11}
$$

We first need to figure out what the dependence of $L$ is on this weight. This weight contributes to the final result only via its contribution to the activation $a _ { j } ^ { ( \ell ) }$ . This allows us to use the chain

![](images/27cf522473d86f8c4a24788d92703df8f9a6206ad853ec7fbeb28b9571ed931f.jpg)  
Figure 4.4: Gradient of the loss function in a neural network with respect to a weight. It depends on the input value $z _ { m }$ and the “error” corresponding to the activation value at the output end of the weight.

rule to simplify Equation 4.11 as:

$$
\frac {\partial L}{\partial w _ {j m} ^ {(\ell)}} = \frac {\partial L}{\partial a _ {j} ^ {(\ell)}} \cdot \frac {\partial a _ {j} ^ {(\ell)}}{\partial w _ {j m} ^ {(\ell)}}. \tag {4.12}
$$

The first part of this is the, typically non-linear, dependence of loss on activation. The second part is the linear dependence of activation on weight. Using Equation 4.9, we have that:

$$
\frac {\partial a _ {j} ^ {(\ell)}}{\partial w _ {j m} ^ {(\ell)}} = z _ {m} ^ {(\ell - 1)},
$$

and just the value of the input from the previous layer. We now introduce the following notation for the first term,

$$
\delta_ {j} ^ {(\ell)} = \frac {\partial L}{\partial a _ {j} ^ {(\ell)}}, \tag {4.13}
$$

where $\delta _ { j } ^ { ( \ell ) }$ values are referred to as errors. We rewrite Equation 4.12 as:

$$
\frac {\partial L}{\partial w _ {j m} ^ {(\ell)}} = \delta_ {j} ^ {(\ell)} z _ {m} ^ {(\ell - 1)}. \tag {4.14}
$$

The implications of Equation 4.14 are significant for understanding backpropagation. The derivative of the loss with respect to an arbitrary weight in the network can be calculated as the product of the error $\delta _ { j } ^ { ( \ell ) }$ at the “output end of that weight” and the value z(ℓ−1)m at the “input end $z _ { m } ^ { ( \ell - 1 ) }$ of the weight.” We visualize this property in Figure 4.4 (dropping the layer subscripting).

To compute the derivatives, it suffices to compute the values of $\delta _ { j }$ for each node, also saving the output values $z _ { m }$ during the forward pass through the network (to be multiplied by the values of $\delta _ { j }$ to get partials).

$\star$ We will only have “errors values” $\delta _ { j }$ for the hidden and output units of our network. This is logical because there is no notion of applying an error to our input data, which we have no control over.

We now consider how to compute these error values. For a unit in the output layer, indexing it here by $k$ , and assuming the output activation function is linear and adopting least squares loss (i.e., regression), we have for the dependence of loss on the activation of this unit,

$$
\delta_ {k} ^ {(\ell)} = \frac {\partial L}{\partial a _ {k} ^ {(\ell)}} = \frac {\partial L}{\partial \hat {y} _ {k}} = \frac {\mathrm {d} \left(\frac {1}{2} (\hat {y} _ {k} - y _ {k}) ^ {2}\right)}{\mathrm {d} \hat {y} _ {k}} = \hat {y} _ {k} - y _ {k}.
$$

Here, we use shorthand $a _ { k } ^ { ( \ell ) } = \hat { y } _ { k }$ , providing the $k$ th dimension of the prediction of the model. Although a regression problem, we’re imagining here that there are multiple regression targets (say, the height, weight and blood pressure of an individual). Here, $y _ { k }$ is the true target value for this data point. Note that this is for OLS. The expression would be different for a classification problem and negated log likelihood as the loss.

To compute the error $\delta _ { j } ^ { ( \ell ) }$ for a hidden unit $j$ in a layer $\ell$ , we again make use of the chain rule, and write:

$$
\delta_ {j} ^ {(\ell)} = \frac {\partial L}{\partial a _ {j} ^ {(\ell)}} = \sum_ {m = 1} ^ {M} \frac {\partial L}{\partial a _ {m} ^ {(\ell + 1)}} \frac {\partial a _ {m} ^ {(\ell + 1)}}{\partial a _ {j} ^ {(\ell)}}, \tag {4.15}
$$

where the summation runs over all of the $M$ nodes to which the node $j$ in layer $\ell$ sends connections, as seen in Figure 4.5. This expression recognizes that the activation value of this unit contributes only via its contribution to the activation value of each unit to which it is connected in the next layer. The first term in one of the products in the summation is the, typically non-linear, dependence between loss and activation value of a unit in the next layer. The second term in one of the products captures the relationship between this activation and the subsequent activation.

Now, we can simplify by noticing that:

$$
\frac {\partial L}{\partial a _ {m} ^ {(\ell + 1)}} = \delta_ {m} ^ {(\ell + 1)} \quad \left\{\text {b y} \right. \tag {4.16}
$$

$$
\frac {\partial a _ {m} ^ {(\ell + 1)}}{\partial a _ {j} ^ {(\ell)}} = \frac {\mathrm {d} h \left(a _ {j} ^ {(\ell)}\right)}{\mathrm {d} a _ {j} ^ {(\ell)}} \cdot w _ {m j} ^ {(\ell + 1)} = h ^ {\prime} \left(a _ {j} ^ {(\ell)}\right) \cdot w _ {m j} ^ {(\ell + 1)}. \quad \text {c h a i n r u l e} \rbrace \tag {4.17}
$$

Substituting, and pulling forward the derivative of the activation function, we can rewrite the expression for the error on a hidden unit $j$ in layer $\ell$ as:

$$
\delta_ {j} ^ {(\ell)} = h ^ {\prime} \left(a _ {j} ^ {(\ell)}\right) \sum_ {m = 1} ^ {M} w _ {m j} ^ {(\ell + 1)} \delta_ {m} ^ {(\ell + 1)}. \tag {4.18}
$$

This is very useful, and is the key insight in backpropagation. It means that the value of the errors can be computed by “passing back” (backpropagating) the errors for nodes farther up in the network! Since we know the values of $\delta$ for the final layer of output node, we can recursively apply Equation 4.18 to compute the values of $\delta$ for all the nodes in the network.

Remember that all of these calculations were done for a single input data point that generated the loss $L$ . If we were using SGD with mini-batches, then we would perform same calculation for each data point in mini-batch $B$ , and average the gradients as follows:

$$
\frac {\partial L}{\partial w _ {j m} ^ {(\ell)}} = \frac {1}{| B |} \sum_ {n \in B} \frac {\partial L _ {n}}{\partial w _ {j m} ^ {(\ell)}}, \tag {4.19}
$$

![](images/bd9584f00343b90bff86ed835fee9aa9165694b8f0d9d8ef744c8f610d0085b3.jpg)  
Figure 4.5: Summation over the nodes (blue) in layer $\ell + 1$ to which node $j$ in layer $\ell$ (gold) sends connections (green). Note: read this as $m$ and $m ^ { \prime }$ .

where $L _ { n }$ is the loss on example $n$ .

To solidify our understanding of the backpropagation algorithm, it can be useful to try a concrete example.

Example 4.1 (Backpropagation Example): Imagine the case of a simple two layer neural network as in Figure 4.3, with $K$ outputs (we denote them $\hat { y } _ { 1 }$ through $\hat { y } _ { K }$ for a given data point). We imagine this is a regression problem, but one with multiple dimensions to the output, and assume OLS. For a given data point, the loss is computed as:

$$
L = \sum_ {k = 1} ^ {K} \frac {1}{2} (\hat {y} _ {k} - y _ {k}) ^ {2},
$$

where we write $y _ { k }$ for the $k$ th dimension of the target value. For a unit in the hidden layer, with activation value $a$ , we make use of the sigmoid activation, with

$$
z = \sigma (a) = \frac {1}{1 + \exp (- a)},
$$

whose derivative is given by:

$$
\frac {\partial \sigma (a)}{\partial a} = \sigma (a) (1 - \sigma (a)).
$$

For an input data point $\mathbf { x }$ , we forward propagate through the network to get the activations of the hidden layer, and for each $m$ in this layer we have:

$$
a _ {m} ^ {(1)} = \sum_ {d = 0} ^ {D} w _ {m d} ^ {(1)} x _ {d},
$$

given weights $w _ { m 0 } ^ { ( 1 ) } , \ldots , w _ { m D } ^ { ( 1 ) }$ , and with output value from unit $m$ as,

$$
z _ {m} ^ {(1)} = \sigma (a _ {m} ^ {(1)}).
$$

We propagate these output values forward to get the outputs, and for each output unit $k$ , we have:

$$
\hat {y} _ {k} = \sum_ {m = 0} ^ {M} w _ {k m} ^ {(2)} z _ {m} ^ {(1)},
$$

where w k0 , . . . , $w _ { k 0 } ^ { ( 2 ) } , \ldots , w _ { k M } ^ { ( 2 ) }$ wkM are the weights for unit $k$ .

Now that we’ve propagated forward, we propagate our errors backwards! We start by computing the errors for the output layer as follows:

$$
\delta_ {k} ^ {(2)} = \frac {\partial L}{\partial \hat {y} _ {k}} = \hat {y} _ {k} - y _ {k}.
$$

We then backpropagate these errors back to each hidden unit $m$ in layer 1 as follows:

$$
\begin{array}{l} \delta_ {m} ^ {(1)} = \frac {\partial L}{\partial a _ {m} ^ {(1)}} = h ^ {\prime} (a _ {m} ^ {(1)}) \sum_ {k = 1} ^ {K} w _ {k m} ^ {(2)} \delta_ {k} ^ {(2)} \\ = \sigma (a _ {m} ^ {(1)}) (1 - \sigma (a _ {m} ^ {(1)})) \sum_ {k = 1} ^ {K} w _ {k m} ^ {(2)} (\hat {y} _ {k} - y _ {k}) \\ = z _ {m} ^ {(1)} (1 - z _ {m} ^ {(1)}) \sum_ {k = 1} ^ {K} w _ {k m} ^ {(2)} (\hat {y} _ {k} - y _ {k}). \\ \end{array}
$$

And now that we have our errors for the hidden and output layers, we can compute the derivative of the loss with respect to our weights as follows, for the $d$ th weight on the $m$ th unit in layer 1, and the $m$ th weight on the $k$ th unit in layer 2:

$$
\frac {\partial L}{\partial w _ {m d} ^ {(1)}} = \delta_ {m} ^ {(1)} x _ {d}, \quad \frac {\partial L}{\partial w _ {k m} ^ {(2)}} = \delta_ {k} ^ {(2)} z _ {m} ^ {(1)}.
$$

We then use these derivatives along with an optimization technique such as stochastic gradient descent to improve the model weights.

# 4.5 Choosing a Network Structure

Now that we know the general form of a neural network and how the training process works, we must step back and consider the question of how we actually arrive at an optimal network structure. We’ll begin with an idea we’ve already seen before: cross validation.

# 4.5.1 Cross Validation for Neural Networks

We’ve previously discussed cross validation in the chapter on linear regression. We used it then to compare the performance of different models, attempting to identify the best model while also

![](images/61310e6d6135126e370fa7c369b6b75e2310b42e2f779716c077da4450c6017c.jpg)

![](images/5c4aed6bd0d0b0d2db2433d71667805ce7e9722edd67ac9915bb6a3b3e7f31b3.jpg)

![](images/4fa70cdcb769edd540c42de697447af20241a61aa586d773ca67da01562aebb1.jpg)  
Figure 4.6: Networks with different structures and numbers of internal nodes.

avoiding overfitting. We can use a similar process to identify a reasonable network structure.

First of all, the input and output parameters of a neural network are generally decided for us: the dimensionality of our input data dictates the number of input units and the dimensionality of the required output dictates the number of output units. For example, if we have an 8-by-8 pixel image and need to predict whether it is a ‘0’ or a ‘1’, our input dimensions are fixed at 64 and our output dimensions are fixed at 2. Depending on whether you wish to perform some sort of pre or post-processing on the inputs/outputs of your network, this might not actually be the case, but in general when choosing a network structure we don’t consider the first or last layer of nodes as being a relevant knob that we can tune.

That leaves us to choose the structure of the hidden layers in our network. Unsurprisingly, the more hidden layers we have and the more nodes we have in each of those layers, the more variation we will produce in our results and the closer we will come to overfitting.

Thus, we can use cross validation in the same way we’ve done before: train our model with differing numbers of internal units and structures (as in Figure 4.6) and then select the model that performs best on the validation set.

$\star$ There are other considerations at play beyond performance when choosing a network structure. For example, the more internal units you have in your network, the more storage and compute time you will need to train them. If either training time or response time after training a model is critical, you may need to consider consolidating your network at the expense of some performace.

# 4.5.2 Preventing Overfitting

Besides keeping your neural network small, there are other means of preventing it from overfitting.

# Regularization

You can also apply regularization to the weights in your network to help prevent overfitting. For example, we could introduce a simple quadratic regularizer of the form $\frac { \lambda } { 2 } \mathbf { w } ^ { T } \mathbf { w }$ to our objective function. There are other considerations to be made here, for example we would like our regularizer to be invariant to scaling, meaning that multiplying our input data by a constant would produce a proportionally equivalent network after training. The quadratic regularizer is not invariant to scaling, but the basic concept of avoiding extreme weights is the same nonetheless.

# Data Augmentation

We can use transformations to augment our data sets, which helps prevent overfitting. This technique is not specific to neural networks, but often the types of unstructured data for which we use

neural networks can benefit greatly from it.

Definition 4.5.1 (Data Augmentation): Data augmentation refers to the practice of increasing the size and diversity of your training data by applying transformations to the initial data set.

For example, if we are working with image data, we might choose to rotate or reflect the image, depending on the type of network we are trying to build and whether or not this would preserve the integrity of the image. We might also change something like the brightness or density of the image data. In this way, we can produce more and more varied training points, thus reducing the likelihood of overfitting.

# 4.6 Specialized Forms of Neural Networks

Simple neural networks are useful for a general set of tasks, and as universal function approximators, they could be useful for any task. However, there are certain data types and use cases for which we’ve developed more specialized forms of neural networks that perform even better in their respective domains. We will take a high level view of these different flavors of neural networks.

# 4.6.1 Convolutional Neural Networks (CNNs)

Convolutional neural networks (abbreviated CNNs) are most often used for image data, but their underlying principles apply in other domains as well.

To understand why a CNN is useful, consider this specific problem: you are trying to determine whether or not there is a dog in an image. There are two general difficulties we have to deal with in solving this problem. First, while dogs have a lot of similar features (ears, tails, paws, etc.), we need some means of breaking an image down into smaller pieces that we can identify as being ears or tails or paws. Second, what happens if we train on images of dogs that are all in the center of the photo, and then we try to test our network on an image where the dog is in the upper left hand corner? It’s going to fail miserably.

CNNs overcome these problems by extracting smaller local features from images via what’s known as a sliding window. You can imagine this sliding window as a matrix kernel that moves over every subsection of an image, producing a summary of those subsections that feed into the next layer in our network. We do this over the entire image, and with several different sliding windows. Without going into too many details, this solves the two general problems we had above: our small sliding window can summarize a feature of interest (such as a dog ear) and it is also location invariant, meaning that we can identify that dog ear anywhere in an image.

# 4.6.2 Recurrent Neural Networks (RNNs)

As with CNNs, recurrent neural networks (abbreviated RNNs) are used to more efficiently solve a specific problem type. To motivate the structure of an RNN, we will turn again to a specific example.

Imagine we were building a tool with the goal of predicting what word comes next in a newspaper article. Obviously, the words that came before the word that we are currently trying to predict are crucial to predicting the next word. Imagine we propagate the preceding ten words through our network to predict which word we think will come next. It would also be useful if we could send some of the information at each layer backwards through the network to help with the next prediction - since we know the sequence of words matters. In this sense, our network is ‘stateful’

![](images/434c9647fb323fc4a3fd0af00284d7044302f059b7300dca1f5da4948421171d.jpg)  
Figure 4.7: Excellent diagram of the structure of a CNN. source: https://www.mathworks.com/videos/introduction-to-deep-learning-what-are-convolutionalneural-networks–1489512765771.html

![](images/dbf640697a729f71d5b6c830614f5cc5e8920e07874e8d1e774ca840164b7a12.jpg)  
Figure 4.8: Simple example of an RNN.

because it’s remembering what came before. We don’t have this ability with a feed-forward network, which by design only propagates information forward through the network. RNNs add backward passing of activations into their network structure to improve predictions on data where there is some temporal dependence on what came previously.

# 4.6.3 Bayesian Neural Networks (BNNs)

Up until now, our training process has been one of maximum likelihood estimation, or a maximum posterior approach if we utilize a regularizer that can be interpreted as introducing a prior.

A Bayesian neural network, or BNN, does exactly what you might imagine: it introduces a distribution over the parameters of our model, which then requires marginalizing over those distributions in order to make a prediction. The specifics of how exactly a BNN is constructed are beyond the scope of this textbook, but the idea behind why we would utilize a BNN is the same as the reason we utilize Bayesian techniques in other domains, particularly the use of prior information to aid model performance.

# Chapter 5

# Support Vector Machines

In this chapter, we will explore what are known as a support vector machines, or SVMs for short. SVMs are broadly useful for problems in classification and regression, and they are part of a family of techniques known as margin methods. The defining goal of margin methods, and SVMs specifically, is to put as much distance as possible between data points and decision boundaries. We will dig deeper into what exactly this means over the course of the chapter. One of the most appealing aspects of SVMs is that they can be solved as convex optimization problems, for which we can find a global optimum with relative ease. We will explore the mathematical underpinnings of SVMs, which can be slightly more challenging than our previous topics, as well as their typical use cases.

# 5.1 Motivation

While SVMs can be used for classification or regression, we will reason about them in the classification case as it is more straightforward.

The grand idea behind SVMs is that we should construct a linear hyperplane in our feature space that maximally separates our classes, which means that the different classes should be as far from that hyperplane as possible. The distance of our data from the hyperplane is known as margin.

Definition 5.1.1 (Margin): Margin is the distance of the nearest data point from the separating hyperplane of an SVM model, as seen in Figure 5.1. Larger margins often lead to more generalizable models.

A larger margin tends to mean that our model will generalize better, since it provides more wiggle room to correctly classify unseen data (think about new data being a perturbation on current data).

This idea of the margin of a separator is quite intuitive. If you were presented with Figure 5.1 and were asked to separate the two classes, you would likely draw the line that keeps data points as far from it as possible. SVMs and other margin-based methods will attempt to algorithmically recreate this intuition.

# 5.1.1 Max Margin Methods

SVMs are a specific instance of a broader class of model known as max margin methods. Their name describes them well: they deal with creating a maximum margin between training data and

![](images/d492c8f064c6113f1046f70a607948ac727279453fbfc0f0e7fc4e88e2024d2d.jpg)  
Figure 5.1: Hyperplane with margin between different classes.

decision boundary, with the idea that this leads to model generalizability.

Other max margin methods are outside the scope of this textbook. These alternative methods may differ from SVMs in a non-trivial way. For example, SVMs do not produce probabilities on different classes, but rather decision rules for handling new data points. If you needed probabilities, there are other max margin methods that can be used for the task.

# ML Framework Cube: Support Vector Machines

SVMs are typically used in settings with discrete outputs. We need labeled training data to identify the relevant hyperplane in an SVM model. Finally, SVMs operate in a non-probabilistic setting.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Discrete</td><td>Supervised</td><td>No</td></tr></table>

# 5.1.2 Applications

The theory behind SVMs has been around for quite some time (since 1963), and prior to the rise of neural networks and other more computationally intensive techniques, SVMs were used extensively for image recognition, object categorization, and other typical machine learning tasks.

SVMs are still widely used in practice, for example for classification problems known as anomaly detection.

$\star$ The purpose of anomaly detection is to identify unusual data points. For example, if we are manufacturing shoes, we may wish to inspect and flag any shoe that seems atypical with respect to the rest of the shoes we produce.

Anomaly detection can be as simple as a binary classification problem where the data set is comprised of anomalous and non-anomalous data points. As we will see, an SVM can be constructed from this data set to identify future anomalous points very efficiently. SVMs extend beautifully to settings where we want to use basis functions, and thus non-linear interactions on features. For this reason, they continue to be competitive in many real-world situations where these kinds of interactions are important to work with.

# 5.2 Hard Margin Classifier for Linearly Separable Data

We will learn the theory behind SVMs by starting with a simple two-class classification problem, as we’ve seen several times in previous chapters. We will constrain the problem even further by assuming, at least to get started, that the two classes are linearly separable, which is the basis of the hard margin formulation for SVMs.

$\star$ The expression ‘hard margin’ simply means that we don’t allow any data to be classified incorrectly. If it’s not possible to find a hyperplane that perfectly separates the data based on class, then the hard margin classifier will return no solution.

# 5.2.1 Why the Hard Margin

The hard margin constraint, which assumes that our data is linearly separable, is not actually a requirement for constructing an SVM, but it simplifies the problem initially and makes our derivations significantly easier. After we’ve established the hard margin formulation, we will extend the technique to work in situations where our data is not linearly separable.

# 5.2.2 Deriving our Optimization Problem

Recall that our goal is to define a hyperplane that separates our data points and maintains the maximum possible distance between the hyperplane and nearest data points on either side of it. There are $N$ examples $\mathbf { x } _ { 1 } , . . . , \mathbf { x } _ { N }$ and there is a bias term $w _ { 0 }$ . Each example has a label $y _ { 1 } , . . . , y _ { N }$ which is either 1 or $^ { - 1 }$ . To uncover this hyperplane, we start with a simple linear model for a two-class classification problem:

$$
h (\mathbf {x}) = \mathbf {w} ^ {\top} \mathbf {x} + w _ {0}. \tag {5.1}
$$

This is the discriminant function and we classify a new example to class 1 or $- 1$ according to the sign produced by our trained model $h ( \mathbf { x } )$ . Later, we will also make this more general by using a basis function, $\phi ( \mathbf { x } )$ to transform to a higher dimensional feature space.

By specifying our model this way, we have implicity defined a hyperplane separating our two classes given by:

$$
\mathbf {w} ^ {\top} \mathbf {x} + w _ {0} = 0 \tag {5.2}
$$

Furthermore, we have that w is orthogonal to the hyperplane, which we demonstrate now:

Derivation 5.2.1 (Hyperplane Orthogonal to w): Imagine two data points $\mathbf { x } _ { 1 }$ and $\mathbf { x } _ { 2 }$ on the hyperplane defined by $\mathbf { w } ^ { \prime } \mathbf { x } + w _ { 0 } = 0$ . When we project their difference onto our model w, we find:

$$
\mathbf {w} ^ {\top} \left(\mathbf {x} _ {1} - \mathbf {x} _ {2}\right) = \mathbf {w} ^ {\top} \mathbf {x} _ {1} - \mathbf {w} ^ {\top} \mathbf {x} _ {2} = - w _ {0} - (- w _ {0}) = 0 \tag {5.3}
$$

which means that w is orthogonal to our hyperplane. We can visualize this in Figure 5.2.

Remember that we’re trying to maximize the margin between our training data and the hyperplane. The fact that w is orthogonal to our hyperplane will help with this.

To determine the distance between a data point $\mathbf { x }$ and the hyperplane, which we denote $d$ , we need the distance in the direction of w between the point and the hyperplane. We denote $\mathbf { x } _ { p }$ to be

![](images/1a960b68414f49059be53708675ace92691d8db5bfdd77e127d2171e840ad682.jpg)  
Figure 5.2: Our weight vector w is orthogonal to the separating hyperplane.

the projection of x onto the hyperplane, which allows us to decompose x as the following:

$$
\mathbf {x} = \mathbf {x} _ {p} + d \frac {\mathbf {w}}{\left| \left| \mathbf {w} \right| \right| _ {2}} \tag {5.4}
$$

which is the sum of the portion of the projection of $\mathbf { x }$ onto the hyperplane and the portion of $\mathbf { x }$ that is parallel to w (and orthogonal to the hyperplane). From here we can solve for $d$ :

Derivation 5.2.2 (Distance from Hyperplane Derivation): We start by left multiplying Equation 5.4 with w⊤,

$$
\mathbf {w} ^ {\top} \mathbf {x} = \mathbf {w} ^ {\top} \mathbf {x} _ {p} + d \frac {\mathbf {w} ^ {\top} \mathbf {w}}{| | \mathbf {w} | | _ {2}}.
$$

Simplifying (note that $\mathbf { w } ^ { \mid } \mathbf { x } _ { p } = - w _ { 0 }$ from Equation 5.3):

$$
\mathbf {w} ^ {\top} \mathbf {x} = - w _ {0} + d | | \mathbf {w} | | _ {2}
$$

Rearranging:

$$
d = \frac {\mathbf {w} ^ {\top} \mathbf {x} + w _ {0}}{| | \mathbf {w} | | _ {2}}.
$$

For each data point $\mathbf { x }$ , we now have the signed distance of that data point from the hyperplane.

For an example that is classified correctly, this signed distance $d$ will be positive for class $y _ { n } = 1$ , and negative for class $y _ { n } = - 1$ . Given this, we can make the distance unsigned (and always positive) for a correctly classified data point by multiplying by $y _ { n }$ . Then, the margin for an correctly classified data point $( x _ { n } , y _ { n } )$ is given by:

$$
\frac {y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right)}{\| \mathbf {w} \| _ {2}}. \tag {5.5}
$$

The margin for an entire data set is given by the margin to the closest point in the data set, and

$$
\min  _ {n} \frac {y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right)}{\| \mathbf {w} \| _ {2}}. \tag {5.6}
$$

Then, it is our goal to maximize this margin with respect to our model parameters w and $w _ { 0 }$ This is given by:

$$
\max  _ {\mathbf {w}, w _ {0}} \frac {1}{\left\| \mathbf {w} \right\| _ {2}} \left[ \min  _ {n} y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right) \right] \tag {5.7}
$$

Here, we pull the $1 / | | \mathbf { w } | | _ { 2 }$ term forward. Note carefully that $w _ { 0 }$ does not play a role in the denominator $| | \mathbf { w } | | _ { 2 }$ .

This is a hard problem to optimize, but we can make it more tractable by recognizing some important features of Equation 5.7. First, rescaling $\mathbf { w } \to \alpha \mathbf { w }$ and $w _ { 0 }  \alpha w _ { 0 }$ , for any $\alpha > 0$ , has no impact on the margin for any correctly classified data point $\mathbf { x } _ { n }$ . This is because the effect of $\alpha$ cancels out in the numerator and denominator of Equation 5.5.

We can use this rescaling liberty to enforce

$$
y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right) \geq 1, \quad \text {f o r a l l} n. \tag {5.8}
$$

This does not change the optimal margin because we can always scale up both w and $w _ { 0 }$ by $\alpha > 0$ to achieve $y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) \geq 1$ , and without affecting the margin. Moreover, since the problem is to maximize $1 / | | \mathbf { w } | | _ { 2 }$ , an optimal solution will want $| | \mathbf { w } | | _ { 2 }$ to be as small as possible, and thus at least one of these constraints (5.8) will be binding and equal to one in an optimal solution.

Thus our optimization problem now looks like:

$$
\max  _ {\mathbf {w}, w _ {0}} \frac {1}{\left\| \mathbf {w} \right\| _ {2}} \quad \text {s . t .} \quad y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right) \geq 1, \quad \text {f o r a l l} n. \tag {5.9}
$$

Here, we recognized that $\begin{array} { r } { \operatorname* { m i n } _ { n } y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) \ = \ 1 } \end{array}$ in an optimal solution with these new constraints, and adopted this in the objective. This simplifies considerably, removing the “min $_ n$ ” part of the objective.

Notice that maximizing 1w $\frac { 1 } { | | \mathbf { w } | | _ { 2 } }$ is equivalent to minimizing $| | \mathbf { w } | | _ { 2 } ^ { 2 }$ . We will also add a constant term $\begin{array} { l } { { \frac { 1 } { 2 } } } \end{array}$ for convenience, leaving the hard-margin formulation of the training problem:

$$
\min  _ {\mathbf {w}, w _ {0}} \frac {1}{2} | | \mathbf {w} | | _ {2} ^ {2} \quad \text {s . t .} \quad y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right) \geq 1, \quad \text {f o r a l l} n. \tag {5.10}
$$

Note that Equation 5.10 is now a quadratic programming problem, which means we wish to optimize a quadratic function subject to a set of linear constraints on our parameters. Arriving at this form was the motivation for the preceding mathematic manipulations. We will discuss shortly how we actually optimize this function.

# 5.2.3 What is a Support Vector

Up until now, we have discussed Support Vector Machines without identifying what a support vector is. We now have enough information from the previous section to define them. Later, when we work with a dual formulation, we will see the precise role that they play.

Say that an example is on the margin boundary if $y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) = 1$ . These are the points that are closest to the hyperplane and have margin $1 / | | \mathbf { w } | | _ { 2 }$ .

Definition 5.2.1 (Support Vector): A support vector in a hard-margin SVM formulation must be a data point that is on the margin boundary of the optimal solution, with $y _ { n } ( \mathbf { w } ^ { \mid } \mathbf { x } _ { n } + w _ { 0 } ) = 1$ and margin $1 / | | \mathbf { w } | | _ { 2 }$ .

![](images/f54f8260fc6cb4333e7c0b9289aee67dd683b473f00334b76ddb6402b391c942.jpg)  
Hard Margin SVM Example   
Figure 5.3: Example of the resulting hyperplane for a hard margin SVM. The filled in data points are support vectors in this example. A support vector for the hard-margin formulation must be on the margin boundary, with a discriminant value of +1 or -1.

In the hard margin case we have constrained the closest data points to have discriminant value $\mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } = 1$ ( $^ { - 1 }$ for a negative example). Figure 5.3 shows a hard margin SVM solution with an illustration of corresponding support vectors.

$\star$ After we have optimized an SVM in the hard margin case, we must have at least two support vectors with discriminant value that is 1 or -1, and thus a margin of $1 / | | \mathbf { w } | | _ { 2 }$ .   
$\star$ Not every example on the margin boundary needs to be a support vector.

# 5.3 Soft Margin Classifier

Thus far, we’ve been operating under the assumption that our data is linearly separable in feature space, which afforded us several convenient guarantees in the derivations of the previous section. For example, given that our data was linearly separable, we could guarantee that every data point would be on the correct side of the hyperplane, which waswhat allowed us to enforce the constraint that $y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) \geq 1$ . We now seek to generalize the work of the previous section to situations where our data is not separable.

# 5.3.1 Why the Soft Margin?

What if our data is not linearly separable in the feature space (even after applying a basis function)? This is the likely case in real applications! Unfortunately, our current hard margin SVM formulation would be useless with non-linearly separable data. That is why we need the soft margin SVM.

At a high level, the soft margin SVM allows for some of our data points to be closer to or even on the incorrect side of the hyperplane. This is desirable if our data set is not linearly separable or contains outliers, and it is also quite intuitive. Examining Figure 5.4, we see that we have a single outlier data point. We can still create a good model by just allowing this single data point to be

![](images/b3bc3e9cb4e95685efcd71b7a44c2f07b4c4202de538eab862e0f7e68055bdaa.jpg)  
Figure 5.4: An outlier can make the hard margin formulation impossible or unable to generalize well.

close to the hyperplane (or in some cases, even on the wrong side of the hyperplane). That is what the soft margin formulation will allow for.

# 5.3.2 Updated Optimization Problem for Soft Margins

To enable the soft margin formulation, we introduce what are known as slack variables denoted $\xi _ { n } \geq 0$ , which simply relax the constraint from Equation 5.9 that we imposed in the hard margin formulation. Say that a data point is “inside the margin region” if its discriminant value is in the range $( - 1 , + 1 )$ . There is a slack variable $\xi _ { n } \geq 0$ for every data point $\mathbf { x } _ { n }$ , and they take the following values according to how we classify $\mathbf { x } _ { n }$ :

$$
\xi_ {n} = \left\{ \begin{array}{l l} = 0 & \text {i f} \mathbf {x} _ {n} \text {i s c o r r e c t l y c l a s s i f i e d} \\ \in (0, 1 ] & \text {i f} \mathbf {x} _ {n} \text {i s c o r r e c t l y c l a s s i f i e d b u t i n s i d e t h e m a r g i n r e g i o n} \\ > 1 & \text {i f} \mathbf {x} _ {n} \text {i s i n c o r r e c t l y c l a s s i f i e d} \end{array} \right. \tag {5.11}
$$

These slack variable penalize data points on the wrong side of the margin boundary, but they don’t forbid us from allowing data points to be on the wrong side if this produces the best model. We now reformulate the optimization problem as follows. This is the soft-margin training problem:

$$
\min  _ {\mathbf {w}, w _ {0}} \frac {1}{2} \left\| \mathbf {w} \right\| _ {2} ^ {2} + C \sum_ {n = 1} ^ {N} \xi_ {n} \tag {5.12}
$$

$\begin{array} { r } { \mathrm { s . t . } \quad y _ { n } ( \mathbf w ^ { \top } \mathbf x _ { n } + w _ { 0 } ) \geq 1 - \xi _ { n } , \quad \mathrm { f o r ~ a l l } ~ n } \end{array}$

$$
\xi_ {n} \geq 0, \quad \text {f o r a l l} n.
$$

Here, $C$ is a regularization parameter that determines how heavily we penalize violations of the hard margin constraints. A large $C$ penalizes violation of the hard margin constraints more heavily, which means our model will follow the data closely and have small regularization. A small $C$ won’t heavily penalize having data points inside the margin region, relaxing the constraint and allowing our model to somewhat disregard more of the data. This means more regularization.

![](images/241894badbf0d7bbd9dc28b33ace014589634676070a5f1de72cd2dc3d71fee7.jpg)  
Soft Margin SVM Example   
Figure 5.5: Example of the resulting hyperplane for a soft margin SVM. The filled in data points illustrate the support vectors in this example and must be either on the margin boundary or on the “wrong side” of the margin boundary.

$\star$ Unlike most regularization parameters we’ve seen thus far, $C$ increases regularization as it gets smaller.

# 5.3.3 Soft Margin Support Vectors

Under the the hard margin formulation, the support vectors were some subset of the data points with $y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) = 1$ , and so situated exactly on the margin boundary, and they points closest to the hyperplane.

The support vectors in the soft margin case must be data points that are either on the margin boundary (and thus $y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) = 1$ ) or for which $\xi _ { n } \in ( 0 , 1 ]$ and in the margin region, or that are misclassified (when $\xi _ { n } > 1$ ). We can visualize this in Figure 5.5.

$\star$ Not every data point on the margin boundary, in the margin region, or that is misclassified needs to be a support vector in the soft-margin formulation. But those that become support vectors must meet one of these criteria.

# 5.4 Conversion to Dual Form

Now that we have the formulation of the optimization problem for SVMs, we need to discuss how we actually go about optimizing to produce a model solution. This will involve converting to a dual form of our problem. We will do this in the hard margin case for technical simplicity, but our solution can be easily modified to also apply to the soft margin formulation.

⋆ A dual form is an equivalent manner of representing an optimization problem, in this case the quadratic programming problem we need to optimize. Dual forms can be easier to work with than their initial form (“the primal form.”)

The dual form will be useful because it will allow us to bring in a basis function into the SVM formulation in a very elegant and computationally efficient way.

# 5.4.1 Lagrange Multipliers

Before we get into deriving the dual form, we need to be aware of a critical piece of math that will enable us to solve our optimization problem: Langrange multipliers.

A Lagrange multiplier is used to find optima of a function subject to certain constraints. This is exactly what we need to solve the optimization problem described by Equation 5.10.

The underlying theory behind Lagrange multipliers is not overly difficult to understand, but it is beyond the scope of this textbook in its fully generality. We will simply offer the method by which you can use them to solve the optimization problem of interest here.

Recall from the “Section 0 math” that we understood there how to use the Lagrangian method to solve optimization problems with equality constraints, for example of the form min $f ( \mathbf { w } )$ s.t. $g _ { n } ( \mathbf { w } ) =$ $0$ , for each constraint $n$ . There, a suitable Lagrangian function was $\begin{array} { r } { L ( \mathbf { w } , \alpha ) = f ( \mathbf { w } ) + \sum _ { n } \alpha _ { n } g _ { n } ( \mathbf { w } ) } \end{array}$ , where $\alpha _ { n } \in \mathbb { R }$ were the Lagrangian variables, and this problem could be solved via the partial derivatives of $L$ with respect to w and $\pmb { \alpha }$ , and setting them to zero.

Here, we have a slightly different problem to solve, which has the following form (recognizing that we can write a constraint $y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) \geq 1$ as $- y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) + 1 \leq 0$ , and thus as an “≤ 0” constraint):

$$
\min  _ {\mathbf {w}} f (\mathbf {w}) \quad \text {s . t .} g _ {n} (\mathbf {w}) \leq 0, \quad \text {a l l} n \tag {5.13}
$$

To solve this, we make use of the same kind of Lagrangian function,

$$
L (\mathbf {w}, \boldsymbol {\alpha}) = f (\mathbf {w}) + \sum_ {n} \alpha_ {n} g _ {n} (\mathbf {w}). \tag {5.14}
$$

But for the new “inequality form” of the constrained optimization problem, we also need to introduce a new subproblem, which for any fixed w, solves

$$
\max  _ {\boldsymbol {\alpha}} L (\mathbf {w}, \boldsymbol {\alpha}) \quad \text {s . t .} \quad \alpha_ {n} \geq 0 \quad \text {f o r a l l} n \tag {5.15}
$$

With this, we consider the problem

$$
\min  _ {\mathbf {w}} \left[ \max  _ {\boldsymbol {\alpha}, \boldsymbol {\alpha} \geq 0} f (\mathbf {w}) + \sum_ {n} \alpha_ {n} g _ {n} (\mathbf {w}) \right] \tag {5.16}
$$

Now, if w violates one or more constraints in (5.13), then the subproblem (5.15) becomes unbounded, with $\alpha _ { n }$ on the corresponding constraints driven arbitrarily large. Otherwise, if we have $g _ { n } ( \mathbf { w } ) < 0$ then we will have $\alpha _ { n } = 0$ , and we conclude $\alpha _ { n } g _ { n } ( \mathbf { x } ) = 0$ in all optimal solutions to (5.16). Therefore, and assuming that problem (5.13) is feasible, we have $L ( \mathbf { w } , \pmb { \alpha } ) = f ( \mathbf { w } )$ in an optimal solution to (5.16). Thus, we establish that (5.16) is an equivalent formulation to (5.13).

Substituting into our problem (5.10), the Lagrangian formulation becomes

$$
\min  _ {\mathbf {w}, w _ {0}} \left[ \max  _ {\boldsymbol {\alpha}, \boldsymbol {\alpha} \geq 0} \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {w} + \sum_ {n} \alpha_ {n} \left(- y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right) + 1\right) \right] \tag {5.17}
$$

Equivalently, it is convenient to write this as

$$
\min  _ {\mathbf {w}, w _ {0}} \left[ \max  _ {\boldsymbol {\alpha}, \boldsymbol {\alpha} \geq 0} \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {w} - \sum_ {n} \alpha_ {n} \left(y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right) - 1\right) \right], \tag {5.18}
$$

with Lagrangian function

$$
L (\mathbf {w}, \boldsymbol {\alpha}, w _ {0}) = \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {w} - \sum_ {n} \alpha_ {n} \left(y _ {n} \left(\mathbf {w} ^ {\top} \mathbf {x} _ {n} + w _ {0}\right) - 1\right). \tag {5.19}
$$

# 5.4.2 Deriving the Dual Formulation

Using this Lagrangian function, we allow ourselves to switch from solving Equation 5.10 to instead solving:

$$
\min  _ {\mathbf {w}, w _ {0}} \left[ \max  _ {\boldsymbol {\alpha}, \boldsymbol {\alpha} \geq 0} L (\mathbf {w}, \boldsymbol {\alpha}, w _ {0}) \right] \tag {5.20}
$$

$\star$ The ‘minw,w0 maxα≥0’ in Equation 5.20 may be initially confusing. The way to read this is that for any choice of $\mathbf { w }$ , $w _ { 0 }$ , the inner “max” problem then finds values of $\pmb { \alpha }$ to try to “defeat” the outer minimization objective.

We now wish to convert the objective in Equation 5.20 to a dual objective. Under the sufficient conditions of strong duality which hold for this problem because Equation 5.10 has a quadratic objective and linear constraints (but whose explanation is beyond the scope of this textbook), we can equivalently reformulate the optimization problem (5.20) as:

$$
\max  _ {\boldsymbol {\alpha}, \boldsymbol {\alpha} \geq 0} \left[ \min  _ {\mathbf {w}, w _ {0}} L (\mathbf {w}, \boldsymbol {\alpha}, w _ {0}) \right]. \tag {5.21}
$$

At this point, we can use first order optimality conditions to solve for w, i.e., the inner minimization problem, for some choice of $\pmb { \alpha }$ values. Taking the gradient, setting them equal to 0, and solving for w, we have:

$$
\nabla L (\mathbf {w}, \boldsymbol {\alpha}, w _ {0}) = \mathbf {w} - \sum_ {n = 1} ^ {N} \alpha_ {n} y _ {n} \mathbf {x} _ {n} = 0
$$

$$
\Leftrightarrow \quad \mathbf {w} ^ {*} = \sum_ {n = 1} ^ {N} \alpha_ {n} y _ {n} \mathbf {x} _ {n}. \tag {5.22}
$$

When we do the same thing for $w _ { 0 }$ , we find the following:

$$
\frac {\partial L (\mathbf {w} , \boldsymbol {\alpha} , w _ {0})}{\partial w _ {0}} = - \sum_ {n = 1} ^ {N} \alpha_ {n} y _ {n} = 0
$$

$$
\Leftrightarrow \quad \sum_ {n = 1} ^ {N} \alpha_ {n} y _ {n} = 0. \tag {5.23}
$$

This is interesting. If $\textstyle \sum _ { n } \alpha _ { n } y _ { n } \ Y $ , then $L ( \cdot )$ is increasing with $w _ { 0 }$ without bound, and the inner-minimization would choose $w _ { 0 } = - \infty$ and this choice of $\pmb { \alpha }$ cannot solve (5.21). If $\sum _ { n } \alpha _ { n } y _ { n } >$ $0$ , then $L ( \cdot )$ is decreasing with $w _ { 0 }$ without bound, and the inner-minimization would choose $w _ { 0 } =$ $+ \infty$ , and this choice of $\pmb { \alpha }$ cannot solve (5.21). We need $\begin{array} { r } { \sum _ { n } \alpha _ { n } y _ { n } \ = \ 0 } \end{array}$ for an optimal solution

to (5.21). So, we don’t yet obtain the optimal value for $w _ { 0 }$ , but we do gain a new constraint on the $\alpha$ -values that will need to hold in an optimal solution.

Now we substitute for w∗ into our Lagrangian function, and also assume (5.23), since this will be adopted as a new constraint in solving the optimization problem. Given this, we obtain:

$$
\begin{array}{l} L (\mathbf {w}, \boldsymbol {\alpha}, w _ {0}) = \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {w} - \mathbf {w} ^ {\top} \sum_ {n} \alpha_ {n} y _ {n} \mathbf {x} _ {n} - w _ {0} \sum_ {n} \alpha_ {n} y _ {n} + \sum_ {n} \alpha_ {n} \\ = - \frac {1}{2} \mathbf {w} ^ {\top} \mathbf {w} + \sum_ {n} \alpha_ {n} \\ = \sum_ {n} \alpha_ {n} - \frac {1}{2} \left(\sum_ {n} \alpha_ {n} y _ {n} \mathbf {x} _ {n}\right) ^ {\top} \left(\sum_ {n ^ {\prime}} \alpha_ {n ^ {\prime}} y _ {n ^ {\prime}} \mathbf {x} _ {n ^ {\prime}}\right) \tag {5.24} \\ \end{array}
$$

where the second equation follows from the first by using (5.22) and (5.23), and the third equation follows by using (5.22).

This is now entirely formulated in terms of $\pmb { \alpha }$ , and provides the hard margin, dual formulation:

$$
\max  _ {\boldsymbol {\alpha}} \sum_ {n} \alpha_ {n} - \frac {1}{2} \sum_ {n} \sum_ {n ^ {\prime}} \alpha_ {n} \alpha_ {n ^ {\prime}} y _ {n} y _ {n ^ {\prime}} \mathbf {x} _ {n} ^ {\top} \mathbf {x} _ {n ^ {\prime}} \tag {5.25}
$$

$$
\text {s . t .} \quad \sum_ {n} \alpha_ {n} y _ {n} = 0, \quad \text {f o r a l l n}
$$

$$
\alpha_ {n} \geq 0, \quad \text {f o r a l l n}
$$

Here, we add $\textstyle \sum _ { n } \alpha _ { n } y _ { n } \ = \ 0$ as a constraint. This is another quadratic objective, subject to linear constraints. This can be solved via SGD or another approach to solving convex optimization problems. With a little more work we can use the optimal $\pmb { \alpha }$ values to make predictions.

Although the derivation is out of scope for this textbook, there is also a very similar dual form for the soft-margin SVM training problem:

$$
\max  _ {\boldsymbol {\alpha}} \sum_ {n} \alpha_ {n} - \frac {1}{2} \sum_ {n} \sum_ {n ^ {\prime}} \alpha_ {n} \alpha_ {n ^ {\prime}} y _ {n} y _ {n ^ {\prime}} \mathbf {x} _ {n} ^ {\top} \mathbf {x} _ {n ^ {\prime}} \tag {5.26}
$$

$$
\text {s . t .} \quad \sum_ {n} \alpha_ {n} y _ {n} = 0, \quad \text {f o r a l l n}
$$

$$
C \geq \alpha_ {n} \geq 0, \quad \text {f o r a l l n}
$$

This puts an upper-bound on $\alpha _ { n }$ to prevent the dual from being unbounded in the case where the hard-margin SVM problem is infeasible because the data cannot be separated. It is not yet clear why any of this has been useful. We will see the value of the dual formulation when working with basis functions.

$\star$ By (5.22) we see how to find the weight vector w from a solution $\pmb { \alpha }$ . We didn’t yet explain how to find $_ { w _ { 0 } }$ . That will be explained next.

# 5.4.3 Making Predictions

Substituting the optimal dual solution into the discriminant function, we have

$$
h (\mathbf {x}) = \sum_ {n = 1} ^ {N} \alpha_ {n} y _ {n} \mathbf {x} _ {n} ^ {\top} \mathbf {x} + w _ {0}. \tag {5.27}
$$

For data points with $\alpha _ { n } > 0$ , this is taking a weighted vote over examples in the training data based on the size of the inner product $\mathbf { x } _ { n } ^ { \mid } \mathbf { x }$ .

Since $\pmb { \alpha }$ are Lagrange multipliers, they are non-negative, and moreover, by reasoning about the “max subproblem” in the min-max formulation (5.16), we know that they take on value zero whenever $y _ { n } h ( \mathbf x _ { n } ) > 1$ . The data points for which $\alpha _ { n } > 0$ are known as support vectors, and they will must be data points that are either on the margin boundary, inside the margin region, or misclassified. For the hard-margin formulation they must be data points on the margin boundary.

This is a major takeaway for the usefulness of SVMs: once we’ve trained our model, we can discard most of our data. We only need to keep the support vectors to make predictions. Soon we also see the “kernel trick.” This also illustrates why we need to solve for the values of $\pmb { \alpha }$ : those values dictate which data points are the support vectors for our model.

# Solving for $w _ { 0 }$

We can solve for $w _ { 0 }$ by recognizing that $y _ { n } ( \mathbf { w } ^ { \top } \mathbf { x } _ { n } + w _ { 0 } ) = 1$ for any data point on the margin boundary. For the hard-margin formulation we can solve for $w _ { 0 }$ using any example for which $\alpha _ { n } > 0$ . For the soft-margin formulation, it can be shown that the only points with $\alpha _ { n } = C$ are those inside the margin region or misclassified, and so that any point with $C > \alpha _ { n } > 0$ is on the margin boundary. Any such point can be solved to solve for $w _ { 0 }$ .

# 5.4.4 Why is the Dual Formulation Helpful?

Beyond having a succinct formulation of the discriminant function (assuming the number of support vectors is small), you might be wondering what exactly we gained by moving to the dual formulation of this problem.

First, the complexity of the optimization problem we’re solving changed from one that is dependent on the number of features $D$ , i.e., the size of the weight vector, to one that is linearly dependent on the number of data points $N$ . Thus, the number of variables to optimize over is now independent of the dimensionality of the feature space.

Second, in the dual formulation, we have the opportunity to take advantage of what’s known as the kernel trick to map our data $\mathbf { x } _ { n }$ into higher dimensions without incurring performance costs. This works as follows: notice that the only way in which a feature vector $\mathbf { x }$ is accessed during the training process (5.26) and at prediction time (5.27) is through the inner product $\mathbf { x } ^ { \top } \mathbf { z }$ between two data points. Suppose that we are using a basis function $\phi : \mathbb { R } ^ { D } \to \mathbb { R } ^ { M }$ , so that this becomes $\phi ( \mathbf { x } ) ^ { \top } \phi ( \mathbf { z } )$ . Now, define kernel function

$$
K (\mathbf {x}, \mathbf {z}) = \phi (\mathbf {x}) ^ {\top} \phi (\mathbf {z}). \tag {5.28}
$$

The idea of the kernel trick is that we might be able to compute $K ( \cdot , \cdot )$ without actually working in the basis function space, $\mathbb { R } ^ { M }$ , but rather be able to compute the Kernel function directly through algebra in the lower dimensional space, $\mathbb { R } ^ { D }$ .

For example, it can be shown that the polynomial kernel

$$
K _ {\text {p o l y}} (\mathbf {x}, \mathbf {z}) = (1 + \mathbf {x} ^ {\top} \mathbf {z}) ^ {q}, \quad \text {f o r i n t e g e r s} q \geq 2 \tag {5.29}
$$

corresponds to computing the inner product with a basis function that makes use of all terms up to degree $q$ . When $q = 2$ , then it is all constant, linear, and quadratic terms. The polynomial kernel function does this without needing to actually project the examples to the higher dimensional space. Rather it takes the inner product in the lower-dimensional space, adds 1 to this scalar, and then raises it to the power of $q$ . The implicit basis is growing exponentially large in $q$ !

$\star$ The kernel trick can even be used to work in an infinite basis. This is the case with the Gaussian kernel. If that is of interest, you should look into Taylor series basis expansions and the Gaussian kernel.

The importance of the kernel trick is that when computations can be done efficiently in the initial space $\mathbb { R } ^ { D }$ , then the training problem can be solved by computing the pairwise $K ( \cdot , \cdot )$ values for all pairs of training examples, and then using SGD to solve the dual, soft-margin training problem (with $N$ decision variables).

# 5.4.5 Kernel Composition

Now that we’ve seen the usefulness of the kernel trick for working in higher dimensional spaces without incurring performance costs or memory overhead, it’s reasonable to wonder what sort of valid kernels we can construct.

To be explicit, by ‘kernel’ we mean a function $K ( \mathbf { x } , \mathbf { z } ) = \phi ( \mathbf { x } ) ^ { \top } \phi ( \mathbf { z } )$ that produces a scalar product from two vectors, as determined by some basis function $\phi$ .

Although it is beyond the scope of this textbook, the condition for a kernel function $K ( \cdot , \cdot )$ to be valid is that the $N \times N$ matrix $\mathbf { K }$ , where element $K _ { n , n ^ { \prime } } = K ( { \bf x } _ { n } , { \bf x } _ { n ^ { \prime } } )$ should be positive semidefinite for any choices of the data set $\left\{ { \bf x } _ { n } \right\}$ . This is known as Mercer’s theorem.

$\star$ The matrix $\mathbf { K }$ of elements $K ( \mathbf { x } _ { n } , \mathbf { x } _ { n ^ { \prime } } )$ is known as the Gram matrix or Kernel matrix.

In practice, this provides for a logic for how different valid kernels can be composed (if they maintain a p.s.d. Gram matrix!).

There exists a set of rules that preserve the validity of kernels through transformations. These include such things as

• scalar multiplication $c \cdot K ( \cdot , \cdot )$   
• exponentiation $\exp \{ K ( \cdot , \cdot ) \}$   
• addition $K _ { 1 } ( { \bf x } , { \bf z } ) + K _ { 2 } ( { \bf x } , { \bf z } )$ , and   
• multiplication $K _ { 1 } ( { \bf x } , { \bf z } ) \cdot K _ { 2 } ( { \bf x } , { \bf z } )$ .

It is always possible to test the validity of a given kernel by demonstrating that its Gram matrix $\mathbf { K }$ is positive semidefinite.

# Chapter 6

# Clustering

In this chapter, we will explore a technique known as clustering. This represents our first foray into unsupervised machine learning techniques. Unlike the previous four chapters, where we explored techniques that assumed a data set of inputs and targets, with the goal of eventually making predictions over unseen data, our data set will no longer contain explicit targets. Instead, these techniques are motivated by the goal of uncovering structure in our data. Identifying clusters of similar data points is a useful and ubiquitous unsupervised technique.

# 6.1 Motivation

The reasons for using an unsupervised technique like clustering are broad. We often don’t have a specific task in mind; rather, we are trying to uncover more information about a potentially opaque data set. For clustering specifically, our unsupervised goal is to group data points that are similar.

There are many reasons why we might separate our data by similarity. For organizational purposes, it’s convenient to have different classes of data. It can be easier for a human to sift through data if it’s loosely categorized beforehand. It may be a preprocessing step for an inference method; for example, by creating additional features for a supervised technique. It can help identify which features make our data points most distinct from one another. It might even provide some idea of how many distinct data types we have in our set.

This idea of data being ‘similar’ means that we need some measure of distance between our data points. While there are a variety of clustering algorithms available, the importance of this distance measurement is consistent between them.

Distance is meant to capture how ‘different’ two data points are from each other. Then, we can use these distance measurements to determine which data points are similar, and thus should be clustered together. A common distance measurement for two data points $\mathbf { x }$ and $\mathbf { x } ^ { \prime }$ is given by:

$$
\left\| \mathbf {x} - \mathbf {x} ^ {\prime} \right\| _ {L 2} = \sqrt {\sum_ {d = 1} ^ {D} \left(\mathbf {x} _ {d} - \mathbf {x} _ {d} ^ {\prime}\right) ^ {2}} \tag {6.1}
$$

where $D$ is the dimensionality of our data. This is known as L2 or Euclidean distance, and you can likely see the similarity to L2 regularization.

There are a variety of distance measurements available for data points living in a $D$ -dimensional Euclidean space, but for other types of data (such as data with discrete features), we would need to select a different distance metric. Furthermore, the metrics we choose to use will have an impact on the final results of our clustering.

# ML Framework Cube: Clustering

In unsupervised learning, the domain refers to the domain of the hidden variable z (which is analogous to y in supervised learning). In clustering, we have z’s that represent the discrete clusters. Furthermore, the techniques that we explore in this chapter are fully unsupervised and non-probabilistic.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Discrete</td><td>Unsupervised</td><td>No</td></tr></table>

# 6.1.1 Applications

Here are a few specific examples of use cases for clustering:

1. Determining the number of phenotypes in a population.   
2. Organizing images into folders according to scene similarity.   
3. Grouping financial data as a feature for anticipating extreme market events.   
4. Identifying similar individuals based on DNA sequences.

As we mentioned above, there are different methods available for clustering. In this chapter, we will explore two of the most common techniques: K-Means Clustering and Hierarchical Agglomerative Clustering. We also touch on the flavors available within each of these larger techniques.

# 6.2 K-Means Clustering

The high level procedure behind K-Means Clustering (known informally as k-means) is as follows:

1. Initialize cluster centers by randomly selecting points in our data set.   
2. Using a distance metric of your choosing, assign each data point to the closest cluster.   
3. Update the cluster centers based on your assignments and distance metric (for example, when using L2 distance, we update the cluster centers by averaging the data points assigned to each cluster).   
4. Repeat steps 1 and 2 until convergence.

In the case where we are using the L2 distance metric, this is known as Lloyd’s algorithm, which we derive in the next section.

# 6.2.1 Lloyd’s Algorithm

Lloyd’s algorithm, named after Stuart P. Lloyd who first suggested the algorithm in 1957, optimizes our cluster assignments via a technique known as coordinate descent, which we will learn more about in later chapters.

# Derivation 6.2.1 (Lloyd’s Algorithm Derivation):

We begin by defining the objective used by Lloyd’s algorithm.

# Objective

The loss function for our current assignment of data points to clusters is given by:

$$
\mathcal {L} (\mathbf {X}, \left\{\boldsymbol {\mu} \right\} _ {c = 1} ^ {C}, \left\{\mathbf {r} \right\} _ {n = 1} ^ {N}) = \sum_ {n = 1} ^ {N} \sum_ {c = 1} ^ {C} r _ {n c} | | \mathbf {x} _ {n} - \boldsymbol {\mu} _ {c} | | _ {2} ^ {2} \tag {6.2}
$$

our data), where $\mathbf { X }$ is our $\left\{ \mu \right\} _ { c = 1 } ^ { C }$ $N \mathrm { x } D$ is the data set ( $C \mathrm { x } D$ matrix of cluster centers ( $N$ is the number of data points and $C$ is the number of clusters we chose), $D$ is the dimensionality of and $\left\{ \mathbf { r } \right\} _ { n = 1 } ^ { N }$ is our $N { \mathrm { x } } C$ matrix of responsibility vectors. These are one-hot encoded vectors (one per data point), where the 1 is in the position of the cluster to which we assigned the $n$ th data point.

We now define the algorithmic portion of Lloyd’s clustering procedure.

# Algorithm

We first adjust our responsibility vectors to minimize each data point’s distance from its cluster center. Formally:

$$
r _ {n c} = \left\{ \begin{array}{l l} = 1 & \text {i f} c = \underset {c ^ {\prime}} {\arg \min } \left| \left| \mathbf {x} _ {n} - \boldsymbol {\mu} _ {c ^ {\prime}} \right| \right| \\ = 0 & \text {o t h e r w i s e} \end{array} \right. \tag {6.3}
$$

After updating our responsibility vectors, we now wish to minimize our loss by updating our cluster centers $\pmb { \mu } _ { c }$ . The cluster centers which minimize our loss can be computed by taking the derivative of our loss with respect to $\pmb { \mu } _ { c }$ , setting equal to 0, and solving for our new cluster centers $\pmb { \mu } _ { c }$ :

$$
\frac {\partial \mathcal {L}}{\partial \boldsymbol {\mu} _ {c}} = - 2 \sum_ {n = 1} ^ {N} r _ {n c} (\mathbf {x} _ {n} - \boldsymbol {\mu} _ {c}) \tag {6.4}
$$

$$
\pmb {\mu} _ {c} = \frac {\sum_ {n = 1} ^ {N} r _ {n c} \mathbf {x} _ {n}}{\sum_ {n = 1} ^ {N} r _ {n c}}
$$

Intuitively, this is the average of all the data points $\mathbf { x } _ { n }$ assigned to the cluster center $\pmb { \mu } _ { c }$

We then update our responsibility vectors based on the new cluster centers, update the cluster centers again, and continue this cycle until we have converged on a stable set of cluster centers and responsibility vectors.

Note that while Lloyd’s algorithm is guaranteed to converge, it is only guaranteed to converge to a locally optimal solution. Finding the globally optimal set of assignments and cluster centers is an NP-hard problem. As a result, a common strategy is to execute Lloyd’s algorithm several times with different random initializations of cluster centers, selecting the assignment that minimizes loss across the different trials. Furthermore, to avoid nonsensical solutions due to scale mismatch between features (which would throw our Euclidean distance measurements off), it makes sense to standardize our data in a preprocessing step. This is as easy as subtracting the mean and dividing by the standard deviation across each feature.

# 6.2.2 Example of Lloyd’s

For some more clarity on exactly how Lloyd’s algorithm works, let’s walk through an example.

Example 6.1 (Lloyd’s Algorithm Example): We start with a data set of size $N = 6$ . Each data point is two-dimensional, with each feature taking on a value between -3 and 3. We also have a ‘Red’ and ‘Green’ cluster. Here is a table and graph of our data points, labelled A through F:

![](images/5e35293e71d0e714fd2509013d1a3d59b58cae5dcc104f4079ee850656f7f646.jpg)

<table><tr><td></td><td>Coordinates</td><td>Dist. to Red</td><td>Dist. to Green</td><td>Cluster Assignn.</td></tr><tr><td>A</td><td>(-3, -3)</td><td>n/a</td><td>n/a</td><td>n/a</td></tr><tr><td>B</td><td>(-1, -3)</td><td>n/a</td><td>n/a</td><td>n/a</td></tr><tr><td>C</td><td>(3, 0)</td><td>n/a</td><td>n/a</td><td>n/a</td></tr><tr><td>D</td><td>(-2, -1)</td><td>n/a</td><td>n/a</td><td>n/a</td></tr><tr><td>E</td><td>(0, 0)</td><td>n/a</td><td>n/a</td><td>n/a</td></tr><tr><td>F</td><td>(-1, -2)</td><td>n/a</td><td>n/a</td><td>n/a</td></tr></table>

Let’s say we wish to have 2 cluster centers. We then randomly initialize those cluster centers by selecting two data points. Let’s say we select B and F. We identify our cluster centers with a red and green ‘X’ respectively:

![](images/13e946b0abda2d10c36f3f48f83c5dda2f5ef38b6a203d49f6ff5336dab5dc14.jpg)

We now begin Lloyd’s algorithm by assigning each data point to its closest cluster center:

![](images/72e19f6c385004ffa095af7ed49659a899694a19a49922cd605c448c8d636e34.jpg)

<table><tr><td></td><td>Coordinates</td><td>Dist. to Red</td><td>Dist. to Green</td><td>Cluster Assignn.</td></tr><tr><td>A</td><td>(-3, -3)</td><td>n/a</td><td>n/a</td><td>Red</td></tr><tr><td>B</td><td>(-1, -3)</td><td>n/a</td><td>n/a</td><td>Red</td></tr><tr><td>C</td><td>(3, 0)</td><td>n/a</td><td>n/a</td><td>Green</td></tr><tr><td>D</td><td>(-2, -1)</td><td>n/a</td><td>n/a</td><td>Green</td></tr><tr><td>E</td><td>(0, 0)</td><td>n/a</td><td>n/a</td><td>Green</td></tr><tr><td>F</td><td>(-1, -2)</td><td>n/a</td><td>n/a</td><td>Green</td></tr></table>

We then update our cluster centers by averaging the data points assigned to each:

![](images/2fa4ce640f06e5ee7228b401d4ddcacdaa95677c26007d6148d94d5af4439776.jpg)

<table><tr><td></td><td>Coordinates</td><td>Dist. to Red</td><td>Dist. to Green</td><td>Cluster Assignn.</td></tr><tr><td>A</td><td>(-3, -3)</td><td>2.00</td><td>2.24</td><td>Red</td></tr><tr><td>B</td><td>(-1, -3)</td><td>0.00</td><td>1.00</td><td>Red</td></tr><tr><td>C</td><td>(3, 0)</td><td>5.00</td><td>4.47</td><td>Green</td></tr><tr><td>D</td><td>(-2, -1)</td><td>2.24</td><td>1.41</td><td>Green</td></tr><tr><td>E</td><td>(0, 0)</td><td>3.16</td><td>2.24</td><td>Green</td></tr><tr><td>F</td><td>(-1, -2)</td><td>1.00</td><td>0.00</td><td>Green</td></tr></table>

We proceed like this, updating our cluster centers and assignments, until convergence. At convergence, we’ve achieved these cluster centers and assignments:

![](images/38fbe0ce4673a43b219f28538b29709d1944f11e549239cfc4d57e42146d6514.jpg)

<table><tr><td></td><td>Coordinates</td><td>Dist. to Red</td><td>Dist. to Green</td><td>Cluster Assignn.</td></tr><tr><td>A</td><td>(-3, -3)</td><td>1.46</td><td>5.41</td><td>Red</td></tr><tr><td>B</td><td>(-1, -3)</td><td>1.06</td><td>3.91</td><td>Red</td></tr><tr><td>C</td><td>(3, 0)</td><td>5.26</td><td>1.50</td><td>Green</td></tr><tr><td>D</td><td>(-2, -1)</td><td>1.27</td><td>3.64</td><td>Red</td></tr><tr><td>E</td><td>(0, 0)</td><td>2.85</td><td>1.50</td><td>Green</td></tr><tr><td>F</td><td>(-1, -2)</td><td>0.79</td><td>3.20</td><td>Red</td></tr></table>

Where our red cluster is at (-1.75, -2.25) and our green cluster is at (1.5 , 0). Note that for this random initialization of cluster centers, we deterministically identified the locally optimal set of assignments and cluster centers. For a specific initialization, running Lloyd’s algorithm will always identify the same set of assignments and cluster centers. However, different initializations will produce different results

# 6.2.3 Number of Clusters

You may have wondered about a crucial, omitted detail: how do we choose the proper number of clusters for our data set? There doesn’t actually exist a ‘correct’ number of clusters. The fewer clusters we have, the larger our loss will be, and as we add more clusters, our loss will get strictly smaller. That being said, there is certainly a tradeoff to be made here.

Having a single cluster is obviously useless - we will group our entire data set into the same cluster. Having $N$ clusters is equally useless - each data point gets its own cluster.

One popular approach to identifying a good number of clusters is to perform K-Means with a varying number of clusters, and then to plot the number of clusters against the loss. Typically, that graph will look like Figure 6.1.

Notice that at $x = \ldots$ clusters, there appears to be a slight bend in the decrease of our loss. This is often called the knee, and it is common to choose the number of clusters to be where the knee occurs.

Intuitively, the idea here is that up to a certain point, adding another cluster significantly decreases the loss by more properly grouping our data points. However, eventually the benefit of adding another cluster stops being quite so significant. At this point, we have identified a natural number of groups for our data set.

# 6.2.4 Initialization and K-Means++

Up until now, we have assumed that we should randomly initialize our cluster centers and execute Lloyd’s algorithm until convergence. We also suggested that since Lloyd’s algorithm only produces a local minimum, it makes sense to perform several random initializations before settling on the most optimal assignment we’ve identified.

While this is a viable way to perform K-Means, there are other ways of initializing our original cluster centers that can help us find more optimal results without needing so many random initializations. One of those techniques is known as K-Means++.

The idea behind K-Means $^ { + + }$ is that our cluster centers will typically be spread out when we’ve reached convergence. As a result, it might not make sense to initialize those cluster centers in an entirely random manner. For example, Figure 6.2 would be a poor initialization.

![](images/e2b61d39b3c0d6d93f78caf84ae8f792a6bd59e67dddfdb5236fb743e680af95.jpg)  
Figure 6.1: Finding the Knee.

![](images/a9df8ad0248b29985dfed43fc07c249810bc2af954ef4e784e4b882b4215a28a.jpg)  
Figure 6.2: Bad Cluster Initialization.

![](images/03580bfa3df50adbd3e25958fa8a31713e874a6879ecb9d31a159d0e21bb20e4.jpg)  
Figure 6.3: Good Cluster Initialization.

We would much rather start with a random initialization that looks like Figure 6.3.

We can use the hint that we want our cluster centers somewhat spread out to find a better random initialization. This is where the initialization algorithm presented by K-Means $^ { + + }$ comes in.

For K-Means $^ { + + }$ , we choose the first cluster center by randomly selecting a point in our data set, same as before. However, for all subsequent cluster center initializations, we select points in our data set with probability proportional to the squared distance from their nearest cluster center. The effect of this is that we end up with a set of initializations that are relatively far from one another, as in Figure 6.3.

# 6.2.5 K-Medoids Alternative

Recall that in the cluster center update step (Equation 6.4) presented in the derivation of Lloyd’s algorithm, we average the data points assigned to each cluster to compute the new cluster centers. Note that in some cases, this averaging step doesn’t actually make sense (for example, if we have categorical variables as part of our feature set). In these cases, we can use an alternative algorithm known as $\mathbf { K }$ -Medoids. The idea behind K-Medoids is simple: instead of averaging the data points assigned to that cluster, update the new cluster center to be the data point assigned to that cluster which is most like the others.

# 6.3 Hierarchical Agglomerative Clustering

The motivating idea behind K-Means was that we could use a distance measurement to assign data points to a fixed number of clusters, iteratively improving our assignments and cluster locations

![](images/4e849f54190a7e7b8be998b452681d919b1004f90329b058946c6621c4ed9d21.jpg)  
Figure 6.4: Dendrogram Example.

until convergence.

Moving on to Hierarchical Agglomerative Clustering (also known as HAC - pronounced ‘hack’), the motivating idea is instead to group data from the bottom up. This means every data point starts as its own cluster, and then we merge clusters together based on a distance metric that we define. This iterative merging allows us to construct a tree over our data set that describes relationships between our data. These trees are known as dendrograms, with an example found in Figure 6.4. Notice that the individual data points are the leaves of our tree, and the trunk is the cluster that contains the entirety of our data set.

We now formally define the HAC algorithm, and in the process, explain how we construct such a tree.

# 6.3.1 HAC Algorithm

1. Start with $N$ clusters, one for each data point.   
2. Measure the distance between clusters. This will require an inter-cluster distance measurement that we will define shortly.   
3. Merge the two ‘closest’ clusters together, reducing the number of clusters by 1. Record the distance between these two merged clusters.   
4. Repeat step 2 until we’re left with only a single cluster.

In the remainder of the chapter, we’ll describe this procedure in greater detail (including how to measure the distance between clusters), explain the clustering information produced by the tree, and discuss how HAC differs from K-Means. But first, to make this algorithm a little more clear, let’s perform HAC one step at a time on a toy data set, constructing the dendrogram as we go.

Example 6.2 (HAC Algorithm Example): Let’s say we have a data set of five points A, B, C, D, E that we wish to perform HAC on. These points will simply be scalar data that we can represent on a number line. We start with 5 clusters and no connections at all:

![](images/bf85c68513b9d62b0c882af9e4db9f01fb8f625673e57843e8a87d681ed9bf76.jpg)

We find the closest two clusters to merge first. A and B are nearest (it’s actually tied with C and D, but we can arbitrarily break these ties), so we start by merging them. Notice that we also annotate the distance between them in the tree, which in this case is 1:

![](images/dbac375fb116c02158e18623fade0ec0eb6b643714174e9cbb47ca2138905dc7.jpg)

We now have four clusters: (A, B), C, D, and E. We again find the closest two clusters, which in this case is C and D:

![](images/35341ec1320736f352aff6cae45799d88deed4352575fed9adb4d62cb9a51d88.jpg)

We now have three remaining clusters: (A, B), (C, D), and E. We proceed as before, identifying the two closest clusters to be (A, B) and (C, D). Merging them:

![](images/40813af4c99146ee5fa9fc8708294e25192e6e4d1cb69d614b265e5f11b20375.jpg)

Finally we are left with two clusters: (A, B, C, D) and E. The remaining two clusters are obviously the closest together, so we merge them:

![](images/db3ff893b5cc07f9f83aee32a07dcba99315c9446de1378ee3c6373e0f862de6.jpg)

At this point there is only a single cluster. We have constructed our tree and are finished with HAC.

Notice how the distance between two merged clusters manifests itself through the height of the dendrogram where they merge (which is why we tracked those distances as we constructed the tree). Notice also that we now have many layers of clustering: if we’re only only interested in clusters whose elements are at least $k$ units away from each other, we can ‘cut’ the dendrogram at that height and examine all the clusters that exist below that cut point.

Finally, we need to handle the important detail of how to compute the distance between clusters. In the preceding example, we designated the distance between two clusters to be the minimum distance between any two data points in the clusters. This is what is known as the Min-Linkage Criterion. However, there are certainly other ways we could have computed the distance between clusters, and using a different distance measurement can produce different clustering results. We now turn to these different methods and the properties of clusters they produce.

# 6.3.2 Linkage Criterion

Here are a few of the most common linkage criteria.

# Min-Linkage Criteria

We’ve already seen the Min-Linkage Criterion in action from the previous example. Formally, the criterion says that the distance $d _ { C , C ^ { \prime } }$ between each cluster pair $C$ and $C ^ { \prime }$ is given by

$$
d _ {C, C ^ {\prime}} = \min  _ {k, k ^ {\prime}} | | \mathbf {x} _ {k} - \mathbf {x} _ {k ^ {\prime}} | | \tag {6.5}
$$

where $\mathbf { x } _ { k }$ are data points in cluster $C$ and $\mathbf { x } _ { k ^ { \prime } }$ are data points in cluster $C ^ { \prime }$ . After computing these pairwise distances, we choose to merge the two clusters that are closest together.

# Max-Linkage Criterion

We could also imagine defining the distance $d _ { C , C ^ { \prime } }$ between two clusters as being the distance between the two points that are farthest apart in each cluster. This is known as the Max-Linkage Criterion.

![](images/8ce4df7bfb3bfe267f133f38accbbe6ad03902fa5efff990d99b920e3da427bd.jpg)

![](images/1fd742b82c1ccc8b8690c2c953303d4c0c9af34240fee077662e62b443c74a3f.jpg)  
Figure 6.5: Different Linkage Criteria.

The distance between two clusters is then given by:

$$
d _ {C, C ^ {\prime}} = \max  _ {k, k ^ {\prime}} \left\| \mathbf {x} _ {k} - \mathbf {x} _ {k ^ {\prime}} \right\| \tag {6.6}
$$

As with the Min-Linkage Criterion, after computing these pairwise distances, we choose to merge the two clusters that are closest together.

$\star$ Be careful not to confuse the linkage criterion with which clusters we choose to merge. We always merge the clusters that have the smallest distance between them. How we compute that distance is given by the linkage criterion.

# Average-Linkage Criterion

The Average-Linkage Criterion averages the pairwise distance between each point in each cluster. Formally, this is given by:

$$
d _ {C, C ^ {\prime}} = \frac {1}{K K ^ {\prime}} \sum_ {k = 1} ^ {K} \sum_ {k ^ {\prime} = 1} ^ {K ^ {\prime}} \left\| \mathbf {x} _ {k} - \mathbf {x} _ {k ^ {\prime}} \right\| \tag {6.7}
$$

# Centroid-Linkage Criterion

The Centroid-Linkage Criterion uses the distance between the centroid of each cluster (which is the average of the data points in a cluster). Formally, this is given by:

$$
d _ {C, C ^ {\prime}} = \left\| \frac {1}{K} \sum_ {k = 1} ^ {K} \mathbf {x} _ {k} - \frac {1}{K ^ {\prime}} \sum_ {k ^ {\prime} = 1} ^ {K ^ {\prime}} \mathbf {x} _ {k ^ {\prime}} \right\| \tag {6.8}
$$

# Different Linkage Criteria Produce Different Clusterings

It’s important to note that the linkage criterion you choose to use will influence your final clustering results. For example, the min-linkage criterion tends to produce ‘stringy’ clusters, while the maxlinkage criterion tends to produce more compact clusters. You can see the difference between the results of these two linkage criteria in Figure 6.5.

$\star$ You should convince yourself of the different flavors of linkage criteria. For example, when using the min-linkage criterion, we get these ‘stringy’ results because we’re most inclined to extend existing clusters by grabbing whichever data points are closest.

# 6.3.3 How HAC Differs from K-Means

Now that we’re aware of two distinct clustering techniques and their variants, we consider the differences between the two methods.

First of all, there is a fundamental difference in determinism between HAC and K-Means. In general, K-Means incurs a certain amount of randomness and needs to be run multiple times to ensure a good result. On the other hand, once you’ve selected a linkage criterion for HAC, the clusters you calculate are deterministic. You only need to run HAC a single time.

Another difference between HAC and K-Means comes from the assumptions we make. For K-Means, we need to specify the number of clusters up front before running our algorithm, potentially using something like the knee-method to decide on the number of clusters. On the other hand, you don’t need to assume anything to run HAC, which simplifies its usage. However, the downside for HAC is that when you wish to present your final clustering results, you need to decide on the max distance between elements in each cluster (so that you can cut the dendrogram).

The fact that you need to make a decision about where to cut the dendrogram means that running HAC once gives you several different clustering options. Furthermore, the dendrogram in and of itself can be a useful tool for visualizing data. We don’t get the same interactivity from K-Means clustering.

$\star$ We often use dendrograms to visualize evolutionary lineage.

# Chapter 7

# Dimensionality Reduction

In previous chapters covering supervised learning techniques, we often used basis functions to project our data into higher dimensions prior to applying an inference technique. This allowed us to construct more expressive models, which ultimately produced better results. While it may seem counterintuitive, in this chapter we’re going to focus on doing exactly the opposite: reducing the dimensionality of our data through a technique known as Principal Component Analysis (PCA). We will also explore why it is useful to reduce the dimensionality of some data sets.

# 7.1 Motivation

Real-world data is often very high dimensional, and it’s common that our data sets contain information we are unfamiliar with because the dimensionality is too large for us to comb through all the features the by hand.

In these situations, it can be very difficult to manipulate or utilize our data effectively. We don’t have a sense for which features are ‘important’ and which ones are just noise. Fitting a model to the data may be computationally expensive, and even if we were to fit some sort of model to our data, it may be difficult to interpret why we obtain specific results. It’s also hard to gain intuition about our data through visualization since humans struggle to think in more than three dimensions. All of these are good reasons that we may wish to reduce the dimensionality of a data set.

# ML Framework Cube: Dimensionality Reduction

Dimensionality reduction operates primarily on continuous feature spaces, is fully unsupervised, and is non-probabilistic for the techniques we explore in this chapter.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Continuous</td><td>Unsupervised</td><td>No</td></tr></table>

While dimensionality reduction is considered an unsupervised technique, it might also be thought of as a tool used to make data more manageable prior to taking some other action. In fact, it’s an important preprocessing step for a host of use cases.

![](images/aa4f46a730ac3f288008a5815888590283ca117a2c97b60e2ebbbabc3424eb35.jpg)  
Bear Heights and Weights   
Figure 7.1: Graph of bear heights and weights.

# 7.2 Applications

As described above, we need a tool like dimensionality reduction in situations where high-dimensional data hinders us. Here are a few specific situations where we would use such a technique:

1. Presenting differences between complex molecules in two dimensions (via a graph).   
2. Understanding the results of a credit trustworthiness algorithm.   
3. Efficiently training a neural network to predict supermarket sales on a data set with many input features.   
4. Identifying which costly measurements are worth collecting when experimenting with new chemicals.

With a few of these use cases in mind, we now turn to the math that underpins the dimensionality reduction technique known as Priniciple Component Analysis.

# 7.3 Principal Component Analysis

The main idea behind Principal Component Analysis (PCA) is that we can linearly project our data set onto a subspace without losing too much information. For example, three-dimensional data might primarily exist in a subspace that is actually a two-dimensional plane.

One way to think about this is to identify and preserve the features along which there is the most variance. For example, imagine we had a data set comprised of the heights and weights of individual bears. As an extreme case, let’s suppose all the bears were exactly the same height but had a wide range of weights.

# Bear Weights

![](images/1188d03bb80fd607f2c8fad8773f61a5540bec9edadf9d82cf8ae7cc6f09a5a8.jpg)  
Figure 7.2: Bear weights.

# Converting Between Reduced and Original Data

![](images/b4250b09b3576e184d9243915530c019fca2031c6955181788248e62d2b1d6e1.jpg)  
Figure 7.3: Converting between the reduced data and original data.

To differentiate our data points, we obviously only need to report the weights of the bears. The variance of the heights is 0, and the variance of the weights is some non-zero number. Intuitively, the most interesting features from our data sets are those that vary the most.

$\star$ In this simple example, the direction of maximal variance occurs exactly along the $x _ { 1 }$ axis, but in general it will occur on a plane described by a combination of our input features.

The second way to think about PCA is that we are minimizing the error we incur when we move from the lower-dimensional representation back to the original representation. This is known as reconstruction loss. We can consider the meaning of this using our bear example.

Let’s say we project the data set from Figure 7.1 here down to a single dimension by recording only the weights:

Then, to reconstruct our original graph, we need only to keep track of a slope and bias term in the form of the familiar equation $x _ { 2 } = m x _ { 1 } + b$ . In this case our slope is $m = 0$ and our bias $b = 3$ . Note that this storage overhead is constant (just remembering the slope and bias) regardless of how big our data set gets. Thus we can go from our low-dimensional representation back to our original data:

It will be our goal to determine a low-dimensional representation of our data that allows us to return to our high-dimensional data while losing as little information as possible. We wish to preserve everything salient about the data while discarding as much redundant information as possible. We now turn to how this can be achieved.

# 7.3.1 Reconstruction Loss

We identified a key tenet of dimensionality reduction in the previous section: finding subspaces of our data that preserve as much information as possible. Concretely, this means we want to convert our original data point $\mathbf { x } _ { n }$ in $D$ dimensions into a data point $\mathbf { x } _ { n } ^ { \prime }$ in $D ^ { \prime }$ dimensions where $D ^ { \prime } < D$ .

![](images/5d8fd676a7064d4aad0265f4ee326bc8c918ef09ed8b581e1a4e701cf86ac6a3.jpg)  
Data Reconstruction   
Figure 7.4: Far left: our original data. Middle: our reduced data in scalar form after the projection given by $\mathbf{x} \cdot \mathbf{w}$ . Right: our reconstructed data points given by $(\mathbf{x} \cdot \mathbf{w})\mathbf{w}$ . Notice that our reconstructed data points are not the same as our original data.

$\star$ We're going to assume that our data set has been mean-centered such that each feature in $\mathbf{x}_n$ has mean 0. This will not affect the application of the method (we can always convert back to the uncentered data by adding back the mean of each feature), but will make our derivations more convenient to work with.

Let's consider a simple case first: $D' = 1$ . This means that we're projecting our $D$ dimensional data down onto just a single dimension, or in geometric terms, we're projecting our data points $\mathbf{x}_n$ onto a line through the origin. We can define this line as the unit vector $\mathbf{w} \in \mathbb{R}^{D \times 1}$ , and the projection is given by the dot product $\mathbf{x} \cdot \mathbf{w}$ .

$\star$ The unit vector $\mathbf{w}$ onto which we project our data is known as a principal component, from which PCA gets its name.

This projection produces a scalar, and that scalar defines how far our projection $\mathbf{x} \cdot \mathbf{w}$ is from the origin. We can convert this scalar back to $D$ dimensional space by multiplying it with the unit vector $\mathbf{w}$ . This means that $(\mathbf{x} \cdot \mathbf{w})\mathbf{w}$ is the result of projecting our data point $\mathbf{x}$ down into one-dimension and then converting it to its coordinate location in $D$ dimensions. We refer to these as our projection vectors, and we can observe what this looks like geometrically in Figure 7.4.

The projection vectors we recover from the expression $(\mathbf{x} \cdot \mathbf{w})\mathbf{w}$ will be in $D$ dimensions, but they will obviously not be identical to the original $D$ dimensional vectors (Figure 7.4 demonstrates why that is the case). This difference between the original and projection vectors can be thought of as error, since it is information lost from our original data. For a given data point $\mathbf{x}_n$ and unit vector $\mathbf{w}$ , we can measure this error through the expression:

$$
\left| \left| \mathbf {x} _ {n} - (\mathbf {x} \cdot \mathbf {w}) \mathbf {w} \right| \right| ^ {2} \tag {7.1}
$$

which is known as reconstruction loss because it measures the error incurred when reconstructing our original data from its projection.

Definition 7.3.1 (Reconstruction Loss): Reconstruction loss is the difference (measured via a distance metric such as Euclidean distance) between an original data set and its reconstruction from a lower dimensional representation. It indicates how much information is lost during dimensionality reduction.

Reconstruction loss is then a metric for evaluating how 'good' a subspace in $D'$ dimensions is

at representing our original data in $D$ dimensions. The better it is, the less information we lose, and the reconstruction loss is lower as a result.

# 7.3.2 Minimizing Reconstruction Loss

We now know that our goal is to find a good subspace to project onto, and we also know that finding this good subspace is equivalent to minimizing the reconstruction loss it incurs. We can now formalize this as an optimization problem.

First, we simplify the reconstruction loss for a single data point $\mathbf{x}_n$ as follows:

$$
\begin{array}{l} \left| \left| \mathbf {x} _ {n} - (\mathbf {x} _ {n} \cdot \mathbf {w}) \mathbf {w} \right| \right| ^ {2} = (\mathbf {x} _ {n} - (\mathbf {x} _ {n} \cdot \mathbf {w}) \mathbf {w}) (\mathbf {x} _ {n} - (\mathbf {x} _ {n} \cdot \mathbf {w}) \mathbf {w}) \\ = | | \mathbf {x} _ {n} | | ^ {2} - 2 (\mathbf {x} _ {n} \cdot \mathbf {w}) ^ {2} + (\mathbf {x} _ {n} \cdot \mathbf {w}) ^ {2} | | \mathbf {w} | | ^ {2} \\ = \left\| \mathbf {x} _ {n} \right\| ^ {2} - \left(\mathbf {x} _ {n} \cdot \mathbf {w}\right) ^ {2} \\ \end{array}
$$

where $||\mathbf{w}||^2 = 1$ because it is a unit vector. Note that we can define reconstruction loss over our entire data set as follows:

$$
R L (\mathbf {w}) = \frac {1}{N} \sum_ {n = 1} ^ {N} \left\| \mathbf {x} _ {n} \right\| ^ {2} - \left(\mathbf {x} _ {n} \cdot \mathbf {w}\right) ^ {2} \tag {7.2}
$$

Recall that our goal is to minimize reconstruction loss over our data set by optimizing the subspace defined by $\mathbf{w}$ . Let's first rewrite Equation 7.2 as:

$$
R L (\mathbf {w}) = \frac {1}{N} \sum_ {n = 1} ^ {N} | | \mathbf {x} _ {n} | | ^ {2} - \frac {1}{M} \sum_ {n = 1} ^ {N} (\mathbf {x} _ {n} \cdot \mathbf {w}) ^ {2}
$$

where we can see that our optimization will depend only on maximizing the second term:

$$
\max  _ {\mathbf {w}} \frac {1}{N} \sum_ {n = 1} ^ {N} \left(\mathbf {x} _ {n} \cdot \mathbf {w}\right) ^ {2} \tag {7.3}
$$

since it is the only one involving $\mathbf{w}$ . Recall that the sample mean of a data set is given by the expression $\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_n$ , and note that Equation 7.3 is the sample mean of $(\mathbf{x} \cdot \mathbf{w})^2$ . Using the definition of variance for a random variable $\mathbf{Z}$ (which is given by $Var(\mathbf{Z}) = \mathbb{E}(\mathbf{Z}^2) - (\mathbb{E}(\mathbf{Z}))^2$ ), we can rewrite Equation 7.3 as:

$$
\frac {1}{N} \sum_ {n = 1} ^ {N} (\mathbf {x} _ {n} \cdot \mathbf {w}) ^ {2} = V a r \big [ \{\mathbf {x} _ {n} \cdot \mathbf {w} \} _ {n = 1} ^ {N} \big ] + \left(\mathbb {E} \big [ \{\mathbf {x} _ {n} \cdot \mathbf {w} \} _ {n = 1} ^ {N} \big ]\right) ^ {2}
$$

Recall that we centered our data $\mathbf{x}_n$ to have mean 0 such that the expression above simplifies to:

$$
\frac {1}{N} \sum_ {n = 1} ^ {N} \left(\mathbf {x} _ {n} \cdot \mathbf {w}\right) ^ {2} = \operatorname {V a r} \left[ \left\{\mathbf {x} _ {n} \cdot \mathbf {w} \right\} _ {n = 1} ^ {N} \right] \tag {7.4}
$$

and therefore $Var\left[\{\mathbf{x}_n \cdot \mathbf{w}\}_{n=1}^N\} \right]$ is the term we wish to maximize. This means that minimizing the reconstruction loss is equivalent to maximizing the variance of our projections $\{\mathbf{x}_n \cdot \mathbf{w}\}_{n=1}^N$ .

$\star$ Note the intuitiveness of this result. We should like to find a subspace that maintains the spread in our data.

# 7.3.3 Multiple Principal Components

Up until now, we've been considering how we would project onto a single principal component $\mathbf{w} \in \mathbb{R}^{D \times 1}$ . This will reduce our data down to one dimension, just a scalar. In general, we will wish to preserve more of our data than just a single dimension (in order to capture more of the variance in our data and reduce reconstruction loss), which means that we will need to have multiple principal components. For now we'll assume that our principal components are orthogonal (we will prove this later), which then allows us to describe the projection of our data $\mathbf{x}_n$ onto this subspace as the sum of the projections onto $D'$ orthogonal vectors:

$$
\sum_ {d ^ {\prime} = 1} ^ {D ^ {\prime}} (\mathbf {x} _ {n} \cdot \mathbf {w} _ {d ^ {\prime}}) \mathbf {w} _ {d ^ {\prime}} \tag {7.5}
$$

# 7.3.4 Identifying Directions of Maximal Variance in our Data

We now know from the previous section that we find our principal components (and thus the subspace we will project onto) by identifying the directions of maximum variance in our projected data set. We know from Equation 7.4 that the variance is equivalent to:

$$
\sigma_ {\mathbf {w}} ^ {2} \equiv V a r \big [ \{\mathbf {x} _ {n} \cdot \mathbf {w} \} _ {n = 1} ^ {N} \big ] = \frac {1}{N} \sum_ {n = 1} ^ {N} (\mathbf {x} _ {n} \cdot \mathbf {w}) ^ {2}
$$

Rewriting this in terms of matrix notation we have that:

$$
\sigma_ {\mathbf {w}} ^ {2} = \frac {1}{N} (\mathbf {X w}) ^ {T} (\mathbf {X w})
$$

We can further simplify this:

$$
\sigma_ {\mathbf {w}} ^ {2} = \frac {1}{N} \mathbf {w} ^ {T} \mathbf {X} ^ {T} \mathbf {X} \mathbf {w}
$$

$$
\sigma_ {\mathbf {w}} ^ {2} = \mathbf {w} ^ {T} \frac {\mathbf {X} ^ {T} \mathbf {X}}{N} \mathbf {w}
$$

$$
\sigma_ {\mathbf {w}} ^ {2} = \mathbf {w} ^ {T} \mathbf {S} \mathbf {w}
$$

where, since we assume the design matrix $\mathbf{X}$ is mean-centered, then $\mathbf{S} = \frac{\mathbf{X}^T\mathbf{X}}{N}$ is the empirical covariance matrix of our data set.

$\star$ Notice that by convention we describe the empirical covariance of a data set with the term $\mathbf{S}$ instead of the usual covariance term $\Sigma$ .

Our goal is to maximize the term $\sigma_{\mathbf{w}}^2 = Var\big[\{\mathbf{x}_n\cdot \mathbf{w}\}_{n = 1}^N\big]$ with respect to $\mathbf{w}$ . Furthermore, $\mathbf{w}$ is a unit vector, so we must optimize subject to the constraint $\mathbf{w}^T\mathbf{w} = 1$ . Recalling the discussion of

Lagrange multipliers from Chapter 6 on Support Vector Machines, we incorporate this constraint by reformulating our optimization problem as the Lagrangian equation:

$$
\mathcal {L} (\mathbf {w}, \lambda) = \mathbf {w} ^ {T} \mathbf {S} \mathbf {w} - \lambda (\mathbf {w} ^ {T} \mathbf {w} - 1)
$$

As usual, we proceed by taking the derivative with respect to each parameter:

$$
\frac {d \mathcal {L} (\mathbf {w} , \lambda)}{d \mathbf {w}} = 2 \mathbf {S} \mathbf {w} - 2 \lambda \mathbf {w}
$$

$$
\frac {d \mathcal {L} (\mathbf {w} , \lambda)}{d \lambda} = \mathbf {w} ^ {T} \mathbf {w} - 1
$$

We can now set these equal to 0 and solve for the optimal values:

$$
\mathbf {S} \mathbf {w} = \lambda \mathbf {w}
$$

$$
\mathbf {w} ^ {T} \mathbf {w} = 1
$$

This result is very significant! As we knew already, we needed $\mathbf{w}$ to be a unit vector. However, we also see that $\mathbf{w}$ is an eigenvector of the empirical covariance matrix $\mathbf{w}$ . Furthermore, the eigenvector that will maximize our quantity of interest $\sigma_{\mathbf{w}}^2 = \mathbf{w}^T\mathbf{S}\mathbf{w}$ will be the eigenvector with the largest eigenvalue $\lambda$ .

Linear algebra gives us many tools for finding eigenvectors, and as a result we can efficiently identify our principal components. Note also that the eigenvectors of a symmetric matrix are orthogonal, which proves our earlier assumption that our principal components are orthogonal.

To recap, we've learned that the optimal principal components (meaning the vectors describing our projection subspace) are the eigenvectors of the empirical covariance matrix of our data set. The vector preserving the most variance in our data (and thus minimizing the reconstruction loss) is given by the eigenvector with the largest eigenvalue, followed by the eigenvector with the next largest eigenvalue, and so on. Furthermore, while it is somewhat outside the scope of this textbook, we are guaranteed to have $D$ distinct, orthogonal eigenvectors with eigenvalues $\geq 0$ . This is a result of linear algebra that hinges on the fact that our empirical covariance matrix $\mathbf{S}$ is symmetric and positive semi-definite.

# 7.3.5 Choosing the Optimal Number of Principal Components

We now know that the eigenvectors of the empirical covariance matrix $\mathbf{S}$ give us the principal components that form our projection subspace. Note that the exact procedure for finding these eigenvectors is a topic better suited for a book on linear algebra, but one approach is to use singular value decomposition (SVD).

The eigenvalue decomposition of a square matrix is $\mathbf{S} = \mathbf{V}\mathbf{L}\mathbf{V}^{\top}$ , where the columns of $\mathbf{V}$ are the eigenvectors of $\mathbf{S}$ , and $\mathbf{L}$ is a diagonal matrix whose entries are the corresponding eigenvalues $\{\lambda_i\}_i$ . The SVD of a real-valued, square or rectangular matrix $\mathbf{X}$ is $\mathbf{X} = \mathbf{U}\mathbf{Z}\mathbf{V}^{\top}$ with diagonal

![](images/8c3db8feffc4c3fb9d2188c4ade09198f2acac8d8b2a1744e17062d126943d86.jpg)  
Four Dimensional Iris Data Set in Three Dimensions   
Figure 7.5: PCA applied to Fisher's Iris data set, which is originally in four dimensions. We reduce it to three dimensions for visualization purposes and label the different flower types. This example is taken from the sklearn documentation.

matrix $\mathbf{Z}$ , and orthogonal matrices $\mathbf{U}$ and $\mathbf{V}$ (i.e., $\mathbf{U}^{\top}\mathbf{U} = \mathbf{I}$ and $\mathbf{V}^{\top}\mathbf{V} = \mathbf{I}$ ). Matrix $\mathbf{V}$ is the right singular matrix, and its columns the right singular vectors. The entries in $\mathbf{Z}$ are the corresponding singular values. The column vectors in $\mathbf{V}$ also the eigenvectors of $\mathbf{X}^{\top}\mathbf{X}$ . To see this, we have $\mathbf{X}^{\top}\mathbf{X} = \mathbf{V}\mathbf{Z}\mathbf{U}^{\top}\mathbf{U}\mathbf{Z}\mathbf{V}^{\top} = \mathbf{V}\mathbf{Z}^{2}\mathbf{V}^{\top}$ , where we substitute the SVD for $\mathbf{X}$ . We also see that the eigenvalues $\lambda_{i} = z_{i}^{2}$ , so that they are the squared singular values.

For a mean-centered design matrix $\mathbf{X}$ , so that $(1/N)\mathbf{X}^\top \mathbf{X} = \operatorname{cov}(\mathbf{X})$ , we can compute the SVD on the design matrix $\mathbf{X}$ , and then read off the eigenvectors of the covariance as the columns of the right singular matrix and the eigenvalues as $\lambda_i = z_i^2 / N$ . Alternatively, you can first divide the mean-centered design matrix by $\sqrt{N}$ before taking its SVD, in which case the square of the singular values corresponds to the eigenvalues.

Because the principal components are orthogonal, the projections they produce will be entirely uncorrelated. This means we can project our original data onto each component individually and then sum those projections to create our lower dimensional data points. Note that it doesn't make sense that we would use every one of our $D$ principal components to define our projection subspace, since that wouldn't lead to a reduction in the dimensionality of our data at all (the $D$ orthogonal principal components span the entire $D$ dimensional space of our original data set). We now need to decide how many principal components we will choose to include, and therefore what subspace we will be projecting onto.

The 'right' number of principal components to use depends on our goals. For example, if we simply wish to visualize our data, then we would project onto a 2D or 3D space. Therefore, we would choose the first 2 or 3 principal components, and project our original data onto the subspace defined by those vectors. This might look something like Figure 7.5.

However, it's more complicated to choose the optimal number of principal components when our goal is not simply visualization. We're now left with the task of trading off how much dimensionality

# Reconstruction Loss vs. Number of Principal Components

![](images/a6906e31fd3a10d4c53adb3edcfe08ee5982578358d6c87c6652bb4c601e53fd.jpg)  
Figure 7.6: Reconstruction loss versus the number of principal components. Notice the similarity to the 'elbow' method of K-Means clustering.

reduction we wish to achieve with how much information we want to preserve in our data.

One way to do this is similar to the informal 'elbow' method described for K-Means clustering. We graph our reconstruction loss against the number of principal components used, as seen in Figure 7.6. The idea is to add principal components to our subspace one at a time, calculating the reconstruction loss as we go. The first few principal components will greatly reduce the reconstruction loss, before eventually leveling off. We can identify the 'elbow' where the reduction in loss starts to diminish, and choose to use that number of principal components.

Another way to do this is to consider how much variance we wish to preserve in our data. Each principal component is associated with an eigenvalue $\lambda_{d}$ that indicates what proportion of the variance that principal component is responsible for in our data set. Then the fraction of variance retained from our data set if we choose to keep $D^{\prime}$ principal components is given by:

$$
\text {r e t a i n e d} = \frac {\sum_ {d ^ {\prime} = 1} ^ {D ^ {\prime}} \lambda_ {d ^ {\prime}}}{\sum_ {d = 1} ^ {D} \lambda_ {d}} \tag {7.6}
$$

For different applications, there may be different levels of acceptable variance retention, which can help us decide how many principal components to keep.

Finally, once we have selected our principal components, we have also defined the subspace onto which we will be projecting our original data. And although this subspace is defined by the basis given by our principal components, these principal components are not a unique description of that subspace. We could choose to use any basis after we've identified our subspace through the principal components. The importance of this idea is simply that although our principal components are unique, they are not the only basis we could use to define the same projection subspace.

# 7.4 Conclusion

Principal component analysis is useful for visualization purposes, removing redundant information, or making a data set more computationally manageable. PCA is also a good tool for data exploration, particularly when digging into an unfamiliar data set for the first time.

It is not essential that we know by heart the exact derivation for arriving at the principal components of a data set. The same can be said of the linear algebra machinery needed to compute principal components. However, it is important to have an intuitive grasp over how variance in our data set relates to principal components, as well as an understanding of how subspaces in our data can provide compact representations of that data set. These are critical concepts for working effectively with real data, and they will motivate related techniques in machine learning.

# Chapter 8

# Graphical Models

Mathematics, statistics, physics, and other academic fields have useful notational systems. As a hybrid of these and other disciplines, machine learning borrows from many of the existing systems. Notational abstractions are important to enable consistent and efficient communication of ideas, for both teaching and knowledge creation purposes. Much of machine learning revolves around modeling data processes, and then performing inference over those models to generate useful insights. In this chapter, we will be introducing a notational system known as the directed graphical model (DGM) that will help us reason about a broad class of models.

# 8.1 Motivation

Up until this point, we've defined notation on the fly, relied on common statistical concepts, and used diagrams to convey meaning about the problem setup for different techniques. We've built up enough working knowledge and intuition at this point to switch to a more general abstraction for defining models: directed graphical models (DGMS). DGMs will allow us to both consolidate our notation and convey information about arbitrary problem formulations. An example of what a DGM looks like for a linear regression problem setup is given in Figure 8.1, and over the course of the chapter, we'll explain how to interpret the symbols in this diagram.

We need graphical models for a couple of reasons. First, and most importantly, a graphical model unambiguously conveys a problem setup. This is useful both to share models between people (communication) and to keep all the information in a specific model clear in your own head (consolidation). Once we understand the meaning of the symbols in a DGM, it will be far easier to examine one of them than it will be to read several sentences describing the type of model we're imagining for a specific problem. Another reason we use DGMs is that they help us reason about independence properties between different parts of our model. For simple problem setups this may be easy to keep track of in our heads, but as we introduce more complicated models it will be useful to reason about independence properties simply by examining the DGM describing that model.

Ultimately, directed graphical models are a tool to boost efficiency and clarity. We'll examine the core components of a DGM, as well as some of their properties regarding independence and model complexity. The machinery we develop here will be used heavily in the coming chapters.

# 8.2 Directed Graphical Models (Bayesian Networks)

There are a few fundamental components to all of the models we've examined so far. In fact, we can model just about everything we've discussed to this point using just random variables, deterministic

![](images/421afa7721b14e104855c45a9b5831614d33861c565e264c2ce41618dd437b5a.jpg)

![](images/7d7a6a516d2ce3b7ca60157f68c96f2310c709446a4fecf2fa7a7773c9d50b5f.jpg)  
Figure 8.1: Linear regression model expressed with a DGM.   
Figure 8.2: Random variables are denoted with an open circle, and it is shaded if the variable is observed.

parameters, and arrows to indicate the relationships between them. Let's consider linear regression as a simple but comprehensive example. We have a random variable $y_{n}$ , the object of predictive interest, which depend on deterministic parameters in the form of data $\mathbf{x}_n$ and weights $\mathbf{w}$ . This results in the DGM given by Figure 8.1. There are four primary pieces of notation that the linear regression setup gives rise to, and these four components form the backbone of every DGM we would like to construct.

First, we the random variable $y_{n}$ represented by an open circle, shown in Figure 8.2. If we observe a random variable of a given model, then it is shaded. If the random variable is unobserved (sometimes called latent), it is unshaded.

Second, we have deterministic parameters represented by a tight, small dot, shown in Figure 8.3.

Third, we have arrows that indicate the dependence relationship between different random variables and parameters, shown in Figure 8.4. Note that an arrow from $X$ into $Y$ means that $Y$ depends on $X$ .

And finally, we have plate notation to indicate that we have repeated sets of variables in our model setup, shown in Figure 8.5.

![](images/97add7d5068667d9919585e55db9999911d8be63325a3a3006c61132c253f4fe.jpg)  
Figure 8.3: Deterministic parameters are denoted with a tight, small dot.

![](images/4a048e13a72ba51815bd97ded3dc87e418efcbb570d5bc6ce945efe49e31b33c.jpg)  
Figure 8.4: Arrows indicate dependence relationships.

![](images/103cc0dfc05fd344c1846b1ee2fb86af895934bb826e1edcee947bd3c1c94897.jpg)  
Figure 8.5: Plates indicate repeated sets of variables. Often there will be a number in one of the corners ( $N$ in this case) indicating how many times that variable is repeated.

![](images/d62129daff21e93526a0253d035924acf1dbcb5b38290941fd50ef720fa3aaca.jpg)  
Figure 8.6: DGM for the joint distribution give by Equation 8.1.

With these four constructs, we can describe complex model setups diagrammatically. We can have an arbitrary number of components and dependence structure. DGMs can be useful as a reference while working on a problem, and they also make it easy to iterate on an existing model setup.

# 8.2.1 Joint Probability Distributions

We'll now consider how DGMs simplify the task of reasoning about a joint probability distribution over multiple random variables. For any joint probability distribution, regardless of our knowledge about the dependence structure in the model, it's always valid to write a generic joint probability distribution as follows:

$$
p (A, B, C)
$$

where in this setup, we are interested in the joint distribution between three random variables $A, B, C$ . However, this doesn't tell us anything about the structure of the problem at hand. We would like to know where there is independence and use that to simplify our model. For example, if we knew that $B$ and $C$ were independent and we also knew the conditional distribution of $A|B, C$ then we would much rather setup our joint probability equation as:

$$
p (A, B, C) = p (B) p (C) p (A \mid B, C) \tag {8.1}
$$

DGMs assist in this process of identifying the appropriate factorization, as their structure allows us to read off valid factorizations directly. For example, the joint distribution given by Equation 8.1 can be read from Figure 8.6.

We can translate between a DGM and a factorized joint distribution by interpreting the arrows as dependencies. If a random variable has no dependencies (as neither $B$ nor $C$ do in this example), they can be written on their own as marginal probabilities $p(B)$ and $p(C)$ . Since arrows indicate dependence of the random variable at the tip on the random variable at the tail, the factorized joint probability distribution of the dependent random variable is written as a conditional probability, i.e. $P(A|B,C)$ in 8.6. In this way, we can move back and forth between DGMs and factorized joint distributions with ease.

# 8.2.2 Generative Models

DGMs also show us the process by which data comes into existence (sometimes referred to as the data generating story or generative process). Thus, a DGM conveys the information required to

![](images/f1eb432f1374c31330b33183c894361fddcdf1834d511b32ca655305e7c79851.jpg)  
Figure 8.7: Example of a generative model.

identify how the data are generated, and if we have the proper tools, how we could generate new data ourselves.

Definition 8.2.1 (Generative Models): A generative model describes the entire process by which data comes into existence. It enables the creation of new data by sampling from the generative model, but generative models are not required to make predictions or perform other kinds of inference.

$\star$ Note that we can create graphical models for both generative and discriminative models. Discriminative models will only model the conditional distribution $p(Y|Z)$ , while generative models will model the full joint distribution $p(Z,Y)$ .

Let's consider a simple example to see how this works in practice. Consider the flow of information present in Figure 8.7. First, there is some realization of the random variable $Z$ . Then conditioned on that value of $Z$ , there is some realization of the random variable $Y$ . The equivalent joint factorization for this DGM is given by $p(Z)p(Y|Z)$ . More intuitively, the data are created by first sampling from $Z$ 's distribution, and then based on the sampled value $Z = z$ , sampling from the conditional distribution $p(Y|Z = z)$ .

As a concrete example, let $Z$ be a random variable representing dog breed and $Y$ be a random variable representing snout length of a dog of a given breed, which is conditional on the breed of dog. Notice that we have not specified the distributional form of either $Z$ or $Y$ ; we only have the story of how they relate to each other (i.e. the dependence relation).

This story also shows us that if we had some model for $Z$ and $Y|Z$ , we could generate data points ourselves. Our procedure would simply be to sample from $Z$ and then to sample from $Y$ conditioned on that value of $Z$ . We could perform this process as many times as we like to generate new data. This is in contrast to sampling directly from the joint $p(Z,Y)$ , which is difficult if we do not know the exact form of the joint distribution (which we often do not) or if the joint distribution is difficult to sample from directly.

The technique of sampling from distributions in the order indicated by their DGM is known as ancestral sampling, and it is a major benefit of generative models.

Definition 8.2.2 (Ancestral Sampling): Ancestral sampling is a technique used to generate data from an arbitrary DGM. It works by sampling from the random variables in topological order, meaning that we first sample from all random variables without any dependence, then from the

![](images/1c39c2dd59f7687b78f4a4ede08d6771ef2ee76da07f28365e9829c7f73591a0.jpg)  
Figure 8.8: Notice we have to sample these in topological order: A, D, B, E, C.

random variables that depend on those initially sampled random variables, and so on until all the random variables have been sampled. This is demonstrated in Figure 8.8.

# 8.2.3 Generative Modeling vs. Discriminative Modeling

In the previous section, we described generative modeling, which describes how data come into existence. Another type of modeling is discriminative modeling, and we have already seen several examples.

A discriminative model directly models the dependence of a random variable on input random variables (i.e. it models the conditional $p(Y|Z)$ , skipping $p(Y,Z)$ ). For example, if we wish to predict what value a response variable $Y$ will take on, we can consider input values to be fixed parameters without an underlying distribution, and then our model is simply tasked with predicting the response variable based on those parameters. This is in contrast to a generative model, which models how all the values were generated by assigning a distribution to each of them.

To make the distinction more concrete, consider the DGM describing linear regression from Figure 8.1. Notice that the data points $\mathbf{x}_n$ are not random variables but instead fixed parameters of the DGM. To be a generative model, the DGM would need to have the form shown in Figure 8.9.

The difference between these model setups is significant. The generative model for linear regression would allow us to generate new input data points, but it is also significantly more complex to handle: instead of only predicting a single response variable based on the input values, we also model the generation of the input data points $\mathbf{x}_n$ . This may be difficult for both conceptual and computational reasons. This doesn't mean we'll never try to create generative models, but if our goal is only making predictions about the response variable $y_{n}$ , it is typically unnecessary and less effective to use a generative model. Another example of discriminative modeling include logistic regression, and oftentimes SVMs and neural networks are set up as discriminative models as well.

In essence, the distinction between generative and discriminative models lies in whether the model tries to describe how the data is realized or if the model only performs a specific inference task without modeling the entirety of the data generating process. Neither type is better; they are different techniques in your toolkit and should be used depending on your modeling and inferential

![](images/548d4c1c5e4836d8950bcccc77391ed82dc8c3709614a8703a4b12c435cd6101.jpg)  
Figure 8.9: Linear regression DGM, modeling $\mathbf{x}_n$ as a random variable.

needs.

# 8.2.4 Understanding Complexity

We've already motivated one of the primary uses of DGMs as being the ability to convert a joint distribution into a factorization. At the heart of that task was recognizing and exploiting independence properties in a model over multiple random variables. Another benefit of this process is that it allows us to easily reason about the size (also called 'complexity') of a joint factorization over discrete random variables. In other words, it allows us to determine how many parameters we will have to learn to describe the factorization for a given DGM.

Let us consider an example to make this concept clear. Suppose we have four categorical random variables $A, B, C, D$ which take on 2, 4, 8, and 16 values respectively. If we were to assume full dependence between each of these random variables, then a joint distribution table over all of these random variables would require $(2 * 4 * 8 * 16) - 1 = 1023$ total parameters (where each parameter corresponds to the probability of a specific permutation of the values $A, B, C, D$ ).

$\star$ Notice that the number of parameters we need is $(2*4*8*16) - 1$ and not $(2*4*8*16)$ . This is because if we know the first $(2*4*8*16) - 1$ parameters, the probability of the final combination of values is fixed since joint probabilities sum up to 1.

However, if we knew that some of these random variables were conditionally independent, then the number of parameters would change. For example, consider the joint distribution given by $p(A,B,C,D) = p(A)p(B|A)p(C|A)p(D|A)$ . This would imply that conditioned on $A$ , each of $B,C,D$ were conditionally independent. This can also be shown by the DGM in Figure 8.10.

In this case, a table of parameters to describe this joint distribution would only require $2 * ((4 - 1) + (8 - 1) + (16 - 1)) = 50$ parameters, which is significantly less. In general, the more conditional independence we can identify between random variables, the easier they are to model and compute.

![](images/c38a7dbda82bd0673094162a5421a77748562449f6329297451e5a4c39a07bb2.jpg)  
Figure 8.10: Conditioned on A, we have that B, C, and D are independent.

![](images/fb712f538bcf03b03ad2c9d69b7647dcf7ccf966f995bf5bdf32543bcfafd6d3.jpg)  
Figure 8.11: The three random variable relationships that will tell us about independence relationships.

# 8.2.5 Independence and D-Separation

Another major benefit of DGMs is the ability to visually reason about the independence properties of a given model. The form of a graphical model determines which variables are independent under specific observation assumptions. With just three basic cases of relationships between three random variables, we can reason about any arbitrarily complex model. The three cases are shown in Figure 8.11. For each of these cases, there is a notion of information (dependence) either flowing from one random variable to the other or being 'blocked' (implying independence) either by an observation or the absence of an observation. Specifically, case 1 and case 2 (left and middle DGMs) have information between nodes A and C 'blocked' by observing node B, and case 3 has information between nodes A and C 'blocked' by not observing node B.

To understand what it means to block information flow by observing a random variable, consider the first example random variable structure, shown in Figure 8.12.

We can factorize this as follows:

$$
p (A, B, C) = p (B) p (A | B) p (C | B)
$$

We know that for this case, A and C are not independent (note that we have not observed B). Therefore, we say that information flows from A to C. However, once we've observed B, we know that A and B are conditionally independent, which is shown in Figure 8.13.

We now say that the flow of information from A to C is 'blocked' by the observation of B. Intuitively, if we observe A but not B, then we have some information about the value of B, which

![](images/b2b31b3b2e27f95ac55c3b14854dcbc8a3d09939d43922ebceab491883b20cd5.jpg)  
Figure 8.12: First structure, unobserved.

![](images/3972de636359635f1a8240dc062724347b518c7e1703a205763539a710343f29.jpg)  
Figure 8.13: First structure, observed.

![](images/0c67bc65ae28373714dab3ddbb5aacae49dab6f12bbcb52fa2d4e7ee2fa06d9d.jpg)  
Figure 8.14: Second structure, unobserved.

also gives some information about the value of C. The same applies in the other direction: observing C but not B has implications on the distribution of B and A.

In the second random variable structure, shown in Figure 8.14, we again consider the unobserved case first.

This allows us to write the joint distribution as:

$$
p (A, B, C) = p (A) p (B | A) p (C | B)
$$

Again, A and C are dependent if we have not observed B. Information is flowing from A to C through B. However, once we've observed B, then A and C are again conditionally independent, shown in Figure 8.15.

The flow of information from A to C is 'blocked' by the observation of B. Intuitively, if we observed A but not B, we have some information about what B might be and therefore what C might be as well. The same applies in the other direction: observing C but not B.

Notice that these first two cases behave in the same manner: observing a random variable in between two other random variables 'blocks' information from flowing between the two outer random variables. In the third and final case the opposite is true. Not observing data in this case will 'block' information, and we will explain this shift through an idea known as 'explaining away'.

We have the third and final random variable structure, shown in Figure 8.16. We consider the unobserved case first.

In this setup, we say that information from A to C is being 'blocked' by the unobserved variable B. Thus A and C are currently conditionally independent. However, once we've observed B, as shown in Figure 8.17, the information flow changes.

Now, information is flowing between A and C through the observed variable B, making A and C conditionally dependent. This phenomenon, where the observation of the random variable in the middle creates conditional dependence is known as explaining away. The idea relies on knowledge

![](images/1a3778d9e20d4788d2b09dc1719b0ce254d23b778ab67b94f4b0a6237cb8e13a.jpg)  
Figure 8.15: Second structure, observed.

![](images/bd4104518f5cf6030eae1655fa794800454f38603b00cd1503de59cc685b838c.jpg)  
Figure 8.16: Third structure, unobserved.

![](images/81e5035d9d1f992db17927b980bf02384218950e5ee02fa9160be36d534fe031.jpg)  
Figure 8.17: Third structure, observed.

of the value for B giving information about how much A or C may have contributed to B adopting that value.

Consider the following example: let the random variables A correspond to whether or not it rained on a certain day, B correspond to the lawn being wet, and C correspond to the sprinkler being on. Let's say we observe B: the lawn is wet. Then, if we observe variable A: it has not rained today, we would infer that variable C has the value: the sprinkler has been on, because that's the only way for the lawn to be wet. This is the phenomenon of explaining away. Observing B unblocks the flow of information between A and C because we can now use an observation to 'explain' how B got its value, and therefore determine what the other unobserved value might have been.

Notice that we've only described three simple cases relating dependence relationships between random variables in a DGM, but with just these three cases, we can determine the dependence structure of any arbitrarily complicated DGM. We just have to consider how information flows from node to node. If information gets 'blocked' at any point in our DGM network because of an observation (or lack thereof), then we gain some knowledge about independence within our model.

Consider the dependence between random variables A and F in Figure 8.18. Initially, before any observations are made, we can see that A and F are dependent (information flows from A through B). However, after observing B, the nodes A and F become independent (because information blocked at both the observed B and the unobserved D). Finally, after observing D, dependence is restored between A and F because information flows from A through D.

![](images/cd96974963138b4d26ff08a3ebc5d45f7aec734d1521ece670d86610f38e8704.jpg)  
A∠F

![](images/2f155f1fc66327667c5175f6af5ff2fa19eeb3ef7cc2c59f3526035584e2459b.jpg)  
$A\bot F$

![](images/5c8ea088a1478beb549a22dfa827d17c37b6cde8711ff8e73c4482dcace8b7bc.jpg)  
A∠F   
Figure 8.18: Notice how the independence between A and F depends on observations made within the network.

![](images/081e63d60146feb964e7176f38b83837db751dc91f021118c695cb9a9a7c64a6.jpg)  
Figure 8.19: DGM for Naive Bayes problem setup.

$\star$ In some other resources, you'll come upon the idea of 'D-separation' or 'D-connection'. D-separation is simply applying the principles outlined above to determine if two nodes are independent, or D-separated. By contrast, two-nodes that are D-connected are dependent on one another.

# 8.3 Example: Naive Bayes

Recall from the chapter on classification the Naive Bayes model. As a quick recap, Naive Bayes makes the assumption that for a single observation coming from a specific class (for example, our classes could be different dog breeds), the data associated with that observation are independent (for example, the data could be fur color, snout length, weight, etc.).

We can set up the Naive Bayes problem specification using the DGM form, as we see in Figure 8.19.

Even if we hadn't heard of Naive Bayes before, we would understand this model and the implications of the model, simply by an examination of the DGM that describes it. We can directly read off the factorization described above that is the whole point of Naive Bayes:

$$
p \left(y _ {n}, x _ {n 1},..., x _ {n J}\right) = p \left(y _ {n}\right) p \left(x _ {n 1} \mid y _ {n}\right) \dots p \left(x _ {n J} \mid y _ {n}\right)
$$

Writing this factorization is facilitated directly by our DGM, even if we've never heard of Naive Bayes before. It provides a common language for us to move fluidly between detailed probability factorizations and general modeling intuition.

# 8.4 Conclusion

Directed graphical models are indispensable for model visualization and effective communication of modeling ideas. With an understanding of what DGMs represent, it's much easier to analyze more complex probabilistic models. In many ways, this chapter is preparation for where we head next. The topics of the following chapters will rely heavily on DGMs to explain their structure, use cases, and interesting variants.

# Chapter 9

# Mixture Models

The real world often generates observable data that falls into a combination of unseen categories. For example, at a specific moment in time I could record sound waves on a busy street that come from a combination of cars, pedestrians, and animals. If I were to try to model my data points, it would be helpful if I could group them by source, even though I didn't observe where each sound wave came from individually.

In this chapter we explore what are known as mixture models. Their purpose is to handle data generated by a combination of unobserved categories. We would like to discover the properties of these individual categories and determine how they mix together to produce the data we observe. We consider the statistical ideas underpinning mixture models, as well as how they can be used in practice.

# 9.1 Motivation

Mixture models are used to model data involving latent variables.

Definition 9.1.1 (Latent Variable): A latent variable is a piece of data that is not observed, but that influences the observed data. We often wish to create models that capture the behavior of our latent variables.

We are sometimes unable to observe all the data present in a given system. For example, if we measure the snout length of different animals but only get to see the snout measurements themselves, the latent variable would be the type of animal we are measuring for each data point. For most data generating processes, we will only have access to a portion of the data and the rest will be hidden from us. However, if we can find some way to also model the latent variables, our model will potentially be much richer, and we will also be able to probe it with more interesting questions. To build some intuition about latent variable models, we present a simple directed graphical model with a latent variable $\mathbf{z}_n$ in Figure 9.1.

One common means of modeling data involving latent variables, and the topic of this chapter, is known as a mixture model.

Definition 9.1.2 (Mixture Model): A mixture model captures the behavior of data coming from a combination of different distributions.

At a high level, a mixture model operates under the assumption that our data is generated by first sampling a discrete class, and then sampling a data point from within that category according

![](images/056ec306b9dea12dd834f954a3f83110955d01758025f3d65261f8a20528ef85.jpg)  
Figure 9.1: Directed graphical model with a latent variable $\mathbf{z}$ .

to the distribution for that category. For the example of animal snouts, we would first sample a species of animal, and then based on the distribution of snout lengths in that species, we would sample an observation to get a complete data point.

Probabilistically, sampling a class (which is our latent variable, since we don't actually observe it) happens according to a Categorical distribution, and we typically refer to the latent variable as $\mathbf{z}$ . Thus:

$$
p (\mathbf {z} = C _ {k}; \boldsymbol {\theta}) = \theta_ {k}
$$

where $C_k$ is class $k$ , and $\pmb{\theta}$ is the parameter to the Categorical distribution that specifies the probability of drawing each class. We write the latent variable in bold $\mathbf{z}$ because we will typically consider it to be one-hot encoded (of dimension $K$ , for $K$ classes). Then, once we have a class, we have a distribution for the observed data point coming from that class:

$$
p (\mathbf {x} | \mathbf {z} = C _ {k}; \mathbf {w})
$$

$\star$ The distribution given by $p(\mathbf{x}|\mathbf{z} = C_k;\mathbf{w})$ is known as the class-conditional distribution.

This distribution depends on the type of data we are observing, and is parameterized by an arbitrary parameter $\mathbf{w}$ whose form depends on what is chosen as the class-conditional distribution. For the case of snout lengths, and many other examples, this conditional distribution is often modeled using a Gaussian distribution, in which case our model is known as a Gaussian Mixture Model. We will discuss Gaussian Mixture Models in more detail later in the chapter.

If we can effectively model the distribution of our observed data points and the latent variables responsible for producing the data, we will be able to ask interesting questions of our model. For example, upon observing a new data point $\mathbf{x}'$ we will be able to produce a probability that it came from a specific class $\mathbf{z}' = C_k$ using Bayes' rule and our model parameters:

$$
p (\mathbf {z} ^ {\prime} = C _ {k} | \mathbf {x} ^ {\prime}) = \frac {p (\mathbf {x} ^ {\prime} | \mathbf {z} ^ {\prime} = C _ {k} ; \mathbf {w}) p (\mathbf {z} ^ {\prime} = C _ {k} ; \boldsymbol {\theta})}{\sum_ {k ^ {\prime}} p (\mathbf {x} ^ {\prime} | \mathbf {z} ^ {\prime} = C _ {k ^ {\prime}} ; \mathbf {w}) p (\mathbf {z} ^ {\prime} = C _ {k ^ {\prime}} ; \boldsymbol {\theta})}
$$

Furthermore, after modeling the generative process, we will be able to generate new data points by sampling from our categorical class distribution, and then from the class-conditional distribution for that category:

$$
\mathbf {z} \sim C a t (\boldsymbol {\theta})
$$

$$
\mathbf {x} \sim p (\mathbf {x} | \mathbf {z} = C _ {k}; \mathbf {w})
$$

Finally, it will also be possible for us to get a sense of the cardinality of $\mathbf{z}$ (meaning the number of classes our data falls into), even if that was not something we were aware of a priori.

# ML Framework Cube: Mixture Models

The classes of data $\mathbf{z}$ in a mixture model will typically be discrete. Notice also that this is an unsupervised technique: while we have a data set $\mathbf{X}$ of observations, our goal is not to make predictions. Rather, we are trying to model the generative process of this data by accounting for the latent variables that generated the data points. Finally, this is a probabilistic model both for the latent variables and for our observed data.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Discrete</td><td>Unsupervised</td><td>Yes</td></tr></table>

# 9.2 Applications

Since much of the data we observe in our world has some sort of unobserved category associated with it, there are a wide variety of applications for mixture models. Here are just a few:

1. Handwriting image recognition. The categories are given by the characters (letters, numbers, etc.) and the class-conditional is a distribution over what each of those characters might look like.   
2. Noise classification. The categories are given by the source of a noise (e.g. we could have different animal noises), and the class-conditional is a distribution over what the sound waves for each animal noise look like.   
3. Vehicle prices. The categories are given by the brand of vehicle (we could alternatively categorize by size, safety, year, etc.), and the class-conditional is a distribution over the price of each brand.

# 9.3 Fitting a Model

We've defined the general form of a mixture model: we have a distribution $p(\mathbf{z};\pmb{\theta})$ over our classes and a distribution $p(\mathbf{x}|\mathbf{z} = C_k;\mathbf{w})$ as our class-conditional distribution. A natural approach would be to compute the maximum likelihood values for our parameters $\pmb{\theta}$ and $\mathbf{w}$ . Let's consider how we might go about this for a mixture model.

# 9.3.1 Maximum Likelihood for Mixture Models

Our goal is to maximize the likelihood of our observed data. Because we don't actually observe the latent variables $\mathbf{z}_n$ which determine the class of each observed data point $\mathbf{x}_n$ , we can simply sum

over the possible classes for each of our $N$ data points as follows:

$$
p (\mathbf {X}; \boldsymbol {\theta}, \mathbf {w}) = \prod_ {n = 1} ^ {N} \sum_ {k = 1} ^ {K} p (\mathbf {x} _ {n}, z _ {n, k}; \boldsymbol {\theta}, \mathbf {w})
$$

This uses $p(\mathbf{x}_n; \boldsymbol{\theta}, \mathbf{w}) = \sum_k p(\mathbf{x}_n, z_{n,k}; \boldsymbol{\theta}, \mathbf{w})$ (marginalizing out over the latent class). Taking the logarithm to get our log-likelihood as usual:

$$
\log p (\mathbf {X}; \boldsymbol {\theta}, \mathbf {w}) = \sum_ {n = 1} ^ {N} \log \left[ \sum_ {k = 1} ^ {K} p \left(\mathbf {x} _ {n}, z _ {n, k}; \boldsymbol {\theta}, \mathbf {w}\right) \right] \tag {9.1}
$$

It may not be immediately obvious, but under this setup, the maximum likelihood calculation for our parameters $\pmb{\theta}$ and $\mathbf{w}$ is now intractable. The summation over the $K$ classes of our latent variable $\mathbf{z}_n$ , which is required because we don't actually observe those classes, is inside of the logarithm, which prevents us from arriving at an analytical solution (it may be helpful to try to solve this yourself, you'll realize that consolidating a summation inside of a logarithm is not possible). You could still try to use gradient descent, but the problem is non-convex and we'll see a much more elegant approach. The rest of this chapter will deal with how we can optimize our mixture model in the face of this challenge.

# 9.3.2 Complete-Data Log Likelihood

We have a problem with computing the MLE for our model parameters. If we only knew which classes our data points came from, i.e., if we had $\mathbf{z}_n$ for each example $n$ , then we would be able to calculate $\log p(\mathbf{x},\mathbf{z})$ with relative ease because we would no longer require a summation inside the logarithm:

$$
\begin{array}{l} \log p (\mathbf {X}, \mathbf {Z}) = \sum_ {n = 1} ^ {N} \log p \left(\mathbf {x} _ {n}, \mathbf {z} _ {n}; \boldsymbol {\theta}, \mathbf {w}\right) (9.2) \\ = \sum_ {n = 1} ^ {N} \log [ p (\mathbf {x} _ {n} | \mathbf {z} _ {n}; \mathbf {w}) p (\mathbf {z} _ {n}; \boldsymbol {\theta}) ] (9.3) \\ = \sum_ {n = 1} ^ {N} \log p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n}; \mathbf {w}\right) + \log p \left(\mathbf {z} _ {n}; \boldsymbol {\theta}\right) (9.4) \\ \end{array}
$$

Notice that because we've now observed $\mathbf{z}_n$ , we don't have to marginalize over its possible values. This motivates an interesting approach that takes advantage of our ability to work with $p(\mathbf{x},\mathbf{z})$ if we only knew $\mathbf{z}$ .

The expression $p(\mathbf{x}, \mathbf{z})$ is known as the complete-data likelihood because it assumes that we have both our observation $\mathbf{x}$ and the class $\mathbf{z}$ that $\mathbf{x}$ came from. Our ability to efficiently calculate the complete-data log likelihood $\log p(\mathbf{x}, \mathbf{z})$ is the crucial piece of the algorithm we will present to optimize our mixture model parameters. This algorithm is known as Expectation-Maximization, or EM for short.

# 9.4 Expectation-Maximization (EM)

The motivation for the EM algorithm, as presented in the previous section, is that we do not have a closed form optimization for our mixture model parameters due to the summation inside of the

logarithm. This summation was required because we didn't observe a crucial piece of data, the class $\mathbf{z}$ , and therefore we had to sum over its values.

EM uses an iterative approach to optimize our model parameters. It proposes a soft value for $\mathbf{z}$ using an expectation calculation (we can think about this as giving a distribution on $\mathbf{z}_n$ for each $n$ ), and then based on that proposed value, it maximizes the expected complete-data log likelihood with respect to the model parameters $\pmb{\theta}$ and $\mathbf{w}$ via a standard MLE procedure.

Notice that EM is composed of two distinct steps: an "E step" that finds the expected value of the latent class variables given the current set of parameters, and an "M step" that improves the model parameters by maximizing expected complete-data log likelihood given these soft assignments to class variables. These two steps give the algorithm its name, and more generally, this type of approach is also referred to as coordinate ascent. The idea behind coordinate ascent is that we can replace a hard problem (maximizing the log likelihood for our mixture model directly) with two easier problems, namely the E- and M-step. We alternate between the two easier problems, executing each of them until we reach a point of convergence or decide that we've done enough. We may also restart because EM will provide a local but not global optimum.

We'll walk through the details of each of these two steps and then tie them together with the complete algorithm.

$\star$ K-Means, an algorithm we discussed in the context of clustering, is also a form of coordinate ascent. K-Means is sometimes referred to as a "maximization-maximization" algorithm because we iteratively maximize our assignments (by assigning each data point to just a single cluster) and then update our cluster centers to maximize their likelihood with respect to the new assignments. That is, it does a "max" in place of the E-step, making a hard rather than soft assignment.

# 9.4.1 Expectation Step

The purpose of the E-step is to find expected values of the latent variables $\mathbf{z}_n$ for each example given the current parameter values. Let's consider what this looks like with a concrete example.

Let's say our data points $\mathbf{x}_n$ can come from one of three classes. Then, we can represent the latent variable $\mathbf{z}_n$ associated with each data point using a one-hot encoded vector. For example, if $\mathbf{z}_n$ came from class $C_1$ , we would denote this:

$$
\mathbf {z} _ {n} = \left[ \begin{array}{c} 1 \\ 0 \\ 0 \end{array} \right]
$$

As we've already explained, we don't know the true value of this latent variable. Instead, we will compute its conditional expectation based on the current setting of our model parameters and our observed data $\mathbf{x}_n$ . We denote the expectation of our latent variables as $\mathbf{q}_n$ , and we calculate them as follows:

$$
\mathbf {q} _ {n} = \mathbb {E} [ \mathbf {z} _ {n} | \mathbf {x} _ {n} ] = \left[ \begin{array}{l} p (\mathbf {z} _ {n} = C _ {1} | \mathbf {x} _ {n}; \boldsymbol {\theta}, \mathbf {w}) \\ p (\mathbf {z} _ {n} = C _ {2} | \mathbf {x} _ {n}; \boldsymbol {\theta}, \mathbf {w}) \\ p (\mathbf {z} _ {n} = C _ {3} | \mathbf {x} _ {n}; \boldsymbol {\theta}, \mathbf {w}) \end{array} \right] \propto \left[ \begin{array}{l} p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {1}; \mathbf {w}) p (\mathbf {z} _ {n} = C _ {1}; \boldsymbol {\theta}) \\ p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {2}; \mathbf {w}) p (\mathbf {z} _ {n} = C _ {2}; \boldsymbol {\theta}) \\ p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {3}; \mathbf {w}) p (\mathbf {z} _ {n} = C _ {3}; \boldsymbol {\theta}) \end{array} \right]
$$

The expectation of a 1-hot encoded vector is equivalent to a distribution on the values that the latent variable might take on. Notice that we can switch from proportionality in our $\mathbf{q}_n$ values to

actual probabilities by simply dividing each unnormalized value by the sum of all the unnormalized values. Then, our $\mathbf{q}_n$ values will look something like the following, where a larger number indicates a stronger belief that the data point $\mathbf{x}_n$ came from that class:

$$
\mathbf {q} _ {n} = \left[ \begin{array}{c} 0. 8 \\ 0. 1 \\ 0. 1 \end{array} \right]
$$

There are two important things to note about the expectation step. First, the model parameters $\pmb{\theta}$ and $\mathbf{w}$ are held fixed. We're computing the expectation of our latent variables based on the current setting of those model parameters. Those parameters are randomly initialized if this is our first time running the expectation step.

Second, we have a value of $\mathbf{q}_n$ for every data point $\mathbf{x}_n$ in our data set. As a result, $\mathbf{q}_n$ are sometimes called "local parameters," since there is one assigned to each data point. This is in contrast to our model parameters $\pmb{\theta}$ and $\mathbf{w}$ , which are "global parameters." The size of the global model parameters doesn't fluctuate based on the size of our data set.

After performing the E-step, we now have an expectation for our latent variables, given by $\mathbf{q}_n$ . In the maximization step, which we describe next, we use these $\mathbf{q}_n$ values to improve our global parameters.

# 9.4.2 Maximization Step

After the expectation step, we have a $\mathbf{q}_n\in [0,1]^K$ (and summing to one) associated with each data point $\mathbf{x}_n$ , which describes our belief that the data point came from each class $C_k$ . Now that we have these expected 'class assignments', it's possible for us to maximize the expected complete-data likelihood with respect to our model parameters $\pmb{\theta}$ and $\mathbf{w}$ .

Recall that optimizing our parameters using the complete-data log likelihood is tractable because we avoid summing over the classes inside of the logarithm. Although we do not have the actual complete-data, since we don't know the true $\mathbf{z}_n$ values, we now have a distribution over these latent variables (given by $\mathbf{q}_n$ ).

Notice that our $\mathbf{q}_n$ values are 'soft' assignments - meaning that unlike the $\mathbf{z}_n$ values, which are one-hot encodings of assignments to a class, the $\mathbf{q}_n$ values have a probability that a data point $\mathbf{x}_n$ came from each class. Recall the expression for complete-data log likelihood:

$$
\log p (\mathbf {X}, \mathbf {Z}) = \sum_ {n = 1} ^ {N} \log p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n}; \mathbf {w}\right) + \log p \left(\mathbf {z} _ {n}; \boldsymbol {\theta}\right) \tag {9.5}
$$

We work instead with the expected complete-data log likelihood, using $\mathbf{q}_n$ to provide the distribution on $\mathbf{z}_n$ for each example $n$ :

$$
\begin{array}{l} \operatorname {E} _ {\mathbf {z} _ {n} \mid \mathbf {x} _ {n}} [ \log p (\mathbf {X}, \mathbf {Z}) ] = \operatorname {E} _ {\mathbf {z} _ {n} \mid \mathbf {x} _ {n}} \left[ \sum_ {n = 1} ^ {N} \log p (\mathbf {x} _ {n} \mid \mathbf {z} _ {n}; \mathbf {w}) + \log p (\mathbf {z} _ {n}; \boldsymbol {\theta}) \right] (9.6) \\ = \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} | \mathbf {x} _ {n}} \left[ \log p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n}; \mathbf {w}\right) + \log p \left(\mathbf {z} _ {n}; \boldsymbol {\theta}\right) \right] (9.7) \\ = \sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {K} p \left(\mathbf {z} _ {n} = C _ {k} | \mathbf {x} _ {n}\right) \left(\log p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k}; w\right) + \log p \left(\mathbf {z} _ {n} = C _ {k}; \boldsymbol {\theta}\right)\right) (9.8) \\ = \sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {K} q _ {n, k} \left(\log p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k}; \mathbf {w}\right) + \log p \left(\mathbf {z} _ {n} = C _ {k}; \boldsymbol {\theta}\right)\right) (9.9) \\ \end{array}
$$

Notice the crucial difference between this summation and that of Equation 9.1: the summation over the classes is now outside of the logarithm! Recall that using the log-likelihood directly was intractable precisely because the summation over the classes was inside of the logarithm. This maximization became possible by taking the expectation over our latent variables (using the values we computed in the E-step), which moved the summation over the classes outside of the logarithm.

We can now complete the M-step by maximizing Equation 9.6 with respect to our model parameters $\pmb{\theta}$ and $\mathbf{w}$ . This has an analytical solution. We take the derivative with respect to the parameter of interest, set to 0, solve, and update the parameter with the result.

# 9.4.3 Full EM Algorithm

Now that we have a grasp on the purpose of the EM algorithm, as well as an understanding of the expectation and maximization steps individually, we are ready to put everything together to describe the entire EM algorithm.

1. Begin by initializing our model parameters $\mathbf{w}$ and $\pmb{\theta}$ , which we can do at random. Since the EM algorithm is performed over a number of iterative steps, we will denote these initial parameter values $\mathbf{w}^{(0)}$ and $\pmb{\theta}^{(0)}$ . We will increment those values as the algorithm proceeds.   
2. E-step: compute the values of $\mathbf{q}_n$ based on the current setting of our model parameters.

$$
\mathbf {q} _ {n} = \mathbb {E} [ \mathbf {z} _ {n} | \mathbf {x} _ {n} ] = \left[ \begin{array}{c} p (\mathbf {z} _ {n} = C _ {1} | \mathbf {x} _ {n}; \pmb {\theta} ^ {(i)}, \mathbf {w} ^ {(i)}) \\ \vdots \\ p (\mathbf {z} _ {n} = C _ {K} | \mathbf {x} _ {n}; \pmb {\theta} ^ {(i)}, \mathbf {w} ^ {(i)}) \end{array} \right] \propto \left[ \begin{array}{c} p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {1}; \mathbf {w} ^ {(i)}) p (\mathbf {z} _ {n} = C _ {1}; \pmb {\theta} ^ {(i)}) \\ \vdots \\ p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {K}; \mathbf {w} ^ {(i)}) p (\mathbf {z} _ {n} = C _ {K}; \pmb {\theta} ^ {(i)}) \end{array} \right]
$$

3. M-step: compute the values of $\mathbf{w}$ and $\pmb{\theta}$ that maximize our expected complete-data log likelihood for the current setting of the values of $\mathbf{q}_n$ :

$$
\mathbf {w} ^ {(i + 1)}, \boldsymbol {\theta} ^ {(i + 1)} \in \underset {\mathbf {w}, \boldsymbol {\theta}} {\arg \max } \mathbb {E} _ {\mathbf {Z} | \mathbf {X}} [ \log p (\mathbf {X}, \mathbf {Z}; \mathbf {w}, \boldsymbol {\theta}) ] \tag {9.10}
$$

4. Return to step 2, repeating this cycle until our likelihood converges. Note that the likelihood is guaranteed to (weakly) increase at each step using this procedure.

It is also typical to re-start the procedure because we are guaranteed a local but not global optimum.

# 9.4.4 The Math of EM

The above sections supply an intuitive sense of why EM should work in scenarios where the latent variables follow categorical distributions. We now supply a more rigorous proof that iteratively computing the quantities derived above will increase the data log likelihood in a more general setting.

# The Evidence Lower Bound (ELBO)

Despite our inability to directly optimize the observed data log likelihood, it remains the quantity of interest. Note the following equivalence:

$$
\begin{array}{l} \sum_ {n = 1} ^ {N} \log \left[ \sum_ {k = 1} ^ {K} p \left(\mathbf {x} _ {n}, \mathbf {z} _ {n} = C _ {k}; \boldsymbol {\theta}, \mathbf {w}\right) \right] = \sum_ {n = 1} ^ {N} \log \left[ \sum_ {k = 1} ^ {K} p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k}; \boldsymbol {\theta}, \mathbf {w}\right) p \left(\mathbf {z} _ {n} = C _ {k} \mid \boldsymbol {\theta}, \mathbf {w}\right) \right] (9.11) \\ = \sum_ {n = 1} ^ {N} \log \mathrm {E} _ {\mathbf {z} _ {n} \sim p (\mathbf {z} _ {n} | \boldsymbol {\theta})} [ p (\mathbf {x} _ {n} \mid \mathbf {z} _ {n}, \mathbf {w}) ] (9.12) \\ \end{array}
$$

We adopt this latter expression, since it also holds in cases where $\mathbf{z}_n$ is not discrete.

As touched on earlier, the main issues with directly optimizing this expression are as follows:

1. If we were to take a gradient, the distribution over which we're taking an expectation involves one of the parameters, $\theta$ , and hence computing the gradient with respect to $\theta$ is difficult.   
2. There's an expectation (previously a sum) inside the logarithm.

To solve this, we ultimately derive a lower bound on the observed data log likelihood which proves to be computationally tractable to optimize. First, let $q(\mathbf{z}_n)$ denote another probability distribution on $\mathbf{z}_n$ . Then:

$$
\begin{array}{l} \sum_ {n = 1} ^ {N} \log \left[ \sum_ {k = 1} ^ {K} p (\mathbf {x} _ {n}, \mathbf {z} _ {n} = C _ {k}; \boldsymbol {\theta}, \mathbf {w}) \right] \\ = \sum_ {n = 1} ^ {N} \log \left[ \sum_ {k = 1} ^ {K} p (\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k}; \boldsymbol {\theta}, \mathbf {w}) p (\mathbf {z} _ {n} = C _ {k} \mid \boldsymbol {\theta}, \mathbf {w}) \right] \\ = \sum_ {n = 1} ^ {N} \log \left[ \sum_ {k = 1} ^ {K} p (\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k}; \boldsymbol {\theta}, \mathbf {w}) p (\mathbf {z} _ {n} = C _ {k} \mid \boldsymbol {\theta}, \mathbf {w}) \cdot \frac {q \left(\mathbf {z} _ {n} = C _ {k}\right)}{q \left(\mathbf {z} _ {n} = C _ {k}\right)} \right] \\ = \sum_ {n = 1} ^ {N} \log \left[ \sum_ {k = 1} ^ {K} \frac {p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\theta} , \mathbf {w}\right) p \left(\mathbf {z} _ {n} = C _ {k} \mid \boldsymbol {\theta} , \mathbf {w}\right)}{q \left(\mathbf {z} _ {n} = C _ {k}\right)} \cdot q \left(\mathbf {z} _ {n} = C _ {k}\right) \right] \\ = \sum_ {n = 1} ^ {N} \log \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \frac {p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} ; \boldsymbol {\theta} , \mathbf {w}\right) p \left(\mathbf {z} _ {n} \mid \boldsymbol {\theta} , \mathbf {w}\right)}{q \left(\mathbf {z} _ {n}\right)} \right] \tag {9.13} \\ \end{array}
$$

The above derivation again restricts to discrete $\mathbf{z}_n$ , but the equivalence between the expressions in (9.12) and (9.13) is in fact more general, holding whenever $p$ is absolutely continuous with respect to the chosen $q$ .

In any case, we've now fixed the first of the two issues. By introducing the distribution $q$ , the expectation is no longer over some distribution depending on the parameter $\pmb{\theta}$ .

To fix the second issue, we must somehow pass the log into the expectation. This is accomplished using Jensen's inequality, at the cost of turning the equality into a lower bound:

$$
\begin{array}{l} \sum_ {n = 1} ^ {N} \log \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \frac {p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} ; \boldsymbol {\theta} , \mathbf {w}\right) p \left(\mathbf {z} _ {n} \mid \boldsymbol {\theta} , \mathbf {w}\right)}{q (\mathbf {z} _ {n})} \right] (9.14) \\ \geq \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \log \left(\frac {p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} ; \boldsymbol {\theta} , \mathbf {w}\right) p \left(\mathbf {z} _ {n} \mid \boldsymbol {\theta} , \mathbf {w}\right)}{q \left(\mathbf {z} _ {n}\right)}\right) \right] (9.15) \\ \end{array}
$$

In summary, the two issues with directly optimizing the observed data log likelihood were resolved as follows:

- Introducing auxiliary variables $q(\mathbf{z}_n)$ allow us to reparametrize the expectation to be over $q$ rather than $p(\mathbf{z}_n \mid \boldsymbol{\theta})$ .   
- Jensen's inequality allows the log to pass through the expectation.

However, this came at the cost of converting our objective into a lower bound of the original quantity, as well as introducing the new parameter that is the distribution $q$ .

It turns out that the iterative process given in the section above amounts to alternating between optimizing the parameters $\mathbf{w},\pmb{\theta}$ and the distribution $q$ .

# Optimization

Before delving into the optimization process, we first establish two identities involving the ELBO.

$$
\operatorname {E L B O} (\mathbf {w}, q) = \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \log \left(\frac {p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} ; \boldsymbol {\theta} , \mathbf {w}\right) p \left(\mathbf {z} _ {n} \mid \boldsymbol {\theta} , \mathbf {w}\right)}{q \left(\mathbf {z} _ {n}\right)}\right) \right] \tag {9.16}
$$

First,

$$
\begin{array}{l} \operatorname {E L B O} (\mathbf {w}, q) = \sum_ {n = 1} ^ {N} \left(\operatorname {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} [ \log (p (\mathbf {x} _ {n} \mid \mathbf {z} _ {n}; \boldsymbol {\theta}, \mathbf {w}) p (\mathbf {z} _ {n} \mid \boldsymbol {\theta}, \mathbf {w})) ] - \operatorname {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} [ \log q (\mathbf {z} _ {n}) ]\right) \\ = \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \log \left(p (\mathbf {x} _ {n} \mid \mathbf {z} _ {n}; \pmb {\theta}, \mathbf {w}) p (\mathbf {z} _ {n} \mid \pmb {\theta}, \mathbf {w})\right) \right] - \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \log q (\mathbf {z} _ {n}) \right] \\ = \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} [ \log p (\mathbf {x} _ {n}, \mathbf {z} _ {n} \mid \boldsymbol {\theta}, \mathbf {w}) ] - \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} [ \log q (\mathbf {z} _ {n}) ] \tag {9.17} \\ \end{array}
$$

The first term in (9.17) is the expected complete data log likelihood, and the second is the entropy of the distribution $q$ .

Second, using Bayes' rule:

$$
\begin{array}{l} \operatorname {E L B O} (\mathbf {w}, q) = \sum_ {n = 1} ^ {N} \operatorname {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \log \left(\frac {p (\mathbf {x} _ {n} \mid \boldsymbol {\theta} , \mathbf {w}) p (\mathbf {z} _ {n} \mid \mathbf {x} _ {n} ; \boldsymbol {\theta} , \mathbf {w})}{q (\mathbf {z} _ {n})}\right) \right] \\ = \sum_ {n = 1} ^ {N} \left(\log p (\mathbf {x} _ {n} \mid \boldsymbol {\theta}, \mathbf {w}) + \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} \left[ \log \frac {p (\mathbf {z} _ {n} \mid \mathbf {x} _ {n} ; \boldsymbol {\theta} , \mathbf {w})}{q (\mathbf {z} _ {n})} \right]\right) \\ = \sum_ {n = 1} ^ {N} \log p \left(\mathbf {x} _ {n} \mid \boldsymbol {\theta}, \mathbf {w}\right) - \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q \left(\mathbf {z} _ {n}\right)} \left[ \log \frac {q \left(\mathbf {z} _ {n}\right)}{p \left(\mathbf {z} _ {n} \mid \mathbf {x} _ {n} ; \boldsymbol {\theta} , \mathbf {w}\right)} \right] \tag {9.18} \\ \end{array}
$$

The first term is exactly the observed data log likelihood, and the second is known as the KL divergence between $q(\mathbf{z}_n)$ and $p(\mathbf{z}_n \mid \mathbf{x}_n; \boldsymbol{\theta}, \mathbf{w})$ . In particular, the KL-divergence between distributions $P$ and $Q$ is defined as

$$
\mathrm {D} _ {\mathrm {K L}} (P \| Q) = \mathrm {E} _ {x \sim P (x)} \left[ \log \frac {P (x)}{Q (x)} \right] \tag {9.19}
$$

The key property of KL-divergence is that it is nonnegative, that is $\mathrm{D}_{\mathrm{KL}}(P\| Q)\geq 0$ always, with equality if and only if $P = Q$ :

$$
\begin{array}{l} - \mathrm {D} _ {\mathrm {K L}} (P \| Q) = \mathrm {E} _ {x \sim P (x)} \left[ \log \frac {Q (x)}{P (x)} \right] \leq \log \mathrm {E} _ {x \sim P (x)} \left[ \frac {Q (x)}{P (x)} \right] \\ \leq \log \sum \frac {Q (x)}{P (x)} \cdot P (x) \\ \leq 0 \\ \end{array}
$$

where we have equality when the equality condition of Jensen's is met, meaning $Q(x) = cP(x)$ , which forces $Q(x) = P(x)$ since both are distributions. The proof above is only for discrete variables, but can again be extended to the continuous case.

Using this, we rewrite (9.18) as

$$
\operatorname {E L B O} (\mathbf {w}, q) = \sum_ {n = 1} ^ {N} \log p \left(\mathbf {x} _ {n} \mid \boldsymbol {\theta}, \mathbf {w}\right) - \sum_ {n = 1} ^ {N} \mathrm {D} _ {\mathrm {K L}} \left(q \left(\mathbf {z} _ {n}\right) \| p \left(\mathbf {z} _ {n} \mid \mathbf {x} _ {n}; \boldsymbol {\theta}, \mathbf {w}\right)\right) \tag {9.20}
$$

We now describe the E and M steps of the procedure. Begin with some random assignment of $\pmb{\theta}$ , $\mathbf{w}$ , and $q$ .

1. The E-step: Fixing the parameters $\pmb{\theta},\mathbf{w}$ , choose the distributions $q$ that maximizes $\mathrm{ELBO}(\mathbf{w},q)$ . Observe that only the second term of (9.20) depends on the distributions $q$ . Hence maximizing the ELBO is equivalent to minimizing the sum of KL divergences $\sum_{n = 1}^{N}\mathrm{D}_{\mathrm{KL}}(q(\mathbf{z}_n)\| p(\mathbf{z}_n\mid \mathbf{x}_n;\pmb {\theta},\mathbf{w}))$ . This immediately yields that

$$
q (\mathbf {z} _ {n}) = p (\mathbf {z} _ {n} \mid \mathbf {x} _ {n}; \boldsymbol {\theta}, \mathbf {w}).
$$

2. The M-step: Fixing the distributions $q$ , maximize the ELBO with respect to the parameters $\theta, \mathbf{w}$ .

Observe that only the first term of (9.17) depends on the parameters, and hence this is equivalent to maximizing the expected complete data log likelihood,

$$
\sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim q (\mathbf {z} _ {n})} [ \log p (\mathbf {x} _ {n}, \mathbf {z} _ {n} \mid \boldsymbol {\theta}, \mathbf {w}) ]. \tag {9.21}
$$

An equivalent formulation is then as follows. Define the auxiliary function $Q(\mathbf{w}, \boldsymbol{\theta} \mid \mathbf{w}^{\mathrm{old}}, \boldsymbol{\theta}^{\mathrm{old}})$ as

$$
Q (\mathbf {w}, \boldsymbol {\theta} \mid \mathbf {w} ^ {\text {o l d}}, \boldsymbol {\theta} ^ {\text {o l d}}) = \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim p (\mathbf {z} _ {n} | \mathbf {x} _ {n}; \mathbf {w} ^ {\text {o l d}}, \boldsymbol {\theta} ^ {\text {o l d}})} [ \log p (\mathbf {x} _ {n}, \mathbf {z} _ {n} \mid \mathbf {w}, \boldsymbol {\theta}) ] \tag {9.22}
$$

Then as before, begin with some random assignment of $\mathbf{w},\pmb {\theta},q$

1. The E-step: compute the auxiliary function $Q(\mathbf{w}, \boldsymbol{\theta}; \mathbf{w}^{\mathrm{old}}, \boldsymbol{\theta}^{\mathrm{old}})$ . Note that this essentially just requires computing the posterior distribution $p(\mathbf{z}_n \mid \mathbf{x}_n; \mathbf{w}^{\mathrm{old}}, \boldsymbol{\theta}^{\mathrm{old}})$ , as before.   
2. The M-step: maximize $Q(\mathbf{w}, \pmb{\theta}; \mathbf{w}^{\mathrm{old}}, \pmb{\theta}^{\mathrm{old}})$ with respect to $\mathbf{w}$ and $\pmb{\theta}$ .

Why are these equivalent? We've dropped the $q$ s by noting that at each step, they will just be $p(\mathbf{z}_n \mid \mathbf{x}_n; \mathbf{w}^{\mathrm{old}}, \boldsymbol{\theta}^{\mathrm{old}})$ .

It is important to note the significance of expression (9.20). Since the first term is exactly the observed data log likelihood, this indicates that the gap between the ELBO and the observed data log likelihood is exactly the KL divergence between $q(\mathbf{z}_n)$ and $p(\mathbf{z}_n \mid \mathbf{x}_n; \boldsymbol{\theta}, \mathbf{w})$ . If this gap goes to zero, then we will have succeeded in maximizing the observed data log likelihood. This is why it is important to continually maximize over the distributions $q$ -choosing the best $q$ makes the bound as tight as possible every round.

# Correctness

We will only show that EM increases the observed log likelihood at every iteration. We do this using the second of the two formulations.

$$
\begin{array}{l} \sum_ {n = 1} ^ {N} \log p (\mathbf {x} _ {n} \mid \mathbf {w}, \boldsymbol {\theta}) - \sum_ {n = 1} ^ {N} \log p (\mathbf {x} _ {n} \mid \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}}) \\ = \sum_ {n = 1} ^ {N} \log \frac {p (\mathbf {x} _ {n} \mid \mathbf {w} , \boldsymbol {\theta})}{p (\mathbf {x} _ {n} \mid \mathbf {w} ^ {\mathrm {o l d}} , \boldsymbol {\theta} ^ {\mathrm {o l d}})} \\ = \sum_ {n = 1} ^ {N} \log \frac {\sum_ {k = 1} ^ {K} p \left(\mathbf {x} _ {n} , \mathbf {z} _ {n} \mid \mathbf {w} , \boldsymbol {\theta}\right)}{p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} ; \mathbf {w} ^ {\text {o l d}} , \boldsymbol {\theta} ^ {\text {o l d}}\right)} \\ = \sum_ {n = 1} ^ {N} \log \sum_ {k = 1} ^ {K} \frac {p (\mathbf {x} _ {n} , \mathbf {z} _ {n} \mid \mathbf {w} , \boldsymbol {\theta})}{p (\mathbf {x} _ {n} \mid \mathbf {z} _ {n} ; \mathbf {w} ^ {\mathrm {o l d}} , \boldsymbol {\theta} ^ {\mathrm {o l d}}) p (\mathbf {z} _ {n} \mid \mathbf {w} ^ {\mathrm {o l d}} , \boldsymbol {\theta} ^ {\mathrm {o l d}})} p (\mathbf {z} _ {n} \mid \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}}) \\ = \sum_ {n = 1} ^ {N} \log \operatorname {E} _ {\mathbf {z} _ {n} \sim p (\mathbf {z} _ {n} | \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}})} \left[ \frac {p (\mathbf {x} _ {n} , \mathbf {z} _ {n} \mid \mathbf {w} , \boldsymbol {\theta})}{p (\mathbf {x} _ {n} , \mathbf {z} _ {n} \mid \mathbf {w} ^ {\mathrm {o l d}} , \boldsymbol {\theta} ^ {\mathrm {o l d}})} \right] \\ \geq \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim p (\mathbf {z} _ {n} | \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}})} \left[ \log \frac {p (\mathbf {x} _ {n} , \mathbf {z} _ {n} \mid \mathbf {w} , \boldsymbol {\theta})}{p (\mathbf {x} _ {n} , \mathbf {z} _ {n} \mid \mathbf {w} ^ {\mathrm {o l d}} , \boldsymbol {\theta} ^ {\mathrm {o l d}})} \right] \\ = \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim p (\mathbf {z} _ {n} | \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}})} \left[ \log p (\mathbf {x} _ {n}, \mathbf {z} _ {n} | \mathbf {w}, \boldsymbol {\theta}) \right] - \sum_ {n = 1} ^ {N} \mathrm {E} _ {\mathbf {z} _ {n} \sim p (\mathbf {z} _ {n} | \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}})} \left[ \log p (\mathbf {x} _ {n}, \mathbf {z} _ {n} | | \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}}) \right] \\ = Q (\mathbf {w}, \boldsymbol {\theta} \mid \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta}) - Q (\mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta} ^ {\mathrm {o l d}} \mid \mathbf {w} ^ {\mathrm {o l d}}, \boldsymbol {\theta}) \\ \end{array}
$$

We are maximizing $Q$ at each step, and thus this difference is strictly positive.

# Equivalence to Prior Formulation

It is not hard to see that the formulation presented above, centered around maximizing the ELBO, is equivalent to the one presented in section 9.4.3. Note that $q_{n,k}$ is exactly $p(\mathbf{z}_n = C_k \mid \mathbf{x}_n, \mathbf{w}, \pmb{\theta})$ , and thus the quantity being computed in the $M$ -step (as in (9.10)) is exactly the same as in (9.21).

# 9.4.5 Connection to K-Means Clustering

At this point, it's worth considering the similarity between the EM algorithm and another coordinate ascent algorithm that we considered in the context of clustering: K-Means.

Recall that K-Means proceeds according to a similar iterative algorithm: we first make hard assignments of data points to existing cluster centers, and then we update the cluster centers based on the most recent data point assignments.

In fact, the main differences between K-Means clustering and the EM algorithm are that:

1. In the EM setting, we make soft cluster assignments through our $\mathbf{q}_n$ values, rather than definitively assigning each data point to only one cluster.   
2. The EM algorithm is able to take advantage of flexible, class-conditional distributions to capture the behavior of the observed data, whereas K-Means clustering relies only on distance measurements to make assignments and update cluster centers.

In the context of a mixture-of-Gaussian model, which we get to later in the chapter, we can confirm that K-means is equal to the limiting case of EM where the variance of each class-conditional Gaussian goes to 0, the prior probability of each class is uniform, and the distributions are spherical.

# 9.4.6 Dice Example: Mixture of Multinomials

Consider the following example scenario: we have two biased dice (with 6 faces) and one biased coin (with 2 sides). Data is generated as follows: first, the biased coin is flipped. Suppose it lands heads. Then dice 1 is rolled $c = 10$ times. This gives the first example, i.e., $\mathbf{x}_1$ would correspond to the number of times of rolling each of a $1, 2, \ldots, 6$ . Then we repeat, flipping the biased coin. Suppose it lands tails. Then Dice 2 is rolled 10 times. We record the result of the dice rolls as $\mathbf{x}_2$ . We keep repeating, obtaining additional examples.

For example, our observations for the first 10 rolls may look like: 1, 5, 3, 4, 2, 2, 3, 1, 6, 2 and we'd record as our first example

$$
\mathbf {x} _ {1} = \left[ \begin{array}{l} 2 \\ 3 \\ 2 \\ 1 \\ 1 \\ 1 \end{array} \right]
$$

We're going to try to infer the parameters of each of the dice based on these observations. Let's consider how this scenario fits into our idea of a mixture model. First, the latent variable $\mathbf{z}_n$ has a natural interpretation as being which dice was rolled for the $n^{th}$ observed data point $\mathbf{x}_n$ . We can represent $\mathbf{z}_n$ using a one-hot vector, so that if the $n^{th}$ data point came from Dice 1, we'd denote that:

$$
\mathbf {z} _ {n} = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]
$$

We denote the probability vector associated with the biased coin as $\pmb{\theta} \in [0,1]^2$ , summing to 1, with $\theta_{1}$ being the probability of the biased coin landing heads and $\theta_{2}$ being the probability of the biased coin landing tails. Furthermore, we need parameters to describe the behavior of the biased dice. We use $\pi_1, \pi_2 \in [0,1]^6$ , summing to 1, where each 6-dimensional vector describes the probability that the respective dice lands on each face.

For a given dice, this defines a multinomial distribution. For $c$ trials, and counts $x_{1},\ldots ,x_{6}$ for each of 6 faces on a 6-sided dice, and probabilities $\pi$ , this is

$$
p (\mathbf {x}; \boldsymbol {\pi}) = \frac {c !}{x _ {1} ! \cdot \dots \cdot x _ {6} !} \pi_ {1} ^ {x _ {1}} \cdot \dots \cdot \pi_ {6} ^ {x _ {6}} \tag {9.23}
$$

For our purposes, let $p(\mathbf{x}_n|\mathbf{z}_n = C_k; \pmb{\pi}_1, \pmb{\pi}_2)$ denote the multinomial distribution on observation $\mathbf{x}_{nj}$ when latent vector $\mathbf{z}_n = C_k$ .

The model parameters are $\mathbf{w} = \{\pmb {\theta},\pmb {\pi}_1,\pmb {\pi}_2\}$ . We can optimize the model parameters using EM. We start by initializing the parameters $\pmb{\theta}^{(0)},\pmb{\pi}^{(0)}$

In the E-step, we compute the soft assignment values, $\mathbf{q}_n$ . For dice $k$ , this given by

$$
\begin{array}{l} q _ {n k} = p \left(\mathbf {z} _ {n} = C _ {k} \mid \mathbf {x} _ {n}; \boldsymbol {\theta} ^ {(i)}, \boldsymbol {\pi} ^ {(i)}\right) (9.24) \\ = \frac {p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\pi} ^ {(i)}\right) p \left(\mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\theta} ^ {(i)}\right)}{\sum_ {k = 1} ^ {K} p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\pi} ^ {(i)}\right) p \left(\mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\theta} ^ {(i)}\right)} (9.25) \\ = \frac {p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\pi} ^ {(i)}\right) \theta_ {k} ^ {(i)}}{\sum_ {k = 1} ^ {K} p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\pi} ^ {(i)}\right) \theta_ {k} ^ {(i)}} (9.26) \\ \end{array}
$$

We could also use the "product trick" to write a single expression

$$
\begin{array}{l} p (\mathbf {z} _ {n} | \mathbf {x} _ {n}; \boldsymbol {\theta} ^ {(i)}, \boldsymbol {\pi} ^ {(i)}) = \frac {\prod_ {k = 1} ^ {K} \left(p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\pi} ^ {(i)}) \theta_ {k} ^ {(i)}\right) ^ {z _ {n k}}}{\sum_ {k = 1} ^ {K} p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {k} ; \boldsymbol {\pi} ^ {(i)}) \theta_ {k} ^ {(i)}} \\ = \frac {\prod_ {k = 1} ^ {K} p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {k} ; \pmb {\pi} ^ {(i)}) ^ {z _ {n k}} \prod_ {k = 1} ^ {K} (\theta_ {k} ^ {(i)}) ^ {z _ {n k}}}{\sum_ {k = 1} ^ {K} p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {k} ; \pmb {\pi} ^ {(i)}) \theta_ {k} ^ {(i)}} \\ \end{array}
$$

The vector $\mathbf{q}_n$ is defined as:

$$
\mathbf {q} _ {n} = \left[ \begin{array}{l} p \left(\mathbf {z} _ {n} = C _ {1} \mid \mathbf {x} _ {n}; \boldsymbol {\theta} ^ {(i)}, \boldsymbol {\pi} ^ {(i)}\right) \\ p \left(\mathbf {z} _ {n} = C _ {2} \mid \mathbf {x} _ {n}; \boldsymbol {\theta} ^ {(i)}, \boldsymbol {\pi} ^ {(i)}\right) \end{array} \right] \tag {9.27}
$$

After computing the values of $\mathbf{q}_n$ , we are ready to perform the M-step. Recall that we are maximizing the expected complete-data log likelihood, which takes the form:

$$
\begin{array}{l} \mathbb {E} _ {\mathbf {Z} \mid \mathbf {X}} [ \log p (\mathbf {X}, \mathbf {Z}) ] = \mathbb {E} _ {\mathbf {q} _ {n}} \left[ \sum_ {n = 1} ^ {N} \log p \left(\mathbf {z} _ {n}; \boldsymbol {\theta} ^ {(i + 1)}, \boldsymbol {\pi} ^ {(i + 1)}\right) + \log p \left(\mathbf {x} _ {n} \mid \mathbf {z} _ {n}; \boldsymbol {\theta} ^ {(i + 1)}, \boldsymbol {\pi} ^ {(i + 1)}\right) \right] (9.28) \\ = \sum_ {n = 1} ^ {N} \mathbb {E} _ {\mathbf {z} _ {n} | \mathbf {x} _ {n}} \left[ \log p \left(\mathbf {z} _ {n}; \boldsymbol {\theta} ^ {(i + 1)}, \boldsymbol {\pi} ^ {(i + 1)}\right) + \log p \left(\mathbf {x} _ {n} | \mathbf {z} _ {n}; \boldsymbol {\theta} ^ {(i + 1)}, \boldsymbol {\pi} ^ {(i + 1)}\right) \right] (9.29) \\ \end{array}
$$

We can then substitute in for the multinomial expression and simplify, and dropping constants we have that we're looking for parameters that solve

$$
\begin{array}{l} \arg \max _ {\pmb {\theta} ^ {(i + 1)}, \pmb {\pi} ^ {(i + 1)}} \left\{\sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {2} q _ {n, k} \log \theta_ {k} ^ {(i + 1)} + \sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {2} q _ {n, k} \log \left(\pi_ {k, 1} ^ {x _ {n}, 1} \cdot \ldots \cdot \pi_ {k, 6} ^ {x _ {n}, 6}\right) \right\} \\ = \arg \max  _ {\boldsymbol {\theta} ^ {(i + 1)}, \boldsymbol {\pi} ^ {(i + 1)}} \left\{\sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {2} q _ {n, k} \log \theta_ {k} ^ {(i + 1)} + \sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {2} \sum_ {j = 1} ^ {6} q _ {n, k} x _ {n, j} \log \left(\pi_ {k, j}\right) \right\} \tag {9.30} \\ \end{array}
$$

To maximize the expected complete-data log likelihood, it's necessary to introduce Lagrange multipliers to enforce the constraints $\sum_{k}\theta_{k}^{(i + 1)} = 1$ and $\sum_{j}\pi_{k,j}^{(i + 1)} = 1$ , for each $k$ . After doing this, and solving, we recover the following update equations for the model parameters:

$$
\theta_ {k} ^ {(i + 1)} \leftarrow \frac {\sum_ {n = 1} ^ {N} q _ {n , k}}{N}
$$

$$
\boldsymbol {\pi} _ {k} ^ {(i + 1)} \leftarrow \frac {\sum_ {n = 1} ^ {N} q _ {n , k} \mathbf {x} _ {n}}{c \sum_ {n = 1} ^ {N} q _ {n , k}},
$$

where $c = 10$ in out example.

We now have everything we need to perform EM for this setup. After initializing our parameters $\mathbf{w}^{(0)}$ , we perform the E-step by evaluating 9.27. After calculating our values of $\mathbf{q}_n$ in the E-step, we update our parameters $\mathbf{w} = \{\pmb{\theta}, \pmb{\pi}_1, \pmb{\pi}_2\}$ in the M-step by maximizing 9.30 with respect to $\pmb{\theta}, \pmb{\pi}_1, \pmb{\pi}_2$ . We perform these two steps iteratively, until convergence of our parameters. We may also do a restart.

# 9.5 Gaussian Mixture Models (GMM)

Our previous example was a simple but somewhat restricted application of the EM algorithm to solving a latent variable problem. We now turn to a more practical example, used widely in different contexts, called a Gaussian Mixture Model (GMM). As you might expect, a GMM consists of a combination of multiple Gaussian distributions. Among other things, it is useful for modeling scenarios where the observed data is continuous.

Let's go over a more rigorous formulation of the GMM setup. First, we have observed continuous data $\mathbf{x}_n\in \mathbb{R}^m$ and latent variables $\mathbf{z}_n$ which indicate which Gaussian 'cluster' our observed data point was drawn from. In other words:

$$
p (\mathbf {x} _ {n} | \mathbf {z} _ {n} = C _ {k}) = \mathcal {N} (\mathbf {x} _ {n}; \boldsymbol {\mu} _ {k}, \boldsymbol {\Sigma} _ {k})
$$

where $\pmb{\mu}_k, \pmb{\Sigma}_k$ are the mean and covariance parameters respectively for the $k^{th}$ cluster center.

The data generation process works as follows: we first sample a cluster center from a Categorical distribution parameterized by $\pmb{\theta} \in \mathbb{R}^{K}$ . Then, based on the sampled cluster center, we sample a data point $\mathbf{x}_n \in \mathbb{R}^m$ , which is the only piece of data that we actually observe. As usual for a mixture model, it is our goal to use the observed data to determine the cluster means and covariances, as well as the parameters of the Categorical distribution that selects the cluster centers.

Fortunately, this problem setup is perfectly suited for EM. We can apply the same machinery we've discussed throughout the chapter and used in the previous example.

1. First, we randomly initialize our parameters $\pmb{\theta},\{\pmb{\mu}_k,\pmb{\Sigma}_k\}_{k = 1}^K$   
2. [E-Step] Calculate the posterior distribution over $\mathbf{z}_n$ given by $\mathbf{q}_n$ :

$$
\begin{array}{l} \mathbf {q} _ {n} = \operatorname {E} [ \mathbf {z} _ {n} | \mathbf {x} _ {n} ] = \left[ \begin{array}{c} p (\mathbf {z} _ {n} = C _ {1} | \mathbf {x} _ {n}; \theta_ {1}, \boldsymbol {\mu} _ {1}, \boldsymbol {\Sigma} _ {1}) \\ \vdots \\ p (\mathbf {z} _ {n} = C _ {K} | \mathbf {x} _ {n}; \theta_ {K}, \boldsymbol {\mu} _ {K}, \boldsymbol {\Sigma} _ {K}) \end{array} \right] \\ \propto \left[ \begin{array}{c} \theta_ {1} \mathcal {N} (\mathbf {x} _ {n}; \boldsymbol {\mu} _ {1}, \boldsymbol {\Sigma} _ {1}) \\ \vdots \\ \theta_ {K} \mathcal {N} (\mathbf {x} _ {n}; \boldsymbol {\mu} _ {K}, \boldsymbol {\Sigma} _ {K}) \end{array} \right] \\ \end{array}
$$

This is the current expectation for our latent variables $\mathbf{z}_n$ given our data $\mathbf{x}_n$ and the current setting of our model parameters $\pmb{\theta}$ , $\{\pmb{\mu}_k, \pmb{\Sigma}_k\}_{k=1}^K$ .

3. [M-Step] Using our values of $\mathbf{q}_n$ , calculate the expected complete-data log likelihood, and then use that term to optimize our model parameters:

$$
\begin{array}{l} \mathbb {E} _ {\mathbf {q} _ {n}} [ \log p (\mathbf {X}, \mathbf {Z}) ] = \mathbb {E} _ {\mathbf {q} _ {n}} \Big [ \sum_ {n = 1} ^ {N} \ln (p (\mathbf {x} _ {n}, \mathbf {z} _ {n}; \boldsymbol {\theta}, \{\boldsymbol {\mu} _ {k}, \boldsymbol {\Sigma} _ {k} \} _ {k = 1} ^ {K})) \Big ] \\ = \sum_ {n = 1} ^ {N} \sum_ {k = 1} ^ {K} q _ {n, k} \ln \theta_ {k} + q _ {n, k} \ln \mathcal {N} (\mathbf {x} _ {n}; \boldsymbol {\mu} _ {k}, \boldsymbol {\Sigma} _ {k}) \\ \end{array}
$$

We can then use this expected complete-data log likelihood to optimize our model parameters $\pmb{\theta},\{\pmb{\mu}_k,\pmb{\Sigma}_k\}_{k = 1}^K$ by computing the MLE as usual. Using a Lagrange multiplier to enforce $\sum_{k = 1}^{K}\theta_{k} = 1$ , we recover the update equations:

$$
\begin{array}{l} \theta_ {k} ^ {(i + 1)} \leftarrow \frac {\sum_ {n = 1} ^ {N} q _ {n , k}}{N} \\ \boldsymbol {\mu} _ {k} ^ {(i + 1)} \leftarrow \frac {\sum_ {n = 1} ^ {N} q _ {n , k} \mathbf {x} _ {n}}{\sum_ {n = 1} ^ {N} q _ {n , k}} \\ \boldsymbol {\Sigma} _ {k} ^ {(i + 1)} \gets \frac {\sum_ {n = 1} ^ {N} q _ {n , k} (\mathbf {x} _ {n} - \boldsymbol {\mu} _ {k} ^ {(i + 1)}) (\mathbf {x} _ {n} - \boldsymbol {\mu} _ {k} ^ {(i + 1)}) ^ {T}}{\sum_ {n = 1} ^ {N} q _ {n , k}} \\ \end{array}
$$

4. Return to step 2. Repeat until convergence.

Finally, it's worth comparing EM and K-Means clustering as applied to GMMs. First, as discussed previously, EM uses soft assignments of data points to clusters rather than hard assignments. Second, the standard K-Means algorithm does not estimate the covariance of each cluster. However, if we enforce as a part of our GMM setup that the covariance matrices of all the clusters are given by $\epsilon \mathbf{I}$ , then as $\epsilon \rightarrow 0$ , EM and K-Means will in fact produce the same results.

# 9.6 Admixture Models: Latent Dirichlet Allocation (LDA)

With a grasp on mixture models, it is not too difficult to understand admixture models. In a sentence: an admixture model is a mixture of mixture models. Latent Dirichlet Allocation (LDA) is a common form of admixture models, and it is sometimes also referred to as topic modeling, for reasons that will become apparent shortly. Describing LDA using an example will hopefully make the idea of an admixture model more concrete.

# 9.6.1 LDA for Topic Modeling

Consider the following data generating process for a set of text documents. We have a Dirichlet distribution $\pmb{\theta} \sim \text{Dir}(\pmb{\alpha})$ over the possible topics a document can take on.

$\star$ If you haven't seen the Dirichlet before, it is a distribution over an $n$ -dimensional vector whose components sum to 1. For example, a sample from a dirichlet distribution in 3-dimensions could produce a sample that is the vector

$$
\left[ \begin{array}{c} 0. 2 \\ 0. 5 \\ 0. 3 \end{array} \right]
$$

We sample from that Dirichlet distribution to determine the mixture of topics $\theta_{n}$ in our document $D_{n}$ :

$$
\boldsymbol {\theta} _ {n} \sim D i r (\boldsymbol {\alpha})
$$

Then, for each possible topic, we sample from a Dirichlet distribution to determine the mixture of words $\phi_{k}$ in that topic:

$$
\phi_ {k} \sim D i r (\beta)
$$

Then, for each word $\mathbf{w}_{n,j}$ in the document $D_{n}$ , we first sample from a Categorical parameterized by the topic mixture $\theta_{n}$ to determine which topic that word will come from:

$$
\mathbf {z} _ {n, j} \sim C a t (\boldsymbol {\theta} _ {n})
$$

Then, now that we have a topic given by $\mathbf{z}_{n,j}$ for this word $\mathbf{w}_{n,j}$ , we sample from a Categorical parameterized by that topic's mixture over words given by $\phi_{\mathbf{z}_{n,j}}$ :

$$
\mathbf {w} _ {n, j} \sim C a t (\phi_ {\mathbf {z} _ {n, j}})
$$

Notice the mixture of mixtures at play here: we have a mixture model over the topics to produce each document in our corpus, and then for every word in a given document, we have a mixture over the topics to generate each individual word.

The indexing is particularly confusing because there are several layers of mixtures here, but to clarify: $n \in 1..N$ indexes each document $D_{n}$ in our corpus, $k \in 1..K$ indexes each possible topic, and $j \in 1..J$ indexes each word $\mathbf{w}_{n,j}$ in document $D_{n}$ , and $e \in 1..E$ indexes each word in our dictionary (note that $\mathbf{w}_{n,j} \in \mathbb{R}^{E}$ ).

$\pmb{\theta}_{n}$ specifies the distribution over topics in document $D_{n}$ , and $\alpha$ is the hyperparameter for the distribution that produces $\pmb{\theta}_{n}$ . Similarly, $\phi_{k}$ specifies the distribution over words for the $k^{th}$ topic, and $\beta$ is the hyperparameter for the distribution that produces $\phi_{k}$ .

# 9.6.2 Applying EM to LDA

Now that the problem setup and notation are taken care of, let's consider how we can apply EM to optimize the parameters $\pmb{\theta}_{n}$ (the mixture over topics in a document) and $\phi_{k}$ (the mixture over words for a topic). Note that we can simplify the problem slightly by considering $\pmb{\theta}_{n}$ and $\phi_{k}$ to be deterministic parameters for optimization (rather than random variables parameterized by $\alpha$ and $\beta$ ). Then, EM proceeds as follows:

1. First, we randomly initialize our parameters $\{\pmb{\theta}_n\}_{n=1}^N, \{\pmb{\phi}_k\}_{k=1}^K$ .

2. [E-Step] Fix the topic distribution of the document given by $\pmb{\theta}_n$ and the word distribution under a topic given by $\phi_k$ . Calculate the posterior distribution $\mathbf{q}_{n,j} = p(\mathbf{z}_{n,j}|\mathbf{w}_{n,j})$ , and note that this is the distribution over the possible topics of a word:

$$
\begin{array}{l} \mathbf {q} _ {n, j} = \operatorname {E} [ \mathbf {z} _ {n, j} | \mathbf {w} _ {n, j} ] = \left[ \begin{array}{c} p (\mathbf {z} _ {n, j} = C _ {1} | \mathbf {w} _ {n, j}; \boldsymbol {\theta} _ {n}, \boldsymbol {\phi} _ {1}) \\ \vdots \\ p (\mathbf {z} _ {n, j} = C _ {K} | \mathbf {w} _ {n, j}; \boldsymbol {\theta} _ {n}, \boldsymbol {\phi} _ {K}) \end{array} \right] \\ \propto \left[ \begin{array}{c} p (\mathbf {w} _ {n, j} | \mathbf {z} _ {n, j} = C _ {1}; \boldsymbol {\phi} _ {1}) p (\mathbf {z} _ {n, j} = C _ {1}; \boldsymbol {\theta} _ {n}) \\ \vdots \\ p (\mathbf {w} _ {n, j} | \mathbf {z} _ {n, j} = C _ {K}; \boldsymbol {\phi} _ {K}) p (\mathbf {z} _ {n, j} = C _ {K}; \boldsymbol {\theta} _ {n}) \end{array} \right] \\ = \left[ \begin{array}{c} \phi_ {1, \mathbf {w} _ {n, j}} \cdot \theta_ {n, 1} \\ \vdots \\ \phi_ {K, \mathbf {w} _ {n, j}} \cdot \theta_ {n, K} \end{array} \right] \\ \end{array}
$$

3. [M-Step] Using our values of $\mathbf{q}_n$ , calculate the expected complete-data log likelihood (which marginalizes over the unknown hidden variables $\mathbf{z}_{n,j}$ ), and then use that expression to optimize our model parameters $\theta_{n}$ and $\phi_k$ :

$$
\begin{array}{l} \mathbb {E} _ {\mathbf {q} _ {n}} [ \log p (\mathbf {W}, \mathbf {Z}) ] = \mathbb {E} _ {\mathbf {q} _ {n}} \bigg [ \sum_ {n = 1} ^ {N} \sum_ {j = 1} ^ {J} \ln (p (\mathbf {w} _ {n, j}, \mathbf {z} _ {n, j}; \{\pmb {\theta} _ {n} \} _ {n = 1} ^ {N}, \{\phi_ {k} \} _ {k = 1} ^ {K}) \bigg ] \\ = \sum_ {n = 1} ^ {N} \sum_ {j = 1} ^ {J} \sum_ {k = 1} ^ {K} q _ {n, j, k} \ln \theta_ {n, k} + q _ {n, j, k} \ln \phi_ {k, \mathbf {w} _ {n, j}} \\ \end{array}
$$

We can then use this expected complete-data log likelihood to optimize our model parameters $\{\pmb{\theta}_n\}_{n=1}^N, \{\phi_k\}_{k=1}^K$ by computing the MLE as usual. Using Lagrange multipliers to enforce $\forall n \sum_{k=1}^{K} \theta_{n,k} = 1$ and $\forall k \sum_{e=1}^{E} \phi_{k,e} = 1$ (where $e$ indexes each word in our dictionary), we recover the update equations:

$$
\theta_ {n, k} ^ {(i + 1)} \leftarrow \frac {\sum_ {j = 1} ^ {J} q _ {n , j , k}}{J}
$$

$$
\phi_ {k, d} ^ {(i + 1)} \gets \frac {\sum_ {n = 1} ^ {N} \sum_ {j = 1} ^ {J} q _ {n , j , k} w _ {n , j , d}}{\sum_ {n = 1} ^ {N} \sum_ {j = 1} ^ {J} q _ {n , j , k}}
$$

4. Return to step 2. Repeat until convergence.

The largest headache for applying the EM algorithm to LDA is keeping all of the indices in order, and this is the result of working with a mixture of mixtures. Once the bookkeeping is sorted out, the actual updates are straightforward.

# 9.7 Conclusion

Mixture models are one common way of handling data that we believe is generated through a combination of unobserved, latent variables. We've seen that training these models directly is intractable (due to the marginalization over the latent variables), and so we turned to a coordinate ascent based algorithm known as Expectation-Maximization to get around this difficulty. We then explored a couple of common mixture models, including a multinomial mixture, Gaussian Mixture Model, and an admixture model known as Latent Dirichlet Allocation. Mixture models are a subset of a broader range of models known as latent variable models, and the examples seen in this chapter are just a taste of the many different mixture models available to us. Furthermore, EM is just a single algorithm for optimizing these models. A good grasp on the fundamentals of mixture models and the EM algorithm will be useful background for expanding to more complicated, expressive latent variable models.

# Chapter 10

# Hidden Markov Models

Many of the techniques we've considered so far in this book have been motivated by the types of data we could expect to work with. For example, the supervised learning techniques (forms of regression, neural networks, support vector machines, etc.) were motivated by the fact that we had labelled training data. We ventured into clustering to group unlabelled data and discussed dimensionality reduction to handle overly high-dimensional data. In the previous chapter, we examined techniques for managing incomplete data with latent variable models. In this chapter we turn to a technique for handling temporal data.

# 10.1 Motivation

One major type of data we have not yet paid explicit attention to is time series data. Most of the information we record comes with some sort of a timestamp. For example, any time we take an action online, there is a high probability that the database storing the data also tracks it with a timestamp. Physical sensors in the real world always record timestamps because it would be very difficult to make sense of their information if it is not indexed by time. When we undergo medical exams, the results are recorded along with a timestamp. It's almost inconceivable at this point that we would record information without also keeping track of when that data was generated, or at the very least when we saved that data.

For these reasons, its interesting to develop models that are specialized to temporal data. Certainly, time encodes a lot of information that we take for granted about the physical and digital worlds. For example, if the sensors on a plane record the position of the plane at a specific point in time, we would expect the surrounding data points to be relatively similar, or at least move in a consistent direction. In a more general sense, we expect that time constrains other attributes of the data in specific ways.

In this chapter, we will focus on one such model known as a Hidden Markov Model or HMM. At a high level, the goal of an HMM is to model the state of an entity over time, with the caveat that we never actually observe the state itself. Instead, we observe a data point $\mathbf{x}_t$ at each time step (often called an 'emission' or 'observation') that depends on the state $\mathbf{s}_t$ . For example, we could model the position of a robot over time given a noisy estimation of the robot's current position at each time step. Furthermore, we will assume that one state $\mathbf{s}_t$ transitions to the next state $\mathbf{s}_{t+1}$ according to a probabilistic model. Graphically, an HMM looks like Figure 10.1, which encodes the relationships between emissions and hidden states. Here, there are $n$ time steps in total.

We will probe HMMs in more detail over the course of the chapter, but for now let's consider

![](images/e89aee854a9909f570db8cc0215b0683893bb387b621759797628c34291fc8b2.jpg)  
Figure 10.1: The directed graphical model for an HMM.

their high-level properties.

# ML Framework Cube: Hidden Markov Models

First, HMMs handle discrete states, and for the purpose of this text, we will only consider discrete-valued emissions as well. Second, since a HMM does not have access to the states, these are hidden (or latent) and in this sense we can view training as "unsupervised." Finally, we assume a probabilistic relationship between hidden states and observed emissions as well as the transitions between hidden states.

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Discrete</td><td>Unsupervised</td><td>Yes</td></tr></table>

$\star$ Models that treat continuous state variables are commonly referred to as dynamical systems.

# 10.2 Applications

Unsurprisingly, there are many applications for models like HMMs that explicitly account for time and unobserved states, especially those that relate to the physical world. Examples include:

1. The position of a robot arm when its movements may be non-deterministic and sensor readings are noisy. [State = robot arm position; observation = sensor reading]   
2. Speech recognition. [State = phoneme; observation = sound]   
3. Analyzing sequences that occur in the natural world, such as DNA [State = codon, a genetic code in a DNA molecule; observation= one of the four bases, i.e., A, C, T, or G]

# 10.3 HMM Data, Model, and Parameterization

As explained above, HMMs model the state of an entity over time given some noisy observations, as shown in Figure 10.1.

# 10.3.1 HMM Data

The data for a HMM consists of the sequence of one-hot encoded emissions $\mathbf{x}_1, \ldots, \mathbf{x}_n$ , for $n$ total time steps. There corresponding states, $\mathbf{s}_1, \ldots, \mathbf{s}_n$ , are latent and unobserved.

Each state corresponds to one of $K$ possible options, i.e., with one-hot coding, $\mathbf{s}_t\in \{0,1\} ^K$ . Each emission corresponds to one of $M$ possible options, with one-hot coding, $\mathbf{x}_t\in \{0,1\} ^M$ .

$\star$ In general the observed emissions don't have to be discrete, but for the sake of being explicit, we present the discrete interpretation here.

A data set has $N$ data points, meaning $N$ sequences where each sequence is composed of $n$ emissions (in general they can be of different lengths, we assume same length for simplicity). To summarize:

- A data set consists of $N$ sequences.   
Each sequence is composed of $n$ observed emissions $\mathbf{x}_1,\dots ,\mathbf{x}_n$   
Each emission $\mathbf{x}_t$ takes on one of $M$ possible values.   
Each hidden state $\mathbf{s}_t$ take on one of $K$ possible values.

# 10.3.2 HMM Model Assumptions

In modeling the joint distribution over hidden states and observed emissions

$$
p \left(\mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}\right) = p \left(\mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}\right) p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {n} \mid \mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}\right), \tag {10.1}
$$

the HMM model makes the following two assumptions:

1. State $\mathbf{s}_{t + 1}$ depends only on the previous state $\mathbf{s}_t$ :

$$
p (\mathbf {s} _ {t + 1} | \mathbf {s} _ {1}, \dots , \mathbf {s} _ {t}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}) = p (\mathbf {s} _ {t + 1} | \mathbf {s} _ {t})
$$

This is the Markov Property, and it means that given knowledge of the state at the previous time step, we can ignore all other earlier states and emissions. Here, we also assume the transition is stationary, so that the transition model doesn't depend on time.

2. At each time step $t$ , the observed emission $\mathbf{x}_t$ depends only on the current state $\mathbf{s}_t$ :

$$
p (\mathbf {x} _ {t} | \mathbf {s} _ {1}, \dots , \mathbf {s} _ {t}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t - 1}) = p (\mathbf {x} _ {t} | \mathbf {s} _ {t})
$$

$\star$ The Markovian assumption for transitions, as well as the fact that we don't observe the true states, gives rise to the Hidden Markov Model name.

These two assumptions allow us to factorize the large joint distribution given by Equation 10.1 as follows:

$$
p \left(\mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}\right) p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {n} \mid \mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}\right) = p \left(\mathbf {s} _ {1}\right) \prod_ {t = 1} ^ {n - 1} p \left(\mathbf {s} _ {t + 1} \mid \mathbf {s} _ {t}\right) \prod_ {t = 1} ^ {n} p \left(\mathbf {x} _ {t} \mid \mathbf {s} _ {t}\right) \tag {10.2}
$$

This factorization will prove important for making HMM training and inference tractable.

# 10.3.3 HMM Parameterization

Now that we understand the form of the data as well as the modeling assumptions made by a HMM, we can specify the model parameterization explicitly. Referencing the factorized joint distribution from Equation 10.2, we will need three distinct sets of parameters.

1. Parameters for the prior over the initial hidden state $p(\mathbf{s}_1)$ . This will be denoted $\pmb{\theta} \in [0,1]^K$ , with $\sum_{k} \theta_{k} = 1$ , such that:

$$
p (\mathbf {s} _ {1} = k) = \theta_ {k}.
$$

2. Parameters for the transition probabilities between states $p(\mathbf{s}_{t + 1}|\mathbf{s}_t)$ . This will be denoted $\mathbf{T}\in [0,1]^{K\times K}$ , with $\sum_{j}T_{i,j} = 1$ for each $i$ , such that:

$$
p (\mathbf {s} _ {t + 1} = j | \mathbf {s} _ {t} = i) = T _ {i, j},
$$

where $T_{i,j}$ is the probability of transitioning from state $i$ to state $j$ .

3. Parameters for the conditional probability of the emission, $p(\mathbf{x}_t|\mathbf{s}_t)$ , given the state. This will be denoted $\pi \in [0,1]^{K\times M}$ , with $\sum_{m}\pi_{k,m} = 1$ for each $k$ , such that:

$$
p (\mathbf {x} _ {t} = m | \mathbf {s} = k) = \pi_ {k, m}.
$$

For each state, there is a distribution on possible emissions.

In sum, we have three sets of parameters $\pmb{\theta} \in [0,1]^K$ , $\mathbf{T} \in [0,1]^{K \times K}$ , and $\pmb{\pi} \in [0,1]^{K \times M}$ that we need to learn from our data set. Then, using a trained model, we will be able to perform several types of inference over our hidden states, as detailed next.

# 10.4 Inference in HMMs

We will see how to estimate the parameters of a HMM ("training") in Section 10.5. For now we want to understand how to do efficient inference in HMMs. Given parameters $\pmb{\theta},\mathbf{T}$ , and $\pmb{\pi}$ , and given a sequence of observations, $\mathbf{x}_1,\dots,\mathbf{x}_n$ (or $\mathbf{x}_1,\dots,\mathbf{x}_t$ ) there are various queries we may like to perform:

- "p(seq)" $p(\mathbf{x}_1, \ldots, \mathbf{x}_n)$ (what is the distribution on sequences of emissions?)   
- Prediction $p(\mathbf{x}_{t + 1}|\mathbf{x}_1,\dots ,\mathbf{x}_t)$ (what is the prediction of the next emission given what is known so far?)   
- Smoothing $p(\mathbf{s}_t|\mathbf{x}_1,\dots ,\mathbf{x}_n), t \leq n$ (after the fact, what do we predict for some earlier state?)   
- Transition $p(\mathbf{s}_t, \mathbf{s}_{t+1} | \mathbf{x}_1, \ldots, \mathbf{x}_n), t + 1 \leq n$ (after the fact, what do we predict for the joint distribution on some pair of temporally adjacent states?)   
- Filtering $p(\mathbf{s}_t|\mathbf{x}_1,\dots ,\mathbf{x}_t)$ (what is the prediction, in real-time, of the current state?)   
- Best path $\max p(\mathbf{s}_1, \ldots, \mathbf{s}_n | \mathbf{x}_1, \ldots, \mathbf{x}_n)$ (after the fact, what is the most likely sequence of states?)

For just one example, let's consider smoothing $p(\mathbf{s}_t|\mathbf{x}_1,\dots,\mathbf{x}_n)$ . To compute this would require marginalizing over all the unobserved states other than $\mathbf{s}_t$ , as follows:

$$
\begin{array}{l} p \left(\mathbf {s} _ {t} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}\right) \propto p \left(\mathbf {s} _ {t}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}\right) \\ = \sum_ {s _ {1}, \dots , s _ {t - 1}, s _ {t + 1}, \dots , s _ {n}} p (\mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}) \\ = \sum_ {s _ {1}, \dots , s _ {t - 1}, s _ {t + 1}, \dots , s _ {n}} p (\mathbf {s} _ {1}) \prod_ {t = 1} ^ {n - 1} p (\mathbf {s} _ {t + 1} | \mathbf {s} _ {t}) \prod_ {t = 1} ^ {n} p (\mathbf {x} _ {t} | \mathbf {s} _ {t}). \\ \end{array}
$$

Without making use of variable elimination, this requires summing over all possible states other than $t$ , which is very costly. Moreover, suppose we then query this for another state. We'd need to sum again over all the states except for this new state, which duplicates a lot of work. Rather than performing these summations over and over again, we can instead "memoize" (or reuse) these kinds of summations using the Forward-Backward algorithm. This algorithm also makes uses of variable elimination to improve the efficiency of inference.

# 10.4.1 The Forward-Backward Algorithm

The Forward-Backward algorithm uses variable elimination methods to compute two sets of quantities, that we refer to as the "alpha" and "beta" values. The algorithm makes elegant use of dynamic programming (breaking down optimization problem into sub-problems, solving each sub-problem a single time, and storing the solutions). It can be viewed as a preliminary inference step such that the alpha and beta values can then be used for all inference tasks of interest as well as within EM for training a HMM model.

The Forward-Backward algorithm is also an example of a message-passing scheme, which means we can conceptualize it as passing around compact messages along edges of the graphical model that corresponds to a HMM. The algorithm passes messages forwards and backwards through 'time', meaning up and down the chain shown in the graphical model representation in Figure 10.1. The forward messages are defined at each state as $\alpha_{t}(\mathbf{s}_{t})$ , while the backward messages are defined at each state as $\beta_{t}(\mathbf{s}_{t})$ . The overarching idea is to factor the joint distribution

$$
p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}, \mathbf {s} _ {t}\right) = p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {t + 1}, \dots , \mathbf {x} _ {n} \mid s _ {t}\right)
$$

because the factored terms can be efficiently computed. Let's define these $\alpha$ and $\beta$ terms explicitly.

The $\alpha_{t}$ 's represent the joint probability of all our observed emissions from time $1, \ldots, t$ as well as the state at time $t$ :

$$
\alpha_ {t} \left(\mathbf {s} _ {t}\right) = p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}\right) \tag {10.3}
$$

Graphically, the $\alpha_{t}$ 's are capturing the portion of the HMM shown in Figure 10.2.

We can factorize this joint probability using what we know about the conditional independence

![](images/dd49c9280de000b66cfb75aa97ff91ce8e7827923bfe97edc3d7b56c86d16e9a.jpg)  
Figure 10.2: $\alpha_{t}$ 's capture the joint probability for the boxed portion; shown for $\alpha_{3}(\mathbf{s}_{3})$

properties of HMMs as follows:

$$
\begin{array}{l} \alpha_ {t} (\mathbf {s} _ {t}) = p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}\right) \\ = p \left(\mathbf {x} _ {t} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t - 1}, \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t - 1}, \mathbf {s} _ {t}\right) \\ = p \left(\mathbf {x} _ {t} \mid \mathbf {s} _ {t}\right) \sum_ {s _ {t - 1}} p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t - 1}, \mathbf {s} _ {t - 1}, \mathbf {s} _ {t}\right) (10.4) \\ = p \left(\mathbf {x} _ {t} \mid \mathbf {s} _ {t}\right) \sum_ {s _ {t - 1}} p \left(\mathbf {s} _ {t} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t - 1}, \mathbf {s} _ {t - 1}\right) p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t - 1}, \mathbf {s} _ {t - 1}\right) (10.5) \\ = p \left(\mathbf {x} _ {t} \mid \mathbf {s} _ {t}\right) \sum_ {s _ {t - 1}} p \left(\mathbf {s} _ {t} \mid \mathbf {s} _ {t - 1}\right) \alpha_ {t - 1} \left(\mathbf {s} _ {t - 1}\right) (10.6) \\ \end{array}
$$

The first term in Equation (10.4) follows from the Markov property, and for the second term we've expressed this joint probability by explicitly introducing $\mathbf{s}_{t-1}$ and marginalizing out over this variable. Equation 10.6 follows from the Markov property, and by substituting for the definition of the alpha value.

Notice that our expression for $\alpha_{t}(\mathbf{s}_{t})$ includes the expression for $\alpha_{t - 1}(\mathbf{s}_{t - 1})$ , which is the $\alpha$ from the previous time step. This means we can define our messages recursively. After we've computed the $\alpha$ ’s at one time step, we pass them forwards along the chain and use them in the computation of alpha values for the next time step. In other words, we compute the $\alpha$ values for period 1, then pass that message along to compute the $\alpha$ values in period 2, and so forth until we reach the end of the chain and have all the $\alpha$ ’s in hand.

$\star$ These $\alpha$ values are used both for inference and training a HMM via EM (in the E-step)

At this point, we've handled the forward messages, which send information from the beginning to the end of the chain. In the backward portion, we also send information from the end of the chain back to the beginning. In this backward message pass, we will compute our $\beta$ values. The $\beta_{t}$ 's represent the joint probability over all the observed emissions from time $t + 1, \dots, n$ conditioned on the state at time $t$ :

$$
\beta_ {t} \left(\mathbf {s} _ {t}\right) = p \left(\mathbf {x} _ {t + 1}, \dots , \mathbf {x} _ {n} \mid \mathbf {s} _ {t}\right) \tag {10.7}
$$

Graphically, this means that the $\beta_{t}$ 's are capturing the portion of the HMM shown in Figure 10.3.

We can factorize Equation 10.7 in a similar way to how we factorized the distribution described

by the $\alpha$ 's:

$$
\begin{array}{l} \beta_ {t} (\mathbf {s} _ {t}) = p \left(\mathbf {x} _ {t + 1}, \dots , \mathbf {x} _ {n} | \mathbf {s} _ {t}\right) \\ = \sum_ {\mathbf {s} _ {t + 1}} p \left(\mathbf {x} _ {t + 1}, \dots , \mathbf {x} _ {n}, \mathbf {s} _ {t + 1} \mid \mathbf {s} _ {t}\right) (10.8) \\ = \sum_ {\mathbf {s} _ {t + 1}} p \left(\mathbf {s} _ {t + 1} \mid \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {t + 1} \mid \mathbf {s} _ {t}, \mathbf {s} _ {t + 1}\right) p \left(\mathbf {x} _ {t + 2}, \dots , \mathbf {x} _ {n} \mid \mathbf {x} _ {t + 1}, \mathbf {s} _ {t}, \mathbf {s} _ {t + 1}\right) (10.9) \\ = \sum_ {\mathbf {s} _ {t + 1}} p \left(\mathbf {s} _ {t + 1} \mid \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {t + 1} \mid \mathbf {s} _ {t + 1}\right) p \left(\mathbf {x} _ {t + 2}, \dots , \mathbf {x} _ {n} \mid \mathbf {s} _ {t + 1}\right) (10.10) \\ = \sum_ {\mathbf {s} _ {t + 1}} p \left(\mathbf {s} _ {t + 1} \mid \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {t + 1} \mid \mathbf {s} _ {t + 1}\right) \beta_ {t + 1} \left(\mathbf {s} _ {t + 1}\right). (10.11) \\ \end{array}
$$

Here, Equation (10.8) introduces $\mathbf{s}_{t + 1}$ and marginalizes out over this variable. Equation (10.9) is the product rule (recall that $n\geq t + 1$ , so the third conditional probability starts at $x_{t + 2}$ ). Equation (10.10) makes use of the Markov property in the last two terms. Equation (10.11) substitutes in the expression for $\beta_{t + 1}$ from Equation (10.7).

As we saw with our calculation of the $\alpha$ 's, we can calculate $\beta$ recursively. This recursive definition enables us to propagate messages backward and compute the beta values efficiently in one pass. In this case, we start at the end of the chain ( $t = n$ ), and compute our $\beta$ 's for each state by passing messages back toward the front.

To summarize, the Forward-Backward algorithm calculates the $\alpha$ and $\beta$ values as follows:

$$
\alpha_ {t} (\mathbf {s} _ {t}) = \left\{ \begin{array}{l l} p (\mathbf {x} _ {t} | \mathbf {s} _ {t}) \sum_ {\mathbf {s} _ {t - 1}} p (\mathbf {s} _ {t} | \mathbf {s} _ {t - 1}) \alpha_ {t - 1} (\mathbf {s} _ {t - 1}) & 1 <   t \leq n \\ p (\mathbf {x} _ {1} | \mathbf {s} _ {1}) p (\mathbf {s} _ {1}) & \text {o t h e r w i s e} \end{array} \right.
$$

$$
\beta_ {t} (\mathbf {s} _ {t}) = \left\{ \begin{array}{l l} \sum_ {\mathbf {s} _ {t + 1}} p (\mathbf {s} _ {t + 1} | \mathbf {s} _ {t}) p (\mathbf {x} _ {t + 1} | \mathbf {s} _ {t + 1}) \beta_ {t + 1} (\mathbf {s} _ {t + 1}) & 1 \leq t <   n \\ 1 & \text {o t h e r w i s e} \end{array} \right.
$$

$\star$ Notice that the base case for the $\beta$ 's is $n$ . This is a quirk of our indexing, and it ensures we have a defined $\mathbf{s}_n$ when we pass messages back to calculate $\mathbf{s}_{n-1}, \mathbf{s}_{n-2}, \ldots$ .

![](images/467a6727d6529e946185944de4b811921916a8a4f22dd40572eb673aabfff8bb.jpg)  
Figure 10.3: $\beta_{t}$ 's capture the joint probability for the boxed portion of the HMM. Shown here for $\beta_{2}(\mathbf{s}_{2})$

# 10.4.2 Using $\alpha$ 's and $\beta$ 's for Training and Inference

Now that we know how to compute these $\alpha$ and $\beta$ values, let's see how to use them for inference. Consider the product of the $\alpha$ and $\beta$ value at a specific time $t$ :

$$
\alpha_ {t} (\mathbf {s} _ {t}) \beta_ {t} (\mathbf {s} _ {t}) = p (\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}) p (\mathbf {x} _ {t + 1}, \dots , \mathbf {x} _ {n} | \mathbf {s} _ {t}) = p (\mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}, \mathbf {s} _ {t}).
$$

This is the joint distribution over all emissions and the state at time $t$ . Using this as a building block, this can support many kinds of inference.

# $\mathbf{p}(\mathbf{Seq})$

For example, we might like to evaluate the joint distribution over the emissions.

$$
p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}\right) = \sum_ {\mathbf {s} _ {t}} p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}, \mathbf {s} _ {t}\right) = \sum_ {\mathbf {s} _ {t}} \alpha_ {t} \left(\mathbf {s} _ {t}\right) \beta_ {t} \left(\mathbf {s} _ {t}\right) \tag {10.12}
$$

where we can sum over the possible state values. This calculation be defined for any state $\mathbf{s}_t$ .

# Prediction

Another common task is to predict the value of the next emission given the previous emissions.

$$
p (\mathbf {x} _ {t + 1} | \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t})
$$

To compute this we can sum over state $\mathbf{s}_t$ and the next state $\mathbf{s}_{t + 1}$ as follows:

$$
\begin{array}{l} p \left(\mathbf {x} _ {t + 1} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}\right) \propto p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {x} _ {t + 1}\right) \\ = \sum_ {\mathbf {s} _ {t}} \sum_ {\mathbf {s} _ {t + 1}} p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {x} _ {t + 1}, \mathbf {s} _ {t}, \mathbf {s} _ {t + 1}\right) (10.13) \\ = \sum_ {\mathbf {s} _ {t}} \sum_ {\mathbf {s} _ {t + 1}} p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}\right) p \left(\mathbf {s} _ {t + 1} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {t + 1} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}, \mathbf {s} _ {t + 1}\right) (10.14) \\ = \sum_ {\mathbf {s} _ {t}} \sum_ {\mathbf {s} _ {t + 1}} \alpha_ {t} (\mathbf {s} _ {t}) p \left(\mathbf {s} _ {t + 1} \mid \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {t + 1} \mid \mathbf {s} _ {t + 1}\right). (10.15) \\ \end{array}
$$

Here, Equation (10.13) follows by introducing states $\mathbf{s}_t$ and $\mathbf{s}_{t + 1}$ and marginalizing out over them. Equation (10.14) follows from the product rule, and Equation (10.15) by using the Markov property in two places and substituting for $\alpha_{t}(\mathbf{s}_{t})$ .

# Smoothing

Smoothing is the problem of predicting the state at time $t$ given all the observed emissions. We can think about this as updating the beliefs that we would have had in real-time, given emissions up to and including $t$ , given all observed evidence up to period $n$ . Hence the phrasing "smoothing." For this, we have

$$
p \left(\mathbf {s} _ {t} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}\right) \propto p \left(\mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}, \mathbf {s} _ {t}\right) = \alpha_ {t} \left(\mathbf {s} _ {t}\right) \beta_ {t} \left(\mathbf {s} _ {t}\right). \tag {10.16}
$$

# Transition

Finally, we may wish to understand the joint distribution on states $\mathbf{s}_t$ and $\mathbf{s}_{t + 1}$ given all the observed evidence.

$$
\begin{array}{l} p \left(\mathbf {s} _ {t}, \mathbf {s} _ {t + 1} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}\right) \propto p \left(\mathbf {s} _ {t}, \mathbf {s} _ {t + 1}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}\right) \\ = p (\mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}) p (\mathbf {s} _ {t + 1} | \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}) p (\mathbf {x} _ {t + 1} | \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}, \mathbf {s} _ {t}, \mathbf {s} _ {t + 1}) \\ p \left(\mathbf {x} _ {t + 2}, \dots , \mathbf {x} _ {n} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t + 1}, \mathbf {s} _ {t}, \mathbf {s} _ {t + 1}\right) (10.17) \\ = \alpha_ {t} (\mathbf {s} _ {t}) p \left(\mathbf {s} _ {t + 1} \mid \mathbf {s} _ {t}\right) p \left(\mathbf {x} _ {t + 1} \mid \mathbf {s} _ {t + 1}\right) \beta_ {t + 1} \left(\mathbf {s} _ {t + 1}\right). (10.18) \\ \end{array}
$$

Here, Equation (10.17) follows from the product rule, and (10.18) by substituting for $\alpha_{t}(\mathbf{s}_{t})$ , applying the Markov property three times, and substituting for $\beta_{t + 1}(\mathbf{s}_{t + 1})$ .

# Filtering

For filtering, we have

$$
p \left(\mathbf {s} _ {t} \mid \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}\right) \propto p \left(\mathbf {s} _ {t}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}\right) = \alpha_ {t} \left(\mathbf {s} _ {t}\right). \tag {10.19}
$$

# Best path

For the best path problem, we want to find the sequence of states that is most likely to give rise to the observed emissions. We solve

$$
\arg \max  _ {\mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}} p (\mathbf {s} | \mathbf {x}) = \arg \max  _ {\mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}} p (\mathbf {s} _ {1}, \dots , \mathbf {s} _ {n}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {n}).
$$

This sometimes referred to as the "decoding" (or explanation) problem. For this, we can define the following function:

$$
\gamma_ {t} \left(\mathbf {s} _ {t}\right) = \max  _ {\mathbf {s} _ {1}, \dots , \mathbf {s} _ {t - 1}} p \left(\mathbf {s} _ {1}, \dots , \mathbf {s} _ {t}, \mathbf {x} _ {1}, \dots , \mathbf {x} _ {t}\right). \tag {10.20}
$$

This is the likelihood of $\mathbf{x}_1, \ldots, \mathbf{x}_t$ , if the current state is $\mathbf{s}_t$ , and under the best explanation so far (we maximized over $\mathbf{s}_1, \ldots, \mathbf{s}_{t-1}$ ). Recall that the recurrence for alpha is as follows:

$$
\forall \mathbf {s} _ {t}: \alpha_ {t} (\mathbf {s} _ {t}) = \left\{ \begin{array}{l l} p (\mathbf {x} _ {t} | \mathbf {s} _ {t}) \sum_ {\mathbf {s} _ {t - 1}} p (\mathbf {s} _ {t} | \mathbf {s} _ {t - 1}) \alpha_ {t - 1} (\mathbf {s} _ {t - 1}) & \text {i f} 1 <   t \leq n \\ p (\mathbf {x} _ {1} | \mathbf {s} _ {1}) p (\mathbf {s} _ {1}) & \text {o t h e r w i s e} \end{array} \right.
$$

Analogously, the recurrence for this $\gamma$ -value can be shown to be (see if you can derive this for yourself):

$$
\forall \mathbf {s} _ {t}: \quad \gamma_ {t} (\mathbf {s} _ {t}) = \left\{ \begin{array}{l l} p (\mathbf {x} _ {t} | \mathbf {s} _ {t}) \max  _ {\mathbf {s} _ {t - 1}} p (\mathbf {s} _ {t} | \mathbf {s} _ {t - 1}) \gamma_ {t - 1} (\mathbf {s} _ {t - 1}) & \text {i f} 1 <   t \leq n \\ p (\mathbf {x} _ {1} | \mathbf {s} _ {1}) p (\mathbf {s} _ {1}) & \text {o t h e r w i s e} \end{array} \right. \tag {10.21}
$$

To be able to find the optimal sequence, we also store, for each $\mathbf{s}_t$ , the best choice of $\mathbf{s}_{t-1}$ :

$$
z _ {t} ^ {*} (\mathbf {s} _ {t}) = \arg \max  _ {\mathbf {s} _ {t - 1}} [ p (\mathbf {s} _ {t} | \mathbf {s} _ {t - 1}) \gamma_ {t - 1} (\mathbf {s} _ {t - 1}) ] \tag {10.22}
$$

This recursive procedure is known as the Viterbi algorithm and provides an efficient way to infer the "best path" through states given a sequence of observations.

# 10.5 Using EM to Train a HMM

Let's now turn to using EM to train a HMM. Recall the motivation for the Expectation-Maximization algorithm from the previous chapter: we had parameters we wished to optimize, but the presence of unobserved variables made direct optimization of those parameters intractable. We're faced with a similar problem in the context of HMMs.

Given a data set of observed emissions $\{\mathbf{x}^i\}_{i=1}^N$ where each data point $\mathbf{x}^i$ represents a sequence $(\mathbf{x}_1^i, \dots, \mathbf{x}_n^i)$ , our goal is to estimate the parameters $\pmb{\theta}, \mathbf{T}$ , and $\pmb{\pi}$ (refer back to 10.3.3 if you forget the meanings of these parameters).

With knowledge of the hidden states, this would be a relatively straightforward problem of MLE for the parameters. However, the states are latent variables, and for this reason we use the EM algorithm.

This amounts to computing the probability on hidden states in the E-step, and then based on these probabilities, we update our parameters by maximizing the expected complete-data likelihood in the M-step. As usual, we perform these E and M steps iteratively until convergence. We will make use of the Forward-Backward algorithm for the inference on probabilities of the latent states in the E-Step.

We initialize the parameters in EM arbitrarily.

# 10.5.1 E-Step

For the E-Step, we take the parameters $\pmb{\theta},\mathbf{T},\pmb{\pi}$ as fixed. For each data point $\mathbf{x}^i$ , we run Forward-Backward with these parameters to get the $\alpha$ and $\beta$ values for this data point.

The hidden variables are the states $\mathbf{s}_1, \dots, \mathbf{s}_n$ . For each data point $\mathbf{x}^i = (\mathbf{x}_1^i, \dots, \mathbf{x}_n^i)$ , we are interested in computing the predicted probabilities $\mathbf{q}^i \in [0, 1]^{n \times K}$ , for $K$ possible hidden state values. The $\mathbf{q}^i$ represent the predicted probability of each hidden state value for each time period. In particular, we have

$$
q _ {t, k} ^ {i} = p \left(\mathbf {s} _ {t} ^ {i} = k \mid \mathbf {x} _ {1} ^ {i}, \dots , \mathbf {x} _ {n} ^ {i}\right). \tag {10.23}
$$

This is the probability that state $\mathbf{s}_t^i$ takes on value $k$ given the data point $\mathbf{x}^i$ . This is the smoothing operation described in the previous section and we can use Equation 10.16 to compute our $\mathbf{q}^i$ values.

Ordinarily, we'd be done with the E-step after computing the marginal probability of each latent variable. But in this case we will also want to estimate the transition probabilities between hidden states, i.e., parameter matrix $\mathbf{T}$ . For this, we also need to calculate the joint distribution between temporally-adjacent pairs of latent variables (e.g. $\mathbf{s}_t,\mathbf{s}_{t + 1}$ ). For data point $\mathbf{x}^i$ , and for periods $t$ and $t + 1$ , we denote this as $\mathbf{Q}_{t,t + 1}^{i}\in [0,1]^{K\times K}$ , where the entries in this matrix sum to 1. This represents the distribution on pairs of states in periods $t$ and $t + 1$ for this data point. We write $\mathbf{Q}^i$ to denote the corresponding values for all pairs of time periods.

To see how the entries are calculated, we can use $Q_{t,t+1,k,\ell}^i$ to denote the transition from state $k$ at time step $t$ to state $\ell$ at time step $t+1$ ,

$$
Q _ {t, t + 1, k, \ell} ^ {i} = p \left(\mathbf {s} _ {t} ^ {i} = k, \mathbf {s} _ {t + 1} ^ {i} = \ell \mid \mathbf {x} _ {1} ^ {i}, \dots , \mathbf {x} _ {n} ^ {i}\right). \tag {10.24}
$$

This is exactly the transition inference problem that we described in the previous section. Because of this, we can directly use our $\alpha$ and $\beta$ values in the transition operation, as given by Equation (10.18).

With our $\mathbf{q}^i$ and $\mathbf{Q}^i$ values for each data point, we are ready to move on to the maximization step.

# 10.5.2 M-Step

We now solve for the expected complete-data log likelihood problem, making use of the $\mathbf{q}^i$ and $\mathbf{Q}^i$ values from the E-step.

Given knowledge of states, the complete-data likelihood for one data point with observations $\mathbf{x}$ and states $\mathbf{s}$ is

$$
p (\mathbf {x}, \mathbf {s}) = p (\mathbf {s} _ {1}; \boldsymbol {\theta}) \prod_ {t = 1} ^ {n - 1} p (\mathbf {s} _ {t + 1} | \mathbf {s} _ {t}; \mathbf {T}) \prod_ {t = 1} ^ {n} p (\mathbf {x} _ {t} | \mathbf {s} _ {t}; \boldsymbol {\pi}).
$$

With one-hot encoding of $\mathbf{x}_t$ and $\mathbf{s}_t$ , and taking the log, the expression becomes

$$
\ln [ p (\mathbf {x}, \mathbf {s}) ] = \sum_ {k = 1} ^ {K} s _ {1 k} \ln \theta_ {k} + \sum_ {t = 1} ^ {n - 1} \sum_ {k = 1} ^ {K} \sum_ {\ell = 1} ^ {K} s _ {t, k} s _ {t + 1, \ell} \ln T _ {k, \ell} + \sum_ {t = 1} ^ {n} \sum_ {k = 1} ^ {K} s _ {t, k} \sum_ {m = 1} ^ {M} x _ {t, m} \ln (\pi_ {k, m}).
$$

To see this, recall that productorials become summations when we take logarithms. The unbolded symbols represent single entries of the discrete probability distributions, and we create additional summation indices $k,\ell$ over $K$ possible hidden state values for $\mathbf{s}_t,\mathbf{s}_{t + 1}$ ( $K$ is the dimension of the one-hot encoding of $\mathbf{s}$ ). We also create an additional summation index $m$ over $M$ possible emission values for each $x_{t}$ .

From this, we would be able to solve for the MLE for the parameters for the complete-data log likelihood. Now, the states are latent variables, and we need to work with the expected complete-data log likelihood, which for a single data point $\mathbf{x}^i$ , is

$$
\mathbb {E} _ {\mathbf {s} ^ {i}} \left[ \ln \left(p \left(\mathbf {x} ^ {i}, \mathbf {s} ^ {i}\right)\right) \right] = \sum_ {k = 1} ^ {K} q _ {1 k} ^ {i} \ln \theta_ {k} + \sum_ {t = 1} ^ {n - 1} \sum_ {k = 1} ^ {K} \sum_ {\ell = 1} ^ {K} Q _ {t, t + 1, k, \ell} ^ {i} \ln T _ {k, l} + \sum_ {t = 1} ^ {n} \sum_ {k = 1} ^ {K} q _ {t, k} ^ {i} \sum_ {m = 1} ^ {M} x _ {t, m} ^ {i} \ln \pi_ {k, m}. \tag {10.25}
$$

Applying the appropriate Lagrange multipliers, and maximizing with respect to each of the parameters of interest, we can obtain the following update equations for each of the parameters (for $N$ data points):

$$
\theta_ {k} = \frac {\sum_ {i = 1} ^ {N} q _ {1 , k} ^ {i}}{N}, \quad \text {f o r a l l s t a t e s} k \tag {10.26}
$$

$$
T _ {k, l} = \frac {\sum_ {i = 1} ^ {N} \sum_ {t = 1} ^ {n - 1} Q _ {t , t + 1 , k , l} ^ {i}}{\sum_ {i = 1} ^ {N} \sum_ {t = 1} ^ {n - 1} q _ {t , k} ^ {i}}, \quad \text {f o r a l l s t a t e s} k, \ell \tag {10.27}
$$

$$
\pi_ {k, m} = \frac {\sum_ {i = 1} ^ {N} \sum_ {t = 1} ^ {n} q _ {t , k} ^ {i} x _ {t , m} ^ {i}}{\sum_ {i = 1} ^ {N} \sum_ {t = 1} ^ {n} q _ {t , k} ^ {i}}, \quad \text {f o r a l l s t a t e s k , o b s e r v a t i o n s m} \tag {10.28}
$$

After updating our parameter matrices $\theta, \mathbf{T}$ , and $\pi$ , we switch back to the E-step, continuing in this way until convergence. As with other uses of EM, it provides only a local optimum and it can be useful to try a few different random initializations.

# 10.6 Conclusion

The Hidden Markov Model is a type of latent variable model motivated by the combination of time series and discrete observations and states. We relied on the Expectation-Maximization algorithm to train a HMM, and developed the Forward-Backward algorithm to make both inference and training (the E-step) computationally efficient. Many of the ideas developed in this chapter will offer good intuition for how to develop learning and inference methods for dynamical systems and other time series models.

# Chapter 11

# Markov Decision Processes

In the previous chapter we learned about Hidden Markov Models (HMMs) in which we modeled our underlying environment as being a Markov chain where each state was hidden, but produced certain observations that we could use to infer the state. The process of learning in this setting included finding the distribution over initial hidden states, the transition probabilities between states, and the probabilities of each state producing each measurement. The movement of the underlying Markov chain from one state to the next was a completely autonomous process and questions of interest focused mainly on inference.

Markov Decision Processes (MDPs) introduce somewhat of a paradigm shift. Similar to HMMs, in the MDP setting we model our underlying environment as a Markov chain. However, our environment doesn't transition from state to state autonomously like in an HMM, but rather requires that an agent in the environment takes some action. Thus, the probability of transitioning to some state at time $t + 1$ depends on both the state and the action taken at time $t$ . Furthermore, after taking some action the agent receives a reward which you can think of as characterizing the "goodness" of performing a certain action in a certain state. Thus, our overall objective in the MDP setting is different. Rather than trying to perform inference, we rather would like to find a sequence of actions that maximizes the agent's total reward.

While MDPs can have hidden states – these are called partially-observable MDPs (POMDPs) – for the purposes of this course we will only consider MDPs where the agent's state is known. If, like in HMMs, we do not have full knowledge of our environment and thus do not know transition probabilities between states or the rewards associated with each state and action we can try to find the optimal sequence of actions using reinforcement learning—a subfield of machine learning, distinct from supervised and unsupervised learning, that is characterized by problems in which an agent seeks to explore its environment, and simultaneously use that knowledge to perform actions that exploit its environment and obtain rewards. This chapter discusses how we find the optimal set of actions given that we do have full knowledge of the environment.

ML Framework Cube: Markov Decision Processes   

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Discrete or Continuous</td><td>?</td><td>Yes</td></tr></table>

Note that the question mark under the "training" category means that learning in MDPs is neither supervised nor unsupervised but rather falls under the umbrella of reinforcement learning - an entirely separate branch of ML.

# 11.1 Formal Definition of an MDP

In an MDP, we model our underlying environment as containing any one of a set of states $S$ . At each state, an agent can take any action from the set $\mathcal{A}$ which gives them a reward according to the function $r: S \times \mathcal{A} \to [0,1]$ , which we will often see written as $r(s,a)$ . For the purposes of this class, we assume $S$ and $\mathcal{A}$ are discrete and finite, i.e. contain finitely many different elements, and also assume that rewards are deterministic, i.e. $r(s,a)$ is fixed for any $s$ and $a$ .

Given a specified state and action (state-action pair), the environment will transition into a new state according to a transition function $p: \mathcal{S} \times \mathcal{A} \to \Delta \mathcal{S}$ which we will often seen written as $p(s'|s, a)$ . That is, from our current state-action pair $(s, a)$ , for all $s' \in \mathcal{S}$ , the environment will transition to the new state $s'$ with probability $p(s'|s, a)$ . If we fix the action taken, our model behaves like a Markov chain.

We let the action we take at each state be given by a policy $\pi : S \to \mathcal{A}$ which we will often see written as $\pi(a|s)$ . Our goal is to find some optimal policy $\pi^{*}$ that maximizes total rewards. Interacting with the environment according to some policy produces a dataset

$$
\mathcal {D} = \left\{s _ {0}, a _ {0}, r _ {0}, s _ {1}, a _ {1}, r _ {1}, \dots \right\}
$$

where the subscripts indicate timesteps.

![](images/3e1d9befde45615aee7f68f4fab9ba03830b574ea339aa7a811efa7a362a329d.jpg)  
Figure 11.1: A graphical representation of an MDP.

Note that since our state transitions are modeled as a Markov chain, the Markov assumption holds:

$$
p _ {t} (s _ {t + 1} | s _ {1}, \dots s _ {t}, a _ {1}, \dots a _ {t}) = p _ {t} (s _ {t + 1} | s _ {t}, a _ {t})
$$

In other words, the probability that the environment transitions to state $s_{t+1}$ at time $t$ only depends on the state the agent was in and the action the agent took at time $t$ . Furthermore, we can assume that transition probabilities are stationary:

$$
p _ {t} (s _ {t + 1} | s _ {t}, a _ {t}) = p (s _ {t + 1} | s _ {t}, a _ {t})
$$

In other words, the probability of transitioning from one state to another does not depend on the current timestep (note how we dropped the subscript $t$ in the expression on the RHS).

We call the process of finding the optimal policy in an MDP given full knowledge of our environment planning (when we don't have prior knowledge about our environment but know that it behaves according to some MDP with unknown parameters we turn to Reinforcement Learning - see Chapter 12). Our approach to planning changes depending on whether there is a limit to the number of timesteps the agent may act for, which we call a finite horizon, or if they may act forever (or effectively forever), which we call an infinite horizon.

# 11.2 Finite Horizon Planning

In a finite horizon MDP, our objective function is as follows:

$$
\max _ {\pi} \mathbb {E} _ {s \sim p} \left[ \sum_ {t = 0} ^ {T} r _ {t} | \pi \right]
$$

Note that the only source of randomness in the system is the transition function.

We find the optimal policy for a finite horizon MDP using dynamic programming. To formalize this, let's define the optimal value function $V_{(t)}^{*}(s)$ to be the highest value achievable in state $s$ (i.e. acting under the optimal policy $\pi^{*}$ ) with $t$ timesteps remaining for the agent to act. Then, we know

$$
V _ {(1)} ^ {*} (s) = \max _ {a} [ r (s, a) ]
$$

since with only one timestep left to act, the best the agent can do is take the action that maximizes their immediate reward. We can define the optimal value function for $t > 1$ recursively as follows:

$$
V _ {(t + 1)} ^ {*} (s) = \max _ {a} [ r (s, a) + \sum_ {s ^ {\prime} \in \mathcal {S}} p (s ^ {\prime} | s, a) V _ {(t)} ^ {*} (s ^ {\prime}) ]
$$

In other words, with more than one timestep to go we take the action that maximizes not only our immediate reward but also our expected future reward. This formulation makes use of the principle of optimality or the Bellman consistency equation which states that an optimal policy consists of taking an optimal first action and then following the optimal policy from the successive state (these concepts will be discussed further in section 11.3.1). Consequently, we have a different optimal policy at each timestep where

$$
\pi_ {(1)} ^ {*} (s) = \arg \max  _ {a} [ r (s, a) ]
$$

$$
\pi_ {(t + 1)} ^ {*} (s) = \arg \max  _ {a} \left[ r (s, a) + \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} \mid s, a\right) V _ {(t)} ^ {*} \left(s ^ {\prime}\right) \right]
$$

The computational complexity of this approach is $O(|S|^2 |A|T)$ since for each state and each timestep we find the value-maximizing action which involves calculating the expected future reward requiring a summation over the value function for all states.

# 11.3 Infinite Horizon Planning

If the agent is able to act forever, a dynamic programming solution is no longer feasible as there is no base case (there will never only be 1 timestep remaining). Furthermore, we can't use the same objective function that we did when working with finite horizon MDPs since our expected reward could be infinite! Thus, we modify the objective function as follows:

$$
\max _ {\pi} \mathbb {E} _ {s \sim p} \left[ \sum_ {t = 0} ^ {\infty} \gamma^ {t} r _ {t} \right]
$$

where $\gamma \in [0,1]$ is called the discount factor. Multiplying the reward at time $t$ in the infinite sum by $\gamma^t$ makes the sum resolve to a finite number, since our rewards are bounded in $[0,1]$ . As $\gamma \rightarrow 1$ ,

rewards further in the future have more of an impact on the expected total reward and thus the optimal policy will be more "patient", acting in a way to achieve high rewards in the future rather than just maximizing short-term rewards. Conversely, as $\gamma \rightarrow 0$ , rewards further in the future will have less of an impact on the expected total reward and thus the optimal policy will be less patient, preferring to maximize short-term rewards.

The effective time horizon of an infinite horizon MDP is the number of timesteps after which $\gamma^t$ becomes so small that rewards after time $t$ are negligible. Using the formula for the sum of a geometric series we find that this is approximately $\frac{1}{1 - \gamma}$ . This yields our first approach to planning in an infinite horizon MDP which is to convert it into a finite horizon MDP where $T = \frac{1}{1 - \gamma}$ . However, note that this fraction could be arbitrarily large and that the time complexity of the dynamic programming solution is linear with respect to $T$ . This implores us to consider alternate solutions.

# 11.3.1 Value iteration

Define the value function of an infinite time horizon MDP under the policy $\pi$ as the expected discounted value from policy $\pi$ starting at state $s$ :

$$
V ^ {\pi} (s) = \mathbb {E} _ {s \sim p} \left[ \sum_ {t = 0} ^ {\infty} \gamma^ {t} r (s _ {t}, \pi (s _ {t}) | s _ {0} = s, \pi \right]
$$

Then, policy $\pi$ is weakly better than policy $\pi'$ if $V^{\pi}(s) \geq V^{\pi'}(s) \forall s$ . The optimal policy $\pi^{*}$ is that which is weakly better than all other policies.

Consequently, the optimal value function is the value function following policy $\pi^{*}$ :

$$
V ^ {*} (s) = \max _ {\pi} V ^ {\pi} (s)
$$

# Bellman Consistency Equation and Bellman Optimality

The value iteration algorithm, which we will describe soon, is based on an alternate expression for the value function. The Bellman consistency equation gives us an alternate expression for $V^{\pi}(s)$ :

$$
V ^ {\pi} (s) = r (s, \pi (s)) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {\pi} (s) ]
$$

Proof: We begin by expanding our initial expression for $V^{\pi}(s)$ :

$$
\begin{array}{l} V ^ {\pi} (s) = \mathbb {E} _ {s \sim p} \left[ \sum_ {t = 0} ^ {\infty} \gamma^ {t} r (s _ {t}, \pi (s _ {t}) | s _ {0} = s, \pi \right] \\ = r (s, \pi (s)) + \gamma \mathbb {E} _ {s \sim p} [ r (s _ {1}, a _ {1}) + \gamma r (s _ {2}, a _ {2}) + \dots | s _ {0} = s, \pi ] \\ \end{array}
$$

We now apply Adam's law (dropping the subscripts on the expectations to avoid notational clutter):

$$
= r (s, \pi (s)) + \gamma \mathbb {E} [ \mathbb {E} [ r (s _ {1}, a _ {1}) + \gamma r (s _ {2}, a _ {2}) + \dots | s _ {0} = s, s _ {1} = s ^ {\prime}, \pi) ]
$$

By the Markov property:

$$
= r (s, \pi (s)) + \gamma \mathbb {E} [ \mathbb {E} [ r (s _ {1}, a _ {1}) + \gamma r (s _ {2}, a _ {2}) + \dots | s _ {1} = s ^ {\prime}, \pi ] ]
$$

By definition, this is equal to $r(s,\pi (s)) + \gamma \mathbb{E}_{s^{\prime}\sim p}[V^{\pi}(s^{\prime})]$ .

The Bellman optimality conditions are a set of two theorems that tell us important properties about the optimal value function $V^{*}$ .

Theorem 1: $V^{*}$ satisfies the following Bellman consistency equations:

$$
V ^ {*} (s) = \max _ {a} [ r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {*} (s ^ {\prime}) ] ] \forall s
$$

This theorem also tells us that if $\hat{\pi}(s) = \arg \max_{a} [r(s, a) + \gamma \mathbb{E}_{s' \sim p}[V^{*}(s')]]$ then $\hat{\pi}$ is the optimal policy.

Proof: Let $\hat{\pi}(s) = \arg \max_{a}[r(s, a) + \gamma \mathbb{E}_{s' \sim p}[V^{*}(s')]]$ . To prove the claim it suffices to show that $V^{*}(s) \leq V^{\hat{\pi}}(s) \forall s$ . By the Bellman consistency equation we have:

$$
\begin{array}{l} V ^ {*} (s) = r \left(s, \pi^ {*} (s)\right) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} \left[ V ^ {*} \left(s ^ {\prime}\right) \right] \\ \leq \max _ {a} [ r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {*} (s ^ {\prime}) ] ] \\ = r \big (s, \hat {\pi} (s) \big) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {*} (s ^ {\prime}) ] \\ \end{array}
$$

Proceeding recursively:

$$
\begin{array}{l} \leq r (s, \hat {\pi} (s)) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ r (s ^ {\prime}, \hat {\pi} (s ^ {\prime})) + \gamma \mathbb {E} _ {s ^ {\prime \prime} \sim p} [ V ^ {*} (s ^ {\prime \prime}) ] ] \\ \cdot \cdot \cdot \\ \leq \mathbb {E} _ {s, s ^ {\prime}, \dots \sim p} [ r (s, \hat {\pi} (s)) + \gamma r (s ^ {\prime}, \hat {\pi} (s ^ {\prime}) + \dots | \hat {\pi} ] \\ = V ^ {\hat {\pi}} (s) \\ \end{array}
$$

![](images/77e70720e06ce9f9068f567b4350024e22edc99593a2800c4f21d9abad8ce3e8.jpg)

Theorem 2: For any value function $V$ , if $V(s) = \max_{a}[r(s,a) + \gamma \mathbb{E}_{s^{\prime}\sim p}[V(s^{\prime})]]$ $\forall s$ then $V = V^{*}$ . Proof: First, we define the maximal component distance between $V$ and $V^{*}$ as follows:

$$
| | V - V ^ {*} | | _ {\infty} = \max _ {s} | V (s) - V ^ {*} (s) |
$$

For $V$ which satisfies the Bellman optimality equations, if we could show that $\| V - V^{*}\|_{\infty} \leq \gamma \| V - V^{*}\|_{\infty}$ then the proof would be complete since it would imply:

$$
\left|\left| V - V ^ {*} \right|\right| _ {\infty} \leq \gamma \left|\left| V - V ^ {*} \right|\right| _ {\infty} \leq \gamma^ {2} \left|\left| V - V ^ {*} \right|\right| _ {\infty} \leq \dots \leq \lim  _ {k \rightarrow \infty} \gamma^ {k} \left|\left| V - V ^ {*} \right|\right| _ {\infty} = 0
$$

Thus we have:

$$
\begin{array}{l} | V (s) - V ^ {*} (s) | = | m a x _ {a} [ r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V (s ^ {\prime}) ] ] - \max _ {a} [ r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {*} (s ^ {\prime}) ] ] | \\ \leq \max _ {a} | r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V (s ^ {\prime}) ] - r (s, a) - \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {*} (s ^ {\prime}) ] | \\ = \gamma \max _ {a} | \mathbb {E} _ {s ^ {\prime} \sim p} [ V (s ^ {\prime}) ] - \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {*} (s ^ {\prime}) ] | \\ \leq \max _ {a} \mathbb {E} _ {s ^ {\prime} \sim p} [ | V (s ^ {\prime}) - V ^ {*} (s ^ {\prime}) ] \\ \leq \gamma \max _ {a, s ^ {\prime}} | V (s ^ {\prime}) - V ^ {*} (s ^ {\prime}) | \\ = \gamma \max _ {s ^ {\prime}} | V (s ^ {\prime}) - V ^ {*} (s ^ {\prime}) | \\ = \gamma | | V - V ^ {*} | | _ {\infty} \\ \end{array}
$$

![](images/2bce8e53736530c4e0f0d41502b96b5b497de6a60ba968bde6bd768d3179ff55.jpg)

They key takeaways from the Bellman optimality conditions are twofold. Theorem 1 gives us a nice expression for the optimal value function that we will soon show allows us to iteratively improve an arbitrary value function towards optimality. It also tells us what the corresponding optimal policy is. Theorem 2 tells us that the optimal value function is unique so if we find some value function that satisfies the Bellman consistency equations given in Theorem 1 then it must be optimal.

# Bellman Operator

In the previous section we mentioned that the formulation of the optimal value function we get from Theorem 1 will allow us to improve some arbitrary value function iteratively. In this section we will show why this is the case. Define the Bellman operator $B: \mathbb{R}^{|S|} \to \mathbb{R}^{|S|}$ to be a function that takes as input a value function and outputs another value function as follows:

$$
B (V (s)) := \max _ {a} [ r (s, a) + \gamma \mathbb {E} _ {s \sim p} [ V (s ^ {\prime}) ] ]
$$

By the Bellman optimality conditions we know that $B(V^{*}) = V^{*}$ and that this identity property doesn't hold for any other value function. The optimal value function $V^{*}$ is thus the unique fix-point of the function $B$ which means that passing $V^{*}$ into $B$ always returns $V^{*}$ .

Furthermore, we know that $B$ is a contraction mapping which means that $\| B(V) - B(V')\|_{\infty} \leq \gamma \| V - V'\|_{\infty}$ .

Proof:

$$
\begin{array}{l} | B (V (s)) - B (V ^ {\prime} (s)) | = | \max _ {a} [ r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V (s ^ {\prime}) ] - \max _ {a} [ r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V ^ {\prime} (s ^ {\prime}) ] ] | \\ \leq \max _ {a} | r (s, a) + \gamma \mathbb {E} _ {s ^ {\prime} \sim p} [ V (s ^ {\prime}) - r (s, a) - \gamma \mathbb {E} _ {s ^ {\prime} \sim p} V ^ {\prime} (s ^ {\prime}) ] | \\ = \gamma \max _ {a} | \mathbb {E} _ {s ^ {\prime} \sim p} [ V (s ^ {\prime}) - V ^ {\prime} (s ^ {\prime}) ] | \\ \leq \gamma \max _ {a} [ \mathbb {E} _ {s ^ {\prime} \sim p} [ | V (s ^ {\prime}) - V ^ {\prime} (s ^ {\prime}) | ] ] \\ \leq \gamma \max  _ {a, s} | V (s) - V ^ {\prime} (s) | \\ = \gamma | | V - V ^ {\prime} | | _ {\infty} \\ \end{array}
$$

![](images/9ccd7ab51c499fc0729e298e966569d74951d9b626963e7aaaa8f06446a49662.jpg)

# Value Iteration Algorithm

From the previous section we learned that applying the Bellman operator to an arbitrary value function repeatedly will get it closer and closer to the optimal value function. Once we see that applying the Bellman operator no longer changes the input value function, we know we have found the optimal value function. This is the basis for the value iteration algorithm which works as follows:

1. Initialize $V(s) = 0 \forall s$   
2. Repeat until convergence:

$$
\begin{array}{l} V ^ {\prime} (s) \leftarrow \max  _ {a} \left[ r (s, a) + \gamma \sum_ {s ^ {\prime} \in \mathcal {S}} p \left(s ^ {\prime} \mid s, a\right) V \left(s ^ {\prime}\right) \right] \forall s \\ V (s) \leftarrow V ^ {\prime} (s) \forall s \\ \end{array}
$$

It is a theorem that value iteration converges to $V^{*}$ asymptotically. We can also extract the optimal policy from the resulting value function asymptotically. Each iteration of value iteration takes $O(|S|^2 |A|)$ time.

# 11.3.2 Policy Iteration

An alternate approach to finding the optimal policy in an infinite horizon MDP uses the same contraction properties of the Bellman operator but iterates on the policy directly rather than on the value function. The algorithm is as follows:

1. Initialize some arbitrary policy $\pi$   
2. Repeat until convergence:

(a) Policy evaluation: Calculate $V^{\pi}(s) \forall s$   
(b) Policy improvement:

$$
\begin{array}{l} \pi^ {\prime} (s) \leftarrow \arg \max  _ {a} \left[ r (s, a) + \gamma \sum_ {s ^ {\prime} \in S} p \left(s ^ {\prime} \mid s, a\right) V ^ {\pi} \left(s ^ {\prime}\right) \right] \forall s \\ \pi (s) \leftarrow \pi^ {\prime} (s) \forall s \\ \end{array}
$$

Here, convergence means that the improvement step does not modify the policy from the previous iteration. It is a theorem that policy iteration converges to the optimal policy in a finite number of steps and also yields the optimal value function.

# Policy Evaluation

In the previous section, we saw that one of the steps in the policy iteration algorithm was to calculate the value function for each state following a policy $\pi$ . There are two ways to do this:

- Exact policy evaluation:

We know that our value function must satisfy the Bellman consistency equations:

$$
V ^ {\pi} (s) = r (s, \pi (s)) + \gamma \sum_ {s \in \mathcal {S}} p \left(s ^ {\prime} \mid s, \pi (s)\right) V ^ {\pi} \left(s ^ {\prime}\right) \quad \forall s
$$

This is a system of $|\mathcal{S}|$ linear equations that has a unique solution. Rearranging terms, representing functions as matrices and vectors, and replacing sums with matrix multiplication we get that:

$$
\mathbf {V} ^ {\pi} = (\mathbf {I} - \gamma \mathbf {P} ^ {\pi}) ^ {- 1} \mathbf {R} ^ {\pi}
$$

where $\mathbf{V}^{\pi}$ is an $|\mathcal{S}| \times 1$ vector, $\mathbf{I}$ is the identity matrix, $\mathbf{P}^{\pi}$ is an $|\mathcal{S}| \times |\mathcal{S}|$ matrix where $\mathbf{P}_{s,s'}^{\pi} = p(s'|s,\pi(s))$ , and $\mathbf{R}^{\pi}$ is an $|S| \times 1$ vector where $\mathbf{R}_s^\pi = r(s,\pi (s))$ . This subroutine runs in time $O(|\mathcal{S}|^3)$ .

- Iterative policy evaluation:

Rather than solving the system of equations described earlier exactly, we can instead do it iteratively. We perform the following two steps:

1. Initialize $\mathbf{V}^0$ such that $||\mathbf{V}^0 ||_{\infty}\in \left[0,\frac{1}{1 - \gamma}\right]$   
2. Repeat until convergence: $\mathbf{V}^{t + 1}\gets \mathbf{R} + \gamma \mathbf{P}^{\pi}\mathbf{V}^{t}$

. This algorithm runs in time $\tilde{O}\left(\frac{|S|^2}{1 - \gamma}\right)$

Policy iteration requires fewer iterations than value iteration to solve for the optimal policy $\pi^{*}$ , but requires more time per iteration: $O(|S|^2 |A| + |S|^3)$ if we use exact policy evaluation.

# Chapter 12

# Reinforcement Learning

# 12.1 Motivation

In the last chapter, we discussed the Markov Decision Process (MDP): a framework that models a learner's environment as a vector of states, actions, rewards, and transition probabilities. Given this model, we can solve for an optimal (reward-maximizing) policy using either value iteration or policy iteration. Sometimes, however, the learner doesn't have prior knowledge of the rewards they will get from each state or the probability distribution over states they could end up in after taking some action from their current state. Is it still possible to learn a policy that will maximize rewards? In this chapter, we will learn about Reinforcement Learning (RL) - a machine learning technique that addresses this problem.

# ML Framework Cube: Reinforcement Learning

<table><tr><td>Domain</td><td>Training</td><td>Probabilistic</td></tr><tr><td>Discrete or Continuous</td><td>?</td><td>Yes</td></tr></table>

Note that we will only cover RL for discrete state spaces in this chapter. The question mark under the "training" category means that RL is neither supervised nor unsupervised.

# 12.2 General Approaches to RL

Imagine that you are vacationing in a town that has 20 restaurants. Suppose that if you had been able to try the food at every restaurant you would have had a definitive ranking of which was your favorite, second favorite, third favorite and so on. Your vacation is 10 days long and your budget allows you to eat at one restaurant per day. Your goal on your vacation is to have the best experience, which in this case means to eat at the restaurants you like the most. The problem is that you don't have enough time to figure out which restaurants you like the most since you don't have enough time to eat at each one. In this situation, you have two key considerations to balance:

1. Time spent exploring. If you spend the right amount of time exploring new restaurants, then there is a good chance that you will find one that you like a lot. However, spending too much time exploring will result in an average experience overall since some restaurants will be good and some will be bad.

2. Time spent exploiting. If you spend the right amount of time exploiting your knowledge of what restaurants you've liked the most from your exploration you will have a good experience. However, if you spend too long exploiting (and consequently less time exploring) you risk missing out on eating at all the restaurants that you could have liked more.

This is called the exploration vs exploitation tradeoff. The more time you spend exploring the less time you can spend exploiting, and vice versa. In order to have the best vacation experience, you need to find a balance between time spent exploring different restaurants and time spent exploiting what you have learned by eating at that restaurants that you liked the most thus far. A reinforcement learner's task is similar to the vacation restaurant task described above. They must balance time spent exploring the environment by taking actions and observing rewards and penalties, and time spent maximizing rewards based on what they know.

We now return to the problem described in the previous section: how do we learn a policy that will maximize rewards in an MDP where we know nothing about which rewards we will get from each state and which state we will end up in after taking an action? One approach is to try and learn the MDP. In other words, we can create a learner that will explore its environment for some amount of time in order to learn the rewards associated with each state and the transition probabilities associated with each action. Then, they can use an MDP planning algorithm like policy iteration or value iteration to come up with an optimal policy and maximize their rewards. This is called model-based learning. The main advantage of model-based learning is that it can inexpensively incorporate changes in the reward structure or transition function of the environment into the model. However, model-based learning is computationally expensive, and an incorrect model can yield a sub-optimal policy.

Conversely, model-free learning is a family of RL techniques where a learner tries to learn the optimal policy directly without modelling the environment. While this sort of learning is less adaptive to changes in the environment, it is far more computationally efficient than model-based learning.

# 12.3 Model-Free Learning

While there are many methods within the family of model-free learning, we will focus on three value-based methods - SARSA (section 12.3.1), $Q$ -learning (section 12.3.1) and Deep $Q$ -learning (section 12.3.2) - and one policy-based method - Policy Learning (section 12.3.3).

We will begin by discussing value-based methods. In this family of RL algorithms, the learner tries to calculate the expected reward they will receive from a state $s$ upon taking action $a$ . Formally, they are trying to learn a function that maps $(s, a)$ to some value representing the expected reward. This is called the Q-function and is defined as follows for some policy $\pi$ :

$$
Q ^ {\pi} (s, a) = r (s, a) + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) V ^ {\pi} \left(s ^ {\prime}\right) \tag {12.1}
$$

In words, the approximate expected reward (Q value) for taking action $s$ from state $a$ is the actual reward received from the environment by doing so in the current iteration plus the expectation taken over all reachable states of the highest value achievable starting at that state times the

discount factor $\gamma$ . The Q-function following the optimal policy is defined analogously:

$$
Q ^ {*} (s, a) = r (s, a) + \gamma \sum_ {s ^ {\prime}} p \left(s ^ {\prime} \mid s, a\right) V ^ {*} \left(s ^ {\prime}\right) \tag {12.2}
$$

Note, that $V^{*}(s^{\prime}) = \max_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime})$ since the highest value achievable from state $s^\prime$ following policy $*$ is the Q value of taking the optimal action from $s^\prime$ . Substituting this in, we get the following Bellman Equation:

$$
Q ^ {*} (s, a) = r (s, a) + \gamma \sum_ {s} ^ {\prime} p \left(s ^ {\prime} \mid s, a\right) \max  _ {a ^ {\prime}} Q ^ {*} \left(s ^ {\prime}, a\right) \tag {12.3}
$$

Note that we can't directly calculate the term $\gamma \sum_{s}^{\prime}p(s^{\prime}|s,a)\max_{a}^{\prime}Q^{*}(s^{\prime},a)$ since we don't know $p(s^{\prime}|s,a)$ . We will discuss how this is addressed by the two algorithms we will cover in the value-based family.

# 12.3.1 SARSA and Q-Learning

At a high level value-based model-free reinforcement learners work by initializing the values of $Q(s, a)$ for all states $s$ and all actions $a$ in an $s \times a$ matrix. They then repeat the following two steps until satisfactory performance is achieved:

1. Act based on Q values   
2. Use $s$ (current state), $a$ (action), $r$ (reward), $s'$ (next state), $a'$ (action taken from next state) in order to update the approximation of $Q(s, a)$

We will refer to $(s, a, r, s', a')$ as an experience. Let $\pi(s)$ we the action that a learner takes from state $s$ . One strategy for acting that attempts to balance exploration and exploitation is called $\epsilon$ -greedy and defined as follows:

$$
\pi (s) = \left\{ \begin{array}{l} \operatorname {a r g m a x} _ {a} Q (s, a) \text {w i t h p r o b a b i l i t y} 1 - \epsilon \\ \text {r a n d o m w i t h p r o b a b i l i t y} \epsilon \end{array} \right. \tag {12.4}
$$

Here, $\epsilon$ is some number $\in [0,1]$ which controls how likely the learner is to choose a random action as opposed to the currently known optimal action. Varying the value of $\epsilon$ changes the balance between exploration and exploitation.

Once the learner has had an experience, they can begin to learn $Q^{*}$ . We will now describe two

![](images/ed48460b3df71851191cdc74ac3ee3327569749686ad829618360294ee4c334a.jpg)  
Figure 12.1: The process of model-free learning

algorithms which perform this update differently for every new experience:

1. SARSA: $Q(s, a) \gets \alpha_{t}(s, a)[r + \gamma Q(s', a') - Q(s, a)]$ where $\alpha_{t}(s, a)$ is the learning rate, a parameter which controls how much the observation affects $Q(s, a)$ .

The expression $r + \gamma Q(s', a')$ is a 1-step estimate of $Q(s, a)$ . The expression in the square brackets above is called the temporal difference (TD) error and represents the difference between the previous estimate of $Q(s, a)$ and the new one. Since the action $a'$ that we use for our update is the one that was recommended by the policy $\pi$ (recall that $a' = \pi(s'))$ , SARSA is an on-policy algorithm. This means that if there was no epsilon greedy action selection, the reinforcement learner would always act according to the policy $\pi$ and SARSA would converge to $V^{\pi}$ .

2. Q-learning: $Q(s, a) \gets Q(s, a) + \alpha_t(s, a)[r + \gamma \max_a'(s', a') - Q(s, a)]$

Similar to SARSA, the expression in the square brackets is the TD error. Note, that in Q-learning the update depends on the reward-maximizing action $\max_{a}^{\prime}(s^{\prime},a^{\prime})$ corresponding to the Bellman equation $Q^{*}$ rather than the policy-recommended action. This makes it an off-policy algorithm. In other words, Q-learning is computing $V^{*}$ which may or may not correspond to actions recommended by the policy $\pi$ .

This procedure of learning $Q$ by updating $Q(s, a)$ by the TD error is called TD updating.

# Convergence Conditions

Let $\alpha_{t}(s,a) = 0$ for all $(s,a)$ that are not visited at time $t$ . It is a theorem that Q-learning converges to $Q^{*}$ (and hence $\pi$ converges to $\pi^{*}$ ) as $t\to \infty$ as long as the following two conditions are met:

- $\sum_{t} \alpha_{t}(s, a) = \infty \forall s, a$

The sum of the learning rate over infinitely many time steps must diverge. In order for this to happen, each state-action pair $(s,a)$ must be visited infinitely often. Thus, we see the importance of an $\epsilon$ -greedy learner which forces the agent to probabilistically take random actions in order to explore more of the state space.

- $\sum_{t} \alpha(s, a)^2 < \infty \forall s, a$

The sum of the square of the learning rate over infinitely many time steps must converge. Note that in order for $\alpha_{t}(s,a)^{2} < \infty$ , the learning rate must be iteratively reduced. For example, we could set $\alpha_{t}(s,a) = \frac{1}{N_{t}(s,a)}$ where $N_{t}(s,a)$ is the number of times the learner took action $a$ from state $s$ .

SARSA converges to $Q^{*}$ if the above two conditions are met and behavior is greedy in the limit (i.e. $\epsilon \to 0$ as $t \to \infty$ ). One common choice is $\epsilon_{t}(s) = \frac{1}{N_{t}(s)}$ where $N_{t}(s)$ is the number of times the learner visited state $s$ . The notation $\epsilon_{t}(s)$ implies that a separate $\epsilon$ value is maintained for every state rather than just maintaining one value of $\epsilon$ that controls learner behavior across all states (though this is also an option).

Citation: Chapter 6 of Reinforcement Learning: An Introduction by Sutton and Barto.

# 12.3.2 Deep Q-Networks

Sometimes, we have scenarios where there are too many state-action pairs to learn via traditional TD updates in a reasonable amount of time. For example, consider the game of chess where there are mind bogglingly many possible configurations of the board. In cases like these, we can use a universal function approximator, like a convolutional neural network, to estimate the Q-function.

The neural network takes the learner's current state as input and outputs the approximated Q-values for taking each possible action from that state by training parameters $w$ . The learner's next action is the maximum over all outputs of the neural network.

![](images/85dcff9b68a28bd0a7fe2b1b11ff40468479f92939e39e7be969f21b10ed5f7d.jpg)  
Figure 12.2: Q-learning vs Deep Q-learning.(Image citation: https://www.mlq.ai/deep-reinforcement-learning-q-learning/)

While there are several specific variants depending on the problem that is trying to be solved, the general loss function that the neural network tries to minimize at iteration $i$ is as follows:

$$
\mathcal {L} \left(w _ {i}\right) = E \left[ \left(r + \gamma \max  _ {a ^ {\prime}} Q \left(s ^ {\prime}, a ^ {\prime}; w _ {i - 1}\right) - Q \left(s, a; w _ {i}\right)\right) ^ {2} \right] \tag {12.5}
$$

Here, $(s,a,r,s^{\prime},a^{\prime})$ and $\gamma$ are defined the same as in regular Q-learning. $w_{i}$ are the parameters that the neural net is training during the current iteration of the RL algorithm, and $w_{i - 1}$ are the optimal parameters from the previous iteration of the algorithm. The TD error is the term $r + \gamma \max_{a'}Q(s',a';w_{i - 1})$ , and the squared term inside the expectation is the TD target. Since directly optimizing the loss in equation 12.5 is difficult, gradient descent, specifically stochastic gradient descent is used (in order to avoid having to calculate the entire expected value term in 12.5).

The Atari deep Q network that solved the game of brickbreaker used a technique called experience replay to make updates to the network more stable. The experience $(s, a, r, s')$ was put into a replay buffer which was sampled from to perform minibatch gradient descent to minimize the loss function.

# 12.3.3 Policy Learning

There are a few situations in which the RL techniques described thus far don't work well or work relatively less well than alternatives:

1. The Q function is far more complex than the policy being learned.   
2. The action space is continuous.

# 3. Expert knowledge needs to be introduced.

This presents the need for another model-free method called policy learning in which we adopt a differentiable policy $\pi_{\theta}(a|s)$ parametrized by $\theta$ , and try to learn $\theta$ directly without going through a Q function first.

We update $\theta$ iteratively according to the following equation:

$$
\theta \leftarrow \theta + \alpha_ {t} \nabla_ {\theta} J (\theta)
$$

where $J(\theta) = E[r(h)]$ , $r(h) = \sum_{t} r_{t}$ , and $h = (s, a, r, s', a', r', s'', a''\ldots)$ . The gradient term approximates the performance $V^{\pi}$ . We define the likelihood $\mu_{\theta}(h) = \prod_{t} \pi_{\theta}(a_{t}|s_{t}) p(s_{t+1}|s_{t}, a)$ . We can expand $\nabla_{\theta}(\theta)$ as follows:

$$
\nabla_ {\theta} J (\theta) = \nabla_ {\theta} E [ r (n) ] = \nabla_ {\theta} \int_ {h} \mu_ {\theta} (h) r (h) \delta h = \int_ {h} r (h) \nabla_ {\theta} \mu_ {\theta} (h) \delta h \tag {12.6}
$$

We seem to have run into an issue here. The term $\nabla_{\theta}\mu_{\theta}(h)$ depends on the transition probability $p(*|s,a)$ by definition, but point of policy learning was to avoid having to learn these transition probabilities. However, it turns out we can circumvent this issue by applying the useful identity

$$
\nabla_ {\theta} \mu_ {\theta} = \mu_ {\theta} \frac {1}{\mu_ {\theta}} \nabla_ {\theta} \mu_ {\theta} = \mu_ {\theta} \nabla_ {\theta} \ln \mu_ {\theta} \tag {12.7}
$$

Thus, we can rewrite the previous equation as follows:

$$
\begin{array}{l} \int_ {h} r (h) \mu_ {\theta} (h) \nabla_ {\theta} \left[ \sum_ {t} \ln \pi_ {\theta} \left(a _ {t} \mid s _ {t}\right) + \sum_ {t} \ln p \left(s _ {t + 1} \mid s _ {t}, a\right) \right] \delta h (12.8) \\ = E \left[ r (h) \sum_ {t} \nabla_ {\theta} \ln \pi_ {\theta} \left(a _ {t} \mid s _ {t}\right) \right] (12.9) \\ = E \left[ \sum_ {t} \nabla_ {\theta} \ln \pi_ {\theta} \left(a _ {t} \mid s _ {t}\right) r _ {t} \right] (12.10) \\ \end{array}
$$

Equation 12.10 only involves $\pi_{\theta}(a_t|s_t)$ which we are trying to learn, and $r_t$ which we observe from the environment. We can now perform SGD to find the optimal $\theta$ . Note policy learning is on-policy.

# 12.4 Model-Based Learning

![](images/bbdd91ed790fa9ce6709e925385abed23fd773eff2d6cf54e09d9e38db628f97.jpg)  
Figure 12.3: The process of model-based learning.

Recall that in model-based learning, the learner tries to learn the parameters of the MDP underlying their environment and then uses planning to come up with a policy. A model-based learner repeats the following three steps until satisfactory performance is achieved:

1. Act according to some policy for $M$ steps.   
2. Learn/update a model of the underlying MDP based on the experiences gained from step 1.   
3. Use the model from step 2 to plan for the next $M$ steps.

In many implementations, "learning" a model means coming up with a maximum likelihood estimate of the parameters of the underlying MDP: the reward function and transition probabilities (we will not cover the specifics of this process in this section). We can now plan according to this model and follow the recommended policy for the next $M$ steps. One issue with this basic model is that it doesn't do anything to handle time spent exploring vs exploiting. While there are many ways to address this in practice, we will now present three common approaches:

- Incorporate exploration directly into the policy, for example, by using an $\epsilon$ -greedy strategy as discussed in 12.3.1.   
- Optimism under uncertainty: Let $N(s, a)$ be the number of times we visited the state-action pair $(s, a)$ . Each "visit" is a time that the agent was in state $s$ and chose to perform action $a$ . When we are learning, we will assume that if $N(s, a)$ is small, then the next state will have higher reward than predicted by the maximum likelihood model. Since the model thinks taking visiting lesser-known state-action pairs will lead to high reward (we are optimistic when we are uncertain), we'll have a tendency to explore these lesser-known areas.   
- Posterior sampling/Thompson sampling: We maintain a posterior distribution such as a Dirichlet over the transition function $p(*|s, a) \forall s, a$ which we update using the experiences from the previous $M$ steps of acting rather than coming up with a maximum likelihood point estimate. We then sample from this posterior to get a model, which we proceed to plan with. The idea is that when we are less certain about the outcome of the transition function, there's a greater likelihood of the sampling leading us to explore. When we are more certain about what the best outcome is, we're more likely to just stick with that best outcome.

# 12.5 Conclusion

To recap, reinforcement learning is a machine learning technique that allows a learner to find an optimal policy in an environment modeled by an MDP where the transition probabilities and reward function are unknown. The learner gains information by interacting with the environment and receiving rewards and uses this information to update the model they have of the underlying MDP (model-based) or their beliefs about the value of taking particular actions from particular states (value-based model-free). Q-learning and temporal difference updating are key topics in model-free learning that underlie many techniques in the area including SARSA, Q-learning and deep Q-learning. Contemporary RL models such as the one used by AlphaGo often use a combination of supervised and reinforcement learning methods which can be endlessly customized to meet the needs of a problem.

# 关于封面

本书封面插画的标题为“伊斯特里亚人”（“Man from Istria”，伊斯特里亚是克罗地亚面向亚得里亚海的一个很大半岛）。该插画来自克罗地亚斯普利特民族博物馆2008年出版的BalthasarHacquet的《图说西南及东汪达尔人、伊利里亚人和斯拉夫人》（Images and Descriptions of South-western and Eastern Wenda, Illyrians, and Slavs）的最新重印版本。Hacquet（1739—1815）是一名奥地利内科医生及科学家，他花费数年时间去研究各地的植物、地质和人种，这些地方包括奥匈帝国的多个地区，以及伊利里亚部落过去居住的（罗马帝国的）威尼托地区、尤里安阿尔卑斯山脉及西巴尔干等地区。Hacquet发表的很多论文和书籍中都有手绘插图。

Hacquet出版物中丰富多样的插图生动地描绘了200年前西阿尔卑斯和巴尔干西北地区的独特性和个体性。那时候相距几英里的两个村庄村民的衣着都迥然不同，当有社交活动或交易时，不同地区的人们很容易通过着装来辨别。从那之后着装的要求发生了改变，不同地区的多样性也逐渐消亡。现在很难说出不同大陆的居民有多大区别，比如，现在很难区分斯洛文尼亚的阿尔卑斯山地区或巴尔干沿海那些美丽小镇或村庄里的居民和欧洲其他地区或美国的居民。

Manning出版社利用两个世纪之前的服装来设计书籍封面，以此来赞颂计算机产业所具有的创造性、主动性和趣味性。正如本书封面的图片一样，这些图片也把我们带回到过去的生活中去。

# 目录

# 第一部分 分类

# 第1章 机器学习基础

1.1 何谓机器学习·

1.1.1 传感器和海量数据  
1.1.2 机器学习非常重要…

1.2 关键术语· 5

1.3 机器学习的主要任务  
1.4 如何选择合适的算法… 8  
1.5 开发机器学习应用程序的步骤… 9  
1.6 Python语言的优势… 10

1.6.1 可执行伪代码… 10  
1.6.2 Python比较流行 10  
1.6.3 Python语言的特色… 11  
1.6.4 Python语言的缺点 11

1.7 NumPy 函数库基础 … 12   
1.8 本章小结· 13

# 第2章 $k$ -近邻算法 15

2.1 $k$ -近邻算法概述 15

2.1.1 准备：使用Python导入数据……17  
2.1.2 从文本文件中解析数据………19  
2.1.3 如何测试分类器… 20

2.2 示例：使用 $k$ -近邻算法改进约会网站的配对效果

2.2.1 准备数据：从文本文件中解析数据· 21  
2.2.2 分析数据：使用Matplotlib创建散点图 23  
2.2.3 准备数据：归一化数值… 25  
2.2.4 测试算法：作为完整程序验证分类器… 26

2.2.5 使用算法：构建完整可用系统· 27

2.3 示例：手写识别系统 28

2.3.1 准备数据：将图像转换为测试向量… 29  
2.3.2 测试算法：使用 $k$ -近邻算法识别手写数字 30

2.4 本章小结· 31

# 第3章 决策树 32

3.1 决策树的构造 33

3.1.1 信息增益· 35  
3.1.2 划分数据集 37  
3.1.3 递归构建决策树… 39

3.2 在Python中使用Matplotlib注解绘制树形图 42

3.2.1 Matplotlib注解 43  
3.2.2 构造注解树 44

3.3 测试和存储分类器 48

3.3.1 测试算法：使用决策树执行分类· 49  
3.3.2 使用算法：决策树的存储… 50

3.4 示例：使用决策树预测隐形眼镜类型…50  
3.5 本章小结· 52

# 第4章 基于概率论的分类方法：朴素贝叶斯 53

4.1 基于贝叶斯决策理论的分类方法 53  
4.2 条件概率· 55   
4.3 使用条件概率来分类 56  
4.4 使用朴素贝叶斯进行文档分类… 57  
4.5 使用Python进行文本分类 58

4.5.1 准备数据：从文本中构建词向量… 58  
4.5.2 训练算法：从词向量计算概率· 60  
4.5.3 测试算法：根据现实情况修改分类器· 62   
4.5.4 准备数据：文档词袋模型 64

4.6 示例：使用朴素贝叶斯过滤垃圾邮件 64

4.6.1 准备数据：切分文本… 65  
4.6.2 测试算法：使用朴素贝叶斯进行交叉验证 66

4.7 示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向… 68

4.7.1 收集数据：导入 RSS 源 68  
4.7.2 分析数据：显示地域相关的用词· 71

4.8 本章小结 72

第5章 Logistic回归 73

5.1 基于Logistic回归和Sigmoid函数的分类… 74  
5.2 基于最优化方法的最佳回归系数确定

5.2.1 梯度上升法… 75   
5.2.2 训练算法：使用梯度上升找到最佳参数… 77  
5.2.3 分析数据：画出决策边界… 79  
5.2.4 训练算法：随机梯度上升… 80

5.3 示例：从疝气病症预测病马的死亡率…… 85

5.3.1 准备数据：处理数据中的缺失值… 85  
5.3.2 测试算法：用Logistic回归进行分类· 86

5.4 本章小结 ………………………………………… 88

第6章 支持向量机 89

6.1 基于最大间隔分隔数据… 89  
6.2 寻找最大间隔 91  
6.2.1 分类器求解的优化问题… 92

6.2.2 SVM应用的一般框架 93  
6.3 SMO高效优化算法… 94   
6.3.1 Platt的SMO算法… 94   
6.3.2 应用简化版SMO算法处理小规模数据集 94

6.4 利用完整PlattSMO算法加速优化… 99  
6.5 在复杂数据上应用核函数 … 105

6.5.1 利用核函数将数据映射到高维空间 106  
6.5.2 径向基核函数 … 106  
6.5.3 在测试中使用核函数… 108

6.6 示例：手写识别问题回顾 … 111  
6.7 本章小结 ………………………………………… 113

第7章 利用AdaBoost元算法提高分类性能 115

7.1 基于数据集多重抽样的分类器 115

7.1.1 bagging: 基于数据随机重抽样的分类器构建方法…… 116  
7.1.2 boosting 116

7.2 训练算法：基于错误提升分类器的性能 117

7.3 基于单层决策树构建弱分类器 … 118  
7.4 完整AdaBoost算法的实现 122   
7.5 测试算法：基于AdaBoost的分类……124  
7.6 示例：在一个难数据集上应用 AdaBoost 125

7.7 非均衡分类问题 127

7.7.1 其他分类性能度量指标：正确率、召回率及ROC曲线 128  
7.7.2 基于代价函数的分类器决策控制 131  
7.7.3 处理非均衡问题的数据抽样方法 132

7.8 本章小结 ………………………………………… 132

# 第二部分 利用回归预测数值型数据

第8章 预测数值型数据：回归 136

8.1 用线性回归找到最佳拟合直线 ……………………………… 136

8.2 局部加权线性回归 141   
8.3 示例：预测鲍鱼的年龄 ………………………………………… 145   
8.4 缩减系数来“理解”数据 … 146

8.4.1 岭回归 … 146  
8.4.2 lasso 148   
8.4.3 前向逐步回归 … 149

8.5 权衡偏差与方差 … 152   
8.6 示例：预测乐高玩具套装的价格……153

8.6.1 收集数据：使用Google购物的API· 153  
8.6.2 训练算法：建立模型 155

8.7 本章小结 ………………………………………… 158

# 第9章 树回归 … 159

9.1 复杂数据的局部性建模 … 159   
9.2 连续和离散型特征的树的构建 …………………… 160   
9.3 将CART算法用于回归 163

9.3.1 构建树 … 163   
9.3.2 运行代码 165

9.4 树剪枝 167

9.4.1 预剪枝 … 167   
9.4.2 后剪枝 168

9.5 模型树 170  
9.6 示例：树回归与标准回归的比较……173  
9.7 使用Python的Tkinter库创建GUI……176

9.7.1 用Tkinter创建GUI… 177  
9.7.2 集成Matplotlib和Tkinter……179

9.8 本章小结 ………………………………………… 182

# 第三部分 无监督学习

# 第10章 利用K-均值聚类算法对未标注数据分组 184

10.1 K-均值聚类算法 … 185  
10.2 使用后处理来提高聚类性能 … 189  
10.3 二分K-均值算法 ………………………………………… 190  
10.4 示例：对地图上的点进行聚类 …………………… 193

10.4.1 Yahoo! PlaceFinder API……… 194   
10.4.2 对地理坐标进行聚类… 196

10.5 本章小结 ………………………………………… 198

# 第11章 使用Apriori算法进行关联分析 200

11.1 关联分析 201  
11.2 Apriori原理 202  
11.3 使用Apriori算法来发现频繁集… 204

11.3.1 生成候选项集… 204  
11.3.2 组织完整的Apriori算法… 207

11.4 从频繁项集中挖掘关联规则… 209  
11.5 示例：发现国会投票中的模式……212

11.5.1 收集数据：构建美国国会投票记录的事务数据集…213  
11.5.2 测试算法：基于美国国会投票记录挖掘关联规则 219

11.6 示例：发现毒蘑菇的相似特征……220  
11.7 本章小结· 221

# 第12章 使用FP-growth算法来高效发现频繁项集 223

12.1 FP树：用于编码数据集的有效方式 224  
12.2 构建FP树 225

12.2.1 创建FP树的数据结构 226  
12.2.2 构建FP树 227

12.3 从一棵FP树中挖掘频繁项集… 231

12.3.1 抽取条件模式基… 231  
12.3.2 创建条件FP树 232

12.4 示例：在Twitter源中发现一些共现词· 235

12.5 示例：从新闻网站点击流中挖掘……238  
12.6 本章小结· 239

# 第四部分 其他工具

# 第13章 利用PCA来简化数据 242

13.1 降维技术· 242  
13.2 PCA 243

13.2.1 移动坐标轴 243   
13.2.2 在 NumPy 中实现 PCA ………… 246

13.3 示例：利用PCA对半导体制造数据降维 248  
13.4 本章小结· 251

# 第14章 利用SVD简化数据 252

# 第15章 大数据与MapReduce 270

14.1 SVD的应用 252  
14.1.1 隐性语义索引 253   
14.1.2 推荐系统 253  
14.2 矩阵分解 254  
14.3 利用Python实现SVD 255  
14.4 基于协同过滤的推荐引擎… 257  
14.4.1 相似度计算 257  
14.4.2 基于物品的相似度还是基于用户的相似度？ 260  
14.4.3 推荐引擎的评价… 260  
14.5 示例：餐馆菜肴推荐引擎……260  
14.5.1 推荐未尝过的菜肴… 261   
14.5.2 利用SVD提高推荐的效果…263  
14.5.3 构建推荐引擎面临的挑战…265  
14.6 基于SVD的图像压缩… 266  
14.7 本章小结· 268  
15.1 MapReduce: 分布式计算的框架 ……271  
15.2 Hadoop流 273  
15.2.1 分布式计算均值和方差的 mapper 273  
15.2.2 分布式计算均值和方差的reducer 274  
15.3 在Amazon网络服务上运行Hadoop程序 275

15.3.1 AWS上的可用服务… 276  
15.3.2 开启Amazon网络服务之旅…276  
15.3.3 在EMR上运行Hadoop作业· 278

15.4 MapReduce上的机器学习 282  
15.5 在Python中使用mrjob来自动化MapReduce 283

15.5.1 mrjob与EMR的无缝集成…283  
15.5.2 mrjob的一个MapReduce脚本剖析· 284

15.6 示例：分布式SVM的Pegasos算法 286

15.6.1 Pegasos算法 287  
15.6.2 训练算法：用mrjob实现MapReduce版本的SVM… 288

15.7 你真的需要MapReduce吗？ 292  
15.8 本章小结 ………………………………………… 292

附录APython入门 294  
附录B 线性代数 303  
附录C 概率论复习 309   
附录D 资源 312  
索引 313  
版权声明 316

# 第一部分

# 分类

本书前两部分主要探讨监督学习（supervised learning）。在监督学习的过程中，我们只需要给定输入样本集，机器就可以从中推演出指定目标变量的可能结果。监督学习相对比较简单，机器只需从输入数据中预测合适的模型，并从中计算出目标变量的结果。

监督学习一般使用两种类型的目标变量：标称型和数值型。标称型目标变量的结果只在有限目标集中取值，如真与假、动物分类集合{爬行类、鱼类、哺乳类、两栖类、植物、真菌}；数值型目标变量则可以从无限的数值集合中取值，如0.100、42.001、1000.743等。数值型目标变量主要用于回归分析，将在本书的第二部分研究，第一部分主要介绍分类。

本书的前七章主要研究分类算法，第2章讲述最简单的分类算法： $k-$ 近邻算法，它使用距离矩阵进行分类；第3章引入了决策树，它比较直观，容易理解，但是相对难于实现；第4章将讨论如何使用概率论建立分类器；第5章将讨论Logistic回归，如何使用最优参数正确地分类原始数据，在搜索最优参数的过程中，将使用几个经常用到的优化算法；第6章介绍了非常流行的支持向量机；第一部分最后的第7章将介绍元算法——AdaBoost，它由若干个分类器构成，此外还总结了第一部分探讨的分类算法在实际使用中可能面对的非均衡分类问题，一旦训练样本某个分类的数据多于其他分类的数据，就会产生非均衡分类问题。

# 第1章

# 机器学习基础

# 本章内容

□ 机器学习的简单概述  
□ 机器学习的主要任务  
□学习机器学习的原因  
Python语言的优势

最近我和一对夫妇共进晚餐，他们问我从事什么职业，我回应道：“机器学习。”妻子回头问丈夫：“亲爱的，什么是机器学习？”她的丈夫答道：“T-800型终结者。”在《终结者》系列电影中，T-800是人工智能技术的反面样板工程。不过，这位朋友对机器学习的理解还是有所偏差的。本书既不会探讨和计算机程序有关的话题，也不会与计算机探讨人生的意义。机器学习能让我们自数据集中受到启发，换句话说，我们会利用计算机来彰显数据背后的真实含义，这才是机器学习的真实含义。它既不是只会徒然模仿的机器人，也不是具有人类感情的仿生人。

现今，机器学习已应用于多个领域，远超出大多数人的想象，下面就是假想的一日，其中很多场景都会碰到机器学习：假设你想起今天是某位朋友的生日，打算通过邮局给她邮寄一张生日贺卡。你打开浏览器搜索趣味卡片，搜索引擎显示了10个最相关的链接。你认为第二个链接最符合你的要求，点击了这个链接，搜索引擎将记录这次点击，并从中学习以优化下次搜索结果。然后，你检查电子邮件系统，此时垃圾邮件过滤器已经在后台自动过滤垃圾广告邮件，并将其放在垃圾箱内。接着你去商店购买这张生日卡片，并给你朋友的孩子挑选了一些尿布。结账时，收银员给了你一张1美元的优惠券，可以用于购买6罐装的啤酒。之所以你会得到这张优惠券，是因为款台收费软件基于以前的统计知识，认为买尿布的人往往也会买啤酒。然后你去邮局邮寄这张贺卡，手写识别软件识别出邮寄地址，并将贺卡发送给正确的邮车。当天你还去了贷款申请机构，查看自己是否能够申请贷款，办事员并不是直接给出结果，而是将你最近的金融活动信息输入计算机，由软件来判定你是否合格。最后，你还去了赌场想找些乐子，当你步入前门时，尾随你进来的一个家伙被突然出现的保安给拦了下来。“对不起，索普先生，我们不得不请您离开赌场。我们不欢迎老千。”图1-1集中展示了使用到的机器学习应用。

上面提到的所有场景，都有机器学习软件的存在。现在很多公司使用机器学习软件改善商业决策、提高生产率、检测疾病、预测天气，等等。随着技术指数级增长，我们不仅需要使用更好

的工具解析当前的数据，而且还要为将来可能产生的数据做好充分的准备。

![](images/0f62712dbe2282f149708d2713a7c0ac1095ac6b67403b974fb023387e5fc463.jpg)  
图1-1 机器学习在日常生活中的应用，从左上角按照顺时针方向依次使用到的机器学习技术分别为：人脸识别、手写数字识别、垃圾邮件过滤和亚马逊公司的产品推荐

现在正式进入本书机器学习的主题。本章我们将首先介绍什么是机器学习，日常生活中何处将用到机器学习，以及机器学习如何改进我们的工作和生活；然后讨论使用机器学习解决问题的一般办法；最后介绍为什么本书使用Python语言来处理机器学习问题。我们将通过一个Python模块NumPy来简要介绍Python在抽象和处理矩阵运算上的优势。

# 1.1 何谓机器学习

除却一些无关紧要的情况，人们很难直接从原始数据本身获得所需信息。例如，对于垃圾邮件的检测，侦测一个单词是否存在并没有太大的作用，然而当某几个特定单词同时出现时，再辅以考察邮件长度及其他因素，人们就可以更准确地判定该邮件是否为垃圾邮件。简单地说，机器学习就是把无序的数据转换成有用的信息。

机器学习横跨计算机科学、工程技术和统计学等多个学科，需要多学科的专业知识。稍后你就能了解到，它也可以作为实际工具应用于从政治到地质学的多个领域，解决其中的很多问题。甚至可以这么说，机器学习对于任何需要解释并操作数据的领域都有所裨益。

开发出能够识别鸟类的计算机软件，鸟类学者就可以退休了。因为鸟类学者是研究鸟类的专家，因此我们说创建的是一个专家系统。

表1-1是我们用于区分不同鸟类需要使用的四个不同的属性值，我们选用体重、翼展、有无脚蹼以及后背颜色作为评测基准。现实中，你可能会想测量更多的值。通常的做法是测量所有可测属性，而后再挑选出重要部分。下面测量的这四种值称之为特征，也可以称作属性，但本书一律将其称为特征。表1-1中的每一行都是一个具有相关特征的实例。

表1-1 基于四种特征的鸟物种分类表  

<table><tr><td></td><td>体重(克)</td><td>翼展(厘米)</td><td>脚蹼</td><td>后背颜色</td><td>种属</td></tr><tr><td>1</td><td>1000.1</td><td>125.0</td><td>无</td><td>棕色</td><td>红尾鹫</td></tr><tr><td>2</td><td>3000.7</td><td>200.0</td><td>无</td><td>灰色</td><td>鹫鹰</td></tr><tr><td>3</td><td>3300.0</td><td>220.3</td><td>无</td><td>灰色</td><td>鹫鹰</td></tr><tr><td>4</td><td>4100.0</td><td>136.0</td><td>有</td><td>黑色</td><td>普通潜鸟</td></tr><tr><td>5</td><td>3.0</td><td>11.0</td><td>无</td><td>绿色</td><td>瑰丽蜂鸟</td></tr><tr><td>6</td><td>570.0</td><td>75.0</td><td>无</td><td>黑色</td><td>象牙喙啄木鸟</td></tr></table>

表1-1的前两种特征是数值型，可以使用十进制数字；第三种特征（是否有脚蹼）是二值型，只可以取0或1；第四种特征（后背颜色）是基于自定义调色板的枚举类型，这里仅选择一些常用色彩。如果仅仅利用常见的七色作为评测特征，后背颜色也可以是一个整数。当然在七色之中选择一个作为后背颜色有些太简单了，但作为专家系统的演示用例，这已经足够了。

如果你看到了一只象牙喙啄木鸟，请马上通知我！而且千万不要告诉任何人。在我到达之前，一定要看住它，别让它飞跑了。（任何发现活的象牙喙啄木鸟的人都可以得到5万美元的奖励。）

机器学习的主要任务就是分类。本节我们讲述如何使用表1-1进行分类，标识出象牙喙啄木鸟从而获取5万美元的奖励。大家都想从众多其他鸟类中分辨出象牙喙啄木鸟，并从中获利。最简单的做法是安装一个喂食器，然后雇用一位鸟类学者，观察在附近进食的鸟类。如果发现象牙喙啄木鸟，则通知我们。这种方法太昂贵了，而且专家在同一时间只能出现在一个地方。我们可以自动化处理上述过程，安装多个带有照相机的喂食器，同时接入计算机用于标识前来进食的鸟。同样我们可以在喂食器中放置称重仪器以获取鸟的体重，利用计算机视觉技术来提取鸟的翅长、脚的类型和后背色彩。假定我们可以得到所需的全部特征信息，那该如何判断飞入进食器的鸟是不是象牙喙啄木鸟呢？这个任务就是分类，有很多机器学习算法非常善于分类。本例中的类别就是鸟的物种，更具体地说，就是区分是否为象牙喙啄木鸟。

最终我们决定使用某个机器学习算法进行分类，首先需要做的是算法训练，即学习如何分类。通常我们为算法输入大量已分类数据作为算法的训练集。训练集是用于训练机器学习算法的数据样本集合，表1-1是包含六个训练样本的训练集，每个训练样本有4种特征、一个目标变量，如图1-2所示。目标变量是机器学习算法的预测结果，在分类算法中目标变量的类型通常是标称型的，而在回归算法中通常是连续型的。训练样本集必须确定知道目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。正如前文所述，这里的目标变量是物种，也可以简化为标

称型的数值。我们通常将分类问题中的目标变量称为类别，并假定分类问题只存在有限个数的类别。

![](images/2d7ca82dd84659473aa4e3d7550848466a16947d0d3252bc8a7e9308642e92d8.jpg)  
图1-2 特征和标识的目标变量

注意 特征或者属性通常是训练样本集的列，它们是独立测量得到的结果，多个特征联系在一起共同组成一个训练样本。

为了测试机器学习算法的效果，通常使用两套独立的样本集：训练数据和测试数据。当机器学习程序开始运行时，使用训练样本集作为算法的输入，训练完成之后输入测试样本。输入测试样本时并不提供测试样本的目标变量，由程序决定样本属于哪个类别。比较测试样本预测的目标变量值与实际样本类别之间的差别，就可以得出算法的实际精确度。本书的后续章节将会引入更好地使用测试样本和训练样本信息的方法，这里就不再详述。

假定这个鸟类分类程序，经过测试满足精确度要求，是否我们就可以看到机器已经学会了如何区分不同的鸟类了呢？这部分工作称之为知识表示，某些算法可以产生很容易理解的知识表示，而某些算法的知识表示也许只能为计算机所理解。知识表示可以采用规则集的形式，也可以采用概率分布的形式，设置可以是训练样本集中的一个实例。在某些场合中，人们可能并不想建立一个专家系统，而仅仅对机器学习算法获取的信息感兴趣。此时，采用何种方式表示知识就显得非常重要了。

本节介绍了机器学习领域涉及的关键术语，后续章节将会在必要时引入其他的术语，这里就不再进一步说明。下一节将会介绍机器学习算法的主要任务。

# 1.3 机器学习的主要任务

本节主要介绍机器学习的主要任务，并给出一个表格，帮助读者将机器学习算法转化为可实际运作的应用程序。

上节的例子介绍了机器学习如何解决分类问题，它的主要任务是将实例数据划分到合适的分类中。机器学习的另一项任务是回归，它主要用于预测数值型数据。大多数人可能都见过回归的例子——数据拟合曲线：通过给定数据点的最优拟合曲线。分类和回归属于监督学习，之所以称之为监督学习，是因为这类算法必须知道预测什么，即目标变量的分类信息。

与监督学习相对应的是无监督学习，此时数据没有类别信息，也不会给定目标值。在无监督学习中，将数据集合分成由类似的对象组成的多个类的过程被称为聚类；将寻找描述数据统计值的过程称之为密度估计。此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。表1-2列出了机器学习的主要任务，以及解决相应问题的算法。

表1-2 用于执行分类、回归、聚类和密度估计的机器学习算法  

<table><tr><td colspan="2">监督学习的用途</td></tr><tr><td>k-近邻算法</td><td>线性回归</td></tr><tr><td>朴素贝叶斯算法</td><td>局部加权线性回归</td></tr><tr><td>支持向量机</td><td>Ridge回归</td></tr><tr><td>决策树</td><td>Lasso最小回归系数估计</td></tr><tr><td colspan="2">无监督学习的用途</td></tr><tr><td>K-均值</td><td>最大期望算法</td></tr><tr><td>DBSCAN</td><td>Parzen窗设计</td></tr></table>

你可能已经注意到表1-2中的很多算法都可以用于解决同样的问题，有心人肯定会问：“为什么解决同一个问题存在四种方法？精通其中一种算法，是否可以处理所有的类似问题？”本书的下一节将回答这些疑问。

# 1.4 如何选择合适的算法

从表1-2中所列的算法中选择实际可用的算法，必须考虑下面两个问题：一、使用机器学习算法的目的，想要算法完成何种任务，比如是预测明天下雨的概率还是对投票者按照兴趣分组；二、需要分析或收集的数据是什么。

首先考虑使用机器学习算法的目的。如果想要预测目标变量的值，则可以选择监督学习算法，否则可以选择无监督学习算法。确定选择监督学习算法之后，需要进一步确定目标变量类型，如果目标变量是离散型，如是/否、1/2/3、A/B/C或者红/黄/黑等，则可以选择分类器算法；如果目标变量是连续型的数值，如 $0.0\sim 100.00$ 、 $-999\sim 999$ 或者 $+\infty \sim -\infty$ 等，则需要选择回归算法。

如果不想预测目标变量的值，则可以选择无监督学习算法。进一步分析是否需要将数据划分为离散的组。如果这是唯一的需求，则使用聚类算法；如果还需要估计数据与每个分组的相似程度，则需要使用密度估计算法。

在大多数情况下，上面给出的选择方法都能帮助读者选择恰当的机器学习算法，但这也并非一成不变。第9章我们就会使用分类算法来处理回归问题，显然这将与上面监督学习中处理回归问题的原则不同。

其次需要考虑的是数据问题。我们应该充分了解数据，对实际数据了解得越充分，越容易创建符合实际需求的应用程序。主要应该了解数据的以下特性：特征值是离散型变量还是连续型变

量，特征值中是否存在缺失的值，何种原因造成缺失值，数据中是否存在异常值，某个特征发生的频率如何（是否罕见得如同海底捞针），等等。充分了解上面提到的这些数据特性可以缩短选择机器学习算法的时间。

我们只能在一定程度上缩小算法的选择范围，一般并不存在最好的算法或者可以给出最好结果的算法，同时还要尝试不同算法的执行效果。对于所选的每种算法，都可以使用其他的机器学习技术来改进其性能。在处理输入数据之后，两个算法的相对性能也可能会发生变化。后续章节我们将进一步讨论此类问题，一般说来发现最好算法的关键环节是反复试错的迭代过程。

机器学习算法虽然各不相同，但是使用算法创建应用程序的步骤却基本类似，下一节将介绍如何使用机器学习算法的通用步骤。

# 1.5 开发机器学习应用程序的步骤

本书学习和使用机器学习算法开发应用程序，通常遵循以下的步骤。

(1)收集数据。我们可以使用很多方法收集样本数据，如：制作网络爬虫从网站上抽取数据、从RSS反馈或者API中得到信息、设备发送过来的实测数据（风速、血糖等）。提取数据的方法非常多，为了节省时间与精力，可以使用公开可用的数据源。  
(2) 准备输入数据。得到数据之后，还必须确保数据格式符合要求，本书采用的格式是Python语言的List。使用这种标准数据格式可以融合算法和数据源，方便匹配操作。本书使用Python语言构造算法应用，不熟悉的读者可以学习附录A。

此外还需要为机器学习算法准备特定的数据格式，如某些算法要求特征值使用特定的格式，一些算法要求目标变量和特征值是字符串类型，而另一些算法则可能要求是整数类型。后续章节我们还要讨论这个问题，但是与收集数据的格式相比，处理特殊算法要求的格式相对简单得多。

(3) 分析输入数据。此步骤主要是人工分析以前得到的数据。为了确保前两步有效，最简单的方法是用文本编辑器打开数据文件，查看得到的数据是否为空值。此外，还可以进一步浏览数据，分析是否可以识别出模式；数据中是否存在明显的异常值，如某些数据点与数据集中的其他值存在明显的差异。通过一维、二维或三维图形展示数据也是不错的方法，然而大多数时候我们得到数据的特征值都不会低于三个，无法一次图形化展示所有特征。本书的后续章节将会介绍提炼数据的方法，使得多维数据可以压缩到二维或三维，方便我们图形化展示数据。  
这一步的主要作用是确保数据集中没有垃圾数据。如果是在产品化系统中使用机器学习算法并且算法可以处理系统产生的数据格式，或者我们信任数据来源，可以直接跳过第3步。此步骤需要人工干预，如果在自动化系统中还需要人工干预，显然就降低了系统的价值。  
(4) 训练算法。机器学习算法从这一步才真正开始学习。根据算法的不同，第4步和第5步是机器学习算法的核心。我们将前两步得到的格式化数据输入到算法，从中抽取知识或信息。这里得到的知识需要存储为计算机可以处理的格式，方便后续步骤使用。  
如果使用无监督学习算法，由于不存在目标变量值，故而也不需要训练算法，所有与算法相关的内容都集中在第5步。

(5) 测试算法。这一步将实际使用第4步机器学习得到的知识信息。为了评估算法，必须测试算法工作的效果。对于监督学习，必须已知用于评估算法的目标变量值；对于无监督学习，也必须用其他的评测手段来检验算法的成功率。无论哪种情形，如果不满意算法的输出结果，则可以回到第4步，改正并加以测试。问题常常会跟数据的收集和准备有关，这时你就必须跳回第1步重新开始。  
(6) 使用算法。将机器学习算法转换为应用程序，执行实际任务，以检验上述步骤是否可以在实际环境中正常工作。此时如果碰到新的数据问题，同样需要重复执行上述的步骤。

下节我们将讨论实现机器学习算法的编程语言Python。之所以选择Python，是因为它具有其他编程语言不具备的优势，如易于理解、丰富的函数库（尤其是矩阵操作）、活跃的开发者社区等。

# 1.6 Python语言的优势

基于以下三个原因，我们选择Python作为实现机器学习算法的编程语言：(1) Python的语法清晰；(2) 易于操作纯文本文件；(3) 使用广泛，存在大量的开发文档。

# 1.6.1 可执行伪代码

Python具有清晰的语法结构，大家也把它称作可执行伪代码（executable pseudo-code）。默认安装的Python开发环境已经附带了很多高级数据类型，如列表、元组、字典、集合、队列等，无需进一步编程就可以使用这些数据类型的操作。使用这些数据类型使得实现抽象的数学概念非常简单。此外，读者还可以使用自己熟悉的编程风格，如面向对象编程、面向过程编程、或者函数式编程。不熟悉Python的读者可以参阅附录A，该附录详细介绍了Python语言、Python使用的数据类型以及安装指南。

Python语言处理和操作文本文件非常简单，非常易于处理非数值型数据。Python语言提供了丰富的正则表达式函数以及很多访问Web页面的函数库，使得从HTML中提取数据变得非常简单直观。

# 1.6.2 Python比较流行

Python语言使用广泛，代码范例也很多，便于读者快速学习和掌握。此外，在开发实际应用程序时，也可以利用丰富的模块库缩短开发周期。

在科学和金融领域，Python语言得到了广泛应用。SciPy和NumPy等许多科学函数库都实现了向量和矩阵操作，这些函数库增加了代码的可读性，学过线性代数的人都可以看懂代码的实际功能。另外，科学函数库SciPy和NumPy使用底层语言（C和Fortran）编写，提高了相关应用程序的计算性能。本书将大量使用Python的NumPy。

Python的科学工具可以与绘图工具Matplotlib协同工作。Matplotlib可以绘制2D、3D图形，也可以处理科学研究中经常使用到的图形，所以本书也将大量使用Matplotlib。

Python开发环境还提供了交互式shell环境，允许用户开发程序时查看和检测程序内容。

Python开发环境将来还会集成Pylab模块，它将NumPy、SciPy和Matplotlib合并为一个开发环境。在本书写作时，Pylab还没有并入Python环境，但是不远的将来我们肯定可以在Python开发环境找到它。

# 1.6.3 Python语言的特色

诸如MATLAB和Mathematica等高级程序语言也允许用户执行矩阵操作，MATLAB甚至还有许多内嵌的特征可以轻松地构造机器学习应用，而且MATLAB的运算速度也很快。然而MATLAB的不足之处是软件费用太高，单个软件授权就要花费数千美元。虽然也有适合MATLAB的第三方插件，但是没有一个有影响力的大型开源项目。

Java和C等强类型程序设计语言也有矩阵数学库，然而对于这些程序设计语言来说，最大的问题是即使完成简单的操作也要编写大量的代码。程序员首先需要定义变量的类型，对于Java来说，每次封装属性时还需要实现getter和setter方法。另外还要记着实现子类，即使并不想使用子类，也必须实现子类方法。为了完成一个简单的工作，我们必须花费大量时间编写了很多无用冗长的代码。Python语言则与Java和C完全不同，它清晰简练，而且易于理解，即使不是编程人员也能够理解程序的含义，而Java和C对于非编程人员则像天书一样难于理解。

所有人在小学二年级已经学会了写作，然而大多数人必须从事其他更重要的工作。

鲍比·奈特

也许某一天，我们可以在这句话中将“写作”替代为“编写代码”，虽然有些人对于编写代码很感兴趣，但是对于大多数人来说，编程仅是完成其他任务的工具而已。Python语言是高级编程语言，我们可以花费更多的时间处理数据的内在含义，而无须花费太多精力解决计算机如何得到数据结果。Python语言使得我们很容易表达自己的目的。

# 1.6.4 Python语言的缺点

Python语言唯一的不足是性能问题。Python程序运行的效率不如Java或者C代码高，但是我们可以使用Python调用C编译的代码。这样，我们就可以同时利用C和Python的优点，逐步地开发机器学习应用程序。我们可以首先使用Python编写实验程序，如果进一步想要在产品中实现机器学习，转换成C代码也不困难。如果程序是按照模块化原则组织的，我们可以先构造可运行的Python程序，然后再逐步使用C代码替换核心代码以改进程序的性能。 $\mathrm{C}++$ Boost库就适合完成这个任务，其他类似于Cython和PyPy的工具也可以编写强类型的Python代码，改进一般Python程序的性能。

如果程序的算法或者思想有缺陷，则无论程序的性能如何，都无法得到正确的结果。如果解决问题的思想存在问题，那么单纯通过提高程序的运行效率，扩展用户规模都无法解决这个核心问题。从这个角度来看，Python快速实现系统的优势就更加明显了，我们可以快速地检验算法或者思想是否正确，如果需要，再进一步优化代码。

本节大致介绍了本书选择Python语言实现机器学习算法的原因，下节我们将学习Python语言的shell开发环境以及NumPy函数库。

# 1.7 NumPy函数库基础

机器学习算法涉及很多线性代数知识，因此本书在使用Python语言构造机器学习应用时，会经常使用NumPy函数库。如果不熟悉线性代数也不用着急，这里用到线性代数只是为了简化不同的数据点上执行的相同数学运算。将数据表示为矩阵形式，只需要执行简单的矩阵运算而不需要复杂的循环操作。在你使用本书开始学习机器学习算法之前，必须确保可以正确运行Python开发环境，同时正确安装了NumPy函数库。NumPy函数库是Python开发环境的一个独立模块，而且大多数Python发行版没有默认安装NumPy函数库，因此在安装Python之后必须单独安装NumPy函数库。在Windows命令行提示符下输入c:\Python27\python.exe，在Linux或者MacOS的终端上输入python，进入Python shell开发环境。今后，一旦看到下述提示符就意味着我们已经进入Python shell开发环境：

```txt
>>> 
```

在Python shell开发环境中输入下列命令：

```txt
>>> from numpy import \* 
```

上述命令将NumPy函数库中的所有模块引入当前的命名空间。Mac OS上输出结果如图1-3所示。

![](images/6f239fda7234551047d6c72d07927396c62a4afcab2598f1118d8151e256e318.jpg)  
图1-3 命令行启动Python并在Python shell开发环境中导入模块

然后在Python shell开发环境中输入下述命令：

```txt
>>> random.randint(4, 4)
array([[0.70328595, 0.40951383, 0.7475052, 0.07061094],
[0.9571294, 0.97588446, 0.2728084, 0.5257719],
[0.05431627, 0.01396732, 0.60304292, 0.19362288],
[0.10648952, 0.27317698, 0.45582919, 0.04881605]) 
```

上述命令构造了一个 $4 \times 4$ 的随机数组，因为产生的是随机数组，不同计算机的输出结果可能与上述结果完全不同。

# NumPy矩阵与数组的区别

NumPy函数库中存在两种不同的数据类型（矩阵matrix和数组array），都可以用于处理行列表示的数字元素。虽然它们看起来很相似，但是在这两个数据类型上执行相同的数学运算可能得到不同的结果，其中NumPy函数库中的matrix与MATLAB中matrices等价。

调用mat()函数可以将数组转化为矩阵，输入下述命令：

>>randMat $=$ mat(random RAND(4,4))

由于使用随机函数产生矩阵，不同计算机上输出的值可能略有不同：

```txt
>>> randMat.I
matrix([[0.24497106, 1.75854497, -1.77728665, -0.0834912],
[1.49792202, 2.12925479, 1.32132491, -9.75890849],
[2.76042144, 1.67271779, -0.29226613, -8.45413693],
[-2.03011142, -3.07832136, 1.4420448, 9.62598044]]) 
```

. I操作符实现了矩阵求逆的运算。非常简单吧？没有NumPy库，Python也不能这么容易算出来矩阵的逆运算。不记得或者没学过矩阵求逆也没关系，NumPy库帮我们做完了，执行下面的命令存储逆矩阵：

```txt
>>> invRandMat = randMat.I 
```

接着执行矩阵乘法，得到矩阵与其逆矩阵相乘的结果：

```txt
>>> randMat * invRandMat
matrix([[1.00000000e+00, 1.77635684e-15],
[0.00000000e+00, 0.00000000e+00],
[0.00000000e+00, -8.88178420e-16],
[-2.22044605e-16, 1.00000000e+00]]) 
```

结果应该是单位矩阵，除了对角线元素是1， $4 \times 4$ 矩阵的其他元素应该全是0。实际输出结果略有不同，矩阵里还留下了许多非常小的元素，这是计算机处理误差产生的结果。输入下述命令，得到误差值：

```txt
>>> myEye = randMat * invRandMat
>>> myEye - eye(4)
matrix([[0.00000000e+00, -6.59194921e-17, -4.85722573e-17, -4.99600361e-16],
[2.22044605e-16, 0.00000000e+00, -6.03683770e-16, -7.77156117e-16],
[-5.55111512e-17, -1.04083409e-17, -3.33066907e-16, -2.22044605e-16],
[5.55111512e-17, 1.56125113e-17, -5.55111512e-17, 
```

函数eye(4)创建 $4 \times 4$ 的单位矩阵。

只要能够顺利地完成上述例子，就说明已经正确地安装了NumPy函数库，以后我们就可以利用它构造机器学习应用程序。即使没有提前学习所有的函数也没有关系，本书将在需要的时候介绍更多的NumPy函数库的功能。

# 1.8 本章小结

尽管没有引起大多数人的注意，但是机器学习算法已经广泛应用于我们的日常生活之中。每天我们需要处理的数据在不断地增加，能够深入理解数据背后的真实含义，是数据驱动产业必须具备的基本技能。

学习机器学习算法，必须了解数据实例，每个数据实例由多个特征值组成。分类是基本的机器学习任务，它分析未分类数据，以确定如何将其放入已知群组中。为了构建和训练分类器，必须首先输入大量已知分类的数据，我们将这些数据称为训练样本集。

尽管我们构造的鸟类识别专家系统无法像人类专家一样精确地识别不同的鸟类，然而构建接近专家水平的机器系统可以显著地改进我们的生活质量。如果我们可以构造的医生专家系统能够达到人类医生的准确率，则病人可以得到快速的治疗；如果我们可以改进天气预报，则可以减少水资源的短缺，提高食物供给。我们可以列举许许多多这样的例子，机器学习的应用前景几乎是无限的。

第一部分的后续6章主要研究分类问题，它是监督学习算法的一个分支，下一章我们将介绍第一个分类算法—— $k$ -近邻算法。

# 第2章

# $k$ -近邻算法

# 本章内容

□ $k$ -近邻分类算法  
□从文本文件中解析和导入数据  
□ 使用Matplotlib创建扩散图  
□归一化数值

众所周知，电影可以按照题材分类，然而题材本身是如何定义的？由谁来判定某部电影属于哪个题材？也就是说同一题材的电影具有哪些公共特征？这些都是在进行电影分类时必须要考虑的问题。没有哪个电影人会说自己制作的电影和以前的某部电影类似，但我们确实知道每部电影在风格上的确有可能会和同题材的电影相近。那么动作片具有哪些共有特征，使得动作片之间非常类似，而与爱情片存在着明显的差别呢？动作片中也会存在接吻镜头，爱情片中也会存在打斗场景，我们不能单纯依靠是否存在打斗或者亲吻来判断影片的类型。但是爱情片中的亲吻镜头更多，动作片中的打斗场景也更频繁，基于此类场景在某部电影中出现的次数可以用来进行电影分类。本章第一节基于电影中出现的亲吻、打斗出现的次数，使用k-近邻算法构造程序，自动划分电影的题材类型。我们首先使用电影分类讲解k-近邻算法的基本概念，然后学习如何在其他系统上使用k-近邻算法。

本章介绍第一个机器学习算法： $k$ -近邻算法，它非常有效而且易于掌握。首先，我们将探讨 $k$ -近邻算法的基本理论，以及如何使用距离测量的方法分类物品；其次我们将使用Python从文本文件中导入并解析数据；再次，本书讨论了当存在许多数据来源时，如何避免计算距离时可能碰到的一些常见错误；最后，利用实际的例子讲解如何使用 $k$ -近邻算法改进约会网站和手写数字识别系统。

# 2.1 $k$ -近邻算法概述

简单地说， $k$ -近邻算法采用测量不同特征值之间的距离方法进行分类。

# $k$ -近邻算法

优点：精度高、对异常值不敏感、无数据输入假定。

缺点：计算复杂度高、空间复杂度高。

适用数据范围：数值型和标称型。

本书讲解的第一个机器学习算法是 $k$ -近邻算法（kNN），它的工作原理是：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前 $k$ 个最相似的数据，这就是 $k$ -近邻算法中 $k$ 的出处，通常 $k$ 是不大于20的整数。最后，选择 $k$ 个最相似数据中出现次数最多的分类，作为新数据的分类。

现在我们回到前面电影分类的例子，使用 $k$ -近邻算法分类爱情片和动作片。有人曾经统计过很多电影的打斗镜头和接吻镜头，图2-1显示了6部电影的打斗和接吻镜头数。假如有一部未看过的电影，如何确定它是爱情片还是动作片呢？我们可以使用kNN来解决这个问题。

![](images/6e6f94a3d51a1684c56e1205fb8a1028c0a1e6c9e53a5dc2844f01f249ca0e91.jpg)  
图2-1 使用打斗和接吻镜头数分类电影

首先我们需要知道这个未知电影存在多少个打斗镜头和接吻镜头，图2-1中间号位置是该未知电影出现的镜头数图形化展示，具体数字参见表2-1。

表2-1 每部电影的打斗镜头数、接吻镜头数以及电影评估类型  

<table><tr><td>电影名称</td><td>打斗镜头</td><td>接吻镜头</td><td>电影类型</td></tr><tr><td>California Man</td><td>3</td><td>104</td><td>爱情片</td></tr><tr><td>He&#x27;s Not Really into Dudes</td><td>2</td><td>100</td><td>爱情片</td></tr><tr><td>Beautiful Woman</td><td>1</td><td>81</td><td>爱情片</td></tr><tr><td>Kevin Longblade</td><td>101</td><td>10</td><td>动作片</td></tr><tr><td>Robo Slayer 3000</td><td>99</td><td>5</td><td>动作片</td></tr><tr><td>Amped II</td><td>98</td><td>2</td><td>动作片</td></tr><tr><td>?</td><td>18</td><td>90</td><td>未知</td></tr></table>

即使不知道未知电影属于哪种类型，我们也可以通过某种方法计算出来。首先计算未知电影与样本集中其他电影的距离，如表2-2所示。此处暂时不要关心如何计算得到这些距离值，使用Python实现电影分类应用时，会提供具体的计算方法。

表2-2 已知电影与未知电影的距离  

<table><tr><td>电影名称</td><td>与未知电影的距离</td></tr><tr><td>California Man</td><td>20.5</td></tr><tr><td>He&#x27;s Not Really into Dudes</td><td>18.7</td></tr><tr><td>Beautiful Woman</td><td>19.2</td></tr><tr><td>Kevin Longblade</td><td>115.3</td></tr><tr><td>Robo Slayer 3000</td><td>117.4</td></tr><tr><td>Amped ll</td><td>118.9</td></tr></table>

现在我们得到了样本集中所有电影与未知电影的距离，按照距离递增排序，可以找到k个距离最近的电影。假定 $k = 3$ ，则三个最靠近的电影依次是He's Not Really into Dudes、Beautiful Woman和California Man。k-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。

本章主要讲解如何在实际环境中应用 $k$ -近邻算法，同时涉及如何使用Python工具和相关的机器学习术语。按照1.5节开发机器学习应用的通用步骤，我们使用Python语言开发 $k$ -近邻算法的简单应用，以检验算法使用的正确性。

# $k$ -近邻算法的一般流程

(1)收集数据：可以使用任何方法。  
(2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。  
(3) 分析数据：可以使用任何方法。  
(4) 训练算法：此步骤不适用于 $k$ -近邻算法。  
(5) 测试算法：计算错误率。  
(6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行 $k$ -近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。

# 2.1.1 准备：使用Python导入数据

首先，创建名为kNN.py的Python模块，本章使用的所有代码都在这个文件中。读者可以按照自己的习惯学习代码，既可以按照本书学习的进度，在自己创建的Python文件中编写代码，也可以直接从本书的源代码中复制kNN.py文件。我推荐读者从头开始创建模块，按照学习的进度编写代码。

无论大家采用何种方法，我们现在已经有了kNN.py文件。在构造完整的 $k$ -近邻算法之前，我们还需要编写一些基本的通用函数，在kNN.py文件中增加下面的代码：

from numpy import \*   
import operator   
def createDataSet(): group $=$ array([1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]] labels $=$ ['A',A',B',B'] return group, labels

在上面的代码中，我们导入了两个模块：第一个是科学计算包NumPy；第二个是运算符模块，

$k$ -近邻算法执行排序操作时将使用这个模块提供的函数，后面我们将进一步介绍。

为了方便使用createDataSet()函数，它创建数据集和标签，如图2-1所示。然后依次执行以下步骤：保存kNN.py文件，改变当前路径到存储kNN.py文件的位置，打开Python开发环境。无论是Linux、Mac OS还是Windows都需要在打开终端，在命令提示符下完成上述操作。只要我们按照默认配置安装Python，在Linux/Mac OS终端内都可以直接输入python，而在Windows命令提示符下需要输入c:\Python2.6\python.exe，进入Python交互式开发环境。

进入Python开发环境之后，输入下列命令导入上面编辑的程序模块：

```txt
>>> import kNN
```

上述命令导入kNN模块。为了确保输入相同的数据集，kNN模块中定义了函数createDataSet，在Python命令提示符下输入下属命令：

```txt
>>> group,labels = kNN.createDataSet() 
```

上述命令创建了变量group和labels，在Python命令提示符下，输入变量的名字以检验是否正确地定义变量：

```txt
>>> group  
array([[1.， 1.1]，[1.， 1.，[0.， 0.，[0.， 0.1]]）  
>>> labels  
['A', 'A', 'B', 'B']
```

这里有4组数据，每组数据有两个我们已知的属性或者特征值。上面的group矩阵每行包含一个不同的数据，我们可以把它想象为某个日志文件中不同的测量点或者入口。由于人类大脑的限制，我们通常只能可视化处理三维以下的事务。因此为了简单地实现数据可视化，对于每个数据点我们通常只使用两个特征。

向量label包含了每个数据点的标签信息，label包含的元素个数等于group矩阵行数。这里我们将数据点(1, 1.1)定义为类A，数据点(0, 0.1)定义为类B。为了说明方便，例子中的数值是任意选择的，并没有给出轴标签，图2-2是带有类标签信息的四个数据点。

![](images/a43ee62a320ac3e917a81925328d6c0043f0a1afd7be14ea81a42ba3cbdac2d0.jpg)  
图2-2 $k$ -近邻算法：带有4个数据点的简单例子

现在我们已经知道Python如何解析数据，如何加载数据，以及kNN算法的工作原理，接下来我们将使用这些方法完成分类任务。

# 2.1.2 从文本文件中解析数据

本节使用程序清单2-1的函数运行kNN算法，为每组数据分类。这里首先给出 $k$ -近邻算法的伪代码和实际的Python代码，然后详细地解释每行代码的含义。该函数的功能是使用 $k$ -近邻算法将每组数据划分到某个类中，其伪代码如下：

对未知类别属性的数据集中的每个点依次执行以下操作：

(1) 计算已知类别数据集中的点与当前点之间的距离；  
(2) 按照距离递增次序排序；  
(3) 选取与当前点距离最小的 $k$ 个点；  
(4) 确定前 $k$ 个点所在类别的出现频率；  
(5) 返回前 $k$ 个点出现频率最高的类别作为当前点的预测分类。

Python函数classify0()如程序清单2-1所示。

# 程序清单2-1 $k$ -近邻算法

```python
def classify0(inX,dataSet,labels,k):
    dataSetsize = dataSet.shape[0]
    diffMat = tile(inX,(dataSetSize,1)) - dataSet
    sqDiffMat = diffMat**2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances**0.5
    sortedDistIndicies = distances.argsort()
    classCount={'}
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
        sortedClassCount = sorted(classCountitaireItems(), key=operator.itemgetter(1), reverse=True)
        return sortedClassCount[0][0] 
```

classify0()函数有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels，最后的参数k表示用于选择最近邻居的数目，其中标签向量的元素数目和矩阵dataSet的行数相同。程序清单2-1使用欧氏距离公式，计算两个向量点xA和xB之间的距离①：

$$
d = \sqrt {\left(x A _ {0} - x B _ {0}\right) ^ {2} + \left(x A _ {1} - x B _ {1}\right) ^ {2}}
$$

例如，点(0,0)与(1,2)之间的距离计算为：

$$
\sqrt {(1 - 0) ^ {2} + (2 - 0) ^ {2}}
$$

如果数据集存在4个特征值，则点 $(1,0,0,1)$ 与 $(7,6,9,4)$ 之间的距离计算为：

$$
\sqrt {(7 - 1) ^ {2} + (6 - 0) ^ {2} + (9 - 0) ^ {2} + (4 - 1) ^ {2}}
$$

计算完所有点之间的距离后，可以对数据按照从小到大的次序排序。然后，确定前k个距离最小元素所在的主要分类①，输入k总是正整数；最后，将classCount字典分解为元组列表，然后使用程序第二行导入运算符模块的itemgetter方法，按照第二个元素的次序对元组进行排序②。此处的排序为逆序，即按照从最大到最小次序排序，最后返回发生频率最高的元素标签。

为了预测数据所在分类，在Python提示符中输入下列命令：

>>> kNNclassify0([0,0], group, labels, 3)

输出结果应该是B，大家也可以改变输入[0,0]为其他值，测试程序的运行结果。

到现在为止，我们已经构造了第一个分类器，使用这个分类器可以完成很多分类任务。从这个实例出发，构造使用分类算法将会更加容易。

# 2.1.3 如何测试分类器

上文我们已经使用 $k$ -近邻算法构造了第一个分类器，也可以检验分类器给出的答案是否符合我们的预期。读者可能会问：“分类器何种情况下会出错？”或者“答案是否总是正确的？”答案是否定的，分类器并不会得到百分百正确的结果，我们可以使用多种方法检测分类器的正确率。此外分类器的性能也会受到多种因素的影响，如分类器设置和数据集等。不同的算法在不同数据集上的表现可能完全不同，这也是本部分的6章都在讨论分类算法的原因所在。

为了测试分类器的效果，我们可以使用已知答案的数据，当然答案不能告诉分类器，检验分类器给出的结果是否符合预期结果。通过大量的测试数据，我们可以得到分类器的错误率——分类器给出错误结果的次数除以测试执行的总数。错误率是常用的评估方法，主要用于评估分类器在某个数据集上的执行效果。完美分类器的错误率为0，最差分类器的错误率是1.0，在这种情况下，分类器根本就无法找到一个正确答案。读者可以在后面章节看到实际的数据例子。

上一节介绍的例子已经可以正常运转了，但是并没有太大的实际用处，本章的后两节将在现实世界中使用 $k$ -近邻算法。首先，我们将使用 $k$ -近邻算法改进约会网站的效果，然后使用 $k$ -近邻算法改进手写识别系统。本书将使用手写识别系统的测试程序检测 $k$ -近邻算法的效果。

# 2.2 示例：使用 $k$ -近邻算法改进约会网站的配对效果

我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象。尽管约会网站会推荐不同的人选，但她没有从中找到喜欢的人。经过一番总结，她发现曾交往过三种类型的人：

□ 不喜欢的人  
□魅力一般的人  
□极具魅力的人

尽管发现了上述规律，但海伦依然无法将约会网站推荐的匹配对象归入恰当的分类。她觉得可以在周一到周五约会那些魅力一般的人，而周末则更喜欢与那些极具魅力的人为伴。海伦希望

我们的分类软件可以更好地帮助她将匹配对象划分到确切的分类中。此外海伦还收集了一些约会网站未曾记录的数据信息，她认为这些数据更有助于匹配对象的归类。

# 示例：在约会网站上使用 $k$ -近邻算法

(1)收集数据：提供文本文件。  
(2) 准备数据：使用Python解析文本文件。  
(3) 分析数据：使用Matplotlib二维扩散图。  
(4) 训练算法：此步骤不适用于 $k$ -近邻算法。  
(5) 测试算法：使用海伦提供的部分数据作为测试样本。

测试样本和非测试样本的区别在于：测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。

(6) 使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。

# 2.2.1 准备数据：从文本文件中解析数据

海伦收集约会数据已经有了一段时间，她把这些数据存放在文本文件datingTestSet.txt中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下3种特征：

□每年获得的飞行常客里程数  
□ 玩视频游戏所耗时间百分比  
□每周消费的冰淇淋公升数

在将上述特征数据输入到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式。在kNN.py中创建名为file2matrix的函数，以此来处理输入格式问题。该函数的输入为文件名字符串，输出为训练样本矩阵和类标签向量。

将下面的代码增加到kNN.py中。

# 程序清单2-2 将文本记录到转换NumPy的解析程序

def file2matrixfilename): fr = open(filename) arrayOLines $=$ fr.readlines() numberOfLines $=$ len(arrayOLines) returnMat $=$ zeros((numberOfLines,3)) classLabelVector $= []$ index $= 0$ for line in arrayOLines line $=$ line.strip() listFromLine $=$ line.split("\\t') returnMat[index,:] $=$ listFromLine[0:3] classLabelVector.append(int(listFromLine[-1])) index += 1 return returnMat,classLabelVector

从上面的代码可以看到，Python处理文本文件非常容易。首先我们需要知道文本文件包含多少行。打开文件，得到文件的行数①。然后创建以零填充的矩阵NumPy②（实际上，NumPy是一个二维数组，这里暂时不用考虑其用途）。为了简化处理，我们将该矩阵的另一维度设置为固定值3，你可以按照自己的实际需求增加相应的代码以适应变化的输入值。循环处理文件中的每行数据③，首先使用函数line.strip()截取掉所有的回车字符，然后使用tab字符\t将上一步得到的整行数据分割成一个元素列表。接着，我们选取前3个元素，将它们存储到特征矩阵中。Python语言可以使用索引值-1表示列表中的最后一列元素，利用这种负索引，我们可以很方便地将列表的最后一列存储到向量classLabelVector中。需要注意的是，我们必须明确地通知解释器，告诉它列表中存储的元素值为整型，否则Python语言会将这些元素当作字符串处理。以前我们必须自己处理这些变量值类型问题，现在这些细节问题完全可以交给NumPy函数库来处理。

在Python命令提示符下输入下面命令：

```txt
>>> reload(kNN)
>>> datagDataMat, datingLabels = kNN.file2matrix('datingTestSet.txt') 
```

使用函数file2matrix读取文件数据，必须确保文件datingTestSet.txt存储在我们的工作目录中。此外在执行这个函数之前，我们重新加载了kNN.py模块，以确保更新的内容可以生效，否则Python将继续使用上次加载的kNN模块。

成功导入datingTestSet.txt文件中的数据之后，可以简单检查一下数据内容。Python的输出结果大致如下：

```txt
>>> datingDataMat  
array([[7.29170000e+04, 7.10627300e+00, 2.23600000e-01],  
[1.42830000e+04, 2.44186700e+00, 1.90838000e-01],  
[7.34750000e+04, 8.31018900e+00, 8.52795000e-01],  
...  
[1.24290000e+04, 4.43233100e+00, 9.24649000e-01],  
[2.52880000e+04, 1.31899030e+01, 1.05013800e+00],  
[4.91800000e+03, 3.01112400e+00, 1.90663000e-01])  
>>> datingLabels[0:20]  
[3, 2, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 3] 
```

现在已经从文本文件中导入了数据，并将其格式化为想要的格式，接着我们需要了解数据的真实含义。当然我们可以直接浏览文本文件，但是这种方法非常不友好，一般来说，我们会采用图形化的方式直观地展示数据。下面就用Python工具来图形化展示数据内容，以便辨识出一些数据模式。

# NumPy数组和Python数组

本书将大量使用NumPy数组，你既可以直接在Python命令行环境中输入from numpy import array将其导入，也可以通过直接导入所有NumPy库内容来将其导入。由于NumPy库提供的数组操作并不支持Python自带的数组类型，因此在编写代码时要注意不要使用错误的数组类型。

# 2.2.2 分析数据：使用Matplotlib创建散点图

首先我们使用Matplotlib制作原始数据的散点图，在Python命令行环境中，输入下列命令：

```python
>>> import matplotlib
>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> axscatter(datingDataMat[:,1], datingDataMat[:,2])
>>> plt.show() 
```

输出效果如图2-3所示。散点图使用datingDataMat矩阵的第二、第三列数据，分别表示特征值“玩视频游戏所耗时间百分比”和“每周所消费的冰淇淋公升数”。

![](images/5f3f15a50e8966df98718a216cc3030fc73a326ad7f2735e66eebd501e94c59b.jpg)  
图2-3没有样本类别标签的约会数据散点图。难以辨识图中的点究竟属于哪个样本分类

由于没有使用样本分类的特征值，我们很难从图2-3中看到任何有用的数据模式信息。一般来说，我们会采用色彩或其他的记号来标记不同样本分类，以便更好地理解数据信息。Matplotlib库提供的scatter函数支持个性化标记散点图上的点。重新输入上面的代码，调用scatter函数时使用下列参数：

```lisp
>>ax.scanDataMat[:1]， datingDataMat[:2]， 15.0\*array(datingLabels)，15.0\*array(datingLabels))
```

上述代码利用变量datingLabels存储的类标签属性，在散点图上绘制了色彩不等、尺寸不同的点。你可以看到一个与图2-3类似的散点图。从图2-3中，我们很难看到任何有用的信息，然而由

于图2-4利用颜色及尺寸标识了数据点的属性类别，因而我们基本上可以从图2-4中看到数据点所属三个样本分类的区域轮廓。

![](images/3c2ea93986cf2bfb697b416193aeb8cf985841facbb2257e9053e0eefcdbea97.jpg)  
图2-4 带有样本分类标签的约会数据散点图。虽然能够比较容易地区分数据点从属类别，但依然很难根据这张图得出结论性信息

本节我们学习了如何使用Matplotlib图形化展示数据，图2-4使用了矩阵属性列0和1展示数据，虽然可以区别，但图2-5采用不同的属性值可以得到更好效果，图中清晰地标识了三个不同的样本分类区域，具有不同爱好的人其类别区域也不同。

![](images/ec5fec5b10e7369414999d45460f8391f38a916411a20ae4ea02a71a6ddbd1e2.jpg)  
图2-5 每年赢得的飞行常客里程数与玩视频游戏所占百分比的约会数据散点图。约会数据有三个特征，通过图中展示的两个特征更容易区分数据点从属的类别

# 2.2.3 准备数据：归一化数值

表2-3给出了提取的四组数据，如果想要计算样本3和样本4之间的距离，可以使用下面的方法：

$$
\sqrt {(0 - 6 7) ^ {2} + (2 0 0 0 0 - 3 2 0 0 0) ^ {2} + (1 . 1 - 0 . 1) ^ {2}}
$$

我们很容易发现，上面方程中数字差值最大的属性对计算结果的影响最大，也就是说，每年获取的飞行常客里程数对于计算结果的影响将远远大于表2-3中其他两个特征——玩视频游戏的和每周消费冰淇淋公升数——的影响。而产生这种现象的唯一原因，仅仅是因为飞行常客里程数远大于其他特征值。但海伦认为这三种特征是同等重要的，因此作为三个等权重的特征之一，飞行常客里程数并不应该如此严重地影响到计算结果。

表2-3 约会网站原始数据改进之后的样本数据  

<table><tr><td></td><td>玩视频游戏所耗时间百分比</td><td>每年获得的飞行常客里程数</td><td>每周消费的冰淇淋公升数</td><td>样本分类</td></tr><tr><td>1</td><td>0.8</td><td>400</td><td>0.5</td><td>1</td></tr><tr><td>2</td><td>12</td><td>134 000</td><td>0.9</td><td>3</td></tr><tr><td>3</td><td>0</td><td>20 000</td><td>1.1</td><td>2</td></tr><tr><td>4</td><td>67</td><td>32 000</td><td>0.1</td><td>2</td></tr></table>

在处理这种不同取值范围的特征值时，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或者-1到1之间。下面的公式可以将任意取值范围的特征值转化为0到1区间内的值：

newValue $=$ (oldValue-min)/(max-min)

其中min和max分别是数据集中的最小特征值和最大特征值。虽然改变数值取值范围增加了分类器的复杂度，但为了得到准确结果，我们必须这样做。我们需要在文件kNN.py中增加一个新函数autoNorm()，该函数可以自动将数字特征值转化为0到1的区间。

程序清单2-3提供了函数autoNorm()的代码。

# 程序清单2-3 归一化特征值

```python
def autoNorm(dataSet):
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    ranges = maxVals - minVals
    normDataSet = zeros(shape(dataSet))
    m = dataSet.shape[0]
    normDataSet = dataSet - tile(minVals, (m, 1))
    normDataSet = normDataSet/tile(ranges, (m, 1))
    return normDataSet, ranges, minVals 
```

在函数autoNorm()中，我们将每列的最小值放在变量minVals中，将最大值放在变量maxVals中，其中dataSet.min(0)中的参数0使得函数可以从列中选取最小值，而不是选取当前行的最小值。然后，函数计算可能的取值范围，并创建新的返回矩阵。正如前面给出的公式，

为了归一化特征值，我们必须使用当前值减去最小值，然后除以取值范围。需要注意的是，特征值矩阵有 $1000 \times 3$ 个值，而minVals和range的值都为 $1 \times 3$ 。为了解决这个问题，我们使用NumPy库中tile()函数将变量内容复制成输入矩阵同样大小的矩阵，注意这是具体特征值相除①，而对于某些数值处理软件包，/可能意味着矩阵除法，但在NumPy库中，矩阵除法需要使用函数linalg.solve(matA, matB)。

在Python命令提示符下，重新加载kNN.py模块，执行autoNorm函数，检测函数的执行结果：

```txt
>>> reload(kNN)
>>> normMat, ranges, minVals = kNN-autoNorm(datingDataMat)
>>> normMat
array([[0.33060119, 0.58918886, 0.69043973],
[0.49199139, 0.50262471, 0.13468257],
[0.34858782, 0.68886842, 0.59540619],
...
[0.93077422, 0.52696233, 0.58885466],
[0.76626481, 0.44109859, 0.88192528],
[0.0975718, 0.02096883, 0.02443895]])
>>> ranges
array([[8.78430000e+04, 2.02823930e+01, 1.69197100e+00])
>>> minVals
array([[0., 0., 0.001818]) 
```

这里我们也可以只返回normMat矩阵，但是下一节我们将需要取值范围和最小值归一化测试数据。

# 2.2.4 测试算法：作为完整程序验证分类器

上节我们已经将数据按照需求做了处理，本节我们将测试分类器的效果，如果分类器的正确率满足要求，海伦就可以使用这个软件来处理约会网站提供的约会名单了。机器学习算法一个很重要的工作就是评估算法的正确率，通常我们只提供已有数据的 $90\%$ 作为训练样本来训练分类器，而使用其余的 $10\%$ 数据去测试分类器，检测分类器的正确率。本书后续章节还会介绍一些高级方法完成同样的任务，这里我们还是采用最原始的做法。需要注意的是， $10\%$ 的测试数据应该是随机选择的，由于海伦提供的数据并没有按照特定目的来排序，所以我们可以随意选择 $10\%$ 数据而不影响其随机性。

前面我们已经提到可以使用错误率来检测分类器的性能。对于分类器来说，错误率就是分类器给出错误结果的次数除以测试数据的总数，完美分类器的错误率为0，而错误率为1.0的分类器不会给出任何正确的分类结果。代码里我们定义一个计数器变量，每次分类器错误地分类数据，计数器就加1，程序执行完成之后计数器的结果除以数据点总数即是错误率。

为了测试分类器效果，在kNN.py文件中创建函数datingClassTest，该函数是自包含的，你可以在任何时候在Python运行环境中使用该函数测试分类器效果。在kNN.py文件中输入下面的程序代码。

程序清单2-4 分类器针对约会网站的测试代码  
```python
def datingClassTest():
    hoRatio = 0.10
    datingDataMat,datingLabels = file2matrix('datingTestSet.txt')
    normMat, ranges, minVals = autoNorm(datingDataMat)
    m = normMat.shape[0]
    numTestVecs = int(m*hoRatio)
    errorCount = 0.0
    for i in range(numTestVecs):
        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],\datingLabels[numTestVcs:m],3)
        print "the classifier came back with: %d, the real answer is: %d"\% (classifierResult, datingLabels[i])
        if (classifierResult != datingLabels[i]): errorCount += 1.0
    print "the total error rate is: %f" % (errorCount/float(numTestVecs)) 
```

函数datingClassTest如程序清单2-4所示，它首先使用了file2matrix和autoNorm()函数从文件中读取数据并将其转换为归一化特征值。接着计算测试向量的数量，此步决定了normMat向量中哪些数据用于测试，哪些数据用于分类器的训练样本；然后将这两部分数据输入到原始kNN分类器函数classify0。最后，函数计算错误率并输出结果。注意此处我们使用原始分类器，本章花费了大量的篇幅在讲解如何处理数据，如何将数据改造为分类器可以使用的特征值。得到可靠的数据同样重要，本书后续的章节将介绍这个主题。

在Python命令提示符下重新加载kNN模块，并输入kNN.datingClassTest()，执行分类器测试程序，我们将得到下面的输出结果：

```txt
the classifier came back with: 1, the real answer is: 1
the classifier came back with: 2, the real answer is: 2
...
the classifier came back with: 1, the real answer is: 1
the classifier came back with: 2, the real answer is: 2
...
the classifier came back with: 3, the real answer is: 3
the classifier came back with: 3, the real answer is: 1
the classifier came back with: 2, the real answer is: 2
the total error rate is: 0.024000 
```

分类器处理约会数据集的错误率是 $2.4\%$ ，这是一个相当不错的结果。我们可以改变函数 datingClassTest 内变量hoRatio 和变量k 的值，检测错误率是否随着变量值的变化而增加。依赖于分类算法、数据集和程序设置，分类器的输出结果可能有很大的不同。

这个例子表明我们可以正确地预测分类，错误率仅仅是 $2.4\%$ 。海伦完全可以输入未知对象的属性信息，由分类软件来帮助她判定某一对象的可交往程度：讨厌、一般喜欢、非常喜欢。

# 2.2.5 使用算法：构建完整可用系统

上面我们已经在数据上对分类器进行了测试，现在终于可以使用这个分类器为海伦来对人们分类。我们会给海伦一小段程序，通过该程序海伦会在约会网站上找到某个人并输入他的信息。程序会给出她对对方喜欢程度的预测值。

将下列代码加入到kNN.py并重新载入kNN。

程序清单2-5 约会网站预测函数  
```python
def classifyPerson():
    resultList = ['not at all', 'in small doses', 'in large doses']
    percentTats = float(raw_input())
    "percentage of time spent playing video games?")
    ffMiles = float(raw_input("frequent flicker miles earned per year?"))
    iceCream = float(raw_input("liters of ice cream consumed per year?"))
    datingDataMat, datingLabels = file2matrix('datingTestSet2.txt')
    normMat, ranges, minVals = autoNorm(datingDataMat)
    inArr = array([ffMiles, percentTats, iceCream])
    classifierResult = classify0((inArr - \
        minVals)/ranges, normMat, datingLabels, 3)
print "You will probably like this person: ", 
```

上述程序清单中的大部分代码我们在前面都见过。唯一新加入的代码是函数raw_input()。该函数允许用户输入文本行命令并返回用户所输入的命令。为了解程序的实际运行效果，输入如下命令：

```txt
>>> kNNclassifyPerson() percentage of time spent playing video games?10 frequent flier miles earned per year?10000 liters of ice cream consumed per year?0.5 You will probably like this person: in small doses 
```

目前为止，我们已经看到如何在数据上构建分类器。这里所有的数据让人看起来都很容易，但是如何在人不太容易看懂的数据上使用分类器呢？从下一节的例子中，我们会看到如何在二进制存储的图像数据上使用kNN。

# 2.3 示例：手写识别系统

本节我们一步步地构造使用 $k$ -近邻分类器的手写识别系统。为了简单起见，这里构造的系统只能识别数字0到9，参见图2.6。需要识别的数字已经使用图形处理软件，处理成具有相同的色彩和大小①：宽高是32像素 $\times$ 32像素的黑白图像。尽管采用文本格式存储图像不能有效地利用内存空间，但是为了方便理解，我们还是将图像转换为文本格式。

# 示例：使用 $k$ -近邻算法的手写识别系统

(1)收集数据：提供文本文件。  
(2) 准备数据：编写函数 classify0()，将图像格式转换为分类器使用的 list 格式。  
(3) 分析数据：在Python命令提示符中检查数据，确保它符合要求。

(4) 训练算法：此步骤不适用于 $k$ -近邻算法。  
(5) 测试算法：编写函数使用提供的部分数据集作为测试样本，测试样本与非测试样本的区别在于测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。  
(6) 使用算法：本例没有完成此步骤，若你感兴趣可以构建完整的应用程序，从图像中提取数字，并完成数字识别，美国的邮件分拣系统就是一个实际运行的类似系统。

# 2.3.1 准备数据：将图像转换为测试向量

实际图像存储在第2章源代码的两个子目录内：目录trainingDigits中包含了大约2000个例子，每个例子的内容如图2-6所示，每个数字大约有200个样本；目录testDigits中包含了大约900个测试数据。我们使用目录trainingDigits中的数据训练分类器，使用目录testDigits中的数据测试分类器的效果。两组数据没有覆盖，你可以检查一下这些文件夹的文件是否符合要求。

![](images/62618fef2899d0d334fe974fe2d02e7bd56b8cec6e845af3e3d1bf6f2103a457.jpg)  
图2-6 手写数字数据集的例子

为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。我们将把一个 $32 \times 32$ 的二进制图像矩阵转换为 $1 \times 1024$ 的向量，这样前两节使用的分类器就可以处理数字图像信息了。

我们首先编写一段函数img2vector，将图像转换为向量：该函数创建 $1 \times 1024$ 的NumPy数组，然后打开给定的文件，循环读出文件的前32行，并将每行的头32个字符值存储在NumPy数组中，最后返回数组。

```python
def img2vector(filename):
    returnVect = zeros((1, 1024))
    fr = open(filename)
    for i in range(32):
        lineStr = fr.readline()
        for j in range(32):
            returnVect[0, 32*i+j] = int(lineStr[j])
    return returnVect 
```

将上述代码输入到kNN.py文件中，在Python命令行中输入下列命令测试img2vector函数，然后与文本编辑器打开的文件进行比较：

```python
>>> testVector = kNN.img2vector('testDigits/0_13.txt')
>>> testVector[0,0:31]
array(
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.
    0., 1., 1., 1., 1., 0., 0., 0., 0., 0.
    0., 0., 0., 0., 0.})
>>> testVector[0,32:63]
array(
    0., 0., 0., 0., 0., 0., 0., 0., 0.
    1., 1., 1., 1., 1., 1., 0., 0.
    0., 0., 0., 0., 0.]) 
```

# 2.3.2 测试算法：使用 $k$ -近邻算法识别手写数字

上节我们已经将数据处理成分类器可以识别的格式，本节我们将这些数据输入到分类器，检测分类器的执行效果。程序清单2-6所示的自包含函数handwritingClassTest()是测试分类器的代码，将其写入kNN.py文件中。在写入这些代码之前，我们必须确保将from os import listdir写入文件的起始部分，这段代码的主要功能是从os模块中导入函数listdir，它可以列出给定目录的文件名。

程序清单2-6 手写数字识别系统的测试代码  
def handwritingClassTest(   ) :
    hwLabels $= \left\lbrack  \right\rbrack$ trainingFileList $=$ listdir('trainingDigits') $\mathrm{m} = \operatorname{len}\left( {\text{trainingFileList}}\right)$ trainingMat $=$ zeros((m,1024))
    for i in range(m):
     fileNameStr $=$ trainingFileList[i]
    fileName $=$ fileNameStr.split("\\.")
    -classNumStr $=$ int(fileStr.split("\\").[0])
    hwLabels.append(classNumStr)
     trainingMat $\left\lbrack  {i,:}\right\rbrack   =$ img2vector('trainingDigits/%s' % fileNameStr)
     testFileList $=$ listdir('testDigits')
     errorCount $= {0.0}$ mTest $=$ len(testFileList)
    for i in range(mTest):
     fileNameStr $=$ testFileList[i]
     fileName $=$ fileNameStr.split("\\").[0]
     classNumStr $=$ int(fileStr.split("\\").[0])
     vectorUnderTest $=$ img2vector('testDigits/%s' % fileNameStr)
     classifierResult $=$ classify0.vectorUnderTest, \\ trainingMat, hwLabels, 3)
     print "the classifier came back with: %d, the real answer is: %d" \\ %(classifierResult, classifierStr)
    if (classifierResult != classNumStr): errorCount += 1.0
    print "\\nthe total number of errors is: %d" % errorCount
    print "\\nthe total error rate is: %f" % (errorCount/float(mTest))

在程序清单2-6中，将trainingDigits目录中的文件内容存储在列表中①，然后可以得到目录中有

多少文件，并将其存储在变量m中。接着，代码创建一个m行1024列的训练矩阵，该矩阵的每行数据存储一个图像。我们可以从文件名中解析出分类数字②。该目录下的文件按照规则命名，如文件9_45.txt的分类是9，它是数字9的第45个实例。然后我们可以将类代码存储在hwLabels向量中，使用前面讨论的img2vector函数载入图像。在下一步中，我们对testDigits目录中的文件执行相似的操作，不同之处是我们并不将这个目录下的文件载入矩阵中，而是使用classify0()函数测试该目录下的每个文件。由于文件中的值已经在0和1之间，本节并不需要使用2.2节的autoNorm()函数。

在Python命令提示符中输入kNN_handwritingClassTest()，测试该函数的输出结果。依赖于机器速度，加载数据集可能需要花费很长时间，然后函数开始依次测试每个文件，输出结果如下所示：

```txt
>>> kNN_handwritingClassTest()
the classifier came back with: 0, the real answer is: 0
the classifier came back with: 0, the real answer is: 0
...
the classifier came back with: 7, the real answer is: 7
the classifier came back with: 7, the real answer is: 7
the classifier came back with: 8, the real answer is: 8
the classifier came back with: 8, the real answer is: 8
the classifier came back with: 8, the real answer is: 8
the classifier came back with: 6, the real answer is: 8
...
the classifier came back with: 9, the real answer is: 9
the total number of errors is: 11
the total error rate is: 0.011628 
```

$k$ -近邻算法识别手写数字数据集，错误率为 $1.2\%$ 。改变变量 $k$ 的值、修改函数handwriting-ClassTest随机选取训练样本、改变训练样本的数目，都会对 $k$ -近邻算法的错误率产生影响，感兴趣的话可以改变这些变量值，观察错误率的变化。

实际使用这个算法时，算法的执行效率并不高。因为算法需要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行900次，此外，我们还需要为测试向量准备2MB的存储空间。是否存在一种算法减少存储空间和计算时间的开销呢？k决策树就是 $k$ -近邻算法的优化版，可以节省大量的计算开销。

# 2.4 本章小结

$k$ -近邻算法是分类数据最简单最有效的算法，本章通过两个例子讲述了如何使用 $k$ -近邻算法构造分类器。 $k$ -近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。 $k$ -近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。

$k$ -近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。下一章我们将使用概率测量方法处理分类问题，该算法可以解决这个问题。

# 第3章

# 决策树

# 本章内容

□决策树简介  
□在数据集中度量一致性  
□ 使用递归构造决策树  
□ 使用Matplotlib绘制树形图

你是否玩过二十个问题的游戏，游戏的规则很简单：参与游戏的一方在脑海里想某个事物，其他参与者向他提问题，只允许提20个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围。决策树的工作原理与20个问题类似，用户输入一系列数据，然后给出游戏的答案。

我们经常使用决策树处理分类问题，近来的调查表明决策树也是最经常使用的数据挖掘算法①。它之所以如此流行，一个很重要的原因就是使用者基本上不用了解机器学习算法，也不用深究它是如何工作的。

如果你以前没有接触过决策树，完全不用担心，它的概念非常简单。即使不知道它也可以通过简单的图形了解其工作原理，图3-1所示的流程图就是一个决策树，正方形代表判断模块(decision block)，椭圆形代表终止模块（terminating block），表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作分支（branch），它可以到达另一个判断模块或者终止模块。图3-1构造了一个假想的邮件分类系统，它首先检测发送邮件域名地址。如果地址为myEmployer.com，则将其放在分类“无聊时需要阅读的邮件”中。如果邮件不是来自这个域名，则检查邮件内容里是否包含单词曲棍球，如果包含则将邮件归类到“需要及时处理的朋友邮件”，如果不包含则将邮件归类到“无需阅读的垃圾邮件”。

第2章介绍的 $k$ -近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解。

本章构造的决策树算法能够读取数据集合，构建类似于图3-1的决策树。决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列

规则，机器学习算法最终将使用这些机器从数据集中创造的规则。专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。

![](images/043d01807cd935143ad6460f6561fe01d8c7c056696c1971d29bd7d56387957d.jpg)  
图3-1 流程图形式的决策树

现在我们已经大致了解了决策树可以完成哪些任务，接下来我们将学习如何从一堆原始数据中构造决策树。首先我们讨论构造决策树的方法，以及如何编写构造树的Python代码；接着提出一些度量算法成功率的方法；最后使用递归建立分类器，并且使用Matplotlib绘制决策树图。构造完成决策树分类器之后，我们将输入一些隐形眼镜的处方数据，并由决策树分类器预测需要的镜片类型。

# 3.1 决策树的构造

# 决策树

优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

缺点：可能会产生过度匹配问题。

适用数据类型：数值型和标称型。

本节将一步步地构造决策树算法，并会涉及许多有趣的细节。首先我们讨论数学上如何使用信息论划分数据集，然后编写代码将理论应用到具体的数据集上，最后编写代码构建决策树。

在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支

上。如果某个分支下的数据属于同一类型, 则当前无需阅读的垃圾邮件已经正确地划分数据分类, 无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型, 则需要重复划分数据子集的过程。如何划分数据子集的算法和划分原始数据集的方法相同, 直到所有具有相同类型的数据均在一个数据子集内。

创建分支的伪代码函数createBranch()如下所示：

检测数据集中的每个子项是否属于同一分类：

If so return 类标签;

Else

寻找划分数据集的最好特征

划分数据集

创建分支节点

for每个划分的子集

调用函数createBranch并增加返回结果到分支节点中

return 分支节点

上面的伪代码createBranch是一个递归函数，在倒数第二行直接调用了它自己。后面我们将把上面的伪代码转换为Python代码，这里我们需要进一步了解算法是如何划分数据集的。

# 决策树的一般流程

(1)收集数据：可以使用任何方法。  
(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。  
(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。  
(4) 训练算法：构造树的数据结构。  
(5) 测试算法：使用经验树计算错误率。  
(6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。

一些决策树算法采用二分法划分数据，本书并不采用这种方法。如果依据某个属性划分数据将会产生4个可能的值，我们将把数据划分成四块，并创建四个不同的分支。本书将使用ID3算法划分数据集，该算法处理如何划分数据集，何时停止划分数据集（进一步的信息可以参见http://en.wikipedia.org/wiki/ID3_algorithm。每次划分数据集时我们只选取一个特征属性，如果训练集中存在20个特征，第一次我们选择哪个特征作为划分的参考属性呢？

表3-1的数据包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚蹼。我们可以将这些动物分成两类：鱼类和非鱼类。现在我们想要决定依据第一个特征还是第二个特征划分数据。在回答这个问题之前，我们必须采用量化的方法判断如何划分数据。下一小节将详细讨论这个问题。

表3-1 海洋生物数据  

<table><tr><td></td><td>不浮出水面是否可以生存</td><td>是否有脚蹼</td><td>属于鱼类</td></tr><tr><td>1</td><td>是</td><td>是</td><td>是</td></tr><tr><td>2</td><td>是</td><td>是</td><td>是</td></tr><tr><td>3</td><td>是</td><td>否</td><td>否</td></tr><tr><td>4</td><td>否</td><td>是</td><td>否</td></tr><tr><td>5</td><td>否</td><td>是</td><td>否</td></tr></table>

# 3.1.1 信息增益

划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。我们可以在划分数据之前使用信息论量化度量信息的内容。

在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。

在可以评测哪种数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德·香农。

# 克劳德·香农

克劳德·香农被公认为是二十世纪最聪明的人之一，威廉·庞德斯通在其2005年出版的《财富公式》一书中是这样描写克劳德·香农的：

“贝尔实验室和MIT有很多人将香农和爱因斯坦相提并论，而其他人则认为这种对比是不公平的——对香农是不公平的。”①

如果看不明白什么是信息增益（information gain）和熵（entropy），请不要着急——它们自诞生的那一天起，就注定会令世人十分费解。克劳德·香农写完信息论之后，约翰·冯·诺依曼建议使用“熵”这个术语，因为大家都不知道它是什么意思。

熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事务可能划分在多个分类之中，则符号 $x_{i}$ 的信息定义为

$$
I \left(x _ {i}\right) = - \log_ {2} p \left(x _ {i}\right)
$$

其中 $p(x_{i})$ 是选择该分类的概率。

为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：

$$
H = - \sum_ {i = 1} ^ {n} p \left(x _ {i}\right) \log_ {2} p \left(x _ {i}\right)
$$

其中 $n$ 是分类的数目。

下面我们将学习如何使用Python计算信息熵，创建名为trees.py的文件，将程序清单3-1的代码内容录入到trees.py文件中，此代码的功能是计算给定数据集的熵。

# 程序清单3-1 计算给定数据集的香农熵

from math import log   
def calcShannonEnt(dataSet): numEntries $=$ len(dataSet) labelCounts $= \{\}$ forfeatVec in dataSet: currentLabel $=$ featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentTime] $= 0$ labelCounts[currentLabel] $+ = 1$ shannonEnt $= 0.0$ forkey in labelCounts: prob $=$ float.labelCounts[key])/numEntries shannonEnt $= =$ prob $\star$ log(prob,2) return shannonEnt

![](images/7e19027bc44bc8dbc6f133aee3b3ae47b34635f5fb2c25446780a997b931882e.jpg)

为所有可能分类创建字典

![](images/1f3addf82574ac3fcfa6a5b7e0a2f076a442dbeec86f1c52eb968b2f16961916.jpg)

以2为底求对数

程序清单3-1的代码非常简单。首先，计算数据集中实例的总数。我们也可以在需要时再计算这个值，但是由于代码中多次用到这个值，为了提高代码效率，我们显式地声明一个变量保存实例总数。然后，创建一个数据字典，它的键值是最后一列的数值①。如果当前键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。最后，使用所有类标签的发生频率计算类别出现的概率。我们将用这个概率计算香农熵②，统计所有类标签发生的次数。下面我们看看如何使用熵划分数据集。

在trees.py文件中，我们可以利用createDataSet()函数得到表3-1所示的简单鱼鉴定数据集，你可以输入自己的createDataSet()函数：

```python
def createDataSet():  
    dataSet = [[1, 1, 'yes'],
[1, 1, 'yes'],
[1, 0, 'no'],
[0, 1, 'no'],
[0, 1, 'no']]]  
labels = ['no surfacing', 'flippers']  
return dataSet, labels 
```

在Python命令提示符下输入下列命令：

```txt
>>> reload(trees.py)
>>> myDat, labels = trees.createDataSet()
>>> myDat
[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']
>>> trees(calcShannonEnt(myDat)
0.97095059445466858 
```

熵越高, 则混合的数据也越多, 我们可以在数据集中添加更多的分类, 观察熵是如何变化的。这里我们增加第三个名为maybe的分类, 测试熵的变化:

```txt
>>> myDat[0][-1] = 'maybe'
>>> myDat
[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']
>>> trees.calcShannonEnt(myDat)
1.3709505944546687 
```

得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集，下一节我们将具体学习如何划分数据集以及如何度量信息增益。

另一个度量集合无序程度的方法是基尼不纯度①（Gini impurity），简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。本书不采用基尼不纯度方法，这里就不再做进一步的介绍。下面我们将学习如何划分数据集，并创建决策树。

# 3.1.2 划分数据集

上节我们学习了如何度量数据集的无序程度，分类算法除了需要测量信息熵，还需要划分数据集，度量花费数据集的熵，以便判断当前是否正确地划分了数据集。我们将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。想象一个分布在二维空间的数据散点图，需要在数据之间划条线，将它们分成两部分，我们应该按照 $x$ 轴还是 $y$ 轴划线呢？答案就是本节讲述的内容。

要划分数据集，打开文本编辑器，在trees.py文件中输入下列的代码：

# 程序清单3-2 按照给定特征划分数据集

```python
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in DataSet:
        if featVec(axis) == value:
            reducedFeatVec = featVec[:, axis]
            reducedFeatVec.appendfeatVec(axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet 
```

程序清单3-2的代码使用了三个输入参数：待划分的数据集、划分数据集的特征、特征的返回值。需要注意的是，Python语言不用考虑内存分配问题。Python语言在函数中传递的是列表的引用，在函数内部对列表对象的修改，将会影响该列表对象的整个生存周期。为了消除这个不良影响，我们需要在函数的开始声明一个新列表对象。因为该函数代码在同一数据集上被调用多次，为了不修改原始数据集，创建一个新的列表对象①。数据集这个列表中的各个元素也是列表，我们要遍历数据集中的每个元素，一旦发现符合要求的值，则将其添加到新创建的列表中。在if语句中，程序将符合特征的数据抽取出来②。后面讲述得更简单，这里我们可以这样理解这段代码：当我们按照某个特征划分数据集时，就需要将所有符合要求的元素抽取出来。代码中使用了Python语言列表类型自带的extend()和append()方法。这两个方法功能类似，但是在处理多个列表时，这两个方法的处理结果是完全不同的。

假定存在两个列表，a和b：

```txt
>>> a=[1,2,3]  
>>> b=[4,5,6]  
>>> a.append(b)  
>>> a  
[1, 2, 3, [4, 5, 6]] 
```

如果执行a.append(b)，则列表得到了第四个元素，而且第四个元素也是一个列表。然而如果使用extend方法：

```erlang
>> a=[1,2,3]  
>> a extend(b)  
>> a  
[1, 2, 3, 4, 5, 6] 
```

则得到一个包含a和b所有元素的列表。

我们可以在前面的简单样本数据上测试函数splitDataSet()。首先还是要将程序清单3-2的代码增加到trees.py文件中，然后在Python命令提示符内输入下述命令：

```txt
>>> reload(trees)
<module 'trees' from 'trees.pyc>
>>> myDat, labels = trees.createDataSet()
>>> myDat
[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']
>>> trees.splitDataSet(myDat, 0, 1)
[[1, 'yes'], [1, 'yes'], [0, 'no']
>>> trees.splitDataSet(myDat, 0, 0)
[[1, 'no'], [1, 'no']] 
```

接下来我们将遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的特征划分方式。熵计算将会告诉我们如何划分数据集是最好的数据组织方式。

打开文本编辑器，在trees.py文件中输入下面的程序代码。

# 程序清单3-3 选择最好的数据集划分方式

defchooseBestFeatureToSplit(dataSet):
numFeatures $=$ len(dataSet[0])-1
baseEntropy $=$ calcShannonEnt(dataSet)
bestInfoGain $= 0.0$ ;bestFeature $\equiv -1$ fori in range(numFeatures):
featList $=$ [example[i] for example in dataSet]
uniqueVals $=$ set(featList)
newEntropy $= 0.0$ forvaluein uniqueVals:
subDataSet $=$ splitDataSet(dataSet,i, value)
prob $=$ len(subDataSet)/float(len(dataSet))
newEntropy $+ =$ prob\*calcShannonEnt(subDataSet)
infoGain $=$ baseEntropy-newEntropy
if (infoGain $>$ bestInfoGain):
bestInfoGain $=$ infoGain
bestFeature $= i$

程序清单3-3给出了函数chooseBestFeatureToSplit()的完整代码，该函数实现选取特

征，划分数据集，计算得出最好的划分数据集的特征。函数chooseBestFeatureToSplit()使用了程序清单3-1和程序清单3-2中的函数。在函数中调用的数据需要满足一定的要求：第一个要求是，数据必须是一种由列表元素组成的列表，而且所有的列表元素都要具有相同的数据长度；第二个要求是，数据的最后一列或者每个实例的最后一个元素是当前实例的类别标签。数据集一旦满足上述要求，我们就可以在函数的第一行判定当前数据集包含多少特征属性。我们无需限定list中的数据类型，它们既可以是数字也可以是字符串，并不影响实际计算。

在开始划分数据集之前，程序清单3-3的第3行代码计算了整个数据集的原始香农熵，我们保存最初的无序度量值，用于与划分完之后的数据集计算的熵值进行比较。第1个for循环遍历数据集中的所有特征。使用列表推导（ListComprehension）来创建新的列表，将数据集中所有第i个特征值或者所有可能存在的值写入这个新list中①。然后使用Python语言原生的集合（set）数据类型。集合数据类型与列表类型相似，不同之处仅在于集合类型中的每个值互不相同。从列表中创建集合是Python语言得到列表中唯一元素值的最快方法。

遍历当前特征中的所有唯一属性值，对每个特征划分一次数据集②，然后计算数据集的新熵值，并对所有唯一特征值得到的熵求和。信息增益是熵的减少或者是数据无序度的减少，大家肯定对于将熵用于度量数据无序度的减少更容易理解。最后，比较所有特征中的信息增益，返回最好特征划分的索引值③。

现在我们可以测试上面代码的实际输出结果，首先将程序清单3-3的内容输入到文件trees.py中，然后在Python命令提示符下输入下列命令：

```txt
>>> reload(trees)
<module 'trees' from 'trees.py>
>>> myDat, labels = trees.createDataSet()
>>> trees可以选择BestFeatureToSplit(myDat)
0
>>> myDat
[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']) 
```

代码运行结果告诉我们，第0个特征是最好的用于划分数据集的特征。结果是否正确呢？这个结果又有什么实际意义呢？数据集中的数据来源于表3-1,让我们回头再看一下表1-1或者变量myDat中的数据。如果我们按照第一个特征属性划分数据，也就是说第一个特征是1的放在一个组，第一个特征是0的放在另一个组，数据一致性如何？按照上述的方法划分数据集，第一个特征为1的海洋生物分组将有两个属于鱼类，一个属于非鱼类；另一个分组则全部属于非鱼类。如果按照第二个特征分组，结果又是怎么样呢？第一个海洋动物分组将有两个属于鱼类，两个属于非鱼类；另一个分组则只有一个非鱼类。如果不相信目测结果，读者可以使用程序清单3-1的calcShannonEntropy()函数测试不同特征分组的输出结果。

本节我们学习了如何度量数据集的信息熵，如何有效地划分数据集，下一节我们将介绍如何将这些函数功能放在一起，构建决策树。

# 3.1.3 递归构建决策树

目前我们已经学习了从数据集构造决策树算法所需要的子功能模块，其工作原理如下：得到

原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。

递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类，参见图3-2所示。

![](images/c5827762aee856fb919412eb691114c2a918a26d417cb926d604603c1fae2f5c.jpg)  
图3-2 划分数据集时的数据路径

第一个结束条件使得算法可以终止，我们甚至可以设置算法可以划分的最大分组数目。后续章节还会介绍其他决策树算法，如C4.5和CART，这些算法在运行时并不总是在每次划分分组时都会消耗特征。由于特征数目并不是在每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。

打开文本编辑器，在增加下面的函数之前，在trees.py文件顶部增加一行代码：import operator，然后添加下面的代码到trees.py文件中：

```python
def majorityCnt(classList):
    classCount=\}
    for vote in classList:
        if vote not in classCount.keys(): count[vote] = 0
        count[vote] += 1
    sortedClassCount = sorted(count.items(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0] 
```

上面的代码与第2章classify0部分的投票表决代码非常类似,该函数使用分类名称的列表,然后创建键值为classList中唯一值的数据字典,字典对象存储了classList中每个类标签出现的频率,最后利用operator操作键值排序字典,并返回出现次数最多的分类名称。

在文本编辑器中打开trees.py文件，添加下面的程序代码。

# 程序清单3-4 创建树的函数代码

```python
def createTree(dataSet,labels):
    classList = [example[-1] for example in dataSet]
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel:{}}
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    for value in uniqueVals:
        subLabels = labels[:]  
        myTree[bestFeatLabel][value] = createTree.splitDataSet\((dataSet, bestFeat, value),subLabels)
    return myTree 
```

程序清单3-4的代码使用两个输入参数：数据集和标签列表。标签列表包含了数据集中所有特征的标签，算法本身并不需要这个变量，但是为了给出数据明确的含义，我们将它作为一个输入参数提供。此外，前面提到的对数据集的要求这里依然需要满足。上述代码首先创建了名为classList的列表变量，其中包含了数据集的所有类标签。递归函数的第一个停止条件是所有的类标签完全相同，则直接返回该类标签①。递归函数的第二个停止条件是使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组②。由于第二个条件无法简单地返回唯一的类标签，这里使用程序清单3-3的函数挑选出现次数最多的类别作为返回值。

下一步程序开始创建树，这里使用Python语言的字典类型存储树的信息，当然也可以声明特殊的数据类型存储树，但是这里完全没有必要。字典变量myTree存储了树的所有信息，这对于其后绘制树形图非常重要。当前数据集选取的最好特征存储在变量bestFeat中，得到列表包含的所有属性值③。这部分代码与程序清单3-3中的部分代码类似，这里就不再进一步解释了。

最后代码遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()，得到的返回值将被插入到字典变量myTree中，因此函数终止执行时，字典中将会嵌套很多代表叶子节点信息的字典数据。在解释这个嵌套数据之前，我们先看一下循环的第一行subLabels = labels[:]，这行代码复制了类标签，并将其存储在新列表变量subLabels中。之所以这样做，是因为在Python语言中函数参数是列表类型时，参数是按照引用方式传递的。为了保证每次调用函数createTree()时不改变原始列表的内容，使用新变量subLabels代替原始列表。

现在我们可以测试上面代码的实际输出结果，首先将程序清单3-4的内容输入到文件trees.py中，然后在Python命令提示符下输入下列命令：

```txt
>>> reload(trees)
<module 'trees' from 'trees.pyc>
>>> myDat, labels = trees.createDataSet()
>>> myTree = trees.createTree(myDat, labels)
>>> myTree
{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}})} 
```

变量myTree包含了很多代表树结构信息的嵌套字典，从左边开始，第一个关键字nosurfacing是第一个划分数据集的特征名称，该关键字的值也是另一个数据字典。第二个关键字是nosurfacing特征划分的数据集，这些关键字的值是nosurfacing节点的子节点。这些值可能是类标签，也可能是另一个数据字典。如果值是类标签，则该子节点是叶子节点；如果值是另一个数据字典，则子节点是一个判断节点，这种格式结构不断重复就构成了整棵树。本节的例子中，这棵树包含了3个叶子节点以及2个判断节点。

本节讲述了如何正确地构造树，下一节将介绍如何绘制图形，方便我们正确理解数据信息的内在含义。

# 3.2 在Python中使用Matplotlib注解绘制树形图

上节我们已经学习了如何从数据集中创建树，然而字典的表示形式非常不易于理解，而且直接绘制图形也比较困难。本节我们将使用Matplotlib创建树形图。决策树的主要优点就是直观易于理解，如果不能将其直观地显示出来，就无法发挥其优势。虽然前面章节我们使用的图形库已经非常强大，但是Python并没有提供绘制树的工具，因此我们必须自己绘制树形图。本节我们将学习如何编写代码绘制如图3-3所示的决策树。

![](images/cc80f728cb4a4e982ba329680ba22122caf970217f26825b79d0412900d46ae5.jpg)  
图3-3 决策树的范例

# 3.2.1 Matplotlib注解

Matplotlib提供了一个注解工具annotations，非常有用，它可以在数据图形上添加文本注释。注解通常用于解释数据的内容。由于数据上面直接存在文本描述非常丑陋，因此工具内嵌支持带箭头的划线工具，使得我们可以在其他恰当的地方指向数据位置，并在此处添加描述信息，解释数据内容。如图3-4所示，在坐标(0.2, 0.1)的位置有一个点，我们将对该点的描述信息放在(0.35, 0.3)的位置，并用箭头指向数据点(0.2, 0.1)。

![](images/3635d2b1c23d9e314784405a35c05ffeecd50049bd4a9bcf50c91576721d935a.jpg)  
图3-4 Matplotlib注解示例

# 绘制还是图形化

为什么使用单词“绘制”（plot）？为什么在讨论如何在图形上显示数据的时候不使用单词“图形化”（graph）？这里存在一些语言上的差别，英语单词graph在某些学科中具有特定的含义，如在应用数学中，一系列由边连接在一起的对象或者节点称为图。节点的任意联系都可以通过边来连接。在计算机科学中，图是一种数据结构，用于表示数学上的概念。好在汉语并不存在这些混淆的概念，这里就统一使用绘制树形图。

本书将使用Matplotlib的注解功能绘制树形图，它可以对文字着色并提供多种形状以供选择，而且我们还可以反转箭头，将它指向文本框而不是数据点。打开文本编辑器，创建名为treePlotter.py的新文件，然后输入下面的程序代码。

# 程序清单3-5 使用文本注解绘制树节点

import matplotlib.pyplot as plt   
decisionNode $=$ dict (bxstyle $\equiv$ "sawtooth", fc $\equiv$ "0.8") leafNode $=$ dict (bxstyle $\equiv$ "round4", fc $\equiv$ "0.8") arrow_args $=$ dict (arrowstyle $\equiv$ "<-")   
def plotNode(nodeTxt, centerPt, parentPt, nodeType): createPlot=ax1.annotate(nodeTxt, xy=parentPt, xycoordinates $\equiv$ 'axes fraction',

1 定义文本框和箭头格式  
2 绘制带箭头的注解

xytext=centerPt, textcoords='axes fraction', va="center", ha="center", bbox=nodeType, arrowprops=arrow_args)   
def createPlot(): fig $=$ plt.figure(1, facecolor $\equiv$ 'white') fig clf() createPlot(ax1 $=$ plt.subplot(111, frameon $\equiv$ False) plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode) plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode) plt.show()

这是第一个版本的createPlot()函数，与例子文件中的createPlot()函数有些不同，随着内容的深入，我们将逐步添加缺失的代码。代码定义了树节点格式的常量①。然后定义plotNode()函数执行了实际的绘图功能，该函数需要一个绘图区，该区域由全局变量createPlot(ax1定义。Python语言中所有的变量默认都是全局有效的，只要我们清楚知道当前代码的主要功能，并不会引入太大的麻烦。最后定义createPlot()函数，它是这段代码的核心。createPlot()函数首先创建了一个新图形并清空绘图区，然后在绘图区上绘制两个代表不同类型的树节点，后面我们将用这两个节点绘制树形图。

为了测试上面代码的实际输出结果，打开Python命令提示符，导入treePlotter模块：

```txt
>>> import treePlotter
>>> treePlotter.createPlot() 
```

程序的输出结果如图3-5所示，我们也可以改变函数plotNode()②，观察图中x、y位置如何变化。

![](images/2de44ebaf96e413765c9495b4d61c4e29cd30cbd21dd59b7894e664c794e1c50.jpg)  
图3-5 函数plotNode的例子

现在我们已经掌握了如何绘制树节点，下面将学习如何绘制整棵树。

# 3.2.2 构造注解树

绘制一棵完整的树需要一些技巧。我们虽然有 $x$ 、 $y$ 坐标，但是如何放置所有的树节点却是个问题。我们必须知道有多少个叶节点，以便可以正确确定 $x$ 轴的长度；我们还需要知道树有多少层，

以便可以正确确定y轴的高度。这里我们定义两个新函数getNumLeafs()和getTreeDepth()，来获取叶节点的数目和树的层数，参见程序清单3-6，并将这两个函数添加到文件treePlotter.py中。

程序清单3-6 获取叶节点的数目和树的层数  
```python
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = myTree.keys() [0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            numLeafs += getNumLeafs(secondDict[key])
        else:
            numLeafs += 1
    return numLeafs
def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = myTree.keys() [0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:
            thisDepth = 1
    if thisDepth > maxDepth: maxDepth = thisDepth
    return maxDepth 
```

1

测试节点的数据类型是否为字典

上述程序中的两个函数具有相同的结构，后面我们也将使用到这两个函数。这里使用的数据结构说明了如何在Python字典类型中存储树信息。第一个关键字是第一次划分数据集的类别标签，附带的数值表示子节点的取值。从第一个关键字出发，我们可以遍历整棵树的所有子节点。使用Python提供的type()函数可以判断子节点是否为字典类型①。如果子节点是字典类型，则该节点也是一个判断节点，需要递归调用getNumLeafs()函数。getNumLeafs()函数遍历整棵树，累计叶子节点的个数，并返回该数值。第2个函数getTreeDepth()计算遍历过程中遇到判断节点的个数。该函数的终止条件是叶子节点，一旦到达叶子节点，则从递归调用中返回，并将计算树深度的变量加一。为了节省大家的时间，函数retrieveTree输出预先存储的树信息，避免了每次测试代码时都要从数据中创建树的麻烦。

添加下面的代码到文件treePlotter.py中：

```python
def retrieveTree(i):
    listOfTrees = [(‘no surfacing’: {0: 'no', 1: {'flippers': \{0: 'no', 1: 'yes'}})}], {'no surfacing': {0: 'no', 1: {'flippers': \{0: {'head': {0: 'no', 1: 'yes'}}}, 1: 'no'}})] return listOfTrees[i] 
```

保存文件treePlotter.py，在Python命令提示符下输入下列命令：

```txt
>>> reload(treePlotter)
<module 'treePlotter' from 'treePlotter.py">
>>> treePlotter.retrieveTree (1) 
```

```python
{'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}}, 1: 'no'}}}  
>>> myTree = treePlotter.retrieveTree (0)  
>>> treePlotter_NumLeafs(myTree)  
3  
>>> treePlotter.getTreeDepth(myTree)  
2 
```

函数retrieveTree()主要用于测试，返回预定义的树结构。上述命令中调用getNumLeafs()函数返回值为3，等于树0的叶子节点数；调用getTreeVertices()函数也能够正确返回树的层数。

现在我们可以将前面学到的方法组合在一起，绘制一棵完整的树。最终的结果如图3-6所示，但是没有 $x$ 和 $y$ 轴标签。

![](images/4fd09e549b7beb22f3c3647163ca4eda8961088c6e4c779aa21d60dd83c7ff72.jpg)  
图3-6 简单数据集绘制的树形图

打开文本编辑器，将程序清单3-7的内容添加到treePlotter.py文件中。注意，前文已经在文件中定义了函数createPlot()，此处我们需要更新这部分代码。

# 程序清单3-7 plotTree函数

```python
def plotMidText(cntrPt, parentPt, txtString):
    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]
    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]
    createPlot(ax1.text(xMid, yMid, txtString))
def plotTree(myTree, parentPt, nodeTxt):
    numLeafs = getNumLeafs(myTree)
    depth = getTreeDepth(myTree)
    firstStr = myTree.keys()
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.yOff) 
```

```python
plotMidText(cntrPt, parentPt, nodeTxt)  
plotNode(firstStr, cntrPt, parentPt, decisionNode)  
secondDict = myTree[firstStr]  
plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD  
for key in secondDict.keys():  
    if type(secondDict[key]).name == 'dict':  
        plotTree(secondDict[key], cntrPt, str(key))  
    else:  
        plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW  
        plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)  
        plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))  
    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD  
def createPlot(inTree):  
    fig = plt.figure(1, facecolor='white')  
    fig.clf()  
axprops = dict(xTicks=[[], yTicks=[[])  
createPlot=ax1 = plt.subplot(111, frameon=False, **axprops)  
plotTree.totalW = float(getNumLeafs(inTree))  
plotTree.totalD = float(getTreeDepth(inTree))  
plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;  
plotTree(inTree, (0.5,1.0), '')  
plt.show() 
```

函数createPlot()是我们使用的主函数，它调用了plotTree()，函数plotTree又依次调用了前面介绍的函数和plotMidText()。绘制树形图的很多工作都是在函数plotTree()中完成的，函数plotTree()首先计算树的宽和高①。全局变量plotTree.totalW存储树的宽度，全局变量plotTree.totalD存储树的深度，我们使用这两个变量计算树节点的摆放位置，这样可以将树绘制在水平方向和垂直方向的中心位置。与程序清单3-6中的函数getNumLeafs()和getTreeDepth()类似，函数plotTree()也是个递归函数。树的宽度用于计算放置判断节点的位置，主要的计算原则是将它放在所有叶子节点的中间，而不仅仅是它子节点的中间。同时我们使用两个全局变量plotTree.xOff和plotTree.yOff追踪已经绘制的节点位置，以及放置下一个节点的恰当位置。另一个需要说明的问题是，绘制图形的x轴有效范围是0.0到1.0，y轴有效范围也是 $0.0\sim 1.0$ 。为了方便起见，图3-6给出具体坐标值，实际输出的图形中并没有x、y坐标。通过计算树包含的所有叶子节点数，划分图形的宽度，从而计算得到当前节点的中心位置，也就是说，我们按照叶子节点的数目将x轴划分为若干部分。按照图形比例绘制树形图的最大好处是无需关心实际输出图形的大小，一旦图形大小发生了变化，函数会自动按照图形大小重新绘制。如果以像素为单位绘制图形，则缩放图形就不是一件简单的工作。

接着，绘出子节点具有的特征值，或者沿此分支向下的数据实例必须具有的特征值③。使用函数plotMidText()计算父节点和子节点的中间位置，并在此处添加简单的文本标签信息①。

然后，按比例减少全局变量plotTree.yOff，并标注此处将要绘制子节点④，这些节点即可以是叶子节点也可以是判断节点，此处需要只保存绘制图形的轨迹。因为我们是自顶向下绘制图形，因此需要依次递减y坐标值，而不是递增y坐标值。然后程序采用函数getNumLeafs()和getTreeDepth()以相同的方式递归遍历整棵树，如果节点是叶子节点则在图形上画出叶子节点，

如果不是叶子节点则递归调用plotTree()函数。在绘制了所有子节点之后，增加全局变量Y的偏移。

程序清单3-7的最后一个函数是createPlot()，它创建绘图区，计算树形图的全局尺寸，并调用递归函数plotTree()。

现在我们可以验证一下实际的输出效果。添加上述代码到文件treePlotter.py之后，在Python命令提示符下输入下列命令：

```txt
>>> reload(treePlotter)
<module 'treePlotter' from 'treePlotter.pyc>
>>> myTree = treePlotter.retrieveTree(0)
>>> treePlotter.createPlot(myTree) 
```

输出效果如图3-6所示，但是没有坐标轴标签。接着按照如下命令变更字典，重新绘制树形图：

```txt
>>> myTree['no surfacing'] [3] = 'maybe'
>>> myTree
{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}, 3: 'maybe'}}
>>> treePlotter.createPlot(myTree) 
```

输出效果如图3-7所示，有点像一个无头的简笔画。你也可以在树字典中随意添加一些数据，并重新绘制树形图观察输出结果的变化。

到目前为止，我们已经学习了如何构造决策树以及绘制树形图的方法，下节我们将实际使用这些方法，并从数据和算法中得到某些新知识。

![](images/19484e4285abf7cbc7c0e015bd85089d963a5353d3759bdd6c65c7b89990b390.jpg)  
图3-7 超过两个分支的树形图

# 3.3 测试和存储分类器

本书第一部分主要讲解机器学习的分类算法，然而到目前为止，本章学习的主要内容是如何

从原始数据集中创建决策树，并使用Python函数库绘制树形图，方便我们了解数据的真实含义，下面我们将把重点转移到如何利用决策树执行数据分类上。

本节我们将使用决策树构建分类器，以及实际应用中如何存储分类器。下一节我们将在真实数据上使用决策树分类算法，验证它是否可以正确预测出患者应该使用的隐形眼镜类型。

# 3.3.1 测试算法：使用决策树执行分类

依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点；最后将测试数据定义为叶子节点所属的类型。

为了验证算法的实际效果，打开文本编辑器，将程序清单3-8包含的代码添加到文件trees.py中。

# 程序清单3-8 使用决策树的分类函数

```python
def classify(inputTree,featLabels,testVec):
    firstStr = inputTree.keys()
    secondDict = inputTree[firstStr]
    featIndex = featLabels.index(firstStr)
    for key in secondDict.keys():
        if testVec[featIndex] == key:
            if type(secondDict[key])._name == 'dict':
                classLabel = classify(secondDict[key],featLabels,testVec)
            else:
                classLabel = secondDict[key]
    return classLabel 
```

程序清单3-8定义的函数也是一个递归函数，在存储带有特征的数据会面临一个问题：程序无法确定特征在数据集中的位置，例如前面例子的第一个用于划分数据集的特征是nosurfacing属性，但是在实际数据集中该属性存储在哪个位置？是第一个属性还是第二个属性？特征标签列表将帮助程序处理这个问题。使用index方法查找当前列表中第一个匹配firstStr变量的元素①。然后代码递归遍历整棵树，比较testVec变量中的值与树节点的值，如果到达叶子节点，则返回当前节点的分类标签。

将程序清单3-8包含的代码添加到文件trees.py之后，打开Python命令提示符，输入下列命令：

```txt
>>> myDat, labels = trees.createDataSet()
>>> labels
['no surfacing', 'flippers']
>>> myTree = treePlotter.retrieveTree(0)
>>> myTree
{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}
>>> trees-Classify(myTree, labels, [1, 0])
'no'
>>> trees.Classify(myTree, labels, [1, 1])
'yes' 
```

与图3-6比较上述输出结果。第一节点名为no surfacing，它有两个子节点：一个是名字为0的叶子节点，类标签为no；另一个是名为flippers的判断节点，此处进入递归调用，flippers节点有两个子节点。以前绘制的树形图和此处代表树的数据结构完全相同。

现在我们已经创建了使用决策树的分类器，但是每次使用分类器时，必须重新构造决策树，下一节我们将介绍如何在硬盘上存储决策树分类器。

# 3.3.2 使用算法：决策树的存储

构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树。为了解决这个问题，需要使用Python模块pickle序列化对象，参见程序清单3-9。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。任何对象都可以执行序列化操作，字典对象也不例外。

# 程序清单3-9 使用pickle模块存储决策树

```python
def storeTree(inputTree, filename):
    import pickle
    fw = open(filename, 'w')
    pickle.dump(inputTree, fw)
    fw.close()
def grabTree(filename):
    import pickle
    fr = open(filename)
    return pickle.load(fn) 
```

在Python命令提示符中输入下列命令验证上述代码的效果：

```txt
>>> trees/storeTree(myTree, 'classifierStorage.txt')
>>> treesgrabTree('classifierStorage.txt')
{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}) 
```

通过上面的代码，我们可以将分类器存储在硬盘上，而不用每次对数据分类时重新学习一遍，这也是决策树的优点之一，像第2章介绍了 $k$ -近邻算法就无法持久化分类器。我们可以预先提炼并存储数据集中包含的知识信息，在需要对事物进行分类时再使用这些知识。下节我们将使用这些工具处理隐形眼镜数据集。

# 3.4 示例：使用决策树预测隐形眼镜类型

本节我们将通过一个例子讲解决策树如何预测患者需要佩戴的隐形眼镜类型。使用小数据集，我们就可以利用决策树学到很多知识：眼科医生是如何判断患者需要佩戴的镜片类型；一旦理解了决策树的工作原理，我们甚至也可以帮助人们判断需要佩戴的镜片类型。

# 示例：使用决策树预测隐形眼镜类型

(1)收集数据：提供的文本文件。  
(2) 准备数据：解析tab键分隔的数据行。  
(3) 分析数据：快速检查数据，确保正确地解析数据内容，使用createPlot()函数绘制最终的树形图。

(4) 训练算法：使用3.1节的createTree()函数。  
(5) 测试算法：编写测试函数验证决策树可以正确分类给定的数据实例。  
(6) 使用算法：存储树的数据结构，以便下次使用时无需重新构造树。

隐形眼镜数据集①是非常著名的数据集，它包含很多患者眼部状况的观察条件以及医生推荐的隐形眼镜类型。隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。数据来源于UCI数据库，为了更容易显示数据，本书对数据做了简单的更改，数据存储在源代码下载路径的文本文件中。

可以在Python命令提示符中输入下列命令加载数据：

```python
>>> fr = open('lenses.txt')
>>> lenses = [inst.strip().split("\\t") for inst in fr.readlines())
>>> lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']
>>> lensesTree = trees.createTree(lenses, lensesLabels)
>>> lensesTree
{'tearRate': {'reduced': 'no lenses', 'normal': {'astigmatic': {'yes': {'prescript': {'hyper': {'age': {'pre': 'no lenses', 'presbyopic': 'no lenses', 'young': 'hard'}}], 'myope': 'hard'}]}, 'no': {'age': {'pre': 'soft', 'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}}], 'young': 'soft'}}}}
>>> treePlotter.createPlot(lensesTree) 
```

采用文本方式很难分辨出决策树的模样，最后一行命令调用createPlot()函数绘制了如图3-8所示的树形图。沿着决策树的不同分支，我们可以得到不同患者需要佩戴的隐形眼镜类型。从图3-8上我们也可以发现，医生最多需要问四个问题就能确定患者需要佩戴哪种类型的隐形眼镜。

![](images/08171dab9ee64a28e3691358412b6f67f2a1c52fe35388dd1cfc42ace88e64ac.jpg)  
图3-8 由ID3算法产生的决策树

图3-8所示的决策树非常好地匹配了实验数据，然而这些匹配选项可能太多了。我们将这种问题称之为过度匹配（overfitting）。为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点。如果叶子节点只能增加少许信息，则可以删除该节点，将它并入到其他叶子节点中。第9章将进一步讨论这个问题。

第9章将学习另一个决策树构造算法CART，本章使用的算法称为ID3，它是一个好的算法但并不完美。ID3算法无法直接处理数值型数据，尽管我们可以通过量化的方法将数值型数据转化为标称型数值，但是如果存在太多的特征划分，ID3算法仍然会面临其他问题。

# 3.5 本章小结

决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。ID3算法可以用于划分标称型数据集。构建决策树时，我们通常采用递归的方法将数据集转化为决策树。一般我们并不构造新的数据结构，而是使用Python语言内嵌的数据结构字典存储树节点信息。

使用Matplotlib的注解功能，我们可以将存储的树结构转化为容易理解的图形。Python语言的pickle模块可用于存储决策树的结构。隐形眼镜的例子表明决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题。我们可以通过裁剪决策树，合并相邻的无法产生大量信息增益的叶节点，消除过度匹配问题。

还有其他的决策树的构造算法，最流行的是C4.5和CART，第9章讨论回归问题时将介绍CART算法。

本书第2章、第3章讨论的是结果确定的分类算法，数据实例最终会被明确划分到某个分类中。下一章我们讨论的分类算法将不能完全确定数据实例应该划分到某个分类，或者只能给出数据实例属于给定分类的概率。

# 第4章

# 基于概率论的分类方法：朴素贝叶斯

# 本章内容

□ 使用概率分布进行分类  
□学习朴素贝叶斯分类器  
解析RSS源数据  
□ 使用朴素贝叶斯来分析不同地区的态度

前两章我们要求分类器做出艰难决策，给出“该数据实例属于哪一类”这类问题的明确答案。不过，分类器有时会产生错误结果，这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。

概率论是许多机器学习算法的基础，所以深刻理解这一主题就显得十分重要。第3章在计算特征值取某个值的概率时涉及了一些概率知识，在那里我们先统计特征在数据集中取某个特定值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。我们将在此基础上深入讨论。

本章会给出一些使用概率论进行分类的方法。首先从一个最简单的概率分类器开始，然后给出一些假设来学习朴素贝叶斯分类器。我们称之为“朴素”，是因为整个形式化过程只做最原始、最简单的假设。不必担心，你会详细了解到这些假设。我们将充分利用Python的文本处理能力将文档切分成词向量，然后利用词向量对文档进行分类。我们还将构建另一个分类器，观察其在真实的垃圾邮件数据集中的过滤效果，必要时还会回顾一下条件概率。最后，我们将介绍如何从个人发布的大量广告中学习分类器，并将学习结果转换成人类可理解的信息。

# 4.1 基于贝叶斯决策理论的分类方法

# 朴素贝叶斯

![](images/f117190cca5a45d80faeef0c9100e06d04e6479f92867e21f58dd5690d033b2d.jpg)

优点：在数据较少的情况下仍然有效，可以处理多类别问题。

缺点：对于输入数据的准备方式较为敏感。

适用数据类型：标称型数据。

朴素贝叶斯是贝叶斯决策理论的一部分，所以讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。

假设现在我们有一个数据集，它由两类数据组成，数据分布如图4-1所示。

![](images/62c8cf3042644c1d7ee9dc0f05ed1ccfd95128e5fffd8f2b32032382a2684cb5.jpg)  
图4-1 两个参数已知的概率分布，参数决定了分布的形状

假设有位读者找到了描述图中两类数据的统计参数。（暂且不用管如何找到描述这类数据的统计参数，第10章会详细介绍。）我们现在用p1(x,y)表示数据点(x,y)属于类别1（图中用圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2（图中用三角形表示的类别）的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：

□如果 $\mathrm{p1(x,y) > p2(x,y)}$ ，那么类别为1。  
□如果 $\mathsf{p2}(\mathbf{x},\mathbf{y}) > \mathsf{p1}(\mathbf{x},\mathbf{y})$ ，那么类别为2。

也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。回到图4-1，如果该图中的整个数据使用6个浮点数①来表示，并且计算类别概率的Python代码只有两行，那么你会更倾向于使用下面哪种方法来对该数据点进行分类？

(1) 使用第1章的kNN，进行1000次距离计算；  
(2) 使用第2章的决策树，分别沿x轴、y轴划分数据；  
(3)计算数据点属于每个类别的概率，并进行比较。

使用决策树不会非常成功；而和简单的概率计算相比，kNN的计算量太大。因此，对于上述问题，最佳选择是使用刚才提到的概率比较方法。

接下来，我们必须要详述p1及p1概率计算方法。为了能够计算p1与p2，有必要讨论一下条件概率。如果你觉得自己已经相当了解条件概率了，那么可以直接跳过下一节。

# 贝叶斯？

这里使用的概率解释属于贝叶斯概率理论的范畴，该理论非常流行且效果良好。贝叶斯概率以18世纪的一位神学家托马斯·贝叶斯（Thomas Bayes）的名字命名。贝叶斯概率引入先验知识和逻辑推理来处理不确定命题。另一种概率解释称为频数概率（frequency probability），它只从数据本身获得结论，并不考虑逻辑推理及先验知识。

# 4.2 条件概率

接下来花点时间讲讲概率与条件概率。如果你对 $p(x, y \mid c_1)$ 符号很熟悉，那么可以跳过本节。

假设现在有一个装了7块石头的罐子，其中3块是灰色的，4块是黑色的（如图4-2所示）。如果从罐子中随机取出一块石头，那么是灰色石头的可能性是多少？由于取石头有7种可能，其中3种为灰色，所以取出灰色石头的概率为 $3 / 7$ 。那么取到黑色石头的概率又是多少呢？很显然，是4/7。我们使用P(gray)来表示取到灰色石头的概率，其概率值可以通过灰色石头数目除以总的石头数目来得到。

![](images/827b9b197b0eb73646eb000a3be175650af6731232ab879c3c1fb43e9e1dfcec.jpg)  
图4-2 一个包含7块石头的集合，石头的颜色为灰色或者黑色。如果随机从中取一块石头，那么取到灰色石头的概率为3/7。类似地，取到黑色石头的概率为4/7

如果这7块石头如图4-3所示放在两个桶中，那么上述概率应该如何计算？

![](images/ec1218ec1e615712783b89457b08620dc1bd8909841ac01b73ba4305ae5596f5.jpg)

![](images/d6d35b64e0b4576b7e635ce9f6247f15d19aa3b687ae45f1c280938353ffa3d0.jpg)  
图4-3 落到两个桶中的7块石头

要计算P(gray)或者P(black)，事先得知道石头所在桶的信息会不会改变结果？你有可能已经想到计算从B桶中取到灰色石头的概率的办法，这就是所谓的条件概率（conditional probability）。假定计算的是从B桶取到灰色石头的概率，这个概率可以记作P(gray|bucketB)，我们称之为“在已知石头出自B桶的条件下，取出灰色石头的概率”。不难得到，P(gray|bucketA)值为2/4，P(gray|bucketB)的值为1/3。

条件概率的计算公式如下所示：

$$
P (\text {g r a y} \mid \text {b u c k e t B}) = P (\text {g r a y} \text {a n d} \text {b u c k e t B}) / P (\text {b u c k e t B})
$$

我们来看看上述公式是否合理。首先，用B桶中灰色石头的个数除以两个桶中总的石头数，得到P(gray and bucketB) = 1/7。其次，由于B桶中有3块石头，而总石头数为7，于是P(bucketB)就等于3/7。于是有P(gray|bucketB) = P(gray and bucketB)/P(bucketB) = (1/7) / (3/7) = 1/3。这个公式虽然对于这个简单例子来说有点复杂，但当存在更多特征时是非常有效的。用代数方法计算条件概率时，该公式也很有用。

另一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 $\mathrm{P}(\mathbf{x}|\mathbf{c})$ ，要求 $\mathrm{P}(\mathbf{c}|\mathbf{x})$ ，那么可以使用下面的计算方法：

$$
p (c \mid x) = \frac {p (x \mid c) p (c)}{p (x)}
$$

我们讨论了条件概率，接下来的问题是如何将其应用到分类器中。下一节将讨论如何结合贝叶斯决策理论使用条件概率。

# 4.3 使用条件概率来分类

4.1节提到贝叶斯决策理论要求计算两个概率p1(x，y)和p2(x，y)：

□如果p1(x，y）>p2(x，y)，那么属于类别1；  
□如果p2(x，y）>p1(x，y)，那么属于类别2。

但这两个准则并不是贝叶斯决策理论的所有内容。使用p1()和p2()只是为了尽可能简化描述，而真正需要计算和比较的是 $p(c_1|x, y)$ 和 $p(c_2|x, y)$ 。这些符号所代表的具体意义是：给定某个由x、y表示的数据点，那么该数据点来自类别c的概率是多少？数据点来自类别c的概率又是多少？注意这些概率与刚才给出的概率 $p(x, y|c)$ 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到：

$$
p (c _ {i} | x, y) = \frac {p (x , y | c _ {i}) p (c _ {i})}{p (x , y)}
$$

使用这些定义，可以定义贝叶斯分类准则为：

□如果 $\mathrm{P}(c_1|\mathbf{x},\mathbf{y}) > \mathrm{P}(c_2|\mathbf{x},\mathbf{y})$ ，那么属于类别 $c_{1}$ 。  
□如果 $\mathrm{P}(\mathsf{c}_1|\mathsf{x},\mathsf{y}) <   \mathrm{P}(\mathsf{c}_2|\mathsf{x},\mathsf{y})$ ，那么属于类别 $C_2$ 。

使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。后面就会给出利用贝叶斯准则来计算概率并对数据进行分类的代码。现在介绍了一些概率理论，你也了解了基于这些理

论构建分类器的方法，接下来就要将它们付诸实践。下一节会介绍一个简单但功能强大的贝叶斯分类器的应用案例。

# 4.4 使用朴素贝叶斯进行文档分类

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。虽然电子邮件是一种会不断增加的文本，但我们同样也可以对新闻报道、用户留言、政府公文等其他任意类型的文本进行分类。我们可以观察文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是上节介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。

使用每个词作为特征并观察它们是否出现，这样得到的特征数目会有多少呢？针对的是哪一种人类语言呢？当然不止一种语言。据估计，仅在英语中，单词的总数就有 $500000^{1}$ 之多。为了能进行英文阅读，估计需要掌握数千单词。

# 朴素贝叶斯的一般过程

(1)收集数据：可以使用任何方法。本章使用RSS源。  
(2) 准备数据：需要数值型或者布尔型数据。  
(3) 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。  
(4) 训练算法：计算不同的独立特征的条件概率。  
(5) 测试算法：计算错误率。  
(6) 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。

假设词汇表中有1000个单词。要得到好的概率分布，就需要足够的数据样本，假定样本数为 $N$ 。前面讲到的约会网站示例中有1000个实例，手写识别示例中每个数字有200个样本，而决策树示例中有24个样本。其中，24个样本有点少，200个样本好一些，而1000个样本就非常好了。约会网站例子中有三个特征。由统计学知，如果每个特征需要 $N$ 个样本，那么对于10个特征将需要 $N^{10}$ 个样本，对于包含1000个特征的词汇表将需要 $N^{1000}$ 个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。

如果特征之间相互独立，那么样本数就可以从 $N^{1000}$ 减少到 $1000 \times N$ 。所谓独立（independence）指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。举个例子讲，假设单词bacon出现在unhealthy后面与出现在delicious后面的概率相同。当然，我们知道这种假设并不正确，bacon常常出现在delicious附近，而很少出现在unhealthy附近，这个假设正是朴素贝叶斯分类器中朴素（naive）一词的含义。朴素贝叶斯分类器中的另一个假设是，每个特

征同等重要①。其实这个假设也有问题。如果要判断留言板的留言是否得当，那么可能不需要看完所有的1000个单词，而只需要看 $10\sim 20$ 个特征就足以做出判断了。尽管上述假设存在一些小的瑕疵，但朴素贝叶斯的实际效果却很好。

到目前为止，你已经了解了足够的知识，可以开始编写代码了。如果还不清楚，那么了解代码的实际效果会有助于理解。下一节将使用Python来实现朴素贝叶斯分类器，实现中会涉及利用Python进行文本分类的所有相关内容。

# 4.5 使用Python进行文本分类

要从文本中获取特征，需要先拆分文本。具体如何做呢？这里的特征是来自文本的词条（token），一个词条是字符的任意组合。可以把词条想象为单词，也可以使用非单词词条，如URL、IP地址或者任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档中，0表示词条未出现。

以在线社区的留言板为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类别：侮辱类和非侮辱类，使用1和0分别表示。

接下来首先给出将文本转换为数字向量的过程，然后介绍如何基于这些向量来计算条件概率，并在此基础上构建分类器，最后还要介绍一些利用Python实现朴素贝叶斯过程中需要考虑的问题。

# 4.5.1 准备数据：从文本中构建词向量

我们将把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。接下来我们正式开始。打开文本编辑器，创建一个叫bayes.py的新文件，然后将下面的程序清单添加到文件中。

# 程序清单4-1 词表到向量的转换函数

```python
def loadDataSet():
 postingList = [
['my', 'dog', 'has', 'flea', \]
    'problems', 'help', 'please'],
    ['maybe', 'not', 'take', 'him', \]
    'to', 'dog', 'park', 'stupid'],
    ['my', 'dalmation', 'is', 'so', 'cute', \]
    'I', 'love', 'him'],
    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
    ['mr', 'licks', 'ate', 'my', 'steak', 'how', \]
    'to', 'stop', 'him'],
    ['my', 'dog', 'has', 'flea', \]
    'problems', 'help', 'please'],
    ['maybe', 'not', 'take', 'him', \]
    'to', 'dog', 'park', 'stupid'],
    ['my', 'dalmation', 'is', 'so', 'cute', \]
    'I', 'love', 'him'],
    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
    ['mr', 'licks', 'ate', 'my', "steak", "how", \ 
```

['quit'，'buying'，'worthless'，'dog'，'food'，'stupid']classVec $= [0,1,0,1,0,1]$ #1代表侮辱性文字，0代表正常言论return postingList,classVec   
def createVocabList(dataSet): 创建一个空集vocabSet $\equiv$ set([]）for document in dataSet:vocabSet $\equiv$ vocabSet|set(document)return list(vocabSet) 2创建两个集合的并集   
def setOfWords2Vec(vocabList，inputSet):returnVec $= [0]*$ len(vocabList)创建一个其中所含元素都为0的向量for word in inputSet: 3 ifword in vocabList:returnVec[vocabList.index(word)] $= 1$ else:print"the word:%s is not in my Vocabulary!"%wordreturn returnVec

第一个函数loadDataSet()创建了一些实验样本。该函数返回的第一个变量是进行词条切分后的文档集合，这些文档来自斑点犬爱好者留言板。这些留言文本被切分成一系列的词条集合，标点符号从文本中去掉，后面会探讨文本处理的细节。loadDataSet()函数返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性。这些文本的类别由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。

下一个函数createVocabList()会创建一个包含在所有文档中出现的不重复词的列表，为此使用了Python的set数据类型。将词条列表输给set构造函数，set就会返回一个不重复词表。首先，创建一个空集合①，然后将每篇文档返回的新词集合添加到该集合中②。操作符|用于求两个集合的并集，这也是一个按位或（OR）操作符（参见附录C）。在数学符号表示上，按位或操作与集合求并操作使用相同记号。

获得词汇表后，便可以使用函数setOfWords2Vec()，该函数的输入参数为词汇表及某个文档，输出的是文档向量，向量的每一元素为1或0，分别表示词汇表中的单词在输入文档中是否出现。函数首先创建一个和词汇表等长的向量，并将其元素都设置为0。接着，遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1。一切都顺利的话，就不需要检查某个词是否还在vocabList中，后边可能会用到这一操作。

现在看一下这些函数的执行效果，保存bayes.py文件，然后在Python提示符下输入：

```python
>>> import bayes
>>> listOPosts, listOfClasses = bayes.loadDataSet()
>>> myVocabList = bayes.createVocabList(listOPosts)
>>> myVocabList
['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my'] 
```

检查上述词表，就会发现这里不会出现重复的单词。目前该词表还没有排序，需要的话，稍后可以对其排序。

下面看一下函数setOfWords2Vec()的运行效果：

```txt
>>> bayes.setOfWords2Vec(myVocabList, listOPosts[0])
[ \begin{array}{l} 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, \\ 0, 0, 0, 0, 0, 0, 1 \end{array} ]
>>> bayes.setOfWords2Vec(myVocabList, listOPosts[3])
[ \begin{array}{l} 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, \\ 0, 1, 0, 0, 0, 0, 0 \end{array} ] 
```

该函数使用词汇表或者想要检查的所有单词作为输入，然后为其中每一个单词构建一个特征。一旦给定一篇文档（斑点犬网站上的一条留言），该文档就会被转换为词向量。接下来检查一下函数的有效性。myVocabList中索引为2的元素是什么单词？应该是单词help。该单词在第一篇文档中出现，现在检查一下看看它是否出现在第四篇文档中。

# 4.5.2 训练算法：从词向量计算概率

前面介绍了如何将一组单词转换为一组数字，接下来看看如何使用这些数字计算概率。现在已经知道一个词是否出现在一篇文档中，也知道该文档所属的类别。还记得3.2节提到的贝叶斯准则？我们重写贝叶斯准则，将之前的x、y替换为w。粗体w表示这是一个向量，即它由多个数值组成。在这个例子中，数值个数与词汇表中的词个数相同。

$$
p (c _ {i} \mid w) = \frac {p (w \mid c _ {i}) p (c _ {i})}{p (w)}
$$

我们将使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。如何计算呢？首先可以通过类别i（侮辱性留言或非侮辱性留言）中文档数除以总的文档数来计算概率p(c)。接下来计算p(w|c)，这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作p(w0,w1,w2..wN|c)。这里假设所有词都互相独立，该假设也称作条件独立性假设，它意味着可以使用p(w0|c)p(w1|c)p(w2|c)....p(wN|c)来计算上述概率，这就极大地简化了计算的过程。

该函数的伪代码如下：

计算每个类别中的文档数目

对每篇训练文档：

对每个类别：

如果词条出现文档中 $\rightarrow$ 增加该词条的计数值

增加所有词条的计数值

对每个类别：

对每个词条：

将该词条的数目除以总词条数目得到条件概率

返回每个类别的条件概率

我们利用下面的代码来实现上述伪码。打开文本编辑器，将这些代码添加到bayes.py文件中。该函数使用了NumPy的一些函数，故应确保将from numpy import *语句添加到bayes.py文件的最前面。

程序清单4-2 朴素贝叶斯分类器训练函数  
```python
def trainNBO(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = zeros(numWords); p1Num = zeros(numWords)
    p0Denom = 0.0; p1Denom = 0.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
        p1Vect = p1Num/p1Denom #change to log()
        p0Vect = p0Num/p0Denom #change to log()
    return p0Vect, p1Vect, pAbusive 
```

初始化概率

向量相加

对每个元素做除法

代码函数中的输入参数为文档矩阵trainMatrix，以及由每篇文档类别标签所构成的向量trainCategory。首先，计算文档属于侮辱性文档（class=1）的概率，即P(1)。因为这是一个二类分类问题，所以可以通过1-P(1)得到P(0)。对于多于两类的分类问题，则需要对代码稍加修改。

计算 $p(w_i | c_1)$ 和 $p(w_i | c_0)$ ，需要初始化程序中的分子变量和分母变量①。由于中元素如此众多，因此可以使用NumPy数组快速计算这些值。上述程序中的分母变量是一个元素个数等于词汇表大小的NumPy数组。在for循环中，要遍历训练集trainMatrix中的所有文档。一旦某个词语（侮辱性或正常词语）在某一文档中出现，则该词对应的个数（p1Num或者p0Num）就加1，而且在所有的文档中，该文档的总词数也相应加1②。对于两个类别都要进行同样的计算处理。

最后，对每个元素除以该类别中的总词数③。利用NumPy可以很好实现，用一个数组除以浮点数即可，若使用常规的Python列表则难以完成这种任务，读者可以自己尝试一下。最后，函数会返回两个向量和一个概率。

接下来试验一下。将程序清单4-2中的代码添加到bayes.py文件中，在Python提示符下输入：

```python
>>> from numpy import *
>>> reload(bayes)
<module 'bayes' from 'bayes.py>
>>> listOPosts, listOfClasses = bayes.loadDataSet() 
```

该语句从预先加载值中调入数据。

```txt
>>> myVocabList = bayes.createVocabList(listOPosts) 
```

至此我们构建了一个包含所有词的列表myVocabList。

```txt
>>> trainMat = []
>>> for postinDoc in listOPosts:
...     trainMat.append(bayes.setOfWords2Vec(myVocabList, postinDoc)) 
```

该for循环使用词向量来填充trainMat列表。下面给出属于侮辱性文档的概率以及两个类别的概率向量。

```txt
>>> p0V, p1V, pAb = bayes.trainNBO(trainMat, listOfClasses) 
```

接下来看这些变量的内部值：

```html
>> pAb 0.5 
```

这就是任意文档属于侮辱性文档的概率。

```csv
>>p0V   
array([ 0.04166667, 0.04166667, 0.04166667, 0. , 0. , 0.04166667, 0.04166667, 0.125 0.04166667, 0.125 ])   
>>p1V   
array([ 0. , 0. , 0. , 0.05263158, 0.05263158, 0.15789474, 0. , 0.05263158, 0. , 0. , 0.]) 
```

首先，我们发现文档属于侮辱类的概率pAb为0.5，该值是正确的。接下来，看一看在给定文档类别条件下词汇表中单词的出现概率，看看是否正确。词汇表中的第一个词是cute，其在类别0中出现1次，而在类别1中从未出现。对应的条件概率分别为0.04166667与0.0。该计算是正确的。我们找找所有概率中的最大值，该值出现在P(1)数组第21个下标位置，大小为0.15789474。在myVocabList的第26个下标位置上可以查到该单词是stupid。这意味着stupid是最能表征类别1（侮辱性文档类）的单词。

使用该函数进行分类之前，还需解决函数中的一些缺陷。

# 4.5.3 测试算法：根据现实情况修改分类器

利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 $p(w_0|1)p(w_1|1)p(w_2|1)$ 。如果其中一个概率值为0，那么最后的乘积也为0。为降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。

在文本编辑器中打开bayes.py文件，并将trainNBO()的第4行和第5行修改为：

```txt
p0Num = ones(numWords); p1Num = ones(numWords)  
p0Denom = 2.0; p1Denom = 2.0 
```

另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 $p(w_0 | c_i) p(w_1 | c_i) p(w_2 | c_i) \ldots p(w_N | c_i)$ 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（读者可以用Python尝试相乘许多很小的数，最后四舍五入后会得到0。）一种解决办法是对乘积取自然对数。在代数中有 $\ln(a * b) = \ln(a) + \ln(b)$ ，于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。图4-4给出函数 $f(x)$ 与 $\ln(f(x))$ 的曲线。检查这两条曲线，就会发现它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。通过修改return前的两行代码，将上述做法用到分类器中：

```txt
p1Vect = log(p1Num/p1Denom)  
p0Vect = log(p0Num/p0Denom) 
```

![](images/d78b1ac0523251ec230b0c9ebaf841ddbf1243438344cd1e69f031e9e264a5c8.jpg)

![](images/3d9fe0a9f1d847360a952b140af8a06c70fbd2ea40d89aab6733836417c0a71c.jpg)  
图4-4 函数 $f(x)$ 与 $\ln(f(x))$ 会一块增大。这表明想求函数的最大值时，可以使用该函数的自然对数来替换原函数进行求解

现在已经准备好构建完整的分类器了。当使用NumPy向量处理功能时，这一切变得十分简单。打开文本编辑器，将下面的代码添加到bayes.py中：

# 程序清单4-3 朴素贝叶斯分类函数

```python
def classifyNB vec2Classify, p0Vec, plVec, pClass1):
    p1 = sum(doc2Classify * p1Vec) + log(pClass1)
    p0 = sum(doc2Classify * p0Vec) + log(1.0 - pClass1)
    if p1 > p0:
        return 1
    else:
        return 0
def testingNB():
    listOfPosts, listOfClasses = loadDataSet()
    myVocabList = createVocabList(listOPosts)
    trainMat = []
    for postinDoc in listOfPosts:
        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))
    p0V, p1V, pAb = trainNBO(array(trainMat), array(listClasses))
    testEntry = ['love', 'my', 'dalmation']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb)
    testEntry = ['stupid', 'garbage']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb) 
```

程序清单4-3的代码有4个输入：要分类的向量vec2Classify以及使用函数trainNBO()计

算得到的三个概率。使用NumPy的数组来计算两个向量相乘的结果①。这里的相乘是指对应元素相乘，即先将两个向量中的第1个元素相乘，然后将第2个元素相乘，以此类推。接下来将词汇表中所有词的对应值相加，然后将该值加到类别的对数概率上。最后，比较类别的概率返回大概率对应的类别标签。这一切不是很难，对吧？

代码的第二个函数是一个便利函数（convenience function），该函数封装所有操作，以节省输入4.3.1节中代码的时间。

下面来看看实际结果。将程序清单4-3中的代码添加之后，在Python提示符下输入：

```txt
>>> reload(bayes)
<module 'bayes' from 'bayes.pyc>
>>> bayes/testingNB()
['love', 'my', 'dalmation'] classified as: 0
['stupid', 'garbage'] classified as: 1 
```

对文本做一些修改，看看分类器会输出什么结果。这个例子非常简单，但是它展示了朴素贝叶斯分类器的工作原理。接下来，我们会对代码做些修改，使分类器工作得更好。

# 4.5.4 准备数据：文档词袋模型

目前为止，我们将每个词的出现与否作为一个特征，这可以被描述为词集模型（set-of-words model）。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为词袋模型（bag-of-words model）。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数setOfWords2Vec()稍加修改，修改后的函数称为bagOfWords2Vec()。

下面的程序清单给出了基于词袋模型的朴素贝叶斯代码。它与函数setOfWords2Vec()几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为1。

# 程序清单4-4 朴素贝叶斯词袋模型

```python
def bagOfWords2VecMN(vocabList, inputSet):
    returnVec = [0] * len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[ VOCalist.index(word)] += 1
    return returnVec 
```

现在分类器已经构建好了，下面我们将利用该分类器来过滤垃圾邮件。

# 4.6 示例：使用朴素贝叶斯过滤垃圾邮件

在前面那个简单的例子中，我们引入了字符串列表。使用朴素贝叶斯解决一些现实生活中的问题时，需要先从文本内容得到字符串列表，然后生成词向量。下面这个例子中，我们将了解朴素贝叶斯的一个最著名的应用：电子邮件垃圾过滤。首先看一下如何使用通用框架来解决该问题。

# 示例：使用朴素贝叶斯对电子邮件进行分类

(1)收集数据：提供文本文件。  
(2) 准备数据：将文本文件解析成词条向量。  
(3) 分析数据：检查词条确保解析的正确性。  
(4) 训练算法：使用我们之前建立的trainNBO()函数。  
(5)测试算法：使用classifyNB()，并且构建一个新的测试函数来计算文档集的错误率。  
(6) 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。

下面首先给出将文本解析为词条的代码。然后将该代码和前面的分类代码集成为一个函数，该函数在测试分类器的同时会给出错误率。

# 4.6.1 准备数据：切分文本

前一节介绍了如何创建词向量，并基于这些词向量进行朴素贝叶斯分类的过程。前一节中的词向量是预先给定的，下面介绍如何从文本文档中构建自己的词列表。

对于一个文本字符串，可以使用Python的string.split()方法将其切分。下面看看实际的运行效果。在Python提示符下输入：

>>> mySent='This book is the best book on Python or M.L. I have ever laid $\Leftrightarrow$ eyes upon.'   
>>> mySent.split() ['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']

可以看到，切分的结果不错，但是标点符号也被当成了词的一部分。可以使用正则表示式来切分句子，其中分隔符是除单词、数字外的任意字符串。

```coffeescript
>>> import re
>>> regEx = re.compile("\\W*")
>>> listOfTokens = regEx.split(mySent)
>>> listOfTokens
['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', '', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '') 
```

现在得到了一系列词组成的词表，但是里面的空字符串需要去掉。可以计算每个字符串的长度，只返回长度大于0的字符串。

```txt
>>> [tok for tok in listOfTokens if len(tok) > 0] 
```

最后，我们发现句子中的第一个单词是大写的。如果目的是句子查找，那么这个特点会很有用。但这里的文本只看成词袋，所以我们希望所有词的形式都是统一的，不论它们出现在句子中间、结尾还是开头。

Python中有一些内嵌的方法，可以将字符串全部转换成小写（.lower()）或者大写（.upper()），借助这些方法可以达到目的。于是，可以进行如下处理：

```txt
>>> [tok(lower()) for tok in listOfTokens if len(tok) > 0] ['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 
```

```javascript
'1', 'i', 'have', 'ever', 'laid', 'eyes', 'upon'] 
```

现在来看数据集中一封完整的电子邮件的实际处理结果。该数据集放在email文件夹中，该文件夹又包含两个子文件夹，分别是spam与ham。

>>>emailText $\equiv$ open('email/ham/6.txt').read()   
>>> listOfTokens $\equiv$ regEx.split(emailText)

文件夹ham下的6.txt文件非常长，这是某公司告知我他们不再进行某些支持的一封邮件。需要注意的是，由于是URL：answer.py?hl=en&answer=174623的一部分，因而会出现en和py这样的单词。当对URL进行切分时，会得到很多的词。我们是想去掉这些单词，因此在实现时会过滤掉长度小于3的字符串。本例使用一个通用的文本解析规则来实现这一点。在实际的解析程序中，要用更高级的过滤器来对诸如HTML和URI的对象进行处理。目前，一个URI最终会解析成词汇表中的单词，比如www.whitehouse.gov会被解析为三个单词。文本解析可能是一个相当复杂的过程。接下来将构建一个极其简单的函数，你可以根据情况自行修改。

# 4.6.2 测试算法：使用朴素贝叶斯进行交叉验证

下面将文本解析器集成到一个完整分类器中。打开文本编辑器，将下面程序清单中的代码添加到bayes.py文件中。

程序清单4-5 文件解析及完整的垃圾邮件测试函数  
```python
def textParse(longString):
    import re
    listOfTokens = re.split(r'[W*', bigString)
    return [tok.lower() for tok in listOfTokens if len(tok) > 2]
def spamTest():
    docList = []
    classList = []
    fullText = []
    for i in range(1,26):
        wordList = textParse(open('email/spam/%d.txt' % i).read())
        docList.append(wordList)
       FULLText.append(wordList)
        classList.append(1)
        wordList = textParse(open('email/ham/%d.txt' % i).read())
        docList.append(wordList)
        fullText.append(wordList)
        classList.append(0)
    vocabList = createVocabList(docList)
    trainingSet = range(50); testSet = []
    for i in range(10):
        randIndex = int(random.uniform(0, len(trainingSet)))
        testSet.append(trainingSet[randIndex])
        del(trainingSet[randIndex])
    trainMat = []
    for docIndex in trainingSet:
        trainMat.append(setOfWords2Vec(vocabList, docList[qocIndex]))
        trainClasses.append(classList[qocIndex])
    p0V, p1V, pSpam = trainNBO(array(trainMat), array(trainClasses))
    errorCount = 0 
```

```python
for docIndex in testSet:  
    wordVector = setText2Vec(vocabList, docList(docIndex))  
    if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList(docIndex):  
        errorCount += 1  
print 'the error rate is: ', float(errorCount) / len(testSet) 
```

第一个函数textParse()接受一个大字符串并将其解析为字符串列表。该函数去掉少于两个字符的字符串，并将所有字符串转换为小写。你可以在函数中添加更多的解析操作，但是目前的实现对于我们的应用足够了。

第二个函数 spamTest() 对贝叶斯垃圾邮件分类器进行自动化处理。导入文件夹 spam与ham 下的文本文件，并将它们解析为词列表①。接下来构建一个测试集与一个训练集，两个集合中的邮件都是随机选出的。本例中共有 50 封电子邮件，并不是很多，其中的 10 封电子邮件被随机选择为测试集。分类器所需要的概率计算只利用训练集中的文档来完成。Python 变量 trainingSet 是一个整数列表，其中的值从 0 到 49。接下来，随机选择其中 10 个文件②。选择出的数字所对应的文档被添加到测试集，同时也将其从训练集中剔除。这种随机选择数据的一部分作为训练集，而剩余部分作为测试集的过程称为留存交叉验证（hold-out cross validation）。假定现在只完成了一次迭代，那么为了更精确地估计分类器的错误率，就应该进行多次迭代后求出平均错误率。

接下来的for循环遍历训练集的所有文档，对每封邮件基于词汇表并使用setOfWords2Vec()函数来构建词向量。这些词在trainNB0()函数中用于计算分类所需的概率。然后遍历测试集，对其中每封电子邮件进行分类③。如果邮件分类错误，则错误数加1，最后给出总的错误百分比。

下面对上述过程进行尝试。输入程序清单4-5的代码之后，在Python提示符下输入：

```txt
>>> bayes.spamTest()
the error rate is: 0.0
>>> bayes.spamTest()
classification error ['home', 'based', 'business', 'opportunity',
'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance',
'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your',
'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your',
'success', 'work', 'from', 'home', 'finder', 'experts']
the error rate is: 0.1 
```

函数spamTest()会输出在10封随机选择的电子邮件上的分类错误率。既然这些电子邮件是随机选择的，所以每次的输出结果可能有些差别。如果发现错误的话，函数会输出错分文档的词表，这样就可以了解到底是哪篇文档发生了错误。如果想要更好地估计错误率，那么就应该将上述过程重复多次，比如说10次，然后求平均值。我这么做了一下，获得的平均错误率为 $6\%$ 。

这里一直出现的错误是将垃圾邮件误判为正常邮件。相比之下，将垃圾邮件误判为正常邮件要比将正常邮件归到垃圾邮件好。为避免错误，有多种方式可以用来修正分类器，这些将在第7章中进行讨论。

目前我们已经使用朴素贝叶斯来对文档进行分类，接下来将介绍它的另一个应用。下一个例子还会给出如何解释朴素贝叶斯分类器训练所得到的知识。

# 4.7 示例：使用朴素贝叶斯分类器从个人广告中获取区域倾向

本章的最后一个例子非常有趣。我们前面介绍了朴素贝叶斯的两个实际应用的例子，第一个例子是过滤网站的恶意留言，第二个是过滤垃圾邮件。分类还有大量的其他应用。我曾经见过有人使用朴素贝叶斯从他喜欢及不喜欢的女性的社交网络档案学习相应的分类器，然后利用该分类器测试他是否会喜欢一个陌生女人。分类的可能应用确实有很多，比如有证据表示，人的年龄越大，他所用的词也越好。那么，可以基于一个人的用词来推测他的年龄吗？除了年龄之外，还能否推测其他方面？广告商往往想知道关于一个人的一些特定人口统计信息，以便能够更好地定向推销广告。从哪里可以获得这些训练数据呢？事实上，互联网上拥有大量的训练数据。几乎任一个能想到的利基市场①都有专业社区，很多人会认为自己属于该社区。4.3.1节中的斑点犬爱好者网站就是一个非常好的例子。

在这个最后的例子当中，我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的征婚广告信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实是不同，那么他们各自常用的词是哪些？从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解？

# 1.示例：使用朴素贝叶斯来发现地域相关的用词

(1)收集数据：从RSS源收集内容，这里需要对RSS源构建一个接口。  
(2) 准备数据：将文本文件解析成词条向量。  
(3) 分析数据：检查词条确保解析的正确性。  
(4) 训练算法：使用我们之前建立的trainNBO()函数。  
(5) 测试算法：观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果。  
(6) 使用算法：构建一个完整的程序，封装所有内容。给定两个RSS源，该程序会显示最常用的公共词。

下面将使用来自不同城市的广告训练一个分类器，然后观察分类器的效果。我们的目的并不是使用该分类器进行分类，而是通过观察单词和条件概率值来发现与特定城市相关的内容。

# 4.7.1 收集数据：导入 RSS 源

接下来要做的第一件事是使用Python下载文本。幸好，利用RSS，这些文本很容易得到。现在所需要的是一个RSS阅读器。Universal Feed Parser是Python中最常用的RSS程序库。

你可以在http://code.google.com/p/feedparser/下浏览相关文档，然后和其他Python包一样来安装feedparse。首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在Python提示符下敲入>>python setup.py install。

下面使用Craigslist上的个人广告，当然希望是在服务条款允许的条件下。打开Craigslist上的RSS源，在Python提示符下输入：

```txt
>>> import feedparser
>>> ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss') 
```

我决定使用Craigslist中比较纯洁的那部分内容，其他内容稍显少儿不宜。你可以查阅feedparser.org中出色的说明文档以及RSS源。要访问所有条目的列表，输入：

```html
>>>ny['entries']   
>>>len(ny['entries'])   
100 
```

可以构建一个类似于spamTest()的函数来对测试过程自动化。打开文本编辑器，输入下列程序清单中的代码。

程序清单4-6 RSS源分类器及高频词去除函数  
def calcMostFreq(vocabList,fullText): 计算出现频率import operatorfreqDict $= \{\}$ for token in vocabList:freqDict [token] $\equiv$ fullText.count(token)sortedFreq $\equiv$ sorted(freqDict.iterrItems(),key $\equiv$ operator.itemgetter(1),\reverse $\equiv$ True)return sortedFreq[:30]  
def localWordsfeed1,feed0): import feedparserdocList $\equiv$ []；classList $\equiv$ []；fullText $\equiv$ []minLen $\equiv$ min(len(feed1['entries'],len(feedo['entries']))for i in range(minLen): wordList $\equiv$ textParse(feed1['entries'])[i][summary]) docList.append(wordList)fullText.append(wordList) classList.append(1) wordList $\equiv$ textParse(feedo['entries'])[i][summary]) docList.append(wordList)fullText.append(wordList)classList.append(0)vocabList $\equiv$ createVocabList(docList)top30Words $\equiv$ calcMostFreq(vocabList,fullText)for pairW in top30Words: if pairW[0] in vocabList:vocabList.remove(pairW[0])trainingSet $\equiv$ range(2\*minLen); testSet $\equiv$ []for i in range(20): randIndex $\equiv$ int(random.uniform(0,len(trainingSet)))testSet.append(trainingSet[randIndex])del(trainingSet[randIndex])trainMat $= [\cdot ]$ ；trainClasses $= [\cdot ]$ for docIndex in trainingSet:

trainMat.appendbagOfWords2VecMN(vocabList，docList[docIndex])) trainClasses.append(classList[docIndex])   
p0V,p1V,pSpam $\equiv$ trainNBO(array(trainMat)，array(trainClasses))   
errorCount $= 0$ for docIndex in testSet: wordVector $\equiv$ bagOfWords2VecMN(vocabList，docList[docIndex]) if classifyNB(array(wordVector),p0V,p1V,pSpam）！ $=$ classList[docIndex]: errorCount $+ = 1$ print 'the error rate is:'float(errorCount)/len(testSet)   
return vocabList,p0V,p1V

上述代码类似程序清单4-5中的函数spamTest()，不过添加了新的功能。代码中引入了一个辅助函数calcMostFreq()①。该函数遍历词汇表中的每个词并统计它在文本中出现的次数，然后根据出现次数从高到低对词典进行排序，最后返回排序最高的100个单词。你很快就会明白这个函数的重要性。

下一个函数localWords()使用两个RSS源作为参数。RSS源要在函数外导入，这样做的原因是RSS源会随时间而改变。如果想通过改变代码来比较程序执行的差异，就应该使用相同的输入。重新加载RSS源就会得到新的数据，但很难确定是代码原因还是输入原因导致输出结果的改变。函数localWords()与程序清单4-5中的spamTest()函数几乎相同，区别在于这里访问的是RSS源而不是文件。然后调用函数calcMostFreq()来获得排序最高的100个单词并随后将它们移除①。函数的剩余部分与spamTest()基本类似，不同的是最后一行要返回下面要用到的值。

你可以注释掉用于移除高频词的三行代码，然后比较注释前后的分类性能。我自己也尝试了一下，去掉这几行代码之后，我发现错误率为 $54\%$ ，而保留这些代码得到的错误率为 $70\%$ 。这里观察到的一个有趣现象是，这些留言中出现次数最多的前30个词涵盖了所有用词的 $30\%$ 。我在进行测试的时候，vocabList的大小约为3000个词。也就是说，词汇表中的一小部分单词却占据了所有文本用词的一大部分。产生这种现象的原因是因为语言中大部分都是冗余和结构辅助性内容。另一个常用的方法是不仅移除高频词，同时从某个预定词表中移除结构上的辅助词。该词表称为停用词表（stop word list），目前可以找到许多停用词表（在本书写作期间，http://www.ranks.nl/resources/stopwords.html上有一个很好的多语言停用词列表）。

将程序清单4-6中的代码加入到bayes.py文件之后，可以通过输入如下命令在Python中进行测试：

```html
>>> reload(bayes)
<module 'bayes' from 'bayes.py>
>>> ny=feedparser.parse('http://newyork.craiglist.org/stp/index.css')
>>> sf=feedparser.parse('http://sfbay.craiglist.org/stp/index.css')
>>> vocabList, pSF, pNY=bayes.localWords(ny, sf)
the error rate is: 0.1
>>> vocabList, pSF, pNY=bayes.localWords(ny, sf)
the error rate is: 0.35 
```

为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值。这里的错误率要远高于垃圾邮件中的错误率。由于这里关注的是单词概率而不是实际分类，因此这个问题倒不严重。可以通过函数caclMostFreq()改变要移除的单词数目，然后观察错误率的变化情况。

# 4.7.2 分析数据：显示地域相关的用词

可以先对向量pSF与pNY进行排序，然后按照顺序将词打印出来。下面的最后一段代码会完成这部分工作。再次打开bayes.py文件，将下面的代码添加到文件中。

# 程序清单4-7 最具表征性的词汇显示函数

```python
def getTopWords (ny, sf):
    import operator
    vocabList, p0V, p1V = localWords (ny, sf)
    topNY = []
    topSF = []
    for i in range(len(p0V)):
        if p0V[i] > -6.0: topSF.append((vocabList[i], p0V[i]))
        if p1V[i] > -6.0: topNY.append((vocabList[i], p1V[i]))
        sortedSF = sorted(topSF, key= lambda pair: pair[1], reverse=True)
        print "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF*)
        print item in sortedSF:
            print item[0]
            sortedNY = sorted(topNY, key= lambda pair: pair[1], reverse=True)
            print "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY***" for item in sortedNY:
                print item[0] 
```

程序清单4-7中的函数getTopWords()使用两个RSS源作为输入，然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储。与之前返回排名最高的X个单词不同，这里可以返回大于某个阈值的所有词。这些元组会按照它们的条件概率进行排序。

下面看一下实际的运行效果，保存bayes.py文件，在Python提示符下输入：

```txt
>>> reload(bayes)
<module 'bayes' from 'bayes.pyc>
>>> bayes.getTopWords(ny, sf)
the error rate is: 0.2
SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**
love
time
will
there
hit
send
francisco
female
NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**
friend
people
will
single
sex
female
night
420
relationship
play
hope 
```

最后输出的单词很有意思。值得注意的现象是，程序输出了大量的停用词。移除固定的停用词看看结果会如何变化也十分有趣。依我的经验来看，这样做的话，分类错误率也会降低。

# 4.8 本章小结

对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法。

可以通过特征之间的条件独立性假设，降低对数据量的需求。独立性假设是指一个词的出现概率并不依赖于文档中的其他词。当然我们也知道这个假设过于简单。这就是之所以称为朴素贝叶斯的原因。尽管条件独立性假设并不正确，但是朴素贝叶斯仍然是一种有效的分类器。

利用现代编程语言来实现朴素贝叶斯时需要考虑很多实际因素。下溢出就是其中一个问题，它可以通过对概率取对数来解决。词袋模型在解决文档分类问题上比词集模型有所提高。还有其他一些方面的改进，比如说移除停用词，当然也可以花大量时间对切分器进行优化。

本章学习到的概率理论将在后续章节中用到，另外本章也给出了有关贝叶斯概率理论全面具体的介绍。接下来的一章将暂时不再讨论概率理论这一话题，介绍另一种称作Logistic回归的分类方法及一些优化算法。

# 第5章

# Logistic回归

# 本章内容

Sigmoid函数和Logistic回归分类器  
□最优化理论初步   
梯度下降最优化算法  
数据中的缺失项处理

这会是激动人心的一章，因为我们将首次接触到最优化算法。仔细想想就会发现，其实我们日常生活中遇到过很多最优化问题，比如如何在最短时间内从A点到达B点？如何投入最少工作量却获得最大的效益？如何设计发动机使得油耗最少而功率最大？可见，最优化的作用十分强大。接下来，我们介绍几个最优化算法，并利用它们训练出一个非线性函数用于分类。

读者不熟悉回归也没关系，第8章起会深入介绍这一主题。假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。利用Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数集，其背后的数学分析将在下一部分介绍。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。接下来介绍这个二值型输出分类器的数学原理。

# Logistic回归的一般过程

(1)收集数据：采用任意方法收集数据。  
(2) 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。  
(3) 分析数据：采用任意方法对数据进行分析。  
(4) 训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。  
(5)测试算法：一旦训练步骤完成，分类将会很快。  
(6) 使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。

本章首先阐述Logistic回归的定义，然后介绍一些最优化算法，其中包括基本的梯度上升法和一个改进的随机梯度上升法，这些最优化算法将用于分类器的训练。本章最后会给出一个Logistic回归的实例，预测一匹病马是否能被治愈。

# 5.1 基于 Logistic 回归和 Sigmoid 函数的分类

# Logistic回归

优点：计算代价不高，易于理解和实现。

缺点：容易欠拟合，分类精度可能不高。

适用数据类型：数值型和标称型数据。

我们想要的函数应该是，能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出0或1。或许你之前接触过具有这种性质的函数，该函数称为海维塞德阶跃函数（Heaviside step function），或者直接称为单位阶跃函数。然而，海维塞德阶跃函数的问题在于：该函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质①，且数学上更易处理，这就是Sigmoid函数②。Sigmoid函数具体的计算公式如下：

$$
\sigma (z) = \frac {1}{1 + e ^ {- z}}
$$

图5-1给出了Sigmoid函数在不同坐标尺度下的两条曲线图。当 $x$ 为0时，Sigmoid函数值为0.5。随着 $x$ 的增大，对应的Sigmoid值将逼近于1；而随着 $x$ 的减小，Sigmoid值将逼近于0。如果横坐标刻度足够大（图5-1下图），Sigmoid函数看起来很像一个阶跃函数。

因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5即被归入0类。所以，Logistic回归也可以被看成是一种概率估计。

确定了分类器的函数形式之后，现在的问题变成了：最佳回归系数是多少？如何确定它们的大小？这些问题将在下一节解答。

![](images/de8d558e7eec8a9a41088f52b421c31f119c2b248059cd3c9a62c4050ac1d0da.jpg)

![](images/f038ba7a7fbef1e7c469b9004fc96e0d94e43e7d631d1157f4037e87c8915490.jpg)  
图5-1 两种坐标尺度下的Sigmoid函数图。上图的横坐标为-5到5，这时的曲线变化较为平滑；下图横坐标的尺度足够大，可以看到，在 $x = 0$ 点处Sigmoid函数看起来很像阶跃函数

# 5.2 基于最优化方法的最佳回归系数确定

Sigmoid函数的输入记为 $z$ ，由下面公式得出：

$$
z = w _ {0} x _ {0} + w _ {1} x _ {1} + w _ {2} x _ {2} + \dots + w _ {n} x _ {n}
$$

如果采用向量的写法，上述公式可以写成 $z = w^{\mathrm{T}}x$ ，它表示将这两个数值向量对应元素相乘然后全部加起来即得到 $z$ 值。其中的向量 $x$ 是分类器的输入数据，向量 $w$ 也就是我们要找到的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。

下面首先介绍梯度上升的最优化方法，我们将学习到如何使用该方法求得数据集的最佳参数。接下来，展示如何绘制梯度上升法产生的决策边界图，该图能将梯度上升法的分类效果可视化地呈现出来。最后我们将学习随机梯度上升算法，以及如何对其进行修改以获得更好的结果。

# 5.2.1 梯度上升法

我们介绍的第一个最优化算法叫做梯度上升法。梯度上升法基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 $\nabla$ ，则函数 $f(x, y)$ 的梯度由下式表示：

$$
\nabla f (x, y) = \left( \begin{array}{c} \frac {\partial f (x , y)}{\partial x} \\ \frac {\partial f (x , y)}{\partial y} \end{array} \right)
$$

这是机器学习中最易造成混淆的一个地方，但在数学上并不难，需要做的只是牢记这些符号的意义。这个梯度意味着要沿 $x$ 的方向移动 $\frac{\partial f(x,y)}{\partial x}$ ，沿 $y$ 的方向移动 $\frac{\partial f(x,y)}{\partial y}$ 。其中，函数 $f(x,y)$ 必须要在待计算的点上有定义并且可微。一个具体的函数例子见图5-2。

![](images/6a53e8a31bb513bb8b86ee59c857ee7a571aa5dfa8d604f896dd0bc8471578bd.jpg)  
图5-2 梯度上升算法到达每个点后都会重新估计移动的方向。从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1。在P1点，梯度再次被重新计算，并沿新的梯度方向移动到P2。如此循环迭代，直到满足停止条件。迭代的过程中，梯度算子总是保证我们能选取到最佳的移动方向

图5-2中的梯度上升算法沿梯度方向移动了一步。可以看到，梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记做 $\alpha$ 。用向量来表示的话，梯度算法的迭代公式如下：

$$
w := w + \alpha \nabla_ {w} f (w)
$$

该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或算法达到某个可以允许的误差范围。

# 梯度下降算法

你最经常听到的应该是梯度下降算法，它与这里的梯度上升算法是一样的，只是公式中的加法需要变成减法。因此，对应的公式可以写成

$$
w := w + \alpha \nabla_ {w} f (w)
$$

梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。

基于上面的内容，我们来看一个Logistic回归分类器的应用例子，从图5-3可以看到我们采用的数据集。

![](images/3eec1997c4ce762cf36449377d375698075758787842d129e033016c70ca1bb4.jpg)  
图5-3 一个简单数据集，下面将采用梯度上升法找到Logistic回归分类器在此数据集上的最佳回归系数

# 5.2.2 训练算法：使用梯度上升找到最佳参数

图5-3中有100个样本点，每个点包含两个数值型特征：X1和X2。在此数据集上，我们将通过使用梯度上升法找到最佳回归系数，也就是拟合出Logistic回归模型的最佳参数。

梯度上升法的伪代码如下：

每个回归系数初始化为1

重复R次：

计算整个数据集的梯度

使用alpha $\times$ gradient更新回归系数的向量

返回回归系数

下面的代码是梯度上升算法的具体实现。为了解实际效果，打开文本编辑器并创建一个名为logRegres.py的文件，输入下列代码：

程序清单5-1 Logistic回归梯度上升优化算法  
```python
def loadDataSet(   ) :
        dataMat = [ ]; labelMat = [ ]
        for line in fr.readlines(   ) :
         lineArr = line.strip(   ).split(   )
            dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])) ;
        _labelMat.append(int(lineArr[2])) 
```

程序清单5-1的代码在开头提供了一个便利函数loadDataSet()，它的主要功能是打开文本文件testSet.txt并逐行读取。每行前两个值分别是X1和X2，第三个值是数据对应的类别标签。此外，为了方便计算，该函数还将X0的值设为1.0。接下来的函数是5.2节提到的函数sigmoid()。

梯度上升算法的实际工作是在函数gradAscent()里完成的，该函数有两个参数。第一个参数是dataMathIn，它是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。我们现在采用的是100个样本的简单数据集，它包含了两个特征X1和X2，再加上第0维特征X0，所以dataMathIn里存放的将是 $100 \times 3$ 的矩阵。在①处，我们获得输入数据并将它们转换成NumPy矩阵。这是本书首次使用NumPy矩阵，如果你对矩阵数学不太熟悉，那么一些运算可能就会不易理解。比如，NumPy对2维数组和矩阵都提供一些操作支持，如果混淆了数据类型和对应的操作，执行结果将与预期截然不同。对此，本书附录A给出了对NumPy矩阵的介绍。第二个参数是类别标签，它是一个 $1 \times 100$ 的行向量。为了便于矩阵运算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。接下来的代码是得到矩阵大小，再设置一些梯度上升算法所需的参数。

变量alpha是向目标移动的步长，maxCycles是迭代次数。在for循环迭代完成后，将返回训练好的回归系数。需要强调的是，在②处的运算是矩阵运算。变量h不是一个数而是一个列向量，列向量的元素个数等于样本个数，这里是100。对应地，运算dataMatrix * weights代表的不止一次乘积计算，事实上该运算包含了300次的乘积。

最后还需说明一点，你可能对⑦中公式的前两行觉得陌生。此处略去了一个简单的数学推导，我把它留给有兴趣的读者。定性地说，这里是在计算真实类别与预测类别的差值，接下来就是按照该差值的方向调整回归系数。

接下来看看实际效果，打开文本编辑器，添加程序清单5-1的代码。

在Python提示符下，敲入下面的代码：

```txt
>>> import logRegres
>>> dataArr, labelMat = logRegres.loadDataSet()
>>> logRegres.degAscent(dataArr, labelMat)
matrix([[4.12414349],
[0.48007329],
[-0.6168482]]) 
```

# 5.2.3 分析数据：画出决策边界

上面已经解出了一组回归系数，它确定了不同类别数据之间的分隔线。那么怎样画出该分隔线，从而使得优化的过程便于理解呢？下面将解决这个问题，打开logRegres.py并添加如下代码。

# 程序清单5-2 画出数据集和Logistic回归最佳拟合直线的函数

```python
def plotBestFit(wei):
    import matplotlib.pyplot as plt
    weights = wei.getA()
    dataMat, labelMat = loadDataSet()
    dataArr = array(dataMat)
    n = shape(dataArr)[0]
    xcord1 = []
    ycord1 = []
    xcord2 = []
    for i in range(n):
        if int.labelMat[i]) == 1:
            xcord1.append(dataArr[i, 1]); ycord1.append(dataArr[i, 2])
        else:
            xcord2.append(dataArr[i, 1]); ycord2.append(dataArr[i, 2])
        fig = plt.figure()
        ax = fig.add_subplot(111)
        axscatter(xcord1, ycord1, s=30, c='red', marker='s')
        axscatter(xcord2, ycord2, s=30, c='green')
        x = arange(-3.0, 3.0, 0.1)
        y = (-weights[0] - weights[1]*x)/weights[2]
        ax.plot(x, y)
        plt.xlabel('X1'); pltylabel('X2'); plt.show() 
```

程序清单5-2中的代码是直接用Matplotlib画出来的。唯一要指出的是，①处设置了sigmoid函数为0。回忆5.2节，0是两个分类（类别1和类别0）的分界处。因此，我们设定 $0 = w_{0}x_{0} + w_{1}x_{1} + w_{2}x_{2}$ ，然后解出X2和X1的关系式（即分隔线的方程，注意 $X_0 = 1$ ）。

运行程序清单5-2的代码，在Python提示符下输入：

```txt
>>> from numpy import *
>>> reload(logRegres)
<module 'logRegres' from 'logRegres.py>
>>> logRegres.plotBestFit(weights.getA()) 
```

输出的结果如图5-4所示。

![](images/1bb08c3021ac1a751a0a050aa05b12447fe305108c9ba57a2033778f4c1af4ef.jpg)  
图5-4 梯度上升算法在500次迭代后得到的Logistic回归最佳拟合直线

这个分类结果相当不错，从图上看只错分了两到四个点。但是，尽管例子简单且数据集很小，这个方法却需要大量的计算（300次乘法）。因此下一节将对该算法稍作改进，从而使它可以用在真实数据集上。

# 5.2.4 训练算法：随机梯度上升

梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，一次处理所有数据被称作是“批处理”。

随机梯度上升算法可以写成如下的伪代码：

所有回归系数初始化为1

对数据集中每个样本

计算该样本的梯度

使用alpha $\times$ gradient更新回归系数值

返回回归系数值

以下是随机梯度上升算法的实现代码。

# 程序清单5-3 随机梯度上升算法

```python
def stocGradAscent0(dataMatrix, classLabels):
    m, n = shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i] * weights))
        error = classLabels[i] - h
        weights = weights + alpha * error * dataMatrix[i]
    return weights 
```

可以看到，随机梯度上升算法与梯度上升算法在代码上很相似，但也有一些区别：第一，后者的变量h和误差error都是向量，而前者则全是数值；第二，前者没有矩阵的转换过程，所有变量的数据类型都是NumPy数组。

为了验证该方法的结果，我们将程序清单5-3的代码添加到logRegres.py中，并在Python提示符下输入如下命令：

```coffeescript
>>> from numpy import *
>>> reload(logRegres)
<module 'logRegres' from 'logRegres.py>
>>> dataArr, labelMat = logRegres.loadDataSet()
>>> weights = logRegres.stocGradAscent0(array(dataArr), labelMat)
>>> logRegres.plotBestFit(weights) 
```

执行完毕后将得到图5-5所示的最佳拟合直线图，该图与图5-4有一些相似之处。可以看到，拟合出来的直线效果还不错，但并不像图5-4那样完美。这里的分类器错分了三分之一的样本。

直接比较程序清单5-3和程序清单5-1的代码结果是不公平的，后者的结果是在整个数据集上迭代了500次才得到的。一个判断优化算法优劣的可靠方法是看它是否收敛，也就是说参数是否达到了稳定值，是否还会不断地变化？对此，我们在程序清单5-3中随机梯度上升算法上做了些修改，使其在整个数据集上运行200次。最终绘制的三个回归系数的变化情况如图5-6所示。

![](images/613d6f5869c50897a90b9917cafbfa736c29c8f10343dc7dbe54309c0a5651c9.jpg)  
图5-5 随机梯度上升算法在上述数据集上的执行结果，最佳拟合直线并非最佳分类线

![](images/1fcce42e8568bf18d32fc9a9e573f7d492ad0e5e466a1d5df3c7be46660a0b43.jpg)  
图5-6 运行随机梯度上升算法，在数据集的一次遍历中回归系数与迭代次数的关系图。回归系数经过大量迭代才能达到稳定值，并且仍然有局部的波动现象

图5-6展示了随机梯度上升算法在200次迭代过程中回归系数的变化情况。其中的系数2，也就是图5-5中的X2只经过了50次迭代就达到了稳定值，但系数1和0则需要更多次的迭代。另外值得注意的是，在大的波动停止后，还有一些小的周期性波动。不难理解，产生这种现象的原因是存在一些不能正确分类的样本点（数据集并非线性可分），在每次迭代时会引发系数的剧烈改变。我们期望算法能避免来回波动，从而收敛到某个值。另外，收敛速度也需要加快。

对于图5-6存在的问题，可以通过修改程序清单5-3的随机梯度上升算法来解决，具体代码如下。

# 程序清单5-4 改进的随机梯度上升算法

```python
def stocGradAscent1(dataMatrix, classLabels, numIter=150):
    m, n = shape(dataMatrix)
    weights = ones(n)
    for j in range(numIter): dataIndex = range(m)
    for i in range(m):
        alpha = 4/(1.0+j+i)+0.01
        randIndex = int(random.uniform(0,len(dataIndex)))
        h = sigmoid(sum(dataMatrix[randIndex]*weights))
        error = classLabels[randIndex] - h
        weights = weights + alpha * error * dataMatrix[randIndex]
        del(dataIndex[randIndex])
    return weights 
```

程序清单5-4与程序清单5-3类似，但增加了两处代码来进行改进。

第一处改进在①处。一方面，alpha在每次迭代的时候都会调整，这会缓解图5-6上的数据波

动或者高频波动。另外，虽然alpha会随着迭代次数不断减小，但永远不会减小到0，这是因为①中还存在一个常数项。必须这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响。如果要处理的问题是动态变化的，那么可以适当加大上述常数项，来确保新的值获得更大的回归系数。另一点值得注意的是，在降低alpha的函数中，alpha每次减少 $1 / (j + i)$ ，其中j是迭代次数，i是样本点的下标①。这样当 $j << \max(i)$ 时，alpha就不是严格下降的。避免参数的严格下降也常见于模拟退火算法等其他优化算法中。

程序清单5-4第二个改进的地方在②处，这里通过随机选取样本来更新回归系数。这种方法将减少周期性的波动（如图5-6中的波动）。具体实现方法与第3章类似，这种方法每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。

此外，改进算法还增加了一个迭代次数作为第3个参数。如果该参数没有给定的话，算法将默认迭代150次。如果给定，那么算法将按照新的参数值进行迭代。

与stocGradAscent1()类似，图5-7显示了每次迭代时各个回归系数的变化情况。

![](images/303bfe1c6565ebc7cda889be75a5eabb7a329a0994a1b507ec53bcc8e3cef09b.jpg)  
图5-7 使用样本随机选择和alpha动态减少机制的随机梯度上升算法stocGradAscent1()所生成的系数收敛示意图。该方法比采用固定alpha的方法收敛速度更快

比较图5-7和图5-6可以看到两点不同。第一点是，图5-7中的系数没有像图5-6里那样出现周期性的波动，这归功于stocGradAscent1()里的样本随机选择机制；第二点是，图5-7的水平轴

比图5-6短了很多，这是由于stocGradAscent1()可以收敛得更快。这次我们仅仅对数据集做了20次遍历，而之前的方法是500次。

下面看看在同一个数据集上的分类效果。将程序清单5-4的代码添加到logRegres.py文件中，并在Python提示符下输入：

```coffeescript
>>> reload(logRegres)
<module 'logRegres' from 'logRegres.py>
>>> dataArr, labelMat = logRegres.loadDataSet()
>>> weights = logRegres.stocGradAscent1(array(dataArr), labelMat)
>>> logRegres.plotBestFit(weights) 
```

程序运行之后应该能看到类似图5-8的结果图。该分隔线达到了与GradientAscent()差不多的效果，但是所使用的计算量更少。

![](images/cf4c654af2486ada00cc8e1606bb7030222419d27187989ac0fc77d39dc3e691.jpg)  
图5-8 使用改进的随机梯度上升算法得到的系数

默认迭代次数是150，可以通过stocGradAscent()的第3个参数来对此进行修改，例如：

>>>weights $\equiv$ logRegres.stocGradAscent1(array(dataArr),labelMat，500)

目前，我们已经学习了几个优化算法，但还有很多优化算法值得探讨，所幸这方面已有大量的文献可供参考。另外再说明一下，针对给定的数据集，读者完全可以对算法的各种参数进行调整，从而达到更好的效果。

迄今为止我们分析了回归系数的变化情况，但还没有达到本章的最终目标，即完成具体的分类任务。下一节将使用随机梯度上升算法来解决病马的生死预测问题。

# 5.3 示例：从疝气病症预测病马的死亡率

本节将使用Logistic回归来预测患有疝病的马的存活问题。这里的数据①包含368个样本和28个特征。我并非育马专家，从一些文献中了解到，疝病是描述马胃肠痛的术语。然而，这种病不一定源自马的胃肠问题，其他问题也可能引发马疝病。该数据集中包含了医院检测马疝病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。

# 示例：使用Logistic回归估计马逊病的死亡率

(1)收集数据：给定数据文件。  
(2) 准备数据：用Python解析文本文件并填充缺失值。  
(3) 分析数据：可视化并观察数据。  
(4) 训练算法：使用优化算法，找到最佳的系数。  
(5) 测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数。  
(6) 使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以做为留给读者的一道习题。

另外需要说明的是，除了部分指标主观和难以测量外，该数据还存在一个问题，数据集中有 $30\%$ 的值是缺失的。下面将首先介绍如何处理数据集中的数据缺失问题，然后再利用Logistic回归和随机梯度上升算法来预测病马的生死。

# 5.3.1 准备数据：处理数据中的缺失值

数据中的缺失值是个非常棘手的问题，有很多文献都致力于解决这个问题。那么，数据缺失究竟带来了什么问题？假设有100个样本和20个特征，这些数据都是机器收集回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？它们是否还可用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。

下面给出了一些可选的做法：

□ 使用可用特征的均值来填补缺失值；  
□ 使用特殊值来填补缺失值，如-1；  
□ 忽略有缺失值的样本；  
□ 使用相似样本的均值添补缺失值；  
□ 使用另外的机器学习算法预测缺失值。

现在，我们对下一节要用的数据集进行预处理，使其可以顺利地使用分类算法。在预处理阶段需要做两件事：第一，所有的缺失值必须用一个实数值来替换，因为我们使用的NumPy数据类型不允许包含缺失值。这里选择实数0来替换所有缺失值，恰好能适用于Logistic回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。回归系数的更新公式如下：

```javascript
weights = weights + alpha * error * dataMatrix[randIndex] 
```

如果dataMatrix的某特征对应值为0，那么该特征的系数将不做更新，即：

```txt
weights = weights 
```

另外，由于sigmoid(0) = 0.5，即它对结果的预测不具有任何倾向性，因此上述做法也不会对误差项造成任何影响。基于上述原因，将缺失值用0代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为0，因此在某种意义上说它也满足“特殊值”这个要求。

预处理中做的第二件事是，如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换。采用Logistic回归进行分类时这种做法是合理的，而如果采用类似kNN的方法就可能不太可行。

原始的数据集经过预处理之后保存成两个文件：horseColicTest.txt和horseColicTraining.txt。如果想对原始数据和预处理后的数据做个比较，可以在http://archive.ics.uci.edu/ml/datasets/Horse+Colic浏览这些数据。

现在我们有一个“干净”可用的数据集和一个不错的优化算法，下面将把这些部分融合在一起训练出一个分类器，然后利用该分类器来预测病马的生死问题。

# 5.3.2 测试算法：用Logistic回归进行分类

本章前面几节介绍了优化算法，但目前为止还没有在分类上做任何实际尝试。使用Logistic回归方法进行分类并不需要做很多工作，所需做的只是把测试集上每个特征向量乘以最优化方法得来的回归系数，再将该乘积结果求和，最后输入到Sigmoid函数中即可。如果对应的Sigmoid值大于0.5就预测类别标签为1，否则为0。

下面看看实际运行效果，打开文本编辑器并将下列代码添加到logRegres.py文件中。

# 程序清单5-5 Logistic回归分类函数

```python
def classifyVector(inX, weights):
    prob = sigmoid(sum(inX*weights))
    if prob > 0.5: return 1.0
    else: return 0.0
def colicTest():
    frTrain = open('horseColicTraining.txt')
    frTest = open('horseColicTest.txt')
    trainingSet = []
    trainingLabels = []
    for line in frTrain.readlines():
        currLine = line.strip().split("\\t")
        lineArr = [] 
```

```python
for i in range(21):
    lineArr.append(float(currLine[i])
trainingSet.append(lineArr)
trainingLabels.append(float(currLine[21]))
trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)
errorCount = 0; numTestVec = 0.0
for line in frTest.readlines():
    numTestVec += 1.0
    currLine = line.strip().split("\\t")
    lineArr = []
    for i in range(21):
        lineArr.append(float(currLine[i]))
    if int(classifyVector(array(currLine), trainWeights)) != int(currLine[21]):
        errorCount += 1
    errorRate = (float(errorCount)/numTestVec)
print "the error rate of this test is: %f" % errorRate
return errorRate
def multiTest():
    numTests = 10; errorSum = 0.0
    for k in range(numTests):
        errorSum += colicTest()
print "after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests)) 
```

程序清单5-5的第一个函数是classifyVector()，它以回归系数和特征向量作为输入来计算对应的Sigmoid值。如果Sigmoid值大于0.5函数返回1，否则返回0。

接下来的函数是colicTest()，是用于打开测试集和训练集，并对数据进行格式化处理的函数。该函数首先导入训练集，同前面一样，数据的最后一列仍然是类别标签。数据最初有三个类别标签，分别代表马的三种情况：“仍存活”、“已经死亡”和“已经安乐死”。这里为了方便，将“已经死亡”和“已经安乐死”合并成“未能存活”这个标签。数据导入之后，便可以使用函数stocGradAscent1()来计算回归系数向量。这里可以自由设定迭代的次数，例如在训练集上使用500次迭代，实验结果表明这比默认迭代150次的效果更好。在系数计算完成之后，导入测试集并计算分类错误率。整体看来，colicTest()具有完全独立的功能，多次运行得到的结果可能稍有不同，这是因为其中有随机的成分在里面。如果在stocGradAscent1()函数中回归系数已经完全收敛，那么结果才将是确定的。

最后一个函数是multiTest()，其功能是调用函数colicTest()10次并求结果的平均值。下面看一下实际的运行效果，在Python提示符下输入：

```txt
>>> reload(logRegres)
<module 'logRegres' from 'logRegres.py>
>>> logRegres.MultiTest()
the error rate of this test is: 0.358209
the error rate of this test is: 0.432836
the error rate of this test is: 0.373134 
```

```txt
the error rate of this test is: 0.298507  
the error rate of this test is: 0.313433  
after 10 iterations the average error rate is: 0.353731 
```

从上面的结果可以看到，10次迭代之后的平均错误率为 $35\%$ 。事实上，这个结果并不差，因为有 $30\%$ 的数据缺失。当然，如果调整colicTest()中的迭代次数和stockGradAscent1()中的步长，平均错误率可以降到 $20\%$ 左右。第7章中我们还会再次使用到这个数据集。

# 5.4 本章小结

Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。

随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理运算。

机器学习的一个重要问题就是如何处理缺失数据。这个问题没有标准答案，取决于实际应用中的需求。现有一些解决方案，每种方案都各有优缺点。

下一章将介绍与Logistic回归类似的另一种分类算法：支持向量机，它被认为是目前最好的现成的算法之一。

# 第6章

# 支持向量机

# 本章内容

简单介绍支持向量机  
□利用SMO进行优化  
□ 利用核函数对数据进行空间转换  
将SVM和其他分类器进行对比

“由于理解支持向量机（Support Vector Machines，SVM）需要掌握一些理论知识，而这对于读者来说有一定难度，于是建议读者直接下载LIBSVM使用。”我发现，在介绍SVM时，不止一本书都采用了以上这种模式。本书并不打算沿用这种模式。我认为，如果对SVM的理论不甚了解就去阅读其产品级C++代码，那么读懂的难度很大。但如果将产品级代码和速度提升部分剥离出去，那么代码就会变得可控，或许这样的代码就可以读懂了。

有些人认为，SVM是最好的现成的分类器，这里说的“现成”指的是分类器不加修改即可直接使用。同时，这就意味着在数据上应用基本形式的SVM分类器就可以得到低错误率的结果。SVM能够对训练集之外的数据点做出很好的分类决策。

本章首先讲述SVM的基本概念，书中会引入一些关键术语。SVM有很多实现，但是本章只关注其中最流行的一种实现，即序列最小优化①（Sequential Minimal Optimization，SMO）算法。在此之后，将介绍如何使用一种称为核函数（kernel）的方式将SVM扩展到更多数据集上。最后会回顾第1章中手写识别的例子，并考察其能否通过SVM来提高识别的效果。

# 6.1 基于最大间隔分隔数据

# 支持筒量机

优点：泛化错误率低，计算开销不大，结果易解释。

缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。

适用数据类型：数值型和标称型数据。

在介绍SVM这个主题之前，先解释几个概念。考虑图6-1中A-D共4个方框中的数据点分布，一个问题就是，能否画出一条直线将圆形点和方形点分开呢？先考虑图6-2方框A中的两组数据，它们之间已经分隔得足够开，因此很容易就可以在图中画出一条直线将两组数据点分开。在这种情况下，这组数据被称为线性可分（linearly separable）数据。读者先不必担心上述假设是否过于完美，稍后当直线不能将数据点分开时，我们会对上述假设做一些修改。

![](images/4dbddf38f2b87a3daf4268b90bc4f053796cf8ea2d902b47aa89b68a750f97b0.jpg)

![](images/99aa35c2138f81a7ee454fcb4be9a5a7b6879023eb33cafd59db834cdf0a1e3c.jpg)

![](images/150b5b71e6869c09318dec4d3aba92650cb22010b9d177dd47509b75fc1510a3.jpg)

![](images/232de9a83056a39bb5995a98dd9c29ce40740bb732f5081bbc2b5471bef67249.jpg)  
图6-1 4个线性不可分的数据集

上述将数据集分隔开来的直线称为分隔超平面（separating hyperplane）。在上面给出的例子中，由于数据点都在二维平面上，所以此时分隔超平面上就只是一条直线。但是，如果所给的数据集是三维的，那么此时用来分隔数据的就是一个平面。显而易见，更高维的情况可以依此类推。如果数据集是1024维的，那么就需要一个1023维的某某对象来对数据进行分隔。这个1023维的某某对象到底应该叫什么？N-1维呢？该对象被称为超平面（hyperplane），也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。

我们希望能采用这种方式来构建分类器，即如果数据点离决策边界越远，那么其最后的预测结果也就越可信。考虑图6-2框B到框D中的三条直线，它们都能将数据分隔开，但是其中哪一条最好呢？是否应该最小化数据点到分隔超平面的平均距离？来求最佳直线如果是那样，图6-2的B和C框中的直线是否真的就比D框中的直线好呢？如果这样做，是不是有点寻找最佳拟合直线的感觉？是的，上述做法确实有点像直线拟合，但这并非最佳方案。我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为间隔<sup>①</sup>（margin）。我们希望间隔尽可能地大，这是因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分

类器尽可能健壮。

支持向量（support_vector）就是离分隔超平面最近的那些点。接下来要试着最大化支持向量到分隔面的距离，需要找到此问题的优化求解方法。

![](images/46d8cc0993b82f2fe66b03fc78bf2d96d4b750f53bb58bf6491c0c550b32573a.jpg)

![](images/3a5dd6f4a34be92b3f45e6ffd14e484af4d041af92f51436dc39a1b793725e9d.jpg)

![](images/8335f07fa01b6125b632f40a17f919532599ca39009960caf89a308556ce6839.jpg)

![](images/6c9b261cbc35d07573585eda3a110de31ca9aa4b9a4517c25a288f138a9fc516.jpg)  
图6-2 A框中给出了一个线性可分的数据集，B、C、D框中各自给出了一条可以将两类数据分开的直线

![](images/03246f4150551d986184886c442205f9e3380e5b317a8764978e546015e79b92.jpg)

# 6.2 寻找最大间隔

如何求解数据集的最佳分隔直线？先来看看图6-3。分隔超平面的形式可以写成 $\mathbf{w}^{\mathrm{T}}\mathbf{x} + \mathbf{b}$ 。要计算点A到分隔超平面的距离，就必须给出点到分隔面的法线或垂线的长度，该值为 $|\mathbf{w}^{\mathrm{T}}\mathbf{A} + \mathbf{b}| / ||\mathbf{w}||$ 。这里的常数b类似于Logistic回归中的截距 $\mathbf{w}_0$ 。这里的向量w和常数b一起描述了所给数据的分隔线或超平面。接下来我们讨论分类器。

![](images/012b4b65af125784e3e1889bb9d1b75079f4566f7d65923023e20e528fca45d2.jpg)  
图6-3 点A到分隔平面的距离就是该点到分隔面的法线长度

# 6.2.1 分类器求解的优化问题

前面已经提到了分类器，但还没有介绍它的工作原理。理解其工作原理将有助于理解基于优化问题的分类器求解过程。输入数据给分类器会输出一个类别标签，这相当于一个类似于Sigmoid的函数在作用。下面将使用类似海维赛德阶跃函数（即单位阶跃函数）的函数对 $\mathbf{w}^T\mathbf{x} + b$ 作用得到 $f(\mathbf{w}^T\mathbf{x} + b)$ ，其中当 $u < 0$ 时 $f(u)$ 输出-1，反之则输出+1。这和前一章的Logistic回归有所不同，那里的类别标签是0或1。

这里的类别标签为什么采用-1和+1，而不是0和1呢？这是由于-1和+1仅仅相差一个符号，方便数学上的处理。我们可以通过一个统一公式来表示间隔或者数据点到分隔超平面的距离，同时不必担心数据到底是属于-1还是+1类。

当计算数据点到分隔面的距离并确定分隔面的放置位置时，间隔通过label $* (\mathbf{w}^{\mathbf{x}} + b)^{0}$ 来计算，这时就能体现出-1和+1类的好处了。如果数据点处于正方向（即+1类）并且离分隔超平面很远的位置时， $\mathbf{w}^{\mathbf{x}} + b$ 会是一个很大的正数，同时label $* (\mathbf{w}^{\mathbf{x}} + b)$ 也会是一个很大的正数。而如果数据点处于负方向（-1类）并且离分隔超平面很远的位置时，此时由于类别标签为-1，则label $* (\mathbf{w}^{\mathbf{x}} + b)$ 仍然是一个很大的正数。

现在的目标就是找出分类器定义中的w和b。为此，我们必须找到具有最小间隔的数据点，而这些数据点也就是前面提到的支持向量。一旦找到具有最小间隔的数据点，我们就需要对该间隔最大化。这就可以写作：

$$
\arg \max  _ {w, b} \left\{\min  _ {n} (\operatorname {l a b e l} \cdot (w ^ {T} x + b)) \cdot \frac {1}{\| w \|} \right\}
$$

直接求解上述问题相当困难，所以我们将它转换成为另一种更容易求解的形式。首先考察一下上式中大括号内的部分。由于对乘积进行优化是一件很讨厌的事情，因此我们要做的是固定其中一个因子而最大化其他因子。如果令所有支持向量的 $\text{label}^* (\mathbf{w}^{\mathbf{T}}\mathbf{x} + b)$ 都为1，那么就可以通过求 $||\mathbf{w}||^{-1}$ 的最大值来得到最终解。但是，并非所有数据点的 $\text{label}^* (\mathbf{w}^{\mathbf{T}}\mathbf{x} + b)$ 都等于1，只有那些离分隔超平面最近的点得到的值才为1。而离超平面越远的数据点，其 $\text{label}^* (\mathbf{w}^{\mathbf{T}}\mathbf{x} + b)$ 的值也就越大。

在上述优化问题中，给定了一些约束条件然后求最优值，因此该问题是一个带约束条件的优化问题。这里的约束条件就是 $\text{label} * (\mathbf{w}^{\mathrm{T}}\mathbf{x} + b) \geqslant 1.0$ 。对于这类优化问题，有一个非常著名的求解方法，即拉格朗日乘子法。通过引入拉格朗日乘子，我们就可以基于约束条件来表述原来的问题。由于这里的约束条件都是基于数据点的，因此我们就可以将超平面写成数据点的形式。于是，优化目标函数最后可以写成：

$$
\max  _ {\alpha} \left[ \sum_ {i = 1} ^ {m} \alpha - \frac {1}{2} \sum_ {i, j = 1} ^ {m} \operatorname {l a b e l} ^ {(i)} \cdot \operatorname {l a b e l} ^ {(j)} \cdot a _ {i} \cdot a _ {j} \left\langle x ^ {(i)}, x ^ {(j)} \right\rangle \right] ^ {②}
$$

其约束条件为：

$$
\alpha \geqslant 0 , \text {和} \sum_ {i - 1} ^ {m} \alpha_ {i} \cdot \mathrm {l a b e l} ^ {(i)} = 0
$$

至此，一切都很完美，但是这里有个假设：数据必须 $100\%$ 线性可分。目前为止，我们知道几乎所有数据都不那么“干净”。这时我们就可以通过引入所谓松弛变量（slack variable），来允许有些数据点可以处于分隔面的错误一侧。这样我们的优化目标就能保持仍然不变，但是此时新的约束条件则变为：

$$
C \geqslant \alpha \geqslant 0 ,     \text {和}   \sum_ {i - 1} ^ {m} \alpha_ {i} \cdot \mathrm {l a b e l} ^ {(i)} = 0
$$

这里的常数c用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。在优化算法的实现代码中，常数c是一个参数，因此我们就可以通过调节该参数得到不同的结果。一旦求出了所有的alpha，那么分隔超平面就可以通过这些alpha来表达。这一结论十分直接，SVM中的主要工作就是求解这些alpha。

要理解刚才给出的这些公式还需要大量的知识。如果你有兴趣，我强烈建议去查阅相关的教材①②，以获得上述公式的推导细节。

# 6.2.2 SVM应用的一般框架

在第1章中，我们定义了构建机器学习应用的一般步骤，但是这些步骤会随机器学习任务或算法的不同而有所改变，因此有必要在此探讨如何在本章中实现它们。

# SVM的一般流程

(1)收集数据：可以使用任意方法。  
(2) 准备数据：需要数值型数据。  
(3) 分析数据：有助于可视化分隔超平面。  
(4) 训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。  
(5) 测试算法：十分简单的计算过程就可以实现。  
(6) 使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。

到目前为止，我们已经了解了一些理论知识，我们当然希望能够通过编程，在数据集上将这些理论付诸实践。下一节将介绍一个简单但很强大的实现算法。

# 6.3 SMO高效优化算法

接下来，我们根据6.2.1节中的最后两个式子进行优化，其中一个是最小化的目标函数，一个是在优化过程中必须遵循的约束条件。不久之前，人们还在使用二次规划求解工具（quadratic solver）来求解上述最优化问题，这种工具是一种用于在线性约束下优化具有多个变量的二次目标函数的软件。而这些二次规划求解工具则需要强大的计算能力支撑，另外在实现上也十分复杂。所有需要做的围绕优化的事情就是训练分类器，一旦得到alpha的最优值，我们就得到了分隔超平面（2维平面中就是直线）并能够将之用于数据分类。

下面我们就开始讨论SMO算法，然后给出一个简化的版本，以便读者能够正确理解它的工作流程。后一节将会给出SMO算法的完整版，它比简化版的运行速度要快很多。

# 6.3.1 Platt的SMO算法

1996年，John Platt发布了一个称为 $\mathrm{SMO}^{\text{①}}$ 的强大算法，用于训练SVM。SMO表示序列最小优化（Sequential Minimal Optimization）。Platt的SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。在结果完全相同的同时，SMO算法的求解时间短很多。

SMO算法的目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面。

SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个alpha必须要符合一定的条件，条件之一就是这两个alpha必须要在间隔边界之外，而其第二个条件则是这两个alpha还没有进行过区间化处理或者不在边界上。

# 6.3.2 应用简化版SMO算法处理小规模数据集

Platt SMO算法的完整实现需要大量代码。在接下来的第一个例子中，我们将会对算法进行简化处理，以便了解算法的基本工作思路，之后再基于简化版给出完整版。简化版代码虽然量少但执行速度慢。Platt SMO算法中的外循环确定要优化的最佳alpha对。而简化版却会跳过这一部分，首先在数据集上遍历每一个alpha，然后在剩下的alpha集合中随机选择另一个alpha，从而构建alpha对。这里有一点相当重要，就是我们要同时改变两个alpha。之所以这样做是因为我们有一个约束条件：

$$
\sum \alpha_ {i} \cdot \operatorname {l a b e l} ^ {(i)} = 0
$$

由于改变一个alpha可能会导致该约束条件失效，因此我们总是同时改变两个alpha。

为此，我们将构建一个辅助函数，用于在某个区间范围内随机选择一个整数。同时，我们也需要另一个辅助函数，用于在数值太大时对其进行调整。下面的程序清单给出了这两个函数的实现。读者可以打开一个文本编辑器将这些代码加入到svmMLiA.py文件中。

程序清单6-1 SMO算法中的辅助函数  
```python
def loadDataSet(fileName):
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split("\\t")
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(float(lineArr[2]))
    return dataMat, labelMat
def selectJrand(i, m):
    j = i
    while (j == i):
        j = int(random.uniform(0, m))
    return j
def clipAlpha aj, H, L):
    if aj > H:
        aj = H
if L > aj:
    aj = L
return aj 
```

在testSet.txt文件中保存了图6-3所给出的数据。接下来，我们就将在这个文件上应用SMO算法。程序清单6-1中的第一个函数就是我们所熟知的loadDatSet()函数，该函数打开文件并对其进行逐行解析，从而得到每行的类标签和整个数据矩阵。

下一个函数selectJrand()有两个参数值，其中i是第一个alpha的下标，m是所有alpha的数目。只要函数值不等于输入值i，函数就会进行随机选择。

最后一个辅助函数就是clipAlpha()，它是用于调整大于H或小于L的alpha值。尽管上述3个辅助函数本身做的事情不多，但在分类器中却很有用处。

在输入并保存程序清单6-1中的代码之后，运行如下命令：

```python
>>> import smMLiA
>>> dataArr, labelArr = smMLiA.loadDataSet('testSet.txt')
>>> labelArr
[-1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0... 
```

可以看得出来，这里采用的类别标签是-1和1，而不是0和1。

上述工作完成之后，就可以使用SMO算法的第一个版本了。

该SMO函数的伪代码大致如下：

创建一个alpha向量并将其初始化为0向量

当迭代次数小于最大迭代次数时（外循环）

对数据集中的每个数据向量（内循环）：

如果该数据向量可以被优化：

随机选择另外一个数据向量

同时优化这两个向量

如果两个向量都不能被优化，退出内循环

如果所有向量都没被优化，增加迭代数目，继续下一次循环

程序清单6-2中的代码是SMO算法的一个有效版本。在Python中，如果某行以\符号结束，那么就意味着该行语句没有结束并会在下一行延续。下面的代码当中有很多很长的语句必须要分成多行来写。因此，下面的程序中使用了多个\符号。打开文件svmMLiA.py之后输入如下程序清单中的代码。

# 程序清单6-2 简化版SMO算法

```python
def smoSimple(dataMatIn, classLabels, C, toler, maxIter):
    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()
    b = 0; m, n = shape(dataMatrix)
    alphas = mat(zeros((m, 1)))
    iter = 0
while (iter < maxIter):
    alphaPairsChanged = 0
    for i in range(m):
        fXi = float(multiply(alpha, labelMat).T * (
            (dataMatrix * dataMatrix[i, :].T)) + b
        Ei = fXi - float.labelMat[i])
        if ((labelMat[i] * Ei < -toler) and (alpha[0] < C)) or \
            ((labelMat[i] * Ei > toler) and \
            (alphas[i] > 0)):
                j = selectJrand(i, m)
                fXj = float(multiply(alpha, labelMat).T * (
                    (dataMatrix * dataMatrix[j, :].T)) + b
            Ej = fXj - float.labelMat[j])
            alphaIold = alphas[i].copy());
            alphaJold = alphas[j].copy());
        if (labelMat[i] != labelMat[j]):
            L = max(0, alphas[j] - alphas[i])
            H = min(C, C + alphas[j] - alphas[i])
        else:
            L = max(0, alphas[j] + alphas[i] - C)
            H = min(C, alphas[j] + alphas[i])
        if L == H: print "L == H"; continue
            eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - \
            dataMatrix[i, :]*dataMatrix[i, :].T - \
            dataMatrix[j, :]*dataMatrix[j, :].T
        if eta >= 0: print "eta >= 0"; continue
            alphas[j] -= labelMat[j] * (Ei - Ej)/eta
            alphas[j] = clipAlpha(alpha[j], H, L)
        if (abs(alpha[j] - alphaJold) < 0.00001): print \
            "j not moving enough"; continue
            alphas[i] += labelMat[j] * labelMat[i] *
            (alphaJold - alphas[j])
        b1 = b - Ei - labelMat[i] * (alphas[i] - alphaIold) * 
```

```javascript
dataMatrix[i,] \*dataMatrix[i,].T- labelMat[j]\*(alphas[j]-alphaJold)\* dataMatrix[i,] \*dataMatrix[j,].T b2=b-Ej-labelMat[i]\*(alphas[i]-alphaFold)\* dataMatrix[i,] \*dataMatrix[j,].T - labelMat[j]\*(alphas[j]-alphaJold)\* dataMatrix[j,] \*dataMatrix[j,].T if(0<alphas[i])and(C>alphas[i]):b=b1 elif(0<alphas[j])and(C>alphas[j]):b=b2 else:b=(b1+b2)/2.0 alphaPairsChanged+=1 print"iter:%d i:%d,pairs changed%d"%\ (iter,i,alphaPairsChanged) if(alphaPairsChanged==0):iter+=1 else:iter=0 print"iteration number:%d"%iter return b,alphas 
```

这个函数比较大，或许是我所知道的本书中最大的一个函数。该函数有5个输入参数，分别是：数据集、类别标签、常数c、容错率和取消前最大的循环次数。在本书，我们构建函数时采用了通用的接口，这样就可以对算法和数据源进行组合或配对处理。上述函数将多个列表和输入参数转换成NumPy矩阵，这样就可以简化很多数学处理操作。由于转置了类别标签，因此我们得到的就是一个列向量而不是列表。于是类别标签向量的每行元素都和数据矩阵中的行一一对应。我们也可以通过矩阵dataMatIn的shape属性得到常数m和n。最后，我们就可以构建一个alpha列矩阵，矩阵中元素都初始化为0，并建立一个iter变量。该变量存储的则是在没有任何alpha改变的情况下遍历数据集的次数。当该变量达到输入值maxIter时，函数结束运行并退出。

每次循环当中，将alphaPairsChanged先设为0，然后再对整个集合顺序遍历。变量alphaPairsChanged用于记录alpha是否已经进行优化。当然，在循环结束时就会得知这一点。首先，fXi能够计算出来，这就是我们预测的类别。然后，基于这个实例的预测结果和真实结果的比对，就可以计算误差Ei。如果误差很大，那么可以对该数据实例所对应的alpha值进行优化。对该条件的测试处于上述程序清单的①处。在if语句中，不管是正间隔还是负间隔都会被测试。并且在该if语句中，也要同时检查alpha值，以保证其不能等于0或C。由于后面alpha小于0或大于C时将被调整为0或C，所以一旦在该if语句中它们等于这两个值的话，那么它们就已经在“边界”上了，因而不再能够减小或增大，因此也就不值得再对它们进行优化了。

接下来，可以利用程序清单6-1中的辅助函数来随机选择第二个alpha值，即alpha[j]②。同样，可以采用第一个alpha值（alpha[i]）的误差计算方法，来计算这个alpha值的误差。这个过程可以通过copy()的方法来实现，因此稍后可以将新的alpha值与老的alpha值进行比较。Python则会通过引用的方式传递所有列表，所以必须明确地告知Python要为alphaIold和alphaJold分配新的内存；否则的话，在对新值和旧值进行比较时，我们就看不到新旧值的变化。之后我们开始计算L和H③，它们用于将alpha[j]调整到0到c之间。如果L和H相等，就不做任何改变，直接执行continue语句。这在Python中，则意味着本次循环结束直接运行下一次for的循环。

Eta是alpha[j]的最优修改量，在那个很长的计算代码行中得到。如果eta为0，那就是说

需要退出for循环的当前迭代过程。该过程对真实SMO算法进行了简化处理。如果eta为0，那么计算新的alpha[j]就比较麻烦了，这里我们就不对此进行详细的介绍了。有需要的读者可以阅读Platt的原文来了解更多的细节。现实中，这种情况并不常发生，因此忽略这一部分通常也无伤大雅。于是，可以计算出一个新的alpha[j]，然后利用程序清单6-1中的辅助函数以及L与H值对其进行调整。

然后，就是需要检查alpha[j]是否有轻微改变。如果是的话，就退出for循环。然后，alpha[i]和alpha[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反（即如果一个增加，那么另外一个减少）④。在对alpha[i]和alpha[j]进行优化之后，给这两个alpha值设置一个常数项b⑤。

最后，在优化过程结束的同时，必须确保在合适的时机结束循环。如果程序执行到for循环的最后一行都不执行continue语句，那么就已经成功地改变了一对alpha，同时可以增加alphaPairsChanged的值。在for循环之外，需要检查alpha值是否做了更新，如果有更新则将iter设为0后继续运行程序。只有在所有数据集上遍历maxIter次，且不再发生任何alpha修改之后，程序才会停止并退出while循环。

为了解实际效果，可以运行如下命令：

```txt
>>> b, alphas = smMLiA.smoSimple(dataArr, labelArr, 0.6, 0.001, 40) 
```

运行后输出类似如下结果：

```yaml
iteration number: 29  
j not moving enough  
iteration number: 30  
iter: 30 i:17, pairs changed 1  
j not moving enough  
iteration number: 0  
j not moving enough  
iteration number: 1 
```

上述运行过程需要几分钟才会收敛。一旦运行结束，我们可以对结果进行观察：

```txt
>>b matrix([-3.84064413]) 
```

我们可以直接观察alpha矩阵本身，但是其中的零元素太多。为了观察大于0的元素的数量，可以输入如下命令：

```txt
>>> alphas[alphas>0]  
matrix([[0.12735413, 0.24154794, 0.36890208]]) 
```

由于SMO算法的随机性，读者运行后所得到的结果可能会与上述结果不同。alphas[alphas>0]命令是数组过滤（array filtering）的一个实例，而且它只对NumPy类型有用，却并不适用于Python中的正则表（regular list）。如果输入alpha>0，那么就会得到一个布尔数组，并且在不等式成立的情况下，其对应值为正确的。于是，在将该布尔数组应用到原始的矩阵当中时，就会得到一个NumPy矩阵，并且其中矩阵仅仅包含大于0的值。

为了得到支持向量的个数，输入：

```clojure
>> shape (alphas [alphas>0]) 
```

为了解哪些数据点是支持向量，输入：

```python
>>> for i in range(100):
...
...
if alphas[i] > 0.0: print dataArr[i], labelArr[i] 
```

得到的结果类似如下：

.

```json
[4.6581910000000004，3.507396] -1.0  
[3.4570959999999999，-0.082215999999999997] -1.0  
[6.080573000000002，0.41888599999999998] 1.0 
```

在原始数据集上对这些支持向量画圈之后的结果如图6-4所示。

![](images/bb7f01a831ff186ec2bb7cb8fa9c576f5682cdcc37a2d1edb9e18948c3fb73a5.jpg)  
图6-4 示例数据集上运行简化版SMO算法后得到的结果，包括画圈的支持向量与分隔超平面

利用前面的设置，我运行了10次程序并取其平均时间。结果是，这个过程在一台性能较差的笔记本上需要14.5秒。虽然结果看起来并不是太差，但是别忘了这只是一个仅有100个点的小规模数据集而已。在更大的数据集上，收敛时间会变得更长。在下一节中，我们将通过构建完整SMO算法来加快其运行速度。

# 6.4 利用完整PlattSMO算法加速优化

在几百个点组成的小规模数据集上，简化版SMO算法的运行是没有什么问题的，但是在更大的数据集上的运行速度就会变慢。刚才已经讨论了简化版SMO算法，下面我们就讨论完整版的Platt SMO算法。在这两个版本中，实现alpha的更改和代数运算的优化环节一模一样。在优化过程中，唯一的不同就是选择alpha的方式。完整版的Platt SMO算法应用了一些能够提速的启发方法。或许读者已经意识到，上一节的例子在执行时存在一定的时间提升空间。

Platt SMO算法是通过一个外循环来选择第一个alpha值的，并且其选择过程会在两种方式之间进行交替：一种方式是在所有数据集上进行单遍扫描，另一种方式则是在非边界alpha中实现单

遍扫描。而所谓非边界alpha指的就是那些不等于边界0或C的alpha值。对整个数据集的扫描相当容易，而实现非边界alpha值的扫描时，首先需要建立这些alpha值的列表，然后再对这个表进行遍历。同时，该步骤会跳过那些已知的不会改变的alpha值。

在选择第一个alpha值后，算法会通过一个内循环来选择第二个alpha值。在优化过程中，会通过最大化步长的方式来获得第二个alpha值。在简化版SMO算法中，我们会在选择j之后计算错误率Ej。但在这里，我们会建立一个全局的缓存用于保存误差值，并从中选择使得步长或者说Ei-Ej最大的alpha值。

在讲述改进后的代码之前，我们必须要对上节的代码进行清理。下面的程序清单中包含1个用于清理代码的数据结构和3个用于对E进行缓存的辅助函数。我们可以打开一个文本编辑器，输入如下代码。

程序清单6-3 完整版Platt SMO的支持函数  
```python
class optStruct: def init(self,dataMatIn, classLabels, C, toler): self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = shape(dataMatIn) [0] self.alphas = mat(zeros((self.m,1))) self.b = 0 self.eCache = mat(zeros((self.m,2)))   
def calcEk(oS, k): fXk = float(multiply(oS.alphas,oS.labelMat).T\ (oS.X\*oS.X[k,:].T)) + oS.b Ek = fXk - float(oS.labelMat[k]) return Ek   
def selectJ(i, oS, Ei): maxK = -1; maxDeltaE = 0; Ej = 0 oS.eCache[i] = [1,Ei] validEcacheList = nonzero(oS.eCache[(:,0].A) [0] if (len(validEcacheList)) > 1: for k in validEcacheList: if k == i: continue Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE > maxDeltaE): maxK = k; maxDeltaE = deltaE; Ej = Ek return maxK, Ej else: j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ej   
def updateEk(oS, k): Ek = calcEk(oS, k) oS.eCache[k] = [1,Ek] 
```

首要的事情就是建立一个数据结构来保存所有的重要值，而这个过程可以通过一个对象来完成。这里使用对象的目的并不是为了面向对象的编程，而只是作为一个数据结构来使用对象。在将值传给函数时，我们可以通过将所有数据移到一个结构中来实现，这样就可以省掉手工输入的麻烦了。而此时，数据就可以通过一个对象来进行传递。实际上，当完成其实现时，可以很容易通过Python的字典来完成。但是在访问对象成员变量时，这样做会有更多的手工输入操作，对比一下myObject.x和myObject['x']就可以知道这一点。为达到这个目的，需要构建一个仅包含init方法的optStruct类。该方法可以实现其成员变量的填充。除了增加了一个m×2的矩阵成员变量eCache之外①，这些做法和简化版SMO一模一样。eCache的第一列给出的是eCache是否有效的标志位，而第二列给出的是实际的E值。

对于给定的alpha值，第一个辅助函数calcEk()能够计算E值并返回。以前，该过程是采用内嵌的方式来完成的，但是由于该过程在这个版本的SMO算法中出现频繁，这里必须要将其单独拎出来。

下一个函数selectJ()用于选择第二个alpha或者说内循环的alpha值②。回想一下，这里的目标是选择合适的第二个alpha值以保证在每次优化中采用最大步长。该函数的误差值与第一个alpha值Ei和下标i有关。首先将输入值Ei在缓存中设置成为有效的。这里的有效（valid）意味着它已经计算好了。在eCache中，代码nonzero(oS.eCache[:,0].A)[0]构建出了一个非零表。NumPy函数nonzero()返回了一个列表，而这个列表中包含以输入列表为目录的列表值，当然读者可以猜得到，这里的值并非零。nonzero()语句返回的是非零E值所对应的alpha值，而不是E值本身。程序会在所有的值上进行循环并选择其中使得改变最大的那个值③。如果这是第一次循环的话，那么就随机选择一个alpha值。当然，也存在有许多更复杂的方式来处理第一次循环的情况，而上述做法就能够满足我们的目的。

程序清单6-3的最后一个辅助函数是updateEk()，它会计算误差值并存入缓存当中。在对alpha值进行优化之后会用到这个值。

程序清单6-3中的代码本身的作用并不大，但是当和优化过程及外循环组合在一起时，就能组成强大的SMO算法。

接下来将简单介绍一下用于寻找决策边界的优化例程。打开文本编辑器，添加下列清单中的代码。在前面，读者已经看到过下列代码的另外一种形式。

程序清单6-4 完整Platt SMO算法中的优化例程  
def innerL(i,os): 第二个alpha选择中的启发式方法1  
Ei $=$ calcEk(oS,i)  
if((oS.labelMat[i]\*Ei<-oS.tol）and（oS.alphas[i]<oS.C))or $(\mathrm{O}\mathrm{S}.$ labelMat[i]\*Ei>oS.tol）and（oS.alphas[i] $\rightharpoondown$ 0））：j,Ej $=$ selectJ(i,oS,Ei)  
alphaIold $=$ oS.alphas[i].copy();alphaJold $=$ oS.alphas[j].copy();if（oS.labelMat[i]！=oS.labelMat[j]):L $=$ max(0，oS.alphas[j]-oS.alphas[i])H $=$ min(oS.C，oS.C+oS.alphas[j]-oS.alphas[i])else:L $=$ max(0，oS.alphas[j] $^+$ oS.alphas[i]-oS.C)H $=$ min(oS.C，oS.alphas[j] $^+$ oS.alphas[i])

if $\mathbf{L} = =\mathbf{H}$ ：print"L $= = H$ ；return0   
eta $= 2.0$ \*os.X[i,：]\*os.X[j,:].T-os.X[i,:]\*os.X[i,:].T- os.X[j,:]\*os.X[j,:].T   
if eta $\rightharpoondown$ ：print"eta $\rightharpoonup$ 0"；return0   
os.alphas[j] $- =$ os.labelMat[j]\*(Ei-Ej)/eta   
os.alphas[j] $=$ clipAlpha(os.alphas[j],H,L) 更新误差缓存   
updateEk(OS,j)   
if(abs(os.alphas[j]-alphaJold)<0.00001): print "j not moving enough"; return 0   
os.alphas[i] $+ =$ os.labelMat[j]\*os.labelMat[i]\* (alphaJold - os.alphas[j]) 更新误差缓存   
updateEk(OS,i) b1 $=$ os.b-B-Ei-os.labelMat[i]\*os.alphas[i]-alphaIold)\* os.X[i,:]\*os.X[i,:].T- os.labelMat[j]\*os.x[i]-alphaJold]\*os.X[i,:]\*os.X[j,:].T b2 $=$ os.b-Ej-os.labelMat[i]\*os.alphas[i]-alphaIold)\* os.X[i,:]\*os.X[j,:].T- os.labelMat[j]\*os.x[i]-alphaJold)\*os.X[j,:]\*os.X[j,:].T if(0<os.alphas[i])and（os.C>os.alphas[i]):os.b=b1 elif(0<os.alphas[j])and（os.C>os.alphas[j]）:os.b=b2 else:os.b=(b1+b2)/2.0   
return1 else:return0

程序清单6-4中的代码几乎和程序清单6-2中给出的smoSimple()函数一模一样，但是这里的代码已经使用了自己的数据结构。该结构在参数os中传递。第二个重要的修改就是使用程序清单6-3中的SelectJ()而不是selectJrand()来选择第二个alpha的值①。最后，在alpha值改变时更新Ecache②。程序清单6-5将给出把上述过程打包在一起的代码片段。这就是选择第一个alpha值的外循环。打开文本编辑器将下列代码加入到svmMLiA.py文件中。

# 程序清单6-5 完整版Platt SMO的外循环代码

```python
def smoP(dataMatIn, classLabels, C, tol, maxIter, kTup=(lin', 0)):  
    os = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, tol)  
    iter = 0  
    entireSet = True; alphaPairsChanged = 0  
    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):  
        alphaPairsChanged = 0  
    if entireSet:  
        for i in range(os.m):  
            alphaPairsChanged += innerL(i, os)  
            print "fullSet, iter: %d i:%d, pairs changed %d" %  
    (iter, i, alphaPairsChanged)  
    iter += 1  
else:  
    nonBoundIs = nonzero((os.alphaas.A > 0) * (os.alphaas.A < C)) [0]  
    for i in nonBoundIs:  
        alphaPairsChanged += innerL(i, os)  
        print "non-bound, iter: %d i:%d, pairs changed %d" %  
    (iter, i, alphaPairsChanged)  
    iter += 1  
if entireSet: entireSet = False  
elif (alphaPairsChanged == 0): entireSet = True 
```

```txt
print "iteration number: %d" % iter
return os.b,os.alphas 
```

程序清单6-5给出的是完整版的PlattSMO算法，其输入和函数smoSimple()完全一样。函数一开始构建一个数据结构来容纳所有的数据，然后需要对控制函数退出的一些变量进行初始化。整个代码的主体是while循环，这与smoSimple()有些类似，但是这里的循环退出条件更多一些。当迭代次数超过指定的最大值，或者遍历整个集合都未对任意alpha对进行修改时，就退出循环。这里的maxIter变量和函数smoSimple()中的作用有一点不同，后者当没有任何alpha发生改变时会将整个集合的一次遍历过程计成一次迭代，而这里的一次迭代定义为一次循环过程，而不管该循环具体做了什么事。此时，如果在优化过程中存在波动就会停止，因此这里的做法优于smoSimple()函数中的计数方法。

while循环的内部与smoSimple()中有所不同，一开始的for循环在数据集上遍历任意可能的alpha①。我们通过调用innerL()来选择第二个alpha，并在可能时对其进行优化处理。如果有任意一对alpha值发生改变，那么会返回1。第二个for循环遍历所有的非边界alpha值，也就是不在边界0或c上的值②。

接下来，我们对for循环在非边界循环和完整遍历之间进行切换，并打印出迭代次数。最后程序将会返回常数b和alpha值。

为观察上述执行效果，在Python提示符下输入如下命令：

```txt
>>> dataArr, labelArr =svmMLiA.loadDataSet('testSet.txt')  
>>> b, alphas =svmMLiA.smop(dataArr, labelArr, 0.6, 0.001, 40)  
non-bound, iter: 2 i:54, pairs changed 0  
non-bound, iter: 2 i:55, pairs changed 0  
iteration number: 3  
fullSet, iter: 3 i:0, pairs changed 0  
fullSet, iter: 3 i:1, pairs changed 0  
fullSet, iter: 3 i:2, pairs changed 0 
```

类似地，读者也可以检查b和多个alpha的值。那么，相对于简化版SMO算法，上述方法是否更快？基于前面给出的设置在我自己简陋的笔记本上运行10次算法，然后求平均值，最后得到的结果是0.78秒。而在同样的数据集上，smoSimple()函数平均需要14.5秒。在更大规模的数据集上结果可能更好，另外也存在很多方法可以进一步提升其运行速度。

如果修改容错值结果会怎样？如果改变c的值又如何呢？在6.2节末尾曾经粗略地提到，常数c给出的是不同优化问题的权重。常数c一方面要保障所有样例的间隔不小于1.0，另一方面又要使得分类间隔要尽可能大，并且要在这两方面之间平衡。如果c很大，那么分类器将力图通过分隔超平面对所有的样例都正确分类。这种优化的运行结果如图6-5所示。与图6-4相比，会发现图6-5中的支持向量更多。如果回想一下，就会记得图6-4实际来自于简化版算法，该算法是通过随机的方式选择alpha对的。这种简单的方式也可以工作，但是效果却不如完整版本好，后者覆盖了整个数据集。读者可能还认为选出的支持向量应该始终最接近分隔超平面。给定c的设置，图中画圈的支持向量就给出了满足算法的一种解。如果数据集非线性可分，就会发现支持向量会在超平面附近聚集成团。

![](images/a4b6a9635d8ec3ca4746d09de39e93365bf7efa22c989a28bcd739a6f8a5cc7e.jpg)  
图6-5 在数据集上运行完整版SMO算法之后得到的支持向量，其结果与图6-4稍有不同

读者可能会想，刚才我们花了大量时间来计算那些alpha值，但是如何利用它们进行分类呢？这不成问题，首先必须基于alpha值得到超平面，这也包括了w的计算。下面列出的一个小函数可以用于实现上述任务：

```python
def calcWs (alphas, dataArr, classLabels):
    X = mat(dataArr); labelMat = mat(classLabels).transpose()
    m, n = shape(X)
    w = zeros((n, 1))
    for i in range(m):
        w += multiply(alphas[i] * labelMat[i], X[i, :].T)
    return w 
```

上述代码中最重要的部分是for循环，虽然在循环中实现的仅仅是多个数的乘积。看一下前面计算出的任何一个alpha,就不会忘记大部分alpha值为0。而非零alpha所对应的也就是支持向量。虽然上述for循环遍历了数据集中的所有数据，但是最终起作用只有支持向量。由于对w计算毫无作用，所以数据集的其他数据点也就会很容易地被舍弃。

为了使用前面给出的函数，输入如下命令：

```txt
>>> ws = smMLiA.calcWs (alphas, dataArr, labelArr)  
>>> ws  
array([[0.65307162], [-0.17196128]]) 
```

现在对数据进行分类处理，比如对说第一个数据点分类，可以这样输入：

>>>mat $\equiv$ mat(dataArr)   
>>>mat[0]\*mat(ws)+b   
matrix([-0.92555695])

如果该值大于0，那么其属于1类；如果该值小于0，那么则属于-1类。对于数据点0，应该得到的类别标签是-1，可以通过如下的命令来确认分类结果的正确性：

```txt
>> labelArr[0] -1.0 
```

还可以继续检查其他数据分类结果的正确性：

```txt
>>> Mat[2] * mat(ws) + b
matrix([[2.30436336]])
>>> labelArr[2]
1.0
>>> Mat[1] * mat(ws) + b
matrix([-1.36706674])
>>> labelArr[1]
-1.0 
```

读者可将该结果与图6-5进行比较以确认其有效性。

我们现在可以成功训练出分类器了，我想指出的就是，这里两个类中的数据点分布在一条直线的两边。看一下图6-1，大概就可以得到两类的分隔线形状。但是，倘若两类数据点分别分布在一个圆的内部和外部，那么会得到什么样的分类面呢？下一节将会介绍一种方法对分类器进行修改，以说明类别区域形状不同情况下的数据集分隔问题。

# 6.5 在复杂数据上应用核函数

考虑图6-6给出的数据，这有点像图6-1的方框C中的数据。前面我们用这类数据来描述非线性可分的情况。显而易见，在该数据中存在某种可以识别的模式。其中一个问题就是，我们能否像线性情况一样，利用强大的工具来捕捉数据中的这种模式？显然，答案是肯定的。接下来，我们就要使用一种称为核函数（kernel）的工具将数据转换成易于分类器理解的形式。本节首先解释核函数的概念，并介绍它们在支持向量机中的使用方法。然后，介绍一种称为径向基函数（radial bias function）的最流行的核函数。最后，将该核函数应用于我们前面得到的分类器。

![](images/90b0a3be8fb22ebdefa199ace1d36e878ab04ac91c3a7528fd9043116611af87.jpg)  
图6-6 这个数据在二维平面中很难用一条直线分隔，不过很明显，这里存在分隔方形点和圆形点的模式

# 6.5.1 利用核函数将数据映射到高维空间

在图6-6中，数据点处于一个圆中，人类的大脑能够意识到这一点。然而，对于分类器而言，它只能识别分类器的结果是大于0还是小于0。如果只在x和y轴构成的坐标系中插入直线进行分类的话，我们并不会得到理想的结果。我们或许可以对圆中的数据进行某种形式的转换，从而得到某些新的变量来表示数据。在这种表示情况下，我们就更容易得到大于0或者小于0的测试结果。在这个例子中，我们将数据从一个特征空间转换到另一个特征空间。在新空间下，我们可以很容易利用已有的工具对数据进行处理。数学家们喜欢将这个过程称之为从一个特征空间到另一个特征空间的映射。在通常情况下，这种映射会将低维特征空间映射到高维空间。

这种从某个特征空间到另一个特征空间的映射是通过核函数来实现的。读者可以把核函数想象成一个包装器（wrapper）或者是接口（interface），它能把数据从某个很难处理的形式转换成为另一个较容易处理的形式。如果上述特征空间映射的说法听起来很让人迷糊的话，那么可以将它想象成为另外一种距离计算的方法。前面我们提到过距离计算的方法。距离计算的方法有很多种，不久我们也将看到，核函数一样具有多种类型。经过空间转换之后，我们可以在高维空间中解决线性问题，这也就等价于在低维空间中解决非线性问题。

SVM优化中一个特别好的地方就是，所有的运算都可以写成内积（inner product，也称点积）的形式。向量的内积指的是两个向量相乘，之后得到单个标量或者数值。我们可以把内积运算替换成核函数，而不必做简化处理。将内积替换成核函数的方式被称为核技巧（kernel trick）或者核“变电”（kernel substation）。

核函数并不仅仅应用于支持向量机，很多其他的机器学习算法也都用到核函数。接下来，我们将要来介绍一个流行的核函数，那就是径向基核函数。

# 6.5.2 径向基核函数

径向基函数是SVM中常用的一个核函数。径向基函数是一个采用向量作为自变量的函数，能够基于向量距离运算输出一个标量。这个距离可以是从 $<0,0>$ 向量或者其他向量开始计算的距离。接下来，我们将会使用到径向基函数的高斯版本，其具体公式为：

$$
k (x, y) = \exp \left(\frac {- \| x - y \| ^ {2}}{2 \sigma^ {2}}\right)
$$

其中， $\sigma$ 是用户定义的用于确定到达率（reach）或者说函数值跌落到0的速度参数。

上述高斯核函数将数据从其特征空间映射到更高维的空间，具体来说这里是映射到一个无穷维的空间。关于无穷维空间，读者目前不需要太担心。高斯核函数只是一个常用的核函数，使用者并不需要确切地理解数据到底是如何表现的，而且使用高斯核函数还会得到一个理想的结果。在上面的例子中，数据点基本上都在一个圆内。对于这个例子，我们可以直接检查原始数据，并意识到只要度量数据点到圆心的距离即可。然而，如果碰到了一个不是这种形式的新数据集，那么我们就会陷入困境。在该数据集上，使用高斯核函数可以得到很好的结果。当然，该函数也可以用于许多其他的数据集，并且也能得到低错误率的结果。

如果在svmMLiA.py文件中添加一个函数并稍做修改，那么我们就能够在已有代码中使用核函数。首先，打开svMLiA.py代码文件并输入函数kernelTrans()。然后，对optStruct类进行修改，得到类似如下程序清单6-6的代码。

程序清单6-6 核转换函数  
```python
def kernelTrans(X, A, kTup):
    m, n = shape(X)
    K = mat(zeros((m, 1)))
    if kTup[0] == 'lin': K = X * A.T
    elif kTup[0] == 'rbf':
        for j in range(m):
            deltaRow = X[j,:] - A
            K[j] = deltaRow * deltaRow.T
            K = exp(K / (-1 * kTup[1] ** 2))
        else:
            raise NameError('Houston We Have a Problem -- \\
That Kernel is not recognized')
    return K
class optStruct:
    def __init__(self, dataMatIn, classLabels, C, tol, kTup):
        self.X = dataMatIn
        self.labelMat = classLabels
        self.C = C
        self.tol = tol
        self.m = shape(dataMatIn) [0]
        self.alphas = mat(zeros((self.m, 1)))
        self.b = 0
        self.eCache = mat(zeros((self.m, 2)))
        self.K = mat(zeros((self.m, self.m)))
        for i in range(self.m):
            self.K[i] = kernelTrans(self.X, self.X[i,:], kTup) 
```

我建议读者最好看一下optStruct类的新版本。除了引入了一个新变量kTup之外，该版本和原来的optStruct一模一样。kTup是一个包含核函数信息的元组，待会儿我们就能看到它的作用了。在初始化方法结束时，矩阵K先被构建，然后再通过调用函数kernelTrans()进行填充。全局的K值只需计算一次。然后，当想要使用核函数时，就可以对它进行调用。这也省去了很多冗余的计算开销。

当计算矩阵K时，该过程多次调用了函数kernelTrans()。该函数有3个输入参数：2个数值型变量和1个元组。元组kTup给出的是核函数的信息。元组的第一个参数是描述所用核函数类型的一个字符串，其他2个参数则都是核函数可能需要的可选参数。该函数首先构建出了一个列向量，然后检查元组以确定核函数的类型。这里只给出了2种选择，但是依然可以很容易地通过添加elif语句来扩展到更多选项。

在线性核函数的情况下，内积计算在“所有数据集”和“数据集中的一行”这两个输入之间展开。在径向基核函数的情况下，在for循环中对于矩阵的每个元素计算高斯函数的值。而在for循环结束之后，我们将计算过程应用到整个向量上去。值得一提的是，在NumPy矩阵中，除法符号意味着对矩阵元素展开计算而不像在MATLAB中一样计算矩阵的逆①。

最后，如果遇到一个无法识别的元组，程序就会抛出异常，因为在这种情况下不希望程序再继续运行，这一点相当重要。

为了使用核函数，先期的两个函数innerL()和calcEk()的代码需要做些修改。修改的结果参见程序清单6-7。本来我并不想这样列出代码，但是重新列出函数的所有代码需要超过90行，我想任何人都不希望这样。读者可以直接从下载的源码中复制代码段，而不必对修改片段进行手工输入。下面列出的就是修改的代码片段。

程序清单6-7 使用核函数时需要对innerL()及calcEk()函数进行的修改

```python
innerL():  
eta = 2.0 * os.K[i, j] - os.K[i, i] - os.K[j, j]  
b1 = os.b - Ei- os.labelMat[i] * (os.alphas[i] - alphaFold) * os.K[i, i] - os.labelMat[j] * (os.alphas[j] - alphaJold) * os.K[i, j]  
b2 = os.b - Ej- os.labelMat[i] * (os.alphas[i] - alphaFold) * os.K[i, j] - os.labelMat[j] * (os.alphas[j] - alphaJold) * os.K[j, j]  
def calcEk(oS, k):  
fXk = float(multiply(os.alphas, os.labelMat).T * os.K(:, k] + os.b)  
Ek = fXk - float(os.labelMat[k])  
return Ek 
```

你已经了解了如何在训练过程中使用核函数，接下来我们就去了解如何在测试过程中使用核函数。

# 6.5.3 在测试中使用核函数

接下来我们将构建一个对图6-6中的数据点进行有效分类的分类器，该分类器使用了径向基核函数。前面提到的径向基函数有一个用户定义的输入 $\sigma$ 。首先，我们需要确定它的大小，然后利用该核函数构建出一个分类器。整个测试函数将如程序清单6-8所示。读者也可以打开一个文本编辑器，并且加入函数testRbf()。

# 程序清单6-8 利用核函数进行分类的径向基测试函数

def testRbf $(\mathrm{k1} = 1.3)$ ：dataArr, labelArr $=$ loadDataSet('testSetRBF.txt')b, alphas $=$ smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1))datMat=mat(dataArr); labelMat $=$ mat.labelArr).transpose()svInd=nonzero(alphas.A>0)[0]sVs=datMat[svInd]labelSV $=$ labelMat[svInd];

```python
print "there are %d Support Vectors" % shape(sVs) [0]  
m, n = shape(matMat)  
errorCount = 0  
for i in range(m):  
    kernelEval = kernelTrans(sVs, mat[i, :, ('rbf', k1))  
    predict = kernelEval.T * multiply.labelSV, alphas[svInd]) + b  
    if sign(predict) != sign.labelArr[i]): errorCount += 1  
print "the training error rate is: %f" % (float(errorCount) / m)  
dataArr, labelArr = loadDataSet('testSetRBF2.txt')  
errorCount = 0  
datMat = mat(dataArr); labelMat = mat LabelsArr).transpose()  
m, n = shape(matMat)  
for i in range(m):  
    kernelEval = kernelTrans(sVs, mat[i, :, ('rbf', k1))  
    predict = kernelEval.T * multiply.labelSV, alphas[svInd]) + b  
    if sign(predict) != sign LabelsArr[i]): errorCount += 1  
print "the test error rate is: %f" % (float(errorCount) / m) 
```

上述代码只有一个可选的输入参数，该输入参数是高斯径向基函数中的一个用户定义变量。整个代码主要是由以前定义的函数集合构成的。首先，程序从文件中读入数据集，然后在该数据集上运行Platt SMO算法，其中核函数的类型为'rbf'。

优化过程结束后，在后面的矩阵数学运算中建立了数据的矩阵副本，并且找出那些非零的alpha值，从而得到所需要的支持向量；同时，也就得到了这些支持向量和alpha的类别标签值。这些值仅仅是需要分类的值。

整个代码中最重要的是for循环开始的那两行，它们给出了如何利用核函数进行分类。首先利用结构初始化方法中使用过的kernelTrans()函数，得到转换后的数据。然后，再用其与前面的alpha及类别标签值求积。其中需要特别注意的另一件事是，在这几行代码中，是如何做到只需要支持向量数据就可以进行分类的。除此之外，其他数据都可以直接舍弃。

与第一个for循环相比，第二个for循环仅仅只有数据集不同，后者采用的是测试数据集。读者可以比较不同的设置在测试集和训练集上表现出的性能。

为测试程序清单6-8的代码，可以在Python提示符下输入命令：

```txt
>>> reload(svmMLiA)
<module 'svmMLiA' from 'svmMLiA.pyc>
>>>svmMLiA.testRbf()
...
fullSet, iter: 11 i:497, pairs changed 0
fullSet, iter: 11 i:498, pairs changed 0
fullSet, iter: 11 i:499, pairs changed 0
iteration number: 12
there are 27 Support Vectors
the training error rate is: 0.030000
the test error rate is: 0.040000 
```

你可以尝试更换不同的k1参数以观察测试错误率、训练错误率、支持向量个数随k1的变化情况。图6-7给出了当k1非常小（=0.1）时的结果。

![](images/9f6092daacc1bb7f41f188f94f2041f6ec8f98f5b19379e8bc947515a039e901.jpg)  
图6-7 在用户自定义参数 $\mathrm{k1} = 0.1$ 时的径向基函数。该参数此时减少了每个支持向量的影响程度，因此需要更多的支持向量

图6-7中共有100个数据点，其中的85个为支持向量。优化算法发现，必须使用这些支持向量才能对数据进行正确分类。这就可能给了读者径向基函数到达率太小的直觉。我们可以通过增加 $\sigma$ 来观察错误率的变化情况。增加 $\sigma$ 之后得到的另一个结果如图6-8所示。

![](images/b0ba257bb7ea81b6106dbdc9dd8cf054349e9fefabc8401f181a329019d65306.jpg)  
图6-8 在用户自定义参数 $\mathrm{k1} = 1.3$ 时的径向基函数。这里的支持向量个数少于图6-7的，而这些支持向量在决策边界周围聚集

同图6-7相比，图6-8中只有27个支持向量，其数目少了很多。这时观察一下函数testRbf()的输出结果就会发现，此时的测试错误率也在下降。该数据集在这个设置的某处存在着最优值。如果降低 $\sigma$ ，那么训练错误率就会降低，但是测试错误率却会上升。

支持向量的数目存在一个最优值。SVM的优点在于它能对数据进行高效分类。如果支持向量太少，就可能会得到一个很差的决策边界（下个例子会说明这一点）；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类方法称为k近邻。

我们可以对SMO算法中的其他设置进行随意地修改或者建立新的核函数。接下来，我们将在一个更大的数据上应用支持向量机，并与以前介绍的一个分类器进行对比。

# 6.6 示例：手写识别问题回顾

考虑这样一个假想的场景。你的老板过来对你说：“你写的那个手写体识别程序非常好，但是它占用的内存太大了。顾客不能通过无线的方式下载我们的应用（在写本书时，无线下载的限制容量为10MB，可以肯定，这将来会成为笑料的。）我们必须在保持其性能不变的同时，使用更少的内存。我呢，告诉了CEO，你会在一周内准备好，但你到底还得多长时间才能搞定这件事？”我不确定你到底会如何回答，但是如果想要满足他们的需求，你可以考虑使用支持向量机。尽管第2章所使用的kNN方法效果不错，但是需要保留所有的训练样本。而对于支持向量机而言，其需要保留的样本少了很多（即只保留支持向量），但是能获得可比的效果。

# 示例：基于SVM的数字识别

(1)收集数据：提供的文本文件。  
(2) 准备数据：基于二值图像构造向量。  
(3) 分析数据：对图像向量进行目测。  
(4)训练算法：采用两种不同的核函数，并对径向基核函数采用不同的设置来运行SMO算法。  
(5) 测试算法：编写一个函数来测试不同的核函数并计算错误率。  
(6)使用算法：一个图像识别的完整应用还需要一些图像处理的知识，这里并不打算深入介绍。

使用第2章中的一些代码和SMO算法，可以构建一个系统去测试手写数字上的分类器。打开svmMLiA.py并将第2章knn.py中的img2vector()函数复制过来。然后，加入程序清单6-9中的代码。

# 程序清单6-9 基于SVM的手写数字识别

```python
def loadImages(dirName):
    from os import listdir
    hwLabels = []
    trainingFileList = listdir(dirName)
    m = len(trainingFileList)
    trainingMat = zeros((m, 1024))
    for i in range(m): 
```

```txt
filestr = trainingFileList[i]  
fileStr = fileNameStr.split’.）[0]  
classNumStr = int(fileStr.split‘_）[0])  
if classNumStr == 9: hwlabels.append(-1)  
else: hwlabels.append(1)  
trainingMat[i,:] = img2vector('%s/%s' % (dirName, fileName))  
return trainingMat, hwlabels  
def testDigits(kTup=('rbf', 10)):  
dataArr, labelArr = loadImages('trainingDigits')  
b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)  
datMat = mat(dataArr); labelMat = mat.labelArr).transpose()  
svInd = nonzero(alphas.A > 0) [0]  
svs = mat[svInd]  
labelSV = labelMat[svInd];  
print "there are %d Support Vectors" % shape(sVs) [0]  
m, n = shape(matMat)  
errorCount = 0  
for i in range(m):  
    kernelEval = kernelTrans(sVs, mat[i,:], kTup)  
    predict = kernelEval.T * multiply.labelSV, alphas[svInd]) + b  
    if sign(predict) != sign.labelArr[i]): errorCount += 1  
print "the training error rate is: %f" % (float(errorCount)/m)  
dataArr, labelArr = loadImages('testDigits')  
errorCount = 0  
datMat = mat(dataArr); labelMat = mat.labelArr).transpose()  
m, n = shape(matMat)  
for i in range(m):  
    kernelEval = kernelTrans(sVs, mat[i,:], kTup)  
    predict = kernelEval.T * multiply.labelSV, alphas[svInd]) + b  
    if sign(predict) != sign.labelArr[i]): errorCount += 1  
print "the test error rate is: %f" % (float(errorCount)/m) 
```

函数loadImages()是作为前面kNN.py中的handwritingClassTest()的一部分出现的。它已经被重构为自身的一个函数。其中仅有的一个大区别在于，在kNN.py中代码直接应用类别标签，而同支持向量机一起使用时，类别标签为-1或者+1。因此，一旦碰到数字9，则输出类别标签-1，否则输出+1。本质上，支持向量机是一个二类分类器，其分类结果不是+1就是-1。基于SVM构建多类分类器已有很多研究和对比了，如果读者感兴趣，建议阅读C.W.Huset等人发表的一篇论文“A Comparison of Methods for Multiclass Support Vector Machines”<sup>①</sup>。由于这里我们只做二类分类，因此除了1和9之外的数字都被去掉了。

下一个函数testDigits()并不是全新的函数，它和testRbf()的代码几乎一样，唯一的大区别就是它调用了loadImages()函数来获得类别标签和数据。另一个细小的不同是现在这里的函数元组kTup是输入参数，而在testRbf()中默认的就是使用rbf核函数。如果对于函数testDigits()不增加任何输入参数的话，那么kTup的默认值就是('rbf',10)。

输入程序清单6-9中的代码之后，将之保存为svmMLiA.py并输入如下命令：

>>>svmMLiA.testDigits('rbf'，20) $\mathrm{L} = = \mathrm{H}$ fullSet，iter:3i:401，pairs changed 0   
iteration number:4   
there are 43 Support Vectors   
the training error rate is:0.017413   
the test error rate is:0.032258

我尝试了不同的 $\sigma$ 值，并尝试了线性核函数，总结得到的结果如表6-1所示。

表6-1 不同σ值的手写数字识别性能  

<table><tr><td>内核,设置</td><td>训练错误率(%)</td><td>测试错误率(%)</td><td>支持向量数</td></tr><tr><td>RBF,0.1</td><td>0</td><td>52</td><td>402</td></tr><tr><td>RBF,5</td><td>0</td><td>3.2</td><td>402</td></tr><tr><td>RBF,10</td><td>0</td><td>0.5</td><td>99</td></tr><tr><td>RBF,50</td><td>0.2</td><td>2.2</td><td>41</td></tr><tr><td>RBF,100</td><td>4.5</td><td>4.3</td><td>26</td></tr><tr><td>Linear</td><td>2.7</td><td>2.2</td><td>38</td></tr></table>

表6-1给出的结果表明，当径向基核函数中的参数σ取10左右时，就可以得到最小的测试错误率。该参数值比前面例子中的取值大得多，而前面的测试错误率在1.3左右。为什么差距如此之大？原因就在于数据的不同。在手写识别的数据中，有1024个特征，而这些特征的值有可能高达1.0。而在6.5节的例子中，所有数据从-1到1变化，但是只有2个特征。如何才能知道该怎么设置呢？说老实话，在写这个例子时我也不知道。我只是对不同的设置进行了多次尝试。C的设置也会影响到分类的结果。当然，存在另外的SVM形式，它们把C同时考虑到了优化过程中，例如v-SVM。有关v-SVM的一个较好的讨论可以参考本书第3章介绍过的Sergios Theodoridis和Konstantinos Koutroumbas撰写的Pattern Recognition①。

你可能注意到了一个有趣的现象，即最小的训练错误率并不对应于最小的支持向量数目。另一个值得注意的就是，线性核函数的效果并不是特别的糟糕。可以以牺牲线性核函数的错误率来换取分类速度的提高。尽管这一点在实际中是可以接受的，但是还得取决于具体的应用。

# 6.7 本章小结

支持向量机是一种分类器。之所以称为“机”是因为它会产生一个二值决策结果，即它是一种决策“机”。支持向量机的泛化错误率较低，也就是说它具有良好的学习能力，且学到的结果具有很好的推广性。这些优点使得支持向量机十分流行，有些人认为它是监督学习中最好的定式算法。

支持向量机试图通过求解一个二次优化问题来最大化分类间隔。在过去，训练支持向量机常采用非常复杂并且低效的二次规划求解方法。John Platt引入了SMO算法，此算法可以通过每次只优化2个alpha值来加快SVM的训练速度。本章首先讨论了一个简化版本所实现的SMO优化过程，

接着给出了完整的Platt SMO算法。相对于简化版而言，完整版算法不仅大大地提高了优化的速度，还使其存在一些进一步提高运行速度的空间。有关这方面的工作，一个经常被引用的参考文献就是“Improvements to Platt's SMO Algorithm for SVM Classifier Design”①。

核方法或者说核技巧会将数据（有时是非线性数据）从一个低维空间映射到一个高维空间，可以将一个在低维空间中的非线性问题转换成高维空间下的线性问题来求解。核方法不止在SVM中适用，还可以用于其他算法中。而其中的径向基函数是一个常用的度量两个向量距离的核函数。

支持向量机是一个二类分类器。当用其解决多类问题时，则需要额外的方法对其进行扩展。SVM的效果也对优化参数和所用核函数中的参数敏感。

下一章将通过介绍一个称为boosting的方法来结束我们有关分类的介绍。读者不久就会看到，在boosting和SVM之间存在着许多相似之处。

# 第7章

# 利用AdaBoost元算法提高分类性能

# 本章内容

□ 组合相似的分类器来提高分类性能  
□应用AdaBoost算法  
□ 处理非均衡分类问题

当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。机器学习处理问题时又何尝不是如此？这就是元算法（meta-algorithm）背后的思路。元算法是对其他算法进行组合的一种方式。接下来我们将集中关注一个称作AdaBoost的最流行的元算法。由于某些人认为AdaBoost是最好的监督学习的方法，所以该方法是机器学习工具箱中最强有力的工具之一。

本章首先讨论不同分类器的集成方法，然后主要关注boosting方法及其代表分类器AdaBoost。再接下来，我们就会建立一个单层决策树（decision stump）分类器。实际上，它是一个单节点的决策树。AdaBoost算法将应用在上述单层决策树分类器之上。我们将在一个难数据集上应用AdaBoost分类器，以了解该算法是如何迅速超越其他分类器的。

最后，在结束分类话题之前，我们将讨论所有分类器都会遇到的一个通用问题：非均衡分类问题。当我们试图对样例数目不均衡的数据进行分类时，就会遇到这个问题。信用卡使用中的欺诈检测就是非均衡问题中的一个极好的例子，此时我们可能会对每一个正例样本都有1000个反例样本。在这种情况下，分类器将如何工作？读者将会了解到，可能需要利用修改后的指标来评价分类器的性能。而就这个问题而言，并非AdaBoost所独用，只是因为这是分类的最后一章，因此到了讨论这个问题的最佳时机。

# 7.1 基于数据集多重抽样的分类器

前面已经介绍了五种不同的分类算法，它们各有优缺点。我们自然可以将不同的分类器组合起来，而这种组合结果则被称为集成方法（ensemble method）或者元算法（meta-algorithm）。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一算法在不同设置下的集成，

还可以是数据集不同部分分配给不同分类器之后的集成。接下来，我们将介绍基于同一种分类器多个不同实例的两种计算方法。在这些方法当中，数据集也会不断变化，而后应用于不同的实例分类器上。最后，我们会讨论如何利用机器学习问题的通用框架来应用AdaBoost算法。

# AdaBoost

优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。

缺点：对离群点敏感。

适用数据类型：数值型和标称型数据。

# 7.1.1 bagging：基于数据随机重抽样的分类器构建方法

自举汇聚法（bootstrap aggregating），也称为bagging方法，是在从原始数据集选择S次后得到S个新数据集的一种技术。新数据集和原数据集的大小相等。每个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的①。这里的替换就意味着可以多次地选择同一样本。这一性质就允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中则不再出现。

在 $S$ 个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了 $S$ 个分类器。当我们要对新数据进行分类时，就可以应用这 $S$ 个分类器进行分类。与此同时，选择分类器投票结果中最多的类别作为最后的分类结果。

当然，还有一些更先进的bagging方法，比如随机森林（random forest）。有关这些方法的一个很好的讨论材料参见网页http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm。接下来我们将注意力转向一个与bagging类似的集成分类器方法boosting。

# 7.1.2 boosting

boosting是一种与bagging很类似的技术。不论是在boosting还是bagging当中，所使用的多个分类器的类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。

由于boosting分类的结果是基于所有分类器的加权求和结果的，因此boosting与bagging不太一样。bagging中的分类器权重是相等的，而boosting中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。

boosting方法拥有多个版本，本章将只关注其中一个最流行的版本AdaBoost。

# AdaBoost的一般流程

(1)收集数据：可以使用任意方法。  
(2) 准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。当然也可以使用任意分类器作为弱分类器，第2章到第6章中的任一分类器都可以充当弱分类器。作为弱分类器，简单分类器的效果更好。  
(3) 分析数据：可以使用任意方法。  
(4) 训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器。  
(5) 测试算法：计算分类的错误率。  
(6) 使用算法：同SVM一样，AdaBoost预测两个类别中的一个。如果想把它应用到多个类别的场合，那么就要像多类SVM中的做法一样对AdaBoost进行修改。

下面我们将要讨论AdaBoost背后的一些理论，并揭示其效果不错的原因。

# 7.2 训练算法：基于错误提升分类器的性能

能否使用弱分类器和多个实例来构建一个强分类器？这是一个非常有趣的理论问题。这里的“弱”意味着分类器的性能比随机猜测要略好，但是也不会好太多。这就是说，在二分类情况下弱分类器的错误率会高于 $50\%$ ，而“强”分类器的错误率将会低很多。AdaBoost算法即脱胎于上述理论问题。

AdaBoost是adaptive boosting（自适应boosting）的缩写，其运行过程如下：训练数据中的每个样本，并赋予其一个权重，这些权重构成了向量 $D$ 。一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。其中，错误率 $\varepsilon$ 的定义为：

$$
\varepsilon = \frac {\text {未 正 确 分 类 的 样 本 数 目}}{\text {所 有 样 本 数 目}}
$$

而alpha的计算公式如下：

$$
\alpha = \frac {1}{2} \ln \left(\frac {1 - \varepsilon}{\varepsilon}\right)
$$

AdaBoost算法的流程如图7-1所示。

![](images/084cb58964124f092b9cb42cb4439f7194ebbe038740b85fbab2701918893c06.jpg)  
图7-1 AdaBoost算法的示意图。左边是数据集，其中直方图的不同宽度表示每个样例上的不同权重。在经过一个分类器之后，加权的预测结果会通过三角形中的alpha值进行加权。每个三角形中输出的加权结果在圆形中求和，从而得到最终的输出结果

计算出alpha值之后，可以对权重向量 $D$ 进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。 $D$ 的计算方法如下。

如果某个样本被正确分类，那么该样本的权重更改为：

$$
D _ {i} ^ {(t + 1)} = \frac {D _ {i} ^ {(t)} \mathrm {e} ^ {- \alpha}}{\operatorname {S u m} (D)}
$$

而如果某个样本被错分，那么该样本的权重更改为：

$$
D _ {i} ^ {(t + 1)} = \frac {D _ {i} ^ {(t)} \mathrm {e} ^ {\alpha}}{\operatorname {S u m} (D)}
$$

在计算出 $D$ 之后，AdaBoost又开始进入下一轮迭代。AdaBoost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或者弱分类器的数目达到用户的指定值为止。

接下来，我们将建立完整的AdaBoost算法。在这之前，我们首先必须通过一些代码来建立弱分类器及保存数据集的权重。

# 7.3 基于单层决策树构建弱分类器

单层决策树（decision stump，也称决策树桩）是一种简单的决策树。前面我们已经介绍了决策树的工作原理，接下来将构建一个单层决策树，而它仅基于单个特征来做决策。由于这棵树只有一次分裂过程，因此它实际上就是一个树桩。

在构造AdaBoost的代码时，我们将首先通过一个简单数据集来确保在算法实现上一切就绪。然后，建立一个叫adaboost.py的新文件并加入如下代码：

```python
def loadSimpData():
    mat = matrix([[1.，2.1],
[2.，1.1],
[1.3，1.],
[1.，1.],
[2.，1.])
]) 
classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]
return mat, classLabels
```

图7-2给出了上述数据集的示意图。如果想要试着从某个坐标轴上选择一个值（即选择一条与坐标轴平行的直线）来将所有的圆形点和方形点分开，这显然是不可能的。这就是单层决策树难以处理的一个著名问题。通过使用多棵单层决策树，我们就可以构建出一个能够对该数据集完全正确分类的分类器。

![](images/248d199b0acc9ffcbb556a5097f799269c309f7904d48e740dd5df1dec23e177.jpg)  
图7-2 用于检测AdaBoost构建函数的简单数据。这不可能仅仅通过在某个坐标轴上选择某个阈值来将圆形点和方形点分开。AdaBoost需要将多个单层决策树组合起来才能对该数据集进行正确分类

通过键入如下命令可以实现数据集和类标签的导入：

```txt
>>> import adaboost
>>> matplotlib classLabels = adaboost.loadSimpData() 
```

有了数据，接下来就可以通过构建多个函数来建立单层决策树。

第一个函数将用于测试是否有某个值小于或者大于我们正在测试的阈值。第二个函数则更加复杂一些，它会在一个加权数据集中循环，并找到具有最低错误率的单层决策树。

这个程序的伪代码看起来大致如下：

将最小错误率minError设为 $+\infty$

对数据集中的每一个特征（第一层循环）：

对每个步长（第二层循环）：

对每个不等号（第三层循环）：

建立一棵单层决策树并利用加权数据集对它进行测试

如果错误率低于minError，则将当前单层决策树设为最佳单层决策树

返回最佳单层决策树

接下来，我们开始构建这个函数。将程序清单7-1中的代码输入到boost.py中并保存文件。

# 程序清单7-1 单层决策树生成函数

```python
def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):
    retArray = ones((shape(dataMatrix) [0], 1))
    if threshIneq == 'lt':
        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0
    else:
        retArray[dataMatrix[:, dimen] > threshVal] = -1.0
    return retArray
def buildStump(dataArr, classLabels, D):
    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T
    m, n = shape(dataMatrix)
    numSteps = 10.0; bestStump = {}; bestClsEst = mat(zeros((m, 1)))
    minError = inf
    for i in range(n):
        rangeMin = dataMatrix[:, i].min(); rangeMax = dataMatrix[:, i].max();
        stepSize = (rangeMax-rangeMin)/numSteps
        for j in range(-1, int(numSteps) + 1):
            for unequal in ['lt', 'gt':
                threshVal = (rangeMin + float(j) * stepSize)
                predictedVals = \
                stumpClassify(dataMatrix, i, threshVal, inequality)
                errArr = mat(ones((m, 1)))
                errArr[predictedVals == labelMat] = 0
                weightedError = D.T*errArr
            #print "split: dim %d, thresh %.2f, thresh inequality: \
            %s, the weighted error is %.3f" %
            (i, threshVal, inequality, weightedError)
        if weightedError < minError:
            minError = weightedError
            bestClsEst = predictedVals.copy()
            bestStump['dim'] = i
            bestStump['thresh'] = threshVal
            bestStump['ineq'] = unequal
    return bestStump,minError, bestClsEst 
```

上述程序包含两个函数。第一个函数 stumpClassify() 是通过阈值比较对数据进行分类的。所有在阈值一边的数据会分到类别-1，而在另外一边的数据分到类别+1。该函数可以通过数组过滤来实现，首先将返回数组的全部元素设置为1，然后将所有不满足不等式要求的元素设置为-1。可以基于数据集中的任一元素进行比较，同时也可以将不等号在大于、小于之间切换。

第二个函数buildStump()将会遍历 stumpClassify()函数所有的可能输入值，并找到数据集上最佳的单层决策树。这里的“最佳”是基于数据的权重向量D来定义的，读者很快就会看到其具体定义了。在确保输入数据符合矩阵格式之后，整个函数就开始执行了。然后，函数将构建一个称为bestStump的空字典，这个字典用于存储给定权重向量D时所得到的最佳单层决策树的相关信息。变量numSteps用于在特征的所有可能值上进行遍历。而变量minError则在一开始就初始化成正无穷大，之后用于寻找可能的最小错误率。

三层嵌套的for循环是程序最主要的部分。第一层for循环在数据集的所有特征上遍历。考虑到数值型的特征，我们就可以通过计算最小值和最大值来了解应该需要多大的步长。然后，第二层for循环再在这些值上遍历。甚至将阈值设置为整个取值范围之外也是可以的。因此，在取值范围之外还应该有两个额外的步骤。最后一个for循环则是在大于和小于之间切换不等式。

在嵌套的三层for循环之内，我们在数据集及三个循环变量上调用 stumpClassify()函数。基于这些循环变量，该函数将会返回分类预测结果。接下来构建一个列向量errArr，如果 predictedVals中的值不等于labelMat中的真正类别标签值，那么errArr的相应位置为1。将错误向量errArr和权重向量D的相应元素相乘并求和，就得到了数值weightedError①。这就是AdaBoost和分类器交互的地方。这里，我们是基于权重向量D而不是其他错误计算指标来评价分类器的。如果需要使用其他分类器的话，就需要考虑D上最佳分类器所定义的计算过程。

程序接下来输出所有的值。虽然这一行后面可以注释掉，但是它对理解函数的运行还是很有帮助的。最后，将当前的错误率与已有的最小错误率进行对比，如果当前的值较小，那么就在词典bestStump中保存该单层决策树。字典、错误率和类别估计值都会返回给AdaBoost算法。

为了解实际运行过程，在Python提示符下输入如下命令：

```txt
>>> D = mat(ones((5,1))/5)
>>> adaboost.buildStump(matMat,labels,D)
split: dim 0, thresh 0.90, thresh inequality: lt, the weighted error is 0.400
split: dim 0, thresh 0.90, thresh inequality: gt, the weighted error is 0.600
split: dim 0, thresh 1.00, thresh inequality: lt, the weighted error is 0.400
split: dim 0, thresh 1.00, thresh inequality: gt, the weighted error is 0.600
...
split: dim 1, thresh 2.10, thresh inequality: lt, the weighted error is 0.600
split: dim 1, thresh 2.10, thresh inequality: gt, the weighted error is 0.400
({'dim': 0, 'ineq': 'lt', 'thresh': 1.3}, matrix([[0.2]]), array([-1.],
[1.],
[-1.],
[-1.],
[1.])}) 
```

buildStump在所有可能的值上遍历的同时，我们也可以看到输出的结果，并且最后会看到返回的字典。读者可以思考一下，该词典是否对应了最小可能的加权错误率？是否存在其他的设计也能得到相同的错误率？

上述单层决策树的生成函数是决策树的一个简化版本。它就是所谓的弱学习器，即弱分类算法。到现在为止，我们已经构建了单层决策树，并生成了程序，做好了过渡到完整AdaBoost算法的准备。在下一节当中，我们将使用多个弱分类器来构建AdaBoost代码。

# 7.4 完整 AdaBoost 算法的实现

在上一节，我们构建了一个基于加权输入值进行决策的分类器。现在，我们拥有了实现一个完整AdaBoost算法所需要的所有信息。我们将利用7.3节构建的单层决策树来实现7.2节中给出提纲的算法。

整个实现的伪代码如下：

对每次迭代：

利用buildStump()函数找到最佳的单层决策树

将最佳单层决策树加入到单层决策树数组

计算alpha

计算新的权重向量 $D$

更新累计类别估计值

如果错误率等于0.0，则退出循环

为了将该函数放入Python中，打开adaboost.py文件并将程序清单7-2的代码加入其中。

程序清单7-2 基于单层决策树的AdaBoost训练过程  
```python
def adaBoostTrainDS(dataArr, classLabels, numIt=40):
    weakClassArr = []
    m = shape(dataArr) [0]
    D = mat(ones((m, 1)) / m)
    aggClassEst = mat(zeros((m, 1)))
    for i in range(numIt):
        bestStump, error, classEst = buildStump(dataArr, classLabels, D)
        print "D:", D.T
        alpha = float(0.5*log((1.0-error)/max(error, 1e-16)))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
        print "classEst:", classEst.T
        expon = multiply(-1*alpha*mat(classLabels).T, classEst)
        D = multiply(D, exp(expon))
        D = D/D.sum()
        aggClassEst += alpha*classEst
        print "aggClassEst:", aggClassEst.T
        aggErrors = multiply(sign(aggClassEst)) != mat(classLabels).T, ones((m, 1)))
        errorRate = aggErrors-sum()/m
        print "total error:", errorRate,"\n"
        if errorRate == 0.0: break
    return weakClassArr
>>> classifierArray = adaboost.adaboostTrainDS(matMat, classLabels, 9)
D: [[0.2 0.2 0.2 0.2 0.2]]
classEst: [[-1. 1. -1. -1. 1.]]
aggClassEst: [[-0.69314718 0.69314718 -0.69314718 -0.69314718
0.69314718]] 
```

```txt
D:[[0.5 0.125 0.125 0.125 0.125]]  
classEst: [[1. 1. -1. -1. -1.]]  
aggClassEst: [[0.27980789 1.66610226 -1.66610226 -1.66610226 -0.27980789]]  
total error: 0.2  
D:[[0.28571429 0.07142857 0.07142857 0.07142857 0.5 ]]  
classEst: [[1. 1. 1. 1. 1.]]  
aggClassEst: [[1.17568763 2.56198199 -0.77022252 -0.77022252 0.61607184]]  
total error: 0.0 
```

AdaBoost算法的输入参数包括数据集、类别标签以及迭代次数numIt，其中numIt是在整个AdaBoost算法中唯一需要用户指定的参数。

我们假定迭代次数设为9，如果算法在第三次迭代之后错误率为0，那么就会退出迭代过程，因此，此时就不需要执行所有的9次迭代过程。每次迭代的中间结果都会通过print语句进行输出。后面，读者可以把print输出语句注释掉，但是现在可以通过中间结果来了解AdaBoost算法的内部运行过程。

函数名称尾部的DS代表的就是单层决策树（decision stump），它是AdaBoost中最流行的弱分类器，当然并非唯一可用的弱分类器。上述函数确实是建立于单层决策树之上的，但是我们也可以很容易对此进行修改以引入其他基分类器。实际上，任意分类器都可以作为基分类器，本书前面讲到的任何一个算法都行。上述算法会输出一个单层决策树的数组，因此首先需要建立一个新的Python表来对其进行存储。然后，得到数据集中的数据点的数目 $m$ ，并建立一个列向量 $D$ 。

向量 $D$ 非常重要，它包含了每个数据点的权重。一开始，这些权重都赋予了相等的值。在后续的迭代中，AdaBoost算法会在增加错分数据的权重的同时，降低正确分类数据的权重。 $D$ 是一个概率分布向量，因此其所有的元素之和为1.0。为了满足此要求，一开始的所有元素都会被初始化成 $1 / \mathfrak{m}$ 。同时，程序还会建立另一个列向量aggClassEst，记录每个数据点的类别估计累计值。

AdaBoost算法的核心在于for循环，该循环运行numIt次或者直到训练错误率为0为止。循环中的第一件事就是利用前面介绍的buildStump()函数建立一个单层决策树。该函数的输入为权重向量 $D$ ，返回的则是利用D而得到的具有最小错误率的单层决策树，同时返回的还有最小的错误率以及估计的类别向量。

接下来，需要计算的则是alpha值。该值会告诉总分类器本次单层决策树输出结果的权重。其中的语句max(error, 1e-16)用于确保在没有错误时不会发生除零溢出。而后，alpha值加入到bestStump字典中，该字典又添加到列表中。该字典包括了分类所需要的所有信息。

接下来的三行①则用于计算下一次迭代中的新权重向量 $D$ 。在训练错误率为0时，就要提前结束for循环。此时程序是通过aggClassEst变量保持一个运行时的类别估计值来实现的②。该值只是一个浮点数，为了得到二值分类结果还需要调用sign()函数。如果总错误率为0，则由break语句中止for循环。

接下来我们观察一下中间的运行结果。还记得吗，数据的类别标签为[1.0, 1.0, -1.0, -1.0, 1.0]。在第一轮迭代中， $D$ 中的所有值都相等。于是，只有第一个数据点被错分了。因此在第二轮迭代

中， $D$ 向量给第一个数据点0.5的权重。这就可以通过变量aggClassEst的符号来了解总的类别。第二次迭代之后，我们就会发现第一个数据点已经正确分类了，但此时最后一个数据点却是错分了。 $D$ 向量中的最后一个元素变成0.5，而 $D$ 向量中的其他值都变得非常小。最后，第三次迭代之后aggClassEst所有值的符号和真实类别标签都完全吻合，那么训练错误率为0，程序就此退出。

为了观察classifierArray的值，键入：

```txt
>>> classifierArray
[{'dim': 0, 'ineq': 'lt', 'thresh': 1.3, 'alpha': 0.69314718055994529}, {'dim': 1, 'ineq': 'lt', 'thresh': 1.0, 'alpha': 0.9729550745276565}, {'dim': 0, 'ineq': 'lt', 'thresh': 0.9000000000000002, 'alpha': 0.89587973461402726}] 
```

该数组包含三部词典，其中包含了分类所需要的所有信息。此时，一个分类器已经构建成功，而且只要我们愿意，随时都可以将训练错误率降到0。那么测试错误率会如何呢？为了观察测试错误率，我们需要编写分类的一些代码。下一节我们将讨论分类。

# 7.5 测试算法：基于AdaBoost的分类

一旦拥有了多个弱分类器以及其对应的alpha值，进行测试就变得相当容易了。在程序清单7-2的adaBoostTrainDS()中，我们实际已经写完了大部分的代码。现在，需要做的就只是将弱分类器的训练过程从程序中抽出来，然后应用到某个具体的实例上去。每个弱分类器的结果以其对应的alpha值作为权重。所有这些弱分类器的结果加权求和就得到了最后的结果。在程序清单7-3中列出了实现这一过程的所有代码。然后，将下列代码添加到adaboost.py中，就可以利用它基于adaboostTrainDS()中的弱分类器对数据进行分类。

# 程序清单7-3 AdaBoost分类函数

def adaClassifydatToClass,classifierArr): dataMatrix $\equiv$ mat的数据ToClass) m $=$ shape(dataMatrix)[0] aggClassEst $\equiv$ mat(zeros((m,1))) for i in range(len(classifierArr)): classEst $\equiv$ stumpClassify(dataMatrix,classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq']) aggClassEst $+ =$ classifierArr[i]['alpha']\*classEst print aggClassEst return sign(aggClassEst)

读者也许可以猜到，上述的adaClassify()函数就是利用训练出的多个弱分类器进行分类的函数。该函数的输入是由一个或者多个待分类样例datToClass以及多个弱分类器组成的数组classifierArr。函数adaClassify()首先将datToClass转换成了一个NumPy矩阵，并且得到datToClass中的待分类样例的个数m。然后构建一个0列向量aggClassEst，这个列向量与adaBoostTrainDS()中的含义一样。

接下来，遍历classifierArr中的所有弱分类器，并基于 stumpClassify()对每个分类器得到一个类别的估计值。在前面构建单层决策树时，我们已经见过了 stumpClassify()函数，

在那里，我们在所有可能的树桩值上进行迭代来得到具有最小加权错误率的单层决策树。而这里我们只是简单地应用了单层决策树。输出的类别估计值乘上该单层决策树的alpha权重然后累加到aggClassEst上，就完成了这一过程。上述程序中加入了一条print语句，以便我们了解aggClassEst每次迭代后的变化结果。最后，程序返回aggClassEst的符号，即如果aggClassEst大于0则返回+1，而如果小于0则返回-1。

我们再看看实际中的运行效果。加入程序清单7-3中的代码之后，在Python提示符下输入：

```html
>>> reload(adaboost) <module 'adaboost' from 'adaboost.py'> 
```

如果没有弱分类器数组，可以输入如下命令：

```txt
>>> datArr, labelArr = adaboost.loadSimpData()
>>> classifierArr = adaboost.adaboostTrainDS(daR, labelArr, 30) 
```

于是，可以输入如下命令进行分类：

```txt
>>> adaboost.adaclassify([0, 0], classifierArr)  
[[-0.69314718]]  
[[-1.66610226]]  
[[-2.56198199]]  
matrix([[[-1.]]]) 
```

可以发现，随着迭代的进行，数据点[0,0]的分类结果越来越强。当然，我们也可以在其他点上进行分类：

```txt
>>> adaboost.adaclassify([[5, 5], [0, 0]], classifierArr)  
[[0.69314718]  
[[-2.56198199]]  
matrix([[1.], [-1.]]) 
```

这两个点的分类结果也会随着迭代的进行而越来越强。在下一节中，我们会将该分类器应用到一个规模更大、难度也更大的真实数据集中。

# 7.6 示例：在一个难数据集上应用 AdaBoost

本节我们将在第4章给出的马病症数据集上应用AdaBoost分类器。在第4章，我们曾经利用Logistic回归来预测患有症病的马是否能够存活。而在本节，我们则想要知道如果利用多个单层决策树和AdaBoost能不能预测得更准。

# 示例：在一个难数据集上的AdaBoost应用

(1)收集数据：提供的文本文件。  
(2) 准备数据：确保类别标签是 $+1$ 和 $-1$ 而非 $1$ 和 $0$ 。  
(3) 分析数据：手工检查数据。  
(4) 训练算法：在数据上，利用adaBoostTrainDS()函数训练出一系列的分类器。

(5) 测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对AdaBoost和Logistic回归的结果进行完全对等的比较。  
(6) 使用算法：观察该例子上的错误率。不过，也可以构建一个Web网站，让驯马师输入马的症状然后预测马是否会死去。

在使用上述程序清单中的代码之前，必须要有向文件中加载数据的方法。一个常见的loadDataset()的程序如下所示。

# 程序清单7-4 自适应数据加载函数

```python
def loadDataSet(fileName):
    numFeat = len(open(fileName).readline().split("\\t")) 
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split("\\t")
        for i in range(numFeat-1):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat, labelMat 
```

之前，读者可能多次见过了上述程序清单中的loadDataSet()函数。在这里，并不必指定每个文件中的特征数目，所以这里的函数与前面的稍有不同。该函数能够自动检测出特征的数目。同时，该函数也假定最后一个特征是类别标签。

将上述代码添加到adaboost.py文件中并且将其保存之后，就可以输入如下命令来使用上述函数：

```html
>>> datArr, labelArr = adaboost.loadDataSet('horseColicTraining2.txt')
>>> classifierArray = adaboost.adaboostTrainDS(daArr, labelArr, 10)
total error: 0.284280936455
total error: 0.284280936455
...
total error: 0.230769230769
>>> testArr, testLabelArr = adaboost.loadDataSet('horseColicTest2.txt')
>>> prediction10 = adaboost AdaClassify(testArr, classifierArray)
To get the number of misclassified examples type in:
>>> errArr = mat(ones((67, 1)))
>>> errArr[prediction10 != mat(testLabelArr).T].sum()
16.0 
```

要得到错误率，只需将上述错分样例的个数除以67即可。

将弱分类器的数目设定为1到10000之间的几个不同数字，并运行上述过程。这时，得到的结果就会如表7-1所示。在该数据集上得到的错误率相当低。如果没忘的话，在第5章中，我们在同一数据集上采用Logistic回归得到的平均错误率为0.35。而采用AdaBoost，得到的错误率就永远不会那么高了。从表中可以看出，我们仅仅使用50个弱分类器，就达到了较高的性能。

表7-1 不同弱分类器数目情况下的AdaBoost测试和分类错误率。该数据集是个难数据集。通常情况下，AdaBoost会达到一个稳定的测试错误率，而并不会随分类器数目的增多而提高  

<table><tr><td>分类器数目</td><td>训练错误率 (%)</td><td>测试错误率 (%)</td></tr><tr><td>1</td><td>0.28</td><td>0.27</td></tr><tr><td>10</td><td>0.23</td><td>0.24</td></tr><tr><td>50</td><td>0.19</td><td>0.21</td></tr><tr><td>100</td><td>0.19</td><td>0.22</td></tr><tr><td>500</td><td>0.16</td><td>0.25</td></tr><tr><td>1000</td><td>0.14</td><td>0.31</td></tr><tr><td>10000</td><td>0.11</td><td>0.33</td></tr></table>

观察表7-1中的测试错误率一栏，就会发现测试错误率在达到了一个最小值之后又开始上升了。这类现象称之为过拟合（overfitting，也称过学习）。有文献声称，对于表现好的数据集，AdaBoost的测试错误率就会达到一个稳定值，并不会随着分类器的增多而上升。或许在本例子中的数据集也称不上“表现好”。该数据集一开始有 $30\%$ 的缺失值，对于Logistic回归而言，这些缺失值的假设就是有效的，而对于决策树却可能并不合适。如果回到数据集，将所有的0值替换成其他值，或者给定类别的平均值，那么能否得到更好的性能？

很多人都认为，AdaBoost和SVM是监督机器学习中最强大的两种方法。实际上，这两者之间拥有不少相似之处。我们可以把弱分类器想象成SVM中的一个核函数，也可以按照最大化某个最小间隔的方式重写AdaBoost算法。而它们的不同就在于其所定义的间隔计算方式有所不同，因此导致的结果也不同。特别是在高维空间下，这两者之间的差异就会更加明显。

在下一节中，我们不再讨论AdaBoost，而是转而关注所有分类器中的一个普遍问题。

# 7.7 非均衡分类问题

在我们结束分类这个主题之前，还必须讨论一个问题。在前面六章的所有分类介绍中，我们都假设所有类别的分类代价是一样的。例如在第5章，我们构建了一个用于检测患疝病的马匹是否存活的系统。在那里，我们构建了分类器，但是并没有对分类后的情形加以讨论。假如某人给我们牵来一匹马，他希望我们能预测这匹马能否生存。我们说马会死，那么他们就可能会对马实施安乐死，而不是通过给马喂药来延缓其不可避免的死亡过程。我们的预测也许是错误的，马本来是可以继续活着的。毕竟，我们的分类器只有 $80\%$ 的精确率（accuracy）。如果我们预测错误，那么我们将会错杀了一个如此昂贵的动物，更不要说人对马还存在情感上的依恋。

如何过滤垃圾邮件呢？如果收件箱中会出现某些垃圾邮件，但合法邮件永远不会扔进垃圾邮件夹中，那么人们是否会满意呢？癌症检测又如何呢？只要患病的人不会得不到治疗，那么再找一个医生来看看会不会更好呢（即情愿误判也不漏判）？

还可以举出很多很多这样的例子，坦白地说，在大多数情况下不同类别的分类代价并不相等。在本节中，我们将会考察一种新的分类器性能度量方法，并通过图像技术来对在上述非均衡问题

下不同分类器的性能进行可视化处理。然后，我们考察这两种分类器的变换算法，它们能够将不同决策的代价考虑在内。

# 7.7.1 其他分类性能度量指标：正确率、召回率及ROC曲线

到现在为止，本书都是基于错误率来衡量分类器任务的成功程度的。错误率指的是在所有测试样例中错分的样例比例。实际上，这样的度量错误掩盖了样例如何被分错的事实。在机器学习中，有一个普遍适用的称为混淆矩阵（confusion matrix）的工具，它可以帮助人们更好地了解分类中的错误。有这样一个关于在房子周围可能发现的动物类型的预测，这个预测的三类问题的混淆矩阵如表7-2所示。

表7-2 一个三类问题的混淆矩阵  

<table><tr><td rowspan="2" colspan="2"></td><td colspan="3">预测结果</td></tr><tr><td>狗</td><td>猫</td><td>鼠</td></tr><tr><td rowspan="3">真实结果</td><td>狗</td><td>24</td><td>2</td><td>5</td></tr><tr><td>猫</td><td>2</td><td>27</td><td>0</td></tr><tr><td>鼠</td><td>4</td><td>2</td><td>30</td></tr></table>

利用混淆矩阵就可以更好地理解分类中的错误了。如果矩阵中的非对角元素均为0，就会得到一个完美的分类器。

接下来，我们考虑另外一个混淆矩阵，这次的矩阵只针对一个简单的二类问题。在表7-3中，给出了该混淆矩阵。在这个二类问题中，如果将一个正例判为正例，那么就可以认为产生了一个真正例（True Positive，TP，也称真阳）；如果对一个反例正确地判为反例，则认为产生了一个真反例（True Negative，TN，也称真阴）。相应地，另外两种情况则分别称为伪反例（False Negative, FN，也称假阴）和伪正例（False Positive，FP，也称假阳）。这4种情况如表7-3所示。

表7-3 一个二类问题的混淆矩阵，其中的输出采用了不同的类别标签  

<table><tr><td rowspan="2" colspan="2"></td><td colspan="2">预测结果</td></tr><tr><td>+1</td><td>-1</td></tr><tr><td rowspan="2">真实结果</td><td>+1</td><td>真正例（TP）</td><td>伪反例（FN）</td></tr><tr><td>-1</td><td>伪正例（FP）</td><td>真反例（TN）</td></tr></table>

在分类中，当某个类别的重要性高于其他类别时，我们就可以利用上述定义来定义出多个比错误率更好的新指标。第一个指标是正确率（Precision），它等于TP/(TP+FP)，给出的是预测为正例的样本中的真正正例的比例。第二个指标是召回率（Recall），它等于TP/(TP+FN)，给出的是预测为正例的真实正例占所有真实正例的比例。在召回率很大的分类器中，真正判错的正例的数目并不多。

我们可以很容易构造一个高正确率或高召回率的分类器，但是很难同时保证两者成立。如果将任何样本都判为正例，那么召回率达到百分之百而此时正确率很低。构建一个同时使正确率和召回率最大的分类器是具有挑战性的。

另一个用于度量分类中的非均衡性的工具是ROC曲线（ROC curve），ROC代表接收者操作特征（receiver operating characteristic），它最早在二战期间由电气工程师构建雷达系统时使用过。图7-3给出了一条ROC曲线的例子。

![](images/0c9bc673b952f8e86527d914ea32bd3c064e42dce5ab2659daad4b2ac419f10e.jpg)  
图7-3 利用10个单层决策树的AdaBoost马疝病检测系统的ROC曲线

在图7-3的ROC曲线中，给出了两条线，一条虚线一条实线。图中的横轴是伪正例的比例（假阳率 $= \mathrm{FP} / (\mathrm{FP} + \mathrm{TN})$ ），而纵轴是真正例的比例（真阳率 $= \mathrm{TP} / (\mathrm{TP} + \mathrm{FN})$ ）。ROC曲线给出的是当阈值变化时假阳率和真阳率的变化情况。左下角的点所对应的是将所有样例判为反例的情况，而右上角的点对应的则是将所有样例判为正例的情况。虚线给出的是随机猜测的结果曲线。

ROC曲线不但可以用于比较分类器，还可以基于成本效益（cost-versus-benefit）分析来做出决策。由于在不同的阈值下，不同的分类器的表现情况可能各不相同，因此以某种方式将它们组合起来或许会更有意义。如果只是简单地观察分类器的错误率，那么我们就难以得到这种更深入的洞察效果了。

在理想的情况下，最佳的分类器应该尽可能地处于左上角，这就意味着分类器在假阳率很低的同时获得了很高的真阳率。例如在垃圾邮件的过滤中，这就相当于过滤了所有的垃圾邮件，但没有将任何合法邮件误识为垃圾邮件而放入垃圾邮件的文件夹中。

对不同的ROC曲线进行比较的一个指标是曲线下的面积（Area Unser the Curve，AUC）。AUC给出的是分类器的平均性能值，当然它并不能完全代替对整条曲线的观察。一个完美分类器的AUC为1.0，而随机猜测的AUC则为0.5。

为了画出ROC曲线，分类器必须提供每个样例被判为阳性或者阴性的可信程度值。尽管大多数分类器都能做到这一点，但是通常情况下，这些值会在最后输出离散分类标签之前被清除。朴

素贝叶斯能够提供一个可能性，而在Logistic回归中输入到Sigmoid函数中的是一个数值。在AdaBoost和SVM中，都会计算出一个数值然后输入到sign()函数中。所有的这些值都可以用于衡量给定分类器的预测强度。为了创建ROC曲线，首先要将分类样例按照其预测强度排序。先从排名最低的样例开始，所有排名更低的样例都被判为反例，而所有排名更高的样例都被判为正例。该情况的对应点为 $<1.0,1.0>$ 。然后，将其移到排名次低的样例中去，如果该样例属于正例，那么对真阳率进行修改；如果该样例属于反例，那么对假阴率进行修改。

上述过程听起来有点容易让人混淆，但是如果阅读一下程序清单7-5中的代码，一切就会变得一目了然了。打开adaboost.py文件并加入如下代码。

程序清单7-5 ROC曲线的绘制及AUC计算函数  
```python
def plotROC(predStrengths, classLabels):
    import matplotlib.pyplot as plt
    cur = (1.0, 1.0)
    ySum = 0.0
    numPosCls = sum(array(classLabels) == 1.0)
    yStep = 1/float(numPosCls)
    xStep = 1/float(len(classLabels) - numPosCls)
    sortedIndices = predStrengths.argsort()
    fig = plt.figure()
    figclf()
    ax = plt.subplot(111)
    for index in sortedIndices.tolist() [0]:
        if classLabels[index] == 1.0:
            delX = 0; delY = yStep;
        else:
            delX = xStep; delY = 0;
            ySum += cur[1]
        ax.plot([cur[0], cur[0] - delX], [cur[1], cur[1] - delY], c='b')
        cur = (cur[0] - delX, cur[1] - delY)
ax.plot([0, 1], [0, 1], 'b--')
plt.xlabel('False Positive Rate'); pltylabel('True Positive Rate')
plt.title('ROC curve for AdaBoost Horse Colic Detection System')
ax-axis([0, 1, 0, 1])
plt.show()
print "the Area Under the Curve is: ", ySum * xStep 
```

上述程序中的函数有两个输入参数，第一个参数是一个NumPy数组或者一个行向量组成的矩阵。该参数代表的则是分类器的预测强度。在分类器和训练函数将这些数值应用到sign()函数之前，它们就已经产生了。尽管很快就可以看到该函数的实际执行效果，但是我们还是要先讨论一下这段代码。函数的第二个输入参数是先前使用过的classLabels。我们首先导入pyplot，然后构建一个浮点数二元组，并将它初始化为(1,0,1.0)。该元组保留的是绘制光标的位置，变量ySum则用于计算AUC的值。接下来，通过数组过滤方式计算正例的数目，并将该值赋给numPosClas。该值先是确定了在y坐标轴上的步进数目，接着我们在x轴和y轴的0.0到1.0区间上绘点，因此y轴上的步长是1.0/numPosClas。类似地，就可以得到x轴的步长了。

接下来，我们得到了排序索引①，但是这些索引是按照最小到最大的顺序排列的，因此我们需要从点 $<1.0, 1.0>$ 开始绘，一直到 $<0, 0>$ 。跟着的三行代码则是用于构建画笔，并在所有排序值

上进行循环。这些值在一个NumPy数组或者矩阵中进行排序，Python则需要一个表来进行迭代循环，因此我们需要调用tolist()方法。当遍历表时，每得到一个标签为1.0的类，则要沿着y轴的方向下降一个步长，即不断降低真阳率。类似地，对于每个其他类别的标签，则是在x轴方向上倒退了一个步长（假阴率方向）。上述代码只关注1这个类别标签，因此就无所谓是采用1/0标签还是+1/-1标签。

为了计算AUC，我们需要对多个小矩形的面积进行累加。这些小矩形的宽度是xStep，因此可以先对所有矩形的高度进行累加，最后再乘以xStep得到其总面积。所有高度的和（ySum）随着x轴的每次移动而渐次增加。一旦决定了是在x轴还是y轴方向上进行移动的，我们就可以在当前点和新点之间画出一条线段。然后，当前点cur更新了。最后，我们就会得到一个像样的绘图并将AUC打印到终端输出。

了解实际运行效果，我们需要将adaboostTrainDS()的最后一行代码替换成：

```txt
returnweakClassArr,aggClassEst 
```

以得到aggClassEst的值。然后，在Python提示符下键入如下命令：

```txt
>>> reload(adaboost)
<module 'adaboost' from 'adaboost.pyc>
>>> datArr, labelArr = adaboost.loadDataSet('horseColicTraining2.txt')
>>> classifierArray, aggClassEst = adaboost.adaboostTrainDS(matArr, labelArr, 10)
>>> adaboost.plotROC(aggClassEst.T, labelArr)
the Area Under the Curve is: 0.858296963506 
```

我们也会了解到和图7-3一样的ROC曲线。这是在10个弱分类器下，AdaBoost算法性能的结果。我们还记得，当初我们在40个弱分类器下得到了最优的分类性能，那么这种情况下的ROC曲线会如何呢？这时的AUC是不是更好呢？

# 7.7.2 基于代价函数的分类器决策控制

除了调节分类器的阈值之外，我们还有一些其他可以用于处理非均匀分类代价问题的方法，其中的一种称为代价敏感的学习（cost-sensitive learning）。考虑表7-4中的代价矩阵，第一张表给出的是到目前为止分类器的代价矩阵（代价不是0就是1）。我们可以基于该代价矩阵计算其总代价： $\mathrm{TP}^* 0 + \mathrm{FN}^* 1 + \mathrm{FP}^* 1 + \mathrm{TN}^* 0$ 。接下来我们考虑下面的第二张表，基于该代价矩阵的分类代价的计算公式为： $\mathrm{TP}^* (-5) + \mathrm{FN}^* 1 + \mathrm{FP}^* 50 + \mathrm{TN}^* 0$ 。采用第二张表作为代价矩阵时，两种分类错误的代价是不一样的。类似地，这两种正确分类所得到的收益也不一样。如果在构建分类器时，知道了这些代价值，那么就可以选择付出最小代价的分类器。

在分类算法中，我们有很多方法可以用来引入代价信息。在AdaBoost中，可以基于代价函数来调整错误权重向量 $D$ 。在朴素贝叶斯中，可以选择具有最小期望代价而不是最大概率的类别作为最后的结果。在SVM中，可以在代价函数中对于不同的类别选择不同的参数c。上述做法就会给较小类更多的权重，即在训练时，小类当中只允许更少的错误。

表7-4 一个二类问题的代价矩阵  

<table><tr><td rowspan="2" colspan="2"></td><td colspan="2">预测结果</td></tr><tr><td>+1</td><td>-1</td></tr><tr><td rowspan="2">真实结果</td><td>+1</td><td>0</td><td>1</td></tr><tr><td>-1</td><td>1</td><td>0</td></tr></table>

<table><tr><td rowspan="2" colspan="2"></td><td colspan="2">预测结果</td></tr><tr><td>+1</td><td>-1</td></tr><tr><td rowspan="2">真实结果</td><td>+1</td><td>-5</td><td>1</td></tr><tr><td>-1</td><td>50</td><td>0</td></tr></table>

# 7.7.3 处理非均衡问题的数据抽样方法

另外一种针对非均衡问题调节分类器的方法，就是对分类器的训练数据进行改造。这可以通过欠抽样（undersampling）或者过抽样（oversampling）来实现。过抽样意味着复制样例，而欠抽样意味着删除样例。不管采用哪种方式，数据都会从原始形式改造为新形式。抽样过程则可以通过随机方式或者某个预定方式来实现。

通常也会存在某个罕见的类别需要我们来识别，比如在信用卡欺诈当中。如前所述，正例类别属于罕见类别。我们希望对于这种罕见类别能尽可能保留更多的信息，因此，我们应该保留正例类别中的所有样例，而对反例类别进行欠抽样或者样例删除处理。这种方法的一个缺点就在于要确定哪些样例需要进行剔除。但是，在选择剔除的样例中可能携带了剩余样例中并不包含的有价值信息。

上述问题的一种解决办法，就是选择那些离决策边界较远的样例进行删除。假定我们有一个数据集，其中有50例信用卡欺诈交易和5000例合法交易。如果我们想要对合法交易样例进行欠抽样处理，使得这两类数据比较均衡的话，那么我们就需要去掉4950个样例，而这些样例中可能包含很多有价值的信息。这看上去有些极端，因此有一种替代的策略就是使用反例类别的欠抽样和正例类别的过抽样相混合的方法。

要对正例类别进行过抽样，我们可以复制已有样例或者加入与已有样例相似的点。一种方法是加入已有数据点的插值点，但是这种做法可能会导致过拟合的问题。

# 7.8 本章小结

集成方法通过组合多个分类器的分类结果，获得了比简单的单分类器更好的分类结果。有一些利用不同分类器的集成方法，但是本章只介绍了那些利用同一类分类器的集成方法。

多个分类器组合可能会进一步凸显出单分类器的不足，比如过拟合问题。如果分类器之间差别显著，那么多个分类器组合就可能会缓解这一问题。分类器之间的差别可以是算法本身或者是应用于算法上的数据的不同。

本章介绍的两种集成方法是bagging和boosting。在bagging中，是通过随机抽样的替换方式，得到了与原始数据集规模一样的数据集。而boosting在bagging的思路上更进了一步，它在数据集上顺序应用了多个不同的分类器。另一个成功的集成方法就是随机森林，但是由于随机森林不如AdaBoost流行，所以本书并没有对它进行介绍。

本章介绍了 boosting方法中最流行的一个称为 AdaBoost 的算法。AdaBoost 以弱学习器作为基分类器，并且输入数据，使其通过权重向量进行加权。在第一次迭代当中，所有数据都等权重。但是在后续的迭代当中，前次迭代中分错的数据的权重会增大。这种针对错误的调节能力正是 AdaBoost 的长处。

本章以单层决策树作为弱学习器构建了AdaBoost分类器。实际上，AdaBoost函数可以应用于任意分类器，只要该分类器能够处理加权数据即可。AdaBoost算法十分强大，它能够快速处理其他分类器很难处理的数据集。

非均衡分类问题是指在分类器训练时正例数目和反例数目不相等（相差很大）。该问题在错分正例和反例的代价不同时也存在。本章不仅考察了一种不同分类器的评价方法——ROC曲线，还介绍了正确率和召回率这两种在类别重要性不同时，度量分类器性能的指标。

本章介绍了通过过抽样和欠抽样方法来调节数据集中的正例和反例数目。另外一种可能更好的非均衡问题的处理方法，就是在训练分类器时将错误的代价考虑在内。

到目前为止，我们介绍了一系列强大的分类技术。本章是分类部分的最后一章，接下来我们将进入另一类监督学习算法——回归方法，这也将完善我们对监督方法的学习。回归很像分类，但是和分类输出标称型类别值不同的是，回归方法会预测出一个连续值。

![](images/176685fb5f8baeccbca8eae11258f93bfd377e9dfed109e2e2ec2492238bd66a.jpg)

# 利用回归预测数值型数据

本书的第二部分由第8章和第9章组成，主要介绍了回归方法。回归是第 $1\sim 7$ 章的监督学习方法的延续。前面说过，监督学习指的是有目标变量或预测目标的机器学习方法。回归与分类的不同，就在于其目标变量是连续数值型。

第8章介绍了线性回归、局部加权线性回归和收缩方法。第9章则借用了第3章树构建的一些思想并将其应用于回归中，从而得到了树回归。

# 本章内容

□ 线性回归   
□局部加权线性回归   
□岭回归和逐步线性回归  
□预测鲍鱼年龄和玩具售价

本书前面的章节介绍了分类，分类的目标变量是标称型数据，而本章将会对连续型的数据做出预测。读者很可能有这样的疑问：“回归能用来做些什么呢？”。我的观点是，回归可以做任何事情。然而大多数公司常常使用回归法做一些比较沉闷的事情，例如销售量预测或者制造缺陷预测。我最近看到一个比较有新意的应用，就是预测名人的离婚率。

本章首先介绍线性回归，包括其名称的由来和Python实现。在这之后引入了局部平滑技术，分析如何更好地拟合数据。接下来，本章将探讨回归在“欠拟合”情况下的缩减（shrinkage）技术，探讨偏差和方差的概念。最后，我们将融合所有技术，预测鲍鱼的年龄和玩具的售价。此外为了获取一些玩具的数据，我们还将使用Python来做一些采集的工作。这一章的内容会十分丰富。

# 8.1 用线性回归找到最佳拟合直线

# 线性回归

优点：结果易于理解，计算上不复杂。

缺点：对非线性的数据拟合不好。

适用数据类型：数值型和标称型数据。

回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。假如你想要预测姐姐男友汽车的功率大小，可能会这么计算：

```txt
HorsePower = 0.0015*annualSalary -0.99*hoursListeningToPublicRadio 
```

这就是所谓的回归方程（regression equation），其中的0.0015和-0.99称作回归系数（regression weights），求这些回归系数的过程就是回归。一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值①。

说到回归，一般都是指线性回归（linear regression），所以本章里的回归和线性回归代表同一个意思。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。需要说明的是，存在另一种称为非线性回归的回归模型，该模型不认同上面的做法，比如认为输出可能是输入的乘积。这样，上面的功率计算公式也可以写做：

HorsePower = 0.0015*annualSalary/hoursListeningToPublicRadio

这就是一个非线性回归的例子，但本章对此不做深入讨论。

# 回归的一般方法

(1)收集数据：采用任意方法收集数据。  
(2) 准备数据：回归需要数值型数据，标称型数据将被转成二值型数据。  
(3) 分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比。  
(4) 训练算法：找到回归系数。   
(5) 测试算法：使用R2或者预测值和数据的拟合度，来分析模型的效果。  
(6) 使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。

# “回归”一词的来历

今天我们所知道的回归是由达尔文(Charles Darwin)的表兄弟Francis Galton发明的。Galton于1877年完成了第一次回归预测，目的是根据上一代豌豆种子（双亲）的尺寸来预测下一代豌豆种子（孩子）的尺寸。Galton在大量对象上应用了回归分析，甚至包括人的身高。他注意到，如果双亲的高度比平均高度高，他们的子女也倾向于比平均高度高，但尚不及双亲。孩子的高度向着平均高度回退（回归）。Galton在多项研究上都注意到这个现象，所以尽管这个英文单词跟数值预测没有任何关系，但这种研究方法仍被称作回归②。

应当怎样从一大堆数据里求出回归方程呢？假定输入数据存放在矩阵 $\mathbf{x}$ 中，而回归系数存放在向量 $\mathbf{w}$ 中。那么对于给定的数据 $\mathbf{x}_1$ ，预测结果将会通过 $\mathbf{Y}_1 = \mathbf{X}^T_1\mathbf{w}$ 给出。现在的问题是，手里有一些 $\mathbf{x}$ 和对应的 $\mathbf{y}$ ，怎样才能找到 $\mathbf{w}$ 呢？一个常用的方法就是找出使误差最小的 $\mathbf{w}$ 。这里的误差是指预测 $\mathbf{y}$ 值和真实 $\mathbf{y}$ 值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用平方误差。

平方误差可以写做：

$$
\sum_ {i = 1} ^ {m} \left(y _ {i} - x _ {i} ^ {\mathrm {T}} w\right) ^ {2}
$$

用矩阵表示还可以写做 $(\mathbf{y} - \mathbf{Xw})^{\mathrm{T}}(\mathbf{y} - \mathbf{Xw})$ 。如果对 $\mathbf{w}$ 求导，得到 $\mathbf{X}^{\mathrm{T}}(\mathbf{Y} - \mathbf{Xw})$ ，令其等于零，解出 $\mathbf{w}$ 如下：

$$
\hat {w} = \left(\boldsymbol {X} ^ {\mathrm {T}} \boldsymbol {X}\right) ^ {- 1} \boldsymbol {X} ^ {\mathrm {T}} \boldsymbol {y}
$$

w上方的小标记表示，这是当前可以估计出的w的最优解。从现有数据上估计出的w可能并不是数据中的真实w值，所以这里使用了一个“帽”符号来表示它仅是w的一个最佳估计。

值得注意的是，上述公式中包含 $\mathbf{x}^{\mathrm{T}}\mathbf{x}^{-1}$ ，也就是需要对矩阵求逆，因此这个方程只在逆矩阵存在的时候适用。然而，矩阵的逆可能并不存在，因此必须要在代码中对此作出判断。

上述的最佳w求解是统计学中的常见问题，除了矩阵方法外还有很多其他方法可以解决。通过调用NumPy库里的矩阵方法，我们可以仅使用几行代码就完成所需功能。该方法也称作OLS，意思是“普通最小二乘法”（ordinary least squares）。

下面看看实际效果，对于图8-1中的散点图，下面来介绍如何给出该数据的最佳拟合直线。

![](images/fe121db173b688366fcdeffb6c83e152c94dd5708083adc9f3686412ee6af960.jpg)  
图8-1 从ex0.txt得到的样例数据

程序清单8-1可以完成上述功能。打开文本编辑器并创建一个新的文件regression.py，添加其中的代码。

程序清单8-1 标准回归函数和数据导入函数  
from numpy import *   
def loadDataSet(fileName): numFeat = len(open(fileName).readline().split("\\t")) - 1 dataMat $=$ []; labelMat $=$ [] fr $=$ open(fileName) for line in fr.readlines(): lineArr $\coloneqq$ [] curLine $=$ line.strip().split("\\t") for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat, labelMat   
def standRegres(xArr,yArr): xMat $=$ mat(xArr); yMat $=$ mat(yArr).T xTx $=$ xMat.T\*xMat if linalg det(xTx) $= = 0.0$ : print "This matrix is singular, cannot do inverse" return ws $=$ xTx.I \* (xMat.T\*yMat) return ws

第一个函数loadDataSet()与第7章的同名函数是一样的。该函数打开一个用tab键分隔的文本文件，这里仍然默认文件每行的最后一个值是目标值。第二个函数standRegres()用来计算最佳拟合直线。该函数首先读入x和y并将它们保存到矩阵中；然后计算 $\mathbf{x}^{\mathrm{T}}\mathbf{x}$ ，然后判断它的行列式是否为零，如果行列式为零，那么计算逆矩阵的时候将出现错误。NumPy提供一个线性代数的库linalgg，其中包含很多有用的函数。可以直接调用linalg.g det()来计算行列式。最后，如果行列式非零，计算并返回w。如果没有检查行列式是否为零就试图计算矩阵的逆，将会出现错误。NumPy的线性代数库还提供一个函数来解未知矩阵，如果使用该函数，那么代码ws=xTx.I *(xMat.T*yMat)应写成ws=linalg.solve(xTx, xMat.T*yMatT)。

下面看看实际效果，使用loadDataSet()将从数据中得到两个数组，分别存放在x和y中。与分类算法中的类别标签类似，这里的y是目标值。

首先看前两条数据：  
```python
>>> import regression
>>> from numpy import *
>>> xArr, yArr = regression.loadDataSet('ex0.txt') 
```

```json
>> xArr[0:2]  
[[1.0, 0.067732000000000001], [1.0, 0.4278100000000002]] 
```

第一个值总是等于1.0，即x0。我们假定偏移量就是一个常数。第二个值x1，也就是我们图中的横坐标值。

现在看一下standRegres()函数的执行效果：  
>>ws $\equiv$ regressionstandRegres(xArr,yArr)   
>>ws   
matrix([[3.00774324]， [1.69532264]]）

变量ws存放的就是回归系数。在用内积来预测y的时候，第一维将乘以前面的常数x0，第二维将乘以输入变量x1。因为前面假定了 $\mathbf{x}0 = 1$ ，所以最终会得到 $\mathrm{y = ws[0] + ws[1]*x1}$ 。这里的y实际是预测出的，为了和真实的y值区分开来，我们将它记为yHat。下面使用新的ws值计算yHat：

>>xMat $\equiv$ mat(xArr)   
>>yMat $\equiv$ mat(yArr)   
>>yHat $=$ xMat\*ws

现在就可以绘出数据集散点图和最佳拟合直线图：

```txt
>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> axscatter(xMat(:,1]. flatten().A[0], yMat.T(:,0]. flatten().A[0])
<matplotlibcollections.CircleCollection object at 0x04ED9D30> 
```

上述命令创建了图像并绘出了原始的数据。为了绘制计算出的最佳拟合直线，需要绘出yHat的值。如果直线上的数据点次序混乱，绘图时将会出现问题，所以首先要将点按照升序排列：

```txt
>>> xCopy = xMat.copy()
>>> xCopy.sort(0)
>>> yHat = xCopy * ws
>>> ax.plot(xCopy[:, 1], yHat)
[<matplotlib.axes.Line2D object at 0x0343F570>] >> plt.show() 
```

我们将会看到类似于图8-2的效果图。

![](images/a43985b0d55368b7b231d84f10965b1991167f9b8c0fee4aa16e8569bca8ee87.jpg)  
图8-2 ex0.txt的数据集与它的最佳拟合直线

几乎任一数据集都可以用上述方法建立模型，那么，如何判断这些模型的好坏呢？比较一下图8-3的两个子图，如果在两个数据集上分别作线性回归，将得到完全一样的模型（拟合直线）。显然两个数据是不一样的，那么模型分别在二者上的效果如何？我们当如何比较这些效果的好坏呢？有种方法可以计算预测值yHat序列和真实值y序列的匹配程度，那就是计算这两个序列的相关系数。

![](images/f0b35fe8037f5fcf54b034651ac953de221d03a6f73271fdb8c6f4f42df06440.jpg)

![](images/79168fada8e4bb8afeb09bc836c406d719aaba8ca2de79b2451bbde59a5b7101.jpg)  
图8-3 具有相同回归系数（0和2.0）的两组数据。上图的相关系数是0.58，而下图的相关系数是0.99

在Python中，NumPy库提供了相关系数的计算方法：可以通过命令corrcoef(yEstimate, yActual)来计算预测值和真实值的相关性。下面我们就在前面的数据集上做个实验。

与之前一样，首先计算出y的预测值yMat：

```txt
>> yHat = xMat*ws 
```

再来计算相关系数（这时需要将yMat转置，以保证两个向量都是行向量）：

```txt
>>> corrcoef(yHat.T, yMat)  
array([[1., 0.98647356], [0.98647356, 1. ]] 
```

该矩阵包含所有两两组合的相关系数。可以看到，对角线上的数据是1.0，因为yMat和自己的匹配是最完美的，而YHat和yMat的相关系数为0.98。

最佳拟合直线方法将数据视为直线进行建模，具有十分不错的表现。但是图8-2的数据当中似乎还存在其他的潜在模式。那么如何才能利用这些模式呢？我们可以根据数据来局部调整预测，下面就会介绍这种方法。

# 8.2 局部加权线性回归

线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方误差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。

其中的一个方法是局部加权线性回归（Locally Weighted Linear Regression，LWLR）。在该算法中，我们给待预测点附近的每个点赋予一定的权重；然后与8.1节类似，在这个子集上基于最小均方差来进行普通的回归。与kNN一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数w的形式如下：

$$
\hat {w} = \left(\boldsymbol {X} ^ {\mathrm {T}} \boldsymbol {W} \boldsymbol {X}\right) ^ {- 1} \boldsymbol {X} ^ {\mathrm {T}} \boldsymbol {W} \boldsymbol {y}
$$

其中 $\mathbf{w}$ 是一个矩阵，用来给每个数据点赋予权重。

LWLR使用“核”（与支持向量机中的核类似）来对附近的点赋予更高的权重①。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下：

$$
w (i, i) = \exp \left(\frac {\left| x ^ {(i)} - x \right|}{- 2 k ^ {2}}\right)
$$

这样就构建了一个只含对角元素的权重矩阵 $\mathbf{w}$ ，并且点 $\mathbf{x}$ 与 $\mathbf{x}(i)$ 越近， $\mathbf{w}(i, i)$ 将会越大。上述公式包含一个需要用户指定的参数 $k$ ，它决定了对附近的点赋予多大的权重，这也是使用LWL时唯一需要考虑的参数，在图8-4中可以看到参数 $k$ 与权重的关系。

![](images/d28a094b378866b9a30e14026b3da47bc7a30fcf011ca0e04eb474bf7baedac3.jpg)  
图8-4 每个点的权重图（假定我们正预测的点是 $x = 0.5$ ），最上面的图是原始数据集，第二个图显示了当 $k = 0.5$ 时，大部分的数据都用于训练回归模型；而最下面的图显示当 $k = 0.01$ 时，仅有很少的局部点被用于训练回归模型

下面看看模型的效果，打开文本编辑器，将程序清单8-2的代码添加到文件regression.py中。

程序清单8-2 局部加权线性回归函数  
def lwlr(testPoint,xArr,yArr,k=1.0): xMat $=$ mat(xArr);yMat $=$ mat(yArr).T m $=$ shape(xMat)[0] weights $=$ mat(eye((m))) for j in range(m): diffMat $=$ testPoint-xMat[j,:] weights[j,j] $=$ exp(diffMat\*diffMat.T/(-2.0*k**2)) $\mathbf{xTx} = \mathbf{xMat}.T^{*}$ (weights $\star$ xMat) iflinalg.det(xTx) $= = 0.0$ : print"This matrix is singular, cannot do inverse" return ws $=$ xTx.I $\star$ (xMat.T $\star$ (weights $\star$ yMat)) return testPoint \* ws   
deflwlrTest(testArr,xArr,yArr,k=1.0): m $=$ shape(testArr)[0] yHat $=$ zeros(m) for i in range(m): yHat[i] $=$ lwrl(testArr[i],xArr,yArr,k) return yHat

程序清单8-2中代码的作用是，给定x空间中的任意一点，计算出对应的预测值yHat。函数lwlr()的开头与程序清单8-1类似，读入数据并创建所需矩阵，之后创建对角权重矩阵weights①。权重矩阵是一个方阵，阶数等于样本点个数。也就是说，该矩阵为每个样本点初始化了一个权重。接着，算法将遍历数据集，计算每个样本点对应的权重值：随着样本点与待预测点距离的递增，权重将以指数级衰减②。输入参数k控制衰减的速度。与之前的函数stand-Regression()一样，在权重矩阵计算完毕后，就可以得到对回归系数ws的一个估计。

程序清单8-2中的另一个函数是1wlrTest()，用于为数据集中每个点调用1wlr()，这有助于求解k的大小。

下面看看实际效果，将程序清单8-2的代码加入到regression.py中并保存，然后在Python提示符下输入如下命令：

```txt
>>> reload(regression)
<module 'regression' from 'regression.py'> 
```

如果需要重新载入数据集，则输入：

```javascript
>> xArr, yArr = regression.loadDataSet('ex0.txt') 
```

可以对单点进行估计：

```txt
>>> yArr[0]
3.1765129999999999
>>> regression.lwlr(xArr[0], xArr, yArr, 1.0)
matrix([[3.12204471]])
>>> regression.lwlr(xArr[0], xArr, yArr, 0.001)
matrix([[3.20175729]]) 
```

为了得到数据集里所有点的估计，可以调用lwrTest()函数：

```txt
>>> yHat = regression.lwlrTest(xArr, xArr, yArr, 0.003) 
```

下面绘出这些估计值和原始值，看看yHat的拟合效果。所用的绘图函数需要将数据点按序排列，首先对xArr排序：

$\mathbf{xMat} = \mathbf{mat}(\mathbf{xArr})$ >>srtInd $=$ xMat[:,1].argsort(0)  
>>xSort=xMat[srtInd] [::,0,:]

然后用Matplotlib绘图：

```python
>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(xSort(:,1],yHat[srtInd])
[<matplotlib.axes.Line2D object at 0x03639550>
>>> axscatter(xMat(:,1]. flatten().A[0], mat(yArr).T Flatten().A[0], s=2, c='red')
<matplotlibcollections.PathCollection object at 0x03859110>
>>> plt.show() 
```

可以观察到如图8-5所示的效果。图8-5给出了 $k$ 在三种不同取值下的结果图。当 $k = 1.0$ 时权重很大，如同将所有的数据视为等权重，得出的最佳拟合直线与标准的回归一致。使用 $k = 0.01$ 得到了非常好的效果，抓住了数据的潜在模式。下图使用 $k = 0.003$ 纳入了太多的噪声点，拟合的直线与数据点过于贴近。所以，图8-5中的最下图是过拟合的一个例子，而最上图则是欠拟合的一个例子。下一节将对过拟合和欠拟合进行量化分析。

![](images/4ad47f3f0f3a15faa02d93baba08510812bb999f86e775c9f650351b1678bab5.jpg)  
图8-5 使用3种不同平滑值绘出的局部加权线性回归结果。上图中的平滑参数 $k = 1.0$ ，中图 $k = 0.01$ ，下图 $k = 0.003$ 。可以看到， $k = 1.0$ 时的模型效果与最小二乘法差不多， $k = 0.01$ 时该模型可以挖出数据的潜在规律，而 $k = 0.003$ 时则考虑了太多的噪声，进而导致了过拟合现象

局部加权线性回归也存在一个问题，即增加了计算量，因为它对每个点做预测时都必须使用整个数据集。从图8-5可以看出， $k = 0.01$ 时可以得到很好的估计，但是同时看一下图8-4中 $k = 0.01$ 的情况，就会发现大多数数据点的权重都接近零。如果避免这些计算将可以减少程序运行时间，从而缓解因计算量增加带来的问题。

到此为止，我们已经介绍了找出最佳拟合直线的两种方法，下面用这些技术来预测鲍鱼的年龄。

# 8.3 示例：预测鲍鱼的年龄

接下来，我们将回归用于真实数据。在data目录下有一份来自UCI数据集合的数据，记录了鲍鱼（一种介壳类水生动物）的年龄。鲍鱼年龄可以从鲍鱼壳的层数推算得到。

在regression.py中加入下列代码：

```python
def rssError(yArr,yHatArr):
    return((yArr-yHatArr)\*\*2).sum()
>>>abX,abY=regression.loadDataSet('abalone.txt')
>>>yHat01=regression.lwlrTest(abX[0:99],abX[0:99],abY[0:99],0.1)
>>>yHat1=regression.lwlrTest(abX[0:99],abX[0:99],abY[0:99],1)
>>>yHat10=regression.lwlrTest(abX[0:99],abX[0:99],abY[0:99],10) 
```

为了分析预测误差的大小，可以用函数rssError()计算出这一指标：

```txt
>>> regression.rssError(abY[0:99], yHat01.T)
56.842594430533545
>>> regression.rssError(abY[0:99], yHat1.T)
429.89056187006685
>>> regression.rssError(abY[0:99], yHat10.T)
549.11817088257692 
```

可以看到，使用较小的核将得到较低的误差。那么，为什么不在所有数据集上都使用最小的核呢？这是因为使用最小的核将造成过拟合，对新数据不一定能达到最好的预测效果。下面就来看看它们在新数据上的表现：

```txt
>>> yHat01 = regression.lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)
>>> regression.rssError(abY[100:199], yHat01.T)
25619.926899338669
>>> yHat1 = regression.lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)
>>> regression.rssError(abY[100:199], yHat1.T)
573.5261441895808
>>> yHat10 = regression.lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)
>>> regression.rssError(abY[100:199], yHat10.T)
517.57119053830979 
```

从上述结果可以看到，在上面的三个参数中，核大小等于10时的测试误差最小，但它在训练集上的误差却是最大的。接下来再来和简单的线性回归做个比较：

```txt
>>> ws = regressionstandRegres(abX[0:99],abY[0:99])
>>> yHat=mat(abX[100:199]) *ws
>>> regression.rssError(abY[100:199],yHat.T.A)
518.63631532450131 
```

简单线性回归达到了与局部加权线性回归类似的效果。这也表明一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是10吗？或许是，但如果想得到更好的效果，应该用10个不同的样本集做10次测试来比较结果。

本例展示了如何使用局部加权线性回归来构建模型，可以得到比普通线性回归更好的效果。局部加权线性回归的问题在于，每次必须在整个数据集上运行。也就是说为了做出预测，必须保存所有的训练数据。下面将介绍另一种提高预测精度的方法，并分析它的优势所在。

# 8.4 缩减系数来“理解”数据

如果数据的特征比样本点还多应该怎么办？是否还可以使用线性回归和之前的方法来做预测？答案是否定的，即不能再使用前面介绍的方法。这是因为在计算 $(\mathbf{x}^{\mathrm{T}}\mathbf{x})^{-1}$ 的时候会出错。

如果特征比样本点还多 $(n > m)$ ，也就是说输入数据的矩阵 $\mathbf{x}$ 不是满秩矩阵。非满秩矩阵在求逆时会出现问题。

为了解决这个问题，统计学家引入了岭回归（ridge regression）的概念，这就是本节将介绍的第一种缩减方法。接着是lasso法，该方法效果很好但计算复杂。本节最后介绍了第二种缩减方法，称为前向逐步回归，可以得到与lasso差不多的效果，且更容易实现。

# 8.4.1 岭回归

简单说来，岭回归就是在矩阵 $\mathbf{x}^{\mathrm{T}}\mathbf{x}$ 上加一个 $\lambda \mathbf{I}$ 从而使得矩阵非奇异，进而能对 $\mathbf{x}^{\mathrm{T}}\mathbf{x} + \lambda \mathbf{I}$ 求逆。其中矩阵 $\mathbf{I}$ 是一个 $m\times m$ 的单位矩阵，对角线上元素全为1，其他元素全为0。而 $\lambda$ 是一个用户定义的数值，后面会做介绍。在这种情况下，回归系数的计算公式将变成：

$$
\hat {w} = \left(\boldsymbol {X} ^ {\mathrm {T}} \boldsymbol {X} + \lambda \boldsymbol {I}\right) ^ {- 1} \boldsymbol {X} ^ {\mathrm {T}} y
$$

岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入 $\lambda$ 来限制了所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫做缩减（shrinkage）。

# 岭回归中的岭是什么？

岭回归使用了单位矩阵乘以常量 $\lambda$ ，我们观察其中的单位矩阵 $I$ ，可以看到值1贯穿整个对角线，其余元素全是0。形象地，在0构成的平面上有一条1组成的“岭”，这就是岭回归中的“岭”的由来。

缩减方法可以去掉不重要的参数，因此能更好地理解数据。此外，与简单的线性回归相比，缩减法能取得更好的预测效果。

与前几章里训练其他参数所用的方法类似，这里通过预测误差最小化得到 $\lambda$ ：数据获取之后，首先抽一部分数据用于测试，剩余的作为训练集用于训练参数w。训练完毕后在测试集上测试预测性能。通过选取不同的λ来重复上述测试过程，最终得到一个使预测误差最小的λ。

下面看看实际效果，打开regression.py文件并添加程序清单8-3的代码。

# 程序清单8-3 岭回归

```python
def ridgeRegres(xMat,yMat,lam=0.2):
    xTx = xMat.T*xMat
    denom = xTx + eye(shape(xMat)[1])*lam
    if linalg.det(Xlam) == 0.0:
        print "This matrix is singular, cannot do inverse"
        return
    ws = denom.I * (xMat.T*yMat)
    return ws
def ridgeTest(xArr,yArr):
    xMat = mat(xArr); yMat = mat(yArr).T
    yMean = mean(yMat,0)
    yMat = yMat - yMean
    xMeans = mean(xMat,0)
    xVar = var(xMat,0)
    xMat = (xMat - xMeans)/xVar
    numTestPts = 30
    wMat = zeros((numTestPts, shape(xMat)[1]))
    for i in range(numTestPts):
        ws = ridgeRegres(xMat,yMat,exp(i-10))
        wMat[i,:] = ws.T
    return wMat 
```

程序清单8-3中的代码包含了两个函数：函数ridgeRegres()用于计算回归系数，而函数ridgeTest()用于在一组 $\lambda$ 上测试结果。

第一个函数ridgeRegres()实现了给定lambda下的岭回归求解。如果没指定lambda，则默认为0.2。由于lambda是Python保留的关键字，因此程序中使用了lam来代替。该函数首先构建矩阵 $\mathbf{x}^{\mathrm{T}}\mathbf{x}$ ，然后用lam乘以单位矩阵（可调用NumPy库中的方法eye()来生成）。在普通回归方法可能会产生错误时，岭回归仍可以正常工作。那么是不是就不再需要检查行列式是否为零，对吗？不完全对，如果lambda设定为0的时候一样可能会产生错误，所以这里仍需要做一个检查。最后，如果矩阵非奇异就计算回归系数并返回。

为了使用岭回归和缩减技术，首先需要对特征做标准化处理。回忆一下，第2章已经用过标准化处理技术，使每维特征具有相同的重要性（不考虑特征代表什么）。程序清单8-3中的第二个函数ridgeTest()就展示了数据标准化的过程。具体的做法是所有特征都减去各自的均值并除以方差①。

处理完成后就可以在30个不同的lambda下调用ridgeRegres()函数。注意，这里的lambda应以指数级变化，这样可以看出lambda在取非常小的值时和取非常大的值时分别对结果造成的影响。最后将所有的回归系数输出到一个矩阵并返回。

下面看一下鲍鱼数据集上的运行结果。

```python
>>> reload(regression)
>>> abX, abY = regression.loadDataSet('abalone.txt')
>>> ridgeWeights = regression=ridgeTest(abX, abY) 
```

这样就得到了30个不同lambda所对应的回归系数。为了看到缩减的效果，在Python提示符下输入如下代码：

```python
>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.plot(ridgeWeights)
>>> plt.show() 
```

运行之后应该看到一个类似图8-6的结果图，该图绘出了回归系数与 $\log (\lambda)$ 的关系。在最左边，即最小时，可以得到所有系数的原始值（与线性回归一致）；而在右边，系数全部缩减成0；在中间部分的某值将可以取得最好的预测效果。为了定量地找到最佳参数值，还需要进行交叉验证。另外，要判断哪些变量对结果预测最具有影响力，在图8-6中观察它们对应的系数大小就可以。

![](images/4d13a6b2e130b3f8a4d7803539c2c71f96beab2eaaf9deac760a9c0aba8bee5c.jpg)  
图8-6 岭回归的回归系数变化图。 $\lambda$ 非常小时，系数与普通回归一样。而 $\lambda$ 非常大时，所有回归系数缩减为0。可以在中间某处找到使得预测的结果最好的 $\lambda$ 值

还有一些其他缩减方法，如lasso、LAR、PCA回归以及子集选择等。与岭回归一样，这些方法不仅可以提高预测精确率，而且可以解释回归系数。下面将对lasso方法稍作介绍。

# 8.4.2 lasso

不难证明，在增加如下约束时，普通的最小二乘法回归会得到与岭回归的一样的公式：

$$
\sum_ {k = 1} ^ {n} w _ {k} ^ {2} \leqslant \lambda
$$

上式限定了所有回归系数的平方和不能大于 $\lambda$ 。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得出一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。

与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，对应的约束条件如下：

$$
\sum_ {k = 1} ^ {n} \left| w _ {k} \right| \leqslant \lambda
$$

唯一的不同点在于，这个约束条件使用绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭：在 $\lambda$ 足够小的时候，一些系数会因此被迫缩减到0，这个特性可以帮助我们更好地理解数据。这两个约束条件在公式上看起来相差无几，但细微的变化却极大地增加了计算复杂度（为了在这个新的约束条件下解出回归系数，需要使用二次规划算法）。下面将介绍一个更为简单的方法来得到结果，该方法叫做前向逐步回归。

# 8.4.3 前向逐步回归

前向逐步回归算法可以得到与lasso差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有的权重都设为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。

该算法的伪代码如下所示：

数据标准化，使其分布满足0均值和单位方差

在每轮迭代过程中：

设置当前最小误差lowestError为正无穷

对每个特征：

增大或缩小：

改变一个系数得到一个新的W

计算新W下的误差

如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W将W设置为新的Wbest

下面看看实际效果，打开regression.py文件并加入下列程序清单中的代码。

# 程序清单8-4 前向逐步线性回归

```python
def stageWise(xArr,yArr,eps=0.01,numIt=100):
    xMat = mat(xArr); yMat = mat(yArr).T
    yMean = mean(yMat,0)
    yMat = yMat - yMean
    xMat = regularize(xMat)
    m,n=shape(xMat)
    returnMat = zeros((numIt,n))
    ws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy()
    for i in range(numIt):
        print ws.T 
```

```python
lowestError = inf;  
for j in range(n):  
    for sign in [-1, 1]:  
        wsTest = ws.copy()  
        wsTest[j] += eps*sign  
        yTest = xMat*wsTest  
        rssE = rssError(yMat.A, yTest.A)  
if rssE < lowestError:  
    lowestError = rssE  
    wsMax = wsTest  
ws = wsMax.copy()  
return Mat[i, :] = ws.T  
return returnMat 
```

程序清单8-4中的函数stageWise()是一个逐步线性回归算法的实现，它与lasso做法相近但计算简单。该函数的输入包括：输入数据xArr和预测变量yArr。此外还有两个参数：一个是eps，表示每次迭代需要调整的步长；另一个是numIt，表示迭代次数。

函数首先将输入数据转换并存入矩阵中，然后把特征按照均值为0方差为1进行标准化处理。在这之后创建了一个向量ws来保存w的值，并且为了实现贪心算法建立了ws的两份副本。接下来的优化过程需要迭代numIt次，并且在每次迭代时都打印出w向量，用于分析算法执行的过程和效果。

贪心算法在所有特征上运行两次for循环，分别计算增加或减少该特征对误差的影响。这里使用的是平方误差，通过之前的函数rssError()得到。该误差初始值设为正无穷，经过与所有的误差比较后取最小的误差。整个过程循环迭代进行。

下面看一下实际效果，在regression.py里输入程序清单8-4的代码并保存，然后在Python提示符下输入如下命令：

```markdown
>>> reload(regression)
<module 'regression' from 'regression.pyc>
>>> xArr, yArr = regression.loadDataSet('abalone.txt')
>>> regression stageWise(xArr, yArr, 0.01, 200)
[[0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1
[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1
[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. -0.64 0. 36]] [[0. 05 0. 05 0.5555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555.
```
>>> regresion stageWise(xArr, yArr, 0.01, 200)
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1]
[[0. 1. 1. 1. 1. 1. 1.
[[[1111111111111111111111111111111111111111111111111111111111111111111111111111111] 
```

上述结果中值得注意的是w1和w6都是0，这表明它们不对目标值造成任何影响，也就是说这些特征很可能是不需要的。另外，在参数eps设置为0.01的情况下，一段时间后系数就已经饱和并在特定值之间来回震荡，这是因为步长太大的缘故。这里会看到，第一个权重在0.04和0.05之间来回震荡。

下面试着用更小的步长和更多的步数：

```txt
>>> regression stagewise(xArr, yArr, 0.001, 5000)
[[0. 0. 0. 0. 0. 0. 0. 0.]]  
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ]]  
[[0. 0. 0. 0. 0. 0. 0. 0. 0. ]] 
```

```txt
[ [ 0.044 -0.011 0.12 0.022 2.023 -0.963 -0.105 0.187]  
[ [ 0.043 -0.011 0.12 0.022 2.023 -0.963 -0.105 0.187]  
[ [ 0.044 -0.011 0.12 0.022 2.023 -0.963 -0.105 0.187] 
```

接下来把这些结果与最小二乘法进行比较，后者的结果可以通过如下代码获得：

```txt
>>> xMat = mat(xArr)
>>> yMat = mat(yArr).T
>>> xMat = regression.regularize(xMat)
>>> yM = mean(yMat, 0)
>>> yMat = yMat - yM
>>> weights = regression.regandResgrees(xMat, yMat.T)
>>> weights.T
matrix([[0.0430442, -0.02274163, 0.13214087, 0.02075182, 2.22403814, -0.99895312, -0.11725427, 0.16622915]]) 
```

可以看到在5000次迭代以后，逐步线性回归算法与常规的最小二乘法效果类似。使用0.005的epsilon值并经过1000次迭代后的结果参见图8-7。

![](images/af22f0c87d3ceb8ed42b35d826004284d72b0761ad23c1a85c917ec83ac401a9.jpg)  
图8-7 鲍鱼数据集上执行逐步线性回归法得到的系数与迭代次数间的关系。逐步线性回归得到了与lasso相似的结果，但计算起来更加简便

逐步线性回归算法的实际好处并不在于能绘出图8-7这样漂亮的图，主要的优点在于它可以帮助人们理解现有的模型并做出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择使误差最小的模型。

当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差（bias），与此同时却减小了模型的方差。下一节将揭示这些概念之间的关系并分析它们对结果的影响。

# 8.5 权衡偏差与方差

任何时候，一旦发现模型和测量值之间存在差异，就说出现了误差。当考虑模型中的“噪声”或者说误差时，必须考虑其来源。你可能会对复杂的过程进行简化，这将导致在模型和测量值之间出现“噪声”或误差，若无法理解数据的真实生成过程，也会导致差异的发生。另外，测量过程本身也可能产生“噪声”或者问题。下面举一个例子，8.1节和8.2节处理过一个从文件导入的二维数据。实话来讲，这个数据是我自己造出来的，其具体的生成公式如下：

$$
y = 3. 0 + 1. 7 x + 0. 1 \sin (3 0 x) + 0. 0 6 N (0, 1),
$$

其中 $\mathbf{N}(0,1)$ 是一个均值为 0、方差为 1 的正态分布。在 8.1 节中，我们尝试过仅用一条直线来拟合上述数据。不难想到，直线所能得到的最佳拟合应该是 $3.0 + 1.7x$ 这一部分。这样的话，误差部分就是 $0.1\sin(30x) + 0.06\mathrm{N}(0,1)$ 。在 8.2 节和 8.3 节，我们使用了局部加权线性回归来试图捕捉数据背后的结构。该结构拟合起来有一定的难度，因此我们测试了多组不同的局部权重来找到具有最小测试误差的解。

图8-8给出了训练误差和测试误差的曲线图，上面的曲线就是测试误差，下面的曲线是训练误差。根据8.3节的实验我们知道：如果降低核的大小，那么训练误差将变小。从图8-8来看，从左到右就表示了核逐渐减小的过程。

![](images/2a6576742457a247b686def388959ae85ef37029ced19e8f8d56da0024ee1af1.jpg)  
图8-8 偏差方差折中与测试误差及训练误差的关系。上面的曲线就是测试误差，在中间部分最低。为了做出最好的预测，我们应该调整模型复杂度来达到测试误差的最小值

一般认为，上述两种误差由三个部分组成：偏差、测量误差和随机噪声。在8.2节和8.3节，我们通过引入了三个越来越小的核来不断增大模型的方差。

8.4节介绍了缩减法，可以将一些系数缩减成很小的值或直接缩减为0，这是一个增大模型偏差的例子。通过把一些特征的回归系数缩减到0，同时也就减少了模型的复杂度。例子中有8个特征，消除其中两个后不仅使模型更易理解，同时还降低了预测误差。图8-8的左侧是参数缩减过于严厉的结果，而右侧是无缩减的效果。

方差是可以度量的。如果从鲍鱼数据中取一个随机样本集（例如取其中100个数据）并用线性模型拟合，将会得到一组回归系数。同理，再取出另一组随机样本集并拟合，将会得到另一组回归系数。这些系数间的差异大小也就是模型方差大小的反映①。上述偏差与方差折中的概念在机器学习十分流行并且反复出现。

下一节将介绍上述理论的应用：首先从拍卖站点抽取一些数据，再使用一些回归法进行实验来为数据找到最佳的岭回归模型。这样就可以通过实际效果来看看偏差和方差间的折中效果。

# 8.6 示例：预测乐高玩具套装的价格

你对乐高（LEGO）品牌的玩具了解吗？乐高公司生产拼装类玩具，由很多大小不同的塑料插块组成。这些塑料插块的设计非常出色，不需要任何粘合剂就可以随意拼装起来。除了简单玩具之外，乐高玩具在一些成人中也很流行。一般来说，这些插块都成套出售，它们可以拼装成很多不同的东西，如船、城堡、一些著名建筑，等等。乐高公司每个套装包含的部件数目从10件到5000件不等。

一种乐高套装基本上在几年后就会停产，但乐高的收藏者之间仍会在停产后彼此交易。Dangler喜欢为乐高套装估价，下面将用本章的回归技术帮助他建立一个预测模型。

# 示例：用回归法预测乐高套装的价格

(1)收集数据：用GoogleShopping的API收集数据。  
(2) 准备数据：从返回的JSON数据中抽取价格。  
(3) 分析数据：可视化并观察数据。  
(4) 训练算法：构建不同的模型，采用逐步线性回归和直接的线性回归模型。  
(5) 测试算法：使用交叉验证来测试不同的模型，分析哪个效果最好。  
(6) 使用算法：这次练习的目标就是生成数据模型。

在这个例子中，我们将从不同的数据集上获取价格，然后对这些数据建立回归模型。需要做的第一件事就是如何获取数据。

# 8.6.1 收集数据：使用 Google 购物的 API

Google已经为我们提供了一套购物的API来抓取价格。在使用API之前，需要注册一个Google账号，然后访问Google API的控制台来确保购物API可用。完成之后就可以发送HTTP请求，API

将以JSON格式返回所需的产品信息。Python提供了JSON解析模块，我们可以从返回的JSON格式里整理出所需数据。详细的API介绍可以参见：http://code.google.com apis/shopping/search/v1/getting_started.html。

打开regression.py文件并加入如下代码。

# 程序清单8-5 购物信息的获取函数

from time import sleep   
import json   
import urllib2   
def searchForSet(retX, retY, setNum, yr, numPce, origPrc): sleep(10) myAPIstr = 'get from code.google.com' searchURL = 'https://www.googleapis.com/shopping/search/v1/public/ products?\\ key=%s&country $\equiv$ US&q $\equiv$ LEGO.com/shopping/search/v1/public/ products\\n key=%s&country $\equiv$ US&q $\equiv$ LEGO.com/shopping/search/v1/public/ products\\n pg $\equiv$ urllib2(urlopen_searchURL) retDict $=$ json.loads(pg.read()) for i in range(len(retDict['items'])) try: currItem $=$ retDict['items'] [i] if currItem['product'] ['condition'] $= =$ 'new': newFlag $= 1$ else: newFlag $= 0$ listOfInv $=$ currItem['product'] ['inventories'] for item in listOfInv: sellingPrice $=$ item['price'] if sellingPrice $>$ origPrc * 0.5: print"%d\td%d\td%f\td%f" %\ (yr,numPce,newFlag,origPrc,sellingPrice) retX.append([yr,numPce,newFlag,origPrc]) retY.append(sellingPrice) except: print 'problem with item %d' % i   
defSETSDataCollect(retX,retY): searchForSet(retX,retY,8288,2006,800,49.99) searchForSet(retX,retY,10030,2002,3096,269.99) searchForSet(retX,retY,10179,2007,5195,499.99) searchForSet(retX,retY,10181,2007,3428,199.99) searchForSet(retX,retY,10189,2008,5922,299.99) searchForSet(retX,retY,10196,2009,3263,249.99)

上述程序清单中的第一个函数是searchForSet()，它调用Google购物API并保证数据抽取的正确性。这里需要导入新的模块：time.sleep()、json和urllib2。但是一开始要休眠IO秒钟，这是为了防止短时间内有过多的API调用。接下来，我们拼接查询的URL字符串，添加API的key和待查询的套装信息，打开和解析操作通过json.loads()方法实现。完成后我们将得到一部字典，下面需要做的是从中找出价格和其他信息。

部分返回结果的是一个产品的数组，我们将在这些产品上循环迭代，判断该产品是否是新产品并抽取它的价格。我们知道，乐高套装由很多小插件组成，有的二手套装很可能会缺失其中一两件。也就是说，卖家只出售套装的若干部件（不完整）。因为这种不完整的套装也会通过检索

结果返回，所以我们需要将这些信息过滤掉（可以统计描述中的关键词或者是用贝叶斯方法来判断）。我在这里仅使用了一个简单的启发式方法：如果一个套装的价格比原始价格低一半以上，则认为该套装不完整。程序清单8-5在代码①处过滤掉了这些套装，解析成功后的套装将在屏幕上显示出来并保存在list对象retx和rety中。

程序清单8-5的最后一个函数是setDataCollect()，它负责多次调用searchForSet()。函数searchForSet()的其他参数是从www.brickset.com收集来的，它们也一并输出到文件中。

下面看一下执行结果，添加程序清单8-5中的代码之后保存regression.py，在Python提示符下输入如下命令：

```txt
>>> lgX = []; lgY = []
>>> regression.setDataCollect(1gX, lgY)
2006 800 1 49.990000 549.990000
2006 800 1 49.990000 759.050000
2006 800 1 49.990000 316.990000
2002 3096 1 269.990000 499.990000
2002 3096 1 269.990000 289.990000
2009 3263 0 249.990000 524.990000
2009 3263 1 249.990000 672.000000
2009 3263 1 249.990000 580.000000 
```

检查一下lgX和lgY以确认一下它们非空。下节我们将使用这些数据来构建回归方程并预测乐高玩具套装的售价。

# 8.6.2 训练算法：建立模型

上一节从网上收集到了一些真实的数据，下面将为这些数据构建一个模型。构建出的模型可以对售价做出预测，并帮助我们理解现有数据。看一下Python是如何完成这些工作的。

首先需要添加对应常数项的特征X0（ $\mathrm{X}0 = 1$ ），为此创建一个全1的矩阵：

```erlang
>> shape(1gX)  
(58, 4)  
>> lgX1=mat(ones((58, 5))) 
```

接下来，将原数据矩阵1gx复制到新数据矩阵1gX1的第1到第5列：

```markdown
>>> lgX1[:,1:5] = mat(lgX) 
```

确认一下数据复制的正确性：

```txt
>>> lgX[0]  
[2006.0, 800.0, 0.0, 49.99000000000002]  
>>> lgX1[0]  
matrix([[1.00000000e+00, 2.00600000e+03, 8.00000000e+02, 0.00000000e+00, 4.99900000e+01]]) 
```

很显然，后者除了在第0列加入1之外其他数据都一样。最后在这个新数据集上进行回归处理：

```txt
>>> ws=regressionstandRegres(lgX1,lgY)  
>>> ws 
```

```txt
matrix([[5.53199701e+04],
[-2.75928219e+01],
[-2.68392234e-02],
[-1.12208481e+01],
[2.57604055e+00]])
```

检查一下结果，看看模型是否有效：

```txt
>>>lgX1[0]\*ws   
matrix([[76.07418853]])   
>>>lgX1[-1]\*ws   
matrix([[431.17797672]])   
>>>lgX1[43]\*ws   
matrix([[516.20733105]]) 
```

可以看到模型有效。下面看看具体的模型。该模型认为套装的售价应该采用如下公式计算：

```txt
$55319.97-27.59*Year-0.00268*NumPieces-11.22*NewOrUsed+2.57*original price 
```

这个模型的预测效果非常好，但模型本身并不能令人满意。它对于数据拟合得很好，但看上去没有什么道理。从公式看，套装里零部件越多售价反而会越低。另外，该公式对新套装也有一定的惩罚。

下面使用缩减法中一种，即岭回归再进行一次实验。前面讨论过如何对系数进行缩减，但这次将会看到如何用缩减法确定最佳回归系数。打开regression.py并输入下面的代码。

# 程序清单8-6 交叉验证测试岭回归

defcrossValidation(xArr,yArr,numVal $= 10$ ：  
m $\equiv$ len(yArr)  
indexOf $\equiv$ range(m)  
errorMat $\equiv$ zeros((numVal,30))  
fori in range(numVal):trainX $\equiv$ []；trainY $\equiv$ []testX $\equiv$ []；testY $\equiv$ []random shuffle(indexList)  
forj in range(m):if $j <   m^{*}0.9$ ：trainX.append(xArr[indexList[j]])trainY.append(yArr[indexList[j]])else:testX.append(xArr[indexList[j]])testY.append(yArr[indexList[j]])  
wMat $\equiv$ ridgeTest(trainX,trainY)  
fork in range(30):matTestX $\equiv$ mat(testX);matTrainX $\equiv$ mat(trainX)meanTrain $\equiv$ mean(matTrainX,0)varTrain $\equiv$ var(matTrainX,0)matTestX $\equiv$ (matTestX-meanTrain)/varTrainyEst $\equiv$ matTestX\*mat(wMat[k,:]).T $^+$ mean(trainY)errorMat[i,k]=rssError(yEst.T.A,array(testY))  
meanErrors $\equiv$ mean(errorMat,0)  
minMean $\equiv$ float(min(meanErrors))  
bestWeights $\equiv$ wMat[nonzero(meanErrors==minMean)]xMat $\equiv$ mat(xArr);yMat $\equiv$ mat(yArr).T  
meanX $\equiv$ mean(xMat,0);varX $\equiv$ var(xMat,0)

unReg = bestWeights/varX  
print "the best model from Ridge Regression is:\n", unReg  
print "with constant term: ", $-1 * \text{sum}(\text{multiply}(\text{meanX}, \text{unReg})) + \text{mean}(\text{yMat})$

数据还原

上述程序清单中的函数crossValidation()有三个参数，前两个参数lgX和lgY存有数据集中的X和Y值的list对象，默认lgX和lgY具有相同的长度。第三个参数numVal是算法中交叉验证的次数，如果该值没有指定，就取默认值10。函数crossValidataion()首先计算数据点的个数m。创建好了训练集和测试集容器①，之后创建了一个list并使用Numpy提供的randomshuffle()函数对其中的元素进行混洗（shuffle），因此可以实现训练集或测试集数据点的随机选取。②处将数据集的90%分割成训练集，其余10%为测试集，并将二者分别放入对应容器中。

一旦对数据点进行混洗之后，就建立一个新的矩阵wMat来保存岭回归中的所有回归系数。我们还记得在8.4.1节中，函数ridgeTest()使用30个不同的λ值创建了30组不同的回归系数。接下来我们也在上述测试集上用30组回归系数来循环测试回归效果。岭回归需要使用标准化后的数据，因此测试数据也需要用与测试集相同的参数来执行标准化。在①处用函数rssError()计算误差，并将结果保存在errorMat中。

在所有交叉验证完成后，errorMat保存了ridgeTest()里每个λ对应的多个误差值。为了将得出的回归系数与standRegres()作对比，需要计算这些误差估计值的均值①。有一点值得注意：岭回归使用了数据标准化，而standRegres()则没有，因此为了将上述比较可视化还需将数据还原。在④处对数据做了还原并将最终结果展示。

来看一下整体的运行效果，在regression.py中输入程序清单8-6中的代码并保存，然后执行如下命令：

```markdown
>>> regression CROSSValidation(lgX, lgY, 10)
The best model from Ridge Regression is:
[[-2.96472902e+01 -1.34476433e-03 -3.38454756e+01 2.44420117e+00]] with constant term: 59389.2069537 
```

为了便于与常规的最小二乘法进行比较，下面给出当前的价格公式：

```txt
$59389.21-29.64*Year-0.00134*NumPieces-33.85*NewOrUsed+2.44*original price. 
```

可以看到，该结果与最小二乘法没有太大差异。我们本期望找到一个更易于理解的模型，显然没有达到预期效果。为了达到这一点，我们来看一下在缩减过程中回归系数是如何变化的，输入下面的命令：

```txt
>>> regression.ridgeTest(lgX, lgY)
array([[ -1.45288906e+02, -8.39360442e+03, -3.28682450e+00, 4.42362406e+04],
[ -1.46649725e+02, -1.89952152e+03, -2.80638599e+00, 4.27891633e+04],
[ -4.91045279e-06, 5.01149871e-08, 2.40728171e-05, 8.14042912e-07]])
```
>>> regression.ridgeTest(lgX, lgY) 
```

这些系数是经过不同程度的缩减得到的。首先看第1行，第4项比第2项的系数大5倍，比第1项大57倍。这样看来，如果只能选择一个特征来做预测的话，我们应该选择第4个特征，也就是原始价格。如果可以选择2个特征的话，应该选择第4个和第2个特征。

这种分析方法使得我们可以挖掘大量数据的内在规律。在仅有4个特征时，该方法的效果也许并不明显；但如果有100个以上的特征，该方法就会变得十分有效：它可以指出哪些特征是关键的，而哪些特征是不重要的。

# 8.7 本章小结

与分类一样，回归也是预测目标值的过程。回归与分类的不同点在于，前者预测连续型变量，而后者预测离散型变量。回归是统计学中最有力的工具之一。在回归方程里，求得特征对应的最佳回归系数的方法是最小化误差的平方和。给定输入矩阵 $\mathbf{x}$ ，如果 $\mathbf{x}^{\mathrm{T}}\mathbf{x}$ 的逆存在并可以求得的话，回归法都可以直接使用。数据集上计算出的回归方程并不一定意味着它是最佳的，可以使用预测值yHat和原始值y的相关性来度量回归方程的好坏。

当数据的样本数比特征数还少时候，矩阵 $\mathbf{x}^{\mathrm{T}}\mathbf{x}$ 的逆不能直接计算。即便当样本数比特征数多时， $\mathbf{x}^{\mathrm{T}}\mathbf{x}$ 的逆仍有可能无法直接计算，这是因为特征有可能高度相关。这时可以考虑使用岭回归，因为当 $\mathbf{x}^{\mathrm{T}}\mathbf{x}$ 的逆不能计算时，它仍保证能求得回归参数。

岭回归是缩减法的一种，相当于对回归系数的大小施加了限制。另一种很好的缩减法是lasso。Lasso难以求解，但可以使用计算简便的逐步线性回归方法来求得近似结果。

缩减法还可以看做是对一个模型增加偏差的同时减少方差。偏差方差折中是一个重要的概念，可以帮助我们理解现有模型并做出改进，从而得到更好的模型。

本章介绍的方法很有用。但有些时候数据间的关系可能会更加复杂，如预测值与特征之间是非线性关系，这种情况下使用线性的模型就难以拟合。下一章将介绍几种使用树结构来预测数据的方法。

# 第9章

# 树回归

# 本章内容

□ CART算法  
□ 回归与模型树  
树剪枝算法  
Python中GUI的使用

第8章介绍的线性回归包含了一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙。而且，实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。

一种可行的方法是将数据集切分成很多份易建模的数据，然后利用第8章的线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树结构和回归法就相当有用。

本章首先介绍一个新的叫做CART（Classification And Regression Trees，分类回归树）的树构建算法。该算法既可以用于分类还可以用于回归，因此非常值得学习。然后利用Python来构建并显示CART树。代码会保持足够的灵活性以便能用于多个问题当中。接着，利用CART算法构建回归树并介绍其中的树剪枝技术（该技术的主要目的是防止树的过拟合）。之后引入了一个更高级的模型树算法。与回归树的做法（在每个叶节点上使用各自的均值做预测）不同，该算法需要在每个叶节点上都构建出一个线性模型。在这些树的构建算法中有一些需要调整的参数，所以还会介绍如何使用Python中的Tkinter模块建立图形交互界面。最后，在该界面的辅助下分析参数对回归效果的影响。

# 9.1 复杂数据的局部性建模

第3章使用决策树来进行分类。决策树不断将数据切分成小数据集，直到所有目标变量完全相同，或者数据不能再切分为止。决策树是一种贪心算法，它要在给定时间内做出最佳选择，但并不关心能否达到全局最优。

# 树回归

优点：可以对复杂和非线性的数据建模。

缺点：结果不易理解。

适用数据类型：数值型和标称型数据。

第3章使用的树构建算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另外一种方法是二元切分法，即每次把数据集切成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。

除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。具体的处理方法是：如果特征值大于给定值就走左子树，否则就走右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。

CART是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对CART稍作修改就可以处理回归问题。第3章中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。

下面将实观CART算法和回归树。回归树与分类树的思路类似，但叶节点的数据类型不是离散型，而是连续型。

# 树回归的一般方法

(1)收集数据：采用任意方法收集数据。  
(2) 准备数据：需要数值型的数据，标称型数据应该映射成二值型数据。  
(3) 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树。  
(4) 训练算法：大部分时间都花费在叶节点树模型的构建上。  
(5) 测试算法：使用测试数据上的R2值来分析模型的效果。  
(6) 使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情

有了思路之后就可以开始写代码了。下一节将介绍在Python中利用CART算法构建树的最佳方法。

# 9.2 连续和离散型特征的树的构建

在树的构建过程中，需要解决多种类型数据的存储问题。与第3章类似，这里将使用一部字典来存储树的数据结构，该字典将包含以下4个元素。

□待切分的特征。

□待切分的特征值。  
口右子树。当不再需要切分的时候，也可以是单个值。  
□左子树。与右子树类似。

这与第3章的树结构有一点不同。第3章用一部字典来存储每个切分，但该字典可以包含两个或两个以上的值。而CART算法只做二元切分，所以这里可以固定树的数据结构。树包含左键和右键，可以存储另一棵子树或者单个值。字典还包含特征和特征值这两个键，它们给出切分算法所有的特征和特征值。当然，读者可以用面向对象的编程模式来建立这个数据结构。例如，可以用下面的Python代码来建立树节点：

class treeNode(): def __init__(self,feat，val,right,left): featureToSplitOn $=$ feat valueOfSplit $=$ val rightBranch $=$ right leftBranch $=$ left

当使用C++这样不太灵活的编程语言时，你可能要用面向对象编程模式来实现树结构。Python具有足够的灵活性，可以直接使用字典来存储树结构而无须另外自定义一个类，从而有效地减少代码量。Python不是一种强类型编程语言，因此接下来会看到，树的每个分枝还可以再包含其他树、数值型数据甚至是向量。

本章将构建两种树：第一种是9.4节的回归树（regression tree），其每个叶节点包含单个值；第二种是9.5节的模型树（model tree），其每个叶节点包含一个线性方程。创建这两种树时，我们将尽量使得代码之间可以重用。下面先给出两种树构建算法中的一些共用代码。

函数createTree()的伪代码大致如下：

找到最佳的待切分特征：

如果该节点不能再分，将该节点存为叶节点

执行二元切分

在右子树调用createTree()方法

在左子树调用createTree()方法

打开文本编辑器，创建文件regTrees.py并添加如下代码。

# 程序清单9-1 CART算法的实现代码

from numpy import \*

```python
def loadDataSet(fileName):
    dataMat = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split("\\t")
        fltLine = map(float, curLine)
        dataMat.append(fltsLine)
    return dataMat 
```

![](images/f0947d0d74420f571cec73ae286000e08e67c00fa264eec14345f1c1c9d74ef7.jpg)

将每行映射成浮点数

```python
def binSplitDataSet(dataSet, feature, value):
    mat0 = DataSet[nonzero(dataSet[:, feature] > value) [0], :][0]
    mat1 = DataSet[nonzero(dataSet[:, feature] <= value) [0], :][0]
    return mat0, mat1
def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):
    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)
    if feat == None: return val
    retTree = {}
    retTree['spInd'] = feat
    retTree['spVal'] = val
    lSet, rSet = binSplitDataSet(dataSet, feat, val)
    retTree['left'] = createTree(lSet, leafType, errType, ops)
    retTree['right'] = createTree(rSet, leafType, errType, ops)
    return retTree 
```

上述程序清单包含3个函数：第一个函数是loadDataSet()，该函数与其他章节中同名函数功能类似。在前面的章节中，目标变量会单独存放其自己的列表中，但这里的数据会存放在一起。该函数读取一个以tab键为分隔符的文件，然后将每行的内容保存成一组浮点数①。

第二个函数是binSplitDataSet()，该函数有3个参数：数据集合、待切分的特征和该特征的某个值。在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。

最后一个函数是树构建函数createTree()，它有4个参数：数据集和其他3个可选参数。这些可选参数决定了树的类型：leafType给出建立叶节点的函数；errType代表误差计算函数；而ops是一个包含树构建所需其他参数的元组。

函数createTree()是一个递归函数。该函数首先尝试将数据集分成两个部分，切分由函数chooseBestSplit()完成（这里未给出该函数的实现）。如果满足停止条件，chooseBestSplit()将返回None和某类模型的值②。如果构建的是回归树，该模型是一个常数。如果是模型树，其模型是一个线性方程。后面会看到停止条件的作用方式。如果不满足停止条件，chooseBestSplit()将创建一个新的Python字典并将数据集分成两份，在这两份数据集上将分别继续递归调用createTree()函数。

程序清单9-1的代码很容易理解，但其中的方法chooseBestSplit()现在暂时尚未实现，所以函数还不能看到createTree()的实际效果。但是下面可以先测试其他两个函数的效果。将程序清单9-1的代码保存在文件regTrees.py中并在Python提示符下输入如下命令：

```txt
>>> import regTrees
>>> testMat = mat.eye(4))
>>> testMat
matrix([[1., 0., 0., 0.],
[0., 1., 0., 0.],
[0., 0., 1., 0.],
[0., 0., 0., 1.])
]) 
```

这样就创建了一个简单的矩阵，现在按指定列的某个值来切分该矩阵。

```txt
>>> mat0, mat1 = regTrees.binSplitDataSet(testMat, 1, 0.5)
>>> mat0
matrix([[0., 1., 0., 0.]])
>>> mat1
matrix([[1., 0., 0., 0.],
[0., 0., 1., 0.],
[0., 0., 0., 1.])
>>> 
```

很有趣吧。下面给出回归树的chooseBestSplit()函数，还会看到更有趣的结果。下一节将针对回归树构建，在chooseBestSplit()函数里加入具体代码，之后就可以使用程序清单9-1的CART算法来构建回归树。

# 9.3 将CART算法用于回归

要对数据的复杂关系建模，我们已经决定借用树结构来帮助切分数据，那么如何实现数据的切分呢？怎么才能知道是否已经充分切分呢？这些问题的答案取决于叶节点的建模方式。回归树假设叶节点是常数值，这种策略认为数据中的复杂关系可以用树结构来概括。

为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第3章使用树进行分类，会在给定节点时计算数据的混乱度。那么如何计算连续型数值的混乱度呢？事实上，在数据集上计算混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一的不同就是，方差是平方误差的均值（均方差），而这里需要的是平方误差的总值（总方差）。总方差可以通过均方差乘以数据集中样本点的个数来得到。

有了上述误差计算准则和上一节中的树构建算法，下面就可以开始构建数据集上的回归树了。

# 9.3.1 构建树

构建回归树，需要补充一些新的代码，使程序清单9-1中的函数createTree()得以运转。首先要做的就是实现chooseBestSplit()函数。给定某个误差计算方法，该函数会找到数据集上最佳的二元切分方式。另外，该函数还要确定什么时候停止切分，一旦停止切分会生成一个叶节点。因此，函数chooseBestSplit()只需完成两件事：用最佳方式切分数据集和生成相应的叶节点。

从程序清单9-1可以看出，除了数据集以外，函数chooseBestSplit()还有leafType、errType和ops这三个参数。其中leafType是对创建叶节点的函数的引用，errType是对前面介绍的总方差计算函数的引用，而ops是一个用户定义的参数构成的元组，用以完成树的构建。

上述代码中，函数chooseBestSplit()最复杂，该函数的目标是找到数据集切分的最佳位置。它遍历所有的特征及其可能的取值来找到使误差最小化的切分阈值。该函数的伪代码大致如下：

对每个特征：

对每个特征值：

将数据集切分成两份

计算切分的误差

如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差返回最佳切分的特征和阈值

下面给出上述三个函数的具体实现代码。打开regTrees.py文件并加入程序清单9-2中的代码。

程序清单9-2 回归树的切分函数  
```python
def regLeaf(dataSet):
    return mean(dataSet[:, -1])
def regErr(dataSet):
    return var(dataSet[:, -1]) * shape(dataSet) [0]
def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):
    tolS = ops[0]; tolN = ops[1]
    if len(set(dataSet[:, -1].T.tolist())[0]) == 1:
        return None, leafType(dataSet)
    m,n = shape(dataSet)
    S = errType(dataSet)
bestS = inf; bestIndex = 0; bestValue = 0
forfeatIndex in range(n-1):
    for splitVal in set(dataSet[:,featIndex]):
        mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)
        if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):
            newS = errType(mat0) + errType(mat1)
        if newS < bestS:
            bestIndex = featIndex
            bestValue = splitVal
            bestS = newS
if (S - bestS) < tolS:
    return None, leafType(dataSet)
mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)
if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):
    return None, leafType(dataSet)
return bestIndex, bestValue 
```

上述程序清单中的第一个函数是regLeaf()，它负责生成叶节点。当chooseBestSplit()函数确定不再对数据进行切分时，将调用该regLeaf()函数来得到叶节点的模型。在回归树中，该模型其实就是目标变量的均值。

第二个函数是误差估计函数regErr()。该函数在给定数据上计算目标变量的平方误差。当然也可以先计算出均值，然后计算每个差值再平方。但这里直接调用均方差函数var()更加方便。

因为这里需要返回的是总方差，所以要用均方差乘以数据集中样本的个数。

第三个函数是chooseBestSplit()，它是回归树构建的核心函数。该函数的目的是找到数据的最佳二元切分方式。如果找不到一个“好”的二元切分，该函数返回None并同时调用createTree()方法来产生叶节点，叶节点的值也将返回None。接下来将会看到，在函数chooseBestSplit()中有三种情况不会切分，而是直接创建叶节点。如果找到了一个“好”的切分方式，则返回特征编号和切分特征值。

函数chooseBestSplit()一开始为ops设定了tolS和tolN这两个值。它们是用户指定的参数，用于控制函数的停止时机。其中变量tolS是容许的误差下降值，tolN是切分的最少样本数。接下来通过对当前所有目标变量建立一个集合，函数chooseBestSplit()会统计不同剩余特征值的数目。如果该数目为1，那么就不需要再切分而直接返回①。然后函数计算了当前数据集的大小和误差。该误差s将用于与新切分误差进行对比，来检查新切分能否降低误差。下面很快就会看到这一点。

这样，用于找到最佳切分的几个变量就被建立和初始化了。下面就将在所有可能的特征及其可能取值上遍历，找到最佳的切分方式。最佳切分也就是使得切分后能达到最低误差的切分。如果切分数据集后效果提升不够大，那么就不应进行切分操作而直接创建叶节点②。另外还需要检查两个切分后的子集大小，如果某个子集的大小小于用户定义的参数tolN，那么也不应切分。最后，如果这些提前终止条件都不满足，那么就返回切分特征和特征值③。

# 9.3.2 运行代码

下面在一些数据上看看上节代码的实际效果，以图9-1的数据为例，我们的目标是从该数据生成一棵回归树。

将程序清单9-2中的代码添加到regTrees.py文件并保存，然后在Python提示符下输入：

```python
>>> reload(regTrees)
<module 'regTrees' from 'regTrees.pyc>
>>> from numpy import * 
```

图9-1的数据存储在文件ex00.txt中。

```python
>>> myDat = regTrees.loadDataSet('ex00.txt')
>>> myMat = mat(myDat)
>>> regTrees.createTree(myMat)
{'spInd': 0, 'spVal': matrix([[0.48813]]),
'right': -0.044650285714285733,
'left': 1.018096767241379} 
```

![](images/c1744025812c09c44d6aac2c9657efe65b6a41489d40549ee6bfc102ae1cee77.jpg)  
图9-1 基于CART算法构建回归树的简单数据集

再看一个多次切分的例子，参见图9-2的数据集。

![](images/cf694301b0b080e269f37990da8cb0a102128abd3c52c1eb988e7519ae205e45.jpg)  
图9-2 用于测试回归树的分段常数数据集

图9-2的数据保存在一个以tab键分隔的文本文档ex0.txt中数据。为从上述数据中构建一棵回归树，在Python提示符下敲入如下命令：

```python
>>> myDat1 = regTrees.loadDataSet('ex0.txt')
>>> myMat1 = mat(myDat1)
>>> regTrees.createTree(myMat1)
{'spInd': 1, 'spVal': matrix([[0.39435]]), 'right': {'spInd': 1, 'spVal': matrix([[0.197834]]), 'right': -0.023838155555555553, 'left': 1.0289583666666664}, 'left': {'spInd': 1, 'spVal': matrix([[0.582002]]), 'right': 1.9800350714285717, 'left': {'spInd': 1, 'spVal': matrix([[0.797583]]), 'right': 2.9836209534883724, 'left': 3.9871632000000004}} 
```

可以检查一下该树的结构以确保树中包含5个叶节点。读者也可以在更复杂的数据集上构建回归树并观察实验结果。

到现在为止，已经完成回归树的构建，但是需要某种措施来检查构建过程否得当。下面将介绍树剪枝（tree pruning）技术，它通过对决策树剪枝来达到更好的预测效果。

# 9.4 树剪枝

一棵树如果节点过多，表明该模型可能对数据进行了“过拟合”。那么，如何判断是否发生了过拟合？前面章节中使用了测试集上某种交叉验证技术来发现过拟合，决策树亦是如此。本节将对此进行讨论，并分析如何避免过拟合。

通过降低决策树的复杂度来避免过拟合的过程称为剪枝（pruning）。其实本章前面已经进行过剪枝处理。在函数chooseBestSplit()中的提前终止条件，实际上是在进行一种所谓的预剪枝（prepruning）操作。另一种形式的剪枝需要使用测试集和训练集，称作后剪枝（postpruning）。本节将分析后剪枝的有效性，但首先来看一下预剪枝的不足之处。

# 9.4.1 预剪枝

上节两个简单实验的结果还是令人满意的，但背后存在一些问题。树构建算法其实对输入的参数tolS和tolN非常敏感，如果使用其他值将不太容易达到这么好的效果。为了说明这一点，在Python提示符下输入如下命令：

```javascript
>>regTrees.createTree(myMat,ops=(0,1)) 
```

与上节中只包含两个节点的树相比，这里构建的树过于臃肿，它甚至为数据集中每个样本都分配了一个叶节点。

图9-3中的散点图，看上去与图9-1非常相似。但如果仔细地观察y轴就会发现，前者的数量级是后者的100倍。这将不是问题，对吧？现在用该数据来构建一棵新的树（数据存放在ex2.txt中），在Python提示符下输入以下命令：

```txt
>>> myDat2 = regTrees.loadDataSet('ex2.txt')
>>> myMat2 = mat(myDat2)
>>> regTrees.createTree(myMat2) 
```

```json
{'spInd':0,'spVal':matrix([[0.499171]]]》，'right':{'spInd':0, 'spVal': matrix([[0.457563]]），'right':-3.6244789069767438， 'left':7.9699461249999999}，'1   
0，'spVal':matrix([[0.958512]])，'right':112.42895575000001, 'left':105.248   
2350000001}\}}
```

![](images/6e774126639a7c6539d512b5d6e1134599f5259b29a311b8014a40fe1fdee335.jpg)  
图9-3 将图9-1的数据的y轴放大100倍后的新数据集

不知你注意到没有，从图9-1数据集构建出来的树只有两个叶节点，而这里构建的新树则有很多叶节点。产生这个现象的原因在于，停止条件tols对误差的数量级十分敏感。如果在选项中花费时间并对上述误差容忍度取平方值，或许也能得到仅有两个叶节点组成的树：

```python
>>> regTrees.createTree(myMat2, ops=(10000,4))
{'spInd': 0, 'spVal': matrix([[0.499171]]), 'right': -2.6377193297872341, 'left': 101.35815937735855} 
```

然而，通过不断修改停止条件来得到合理结果并不是很好的办法。事实上，我们常常甚至不确定到底需要寻找什么样的结果。这正是机器学习所关注的内容，计算机应该可以给出总体的概貌。

下节将讨论后剪枝，即利用测试集来对树进行剪枝。由于不需要用户指定参数，后剪枝是一个更理想化的剪枝方法。

# 9.4.2 后剪枝

使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大、

足够复杂，便于剪枝。接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并。

函数prune()的伪代码如下：

基于已有的树切分测试数据：

如果存在任一子集是一棵树，则在该子集递归剪枝过程

计算将当前两个叶节点合并后的误差

计算不合并的误差

如果合并会降低误差的话，就将叶节点合并

为了解实际效果，打开regTrees.py并输入程序清单9-3的代码。

# 程序清单9-3 回归树剪枝函数

```python
def isTree(obj):
    return (type(obj).__name__ == 'dict')
def getMean-tree):
    if isTree-tree['right']):
        tree['right'] = mean.tree['right']
    if isTree-tree['left']):
        tree['left'] = mean.tree['left']
    return (tree['left'] + tree['right']) / 2.0
def prune-tree, testData):
    shape(testData)[0] == 0: return mean.tree)
    if (isTree-tree['right']) or isTree-tree['left']):
        lSet, rSet = binSplitDataSet(testData, tree['spInd'],
tree['spVal'])) if isTree-tree['left']):
    tree['left'] = prune.tree['left'], lSet)
if isTree-tree['right']):
    tree['right'] = prune.tree['right'], rSet)
if not isTree-tree['left']) and not isTree-tree['right']):
    lSet, rSet = binSplitDataSet(testData, tree['spInd'],
tree['spVal'])) errorNoMerge = sum(power(lSet[:, -1] - tree['left'], 2)) + \sum(power(rSet[:, -1] - tree['right'], 2))
treeMean = (tree['left'] + tree['right']) / 2.0
errorMerge = sum(power(testData[:, -1] - treeMean, 2))
if errorMerge < errorNoMerge:
    print "merging"
    return treeMean
else: return tree
else: return tree 
```

程序清单9-3中包含三个函数：isTree()、getMean()和prune()。其中isTree()用于测试输入变量是否是一棵树，返回布尔类型的结果。换句话说，该函数用于判断当前处理的节点是否是叶节点。

函数getMean()是一个递归函数，它从上往下遍历树直到叶节点为止。如果找到两个叶节点则计算它们的平均值。该函数对树进行塌陷处理（即返回树平均值），在prune()函数中调用该函数时应明确这一点。

程序清单9-3的主函数是prune()，它有两个参数：待剪枝的树与剪枝所需的测试数据

testData。prune()函数首先需要确认测试集是否为空①。一旦非空，则反复递归调用函数prune()对测试数据进行切分。因为树是由其他数据集（训练集）生成的，所以测试集上会有一些样本与原数据集样本的取值范围不同。一旦出现这种情况应当怎么办？数据发生过拟合应该进行剪枝吗？或者模型正确不需要任何剪枝？这里假设发生了过拟合，从而对树进行剪枝。

接下来要检查某个分支到底是子树还是节点。如果是子树，就调用函数 prune() 来对该子树进行剪枝。在对左右两个分支完成剪枝之后，还需要检查它们是否仍然还是子树。如果两个分支已经不再是子树，那么就可以进行合并。具体做法是对合并前后的误差进行比较。如果合并后的误差比不合并的误差小就进行合并操作，反之则不合并直接返回。

接下来看看实际效果，将程序清单9-3的代码添加到regTrees.py文件并保存，在Python提示符下输入下面的命令：

```txt
>>> reload(regTrees)
<module 'regTrees' from 'regTrees.pyc'> 
```

为了创建所有可能中最大的树，输入如下命令：

>>>myTree $\equiv$ regTrees.createTree(myMat2，ops=（0,1))

输入以下命令导入测试数据：

```txt
>>> myDatTest = regTrees.loadDataSet('ex2test.txt')
>>> myMat2Test = mat(myDatTest) 
```

输入以下命令，执行剪枝过程：

```txt
>>> regTrees.prune(myTree, myMat2Test)
merging
merging
merging
...
merging
{'spInd': 0, 'spVal': matrix([[0.499171]]), 'right': {'spInd': 0, 'spVal': }
...
01, 'left': {'spInd': 0, 'spVal': matrix([[0.960398]]), 'right': 123.559747,
    'left': 112.386764}}}, 'left': 92.523991499999994}}}) 
```

可以看到，大量的节点已经被剪枝掉了，但没有像预期的那样剪枝成两部分，这说明后剪枝可能不如预剪枝有效。一般地，为了寻求最佳模型可以同时使用两种剪枝技术。

下节将重用部分已有的树构建代码来创建一种新的树。该树仍采用二元切分，但叶节点不再是简单的数值，取而代之的是一些线性模型。

# 9.5 模型树

用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的分段线性（piecewise linear）是指模型由多个线性片段组成。如果读者仍不清楚，下面很快就会给出样例来帮助理解。考虑图9-4中的数据，如果使用两条直线拟

合是否比使用一组常数来建模好呢？答案显而易见。可以设计两条分别从 $0.0 \sim 0.3$ 、从 $0.3 \sim 1.0$ 的直线，于是就可以得到两个线性模型。因为数据集里的一部分数据（ $0.0 \sim 0.3$ ）以某个线性模型建模，而另一部分数据（ $0.3 \sim 1.0$ ）则以另一个线性模型建模，因此我们说采用了所谓的分段线性模型。

决策树相比于其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。

![](images/41e9f6e2c8f8f65979d566e2bf86240aa874b0a7bd239df32f2e1b72a7079ec3.jpg)  
图9-4 用来测试模型树构建函数的分段线性数据

前面的代码稍加修改就可以在叶节点生成线性模型而不是常数值。下面将利用树生成算法对数据进行切分，且每份切分数据都能很容易被线性模型所表示。该算法的关键在于误差的计算。

前面已经给出了树构建的代码，但是这里仍然需要给出每次切分时用于误差计算的代码。不知道读者是否还记得之前createTree()函数里有两个参数从未改变过。回归树把这两个参数固定，而此处略做修改，从而将前面的代码重用于模型树。

下一个问题就是，为了找到最佳切分，应该怎样计算误差呢？前面用于回归树的误差计算方法这里不能再用。稍加变化，对于给定的数据集，应该先用线性的模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值。最后将这些差值的平方求和就得到了所需的误差。为了解实际效果，打开regTrees.py文件并加入如下代码。

# 程序清单9-4 模型树的叶节点生成函数

```python
def linearSolve(dataSet):
    m, n = shape(dataSet)
    X = mat(ones((m, n)))
    Y = mat(ones((m, 1)))
    X[:, 1:n] = DataSet[:, 0:n-1]; Y = DataSet[:, -1]
    xTx = X.T*X
    if linalg.det(xTx) == 0.0:
        raise NameError('This matrix is singular, cannot do inverse, \n try increasing the second value of ops')
    ws = xTx.I * (X.T * Y)
    return ws, X, Y
def modelLeaf(dataSet):
    ws, X, Y = linearSolve(dataSet)
    return ws
def modelErr(dataSet):
    ws, X, Y = linearSolve(dataSet)
    yHat = X * ws
    return sum(power(Y - yHat, 2)) 
```

上述程序清单中的第一个函数是linearSolve()，它会被其他两个函数调用。其主要功能是将数据集格式化成目标变量Y和自变量X①。与第8章一样，X和Y用于执行简单的线性回归。另外在这个函数中也应当注意，如果矩阵的逆不存在也会造成程序异常。

第二个函数modelLeaf()与程序清单9-2里的函数regLeaf()类似，当数据不再需要切分的时候它负责生成叶节点的模型。该函数在数据集上调用linearSolve()并返回回归系数ws。

最后一个函数是modelErr()，可以在给定的数据集上计算误差。它与程序清单9-2的函数regErr()类似，会被chooseBestSplit()调用来找到最佳的切分。该函数在数据集上调用linearSolve()，之后返回yHat和Y之间的平方误差。

至此，使用程序清单9-1和程序清单9-2中的函数构建模型树的全部代码已经完成。为了解实际效果，保存regTrees.py文件并在Python提示符下输入：

```txt
>>> reload(regTrees)
<module 'regTrees' from 'regTrees.pyc'> 
```

图9-4的数据已保存在一个用tab键为分隔符的文本文件exp2.txt里。

```txt
>>> myMat2 = mat(regTrees.loadDataSet('exp2.txt')) 
```

为了调用函数createTree()和模型树的函数，需将模型树函数作为createTree()的参数，输入下面的命令：

```txt
>>> regTrees.createTree(myMat2, regTrees.modelLeaf, regTrees.modelErr,
(1, 10))
{'spInd': 0, 'spVal': matrix([[0.285477]]), 'right': matrix([[3.46877936], [1.18521743]], 'left': matrix([[1.69855694e-03],
[1.19647739e+01]])} 
```

可以看到，该代码以0.285477为界创建了两个模型，而图9-4的数据实际在0.3处分段。createTree()生成的这两个线性模型分别是y=3.468+1.1852和y=0.0016985+11.96477x,

与用于生成该数据的真实模型非常接近。该数据实际是由模型 $y = 3.5 + 1.0x$ 和 $y = 0 + 12x$ 再加上高斯噪声生成的。在图9-5上可以看到图9-4的数据以及生成的线性模型。

![](images/e6ebb8cbf8b814db4a150697b6a9210dcd5c6ec2001d1666429e87ec11872c18.jpg)  
图9-5 在图9-4数据集上应用模型树算法得到的结果

模型树、回归树以及第8章里的其他模型，哪一种模型更好呢？一个比较客观的方法是计算相关系数，也称为 $\mathbb{R}^2$ 值。该相关系数可以通过调用NumPy库中的命令corrcoef(yHat，y, rowvar=0)来求解，其中yHat是预测值，y是目标变量的实际值。

前一章使用了标准的线性回归法，本章则使用了树回归法，下面将通过实例对二者进行比较，最后用函数corrcoef()来分析哪个模型是最优的。

# 9.6 示例：树回归与标准回归的比较

前面介绍了模型树、回归树和一般的回归方法，下面测试一下哪个模型最好。本节首先给出一些函数，它们可以在树构建好的情况下对给定的输入进行预测，之后利用这些函数来计算三种回归模型的测试误差。这些模型将在某个数据上进行测试，该数据涉及人的智力水平和自行车的速度的关系。

这里的数据是非线性的，不能简单地使用第8章的全局线性模型建模。当然这里也需要声明一下，此数据纯属虚构。

下面先给出在给定输入和树结构情况下进行预测的几个函数。打开regTrees.py并加入如下代码。

程序清单9-5 用树回归进行预测的代码  
```python
def regTreeEval(model, inDat):
    return float(model)
def modelTreeEval(model, inDat):
    n = shape(inDat) [1]
    X = mat(ones((1, n+1)))
    X[:, 1:n+1] = inDat
    return float(X*model)
def treeForeCast-tree, inData, modelEval=regTreeEval):
    if not isTree-tree): return modelEval.tree, inData)
    if inData-tree['spInd'] > tree['spVal']: 
        if isTree-tree['left']):
            return treeForeCast-tree['left'], inData, modelEval)
        else:
            return modelEval-tree['left'], inData)
    else:
        if isTree-tree['right']):
            return treeForeCast-tree['right'], inData, modelEval)
        else:
            return modelEval-tree['right'], inData)
def createForeCast-tree, testData, modelEval=regTreeEval):
    m=len(testData)
    yHat=yHat[i,0] = treeForeCast(tree, mat(testData[i]), modelEval)
    return yHat 
```

对于输入的单个数据点或者行向量，函数treeForeCast()会返回一个浮点值。在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。调用函数treeForeCast()时需要指定树的类型，以便在叶节点上能够调用合适的模型。参数modelEval是对叶节点数据进行预测的函数的引用。函数treeForeCast()自顶向下遍历整棵树，直到命中叶节点为止。一旦到达叶节点，它就会在输入数据上调用modelEval()函数，而该函数的默认值是regTreeEval()。

要对回归树叶节点进行预测，就调用函数regTreeEval(); 要对模型树节点进行预测时，就调用modelTreeEval()函数。它们会对输入数据进行格式化处理，在原数据矩阵上增加第0列，然后计算并返回预测值。为了与函数modelTreeEval()保持一致，尽管regTreeEval()只使用一个输入，但仍保留了两个输入参数。

最后一个函数是createForCast()，它会多次调用treeForeCast()函数。由于它能够以向量形式返回一组预测值，因此该函数在对整个测试集进行预测时非常有用。下面很快会看到这一点。

接下来考虑图9-6所示的数据。该数据是我从多个骑自行车的人那里收集得到的。图中给出骑自行车的速度和人的智商之间的关系。下面将基于该数据集建立多个模型并在另一个测试集上进行测试。对应的训练集数据保存在文件bikeSpeedVsIq_train.txt中，而测试集数据保存在文件bikeSpeedVsIq_test.txt中。

![](images/b136a326f3be2050f2671586db7eda8fc4e26d97069181cb8947a8729f8a2a11.jpg)  
图9-6 人们骑自行车的速度和他们智商之间的关系数据。该数据用于比较树回归模型和普通的线性回归模型

下面将为图9-6的数据构建三个模型。首先，将程序清单9-5中的代码保存为regTrees.py，然后在Python提示符下输入以下命令：

```txt
>>>reload(regTrees) 
```

接下来，利用该数据创建一棵回归树：

```txt
>>> trainMat = mat(regTrees.loadDataSet('bikeSpeedVsIq_train.txt'))
>>> testMat = mat(regTrees.loadDataSet('bikeSpeedVsIq_test.txt'))
>>> myTree = regTrees.createTree(trainMat, ops=(1,20))
>>> yHat = regTrees.createForeCast(myTree, testMat[:,0])
>>> corrcoef(yHat, testMat[:,1], rowvar=0) [0,1]
0.96408523182221306 
```

同样地，再创建一棵模型树：

```txt
>>> myTree = regTrees.createTree(trainMat, regTrees.modelLeaf, regTrees.modelErr, (1, 20))
>>> yHat = regTrees.createForeCast(myTree, testMat[:, 0], regTrees.modelTreeEval)
>>> corrcoef(yHat, testMat[:, 1], rowvar=0) [0, 1]
0.9760412191380623 
```

我们知道， $\mathbb{R}^2$ 值越接近1.0越好，所以从上面的结果可以看出，这里模型树的结果比回归树好。下面再看看标准的线性回归效果如何，这里无须导入第8章的任何代码，本章已实现过一个线性方程求解函数linearSolve()：

```csv
>>ws,X,Y=regTrees.regSolve(trainMat)   
>>ws   
matrix([[37.58916794]， [6.18978355]]）
```

为了得到测试集上所有的yHat预测值，在测试数据上循环执行：

```python
>>> for i in range(shape(testMat)[0]):  
    ... yHat[i] = testMat[i, 0] * ws[1, 0] + ws[0, 0] 
```

最后来看一下 $\mathbb{R}^2$ 值：

```txt
>>> corrcoef(yHat, testMat[:,1], rowvar=0) [0,1] 0.94346842356747584 
```

可以看到，该方法在 $\mathbb{R}^2$ 值上的表现上不如上面两种树回归方法。所以，树回归方法在预测复杂数据时会比简单的线性模型更有效，相信读者对这个结论也不会感到意外。下面将展示如何对回归模型进行定性的比较。

下面使用Python提供的框架来构建图形用户界面（GUI），读者可以使用该GUI来探究不同的回归工具。

# 9.7 使用Python的Tkinter库创建GUI

机器学习给我们提供了一些强大的工具，能从未知数据中抽取出有用的信息。因此，能否将这些信息以易于人们理解的方式呈现十分重要。再者，假如人们可以直接与算法和数据交互，将可以比较轻松地进行解释。如果仅仅只是绘制出一幅静态图像，或者只是在Python命令行中输出一些数字，那么对结果做分析和交流将非常困难。如果能让用户不需要任何指令就可以按照他们自己的方式来分析数据，就不需要对数据做出过多解释。其中一个能同时支持数据呈现和用户交互的方式就是构建一个图形用户界面（GUI，Graphical User Interface），如图9-7所示。

![](images/d17effabc63ae0d58b0aa2417dc8b336986c7afeb610918ae9251124614d7c24.jpg)  
图9-7 默认的treeExplore图形用户界面，该界面同时显示了输入数据和一个回归树模型，其中的参数tolN=10，tolS=1.0

# 示例：利用GUI对回归树调优

(1)收集数据：所提供的文本文件。  
(2) 准备数据：用Python解析上述文件，得到数值型数据。  
(3) 分析数据：用Tkinter构建一个GUI来展示模型和数据。  
(4) 训练算法：训练一棵回归树和一棵模型树，并与数据集一起展示出来。  
(5) 测试算法：这里不需要测试过程。  
(6) 使用算法：GUI使得人们可以在预剪枝时测试不同参数的影响，还可以帮助我们选择模型的类型。

接下来将介绍如何用Python来构建GUI。首先介绍如何利用一个现有的模块Tkinter来构建GUI，之后介绍如何在Tkinter和绘图库之间交互，最后通过创建GUI使人们能够自己探索模型树和回归树的奥秘。

# 9.7.1 用Tkinter创建GUI

Python有很多GUI框架，其中一个易于使用的Tkinter，是随Python的标准编译版本发布的。Tkinter可以在Windows、Mac OS和大多数的Linux平台上使用。

下面先从最简单的HelloWorld例子开始。在Python提示符下输入以下命令：

```python
>>> from Tkinter import *
>>> root = Tk() 
```

这时会出现一个小窗口或者一些错误提示。要想在窗口上显示一些文字，可以输入如下命令：

```txt
>>> myLabel = Label(root, text="Hello World")
>>> myLabel.grid() 
```

输入完毕后，文本框里就会显示出你刚才输入的文字。非常简单吧！

为了程序的完整，应该再输入以下命令：

```txt
>>> root mainloop() 
```

这条命令将启动事件循环，使该窗口在众多事件中可以响应鼠标点击、按键和重绘等动作。

Tkinter的GUI由一些小部件（Widget）组成。所谓小部件，指的是文本框（TextBox）、按钮（Button）、标签（Label）和复选按钮（Check Button）等对象。在刚才的Hello World例子中，标签myLabel就是其中唯一的小部件。当调用myLabel的.grid()方法时，就等于把myLabel的位置告诉了布局管理器（Geometry Manager）。Tkinter中提供了几种不同的布局管理器，其中的.grid()方法会把小部件安排在一个二维的表格中。用户可以设定每个小部件所在的行列位置。这里没有做任何设定，myLabel会默认显示在0行0列。

下面将所需的小部件集成在一起构建树管理器。建立一个新的Python文件treeExplore.py，并在其中加入程序清单9-6的代码。

程序清单9-6 用于构建树管理器界面的Tkinter小部件  
from numpy import \*   
from Tkinter import \*   
import regTrees   
def reDraw(tolS,tolN): pass   
def drawNewTree(): pass   
root=Tk()   
Label(root,text $=$ "Plot Place Holder").grid(row $= 0$ ,columnspan $= 3$ Label(root,text $=$ "tolN").grid(row $= 1$ column $= 0$ ) tolNentry $=$ Entry(root) tolNentry.grid(row $= 1$ column $= 1$ ) tolNentry.insert(0,'10')   
Label(root,text $=$ "tolS").grid(row $= 2$ column $= 0$ ) tolSentry $=$ Entry(root) tolSentry.grid(row $= 2$ column $= 1$ ) tolSentry.insert(0,'1.0')   
Button(root,text $=$ "ReDraw",command $\equiv$ drawNewTree).grid(row $= 1$ column $= 2$ ,rowspan $= 3$ chkBtVar $=$ IntVar() chkBtn $=$ Checkbutton(root,text $=$ "Model Tree",variable $=$ chkBtVar) chkBtn.grid(row $= 3$ ,column $= 0$ ,columnspan $= 2$ ）   
reDraw/rawDat $=$ mat(regTrees.loadDataSet('sine.txt'))   
reDraw.testDat $=$ arange(min(reDraw/rawDat[(:,0]),\ max(reDraw/rawDat[(:,0]），0.01)   
reDraw(1.0，10)   
root mainloop()

程序清单9-6的代码建立了一组Tkinter模块，并用网格布局管理器安排了它们的位置，这里还给出了两个绘制占位符（plot placeholder）函数，这两个函数的内容会在后面补充。这里所使用代码的格式与前面的例子一致，即首先创建一个Tk类型的根部件然后插入标签。读者可以使用.grid()方法设定行和列的位置。另外，也可以通过设定columnspan和rowspan的值来告诉布局管理器是否允许一个小部件跨行或跨列。除此之外还有其他设置项可供使用。

还有一些新的小部件暂时未使用到，这些小部件包括文本输入框（Entry）、复选按钮（Checkbutton）和按钮整数值（IntVar）等。其中Entry部件是一个允许单行文本输入的文本框。Checkbutton和IntVar的功能显而易见：为了读取Checkbutton的状态需要创建一个变量，也就是IntVar。

最后初始化一些与reDraw()关联的全局变量，这些变量会在后面用到。这里没有给出“退出”按钮，因为如果用户想退出，可以通过点击右上角关闭整个窗口，增加额外的退出按钮有点多此一举。假如读者真的想添加一个的话，可以输入下面的代码来实现：

```javascript
Button(root, text='Quit', fg="black", command=root.quit).grid(row=1, column=2) 
```

保存程序清单9-6的代码并执行，可以看到与图9-8类似的图。

![](images/4e59717a5005f0db1d9c1bc371961939a3d9627cbe2a9ba016f4fa5ea84a1c66.jpg)  
图9-8 使用多个Tkinter部件创建的树管理器

现在GUI可以按照要求正常运行了，下面利用它来绘图。接下来的小节中将在同一幅图上绘出原始数据集及其对应的树回归预测值。

# 9.7.2 集成Matplotlib和Tkinter

本书已经用Matplotlib绘制过很多图像，能否将这些图像直接放在GUI上呢？下面将首先介绍“后端”的概念，然后通过修改matplotlib后端（仅在我们的GUI上）达到在Tkinter的GUI上绘图的目的。

Mapplotlib的构建程序包含一个前端，也就是面向用户的一些代码，如plot()和scatter()方法等。事实上，它同时创建了一个后端，用于实现绘图和不同应用之间接口。通过改变后端可以将图像绘制在PNG、PDF、SVG等格式的文件上。下面将设置后端为TkAgg（Agg是一个C++的库，可以从图像创建光栅图）。TkAgg可以在所选GUI框架上调用Agg，把Agg呈现在画布上。我们可以在Tk的GUI上放置一个画布，并用.grid()来调整布局。

先用画布来替换绘制占位符，删掉对应标签并添加以下代码：

```txt
reDraw.f = Figure(figsize=(5,4), dpi=100)  
reDraw.canvas = FigureCanvasTkAgg(reDraw.f, master=root)  
reDraw.canvas.show()  
reDraw.canvas.gettkWidget().grid(row=0, columnspan=3) 
```

现在将树创建函数与该画布链接起来。看一下实际效果，打开treeExplore.py并添加下面的代码。注意我们之前实现过reDraw()和drawTree()的存根（stub），确保同一个函数不要重复出现。

# 程序清单9-7 Matplotlib和Tkinter的代码集成

```python
import matplotlib   
matplotlib.use('TkAgg')   
from matplotlib.pyplot.backendtkagg import FigureCanvasTkAgg   
from matplotlib.figure import Figure 
```

——译者注

```python
def reDraw(tolS,tolN):
    reDraw.f clf()
    reDraw.a = reDraw.f.add_subplot(111)
    ifCHKBtVar.get():
        iftolN<2:tolN=2
            myTree=regTrees.createTree(reDraw现阶段,regTrees.modelLeaf,\regTrees.modelErr,(tolS,tolN))
            yHat=regTrees.createForeCast(myTree,reDraw.testDat,\regTrees.modelTreeEval)
        else:
            myTree=regTrees.createTree(reDraw现阶段,ops=(tolS,tolN))
            yHat=regTrees.createForeCast(myTree,reDraw.testDat)
    reDraw.a.scan(reDraw现阶段[(:,0],reDraw现阶段[(:,1],s=5)
    reDraw.a.plot(reDraw现阶段,yHat,linewidth=2.0)
    reDraw.canvas.show()
defgetInputs():
    try:tolN=int(tolNentry.get())
except:
    tolN=10
print"enter Integer for tolN"
tolNentry(delete(0,END)
tolNentry.insert(0,'10')
try:tolS=float(tolSentry.get())
except:
    tolS=1.0
print"enter Float for tolS"
tolSentry(delete(0,END)
tolSentry.insert(0,'1.0')
return tolN,tols
def drawNewTree():
    tolN,tolS=getInputs()
    reDraw(tolS,tolN) 
```

上述程序中一开始导入Matplotlib文件并设定后端为TkAgg。接下来的两个import声明将TkAgg和Matplotlib图链接起来。

先来介绍函数drawNewTree()。从程序清单9-6可知，在有人点击ReDraw按钮时就会调用该函数。函数实现了两个功能：第一，调用getInputs()方法得到输入框的值；第二，利用该值调用reDraw()方法生成一个漂亮的图。下面对这些函数进行逐个介绍。

函数getInputs()试图理解用户的输入并防止程序崩溃。其中tols期望的输入是浮点数，而tolN期望的输入是整数。为了得到用户输入的文本，可以在Entry部件上调用.get()方法。虽然表单验证会在GUI编程时花费大量的时间，但这一点对于用户体验来说必不可少。另外，这里使用了try:和except:模式。如果Python可以把输入文本解析成整数就继续执行，如果不能识别则输出错误消息，同时清空输入框并恢复其默认值①。对tols而言也存在同样的处理过程，最后返回输入值。

函数reDraw()的主要目的是把树绘制出来。该函数假定输入是合法的，它首先要做的是清空之前的图像，使得前后两个图像不会重叠。清空时图像的各个子图也都会被清除，所以

需要重新添加一个新图。接下来函数会检查复选框是否被选中②。根据复选框是否被选中，确定基于tolS和tolN参数构建模型树还是回归树。当树构建完成之后就对测试集testDat进行预测，该测试集与训练集有相同的范围且点的分布均匀。最后，真实数据和预测值都被绘制出来。具体实现是，真实值采用scatter()方法绘制，而预测值则采用plot()方法绘制，这是因为scatter()方法构建的是离散型散点图，而plot()方法则构建连续曲线。

下面看一下实际效果，保存treeExplore.py并执行。如果读者使用开发环境IDE来编码，那么可以用run命令来运行程序。在命令行下可以直接使用命令python treeExplore.py来运行。执行完之后应该可以看到类似于图9-7的结果。

图9-7的GUI包含了图9-8所有的小部件，而占位符采用Matplotlib替换。默认情况下会给出一棵包含八个叶节点的回归树（参见图9-7）。我们也可以尝试模型树。通过选中模型树的复选框，再点击ReDraw按钮，就应该可以看到类似于图9-9的模型树结果。

![](images/cd728dfaf5e925b7e87ae03c3130b5b04257b42d911b606a53bddee14ffdcdad.jpg)  
图9-9 用treeExplore的GUI构建的模型树，该图使用了与图9-7相同的数据和参数。与回归树相比，模型树取得了更好的预测效果

读者可以在上述treeExplore中尝试不同的参数值。整个数据集包含200个样本，可以将tolN设为150后观察执行效果。为构建尽可能大的树，应当将tolN设为1，将tolS设为0。读者可以测试一下并观察执行的效果。

# 9.8 本章小结

数据集中经常包含一些复杂的相互关系，使得输入数据和目标变量之间呈现非线性关系。对这些复杂的关系建模，一种可行的方式是使用树来对预测值分段，包括分段常数或分段直线。一般采用树结构来对这种数据建模。相应地，若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树。

CART算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART算法构建模型树和回归树。该算法构建出的树会倾向于对数据过拟合。一棵过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝（在树的构建过程中就进行剪枝）和后剪枝（当树构建完毕再进行剪枝），预剪枝更有效但需要用户定义一些参数。

Tkinter是Python的一个GUI工具包。虽然并不是唯一的包，但它最常用。利用Tkinter，我们可以轻松绘制各种部件并灵活安排它们的位置。另外，可以为Tkinter构造一个特殊的部件来显示Matplotlib绘出的图。所以，Matplotlib和Tkinter的集成可以构建出更强大的GUI，用户可以以更自然的方式来探索机器学习算法的奥妙。

本章是回归的最后一章，希望读者没有错过。接下来我们将离开监督学习的岛屿，驶向无监督学习的未知港湾。在回归和分类（监督学习）中，目标变量的值是已知的。在后面的章节将会看到，无监督学习中上述条件将不再成立。下一章的主要内容是K-均值聚类算法。

# 无监督学习

这一部分介绍的是无监督机器学习方法。该主题与前两部分有所不同。在无监督学习中，类似分类和回归中的目标变量事先并不存在。与前面“对于输入数据X能预测变量Y”不同的是，这里要回答的问题是：“从数据X中能发现什么？”这里需要回答的X方面的问题可能是：“构成X的最佳6个数据簇都是哪些？”或者“X中哪三个特征最频繁共现？”

第10章介绍了无监督学习中的聚类（将相似项聚团）方法，包括k均值聚类算法。第11章介绍了基于Apriori算法的关联分析或者称购物篮分析。关联分析可以用于回答“哪些物品经常被同时购买？”之类的问题。无监督学习部分的最后一章，即第12章将介绍一个更高效的关联分析算法：FP-growth算法。

# 第10章

# 利用K-均值聚类算法对未标注数据分组

# 本章内容

□K-均值聚类算法  
口对聚类得到的簇进行后处理  
□二分K-均值聚类算法  
□对地理位置进行聚类

在2000年和2004年的美国总统大选中，候选人的得票数比较接近或者说非常接近。任一候选人得到的普选票数的最大百分比为 $50.7\%$ ，而最小百分比为 $47.9\%$ 。如果 $1\%$ 的选民将手中的选票投向另外的候选人，那么选举结果就会截然不同。实际上，如果妥善加以引导与吸引，少部分选民就会转换立场。尽管这类选举者占的比例较低，但当候选人的选票接近时，这些人的立场无疑会对选举结果产生非常大的影响①。如何找出这类选民，以及如何在有限的预算下采取措施来吸引他们？答案就是聚类（Clustering）。

接下来介绍如何通过聚类实现上述目标。首先，收集用户的信息，可以同时收集用户满意或不满意的信息，这是因为任何对用户重要的内容都可能影响用户的投票结果。然后，将这些信息输入到某个聚类算法中。接着，对聚类结果中的每一个簇（最好选择最大簇），精心构造能够吸引该簇选民的消息。最后，开展竞选活动并观察上述做法是否有效。

聚类是一种无监督的学习，它将相似的对象归到同一个簇中。它有点像全自动分类。聚类方法几乎可以应用于所有对象，簇内的对象越相似，聚类的效果越好。本章要学习一种称为K-均值（K-means）聚类的算法。之所以称之为K-均值是因为它可以发现k个不同的簇，且每个簇的中心采用簇中所含值的均值计算而成。下面会逐步介绍该算法的更多细节。

在介绍K-均值算法之前，先讨论一下簇识别（cluster identification）。簇识别给出聚类结

果的含义。假定有一些数据，现在将相似数据归到一起，簇识别会告诉我们这些簇到底都是些什么。聚类与分类的最大不同在于，分类的目标事先已知，而聚类则不一样。因为其产生的结果与分类相同，而只是类别没有预先定义，聚类有时也被称为无监督分类（unsupervised classification）。

聚类分析试图将相似对象归入同一簇，将不相似对象归到不同簇。相似这一概念取决于所选择的相似度计算方法。前面章节已经介绍了不同的相似度计算方法，后续章节它们会继续出现。到底使用哪种相似度计算方法取决于具体应用。

下面会构建K-均值方法并观察其实际效果。接下来还会讨论简单K-均值算法中的一些缺陷。为了解决其中的一些缺陷，可以通过后处理来产生更好的簇。接着会给出一个更有效的称为二分K-均值（bisecting k-means）的聚类算法。本章的最后会给出一个实例，该实例应用二分K-均值算法来寻找同时造访多个夜生活热点地区的最佳停车位。

# 10.1 K-均值聚类算法

# K-均值聚类

优点：容易实现。

缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢。

适用数据类型：数值型数据。

K-均值是发现给定数据集的 $k$ 个簇的算法。簇个数 $k$ 是用户给定的，每一个簇通过其质心（centroid），即簇中所有点的中心来描述。  
K-均值算法的工作流程是这样的。首先，随机确定 $k$ 个初始点作为质心。然后将数据集中的每个点分配到一个簇中，具体来讲，为每个点找距其最近的质心，并将其分配给该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。

上述过程的伪代码表示如下：

创建k个点作为起始质心（经常是随机选择）

当任意一个点的簇分配结果发生改变时

对数据集中的每个数据点

对每个质心

计算质心与数据点之间的距离

将数据点分配到距其最近的簇

对每一个簇，计算簇中所有点的均值并将均值作为质心

# K-均值聚类的一般流程

(1)收集数据：使用任意方法。  
(2) 准备数据：需要数值型数据来计算距离，也可以将标称型数据映射为二值型数据再用于距离计算。  
(3) 分析数据：使用任意方法。  
(4) 训练算法：不适用于无监督学习，即无监督学习没有训练过程。  
(5) 测试算法：应用聚类算法、观察结果。可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果。  
(6) 使用算法：可以用于所希望的任何应用。通常情况下，簇质心可以代表整个簇的数据来做出决策。

上面提到“最近”质心的说法，意味着需要进行某种距离计算。读者可以使用所喜欢的任意距离度量方法。数据集上K-均值算法的性能会受到所选距离计算方法的影响。下面给出K-均值算法的代码实现。首先创建一个名为kMeans.py的文件，然后将下面程序清单中的代码添加到文件中。

程序清单10-1 K-均值聚类支持函数  
from numpy import \*   
def loadDataSet(fileName): dataMat $= []$ fr $=$ open(fileName) for line in fr.readlines(): curLine $=$ line.strip().split("\\t")FLTLine $=$ map(float,curLine) dataMat.append(FLTLine) return dataMat   
def distEclud vecA,vecB): return sqrt(sum(powervecA-vecB,2)))   
def randCent(dataSet,k): n $=$ shape(dataSet)[1] centroids $=$ mat(zeros((k,n))) for j in range(n): minJ $=$ min(dataSet(:,j)] rangeJ $=$ float(max(dataSet(:,j)] -minJ) centroids(:,j] $=$ minJ $^+$ rangeJ \* random rand(k,1) return centroids

程序清单10-1中的代码包含几个K-均值算法中要用到的辅助函数。第一个函数loadData-Set()和上一章完全相同，它将文本文件导入到一个列表中。文本文件每一行为tab分隔的浮点数。每一个列表会被添加到dataMat中，最后返回dataMat。该返回值是一个包含许多其他列表的列表。这种格式可以很容易将很多值封装到矩阵中。

下一个函数distEclud()计算两个向量的欧式距离。这是本章最先使用的距离函数，也可以使用其他距离函数。

最后一个函数是randCent(), 该函数为给定数据集构建一个包含 $k$ 个随机质心的集合。随机质心必须要在整个数据集的边界之内, 这可以通过找到数据集每一维的最小和最大值来完成。然后生成0到1.0之间的随机数并通过取值范围和最小值, 以便确保随机点在数据的边界之内。接下来看一下这三个函数的实际效果。保存kMeans.py文件, 然后在Python提示符下输入:

```python
>>> import kMeans  
>>> from numpy import * 
```

要从文本文件中构建矩阵，输入下面的命令（第10章的源代码中给出了testSet.txt的内容）：

>>>mat $=$ mat(kMeans.loadDataSet('testSet.txt'))

读者可以了解一下这个二维矩阵，后面将使用该矩阵来测试完整的K-均值算法。下面看看randCent()函数是否正常运行。首先，先看一下矩阵中的最大值与最小值：

```txt
>>> min(matMat[:,0])
matrix([[ -5.379713]])
>>> min(matMat[:,1])
matrix([[ -4.232586]])
>>> max(matMat[:,1])
matrix([[ 5.1904]])
>>> max(matMat[:,0])
matrix([[ 4.838138]]) 
```

然后看看randCent()函数能否生成min到max之间的值：

```txt
>>>kMeans.randCent(matMat，2)   
matrix([[-3.24278889，-0.04213842]， [-0.92437171，3.19524231]]])
```

从上面的结果可以看到，函数randCent()确实会生成min到max之间的值。上述结果表明，这些函数都能够按照预想的方式运行。最后测试一下距离计算方法：

```html
>>>kMeans.distEclude(dmMat[0],mat[1]) 5.184632816681332 
```

所有支持函数正常运行之后，就可以准备实现完整的K-均值算法了。该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。这个过程重复数次，直到数据点的簇分配结果不再改变为止。打开kMeans.py文件输入下面程序清单中的代码。

# 程序清单10-2 K-均值聚类算法

```python
def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m, 2)))
    centroids = createCent(dataSet, k)
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        for i in range(m): 
```

minDist $=$ inf; minIndex $= -1$ for j in range(k): distJI $=$ distMeas(centroids[j,:],dataSet[i,:]) if distJI $<$ minDist: minDist $=$ distJI; minIndex $= j$ if clusterAssment[i,0] $= \mathrm{minIndex}$ : clusterChanged $=$ True clusterAssment[i,:] $= \mathrm{minIndex}$ ,minDist\*\*2 print centroids 更新质心的位置 2 for cent in range(k): ptsInClust $=$ dataSet[nonzero(clusterAssment[:,0].A==cent)[0]] centroids[cent,:] $= \mathrm{mean}$ (ptsInClust，axis=0)   
return centroids,clusterAssment

上述清单给出了K-均值算法。kMeans()函数接受4个输入参数。只有数据集及簇的数目是必选参数，而用来计算距离和创建初始质心的函数都是可选的。kMeans()函数一开始确定数据集中数据点的总数，然后创建一个矩阵来存储每个点的簇分配结果。簇分配结果矩阵clusterAssment包含两列：一列记录簇索引值，第二列存储误差。这里的误差是指当前点到簇质心的距离，后边会使用该误差来评价聚类的效果。

按照上述方式（即计算质心-分配-重新计算）反复迭代，直到所有数据点的簇分配结果不再改变为止。程序中可以创建一个标志变量clusterChanged，如果该值为True，则继续迭代。上述迭代使用while循环来实现。接下来遍历所有数据找到距离每个点最近的质心，这可以通过对每个点遍历所有质心并计算点到每个质心的距离来完成①。计算距离是使用distMeas参数给出的距离函数，默认距离函数是distEclud()，该函数的实现已经在程序清单10-1中给出。如果任一点的簇分配结果发生改变，则更新clusterChanged标志。

最后，遍历所有质心并更新它们的取值。具体实现步骤如下：首先通过数组过滤来获得给定簇的所有点；然后计算所有点的均值，选项axis = 0表示沿矩阵的列方向进行均值计算；最后，程序返回所有的类质心与点分配结果。图10-1给出了一个聚类结果的示意图。

![](images/a7ee550cc1f8c98b0d30489eebe31163ab65540ecef7ac59e01e564257e651bb.jpg)  
图10-1 K-均值聚类的结果示意图。图中数据集在三次迭代之后收敛。形状相似的数据点被分到同样的簇中，簇中心使用十字来表示

接下来看看程序清单10-2的运行效果。保存kMeans.py文件后，在Python提示符下输入：

```txt
>>> reload(kMeans) <module 'kMeans' from 'kMeans.pyc'> 
```

如果没有将前面例子中的mat数据复制过来，则可以输入下面的命令（记住要导入NumPy）：

>>>mat $\equiv$ mat(kMeans.loadDataSet('testSet.txt'))

现在就可以对mat中的数据点进行聚类处理。从图像中可以大概预先知道最后的结果应该有4个簇，所以可以输入如下命令：

>>myCentroids，clustAssing $\equiv$ kMeans.kMeans(matMat,4)  
[[-4.06724228 0.21993975]  
[0.73633558 -1.41299247]  
[-2.59754537 3.15378974]  
[4.49190084 3.46005807]]  
[[-3.62111442 -2.36505947]  
[2.21588922 -2.88365904]  
[-2.38799628 2.96837672]  
[2.6265299 3.10868015]]  
[[-3.53973889 -2.89384326]  
[2.65077367 -2.79019029]  
[-2.46154315 2.78737555]  
[2.6265299 3.10868015]]

上面的结果给出了4个质心。可以看到，经过3次迭代之后K-均值算法收敛。这4个质心以及原始数据的散点图在图10-1中给出。

到目前为止，关于聚类的一切进展都很顺利，但事情并不总是如此。接下来会讨论K-均值算法可能出现的问题及其解决办法。

# 10.2 使用后处理来提高聚类性能

前面提到，在K-均值聚类中簇的数目 $k$ 是一个用户预先定义的参数，那么用户如何才能知道 $k$ 的选择是否正确？如何才能知道生成的簇比较好呢？在包含簇分配结果的矩阵中保存着每个点的误差，即该点到簇质心的距离平方值。下面会讨论利用该误差来评价聚类质量的方法。

考虑图10-2中的聚类结果，这是在一个包含三个簇的数据集上运行K-均值算法之后的结果，但是点的簇分配结果值没有那么准确。K-均值算法收敛但聚类效果较差的原因是，K-均值算法收敛到了局部最小值，而非全局最小值（局部最小值指结果还可以但并非最好结果，全局最小值是可能的最好结果）。

一种用于度量聚类效果的指标是SSE（Sum of Squared Error，误差平方和），对应程序清单10-2中clusterAssment矩阵的第一列之和。SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。因为对误差取了平方，因此更加重视那些远离中心的点。一种肯定可以降低SSE值的方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是在保持簇数目不变的情况下提高簇的质量。

那么如何对图10-2的结果进行改进？你可以对生成的簇进行后处理，一种方法是将具有最大SSE值的簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行K-均值

算法，其中的 $k$ 设为2。

![](images/fc469de6d09773b198f45b0a45c971541f2868c676f01504eee2c36f670bb709.jpg)  
图10-2 由于质心随机初始化导致K-均值算法效果不好的一个例子，这需要额外的后处理操作来清理聚类结果

为了保持簇总数不变，可以将某两个簇进行合并。从图10-2中很明显就可以看出，应该将图下部两个出错的簇质心进行合并。可以很容易对二维数据上的聚类进行可视化，但是如果遇到40维的数据应该如何去做？

有两种可以量化的办法：合并最近的质心，或者合并两个使得SSE增幅最小的质心。第一种思路通过计算所有质心之间的距离，然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。接下来将讨论利用上述簇划分技术得到更好的聚类结果的方法。

# 10.3 二分K-均值算法

为克服K-均值算法收敛于局部最小值的问题，有人提出了另一个称为二分K-均值（bisecting K-means）的算法。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。上述基于SSE的划分过程不断重复，直到得到用户指定的簇数目为止。

二分K-均值算法的伪代码形式如下：

将所有点看成一个簇

当簇数目小于 $k$ 时

对于每一个簇

计算总误差

在给定的簇上面进行K-均值聚类（ $k = 2$ ）

计算将该簇一分为二之后的总误差

选择使得误差最小的那个簇进行划分操作

另一种做法是选择SSE最大的簇进行划分，直到簇数目达到用户指定的数目为止。这个做法听起来并不难实现。下面就来看一下该算法的实际效果。打开kMeans.py文件然后加入下面程序清单中的代码。

程序清单10-3 二分K-均值聚类算法  
```python
def biKmeans(dataSet, k, distMeas=distEclud):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m,2)))
    centroid0 = mean(dataSet, axis=0).tolist()
    centList = [centroid0]
    for j in range(m):
        clusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])
    while (len(lenList) < k):
        lowestSSE = inf
        for i in range(len(list):
            ptsInCurrCluster = \
                dataSet[nonzero(clusterAssment(:,0).A==i)[0],:]
            centroidMat, splitClustAss = \
                kMeans(ptsInCurrCluster, 2, distMeas)
            SSESplit = sum(splitClustAss(:,1))
            SSENotSplit = \
                sum(clusterAssment[nonzero(clusterAssment(:,0).A!=i)[0],1])
            print "sseSplit, and notSplit: ", SSESplit, SSENotSplit
            if (sseSplit + SSENotSplit) < lowestSSE:
                bestCentToSplit = i
                bestNewCents = centroidMat
                bestClustAss = splitClustAss.copy()
                lowestSSE = SSESplit + SSENotSplit
            bestClustAss[nonzero(bestClustAss(:,0).A==1)[0],0] = \
                len(len(list))
            bestClustAss[nonzero(bestClustAss(:,0).A==0)[0],0] = \
                bestCentToSplit
            print 'the bestCentToSplit is: ',bestCentToSplit
            print 'the len of bestClustAss is: ', len(bestClustAss)
            centList[bestCentToSplit] = bestNewCents[0,:]
            centList.append(bestNewCents[1,:])
            clusterAssment(nonzero(clusterAssment(:,0).A==\\
                    bestCentToSplit)[0],:]= bestClustAss
    return mat(list), clusterAssment 
```

上述程序中的函数与程序清单10-2中函数kMeans()的参数相同。在给定数据集、所期望的簇数目和距离计算方法的条件下，函数返回聚类结果。同kMeans()一样，用户可以改变所使用的距离计算方法。

该函数首先创建一个矩阵来存储数据集中每个点的簇分配结果及平方误差，然后计算整个数据集的质心，并使用一个列表来保留所有的质心①。得到上述质心之后，可以遍历数据集中所有点来计算每个点到质心的误差值。这些误差值将会在后面用到。

接下来程序进入while循环，该循环会不停对簇进行划分，直到得到想要的簇数目为止。可以通过考察簇列表中的值来获得当前簇的数目。然后遍历所有的簇来决定最佳的簇进行划分。为此需要比较划分前后的SSE。一开始将最小SSE置设为无穷大，然后遍历簇列表centList中的每一个簇。对每个簇，将该簇中的所有点看成一个小的数据集ptsInCurrCluster。将ptsInCurrCluster输入到函数kMeans()中进行处理（ $k = 2$ ）。K-均值算法会生成两个质心（簇），同时给出每个簇的误差值②。这些误差与剩余数据集的误差之和作为本次划分的误差。如果该划分的SSE值最小，则本次划分被保存。一旦决定了要划分的簇，接下来就要实际执行划分操作。划分操作很容易，只需要将要划分的簇中所有点的簇分配结果进行修改即可。当使用kMeans()函数并且指定簇数为2时，会得到两个编号分别为0和1的结果簇。需要将这些簇编号修改为划分簇及新加簇的编号，该过程可以通过两个数组过滤器来完成①。最后，新的簇分配结果被更新，新的质心会被添加到centList中。

当while循环结束时，同kMeans()函数一样，函数返回质心列表与簇分配结果。

下面看一下实际运行效果。将程序清单10-3中的代码添加到文件kMeans.py并保存，然后在Python提示符下输入：

```txt
>>> reload(kMeans)
<module 'kMeans' from 'kMeans.py'> 
```

可以在最早的数据集上运行上述过程，也可以通过如下命令来导入图10-2中那个“较难”的数据集：

```javascript
>>>mat3=mat(kMeans.loadDataSet('testSet2.txt')) 
```

要运行函数biKmeans()，输入如下命令：

```txt
>>> centList, myNewAssments = kMeans.biKmeans(matMat3, 3)
sseSplit, and notSplit: 491.233299302 0.0
the bestCentToSplit is: 0
the len of bestClustAss is: 60
sseSplit, and notSplit: 75.5010709203 35.9286648164
sseSplit, and notSplit: 21.40716341 455.304634485
the bestCentToSplit is: 0
the len of bestClustAss is: 40 
```

现在看看质心结果：

```txt
>>> centList  
{matrix([[[-3.05126255, 3.2361123]]]), matrix([[[-0.28226155, -2.4449763]]), matrix([[3.1084241, 3.0396009]])} 
```

上述函数可以运行多次，聚类会收敛到全局最小值，而原始的kMeans()函数偶尔会陷入局部最小值。图10-3给出了数据集及运行biKmeans()后的的质心的示意图。

![](images/ffca793c4ca7e6d23555a5a596a2e78b4ff59ca6917014666ba957811a0b628d.jpg)  
图10-3 运行二分K-均值算法后的簇分配示意图，该算法总是产生较好的聚类结果

前面已经运行了二分K-均值算法，下面将该算法应用于一些真实的数据集上。下一节将利用地图上的地理位置坐标进行聚类。

# 10.4 示例：对地图上的点进行聚类

假如有这样一种情况：你的朋友Drew希望你带他去城里庆祝他的生日。由于其他一些朋友也会过来，所以需要你提供一个大家都可行的计划。Drew给了你一些他希望去的地址。这个地址列表很长，有70个位置。我把这个列表保存在文件portland-Clubs.txt中，该文件和源代码一起打包。这些地址其实都在俄勒冈州的波特兰地区。

也就是说，一晚上要去70个地方！你要决定一个将这些地方进行聚类的最佳策略，这样就可以安排交通工具抵达这些簇的质心，然后步行到每个簇内地址。Drew的清单中虽然给出了地址，但是并没有给出这些地址之间的距离远近信息。因此，你要得到每个地址的纬度和经度，然后对这些地址进行聚类以安排你的行程。

# 示例：对于地理数据应用二分K-均值算法

(1)收集数据：使用Yahoo!PlaceFinder API收集数据。  
(2) 准备数据：只保留经纬度信息。  
(3) 分析数据：使用Matplotlib来构建一个二维数据图，其中包含簇与地图。  
(4) 训练算法：训练不适用无监督学习。   
(5) 测试算法：使用10.4节中的biKmeans()函数。  
(6) 使用算法：最后的输出是包含簇及簇中心的地图。

你需要一个服务来将地址转换为纬度和经度。幸运的是，雅虎提供了这样的服务。下面将介绍Yahoo! PlaceFinder API的使用方法。然后，对给出的地址坐标进行聚类，最后画出所有点以及簇中心，并看看聚类结果到底如何。

# 10.4.1 Yahoo! PlaceFinder API

雅虎的牛人们已经为我们提供了一个免费的地址转换API，该API对给定的地址返回该地址对应的纬度与经度。访问下面的URL可以了解更多细节：http://developer.yahoo.com/geo/placefinder/guide/。

为了使用该服务，需要注册以获得一个API key。具体地，你需要在Yahoo!开发者网络（http://developer.yahoo.com/）中进行注册。创建一个桌面应用后会获得一个appid。需要appid来使用geocoder。一个geocoder接受给定地址，然后返回该地址对应的经纬度。下面的代码将上述所有过程进行了封装处理。打开kMeans.py文件，然后加入下列代码。

程序清单10-4 Yahoo!PlaceFinder API  
importurllib   
importjson   
def geoGrab(stAddress，city): apiStem $\equiv$ 'http://where.yahooapis.com/geocode?' params $= \{\}$ params['flags'] $= \mathrm{J}^{\prime}$ params['appid'] $= \mathrm{ppp68N8t}$ params['location] $= \% s\% s'\%$ (stAddress，city)url.params $\equiv$ urllib.urlencode.params) yahooApi $\equiv$ apiStem $^+$ url_parameters print yahooApi c=urllib(urlopen(yahooApi) return json.loadc().read()   
fromtimeimportsleep   
def massPlaceFind(fileName): wf $=$ open('places.txt'，'w') for line in open(fileName).readlines(): line $=$ line.strip() lineArr $=$ line.split("\\t") retDict $=$ geoGrab(lineArr[1],lineArr[2]) if retDict['ResultSet'] ['Error'] $= = 0$ ： lat $=$ float(retDict['ResultSet'] ['Results'] [0] ['latitude']) lng $=$ float(retDict['ResultSet'] ['Results'] [0] ['longitude']) print"%s\\t%f\\t%f" % (lineArr[0],lat,lng) fw.write("%s\\t%f\\t%f\n' % (line,lat,lng)) else:print "error fetching" sleep(1) fw.close()

上述程序包含两个函数：geoGrab()与massPlaceFind()。函数geoGrab()从Yahoo!返回一个字典，massPlaceFind()将所有这些封装起来并且将相关信息保存到文件中。

在函数geoGrab()中，首先为Yahoo API设置apiStem，然后创建一个字典。你可以为字典设置不同值，包括flags = J，以便返回JSON格式的结果①。（不用担心你不熟悉JSON，它是一种用于序列化数组和字典的文件格式，本书不会看到任何JSON。JSON是JavaScript Object Notation的缩写，有兴趣的读者可以在www.json.org找到更多信息。）接下来使用url1ib的urlencode()函数将创建的字典转换为可以通过URL进行传递的字符串格式。最后，打开URL读取返回值。由于返回值是JSON格式的，所以可以使用JSON的Python模块来将其解码为一个字典。一旦返回了解码后的字典，也就意味着你成功地对一个地址进行了地理编码。

程序清单10-4中的第二个函数是massPlaceFind()。该函数打开一个tab分隔的文本文件，获取第2列和第3列结果。这些值被输入到函数geoGrab()中，然后需要检查geoGrab()的输出字典判断有没有错误。如果没有错误，就可以从字典中读取经纬度。这些值被添加到原来对应的行上，同时写到一个新的文件中。如果有错误，就不需要去抽取纬度和经度。最后，调用sleep()函数将massPlaceFind()函数延迟1秒。这样做是为了确保不要在短时间内过于频繁地调用API。如果频繁调用，那么你的请求可能会被封掉，所以将massPlaceFind()函数的调用延迟一下比较好。

保存kMeans.py文件后，在Python提示符下输入：

```txt
>>> reload(kMeans) <module 'kMeans' from 'kMeans.py'> 
```

要尝试geoGrab，输入街道地址和城市的字符串，比如：

```txt
>>> geoResults=kMeans.geoGrab('1 VA Center', 'Augusta, ME')
http://where.yahooapis.com/
geocode?flags=J&location=1+VA+Center+Augusta%2C+ME&appid=ppp68N6k 
```

实际使用的URL会被打印出来，通过这些URL，用户可以看到具体发生了什么。如果并不想看到URL，那么将程序清单10-4中的print语句注释掉也没关系。下面看一下返回结果，应该是一个很大的字典。

```python
>>> geoResults  
{u'ResultSet': {u'Locale': u'us_US', u'ErrorMessage': u'No error', u'Results': [[u'neighborhood': u'', u'house': u'1', u'county': u'Kennebec County', u'street': u'Center St', u'radius': 500, u'quality': 85, u'unit': u'', u'city': u'Augusta', u'countrycode': u'US', u'woeid': 12759521, u'xstreet': u'', u'line4': u'United States', u'line3': u'', u'line2': u'Augusta, ME 04330-6410', u'line1': u'1 Center St', u'state': u'Maine', u'latitude': u'44.307661', u'hash': u'B8BE9F5EE764C449', u'unittype': u'', u'offsetlat': u'44.307656', u'stecode': u'ME', u'postal': u'04330-6410', u"name': u'', u'uzip': u'04330', u'country': u'United States', u'longitude': u'-69.776608', u'countycode': u'', u'offsetlon': u'-69.776528', u'woetype': 11}], u'version': u'1.0', u'Error': 0, u'Found': 1, u'Quality': 87} 
```

上面给出的是一部只包含键ResultSet的字典，该字典又包含分别以Locale、Error-Message、Results、version、Error、Found和Quality为键的其他字典。

读者可以看一下所有这些键的内容，不过我们主要感兴趣的还是Error和Results。

Error键值给出的是错误编码。0意味着没有错误，其他任何值都代表没有获得要找的地址。

可以输入下面内容以获得错误编码：

```txt
>>> geoResults['ResultSet'] ['Error'] 0 
```

现在看一下纬度和经度，可以输入如下命令来实现：

```txt
>>> geoResults['ResultSet']{'Results'}[0]['longitude'] u'-69.776608'   
>>> geoResults['ResultSet'] ['Results'] [0]['latitude'] u'44.307661' 
```

上面给出的都是字符串，可以使用float()函数将它们转换为浮点数。下面看看在多行上的运行效果，输入命令执行程序清单10-4中的第二个函数：

```txt
>>> kMeans_massPlaceFind('portlandClubs.txt')  
Dolphin II 45.486502 -122.788346 
```

```txt
Magic Garden 45.524692 -122.674466  
Mary's Club 45.535101 -122.667390  
Montego's 45.504448 -122.500034 
```

这会在你的工作目录下生成一个称为places.txt的文本文件。接下来将使用这些点进行聚类，并将俱乐部以及它们的簇中心画在城市地图上。

# 10.4.2 对地理坐标进行聚类

现在我们有一个包含格式化地理坐标的列表，接下来可以对这些俱乐部进行聚类。在此过程中使用Yahoo! PlaceFinder API来获得每个点的纬度和经度。下面需要使用这些信息来计算数据点与簇质心的距离。

这个例子中要聚类的俱乐部给出的信息为经度和维度，但这些信息对于距离计算还不够。在北极附近每走几米的经度变化可能达到数10度；而在赤道附近走相同的距离，带来的经度变化可能只是零点几。可以使用球面余弦定理来计算两个经纬度之间的距离。为实现距离计算并将聚类后的俱乐部标识在地图上，打开kMeans.py文件，添加下面程序清单中的代码。

# 程序清单10-5 球面距离计算及簇绘图函数

```python
def distSLC vecA, vecB):
    a = sin vecA[0,1]*pi/180) * sin (vecB[0,1]*pi/180)
    b = cos (vecA[0,1]*pi/180) * cos (vecB[0,1]*pi/180) * \
        cos(pi * (vecB[0,0] - vecA[0,0]) / 180)
    return arccos(a + b)*6371.0
import matplotlib
import matplotlib.pyplot as plt
def clusterClubs(numClust=5):
    datList = []
    for line in open('places.txt').readlines():
        lineArr = line.split("\\t")
        datList.append ([float(lineArr[4]), float(lineArr[3])) 
```

datMat $=$ mat(matList)   
myCentroids,clustAssing $\equiv$ biKmeans(matMat，numClust， distMeas=distSLC)   
fig $=$ plt.figure()   
rect=[0.1,0.1,0.8,0.8]   
scatterMarkers $= [\textsf{S}^{\prime},\textsf{o}^{\prime},\textsf{I}^{\prime \prime},\textsf{I}^{\prime \prime},\textsf{I}^{\prime},\textsf{p}^{\prime},\textsf{v}^{\prime}$ 'd'，'v'，'h'，'>,'<']   
axprops $=$ dict(xticks=[[，yticks=[[）   
ax0=fig.add.axes(rect, label $\coloneqq$ 'ax0'，**axprops) 1 基于图像创建矩阵   
imgP $=$ plt imread('Portland.png')   
ax0.imshow(imgP)   
ax1=fig.add.axes(rect, label $\coloneqq$ 'ax1'，frameon=False)   
for i in range(numClust): ptsInCurrCluster $=$ matplotlib[nonzero(clustAssing(:,0).A==i)[0]，:] $\begin{array}{rl}&\mathrm{markerStyle~=~scatterMarkers[i\%len(scatterMarkers)]}\\&\mathrm{ax1.scatter(ptsInCurrCluster(:,0). flatten().A[0],}\\&\mathrm{ptsInCurrCluster(:,1]. flatten().A[0],}\\&\mathrm{marker=markerStyle,s=90)}\\&\mathrm{ax1.scatter(myCentroids(:,0). flatten().A[0],}\\&\mathrm{myCentroids(:,1]. flatten().A[0],}\end{array}$ marker $=$ markerStyle,s=90)   
ax1.scatter(myCentroids(:,0). flatten().A[0]，  
plt.show()

上述程序清单包含两个函数。第一个函数distSLC()返回地球表面两点之间的距离。第二个函数clusterClubs()将文本文件中的俱乐部进行聚类并画出结果。

函数distSLC()返回地球表面两点间的距离，单位是英里。给定两个点的经纬度，可以使用球面余弦定理来计算两点的距离。这里的纬度和经度用角度作为单位，但是sin()以及cos()以弧度为输入。可以将角度除以180然后再乘以圆周率pi转换为弧度。导入NumPy的时候就会导入pi。

第二个函数clusterClubs()只有一个参数，即所希望得到的簇数目。该函数将文本文件的解析、聚类以及画图都封装在一起，首先创建一个空列表，然后打开places.txt文件获取第4列和第5列，这两列分别对应纬度和经度。基于这些经纬度对的列表创建一个矩阵。接下来在这些数据点上运行biKmeans()并使用distSLC()函数作为聚类中使用的距离计算方法。最后将簇以及簇质心画在图上。

为了画出这些簇，首先创建一幅图和一个矩形，然后使用该矩形来决定绘制图的哪一部分。接下来构建一个标记形状的列表用于绘制散点图。后边会使用唯一的标记来标识每个簇。下一步使用imread()函数基于一幅图像来创建矩阵①，然后使用imshow()绘制该矩阵。接下来，在同一幅图上绘制一张新的图，这允许你使用两套坐标系统并且不做任何缩放或偏移。紧接着，遍历每一个簇并将它们一一画出来。标记类型从前面创建的scatterMarkers列表中得到。使用索引i % len(scatterMarkers)来选择标记形状，这意味着当有更多簇时，可以循环使用这些标记。最后使用十字标记来表示簇中心并在图中显示。

下面看一下实际效果，保存kMeans.py并在Python提示符下输入如下命令：

```txt
>>> reload(kMeans)
<module 'kMeans' from 'kMeans.py>
>>> kMeans.clusterClubs(5)
sseSplit, and notSplit: 3073.83037149 0.0
the bestCentToSplit is: 0
...
sseSplit, and notSplit: 307.687209245 1118.08909015
the bestCentToSplit is: 3
the len of bestClustAss is: 25 
```

执行上面的命令后，会看到与图10-4类似的一个图。

![](images/ecf2fcad892642ba955b2cd3edaff0179e5f4efd5dd01c2a3b962410456a6fcf.jpg)  
图10-4 对俄勒冈州波特兰市夜生活娱乐地点的聚类结果

可以尝试输入不同簇数目得到程序运行的效果。什么数目比较好呢？读者可以思考一下这个问题。

# 10.5 本章小结

聚类是一种无监督的学习方法。所谓无监督学习是指事先并不知道要寻找的内容，即没有目标变量。聚类将数据点归到多个簇中，其中相似数据点处于同一簇，而不相似数据点处于不同簇中。聚类中可以使用多种不同的方法来计算相似度。

一种广泛使用的聚类算法是K-均值算法，其中 $k$ 是用户指定的要创建的簇的数目。K-均值聚类算法以 $k$ 个随机质心开始。算法会计算每个点到质心的距离。每个点会被分配到距其最近的簇

质心，然后紧接着基于新分配到簇的点更新簇质心。以上过程重复数次，直到簇质心不再改变。这个简单的算法非常有效但是也容易受到初始簇质心的影响。为了获得更好的聚类效果，可以使用另一种称为二分K-均值的聚类算法。二分K-均值算法首先将所有点作为一个簇，然后使用K-均值算法（ $k = 2$ ）对其划分。下一次迭代时，选择有最大误差的簇进行划分。该过程重复直到k个簇创建成功为止。二分K-均值的聚类效果要好于K-均值算法。

K-均值算法以及变形的K-均值算法并非仅有的聚类算法，另外称为层次聚类的方法也被广泛使用。下一章将介绍在数据集中查找关联规则的Apriori算法。

# 第11章

# 使用Apriori算法进行关联分析

# 本章内容

□ Apriori算法  
□频繁项集生成  
□ 关联规则生成  
□ 投票中的关联规则发现

在去杂货店买东西的过程，实际包含了许多机器学习的当前及未来应用，这包括物品的展示方式、购物之后优惠券的提供以及用户忠诚度计划，等等。它们都离不开对大量数据的分析。商店希望从顾客身上获得尽可能多的利润，所以他们必然会利用各种技术来达到这一目的。

忠诚度计划是指顾客使用会员卡可以获得一定的折扣，利用这种计划，商店可以了解顾客所购买的商品。即使顾客不使用会员卡，商店也会查看顾客购买商品所使用的信用卡记录。如果顾客不使用会员卡而使用现金付账，商店则可以查看顾客一起购买的商品（如果想知道商店所使用的更多技术，请参考Stephen Baker写的The Numerati一书）。

通过查看哪些商品经常在一起购买，可以帮助商店了解用户的购买行为。这种从数据海洋中抽取的知识可以用于商品定价、市场促销、存货管理等环节。从大规模数据集中寻找物品间的隐含关系被称作关联分析（association analysis）或者关联规则学习（association rule learning）。这里的主要问题在于，寻找物品的不同组合是一项十分耗时的任务，所需的计算代价很高，蛮力搜索方法并不能解决这个问题，所以需要用更智能的方法在合理的时间范围内找到频繁项集。本章将介绍如何使用Aprior算法来解决上述问题。

下面首先详细讨论关联分析，然后讨论Apriori原理，Apriori算法正是基于该原理得到的。接下来创建函数频繁项集高效发现的函数，然后从频繁项集中抽取出关联规则。本章最后给出两个例子，一个是从国会投票记录中抽取出关联规则，另一个是发现毒蘑菇的共同特征。

# 11.1 关联分析

# Apriori算法

优点：易编码实现。

缺点：在大数据集上可能较慢。

适用数据类型：数值型或者标称型数据。

关联分析是一种在大规模数据集中寻找有趣关系的任务。这些关系可以有两种形式：频繁项集或者关联规则。频繁项集（frequent item sets）是经常出现在一块的物品的集合，关联规则（association rules）暗示两种物品之间可能存在很强的关系。下面会用一个例子来说明这两种概念。图11-1给出了某个杂货店的交易清单。

![](images/3f0b84dedf71f3ec4a0838a264dd8f0e3433febc3c60c93ee0d556e9fa024766.jpg)  
图11-1 一个来自HoleFoods天然食品店的简单交易清单

频繁项集是指那些经常出现在一起的物品集合，图11-1中的集合{葡萄酒，尿布，豆奶}就是频繁项集的一个例子（回想一下，集合是由一对大括号“{}”来表示的）。从下面的数据集中也可以找到诸如尿布 $\rightarrow$ 葡萄酒的关联规则。这意味着如果有人买了尿布，那么他很可能也会买葡萄酒。使用频繁项集和关联规则，商家可以更好地理解他们的顾客。尽管大部分关联规则分析的实例来自零售业，但该技术同样可以用于其他行业，比如网站流量分析以及医药行业。

# 尿布与啤酒？

关联分析中最有名的例子是“尿布与啤酒”。据报道，美国中西部的一家连锁店发现，男人们会在周四购买尿布和啤酒。这样商店实际上可以将尿布与啤酒放在一块，并确保在周四全价销售从而获利。当然，这家商店并没有这么做*。

应该如何定义这些有趣的关系？谁来定义什么是有趣？当寻找频繁项集时，频繁（frequent）

的定义是什么？有许多概念可以解答上述问题，不过其中最重要的是支持度和可信度。

一个项集的支持度（support）被定义为数据集中包含该项集的记录所占的比例。从图11-1中可以得到，{豆奶}的支持度为4/5。而在5条交易记录中有3条包含{豆奶，尿布}，因此{豆奶，尿布}的支持度为3/5。支持度是针对项集来说的，因此可以定义一个最小支持度，而只保留满足最小支持度的项集。

可信度或置信度（confidence）是针对一条诸如{尿布} $\rightarrow$ {葡萄酒}的关联规则来定义的。这条规则的可信度被定义为“支持度({尿布, 葡萄酒})/支持度( {尿布})”。从图11-1中可以看到，由于{尿布, 葡萄酒}的支持度为3/5, 尿布的支持度为4/5, 所以“尿布 $\rightarrow$ 葡萄酒”的可信度为 $3 / 4 = 0.75$ 。这意味着对于包含“尿布”的所有记录，我们的规则对其中 $75\%$ 的记录都适用。

支持度和可信度是用来量化关联分析是否成功的方法。假设想找到支持度大于0.8的所有项集，应该如何去做？一个办法是生成一个物品所有可能组合的清单，然后对每一种组合统计它出现的频繁程度，但当物品成千上万时，上述做法非常非常慢。下一节会详细分析这种情况并讨论Aprior原理，该原理会减少关联规则学习时所需的计算量。

# 11.2 Apriori原理

假设我们在经营一家商品种类并不多的杂货店，我们对那些经常在一起被购买的商品非常感兴趣。我们只有4种商品：商品0，商品1，商品2和商品3。那么所有可能被一起购买的商品组合都有哪些？这些商品组合可能只有一种商品，比如商品0，也可能包括两种、三种或者所有四种商品。我们并不关心某人买了两件商品0以及四件商品2的情况，我们只关心他购买了一种或多种商品。

# Apriori算法的一般过程

(1)收集数据：使用任意方法。  
(2) 准备数据：任何数据类型都可以，因为我们只保存集合。  
(3) 分析数据：使用任意方法。  
(4) 训练算法：使用Apriori算法来找到频繁项集。  
(5) 测试算法：不需要测试过程。  
(6) 使用算法：用于发现频繁项集以及物品之间的关联规则。

图11-2显示了物品之间所有可能的组合。为了让该图更容易懂，图中使用物品的编号0来取代物品0本身。另外，图中从上往下的第一个集合是 $\varnothing$ ，表示空集或不包含任何物品的集合。物品集合之间的连线表明两个或者更多集合可以组合形成一个更大的集合。

前面说过，我们的目标是找到经常在一起购买的物品集合。而在11.1节中，我们使用集合的支持度来度量其出现的频率。一个集合的支持度是指有多少比例的交易记录包含该集合。如何对一个给定的集合，比如{0,3}，来计算其支持度？我们遍历每条记录并检查该记录包含0和3，如果记录确实同时包含这两项，那么就增加总计数值。在扫描完所有数据之后，使用统计得到的总

数除以总的交易记录数，就可以得到支持度。上述过程和结果只是针对单个集合 $\{0,3\}$ 。要获得每种可能集合的支持度就需要多次重复上述过程。我们可以数一下图11-2中的集合数目，会发现即使对于仅有4种物品的集合，也需要遍历数据15次。而随着物品数目的增加遍历次数会急剧增长。对于包含N种物品的数据集共有 $2^{N} - 1$ 种项集组合。事实上，出售10000或更多种物品的商店并不少见。即使只出售100种商品的商店也会有 $1.26 \times 10^{30}$ 种可能的项集组合。对于现代的计算机而言，需要很长的时间才能完成运算。

![](images/b79c10efbd6edac9338fa2fe8d355707dde79da3b05c260fb4968a29ce1f53dc.jpg)  
图11-2 集合 $\{0, 1, 2, 3\}$ 中所有可能的项集组合

为了降低所需的计算时间，研究人员发现一种所谓的Apriori原理。Aprior原理可以帮我们减少可能感兴趣的项集。Apriori原理是说如果某个项集是频繁的，那么它的所有子集也是频繁的。对于图11-2给出的例子，这意味着如果{0,1}是频繁的，那么{0}、{1}也一定是频繁的。这个原理直观上并没有什么帮助，但是如果反过来看就有用了，也就是说如果一个项集是非频繁集，那么它的所有超集也是非频繁的（如图11-3所示）。

# Apriori

apriori在拉丁语中指“来自以前”。当定义问题时，通常会使用先验知识或者假设，这被称作“一个先验”（apriori）。在贝叶斯统计中，使用先验知识作为条件进行推断也很常见。先验知识可能来自领域知识、先前的一些测量结果，等等。

在图11-3中，已知阴影项集 $\{2,3\}$ 是非频繁的。利用这个知识，我们就知道项集 $\{0,2,3\}$ ， $\{1,2,3\}$ 以及 $\{0,1,2,3\}$ 也是非频繁的。这也就是说，一旦计算出了 $\{2,3\}$ 的支持度，知道它是非频繁的之后，就不需要再计算 $\{0,2,3\}$ 、 $\{1,2,3\}$ 和 $\{0,1,2,3\}$ 的支持度，因为我们知道这些集合不会满足我们的要求。使用该原理就可以避免项集数目的指数增长，从而在合理时间内计算出频繁项集。

![](images/968d4ca0edccbfad2964f071cc82085f572c0a59c11d0be253135cbf17735294.jpg)  
图11-3 图中给出了所有可能的项集，其中非频繁项集用灰色表示。由于集合 $\{2,3\}$ 是非频繁的，因此 $\{0,2,3\} 、\{1,2,3\}$ 和 $\{0,1,2,3\}$ 也是非频繁的，它们的支持度根本不需要计算

下一节将介绍基于Apriori原理的Apriori算法，并使用Python来实现，然后将其应用于虚拟商店Hole Foods的数据集上。

# 11.3 使用Apriori算法来发现频繁集

11.1节提到，关联分析的目标包括两项：发现频繁项集和发现关联规则。首先需要找到频繁项集，然后才能获得关联规则。本节将只关注于发现频繁项集。

Apriori算法是发现频繁项集的一种方法。Apriori算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个物品的项集列表。接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度的集合会被去掉。然后，对剩下来的集合进行组合以生成包含两个元素的项集。接下来，再重新扫描交易记录，去掉不满足最小支持度的项集。该过程重复进行直到所有项集都被去掉。

# 11.3.1 生成候选项集

在使用Python来对整个程序编码之前，需要创建一些辅助函数。下面会创建一个用于构建初始集合的函数，也会创建一个通过扫描数据集以寻找交易记录子集的函数。数据集扫描的伪代码大致如下：

对数据集中的每条交易记录tran

对每个候选项集can：

检查一下can是否是tran的子集：

如果是，则增加can的计数值

对每个候选项集：

如果其支持度不低于最小值，则保留该项集

返回所有频繁项集列表

下面看一下实际的运行效果，建立一个apriori.py文件并加入下列代码。

程序清单11-1 Apriori算法中的辅助函数  
def loadDataSet(   ): return $\left\lbrack  {\left\lbrack  {1,3,4}\right\rbrack  ,\left\lbrack  {2,3,5}\right\rbrack  ,\left\lbrack  {1,2,3,5}\right\rbrack  ,\left\lbrack  {2,5}\right\rbrack  }\right\rbrack$ def createC1(dataSet): C1 = [] for transaction in DataSet: for item in transaction: if not [item] in C1: C1.append([item]) C1.sort(   ) return map(frozenset, C1) def scanD(D, Ck, minSupport): ssCnt $= \{ \}$ for tid in D: for can in Ck: if can.issubset(tid): if not ssCnt.has_key (can): ssCnt [can] = 1 else: ssCnt [can] += 1 numItems = float(len(D)) retList = [] supportData $= \{ \}$ for key in ssCnt: support $=$ ssCnt [key] /numItems if support >= minSupport: retList.insert(0,key)技术支持[key] = support return retList, supportData

上述程序包含三个函数。第一个函数loadDataSet()创建了一个用于测试的简单数据集，另外两个函数分别是createC1()和scanD()。

不言自名，函数createC1()将构建集合C1。C1是大小为1的所有候选项集的集合。Apriori算法首先构建集合C1,然后扫描数据集来判断这些只有一个元素的项集是否满足最小支持度的要求。那些满足最低要求的项集构成集合L1。而L1中的元素相互组合构成C2，C2再进一步过滤变为L2。到这里，我想读者应该明白了该算法的主要思路。

因此算法需要一个函数createC1()来构建第一个候选项集的列表C1。由于算法一开始是从输入数据中提取候选项集列表，所以这里需要一个特殊的函数来处理，而后续的项集列表则是按一定的格式存放的。这里使用的格式就是Python中的frozenset类型。frozenset是指被“冰冻”的集合，就是说它们是不可改变的，即用户不能修改它们。这里必须要使用frozenset而不是set类型，因为之后必须要将这些集合作为字典键值使用，使用frozenset可以实现这一点，而set却做不到。

首先创建一个空列表c1，它用来存储所有不重复的项值。接下来遍历数据集中的所有交易记

录。对每一条记录，遍历记录中的每一个项。如果某个物品项没有在C1中出现，则将其添加到C1中。这里并不是简单地添加每个物品项，而是添加只包含该物品项的一个列表①。这样做的目的是为每个物品项构建一个集合。因为在Apriori算法的后续处理中，需要做集合操作。Python不能创建只有一个整数的集合，因此这里实现必须使用列表（有兴趣的读者可以试一下）。这就是我们使用一个由单物品列表组成的大列表的原因。最后，对大列表进行排序并将其中的每个单元素列表映射到frozenset()，最后返回frozenset的列表①。

程序清单11-1中的第二个函数是scanD()，它有三个参数，分别是数据集Ck、包含候选集合的列表以及感兴趣项集的最小支持度minSupport。该函数用于从C1生成L1。另外，该函数会返回一个包含支持度值的字典以备后用。scanD()函数首先创建一个空字典ssCnt，然后遍历数据集中的所有交易记录以及C1中的所有候选集。如果C1中的集合是记录的一部分，那么增加字典中对应的计数值。这里字典的键就是集合。当扫描完数据集中的所有项以及所有候选集时，就需要计算支持度。不满足最小支持度要求的集合不会输出。函数也会先构建一个空列表，该列表包含满足最小支持度要求的集合。下一个循环遍历字典中的每个元素并且计算支持度②。如果支持度满足最小支持度要求，则将字典元素添加到retList中。可以使用语句retList.insert(0,key)在列表的首部插入任意新的集合。当然也不一定非要在首部插入，这只是为了让列表看起来有组织。函数最后返回最频繁项集的支持度supportData，该值会在下一节中使用。

下面看看实际的运行效果。保存apriori.py之后，在Python提示符下输入：

```txt
>> import apriori 
```

然后导入数据集：

```txt
>>> dataSet = apriori.loadDataSet()
>>> dataSet
[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] 
```

之后构建第一个候选项集集合C1：

```txt
>>> C1=apriori.createC1(dataSet)  
>>> C1  
[frozenset([1]), frozenset([2]), frozenset([3]), frozenset([4]), frozenset([5])] 
```

可以看到，C1包含了每个frozenset中的单个物品项。下面构建集合表示的数据集D。

```erlang
>>> D=map(set,dataSet)  
>>> D  
[set([1,3,4]), set([2,3,5]), set([1,2,3,5]), set([2,5])] 
```

有了集合形式的数据，就可以去掉那些不满足最小支持度的项集。对上面这个例子，我们使用0.5作为最小支持度水平：

```txt
>>> L1, suppData0=apriori.scanD(D, C1, 0.5)  
>>> L1  
[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])] 
```

——译者注

上述4个项集构成了L1列表，该列表中的每个单物品项集至少出现在 $50\%$ 以上的记录中。由于物品4并没有达到最小支持度，所以没有包含在L1中。通过去掉这件物品，减少了查找两物品项集的工作量。

# 11.3.2 组织完整的Apriori算法

整个Apriori算法的伪代码如下：

当集合中项的个数大于0时

构建一个 $k$ 个项组成的候选项集的列表

检查数据以确认每个项集都是频繁的

保留频繁项集并构建 $k + 1$ 项组成的候选项集的列表

既然可以过滤集合，那么就能够构建完整的Apriori算法了。打开apriori.py文件加入如下程序清单中的代码。

# 程序清单11-2 Apriori算法

```python
def aprioriGen(Lk, k): #creates Ck  
retList = []  
lenLk = len(Lk)  
for i in range(lenLk):  
    for j in range(i+1, lenLk):  
        L1 = list(Lk[i])[k-2]; L2 = list(Lk[j])[k-2]  
        L1.sort(); L2.sort()  
    if L1 == L2:  
        retList.append(Lk[i] | Lk[j])  
return retList  
def apriori(dataSet, minSupport = 0.5):  
    C1 = createC1(dataSet)  
    D = map(set, dataSet)  
    L1, supportData = scanD(D, C1, minSupport)  
    L = [L1]  
    k = 2  
while (len(L[k-2]) > 0):  
    Ck = aprioriGen(L[k-2], k)  
    Lk, supK = scanD(D, Ck, minSupport)  
   技术支持.update(supK)  
    L.append(Lk)  
    k += 1  
return L, supportData 
```

程序清单11-2包含两个函数aprioriGen()与apriori()。其中主函数是apriori(), 它会调用aprioriGen()来创建候选项集CK。

函数aprioriGen()的输入参数为频繁项集列表Lk与项集元素个数k，输出为Ck。举例来说，该函数以{0}、{1}、{2}作为输入，会生成{0,1}、{0,2}以及{1,2}。要完成这一点，首先创建一个空列表，然后计算Lk中的元素数目。接下来，比较Lk中的每一个元素与其他元素，这可以通过

两个for循环来实现。紧接着，取列表中的两个集合进行比较。如果这两个集合的前面k-2个元素都相等，那么就将这两个集合合成一个大小为k的集合①。这里使用集合的并操作来完成，在Python中对应操作符|。

上面的k-2有点让人疑惑。接下来再进一步讨论细节。当利用{0}、{1}、{2}构建{0,1}、{0,2}、{1,2}时，这实际上是将单个项组合到一块。现在如果想利用{0,1}、{0,2}、{1,2}来创建三元素项集，应该怎么做？如果将每两个集合合并，就会得到{0,1,2}、{0,1,2}、{0,1,2}。也就是说，同样的结果集合会重复3次。接下来需要扫描三元素项集列表来得到非重复结果，我们要做的是确保遍历列表的次数最少。现在，如果比较集合{0,1}、{0,2}、{1,2}的第1个元素并只对第1个元素相同的集合求并操作，又会得到什么结果？{0,1,2}，而且只有一次操作！这样就不需要遍历列表来寻找非重复值。

上面所有的操作都被封装在apriori()函数中。给该函数传递一个数据集以及一个支持度，函数会生成候选项集的列表，这通过首先创建C1然后读入数据集将其转化为D（集合列表）来完成。程序中使用map函数将set()映射到dataSet列表中的每一项。接下来，使用程序清单11-1中的scanD()函数来创建L1，并将L1放入列表L中。L会包含L1、L2、L3...。现在有了L1，后面会继续找L2，L3...，这可以通过while循环来完成，它创建包含更大项集的更大列表，直到下一个大的项集为空。如果这听起来让人有点困惑的话，那么等一下你会看到它的工作流程。首先使用aprioriGen()来创建ck，然后使用scanD()基于ck来创建Lk。ck是一个候选项集列表，然后scanD()会遍历ck，丢掉不满足最小支持度要求的项集②。Lk列表被添加到L，同时增加k的值，重复上述过程。最后，当Lk为空时，程序返回L并退出。

下面看看上述程序的执行效果。保存apriori.py文件后，输入如下命令：

```txt
>>> reload(apriori)
<module 'apriori' from 'apriori.pyc'> 
```

上面的命令创建了6个不重复的两元素集合，下面看一下Apriori算法：

```txt
>>> L, suppData=apriori.apriori(dataSet)  
>>> L  
[[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])],  
[frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])],  
[frozenset([2, 3, 5])], []] 
```

L包含满足最小支持度为0.5的频繁项集列表，下面看一下具体值：

```txt
>> L[0]  
[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]  
>> L[1]  
[frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])]  
>> L[2]  
[frozenset([2, 3, 5])]  
>> L[3]  
[] 
```

每个项集都是在函数apriori()中调用函数aprioriGen()来生成的。下面看一下apriori Gen()函数的工作流程：

```python
>> apriori.aprioriGen(L[0], 2)  
[frozenset([1, 3]), frozenset([1, 2]), frozenset([1, 5]),  
frozenset([2, 3]), frozenset([3, 5]), frozenset([2, 5])] 
```

这里的6个集合是候选项集Ck中的元素。其中4个集合在L[1]中，剩下2个集合被函数scanD()过滤掉。

下面再尝试一下 $70\%$ 的支持度：

```txt
>>> L, suppData = apriori.apriori(dataSet, minSupport=0.7)  
>>> L  
[[frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([2, 5])], []] 
```

变量suppData是一个字典，它包含我们项集的支持度值。现在暂时不考虑这些值，不过下一节会用到这些值。

现在可以知道哪些项出现在 $70\%$ 以上的记录中，还可以基于这些信息得到一些结论。我们可以像许多程序一样利用数据得到一些结论，或者可以生成if-then形式的关联规则来理解数据。下一节会就此展开讨论。

# 11.4 从频繁项集中挖掘关联规则

11.2节曾经提到，可以利用关联分析发现许多有趣的内容。人们最常寻找的两个目标是频繁项集与关联规则。上一节介绍如何使用Apriori算法来发现频繁项集，现在需要解决的问题是如何找出关联规则。

要找到关联规则，我们首先从一个频繁项集开始。我们知道集合中的元素是不重复的，但我们想知道基于这些元素能否获得其他内容。某个元素或者某个元素集合可能会推导出另一个元素。从杂货店的例子可以得到，如果有一个频繁项集{豆奶，莴苣}，那么就可能有一条关联规则“豆奶 $\rightarrow$ 莴苣”。这意味着如果有人购买了豆奶，那么在统计上他会购买莴苣的概率较大。但是，这一条反过来并不总是成立。也就是说，即使“豆奶 $\rightarrow$ 莴苣”统计上显著，那么“莴苣 $\rightarrow$ 豆奶”也不一定成立。（从逻辑研究上来讲，箭头左边的集合称作前件，箭头右边的集合称为后件。）

11.3节给出了频繁项集的量化定义，即它满足最小支持度要求。对于关联规则，我们也有类似的量化方法，这种量化指标称为可信度。一条规则 $\mathbf{P} \rightarrow \mathbf{H}$ 的可信度定义为 $\mathrm{support(P|H)/support(P)}$ 。记住，在Python中，操作符表示集合的并操作，而数学上集合并的符号是U。 $\mathbf{P}|\mathbf{H}$ 是指所有出现在集合P或者集合H中的元素。前面一节已经计算了所有频繁项集支持度。现在想获得可信度，所需要做的只是取出那些支持度值做一次除法运算。

从一个频繁项集中可以产生多少条关联规则？图11-4的网格图给出的是从项集 $\{0,1,2,3\}$ 产生的所有关联规则。为找到感兴趣的规则，我们先生成一个可能的规则列表，然后测试每条规则的可信度。如果可信度不满足最小要求，则去掉该规则。

![](images/3a5a062a6010303e3480449708d3cef8724f5b78fcd482dbff61d248ba09f19d.jpg)  
图11-4 对于频繁项集 $\{0,1,2,3\}$ 的关联规则网格示意图。阴影区域给出的是低可信度的规则。如果发现 $0,1,2 \rightarrow 3$ 是一条低可信度规则，那么所有其他以3作为后件的规则可信度也会较低

类似于上一节的频繁项集生成，我们可以为每个频繁项集产生许多关联规则。如果能够减少规则数目来确保问题的可解性，那么计算起来就会好很多。可以观察到，如果某条规则并不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求。以图11-4为例，假设规则 $0,1,2 \rightarrow 3$ 并不满足最小可信度要求，那么就知道任何左部为 $\{0,1,2\}$ 子集的规则也不会满足最小可信度要求。在图11-4中这些规则上都加了阴影来表示。

可以利用关联规则的上述性质属性来减少需要测试的规则数目。类似于程序清单11-2中的Apriori算法，可以首先从一个频繁项集开始，接着创建一个规则列表，其中规则右部只包含一个元素，然后对这些规则进行测试。接下来合并所有剩余规则来创建一个新的规则列表，其中规则右部包含两个元素。这种方法也被称作分级法。下面看一下这种方法的实际效果，打开apriori.py文件，加入下面的代码。

# 程序清单11-3 关联规则生成函数

```python
def generateRules(L, supportData, minConf=0.7):
    bigRuleList = []
    for i in range(1, len(L)):
        for freqSet in L[i]:
            H1 = [frozenset([item]) for item in freqSet]
            if (i > 1):
                rulesFromConseq(freqSet, H1, supportData, bigRuleList, \minConf)
            else:
                calcConf(freqSet, H1, supportData, bigRuleList, minConf)
            return bigRuleList
def calcConf(freqSet, H, supportData, brl, minConf=0.7):
    prunedH = []
    for conseq in H:
        conf = supportData[freqSet] / supportData[freqSet - conseq] 
```

if conf $\Rightarrow$ minConf: print freqSet-conseq,'-->，conseq,'conf：',conf br1.append((freqSet-conseq，conseq，conf)) prunedH.append(conseq) return prunedH   
def rulesFromConseq(freqSet，H，supportData，brl，minConf=0.7）： m = len(H[0]) if (len(freqSet) > (m + 1)): Hmp1 $=$ aprioriGen(H,m+1) Hmp1 $=$ calcConf(freqSet,Hmpl,supportData，brl,minConf) if (len(Hmpl)>1): rulesFromConseq(freqSet，Hmpl,supportData，brl，minConf) 创建Hm+1条新候选规则3

上述程序中包含三个函数。第一个函数generateRules()是主函数，它调用其他两个函数。其他两个函数是rulesFromConseq()和calcConf()，分别用于生成候选规则集合以及对规则进行评估。

函数generateRules()有3个参数：频繁项集列表、包含那些频繁项集支持数据的字典、最小可信度阈值。函数最后要生成一个包含可信度的规则列表，后面可以基于可信度对它们进行排序。这些规则存放在bigRuleList中。如果事先没有给定最小可信度的阈值，那么默认值设为0.7。generateRules()的另两个输入参数正好是程序清单11-2中函数apriori()的输出结果。该函数遍历L中的每一个频繁项集并对每个频繁项集创建只包含单个元素集合的列表H1。因为无法从单元素项集中构建关联规则，所以要从包含两个或者更多元素的项集开始规则构建过程①。如果从集合{0,1,2}开始，那么H1应该是[{0},{1},{2}]。如果频繁项集的元素数目超过2，那么会考虑对它做进一步的合并。具体合并可以通过函数rulesFromConseq()来完成，后面会详细讨论合并过程。如果项集中只有两个元素，那么使用函数calcConf()来计算可信度值。

我们的目标是计算规则的可信度以及找到满足最小可信度要求的规则。所有这些可以使用函数calcConf()来完成，而程序清单11-3中的其余代码都用来准备规则。函数会返回一个满足最小可信度要求的规则列表，为了保存这些规则，需要创建一个空列表prunedH。接下来，遍历H中的所有项集并计算它们的可信度值。可信度计算时使用supportData中的支持度数据。通过导入这些支持度数据，可以节省大量计算时间。如果某条规则满足最小可信度值，那么将这些规则输出到屏幕显示。通过检查的规则也会被返回，并被用在下一个函数rulesFromConseq()中。同时也需要对列表br1进行填充，而br1是前面通过检查的bigRuleList。

为从最初的项集中生成更多的关联规则，可以使用rulesFromConseq()函数。该函数有2个参数：一个是频繁项集，另一个是可以出现在规则右部的元素列表H。函数先计算H中的频繁集大小m②。接下来查看该频繁项集是否大到可以移除大小为m的子集。如果可以的话，则将其移除。可以使用程序清单11-2中的函数aprioriGen()来生成H中元素的无重复组合③。该结果会存储在Hmp1中，这也是下一次迭代的H列表。Hmp1包含所有可能的规则。可以利用calcConf()来测试它们的可信度以确定规则是否满足要求。如果不正一条规则满足要求，那么使用Hmp1送

代调用函数rulesFromConseq()来判断是否可以进一步组合这些规则。

下面看一下实际的运行效果，保存apriori.py文件，在Python提示符下输入：

```python
>>> reload(apriori)
<module 'apriori' from 'apriori.py>
Now let's generate a set of frequent itemsets with a support of 0.5:
>>> L, suppData = apriori.apriori(dataSet, minSupport=0.5)
>>> rules = apriori_generateRules(L, suppData, minConf=0.7)
>>> rules
[(frozenset([1]), frozenset([3]), 1.0), (frozenset([5]), frozenset([2]), 1.0), (frozenset([2]), frozenset([5]), 1.0)]
frozenset([1]) -> frozenset([3]) conf: 1.0
frozenset([5]) -> frozenset([2]) conf: 1.0
frozenset([2]) -> frozenset([5]) conf: 1.0 
```

结果中给出三条规则： $\{1\} \to \{3\}$ 、 $\{5\} \to \{2\}$ 及 $\{2\} \to \{5\}$ 。可以看到，后两条包含2和5的规则可以互换前件和后件，但是前一条包含1和3的规则不行。下面降低可信度阈值之后看一下结果：

```txt
>>> rules=apriori.createRules(L, suppData, minConf=0.5)  
>>> rules  
[(frozenset([3]), frozenset([1]), 0.6666666666666666), (frozenset([1]),  
frozenset([3]), 1.0), (frozenset([5]), frozenset([2]), 1.0),  
(frozenset([2]), frozenset([5]), 1.0), (frozenset([3]), frozenset([2]),  
0.6666666666666666), (frozenset([2]), frozenset([3]), 0.666666666666666),  
(frozenset([5]), frozenset([3]), 0.666666666666666), (frozenset([3]),  
frozenset([5]), 0.666666666666666), (frozenset([5]), frozenset([2, 3]),  
0.666666666666666), (frozenset([3]), frozenset([2], 5]),  
0.66666666666666)  
]  
frozenset([3]) --> frozenset([1]) conf: 0.666666666667  
frozenset([1]) --> frozenset([3]) conf: 1.0  
frozenset([5]) --> frozenset([2]) conf: 1.0  
frozenset([2]) --> frozenset([5]) conf: 1.0  
frozenset([3]) --> frozenset([2]) conf: 0.66666666667  
frozenset([2]) --> frozenset([3]) conf: 0.6666666667  
frozenset([5]) --> frozenset([3]) conf: 0.6666666667  
frozenset([3]) --> frozenset([5]) conf: 0.6666666 
```

一旦降低可信度阈值，就可以获得更多的规则。到现在为止，我们看到上述程序能够在一个小数据集上正常运行，接下来将在一个更大的真实数据集上测试一下效果。具体地，下一节将检查其在美国国会投票记录上的处理效果。

# 11.5 示例：发现国会投票中的模式

前面我们已经发现频繁项集及关联规则，现在是时候把这些工具用在真实数据上了。那么可以使用什么样的数据呢？购物是一个很好的例子，但是前面已经用过了。另一个例子是搜索引擎中的查询词。这个示例听上去不错，不过下面看到的是一个更有趣的美国国会议员投票的例子。

加州大学埃文分校的机器学习数据集合中有一个自1984年起的国会投票记录的数据集：http://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records。这个数据集有点偏旧，而且其中的议题对我来讲意义也不大。我们想尝试一些更新的数据。目前有不少组织致力于将政府数据公开化，其中的一个组织是智能投票工程（Project Vote Smart，网址：http://www.votesmart.org），它提供了一个公共的API。下面会看到如何从Votesmart.org获取数据，并将其转化为用于生成频繁项集与关联规则的格式。该数据可以用于竞选的目的或者预测政治家如何投票。

# 示例：在美国国会投票记录中发现关联规则

(1)收集数据：使用votesmart模块来访问投票记录。  
(2) 准备数据：构造一个函数来将投票转化为一串交易记录。  
(3) 分析数据：在Python提示符下查看准备的数据以确保其正确性。  
(4) 训练算法：使用本章早先的apriori()和generateRules()函数来发现投票记录中的有趣信息。  
(5) 测试算法：不适用，即没有测试过程。  
(6) 使用算法：这里只是出于娱乐的目的，不过也可以使用分析结果来为政治竞选活动服务，或者预测选举官员会如何投票。

接下来，我们将处理投票记录并创建一个交易数据库。这需要一些创造性思维。最后，我们会使用本章早先的代码来生成频繁项集和关联规则的列表。

# 11.5.1 收集数据：构建美国国会投票记录的事务数据集

智能投票工程已经收集了大量的政府数据，他们同时提供了一个公开的API来访问该数据http://api.votesmart.org/docs/terms.html。Sunlight实验室写过一个Python模块用于访问该数据，该模块在https://github.com/sunlightlabs/python-votesmart中有很多可供参考的文档。下面要从美国国会获得一些最新的投票记录并基于这些数据来尝试学习一些关联规则。

我们希望最终数据的格式与图11-1中的数据相同，即每一行代表美国国会的一个成员，而每列都是他们投票的对象。接下来从国会议员最近投票的内容开始。如果没有安装python-votesmart，或者没有获得API key，那么需要先完成这两件事。关于如何安装python-votesmart可以参考附录A。

要使用votesmartAPI，需要导入votesmart模块：

>>> from votesmart import votesmart

接下来，输入你的API key:

>>>votesmart.apikey $\equiv$ 49024thereoncewasamanfromnantucket94040

现在就可以使用votesmartAPI了。为了获得最近的100条议案，输入：

```txt
>>> bills = votesmart.votes.getBillsByStateRecent() 
```

为了看看每条议案的具体内容，输入：

```txt
>>> for bill in bills:  
... print bill.title,bill.billId  
...  
Amending FAA Rulemaking Activities 13020  
Prohibiting Federal Funding of National Public Radio 12939  
Additional Continuing Appropriations 12888  
Removing Troops from Afghanistan 12940  
...  
...  
"Whistleblower Protection" for Offshore Oil Workers 11820 
```

读者在看本书时，最新的100条议案内容将会有所改变。所以这里我将上述100条议案的标题及ID号（billId）保存为recent100bills.txt文件。

可以通过getBill()方法，获得每条议案的更多内容。比如，对刚才的最后一条议案“Whistleblower Protection”，其ID号为11820。下面看看实际结果：

>>>bill $\equiv$ votesmart.votes.getBill(11820)

上述命令会返回一个BillDetail对象，其中包含大量完整信息。我们可以查看所有信息，不过这里我们所感兴趣的只是围绕议案的所有行为。可以通过输入下列命令来查看实际结果：

```html
>>>bill(actions 
```

上述命令会返回许多行为，议案包括议案被提出时的行为以及议案在投票时的行为。我们对投票发生时的行为感兴趣，可以输入下面命令来获得这些信息：

```txt
>>> for action in bill/actions:  
... if action stage == 'Passage':  
... print action.actionId  
31670 
```

上述信息并不完整，一条议案会经历多个阶段。一项议案被提出之后，经由美国国会和众议院投票通过后，才能进入行政办公室。其中的Passage（议案通过）阶段可能存在欺骗性，因为这有可能是行政办公室的Passage阶段，那里并没有任何投票。

为获得某条特定议案的投票信息，使用getBillActionVotes()方法：

>>>VoteList $\equiv$ votesmart.votes.getBillActionVotes(31670)

其中，voteList是一个包含Vote对象的列表。输入下面的命令来看一下里面包含的内容：

```python
>>> voteList[22]
Vote({u'action': u'No Vote', u'candidateId': u'430', u'officeParties': u'Democratic', u'candidateName': u'Berry, Robert'})  
>>> voteList[21]  
Vote({u'action': u'Yea', u'candidateId': u'26756', u'officeParties': u'Democratic', u'candidateName': u'Berman, Howard'}) 
```

现在为止，我们已经用过这些相关API，可以将它们组织到一块了。接下来会给出一个函数将文本文件中的bill1d转化为actionId。如前所述，并非所有的议案都被投票过，另外可能有一些议案在多处进行了议案投票。也就是说需要对actionId进行过滤只保留包含投票数据的actionId。这样处理之后将100个议案过滤到只剩20个议案，这些剩下的议案都是我认为有趣的议案，它们被保存在文件recent20bills.txt中。下面给出一个getActionIds()函数来处理actionIds的过滤。打开apriori.py文件，输入下面的代码①。

程序清单11-4 收集美国国会议案中actionID的函数  
```python
from time import sleep  
from votesmart import votesmart  
votesmart.aikey = '49024thereoncewasamanfromnantucket94040'  
def getActionIds():  
    actionIdList = []; billTitleList = [];  
    fr = open('recent20bills.txt')  
    for line in fr.readlines():  
        billNum = int(line.split("\\t))[0])  
    try:  
        billDetail = votesmart.votes.getBill billNum)  
        for action in billDetail actions:  
            if action.level == 'House' and (action階段 == 'Passage' or action階段 == 'Amendment Vote'):  
                actionId = int(action.actionId)  
                print 'bill:%d has actionId:%d' % (billNum, actionId)  
                actionIdList.append(actionId)  
                billTitleList.append(line.strip().split("\\t))[1])  
except:  
    print "problem getting bill %d" % billNum 2 为礼貌访问网站而做些延迟  
sleep(1) 
```

上述程序中导入了votesmart模块并通过引入sleep函数来延迟API调用。getActionsIds()函数会返回存储在recent20bills.txt文件中议案的actionId。程序先导入API key，然后创建两个空列表。这两个列表分别用来返回actionsId和标题。首先打开recent20bills.txt文件，对每一行内不同元素使用tab进行分隔，之后进入try-except模块。由于在使用外部API时可能会遇到错误，并且也不想让错误占用数据获取的时间，上述try-except模块调用是一种非常可行的做法。所以，首先尝试使用getBill()方法来获得一个billDetail对象。接下来遍历议案中的所有行为，来寻找有投票数据的行为①。在Passage阶段与Amendment Vote（修正案投票）阶段都会有投票数据，要找的就是它们。现在，在行政级别上也有一个Passage阶段，但那个阶段并不包含任何投票数据，所以要确保这个阶段是发生在众议院②。如果确实如此，程序就会将actionId打印出来并将它添加到actionIdList中。同时，也会将议案的标题添加到billTitleList中。如果在API调用时发生错误，就不会执行actionIdList的添加操作。一旦有错误就会执行except模块

并将错误信息输出。最后，程序会休眠1秒钟，以避免对Votesmart.org网站的过度频繁访问。程序运行结束时，actionIdList与billTitleList会被返回用于进一步的处理。

下面看一下实际运行效果。将程序清单11-4中的代码加入到apriori.py文件后，输入如下命令：

```txt
>>> reload(apriori)
<module 'apriori' from 'apriori.py>
>>> actionIdList, billTitles = apriori.getActionIds()
bill: 12939 has actionId: 34089
bill: 12940 has actionId: 34091
bill: 12988 has actionId: 34229 
```

可以看到actionId显示了出来，它同时也被添加到actionIdList中输出，以后我们可以使用这些actionId了。如果程序运行错误，则尝试使用try..except代码来捕获错误。我自己就曾经在获取所有actionId时遇到一个错误。接下里可以继续来获取这些actionId的投票信息。

选举人可以投是或否的表决票，也可以弃权。需要一种方法来将这些上述信息转化为类似于项集或者交易数据库之类的东西。前面提到过，一条交易记录数据只包含一个项的出现或不出现信息，并不包含项出现的次数。基于上述投票数据，可以将投票是或否看成一个元素。

美国有两个主要政党：共和党与民主党。下面也会对这些信息进行编码并写到事务数据库中。幸运的是，这些信息在投票数据中已经包括。下面给出构建事务数据库的流程：首先创建一个字典，字典中使用政客的名字作为键值。当某政客首次出现时，将他及其所属政党（民主党或者共和党）添加到字典中，这里使用0来代表民主党，1来代表共和党。下面介绍如何对投票进行编码。对每条议案创建两个条目： $bill + 'Yea'$ 以及 $bill + 'Nay'$ 。该方法允许在某个政客根本没有投票时也能合理编码。图11-5给出了从投票信息到元素项的转换结果。

![](images/17e533ea329e5de27ffae36b8d6fa57bfcf65a12b5cb179a7cabc8681966fe58.jpg)  
图11-5 美国国会信息到元素（项）编号之间的映射示意图

现在，我们已经有一个可以将投票编码为元素项的系统，接下来是时候生成事务数据库了。一旦有了事务数据库，就可以应用早先写的Apriori代码。下面将构建一个使用actionId串作为输入并利用votesmart的API来抓取投票记录的函数。然后将每个选举人的投票转化为一个项集。每个选举人对应于一行或者说事务数据库中的一条记录。下面看一下实际的效果，打开

apriori.py文件并添加下面清单中的代码。

# 程序清单11-5 基于投票数据的事务列表填充函数

```python
def getTransList(actionIdList, billTitleList):
    itemMeaning = ['Republican', 'Democratic']
    for billTitle in billTitleList:
        itemMeaning.append('%s -- Nay' % billTitle)
        itemMeaning.append('%s -- Yea' % billTitle)
    transDict = {}
    voteCount = 2
    for actionId in actionIdList:
        sleep(3)
        print 'getting votes for actionId: %d' % actionId
        try:
            voteList = votesmart.votes.getBillActionVotes(actionId)
        for vote in voteList:
            if not transDict.has_key(vote.candidateName):
                transDict[vote.candidateName] = []
            if vote.officeParties == 'Democratic':
                transDict[vote.candidateName].append(1)
            elif vote.officeParties == 'Republican':
                transDict[vote.candidateName].append(0)
            if vote.action == 'Nay':
                transDict[vote.candidateName].append(voteCount)
            elif vote.action == 'Yea':
                transDict[vote.candidateName].append(voteCount + 1)
        except:
            print "problem getting actionId: %d" % actionId
            voteCount += 2
    return transDict, itemMeaning 
```

函数getTransList()会创建一个事务数据库，于是在此基础上可以使用前面的Apriori代码来生成频繁项集与关联规则。该函数也会创建一个标题列表，所以很容易了解每个元素项的含义。一开始使用前两个元素“Repubulican”和“Democratic”创建一个含义列表itemMeaning。当想知道某些元素项的具体含义时，需要做的是以元素项的编号作为索引访问itemMeaning即可。接下来遍历所有议案，然后在议案标题后添加Nay（反对）或者Yea（同意）并将它们放入itemMeaning列表中①。接下来创建一个空字典用于加入元素项，然后遍历函数getActionIds()返回的每一个actionId。遍历时要做的第一件事是休眠，即在for循环中一开始调用sleep()函数来延迟访问，这样做可以避免过于频繁的API调用。接着将运行结果打印出来，以便知道程序是否在正常工作。再接着通过try..except块来使用VotesmartAPI获取某个特定actionId相关的所有投票信息。然后，遍历所有的投票信息（通常voteList会超过400个投票）。在遍历时，使用政客的名字作为字典的键值来填充transDict。如果之前没有遇到该政客，那么就要获取他的政党信息。字典中的每个政客都有一个列表来存储他投票的元素项或者他的政党信息。接下来会看到该政客是否对当前议案投了赞成（Yea）或反对（Nay）票。如果他们之前有投票，那么不管是投赞成票还是反对票，这些信息都将添加到列表中。如果API调用中发生了什么错误，except模块中的程序就会被调用并将错误信息输出到屏幕上，之后函数仍然继续执行。最后，

程序返回事务字典transDict及元素项含义类表itemMeaning。

下面看一下投票信息的前两项，了解上述代码是否正常工作：

```html
>>> reload(apriori)
<module 'apriori' from 'apriori.py>
>>> transDict, itemMeaning = apriori.getTransList(actionIdList[:2], billTitles[:2])
getting votes for actionId: 34089
getting votes for actionId: 34091 
```

下面看一下transDict中包含的具体内容：

```python
>>> for key in transDict.keys():
... print transDict[key]
[1, 2, 5]
[1, 2, 4]
[0, 3, 4]
[0, 3, 4]
[1, 2, 4]
[0, 3, 4]
[1]
[1, 2, 5]
[1, 2, 4]
[1]
[1, 2, 4]
[0, 3, 4]
[1, 2, 5]
[1, 2, 4]
[0, 3, 4] 
```

如果上面许多列表看上去都类似的话，读者也不要太过担心。许多政客的投票结果都很类似。现在如果给定一个元素项列表，那么可以使用itemMeaning列表来快速“解码”出它的含义：

```txt
>>> transDict.keys()
u' Doyle, Michael 'Mike}'
>>> for item in transDict[' Doyle, Michael 'Mike':
...
...
Republican
Prohibiting Federal Funding of National Public Radio -- Year
Removing Troops from Afghanistan - Nay 
```

上述输出可能因Votesmart服务器返回的结果不同而有所差异。

下面看看完整列表下的结果：

```txt
>>> transDict, itemMeaning = apriori.getTransList(actionIdList, billTitles)
getting votes for actionId: 34089
getting votes for actionId: 34091
getting votes for actionId: 34229 
```

接下来在使用前面开发的Apriori算法之前，需要构建一个包含所有事务项的列表。可以使用类似于前面for循环的一个列表处理过程来完成：

```txt
>>> dataSet = [transDict[key] for key in transDict.keys()) 
```

上面这样的做法会去掉键值（即政客）的名字。不过这无关紧要，这些信息不是我们感兴趣的内容。我们感兴趣的是元素项以及它们之间的关联关系。接下来将使用Apriori算法来挖掘上面例子中的频繁项集与关联规则。

# 11.5.2 测试算法：基于美国国会投票记录挖掘关联规则

现在可以应用11.3节的Apriori算法来进行处理。如果使用默认的支持度阈值 $50\%$ ，那么应该不会产生太多的频繁项集：

```erlang
>>> L, suppData = apriori.apriori(dataSet, minSupport=0.5)  
>>> L  
[[frozenset([4]), frozenset([13]), frozenset([0]), frozenset([21]), [frozenset([13, 21])], []]] 
```

使用一个更小的支持度阈值 $30\%$ 会得到更多频繁项集：

```txt
>>> L, suppData = apriori.apriori(dataSet, minSupport=0.3)  
>>> len(L)  
8 
```

当使用 $30\%$ 的支持度阈值时，会得到许多频繁项集，甚至可以得到包含所有7个元素项的6个频繁集。

```txt
>>> L[6]  
[frozenset([0, 3, 7, 9, 23, 25, 26]), frozenset([0, 3, 4, 9, 23, 25, 26]), frozenset([0, 3, 4, 7, 9, 23, 26]), frozenset([0, 3, 4, 7, 9, 23, 25]), frozenset([0, 4, 7, 9, 23, 25, 26]), frozenset([0, 3, 4, 7, 9, 25, 26])] 
```

获得频繁项集之后就可以结束，也可以尝试使用11.4节的代码来生成关联规则。首先将最小可信度值设为0.7：

```erlang
>>> rules = apriori_generateRules(L, suppData) 
```

这样会产生太多规则，于是可以加大最小可信度值。

```erlang
>>> rules = apriori_generateRules(L, suppData, minConf=0.95)  
frozenset([15]) --> frozenset([1]) conf: 0.961538461538  
frozenset([22]) --> frozenset([1]) conf: 0.951351351351 
```

```python
frozenset([25, 26, 3, 4]) ---> frozenset([0, 9, 7]) conf: 0.97191011236  
frozenset([0, 25, 26, 4]) ---> frozenset([9, 3, 7]) conf: 0.950549450549 
```

# 继续增加可信度值：

```erlang
>>> rules = apriori_generateRules(L, suppData, minConf=0.99)  
frozenset([3]) -> frozenset([9]) conf: 1.0  
frozenset([3]) -> frozenset([0]) conf: 0.995614035088  
frozenset([3]) -> frozenset([0, 9]) conf: 0.995614035088  
frozenset([26, 3]) -> frozenset([0, 9]) conf: 1.0  
frozenset([9, 26]) -> frozenset([0, 7]) conf: 0.957547169811 
```

```txt
frozenset([23, 26, 3, 4, 7]) --> frozenset([0, 9]) conf: 1.0  
frozenset([23, 25, 3, 4, 7]) --> frozenset([0, 9]) conf: 0.994764397906  
frozenset([25, 26, 3, 4, 7]) --> frozenset([0, 9]) conf: 1.0 
```

上面给出了一些有趣的规则。如果要找出每一条规则的含义，则可以将规则号作为索引输入到itemMeaning中：

```txt
>>> itemMeaning[26]  
'Prohibiting the Use of Federal Funds for NASCAR Sponsorships -- Nay'  
>>> itemMeaning[3]  
'Prohibiting Federal Funding of National Public Radio -- Yea'  
>>> itemMeaning[9]  
'Repealing the Health Care Bill -- Yea' 
```

在图11-6中列出了下面的几条规则： $\{3\} \to \{0\}$ 、 $\{22\} \to \{1\}$ 及 $\{9,26\} \to \{0,7\}$ 。

![](images/c259d9f94ca49bd1c2ba1d7db0a4a22cb6f448ce72416006727c3326cd505824.jpg)  
图11-6 关联规则 $\{3\} \to \{0\}$ 、 $\{22\} \to \{1\}$ 与 $\{9,26\} \to \{0,7\}$ 的含义及可信度

数据中还有更多有趣或娱乐性十足的规则。还记得前面最早使用的支持度 $30\%$ 吗？这意味着这些规则至少出现在 $30\%$ 以上的记录中。由于至少会在 $30\%$ 的投票记录中看到这些规则，所以这是很有意义。对于 $\{3\} \rightarrow \{0\}$ 这条规则，在 $99.6\%$ 的情况下是成立的。我真希望在这类事情上赌一把。

# 11.6 示例：发现毒蘑菇的相似特征

有时我们并不想寻找所有频繁项集，而只对包含某个特定元素项的项集感兴趣。在本章这个最后的例子中，我们会寻找毒蘑菇中的一些公共特征，利用这些特征就能避免吃到那些有毒的蘑菇。UCI的机器学习数据集合中有一个关于肋形蘑菇的23种特征的数据集，每一个特征都包含一个标称数据值。我们必须将这些标称值转化为一个集合，这一点与前面投票例子中的做法类似。幸运的是，已经有人已经做好了这种转换①。Roberto Bayardo对UCI蘑菇数据集进行了解析，将每个蘑菇样本转换成一个特征集合。其中，枚举了每个特征的所有可能值，如果某个样本包含特征，那么该特征对应的整数值被包含数据集中。下面我们近距离看看该数据集。它在源数据集合中是一个名为mushroom.dat的文件。下面将它和原始数据集http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data进行比较。

文件mushroom.dat的前几行如下：

```solidity
1 3 9 13 23 25 34 36 38 40 52 54 59 63 67 76 85 86 90 93 98 107 113  
2 3 9 14 23 26 34 36 39 40 52 55 59 63 67 76 85 86 90 93 99 108 114  
2 4 9 15 23 27 34 36 39 41 52 55 59 63 67 76 85 86 90 93 99 108 115 
```

第一个特征表示有毒或者可食用。如果某样本有毒，则值为2。如果可食用，则值为1。下一个特征是蘑菇伞的形状，有六种可能的值，分别用整数3-8来表示。

为了找到毒蘑菇中存在的公共特征，可以运行Apriori算法来寻找包含特征值为2的频繁项集。

```python
>>> mushDataSet = [line.split() for line in open('mushroom.dat').readlines()) 
```

在该数据集上运行Apriori算法：

```txt
>>> L, suppData = apriori.apriori(mushDataSet, minSupport=0.3) 
```

在结果中可以搜索包含有毒特征值2的频繁项集：

```python
>>> for item in L[1]:  
... if item.intersection('2'): print item  
...  
frozenset(['2', '59'])  
frozenset(['39', '2'])  
frozenset(['2', '67'])  
frozenset(['2', '34'])  
frozenset(['2', '23']) 
```

也可以对更大的项集来重复上述过程：

```python
>>> for item in L[3]:  
... if item.intersection('2'): print item  
...  
frozenset(['63', '59', '2', '93'])  
frozenset(['39', '2', '53', '34'])  
frozenset(['2', '59', '23', '85'])  
frozenset(['2', '59', '90', '85'])  
frozenset(['39', '2', '36', '34'])  
frozenset(['39', '63', '2', '85'])  
frozenset(['39', '2', '90', '85'])  
frozenset(['2', '59', '90', '86']) 
```

接下来你需要观察一下这些特征，以便知道了解野蘑菇的那些方面。如果看到其中任何一个特征，那么这些蘑菇就不要吃了。当然，最后还要声明一下：尽管上述这些特征在毒蘑菇中很普遍，但是没有这些特征并不意味该蘑菇就是可食用的。如果吃错了蘑菇，你可能会因此而丧命。

# 11.7 本章小结

关联分析是用于发现大数据集中元素间有趣关系的一个工具集，可以采用两种方式来量化这些有趣的关系。第一种方式是使用频繁项集，它会给出经常在一起出现的元素项。第二种方式是关联规则，每条关联规则意味着元素项之间的“如果……那么”关系。

发现元素项间不同的组合是个十分耗时的任务，不可避免需要大量昂贵的计算资源，这就需要一些更智能的方法在合理的时间范围内找到频繁项集。能够实现这一目标的一个方法是Apriori

算法，它使用Apriori原理来减少在数据库上进行检查的集合的数目。Apriori原理是说如果一个元素项是不频繁的，那么那些包含该元素的超集也是不频繁的。Apriori算法从单元素项集开始，通过组合满足最小支持度要求的项集来形成更大的集合。支持度用来度量一个集合在原始数据中出现的频率。

关联分析可以用在许多不同物品上。商店中的商品以及网站的访问页面是其中比较常见的例子。关联分析也曾用于查看选举人及法官的投票历史。

每次增加频繁项集的大小，Apriori算法都会重新扫描整个数据集。当数据集很大时，这会显著降低频繁项集发现的速度。下一章会介绍FPGrowth算法①，和Apriori算法相比，该算法只需要对数据库进行两次遍历，能够显著加快发现繁琐集的速度。

# 第12章

# 使用FP-growth算法来高效发现频繁项集

# 本章内容

□发现事务数据中的公共模式  
FP-growth算法  
□ 发现Twitter源中的共现词

你用过搜索引擎吗？输入一个单词或者单词的一部分，搜索引擎就会自动补全查询词项。用户甚至事先都不知道搜索引擎推荐的东西是否存在，反而会去查找推荐词项。我也有过这样的经历，当我输入以“为什么”开始的查询时，有时会出现一些十分滑稽的推荐结果。为了给出这些推荐查询词项，搜索引擎公司的研究人员使用了本章将要介绍的一个算法。他们通过查看互联网上的用词来找出经常在一块出现的词对①。这需要一种高效发现频繁集的方法。

本章会在上一章讨论话题的基础上进行扩展，将给出了一个非常好的频繁项集发现算法。该算法称作FP-growth，它比上一章讨论的Apriori算法要快。它基于Apriori构建，但在完成相同任务时采用了一些不同的技术。这里的任务是将数据集存储在一个特定的称作FP树的结构之后发现频繁项集或者频繁项对，即常在一块出现的元素项的集合FP树。这种做法使得算法的执行速度要快于Apriori，通常性能要好两个数量级以上。

上一章我们讨论了从数据集中获取有趣信息的方法，最常用的两种分别是频繁项集与关联规则。第11章中介绍了发现频繁项集与关键规则的算法，本章将继续关注发现频繁项集这一任务。我们会深入探索该任务的解决方法，并应用FP-growth算法进行处理，该算法能够更有效地挖掘数据。这种算法虽然能更为高效地发现频繁项集，但不能用于发现关联规则。

FP-growth算法只需要对数据库进行两次扫描，而Apriori算法对于每个潜在的频繁项集都会扫描数据集判定给定模式是否频繁，因此FP-growth算法的速度要比Apriori算法快。在小规模数据集上，这不是什么问题，但当处理更大数据集时，就会产生较大问题。FP-growth只会扫描数

据集两次，它发现频繁项集的基本过程如下：

(1) 构建FP树   
(2) 从FP树中挖掘频繁项集

下面先讨论FP树的数据结构，然后看一下如何用该结构对数据集编码。最后，我们会介绍两个例子：一个是从Twitter文本流中挖掘常用词，另一个从网民网页浏览行为中挖掘常见模式。

# 12.1 FP树：用于编码数据集的有效方式

# FP-growth算法

优点：一般要快于Apriori。

缺点：实现比较困难，在某些数据集上性能会下降。

适用数据类型：标称型数据。

FP-growth算法将数据存储在一种称为FP树的紧凑数据结构中。FP代表频繁模式（Frequent Pattern）。一棵FP树看上去与计算机科学中的其他树结构类似，但是它通过链接（link）来连接相似元素，被连起来的元素项可以看成一个链表。图12-1给出了FP树的一个例子。

![](images/15935d6cb7607df5dd1f85a529bc137d5111a72a98facb2f72317d25d86afcfe.jpg)  
图12-1一棵FP树，看上去和一般的树没什么两样，包含着连接相似节点的链接

同搜索树不同的是，一个元素项可以在一棵FP树中出现多次。FP树会存储项集的出现频率，而每个项集会以路径的方式存储在树中。存在相似元素的集合会共享树的一部分。只有当集合之间完全不同时，树才会分叉。树节点上给出集合中的单个元素及其在序列中的出现次数，路径会给出该序列的出现次数。上面这一切听起来可能有点让人迷糊，不过不用担心，稍后就会介绍FP树的构建过程。

相似项之间的链接即节点链接（node link），用于快速发现相似项的位置。为了打消读者的疑惑，下面通过一个简单例子来说明。表12-1给出了用于生成图12-1中所示FP树的数据。

表12-1 用于生成图12-1中FP树的事务数据样例  

<table><tr><td>事务ID</td><td>事务中的元素项</td></tr><tr><td>001</td><td>r, z, h, j, p</td></tr><tr><td>002</td><td>z, y, x, w, v, u, t, s</td></tr><tr><td>003</td><td>z</td></tr><tr><td>004</td><td>r, x, n, o, s</td></tr><tr><td>005</td><td>y, r, x, z, q, t, p</td></tr><tr><td>006</td><td>y, z, x, e, q, s, t, m</td></tr></table>

在图12-1中，元素项z出现了5次，集合 $\{\mathbf{r},\mathbf{z}\}$ 出现了1次。于是可以得出结论：z一定是自己本身或者和其他符号一起出现了4次。我们再看下z的其他可能性。集合 $\{\mathrm{t,s,y,x,z}\}$ 出现了2次，集合 $\{\mathrm{t,r,y,x,z}\}$ 出现了1次。元素项z的右边标的是5，表示z出现了5次，其中刚才已经给出了4次出现，所以它一定单独出现过1次。通过观察表12-1看看刚才的结论是否正确。前面提到 $\{\mathrm{t,r,y,x,z}\}$ 只出现过1次，在事务数据集中我们看到005号记录上却是 $\{\mathrm{y,r,x,z,q,t,p}\}$ 。那么，q和p去哪儿了呢？

这里使用第11章给出的支持度定义，该指标对应一个最小阈值，低于最小阈值的元素项被认为是不频繁的。如果将最小支持度设为3，然后应用频繁项分析算法，就会获得出现3次或3次以上的项集。上面在生成图12-1中的FP树时，使用的最小支持度为3，因此q和p并没有出现在最后的树中。

FP-growth算法的工作流程如下。首先构建FP树，然后利用它来挖掘频繁项集。为构建FP树，需要对原始数据集扫描两遍。第一遍对所有元素项的出现次数进行计数。记住第11章中给出的Apriori原理，即如果某元素是不频繁的，那么包含该元素的超集也是不频繁的，所以就不需要考虑这些超集。数据库的第一遍扫描用来统计出现的频率，而第二遍扫描中只考虑那些频繁元素。

# FP-growth的一般流程

(1)收集数据：使用任意方法。  
(2) 准备数据：由于存储的是集合，所以需要离散数据。如果要处理连续数据，需要将它们量化为离散值。  
(3) 分析数据：使用任意方法。  
(4) 训练算法：构建一个FP树，并对树进行挖掘。  
(5) 测试算法：没有测试过程。  
(6) 使用算法：可用于识别经常出现的元素项，从而用于制定决策、推荐元素或进行预测等应用中。

# 12.2 构建FP树

在第二次扫描数据集时会构建一棵FP树。为构建一棵树，需要一个容器来保存树。

# 12.2.1 创建FP树的数据结构

本章的FP树要比书中其他树更加复杂，因此要创建一个类来保存树的每一个节点。创建文件fpGrowth.py并加入下列程序中的代码。

# 程序清单12-1 FP树的类定义

class treeNode: def init_(self, nameValue, numOccur, parentNode): self.name = nameValue self.count $=$ numOccur self.nodeLink $\equiv$ None self.parent $\equiv$ parentNode self.children $\equiv$ {} def inc(self, numOccur): self.count $+ =$ numOccur def disp(self, ind=1): print' \*\*ind,self.name,'',self.count for child in self.children.values(): child_disp(ind+1)

上面的程序给出了FP树中节点的类定义。类中包含用于存放节点名字的变量和1个计数值，nodeLink变量用于链接相似的元素项（参考图12-1中的虚线）。类中还使用了父变量parent来指向当前节点的父节点。通常情况下并不需要这个变量，因为通常是从上往下迭代访问节点的。本章后面的内容中需要根据给定叶子节点上溯整棵树，这时就需要指向父节点的指针。最后，类中还包含一个空字典变量，用于存放节点的子节点。

程序清单12-1中包括两个方法，其中inc()对count变量增加给定值，而另一个方法disp()用于将树以文本形式显示。后者对于树构建来说并不是必要的，但是它对于调试非常有用。

运行一下如下代码：

```python
>>> import fpGrowth
>>> rootNode = fpGrowth.treeNode('pyramid', 9, None) 
```

这会创建树中的一个单节点。接下来为其增加一个子节点：

>>>rootNode.children['eye'] $\equiv$ fpGrowth.treeNode('eye'，13，None)

为显示子节点，输入：

```txt
>>> rootNodedisp()
pyramid 9
eye 13 
```

再添加一个节点看看两个子节点的展示效果：

```txt
>>> rootNode(children['phoenix'] = fpGrowth.treeNode('phoenix', 3, None)
>>> rootNodedisp()
pyramid 9
eye 13
phoenix 3 
```

现在FP树所需数据结构已经建好，下面就可以构建FP树了。

# 12.2.2 构建FP树

除了图12-1给出的FP树之外，还需要一个头指针表来指向给定类型的第一个实例。利用头指针表，可以快速访问FP树中一个给定类型的所有元素。图12-2给出了一个头指针表的示意图。

![](images/44cce2a81999607d916efe23476a2b0c3cd42dfd5aa7850eeebcc7e6d1592ecb.jpg)  
图12-2 带头指针表的FP树，头指针表作为一个起始指针来发现相似元素项

这里使用一个字典作为数据结构，来保存头指针表。除了存放指针外，头指针表还可以用来保存FP树中每类元素的总数。

第一次遍历数据集会获得每个元素项的出现频率。接下来，去掉不满足最小支持度的元素项。再下一步构建FP树。在构建时，读入每个项集并将其添加到一条已经存在的路径中。如果该路径不存在，则创建一条新路径。每个事务就是一个无序集合。假设有集合 $\{z, x, y\}$ 和 $\{y, z, r\}$ ，那么在FP树中，相同项会只表示一次。为了解决此问题，在将集合添加到树之前，需要对每个集合进行排序。排序基于元素项的绝对出现频率来进行。使用图12-2中的头指针节点值，对表12-1中数据进行过滤、重排序后的数据显示在表12-2中。

表12-2 将非频繁项移除并且重排序后的事务数据集  

<table><tr><td>事务ID</td><td>事务中的元素项</td><td>过滤及重排序后的事务</td></tr><tr><td>001</td><td>r, z, h, j, p</td><td>z, r</td></tr><tr><td>002</td><td>z, y, x, w, v, u, t, s</td><td>z, x, y, s, t</td></tr><tr><td>003</td><td>z</td><td>z</td></tr><tr><td>004</td><td>r, x, n, o, s</td><td>x, s, r</td></tr><tr><td>005</td><td>y, r, x, z, q, t, p</td><td>z, x, y, r, t</td></tr><tr><td>006</td><td>y, z, x, e, q, s, t, m</td><td>z, x, y, s, t</td></tr></table>

在对事务记录过滤和排序之后，就可以构建FP树了。从空集（符号为 $\varnothing$ ）开始，向其中不断添加频繁项集。过滤、排序后的事务依次添加到树中，如果树中已存在现有元素，则增加现有元素的值；如果现有元素不存在，则向树添加一个分枝。对表12-2前两条事务进行添加的过程显示在图12-3中。

![](images/ba6a6a9666ac41486122336484082c7f0f441914169a1fb7aaed1b32d228c64c.jpg)  
图12-3 FP树构建过程的一个示意图，图中给出了使用表12-2中数据构建FP树的前两步

通过上面的叙述，我们大致了解了从事务数据集转换为FP树的基本思想，接下来我们通过代码来实现上述过程。打开fpGrowth.py文件，加入下面的代码。

# 程序清单12-2 FP树构建函数

def createTree(dataSet,minSup=1): headerTable $=$ {} for trans in dataSet for item in trans: headerTable[item] $\equiv$ headerTable.get(item,0）+dataSet[trans] for k in headerTable keys(): if headerTable[k] $<$ minSup: del headerTable[k]) freqItemSet $\equiv$ set headerTable keys() if len(freqItemSet) $= = 0$ :return None，None for k in headerTable: headerTable[k] $\equiv$ [headerTable[k],None] retTree $\equiv$ treeNode('Null Set',1，None) for tranSet，count in dataSet.items(): localD $= \{\}$ for item in tranSet: if item in freqItemSet: localD[item] $\equiv$ headerTable[item][0] iflen(localD)>0: orderedItems $\equiv$ [v[0]for v in sorted(localD/items(), key=lambda p:p[1], reverse=True)] updateTree(orderedItems,retTree,\ headerTable,count) 4 使用排序后的频率项集对树进行填充 return retTree，headerTable   
defupdateTree(items,inTree,headerTable,count): if items[0]in inTree children: inTreeChildren-items[0].inc(count) else:

```python
inTree.children[items[0]] =TreeNode(items[0], count, inTree)  
if headerTable[items[0]][1] == None:  
    headerTable[items[0]][1] = inTree.children(items[0])  
else:  
    updateHeader(nodeToTest, targetNode);  
updateTree(items[1::], inTree.Children[inTree.node[0]])  
if len(itemss) > 1:  
    updateTree(itemss[1::], inTree.Children[inTree.node[0]], headerTable, count)  
def updateHeader(nodeToTest, targetNode):  
while (nodeToTest.nodeLink != None):  
    nodeToTest = nodeToTest.nodeLink  
nodeToTest.nodeLink = targetNode 
```

上述代码中包含3个函数。第一个函数createTree()使用数据集以及最小支持度作为参数来构建PP树。树构建过程中会遍历数据集两次。第一次遍历扫描数据集并统计每个元素项出现的频度。这些信息被存储在头指针表中。接下来，扫描头指针表删掉那些出现次数少于minSup的项①。如果所有项都不频繁，就不需要进行下一步处理②。接下来，对头指针表稍加扩展以便可以保存计数值及指向每种类型第一个元素项的指针。然后创建只包含空集合∅的根节点。最后，再一次遍历数据集，这次只考虑那些频繁项③。这些项已经如表12-2所示那样进行了排序，然后调用updateTree()方法④。接下来讨论函数updateTree()。

为了让FP树生长①，需调用updateTree，其中的输入参数为一个项集。图12-3给出了updateTree()中的执行细节。该函数首先测试事务中的第一个元素项是否作为子节点存在。如果存在的话，则更新该元素项的计数；如果不存在，则创建一个新的treeNode并将其作为一个子节点添加到树中。这时，头指针表也要更新以指向新的节点。更新头指针表需要调用函数updateHeader()，接下来会讨论该函数的细节。updateTree()完成的最后一件事是不断迭代调用自身，每次调用时会去掉列表中第一个元素⑤。

程序清单12-2中的最后一个函数是updateHeader()，它确保节点链接指向树中该元素项的每一个实例。从头指针表的nodeLink开始，一直沿着nodeLink直到到达链表末尾。这就是一个链表。当处理树的时候，一种很自然的反应就是迭代完成每一件事。当以相同方式处理链表时可能会遇到一些问题，原因是否能很快地找到迭代调用的次数限制。

在运行上例之前，还需要一个真正的数据集。这可以从代码库中获得，或者直接手工输入。loadSimpDat()函数会返回一个事务列表。这和表12-1中的事务相同。后面构建树时会使用createTree()函数，而该函数的输入数据类型不是列表。其需要的是一部字典，其中项集为字典中的键，而频率为每个键对应的取值。createInitSet()用于实现上述从列表到字典的类型转换过程。将下列代码添加到fpGrowth.py文件中。

程序清单12-3 简单数据集及数据包装器  
```python
def loadSimpDat():  
    simpDat = [[r', 'z', 'h', 'j', 'p'],
['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],
['z'],
['r', 'x', 'n', 'o', 's'],
['y', 'r', 'x', 'z', 'q', 't', 'p'],
['y', 'z', 'x', 'e', 'q', 's', 't', 'm'])  
return simpDat  
def createInitSet(dataSet):  
    retDict = {}
    for trans in dataSet:  
        retDict[frozenset(trans)] = 1  
    return retDict 
```

好了，下面看看实际的效果。将程序清单12-3中的代码加入文件fpGrowth.py之后，在Python提示符下输入命令：

```html
>>> reload(fpGrowth)
<module 'fpGrowth' from 'fpGrowth.py'> 
```

首先，导入数据库实例：

>>>simpDat $=$ fpGrowth.loadSimpDat()   
>>>simpDat $[\mathsf{[r^{\prime}]}$ ，'z'，'h'，'j'，'p']，['z'，'y'，'x'，'w'，'v'，'u'，'t'，'s'], ['z']，['r'，'x'，'n'，'o'，'s']，['y'，'r'，'x'，'z'，'q'，'t'，'p']， ['y'，'z'，'x'，'e'，'q'，'s'，'t'，'m']]

接下来为了函数createTree()，需要对上面的数据进行格式化处理：

```python
>>> initSet = fpGrowth.createInitSet(simpDat)
>>> initSet
{frozenset(['e', 'm', 'q', 's', 't', 'y', 'x', 'z']) : 1, frozenset(['x', 's', 'r', 'o', 'n']) : 1, frozenset(['s', 'u', 't', 'w', 'v', 'y', 'x', 'z']) : 1, frozenset(['q', 'p', 'r', 't', 'y', 'x', 'z']) : 1, frozenset(['h', 'r', 'z', 'p', 'j']) : 1, frozenset(['z']) : 1} 
```

于是可以通过如下命令创建FP树：

```txt
>>> myFPtree, myHeaderTab = fpGrowth.createTree(initSet, 3) 
```

使用disp()方法给出树的文本表示结果：

```txt
>>> myFPtree_disp()
Null Set 1
x 1
s 1
r 1
z 5
x 3
y 3
s 2
t 2
r 1
t 1
r 1 
```

上面给出的是元素项及其对应的频率计数值，其中每个缩进表示所处的树的深度。读者可以验证一下这棵树与图12-2中所示的树是否等价。

现在我们已经构建了FP树，接下来就使用它进行频繁项集挖掘。

# 12.3 从一棵FP树中挖掘频繁项集

实际上，到现在为止大部分比较困难的工作已经处理完了。接下来写的代码不会再像12.1节那样多了。有了FP树之后，就可以抽取频繁项集了。这里的思路与Apriori算法大致类似，首先从单元素项集合开始，然后在此基础上逐步构建更大的集合。当然这里将利用FP树来做实现上述过程，不再需要原始数据集了。

从FP树中抽取频繁项集的三个基本步骤如下：

(1) 从FP树中获得条件模式基；  
(2)利用条件模式基，构建一个条件FP树；  
(3) 迭代重复步骤(1)步骤(2)，直到树包含一个元素项为止。

接下来重点关注第(1)步，即寻找条件模式基的过程。之后，为每一个条件模式基创建对应的条件FP树。最后需要构造少许代码来封装上述两个函数，并从FP树中获得频繁项集。

# 12.3.1 抽取条件模式基

首先从上一节发现的已经保存在头指针表中的单个频繁元素项开始。对于每一个元素项，获得其对应的条件模式基（conditional pattern base）。条件模式基是以所查找元素项为结尾的路径集合。每一条路径其实都是一条前缀路径（prefix path）。简而言之，一条前缀路径是介于所查找元素项与树根节点之间的所有内容。

回到图12-2，符号r的前缀路径是 $\{\mathbf{x},\mathbf{s}\} ,\{\mathbf{z},\mathbf{x},\mathbf{y}\}$ 和 $\{\mathbf{z}\}$ 。每一条前缀路径都与一个计数值关联。该计数值等于起始元素项的计数值，该计数值给了每条路径上r的数目。表12-3列出了上例当中每一个频繁项的所有前缀路径。

表12-3 每个频繁项的前缀路径  

<table><tr><td>频 繁 项</td><td>前缀路径</td></tr><tr><td>z</td><td>\{\}5</td></tr><tr><td>r</td><td>{x,s}1, {z,x,y}1, {z}1</td></tr><tr><td>x</td><td>{z}3, \{\}1</td></tr><tr><td>y</td><td>{z,x}3</td></tr><tr><td>s</td><td>{z,x,y}2, {x}1</td></tr><tr><td>t</td><td>{z,x,y,s}2, {z,x,y,r}1</td></tr></table>

前缀路径将被用于构建条件FP树，但是现在暂时先不需要考虑这件事。为了获得这些前缀路径，可以对树进行穷举式搜索，直到获得想要的频繁项为止，或者使用一个更有效的方法来加速搜索过程。可以利用先前创建的头指针表来得到一种更有效的方法。头指针表包含相同类型元素

链表的起始指针。一旦到达了每一个元素项，就可以以上溯这棵树直到根节点为止。

下面的程序清单给出了前缀路径发现的代码，将其添加到文件fpGrowth.py中。

# 程序清单12-4 发现以给定元素项结尾的所有路径的函数

def ascendTree(leafNode，prefixPath)： if leafNode.parent != None: prefixPath.append(leafNode.name) ascendTree(leafNode.parent，prefixPath)   
def findPrefixPath(basePat，treeNode）： condPats $= \{\}$ while treeNode！ $=$ None: prefixPath $= \left[\right]$ ascendTree-treeNode，prefixPath) iflen(prefixPath）>1: condPats[frozenset(prefixPath[1:])] $=$ treeNode.count treeNode $=$ treeNode.nodeLink return condPats

上述程序中的代码用于为给定元素项生成一个条件模式基，这通过访问树中所有包含给定元素项的节点来完成。当创建树的时候，使用头指针表来指向该类型的第一个元素项，该元素项也会链接到其后续元素项。函数findPrefixPath()遍历链表直到到达结尾。每遇到一个元素项都会调用ascendTree()来上溯FP树，并收集所有遇到的元素项的名称①。该列表返回之后添加到条件模式基字典condPats中。

使用之前构建的树来看一下实际的运行效果：

```python
>>> reload(fpGrowth)
<module 'fpGrowth' from 'fpGrowth.py>
>>> fpGrowth.findPrefixPath('x', myHeaderTab['x'] [1])
{frozenset(['z']: 3}
>>> fpGrowth.findPrefixPath('z', myHeaderTab['z'] [1])
}
>>> fpGrowth.findPrefixPath('r', myHeaderTab['r']: 1]
{frozenset(['x', 's)): 1, frozenset(['z']: 1, frozenset(['y', 'x', 'z']: 1} 
```

读者可以检查一下这些值与表12-3中的结果是否一致。有了条件模式基之后，就可以创建条件FP树。

# 12.3.2 创建条件FP树

对于每一个频繁项，都要创建一棵条件FP树。我们会为z、x以及其他频繁项构建条件树。可以使用刚才发现的条件模式基作为输入数据，并通过相同的建树代码来构建这些树。然后，我们会递归地发现频繁项、发现条件模式基，以及发现另外的条件树。举个例子来说，假定为频繁项t创建一个条件FP树，然后对{t,y}、{t,x}、…重复该过程。元素项t的条件FP树的构建过程如图12-4所示。

# t的条件FP树

条件模式基： $\{\mathbf{y},\mathbf{x},\mathbf{s},\mathbf{z}\} :2,\quad \{\mathbf{y},\mathbf{x},\mathbf{r},\mathbf{z}\} :1$

最小支持度 $= 3$

去掉：s&r

![](images/d6b59a79270e9e13b546543dbc8a737b96c17103e1517cae67c3e2fe23befa59.jpg)  
图12-4 t的条件FP树的创建过程。最初树以空集作为根节点。接下来，原始的集合 $\{y,x,s,z\}$ 中的集合 $\{y,x,z\}$ 被添加进来。因为不满足最小支持度要求，字符s并没有加入进来。类似地， $\{y,x,z\}$ 也从原始集合 $\{y,x,r,z\}$ 中添加进来

在图12-4中，注意到元素项s以及r是条件模式基的一部分，但是它们并不属于条件FP树。原因是什么？如果讨论s以及r的话，它们难道不是频繁项吗？实际上单独来看它们都是频繁项，但是在t的条件树中，它们却不是频繁的，也就是说，{t,r}及{t,s}是不频繁的。

接下来，对集合 $\{\mathfrak{t},\mathbf{z}\} ,\{\mathfrak{t},\mathbf{x}\}$ 以及 $\{\mathfrak{t},\mathbf{y}\}$ 来挖掘对应的条件树。这会产生更复杂的频繁项集。该过程重复进行，直到条件树中没有元素为止，然后就可以停止了。实现代码相对比较直观，使用一些递归加上之前写的代码就可以完成。打开fpGrowth.py，将下面程序中的代码添加进去。

# 程序清单12-5 递归查找频繁项集的mineTree函数

![](images/1203c880bb67d72259b92be59d2d18644cdc2f4088d4ae7198ac58af756cd29b.jpg)

创建条件树、前缀路径以及条件基的过程听起来比较复杂，但是代码起来相对简单。程序首先对头指针表中的元素项按照其出现频率进行排序。（记住这里的默认顺序是按照从小到大。）①然后，将每一个频繁项添加到频繁项集列表freqItemList中。接下来，递归调用程序清单12-4中的findPrefixPath()函数来创建条件基。该条件基被当成一个新数据集输送给createTree()函数。②这里为函数createTree()添加了足够的灵活性，以确保它可以被重用于构建

条件树。最后，如果树中有元素项的话，递归调用mineTree()函数③。

下面将整个程序合并到一块看看代码的实际运行效果。将程序清单12-5中的代码添加到文件fpGrowth.py中并保存，然后在Python提示符下输入：

```html
>>> reload(fpGrowth)
<module 'fpGrowth' from 'fpGrowth.py'> 
```

下面建立一个空列表来存储所有的频繁项集：

```txt
>>>> freqItems = [] 
```

接下来运行mineTree()，显示出所有的条件树：

```txt
>>fpGrowth.mineTree(myFPtree, myHeaderTab, 3, set([]), freqItems)
conditional tree for: set(['y']) 
    Null Set 1
    x 3
    z 3
conditional tree for: set(['y', 'z']) 
    Null Set 1
    x 3
conditional tree for: set(['s']) 
    Null Set 1
    x 3
conditional tree for: set(['t']) 
    Null Set 1
    y 3
    x 3
conditional tree for: set(['x', 't']) 
    Null Set 1
    y 3
conditional tree for: set(['z', 't']) 
    Null Set 1
    y 3
    x 3
conditional tree for: set(['x', 'z', 't']) 
    Null Set 1
    y 3
conditional tree for: set(['x']) 
    Null Set 1
    z 3 
```

为了获得类似于前面代码的输出结果，我在函数mineTree()中添加了两行：

```typescript
print 'conditional tree for: ',newFreqSet myCondTreedisp(1) 
```

这两行被添加到程序中语句if myHead != None:和mineTree()函数调用之间。

下面检查一下返回的项集是否与条件树匹配：

```txt
>>>freqItems [set([['y']），set([['y'，'z']），set([['y'，'x'，'z']），set([['y'，'x']）, set([['s']），set([['x'，'s']），set([['t']），set([['y'，'t']），set([['x'， 't']），set([['y'，'x'，'t']），set([['z'，'t']），set([['y'，'z'，'t']），
```

```txt
set([['x', 'z', 't'])， set([['y', 'x', 'z', 't'])， set([['r'])， set([['x'])， set([['x', 'z'])， set([['z'])].
```

正如我们所期望的那样，返回项集与条件FP树相匹配。到现在为主，完整的FP-growth算法已经可以运行，接下来在一个真实的例子上看一下运行效果。我们将看到是否能从微博网站Twitter中获得一些常用词。

# 12.4 示例：在Twitter源中发现一些共现词

我们会用到一个叫做python-twitter的Python库，其源代码可以在http://code.google.com/p/ python-twitter/下载。正如你猜到的那样，借助它，我们可以使用Python来访问Twitter。Twitter.com实际上是一个和其他人进行交流的通道，其上发表的内容被限制在140个字符以内，发表的一条信息称为推文（tweet）。

有关Twitter API的文档可以在http://dev.twitter.com/doc找到。API文档与Python模块中的关键词并不完全一致。我推荐直接阅读Python文件twitter.py，以完全理解库的使用方法。有关该模块的安装可以参考附录A。虽然这里只会用到函数库的一小部分，但是使用API可以做更多事情，所以我鼓励读者去探索一下API的所有功能。

# 示例：发现Twitter源中的典型词（co-occurring word）

(1)收集数据：使用Python-twitter模块来访问推文。  
(2) 准备数据：编写一个函数来去掉URL、去掉标点、转换成小写并从字符串中建立一个单词集合。  
(3) 分析数据：在Python提示符下查看准备好的数据，确保它的正确性。  
(4)训练算法：使用本章前面开发的createTree()与mineTree()函数执行PP-growth算法。  
(5) 测试算法：这里不适用。  
(6) 使用算法：本例中没有包含具体应用，可以考虑用于情感分析或者查询推荐领域。

在使用API之前，需要两个证书集合。第一个集合是consumer_key和consumer_secret，当注册开发app时（https://dev.twitter.com/apps/new），可以从Twitter开发服务网站获得。这些key对于要编写的app是特定的。第二个集合是access_token_key和access_token_secret，它们是针对特定Twitter用户的。为了获得这些key，需要查看Twitter-Python安装包中的get_access_token.py文件（或者从Twitter开发网站中获得）。这是一个命令行的Python脚本，该脚本使用 OAuth来告诉Twitter应用程序具有用户的权限来发布信息。一旦完成上述工作之后，可以将获得的值放入前面的代码中开始工作。对于给定的搜索词，下面要使用FP_growth算法来发现推文中的频繁单词集合。要提取尽可能多的推文（1400条）然后放到FP_growth算法中运行。将下面的代码添加到fpGrowth.py文件中。

# 程序清单12-6 访问TwitterPython库的代码

```python
import twitter
from time import sleep
import re
def getLotsOfTweetssearchStr):
    CONSUMER_KEY = 'get when you create an app'
    CONSUMER_secret = 'get when you create an app'
    ACCESS_TOKEN_KEY = 'get from Oauth, specific to a user'
    ACCESS_TOKEN_secret = 'get from Oauth, specific to a user'
    api = twitter Api(consumer_key=CONSUMER_KEY,
                    consumer_secret=CONSUMER_secret,
                    access_token_key=ACCESS_TOKEN_KEY,
                    access_token_secret=ACCESS_TOKEN_secret)
    #you can get 1500 results 15 pages * 100 per page
resultsPages = []
for i in range(1,15):
    print "fetching page %d" % i
    searchResults = api.GetSearch.searchStr, per_page=100, page=i)
    resultsPages.append性和 results)
sleep(6)
return resultsPages 
```

这里需要导入三个库，分别是twitter库、用于正则表达式的库，以及sleep函数。后面会使用正则表示式来帮助解析文本。

函数get LotsOfTweets()处理认证然后创建一个空列表。搜索API可以一次获得100条推文。每100条推文作为一页，而Twitter允许一次访问14页。在完成搜索调用之后，有一个6秒钟的睡眠延迟，这样做是出于礼貌，避免过于频繁的访问请求。print语句用于表明程序仍在执行没有死掉。

下面来抓取一些推文，在Python提示符下输入：

```html
>>> reload(fpGrowth)
<module 'fpGrowth' from 'fpGrowth.py'> 
```

接下来要搜索一支名为RIMM的股票：

>>>lotsOtweets $=$ fpGrowth.getLotsOfTweets('RIMM')   
fetching page 1   
fetching page 2

lotsOtweets列表包含14个子列表，每个子列表有100条推文。可以输入下面的命令来查看推文的内容：

```txt
>>>lotsOtweets[0][4].text u"RIM: Open The Network, Says ThinkEquity: In addition, RIMM needs to reinvent its image, not only demonstrating ... http://bit.ly/lvlVlU" 
```

正如所看到的那样，有些人会在推文中放入URL。这样在解析时，结果就会比较乱。因此必须去掉URL，以便可以获得推文中的单词。下面程序清单中的一部分代码用来将推文解析成字符串列

表，另一部分会在数据集上运行FP-growth算法。将下面的代码添加到fpGrowth.py文件中。

# 程序清单12-7 文本解析及合成代码

```python
def textParse(longs):   
    urlslRemoved = re.sub(' (http[s]?: [/] [/] | www.) ([a-z] | [A-Z] | [0-9] | [~]) * ',   
    '.', longString)   
    listOfTokens = re.split(r'\W*', urlsRemoved)   
    return [tok.lower() for tok in listOfTokens if len(tok) > 2]   
def mineTweets(tweetArr, minSup=5):   
    parsedList = []   
    for i in range(14):   
        for j in range(100):   
            parsedList.append(textParse(tweetArr[i] [j].text))   
    initSet = createInitSet(parsedList)   
    myFPtree, myHeaderTab = createTree(initSet, minSup)   
    myFreqList = []   
    mineTree(myFPtree, myHeaderTab, minSup, set [], myFreqList)   
    return myFreqList 
```

上述程序清单中的第一个函数来自第4章，此外这里添加了一行代码用于去除URL。这里通过调用正则表达式模块来移除任何URL。程序清单12-7中的另一个函数mineTweets()为每个推文调用textParse。最后，mineTweets()函数将12.2节中用过的命令封装到一起，来构建FP树并对其进行挖掘。最后返回所有频繁项集组成的列表。

下面看看运行的效果：

```txt
>>> reload(fpGrowth)
<module 'fpGrowth' from 'fpGrowth.py>
Let's look for sets that occur more than 20 times:
>>> listOfTerms = fpGrowth.mineTweets(lotsOtweets, 20)
How many sets occurred in 20 or more of the documents?
>>> len(listOfTerms)
455 
```

我写这段代码的前一天，一家以RIMM股票代码进行交易的公司开了一次电话会议，会议并没有令投资人满意。该股开盘价相对前一天封盘价暴跌 $22\%$ 。下面看下上述情况是否在推文中体现：

```txt
>>> for t in listOfTerms:  
... print t  
set([u'rimm', u'day'])  
set([u'rimm', u'earnings'])  
set([u'pounding', u'value'])  
set([u'pounding', u'overnight'])  
set([u'pounding', u'drops'])  
set([u'pounding', u're shares'])  
set([u'pounding', u'are']) 
```

```txt
set([u'overnight'])  
set([u'drops', u'overnight'])  
set([u'motion', u'drops', u'overnight'])  
set([u'motion', u'drops', u'overnight', u'value'])  
set([u'drops', u'overnight', u'research'])  
set([u'drops', u'overnight', u'value', u'research'])  
set([u'motion', u'drops', u'overnight', u'value', u'research'])  
set([u'motion', u'drops', u'overnight', u'research'])  
set([u'drops', u'overnight', u'value']) 
```

尝试一些其他的minSupport值或者搜索词也是蛮有趣的。

我们还记得FP树的构建是通过每次应用一个实例的方式来完成的。这里假设已经获得了所有数据，所以刚才是直接遍历所有的数据来构建FP树的。实际上可以重写createTree()函数，每次读入一个实例，并随着Twitter流的不断输入而不断增长树。FP-growth算法还有一个map-reduce版本的实现，它也很不错，可以扩展到多台机器上运行。Google使用该算法通过遍历大量文本来发现频繁共现词，其做法和我们刚才介绍的例子非常类似①。

# 12.5 示例：从新闻网站点击流中挖掘

好了，本章的最后一个例子很酷，而你有可能正在想：“伙计，这个算法应该很快，因为只有1400条推文！”你的想法是正确的。下面在更大的文件上看下运行效果。在源数据集合中，有一个kosarak.dat文件，它包含将近100万条记录②。该文件中的每一行包含某个用户浏览过的新闻报道。一些用户只看过一篇报道，而有些用户看过2498篇报道。用户和报道被编码成整数，所以查看频繁项集很难得到更多的东西，但是该数据对于展示FP-growth算法的速度十分有效。

首先，将数据集导入到列表：

```txt
>>> parsedDat = [line.split() for line in open('kogarak.dat').readlines())] 
```

接下来需要对初始集合格式化：

>>>initSet $=$ fpGrowth.createInitSet(parsedat)

然后构建FP树，并从中寻找那些至少被10万人浏览过的新闻报道。

```txt
>>> myFPtree, myHeaderTab = fpGrowth.createTree(initSet, 100000) 
```

在我这台简陋的笔记本电脑上，构建树以及扫描100万行只需要几秒钟，这展示了FP-growth算法的强大威力。下面需要创建一个空列表来保存这些频繁项集：

```txt
>>> myFreqList = []
>>> fpGrowth.mineTree(myFPtree, myHeaderTab, 100000, set [], myFreqList) 
```

接下来看下有多少新闻报道或报道集合曾经被10万或者更多的人浏览过：

```html
>>>len(myFreqList) 9 
```

总共有9个。下面看看都是哪些：

```txt
>>myFreqList [set([['1']])，set([['1'，‘6']），set([['3']），set([['11'，‘3']），set([['11'，‘3' '6']），set([['3'，‘6']），set([['11']），set([['11'，‘6']），set([['6'])]]
```

可以使用其他设置来查看运行结果，比如降低置信度级别。

# 12.6 本章小结

FP-growth算法是一种用于发现数据集中频繁模式的有效方法。FP-growth算法利用Apriori原则，执行更快。Apriori算法产生候选项集，然后扫描数据集来检查它们是否频繁。由于只对数据集扫描两次，因此FP-growth算法执行更快。在FP-growth算法中，数据集存储在一个称为FP树的结构中。FP树构建完成后，可以通过查找元素项的条件基及构建条件FP树来发现频繁项集。该过程不断以更多元素作为条件重复进行，直到FP树只包含一个元素为止。

可以使用PP-growth算法在多种文本文档中查找频繁单词。Twitter网站为开发者提供了大量的API来使用他们的服务。利用Python模块Python-Twitter可以很容易访问Twitter。在Twitter源上对某个话题应用FP-growth算法，可以得到一些有关该话题的摘要信息。频繁项集生成还有其他的一些应用，比如购物交易、医学诊断及大气研究等。

下面几章会介绍一些附属工具。第13章和第14章会介绍一些降维技术，使用这些技术可以提炼数据中的重要信息并且移除噪声。第14章会介绍Map Reduce技术，当数据量超过单台机器的处理能力时，将会需要这些技术。

![](images/187b1f3be1f2bc28b3ebaf97803ff4fd0f355f4b1128b368d82b024a51dac4df.jpg)

# 其他工具

本书第四部分即是最后一部分，主要介绍在机器学习实践时常用的一些其他工具，它们可以应用于前三部分的算法上。这些工具还包括了可以对前三部分中任一算法的输入数据进行预处理的降维技术。这一部分还包括了在上千台机器上分配作业的Map Reduce技术。

降维的目标就是对输入的数目进行削减，由此剔除数据中的噪声并提高机器学习方法的性能。第13章将介绍按照数据方差最大方向调整数据的主成分分析降维方法。第14章解释奇异值分解，它是矩阵分解技术中的一种，通过对原始数据的逼近来达到降维的目的。

第15章是本书的最后一章，主要讨论了在大数据下的机器学习。大数据（big data）指的就是数据集很大以至于内存不足以将其存放。如果数据不能在内存中存放，那么在内存和磁盘之间传输数据时就会浪费大量的时间。为了避免这一点，我们就可以将整个作业进行分片，这样就可以在多机下进行并行处理。Map Reduce就是实现上述过程的一种流行的方法，它将作业分成了Map任务和Reduce任务。第15章将介绍Python中Map Reduce实现的一些常用工具，同时也介绍了将机器学习转换成满足Map Reduce编程范式的方法。

# 第13章

# 利用PCA来简化数据

# 本章内容

□降维技术  
□主成分分析（PCA）  
□对半导体数据进行降维处理

想象这样一种场景：我们正通过电视而非现场观看体育比赛，在电视的纯平显示器上有一个球。显示器大概包含了100万像素，而球则可能是由较少的像素组成的，比如说一千个像素。在大部分体育比赛中，我们关注的是给定时刻球的位置。人的大脑要想了解比赛的进展，就需要了解球在运动场中的位置。对于人来说，这一切显得十分自然，甚至都不需要做任何思考。在这个场景当中，人们实时地将显示器上的百万像素转换成为了一个三维图像，该图像就给出了运动场上球的位置。在这个过程中，人们已经将数据从一百万维降至了三维。

在上述体育比赛的例子中，人们面对的原本是百万像素的数据，但是只有球的三维位置才最重要，这就被称为降维（dimensionality reduction）。刚才我们将超百万的数据值降到了只有三个相关值。在低维下，数据更容易进行处理。另外，其相关特征可能在数据中明确地显示出来。通常而言，我们在应用其他机器学习算法之前，必须先识别出其相关特征。

本章是涉及降维主题的两章中的第一章。在降维中，我们对数据进行了预处理。之后，采用其他机器学习技术对其进行处理。本章一开始对降维技术进行了综述，然后集中介绍一种应用非常普遍的称为主成分分析的技术。最后，我们就通过一个数据集的例子来展示PCA的工作过程。经过PCA处理之后，该数据集就从590个特征降低到了6个特征。

# 13.1 降维技术

始终贯穿本书的一个难题就是对数据和结果的展示，这是因为这本书只是二维的，而在通常的情况下我们的数据不是如此。有时我们会显示三维图像或者只显示其相关特征，但是数据往往拥有超出显示能力的更多特征。数据显示并非大规模特征下的唯一难题，对数据进行简化还有如下一系列的原因：

□使得数据集更易使用；

□ 降低很多算法的计算开销；  
□去除噪声；  
□ 使得结果易懂。

在已标注与未标注的数据上都有降维技术。这里我们将主要关注未标注数据上的降维技术，该技术同时也可以应用于已标注的数据。

第一种降维的方法称为主成分分析（Principal Component Analysis，PCA）。在PCA中，数据从原来的坐标系转换到了新的坐标系，新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。我们会发现，大部分方差都包含在最前面的几个新坐标轴中。因此，我们可以忽略余下的坐标轴，即对数据进行了降维处理。在13.2节我们将会对PCA的细节进行深入介绍。

另外一种降维技术是因子分析（Factor Analysis）。在因子分析中，我们假设在观察数据的生成中有一些观察不到的隐变量（latent variable）。假设观察数据是这些隐变量和某些噪声的线性组合。那么隐变量的数据可能比观察数据的数目少，也就是说通过找到隐变量就可以实现数据的降维。因子分析已经应用于社会科学、金融和其他领域中了。

还有一种降维技术就是独立成分分析（Independent Component Analysis，ICA）。ICA假设数据是从N个数据源生成的，这一点和因子分析有些类似。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中只假设数据是不相关的。同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。

在上述3种降维技术中，PCA的应用目前最为广泛，因此本章主要关注PCA。在下一节中，我们将会对PCA进行介绍，然后再通过一段Python代码来运行PCA。

# 13.2 PCA

# 生成分分析

优点：降低数据的复杂性，识别最重要的多个特征。

缺点：不一定需要，且可能损失有用信息。

适用数据类型：数值型数据。

首先我们讨论PCA背后的一些理论知识，然后介绍如何通过Python的NumPy来实现PCA。

# 13.2.1 移动坐标轴

考虑一下图13-1中的大量数据点。如果要求我们画出一条直线，这条线要尽可能覆盖这些点，那么最长的线可能是哪条？我做过多次尝试。在图13-1中，3条直线中B最长。在PCA中，我们对数据的坐标进行了旋转，该旋转的过程取决于数据的本身。第一条坐标轴旋转到覆盖数据的最大

方差位置，即图中的直线B。数据的最大方差给出了数据的最重要的信息。

在选择了覆盖数据最大差异性的坐标轴之后，我们选择了第二条坐标轴。假如该坐标轴与第一条坐标轴垂直，它就是覆盖数据次大差异性的坐标轴。这里更严谨的说法就是正交（orthogonal）。当然，在二维平面下，垂直和正交是一回事。在图13-1中，直线C就是第二条坐标轴。利用PCA，我们将数据坐标轴旋转至数据角度上的那些最重要的方向。

![](images/6e722f7e204ce7fd3cc9576d3cdfb0d88acd16734863cba33f14c046f98a8007.jpg)  
图13-1 覆盖整个数据集的三条直线，其中直线B最长，并给出了数据集中差异化最大的方向

我们已经实现了坐标轴的旋转, 接下来开始讨论降维。坐标轴的旋转并没有减少数据的维度。考虑图13-2, 其中包含着3个不同的类别。要区分这3个类别, 可以使用决策树。我们还记得决策树每次都是基于一个特征来做决策的。我们会发现, 在 $x$ 轴上可以找到一些值, 这些值能够很好地将这3个类别分开。这样, 我们就可能得到一些规则, 比如当 $(X < 4)$ 时, 数据属于类别0。如果使用SVM这样稍微复杂一点的分类器, 我们就会得到更好的分类面和分类规则, 比如当 $(w0 * x + w1 * y + b) > 0$ 时, 数据也属于类别0。SVM可能比决策树得到更好的分类间隔, 但是分类超平面却很难解释。

通过PCA进行降维处理，我们就可以同时获得SVM和决策树的优点：一方面，得到了和决策树一样简单的分类器，同时分类间隔和SVM一样好。考察图13-2中下面的图，其中的数据来自于上面的图并经PCA转换之后绘制而成的。如果仅使用原始数据，那么这里的间隔会比决策树的间隔更大。另外，由于只需要考虑一维信息，因此数据就可以通过比SVM简单得多的很容易采用的规则进行区分。

![](images/0049f00ea24f46f7725f423db3e1786a94249a0838104f2d1f49884fce4053f7.jpg)

![](images/2a6effc408c75dbffb78883c9a38c7e53275a6780da5cfb9da8a831e7728e99f.jpg)  
图13-2 二维空间的3个类别。当在该数据集上应用PCA时，就可以去掉一维，从而使得该分类问题变得更容易处理

在图13-2中，我们只需要一维信息即可，因为另一维信息只是对分类缺乏贡献的噪声数据。在二维平面下，这一点看上去微不足道，但是如果在高维空间下则意义重大。

我们已经对PCA的基本过程做出了简单的阐述，接下来就可以通过代码来实现PCA过程。前面我曾提到的第一个主成分就是从数据差异性最大（即方差最大）的方向提取出来的，第二个主成分则来自于数据差异性次大的方向，并且该方向与第一个主成分方向正交。通过数据集的协方差矩阵及其特征值分析，我们就可以求得这些主成分的值。

一旦得到了协方差矩阵的特征向量，我们就可以保留最大的 $N$ 个值。这些特征向量也给出了 $N$ 个最重要特征的真实结构。我们可以通过将数据乘上这 $N$ 个特征向量而将它转换到新的空间。

# 特此公告。

特征值分析是线性代数中的一个领域，它能够通过数据的一般格式来揭示数据的“真实”结构，即我们常说的特征向量和特征值。在等式 $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$ 中， $\mathbf{v}$ 是特征向量， $\lambda$ 是特征值。特征值都是简单的标量值，因此 $\mathbf{A}\mathbf{v} = \lambda \mathbf{v}$ 代表的是：如果特征向量 $\mathbf{v}$ 被某个矩阵 $\mathbf{A}$ 左乘，那么它就等于某个标量 $\lambda$ 乘以 $\mathbf{v}$ 。幸运的是，NumPy中有寻找特征向量和特征值的模块linalg，它有eig()方法，该方法用于求解特征向量和特征值。

# 13.2.2 在 NumPy 中实现 PCA

将数据转换成前 $N$ 个主成分的伪码大致如下：

去除平均值

计算协方差矩阵

计算协方差矩阵的特征值和特征向量

将特征值从大到小排序

保留最上面的 $N$ 个特征向量

将数据转换到上述 $N$ 个特征向量构建的新空间中

建立一个名为pca.py的文件并将下列代码加入用于计算PCA。

# 程序清单13-1 PCA算法

```python
from numpy import *  
def loadDataSet(fileName, delim='\t'):  
    fr = open(fileName)  
stringArr = [line.strip().split(delim) for line in fr.readlines()]  
datArr = [map(float,line) for line in stringArr]  
return mat(matArr)  
def PCA(dataMat, topNfeat=9999999):  
    meanVals = mean(dataMat, axis=0)  
    meanRemoved = dataMat - meanVals  
    covMat = cov(meanRemoved, rowvar=0)  
    eigVals, eigVects = linalg.eig(mat(covMat))  
    eigValInd = argsort(eigVals)  
    eigValInd = eigValInd[-((topNfeat+1):-1]  
    redEigVects = eigVects[., eigValInd]  
    lowDDataMat = meanRemoved * redEigVects  
    reconMat = (lowDDataMat * redEigVects.T) + meanVals  
    return lowDDataMat, reconMat 
```

程序清单13-1中的代码包含了通常的NumPy导入和loadDataSet()函数。这里的loadDataSet()函数和前面章节中的版本有所不同，因为这里使用了两个list comprehension来构建矩阵。

- pca()函数有两个参数：第一个参数是用于进行PCA操作的数据集，第二个参数topNfeat则是一个可选参数，即应用的N个特征。如果不指定topNfeat的值，那么函数就会返回前9999999个特征，或者原始数据中全部的特征。

首先计算并减去原始数据集的平均值。然后，计算协方差矩阵及其特征值，接着利用argsort()函数对特征值进行从小到大的排序。根据特征值排序结果的逆序就可以得到topNfeat个最大的特征向量。这些特征向量将构成后面对数据进行转换的矩阵，该矩阵则利用 $N$ 个特征将原始数据转换到新空间中。最后，原始数据被重构后返回用于调试，同时降维之后的数据集也被返回了。

一切都看上去不错，是不是？在进入规模更大的例子之前，我们先看看上面代码的运行效果

以确保其结果正确无误。

```txt
>>> import pca 
```

我们在testSet.txt文件中加入一个由1000个数据点组成的数据集，并通过如下命令将该数据集调入内存：

```txt
>>> dataMat = PCA.loadDataSet('testSet.txt') 
```

于是，我们就可以在该数据集上进行PCA操作：

```txt
>>> lowDMat, reconMat = pca.pca(dataMat, 1) 
```

lowDMat包含了降维之后的矩阵，这里是个一维矩阵，我们通过如下命令进行检查：

```txt
>> shape(lowDMat) (1000, 1) 
```

我们可以通过如下命令将降维后的数据和原始数据一起绘制出来：

```python
>>> import matplotlib
>>> import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
>>> axscatter(dataMat[:,0]. flatten().A[0], dataMat[:,1]. flatten().A[0],
                     marker='^', s=90)
<matplotlibcollections.PathCollection object at 0x029B5C50>
>>> axscatter(reconMat[:,0]. flatten().A[0], reconMat[:,1]. flatten().A[0],
                     marker='o', s=50, c='red')
<matplotlibcollections.PathCollection object at 0x0372A210>plt.show() 
```

我们应该会看到和图13-3类似的结果。使用如下命令来替换原来的PCA调用，并重复上述过程：

```txt
>> lowDMat, reconMat = pca.pca(dataMat, 2) 
```

既然没有剔除任何特征，那么重构之后的数据会和原始的数据重合。我们也会看到和图13-3类似的结果（不包含图13-3中的直线）。

![](images/b88f0f6a5e42f8b20a0c91ef7b7f0cd24d1abe2b158c87d6b41a88cb834c5279.jpg)  
图13-3 原始数据集（三角形点表示）及第一主成分（圆形点表示）

# 13.3 示例：利用PCA对半导体制造数据降维

半导体是在一些极为先进的工厂中制造出来的。工厂或制造设备不仅需要花费上亿美元，而且还需要大量的工人。制造设备仅能在几年内保持其先进性，随后就必须更换了。单个集成电路的加工时间会超过一个月。在设备生命期有限，花费又极其巨大的情况下，制造过程中的每一秒钟都价值巨大。如果制造过程中存在瑕疵，我们就必须尽早发现，从而确保宝贵的时间不会花费在缺陷产品的生产上。

一些工程上的通用解决方案是通过早期测试和频繁测试来发现有缺陷的产品，但仍然有一些存在瑕疵的产品通过了测试。如果机器学习技术能够用于进一步减少错误，那么它就会为制造商节省大量的资金。

接下来我们将考察面向上述任务中的数据集，而它也比前面使用的数据集更大，并且包含了许多特征。具体地讲，它拥有590个特征①。我们看看能否对这些特征进行降维处理。读者也可以通过http://archive.ics.uci.edu/ml/machine-learning-databases/secom/得到该数据集。

该数据包含很多的缺失值。这些缺失值是以NaN（Not a Number的缩写）标识的。对于这些缺失值，我们有一些处理办法（参考第5章）。在590个特征下，几乎所有样本都有NaN，因此去除不完整的样本不太现实。尽管我们可以将所有的NaN替换成0，但是由于并不知道这些值的意义，所以这样做是个下策。如果它们是开氏温度，那么将它们置成0这种处理策略就太差劲了。下面我们用平均值来代替缺失值，平均值根据那些非NaN得到。

将下列代码添加到pca.py文件中。

# 程序清单13-2 将NaN替换成平均值的函数

def replaceNanWithMean(   ) :
    datMat = loadDataSet('secom.data', ' ' )
    数额 = shape(matMat) [1]
    for i in range ( numFeat ) :
     meanVal = mean (matMat [nonzero(~isnan (matMat[(:,i].A)) [0],i]) $\operatorname{datMat}\left\lbrack  {\text{nonzero}\left( {\text{isnan}\left( {\text{matMat}\left\lbrack  {(:,i}\right\rbrack  .A}\right) }\right) \left\lbrack  {0,i}\right\rbrack   = \text{meanVal}}\right\rbrack$ return matMat 将所有NaN置为平均值 ${}^{\text{②}}$

上述代码首先打开了数据集并计算出了其特征的数目，然后再在所有的特征上进行循环。对于每个特征，首先计算出那些非NaN值的平均值①。然后，将所有NaN替换为该平均值②。

我们已经去除了所有NaN，接下来考虑在该数据集上应用PCA。首先确认所需特征和可以去除特征的数目。PCA会给出数据中所包含的信息量。需要特别强调的是，数据（data）和信息（information）之间具有巨大的差别。数据指的是接受的原始材料，其中可能包含噪声和不相关

信息。信息是指数据中的相关部分。这些并非只是抽象概念，我们还可以定量地计算数据中所包含的信息并决定保留的比例。

下面看看该如何实现这一点。首先，利用程序清单13-2中的代码将数据集中所有的NaN替换成平均值：

```txt
dataMat = pca.replaceNanWithMean() 
```

接下来从pca()函数中借用一些代码来达到我们的目的，之所以借用是因为我们想了解中间结果而非最后输出结果。首先调用如下语句去除均值：

```txt
meanVals = mean(dataMat, axis=0)  
meanRemoved = dataMat - meanVals 
```

然后计算协方差矩阵：

```txt
covMat = cov(meanRemoved, rowvar=0) 
```

最后对该矩阵进行特征值分析：

eigVals,eigVects $=$ linalg.eig(mat CovMat))

现在，我们可以观察一下特征值的结果：

```txt
>>>eigVals   
array(5.34151979e+07, 2.17466719e+07, 8.24837662e+06, 2.07388086e+06, 1.31540439e+06, 4.67693557e+05, 2.90863555e+05, 2.83668601e+05, 2.37155830e+05, 2.08513836e+05, 1.96098849e+05, 1.86856549e+05, 
```

```json
0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000 e+01] 
```

我们会看到一大堆值，但是其中的什么会引起我们的注意？我们会发现其中很多值都是0吗？实际上，其中有超过 $20\%$ 的特征值都是0。这就意味着这些特征都是其他特征的副本，也就是说，它们可以通过其他特征来表示，而本身并没有提供额外的信息。

接下来，我们了解一下部分数值的数量级。最前面15个值的数量级大于105，实际上那以后的值都变得非常小。这就相当于告诉我们只有部分重要特征，重要特征的数目也很快就会下降。

最后，我们可能会注意到有一些小的负值，它们主要源自数值误差应该四舍五入成0。

在图13-4中已经给出了总方差的百分比，我们发现，在开始几个主成分之后，方差就会迅速下降。

![](images/f50b45feae4efa6ebb0242af1e97d04f5fdf70c7bb137629e4ca96284242d7d1.jpg)  
图13-4 前20个主成分占总方差的百分比。可以看出，大部分方差都包含在前面的几个主成分中，舍弃后面的主成分并不会损失太多的信息。如果保留前6个主成分，则数据集可以从590个特征约简成6个特征，大概实现了 $100:1$ 的压缩

表13-1给出了这些主成分所对应的方差百分比和累积方差百分比。浏览“累积方差百分比 $(\%)$ ”这一列就会注意到，前六个主成分就覆盖了数据 $96.8\%$ 的方差，而前20个主成分覆盖了 $99.3\%$ 的方差。这就表明了，如果保留前6个而去除后584个主成分，我们就可以实现大概 $100:1$ 的压缩比。另外，由于舍弃了噪声的主成分，将后面的主成分去除便使得数据更加干净。

表13-1 半导体数据中前7个主成分所占的方差百分比  

<table><tr><td>主成分</td><td>方差百分比 (%)</td><td>累积方差百分比 (%)</td></tr><tr><td>1</td><td>59.2</td><td>59.2</td></tr><tr><td>2</td><td>24.1</td><td>83.4</td></tr><tr><td>3</td><td>9.2</td><td>92.5</td></tr><tr><td>4</td><td>2.3</td><td>94.8</td></tr><tr><td>5</td><td>1.5</td><td>96.3</td></tr><tr><td>6</td><td>0.5</td><td>96.8</td></tr><tr><td>7</td><td>0.3</td><td>97.1</td></tr><tr><td>20</td><td>0.08</td><td>99.3</td></tr></table>

于是，我们可以知道在数据集的前面多个主成分中所包含的信息量。我们可以尝试不同的截

断值来检验它们的性能。有些人使用能包含 $90\%$ 信息量的主成分数量，而其他人使用前20个主成分。我们无法精确知道所需要的主成分数目，必须通过在实验中取不同的值来确定。有效的主成分数目则取决于数据集和具体应用。

上述分析能够得到所用到的主成分数目，然后我们可以将该数目输入到PCA算法中，最后得到约简后数据就可以在分类器中使用了。

# 13.4 本章小结

降维技术使得数据变得更易使用，并且它们往往能够去除数据中的噪声，使得其他机器学习任务更加精确。降维往往作为预处理步骤，在数据应用到其他算法之前清洗数据。有很多技术可以用于数据降维，在这些技术中，独立成分分析、因子分析和主成分分析比较流行，其中又以主成分分析应用最广泛。

PCA可以从数据中识别其主要特征，它是通过沿着数据最大方差方向旋转坐标轴来实现的。选择方差最大的方向作为第一条坐标轴，后续坐标轴则与前面的坐标轴正交。协方差矩阵上的特征值分析可以用一系列的正交坐标轴来获取。

本章中的PCA将所有的数据集都调入了内存，如果无法做到，就需要其他的方法来寻找其特征值。如果使用在线PCA分析的方法，你可以参考一篇优秀的论文“Incremental Eigenanalysis for Classification”①。下一章要讨论的奇异值分解方法也可以用于特征值分析。

# 第14章

# 利用SVD简化数据

# 本章内容

□SVD矩阵分解  
□推荐引擎  
□利用SVD提升推荐引擎的性能

餐馆可划分为很多类别，比如美式、中式、日式、牛排馆、素食店，等等。你是否想过这些类别够用吗？或许人们喜欢这些的混合类别，或者类似中式素食店那样的子类别。如何才能知道到底有多少类餐馆呢？我们也许可以问问专家？但是倘若某个专家说应该按照调料分类，而另一个专家则认为应该按照配料分类，那该怎么办呢？忘了专家，我们还是从数据着手吧。我们可以对记录用户关于餐馆观点的数据进行处理，并且从中提取出其背后的因素。

这些因素可能会与餐馆的类别、烹饪时所用的某个特定配料，或其他任意对象一致。然后，我们就可以利用这些因素来估计人们对没有去过的餐馆的看法。

提取这些信息的方法称为奇异值分解（Singular Value Decomposition，SVD）。从生物信息学到金融学等在内的很多应用中，SVD都是提取信息的强大工具。

本章将介绍SVD的概念及其能够进行数据约简的原因。然后，我们将会介绍基于Python的SVD实现以及将数据映射到低维空间的过程。再接下来，我们就将学习推荐引擎的概念和它们的实际运行过程。为了提高SVD的精度，我们将会把其应用到推荐系统中去，该推荐系统将会帮助人们寻找到合适的餐馆。最后，我们讲述一个SVD在图像压缩中的应用例子。

# 14.1 SVD的应用

# 奇异值分解

优点：简化数据，去除噪声，提高算法的结果。

缺点：数据的转换可能难以理解。

适用数据类型：数值型数据。

利用SVD实现，我们能够用小得多的数据集来表示原始数据集。这样做，实际上是去除了噪声和冗余信息。当我们试图节省空间时，去除信息就是很崇高的目标了，但是在这里我们则是从数据中抽取信息。基于这个视角，我们就可以把SVD看成是从噪声数据中抽取相关特征。如果这一点听来奇怪，也不必担心，我们后面会给出若干SVD应用的场景和方法，解释它的威力。

首先，我们会介绍SVD是如何通过隐性语义索引应用于搜索和信息检索领域的。然后，我们再介绍SVD在推荐系统中的应用。

# 14.1.1 隐性语义索引

SVD的历史已经超过上百个年头，但是最近几十年随着计算机的使用，我们发现了其更多的使用价值。最早的SVD应用之一就是信息检索。我们称利用SVD的方法为隐性语义索引（Latent Semantic Indexing，LSI）或隐性语义分析（Latent Semantic Analysis，LSA）。

在LSI中，一个矩阵是由文档和词语组成的。当我们在该矩阵上应用SVD时，就会构建出多个奇异值。这些奇异值代表了文档中的概念或主题，这一特点可以用于更高效的文档搜索。在词语拼写错误时，只基于词语存在与否的简单搜索方法会遇到问题。简单搜索的另一个问题就是同义词的使用。这就是说，当我们查找一个词时，其同义词所在的文档可能并不会匹配上。如果我们从上千篇相似的文档中抽取出概念，那么同义词就会映射为同一概念。

# 14.1.2 推荐系统

SVD的另一个应用就是推荐系统。简单版本的推荐系统能够计算项或者人之间的相似度。更先进的方法则先利用SVD从数据中构建一个主题空间，然后再在该空间下计算其相似度。考虑图14-1中给出的矩阵，它是由餐馆的菜和品菜师对这些菜的意见构成的。品菜师可以采用1到5之间的任意一个整数来对菜评级。如果品菜师没有尝过某道菜，则评级为0。

![](images/61178da7f6e758a102feea08cbd7d0b0d2fb6879123a547de063e25efa3768ec.jpg)  
图14-1 餐馆的菜及其评级的数据。对此矩阵进行SVD处理则可以将数据压缩到若干概念中去。在右边的矩阵当中，标出了一个概念

我们对上述矩阵进行SVD处理，会得到两个奇异值（读者如果不信可以自己试试）。因此，就会仿佛有两个概念或主题与此数据集相关联。我们看看能否通过观察图中的0来找到这个矩阵的具体概念。观察一下右图的阴影部分，看起来Ed、Peter和Tracy对“烤牛肉”和“手撕猪肉”进行了评级，同时这三人未对其他菜评级。烤牛肉和手撕猪肉都是美式烧烤餐馆才有的菜，其他菜则在日式餐馆才有。

我们可以把奇异值想象成一个新空间。与图14-1中的矩阵给出的五维或者七维不同，我们最终的矩阵只有二维。那么这二维分别是什么呢？它们能告诉我们数据的什么信息？这二维分别对应图中给出的两个组，右图中已经标示出了其中的一个组。我们可以基于每个组的共同特征来命名这二维，比如我们得到的美式BBQ和日式食品这二维。

如何才能将原始数据变换到上述新空间中呢？下一节我们将会进一步详细地介绍SVD，届时将会了解到SVD是如何得到u和v两个矩阵的。v矩阵会将用户映射到BBQ/日式食品空间去。类似地，u矩阵会将餐馆的菜映射到BBQ/日式食品空间去。真实的数据通常不会像图14-1中的矩阵那样稠密或整齐，这里如此只是为了便于说明问题。

推荐引擎中可能会有噪声数据，比如某个人对某些菜的评级就可能存在噪声，并且推荐系统也可以将数据抽取为这些基本主题。基于这些主题，推荐系统就能取得比原始数据更好的推荐效果。在2006年末，电影公司Netflix曾经举办了一个奖金为100万美元的大赛，这笔奖金会颁给比当时最好系统还要好 $10\%$ 的推荐系统的参赛者。最后的获奖者就使用了 $\mathrm{SVD}^{\text{①}}$ 。

下一节将介绍SVD的一些背景材料，接着给出利用Python的NumPy实现SVD的过程。然后，我们将进一步深入讨论推荐引擎。当对推荐引擎有相当的了解之后，我们就会利用SVD构建一个推荐系统。

SVD是矩阵分解的一种类型，而矩阵分解是将数据矩阵分解为多个独立部分的过程。接下来我们首先介绍矩阵分解。

# 14.2 矩阵分解

在很多情况下，数据中的一小段携带了数据集中的大部分信息，其他信息则要么是噪声，要么就是毫不相关的信息。在线性代数中还有很多矩阵分解技术。矩阵分解可以将原始矩阵表示成新的易于处理的形式，这种新形式是两个或多个矩阵的乘积。我们可以将这种分解过程想象成代数中的因子分解。如何将12分解成两个数的乘积？(1,12)、(2,6)和(3,4)都是合理的答案。

不同的矩阵分解技术具有不同的性质，其中有些更适合于某个应用，有些则更适合于其他应用。最常见的一种矩阵分解技术就是SVD。SVD将原始的数据集矩阵Data分解成三个矩阵U、Σ和V。如果原始矩阵Data是m行n列，那么U、Σ和V就分别是m行m列、m行n列和n行n列。为了清晰起见，上述过程可以写成如下一行（下标为矩阵维数）：

$$
D a t a _ {m \times n} = U _ {m \times n} \sum_ {m \times n} V ^ {\mathrm {T}} _ {n \times n}
$$

上述分解中会构建出一个矩阵 $\pmb{\Sigma}$ ，该矩阵只有对角元素，其他元素均为0。另一个惯例就是， $\pmb{\Sigma}$ 的对角元素是从大到小排列的。这些对角元素称为奇异值（Singular Value），它们对应了原始数据集矩阵 Data 的奇异值。回想上一章的 PCA，我们得到的是矩阵的特征值，它们告诉我们数据集中的重要特征。 $\pmb{\Sigma}$ 中的奇异值也是如此。奇异值和特征值是有关系的。这里的奇异值就是矩阵 Data * Data^T 特征值的平方根。

前面提到过，矩阵 $\pmb{\Sigma}$ 只有从大到小排列的对角元素。在科学和工程中，一直存在这样一个普遍事实：在某个奇异值的数目（ $r$ 个）之后，其他的奇异值都置为0。这就意味着数据集中仅有 $r$ 个重要特征，而其余特征则都是噪声或冗余特征。在下一节中，我们将看到一个可靠的案例。

我们不必担心该如何进行矩阵分解。在下一节中就会提到，在NumPy线性代数库中有一个实现SVD的方法。如果读者对SVD的编程实现感兴趣的话，请阅读Numerical Linear Algebra<sup>①</sup>。

# 14.3 利用Python实现SVD

如果SVD确实那么好，那么该如何实现它呢？SVD实现了相关的线性代数，但这并不在本书的讨论范围之内。其实，有很多软件包可以实现SVD。NumPy有一个称为linalg的线性代数工具箱。接下来，我们了解一下如何利用该工具箱实现如下矩阵的SVD处理：

$$
\left[ \begin{array}{c c} 1 & 1 \\ 1 & 7 \end{array} \right]
$$

要在Python上实现该矩阵的SVD处理，请键入如下命令：

```python
>>> from numpy import \*   
>>>U,Sigma,VT=linalg.svd([[1,1],[7，7]]) 
```

接下来就可以在如下多个矩阵上进行尝试：

```txt
>>>U   
array([-0.14142136，-0.98994949]， [-0.98994949， 0.14142136]）   
>>>Sigma   
array([10.， 0.])   
>>>VT   
array([-0.70710678，-0.70710678]， [-0.70710678，0.70710678]]）
```

我们注意到，矩阵Sigma以行向量array([10., 0.])返回，而非如下矩阵：

```txt
array([10., 0.], [0., 0.])
```

由于矩阵除了对角元素其他均为0，因此这种仅返回对角元素的方式能够节省空间，这就是由NumPy的内部机制产生的。我们所要记住的是，一旦看到Sigma就要知道它是一个矩阵。好了，接下来我们将在一个更大的数据集上进行更多的分解。

建立一个新文件svdRec.py并加入如下代码：

```python
def loadExData(): return[[1, 1, 1, 0, 0], [2, 2, 2, 0, 0], [1, 1, 1, 0, 0], [5, 5, 5, 0, 0], [1, 1, 0, 2, 2], [0, 0, 0, 3, 3], [0, 0, 0, 1, 1]] 
```

接下来我们对该矩阵进行SVD分解。在保存好文件svdRec.py之后，我们在Python提示符下输入：

```txt
>>> import svdRec
>>> Data = svdRec.loadExData()
>>> U, Sigma, VT = linalg.svd(Data)
>>> Sigma
array(
[9.72140007e+00, 5.29397912e+00, 6.84226362e-01,
7.16251492e-16, 4.85169600e-32]) 
```

前3个数值比其他的值大了很多（如果你的最后两个值的结果与这里的结果稍有不同，也不必担心。它们太小了，所以在不同机器上产生的结果就可能会稍有不同，但是数量级应该和这里的结果差不多）。于是，我们就可以将最后两个值去掉了。

接下来，我们的原始数据集就可以用如下结果来近似：

$$
D a t a _ {m \times n} \approx U _ {m \times 3} \sum_ {3 \times 3} V ^ {\mathrm {T}} _ {3 \times n}
$$

图14-2就是上述近似计算的一个示意图。

![](images/d704fbae430d93190c50ac52a9314b7db351a0788a8d12f43c77c4b6b3dfe7cb.jpg)  
图14-2 SVD的示意图。矩阵Data被分解。浅灰色区域是原始数据，深灰色区域是矩阵近似计算仅需要的数据

我们试图重构原始矩阵。首先构建一个 $3 \times 3$ 的矩阵sig3：

```txt
>>> Sig3=mat([[Sigma[0], 0, 0], [0, Sigma[1], 0], [0, 0, Sigma[2]]]) 
```

接下来我们重构一个原始矩阵的近似矩阵。由于Sig2仅为 $2 \times 2$ 的矩阵，因而我们只需使用矩阵 $\mathbf{u}$ 的前两列和 $\mathbf{v}^{\pi}$ 的前两行。为了在Python中实现这一点，输入如下命令：

```txt
>>U[:：3]\*Sig3\*VT[:3，:] array([[1.，1.，1.，0.，0.], [2.，2.，2.，-0.，-0.], [1.，1.，1.，-0.，-0.], [5.，5.，5.，0.，0.], [1.，1.，-0.，2.，2.], [0.，0.，-0.，3.，3.], [0.，0.，-0.，1.，1.]]）
```

我们是如何知道仅需保留前3个奇异值的呢？确定要保留的奇异值的数目有很多启发式的策略，其中一个典型的做法就是保留矩阵中 $90\%$ 的能量信息。为了计算总能量信息，我们将所有的奇异值求其平方和。于是可以将奇异值的平方和累加到总值的 $90\%$ 为止。另一个启发式策略就是，当矩阵上有上万的奇异值时，那么就保留前面的2000或3000个。尽管后一种方法不太优雅，但是在实际中更容易实施。之所以说它不够优雅，就是在任何数据集上都不能保证前3000个奇异值就能够包含 $90\%$ 的能量信息。但在通常情况下，使用者往往都对数据有足够的了解，从而就能够做出类似的假设了。

现在我们已经通过三个矩阵对原始矩阵进行了近似。我们可以用一个小很多的矩阵来表示一个大矩阵。有很多应用可以通过SVD来提升性能。下面我们将讨论一个比较流行的SVD应用的例子——推荐引擎。

# 14.4 基于协同过滤的推荐引擎

近十年来，推荐引擎对因特网用户而言已经不是什么新鲜事物了。Amazon会根据顾客的购买历史向他们推荐物品，Netflix会向其用户推荐电影，新闻网站会对用户推荐新闻报道，这样的例子还有很多很多。当然，有很多方法可以实现推荐功能，这里我们只使用一种称为协同过滤（collaborative filtering）的方法。协同过滤是通过将用户和其他用户的数据进行对比来实现推荐的。

这里的数据是从概念上组织成了类似图14-2所给出的矩阵形式。当数据采用这种方式进行组织时，我们就可以比较用户或物品之间的相似度了。这两种做法都会使用我们很快就介绍到的相似度的概念。当知道了两个用户或两个物品之间的相似度，我们就可以利用已有的数据来预测未知的用户喜好。例如，我们试图对某个用户喜欢的电影进行预测，推荐引擎会发现有一部电影该用户还没看过。然后，它就会计算该电影和用户看过的电影之间的相似度，如果其相似度很高，推荐算法就会认为用户喜欢这部电影。

在上述场景下，唯一所需要的数学方法就是相似度的计算，这并不是很难。接下来，我们首先讨论物品之间的相似度计算，然后讨论在基于物品和基于用户的相似度计算之间的折中。最后，我们介绍推荐引擎成功的度量方法。

# 14.4.1 相似度计算

我们希望拥有一些物品之间相似度的定量方法。那么如何找出这些方法呢？倘若我们面对的是食品销售网站，该如何处理？或许可以根据食品的配料、热量、某个烹调类型的定义或者其他

类似的信息进行相似度的计算。现在，假设该网站想把业务拓展到餐具行业，那么会用热量来描述一个叉子吗？问题的关键就在于用于描述食品的属性和描述餐具的属性有所不同。倘若我们使用另外一种比较物品的方法会怎样呢？我们不利用专家所给出的重要属性来描述物品从而计算它们之间的相似度，而是利用用户对它们的意见来计算相似度。这就是协同过滤中所使用的方法。它并不关心物品的描述属性，而是严格地按照许多用户的观点来计算相似度。图14-3给出了由一些用户及其对前面给出的部分菜肴的评级信息所组成的矩阵。

![](images/2b58b63c7f31c36e6e552130db7db865f633e31ab4de85998e96a976521d144b.jpg)  
图14-3 用于展示相似度计算的简单矩阵

我们计算一下手撕猪肉和烤牛肉之间的相似度。一开始我们使用欧氏距离来计算。手撕猪肉和烤牛肉的欧氏距离为：

$$
\sqrt {(4 - 4) ^ {2} + (3 - 3) ^ {2} + (2 - 1) ^ {2}} = 1
$$

而手撕猪肉和鳗鱼饭的欧氏距离为：

$$
\sqrt {(4 - 2) ^ {2} + (3 - 5) ^ {2} + (2 - 2) ^ {2}} = 2. 8 3
$$

在该数据中，由于手撕猪肉和烤牛肉的距离小于手撕猪肉和鳗鱼饭的距离，因此手撕猪肉与烤牛肉比与鳗鱼饭更为相似。我们希望，相似度值在0到1之间变化，并且物品对越相似，它们的相似度值也就越大。我们可以用“相似度 $= 1 / (1 + \text{距离})$ ”这样的算式来计算相似度。当距离为0时，相似度为1.0。如果距离真的非常大时，相似度也就趋近于0。

第二种计算距离的方法是皮尔逊相关系数（Pearson correlation）。我们在第8章度量回归方程的精度时曾经用到过这个量，它度量的是两个向量之间的相似度。该方法相对于欧氏距离的一个优势在于，它对用户评级的量级并不敏感。比如某个狂躁者对所有物品的评分都是5分，而另一个忧郁者对所有物品的评分都是1分，皮尔逊相关系数会认为这两个向量是相等的。在NumPy中，皮尔逊相关系数的计算是由函数corrcoef()进行的，后面我们很快就会用到它了。皮尔逊相关系数的取值范围从-1到+1，我们通过 $0.5 + 0.5*$ corrcoef()这个函数计算，并且把其取值范围归一化到0到1之间。

另一个常用的距离计算方法就是余弦相似度（cosinesimilarity），其计算的是两个向量夹角的余弦值。如果夹角为90度，则相似度为0；如果两个向量的方向相同，则相似度为1.0。同皮尔逊

相关系数一样，余弦相似度的取值范围也在-1到+1之间，因此我们也将它归一化到0到1之间。计算余弦相似度值，我们采用的两个向量 $\pmb{A}$ 和 $\pmb{B}$ 夹角的余弦相似度的定义如下：

$$
\cos \theta = \frac {A \cdot B}{\| A \| \| B \|}
$$

其中， $\|A\|$ 、 $\|B\|$ 表示向量 $A$ 、 $B$ 的2范数，你可以定义向量的任一范数，但是如果不指定范数阶数，则都假设为2范数。向量[4,2,2]的2范数为：

$$
\sqrt {4 ^ {2} + 3 ^ {2} + 2 ^ {2}}
$$

同样，NumPy的线性代数工具箱中提供了范数的计算方法linalg(norm()。

接下来我们将上述各种相似度的计算方法写成Python中的函数。打开svdRec.py文件并加入下列代码。

# 程序清单14-1 相似度计算

from numpy import \*   
from numpy import linalg as la   
def ecludSim(inA,inB): return 1.0/(1.0 + 1a(norm(inA - inB))   
def pearsim(inA,inB): if len(inA) $<  3$ :return 1.0 return $0.5 + 0.5^{*}$ corrcoef(inA，inB，rowvar $= 0$ )[0][1]   
def cosSim(inA,inB): num $=$ float(inA.T\*inB) denom $=$ la(norm(inA)\*la(norm(inB) return $0.5 + 0.5^{*}$ (num/denom)

程序中的3个函数就是上面提到的几种相似度的计算方法。为了便于理解，NumPy的线性代数工具箱linalg被作为1a导入，函数中假定inA和inB都是列向量。perasSim()函数会检查是否存在3个或更多的点。如果不存在，该函数返回1.0，这是因为此时两个向量完全相关。

下面我们对上述函数进行尝试。在保存好文件svdRec.py之后，在Python提示符下输入如下命令：

```python
>>> reload(svdRec)
<module 'svdRec' from 'svdRec.pyc'>
>>> myMat = mat(svdRec.loadExData())
>>> svdRec.ecludeSim(myMat[:,0], myMat[:,4])
0.12973190755680383
>>> svdRec.ecludeSim(myMat[:,0], myMat[:,0])
1.0 
```

欧氏距离看上去还行，那么接下来试试余弦相似度：

```csv
>>svdRec.cosSim(myMat[：,0]，myMat[：,4])  
0.5  
>>svdRec.cosSim(myMat[：,0]，myMat[：,0])  
1.000000000000002 
```

余弦相似度似乎也行，就再试试皮尔逊相关系数：

```txt
>>>svdRec.pearsSim(myMat[;,0],myMat[;,4]) 0.20596538173840329>>>svdRec.pearsSim(myMat[;,0],myMat[;,0]) 1.0 
```

上面的相似度计算都是假设数据采用了列向量方式进行表示。如果利用上述函数来计算两个行向量的相似度就会遇到问题（我们很容易对上述函数进行修改以计算行向量之间的相似度）。这里采用列向量的表示方法，暗示着我们将利用基于物品的相似度计算方法。后面我们会阐述其中的原因。

# 14.4.2 基于物品的相似度还是基于用户的相似度？

我们计算了两个餐馆菜肴之间的距离，这称为基于物品（item-based）的相似度。另一种计算用户距离的方法则称为基于用户（user-based）的相似度。回到图14-3，行与行之间比较的是基于用户的相似度，列与列之间比较的则是基于物品的相似度。到底使用哪一种相似度呢？这取决于用户或物品的数目。基于物品相似度计算的时间会随物品数量的增加而增加，基于用户的相似度计算的时间则会随用户数量的增加而增加。如果我们有一个商店，那么最多会有几千件商品。在撰写本书之际，最大的商店大概有100000件商品。而在Netflix大赛中，则会有480000个用户和17700部电影。如果用户的数目很多，那么我们可能倾向于使用基于物品相似度的计算方法。

对于大部分产品导向的推荐引擎而言，用户的数量往往大于物品的数量，即购买商品的用户数会多于出售的商品种类。

# 14.4.3 推荐引擎的评价

如何对推荐引擎进行评价呢？此时，我们既没有预测的目标值，也没有用户来调查他们对预测的满意程度。这里我们就可以采用前面多次使用的交叉测试的方法。具体的做法就是，我们将某些已知的评分值去掉，然后对它们进行预测，最后计算预测值和真实值之间的差异。

通常用于推荐引擎评价的指标是称为最小均方根误差（Root Mean Squared Error，RMSE）的指标，它首先计算均方误差的平均值然后取其平方根。如果评级在1星到5星这个范围内，而我们得到的RMSE为1.0，那么就意味着我们的预测值和用户给出的真实评价相差了一个星级。

# 14.5 示例：餐馆菜肴推荐引擎

现在我们就开始构建一个推荐引擎，该推荐引擎关注的是餐馆食物的推荐。假设一个人在家决定外出吃饭，但是他并不知道该到哪儿去吃饭，该点什么菜。我们这个推荐系统可以帮他做到这两点。

首先我们构建一个基本的推荐引擎，它能够寻找用户没有尝过的菜肴。然后，通过SVD来减少特征空间并提高推荐的效果。这之后，将程序打包并通过用户可读的人机界面提供给人们使用。最后，我们介绍在构建推荐系统时面临的一些问题。

# 14.5.1 推荐未尝过的菜肴

推荐系统的工作过程是：给定一个用户，系统会为此用户返回N个最好的推荐菜。为了实现这一点，则需要我们做到：

(1)寻找用户没有评级的菜肴，即在用户-物品矩阵中的0值；  
(2) 在用户没有评级的所有物品中，对每个物品预计一个可能的评级分数。这就是说，我们认为用户可能会对物品的打分（这就是相似度计算的初衷）；  
(3) 对这些物品的评分从高到低进行排序，返回前 $N$ 个物品。

好了，接下来我们尝试这样做。打开svdRec.py文件并加入下列程序清单中的代码。

# 程序清单14-2 基于物品相似度的推荐引擎

defstandEst(dataMat，user,simMeas，item)： n $=$ shape(dataMat)[1] simTotal $= 0.0$ ；ratSimTotal $= 0.0$ forj in range(n): userRating $\equiv$ dataMat[user,j] ifuserRating $\equiv = 0$ ：continue overLap $\equiv$ nonzero(logical_and(dataMat[:,item].A>0，dataMat[:,j].A>0)) [0] iflen(overLap) $\equiv = 0$ ：similarity $= 0$ else:similarity $\equiv$ simMeas(dataMat[overLap,item]，dataMat[overLap,j]) #print'the%d and%d similarity is:%f' $\%$ (item，j,similarity) simTotal $+ =$ similarity ratSimTotal $+ =$ similarity\*userRating ifsimTotal $\equiv = 0$ ：return0 else: return ratSimTotal/simTotal   
defrecommend(dataMat，user，N=3，simMeas=cosSim，estMethod $\equiv$ standEst）： unratedItems $\equiv$ nonzero(dataMat[user,:].A $\equiv = 0$ )[1] iflen(unratedItems) $\equiv = 0$ ：return 'you rated everything' 寻找未评级的物品 itemScores $= []$ for item in unratedItems: estimatedScore $\equiv$ estMethod(dataMat，user,simMeas，item) itemScores.append((item,estimatedScore)) return sorted(itemScores，key $\equiv$ lambdajj:jj[1]，reverse $\equiv$ True)[:N]

上述程序包含了两个函数。第一个函数是standEst()，用来计算在给定相似度计算方法的条件下，用户对物品的估计评分值。第二个函数是recommend()，也就是推荐引擎，它会调用standEst()函数。我们先讨论standEst()函数，然后讨论recommend()函数。

函数standEst()的参数包括数据矩阵、用户编号、物品编号和相似度计算方法。假设这里的数据矩阵为图14-1和图14-2的形式，即行对应用户、列对应物品。那么，我们首先会得到数据集中的物品数目，然后对两个后面用于计算估计评分值的变量进行初始化。接着，我们遍历行中的每个物品。如果某个物品评分值为0，就意味着用户没有对该物品评分，跳过了这个物品。该循环大体上是对用户评过分的每个物品进行遍历，并将它和其他物品进行比较。变量overLap

给出的是两个物品当中已经被评分的那个元素①。如果两者没有任何重合元素，则相似度为0且中止本次循环。但是如果存在重合的物品，则基于这些重合物品计算相似度。随后，相似度会不断累加，每次计算时还考虑相似度和当前用户评分的乘积。最后，通过除以所有的评分总和，对上述相似度评分的乘积进行归一化。这就可以使得最后的评分值在0到5之间，而这些评分值则用于对预测值进行排序。

函数recommend()产生了最高的N个推荐结果。如果不指定N的大小，则默认值为3。该函数另外的参数还包括相似度计算方法和估计方法。我们可以使用程序清单14-1中的任意一种相似度计算方法。此时我们能采用的估计方法只有一种选择，但是在下一小节中会增加另外一种选择。该函数的第一件事就是对给定的用户建立一个未评分的物品列表②。如果不存在未评分物品，那么就退出函数；否则，在所有的未评分物品上进行循环。对每个未评分物品，则通过调用standEst()来产生该物品的预测得分。该物品的编号和估计得分值会放在一个元素列表itemScores中。最后按照估计得分，对该列表进行排序并返回③。该列表是从大到小逆序排列的，因此其第一个值就是最大值。

接下来看看它的实际运行效果。在保存svdRec.py文件之后，在Python提示符下输入命令：

```txt
>>> reload(svdRec)
<module 'svdRec' from 'svdRec.py'> 
```

下面，我们调入了一个矩阵实例，可以对本章前面给出的矩阵稍加修改后加以使用。首先，调入原始矩阵：

```txt
>>> myMat = mat(svdRec.loadExData()) 
```

该矩阵对于展示SVD的作用非常好，但是它本身不是十分有趣，因此我们要对其中的一些值进行更改：

```python
>>> myMat [0, 1] = myMat [0, 0] = myMat [1, 0] = myMat [2, 0] = 4  
>>> myMat [3, 3] = 2 
```

现在得到的矩阵如下：

```txt
>>>myMat   
matrix([[4，4，0，2，2]， [4，0，0，3，3]， [4，0，0，1，1]， [1，1，1，2，0]， [2，2，2，0，0]， [1，1，1，0，0]， [5，5，5，0，0]]）
```

好了，现在我们已经可以做些推荐了。我们先尝试一下默认的推荐：

```txt
>>>svdRec.recommend(myMat，2)[(2，2.500000000000004)，(1，2.0498713655614456)]
```

这表明了用户2（由于我们从0开始计数，因此这对应了矩阵的第3行）对物品2的预测评分值为2.5，对物品1的预测评分值为2.05。下面我们就利用其他的相似度计算方法来进行推荐：

>>>svdRec.recommend(myMat，2，simMeas $\equiv$ svdRec.ecludSim) [(2，3.0)，（1，2.8266504712098603)]   
>>>svdRec.recommend(myMat，2，simMeas $\equiv$ svdRec.pearsSim) [(2，2.5)，(1，2.0)]

我们可以对多个用户进行尝试，或者对数据集做些修改来了解其给预测结果带来的变化。

这个例子给出了如何利用基于物品相似度和多个相似度计算方法来进行推荐的过程，下面我们介绍如何将SVD应用于推荐。

# 14.5.2 利用SVD提高推荐的效果

实际的数据集会比我们用于展示recommend()函数功能的myMat矩阵稀疏得多。图14-4就给出了一个更真实的矩阵的例子。

![](images/429bff5c102d674cb9ac50f24fbc03ab186462548ddd13e6e24595929f0ba230.jpg)  
图14-4 一个更大的用户－菜肴矩阵，其中有很多物品都没有评分，这比一个全填充的矩阵更接近真实情况

我们可以将该矩阵输入到程序中去，或者从下载代码中复制函数loadExData2()。下面我们计算该矩阵的SVD来了解其到底需要多少维特征。

```python
>>> from numpy import linalg as la
>>> U, Sigma, VT = la.svd(mat(svdRec.loadExData2))
>>> Sigma
array(
1.38487021e+01, 1.15944583e+01, 1.10219767e+01,
5.31737732e+00, 4.55477815e+00, 2.69935136e+00,
1.53799905e+00, 6.46087828e-01, 4.45444850e-01,
9.86019201e-02, 9.96558169e-17]) 
```

接下来我们看看到底有多少个奇异值能达到总能量的 $90\%$ 。首先，对Sigma中的值求平方：

>>> Sig2 $\equiv$ Sigma\*\*2再计算一下总能量：

```txt
>>sum(Sig2) 541.99999999999932 
```

再计算总能量的 $90\%$

```txt
>>sum(Sig2)\*0.9 487.7999999999939 
```

然后，计算前两个元素所包含的能量：

```txt
>>sum(Sig2[:2]) 378.8295595113579 
```

该值低于总能量的 $90\%$ ，于是计算前三个元素所包含的能量：

```txt
>>sum(Sig2[:3]) 500.50028912757909 
```

该值高于总能量的 $90\%$ ，这就可以了。于是，我们可以将一个11维的矩阵转换成一个3维的矩阵。下面对转换后的三维空间构造出一个相似度计算函数。我们利用SVD将所有的菜肴映射到一个低维空间中去。在低维空间下，可以利用前面相同的相似度计算方法来进行推荐。我们会构造出一个类似于程序清单14-2中的standEst()函数。打开svdRec.py文件并加入如下程序清单中的代码。

# 程序清单14-3 基于SVD的评分估计

defsvdEst(dataMat，user,simMeas，item)： n $=$ shape(dataMat)[1] simTotal $= 0.0$ ；ratSimTotal $= 0.0$ U,Sigma,VT $=$ la.svd(dataMat) Sig4 $\equiv$ mat(eye(4)*Sigma[:4]) xformedItems $\equiv$ dataMat.T \*U[::,4] \*Sig4.I forj in range(n): userRating $\equiv$ dataMat 用户,j] ifuserRating $\equiv = 0$ or $\mathbf{j} = =$ item:continue similarity $\equiv$ simMeas(xformedItems[item,:].T,\ xformedItems[j,:].T) print'the%d and%d similarityis:%f' $\%$ (item，j,similarity) simTotal $+ =$ similarity ratSimTotal $+ =$ similarity \*userRating   
ifsimTotal $\equiv = 0$ ：return0 else: return ratSimTotal/simTotal

上述程序中包含有一个函数svdEst()。在recommend()中，这个函数用于替换对stand-Est()的调用，该函数对给定用户给定物品构建了一个评分估计值。如果将该函数与程序清单14-2中的standEst()函数进行比较，就会发现很多行代码都很相似。该函数的不同之处就在于它在第3行对数据集进行了SVD分解。在SVD分解之后，我们只利用包含了 $90\%$ 能量值的奇异值，这些奇异值会以NumPy数组的形式得以保存。因此如果要进行矩阵运算，那么就必须要用这些奇异值构建出一个对角矩阵①。然后，利用u矩阵将物品转换到低维空间中②。

对于给定的用户，for循环在用户对应行的所有元素上进行遍历。这和standEst()函数中的for循环的目的一样，只不过这里的相似度计算是在低维空间下进行的。相似度的计算方法也会作为一个参数传递给该函数。然后，我们对相似度求和，同时对相似度及对应评分值的乘积求

和。这些值返回之后则用于估计评分的计算。for循环中加入了一条print语句，以便能够了解相似度计算的进展情况。如果觉得这些输出很累赘，也可以将该语句注释掉。

接下来看看程序的执行效果。将程序清单14-3中的代码输入到文件svdRec.py中并保存之后，在Python提示符下运行如下命令：

```txt
>>> reload(svdRec)
<module 'svdRec' from 'svdRec.pyc>
>>> svdRec.recommend(myMat, 1, estMethod=svdRec.svdEst)
The 0 and 3 similarity is 0.362287.
...
The 9 and 10 similarity is 0.497753.
[(6, 3.387858021353602), (8, 3.3611246496054976), (7, 3.3587350221130028)] 
```

下面再尝试另外一种相似度计算方法：

>>>svdRec.recommend(myMat，1，estMethod $\equiv$ svdRec.svdEst,   
simMeas $\equiv$ svdRec.pearsSim)   
The0and3similarityis0.116304. ： 1   
The9and10similarityis0.566796. [(6，3.3772856083690845)，（9，3.3701740601550196），(4，3.3675118739831169)]

我们还可以再用其他多种相似度计算方法尝试一下。感兴趣的读者可以将这里的结果和前面的方法（不做SVD分解）进行比较，看看到底哪个性能更好。

# 14.5.3 构建推荐引擎面临的挑战

本节的代码很好地展示出了推荐引擎的工作流程以及SVD将数据映射为重要特征的过程。在撰写这些代码时，我尽量保证它们的可读性，但是并不保证代码的执行效率。一个原因是，我们不必在每次估计评分时都做SVD分解。对于上述数据集，是否包含SVD分解在效率上没有太大的区别。但是在更大规模的数据集上，SVD分解会降低程序的速度。SVD分解可以在程序调入时运行一次。在大型系统中，SVD每天运行一次或者其运行频率并不高，并且还要离线运行。

推荐引擎中还存在其他很多规模扩展性的挑战性问题，比如矩阵的表示方法。在上面给出的例子中有很多0，实际系统中0的数目更多。也许，我们可以通过只存储非零元素来节省内存和计算开销？另一个潜在的计算资源浪费则来自于相似度得分。在我们的程序中，每次需要一个推荐得分时，都要计算多个物品的相似度得分，这些得分记录的是物品之间的相似度。因此在需要时，这些记录可以被另一个用户重复使用。在实际中，另一个普遍的做法就是离线计算并保存相似度得分。

推荐引擎面临的另一个问题就是如何在缺乏数据时给出好的推荐。这称为冷启动（cold-start）问题，处理起来十分困难。这个问题的另一个说法是，用户不会喜欢一个无效的物品，而用户不

喜欢的物品又无效。如果推荐只是一个可有可无的功能，那么上述问题倒也不大。但是如果应用的成功与否和推荐的成功与否密切相关，那么问题就变得相当严重了。

冷启动问题的解决方案，就是将推荐看成是搜索问题。在内部表现上，不同的解决办法虽然有所不同，但是对用户而言却都是透明的。为了将推荐看成是搜索问题，我们可能要使用所需要推荐物品的属性。在餐馆菜肴的例子中，我们可以通过各种标签来标记菜肴，比如素食、美式BBQ、价格很贵等。同时，我们也可以将这些属性作为相似度计算所需要的数据，这被称为基于内容（content-based）的推荐。可能，基于内容的推荐并不如我们前面介绍的基于协同过滤的推荐效果好，但我们拥有它，这就是个良好的开始。

# 14.6 基于SVD的图像压缩

在本节中，我们将会了解一个很好的关于如何将SVD应用于图像压缩的例子。通过可视化的方式，该例子使得我们很容易就能看到SVD对数据近似的效果。在代码库中，我们包含了一张手写的数字图像，该图像在第2章使用过。原始的图像大小是 $32 \times 32 = 1024$ 像素，我们能否使用更少的像素来表示这张图呢？如果能对图像进行压缩，那么就可以节省空间或带宽开销了。

我们可以使用SVD来对数据降维，从而实现图像的压缩。下面我们就会看到利用SVD的手写数字图像的压缩过程了。在下面的程序清单中包含了数字的读入和压缩的代码。要了解最后的压缩效果，我们对压缩后的图像进行了重构。打开svdRec.py文件并加入如下代码。

# 程序清单14-4 图像压缩函数

```python
def printMat(inMat, thresh=0.8):
    for i in range(32):
        for k in range(32):
            if float(inMat[i,k]) > thresh:
                print 1,
            else: print 0,
            print ""
def imgCompress(numSV=3, thresh=0.8):
    myl = []
    for line in open('0_5.txt').readlines():
        newRow = []
        for i in range(32):
            newRow.append(int(line[i]))
        myl.append(newRow)
    myMat = mat(myl)
    print "******original matrix*****
printMat(myMat, thresh)
U, Sigma, VT = la.svd(myMat) 
```

SigRecon $=$ mat(zeros((numSV，numSV))）   
fork in range(numSV): SigRecon[k,k] $\equiv$ Sigma[k]   
reconMat $\equiv$ U[:::numSV]*SigRecon*VT[:numSV,:]   
print"******reconstructed matrix using %d singular values*****% numSV   
printMat(reconMat，thresh)

上述程序中第一个函数printMat()的作用是打印矩阵。由于矩阵包含了浮点数，因此必须定义浅色和深色。这里通过一个阈值来界定，后面也可以调节该值。该函数遍历所有的矩阵元素，当元素大于阈值时打印1，否则打印0。

下一个函数实现了图像的压缩。它允许基于任意给定的奇异值数目来重构图像。该函数构建了一个列表，然后打开文本文件，并从文件中以数值方式读入字符。在矩阵调入之后，我们就可以在屏幕上输出该矩阵了。接下来就开始对原始图像进行SVD分解并重构图像。在程序中，通过将Sigma重新构成SigRecon来实现这一点。Sigma是一个对角矩阵，因此需要建立一个全0矩阵，然后将前面的那些奇异值填充到对角线上。最后，通过截断的u和v矩阵，用SigRecon得到重构后的矩阵，该矩阵通过printMat()函数输出。

下面看看该函数的运行效果：

```csv
>>> reload(svdRec)
<module 'svdRec' from 'svdRec.py>
>>> svdRec.imgCompress(2)
*****
>>> original matrix*****
0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 1 1 1 1 1 1 1 1
0 0 0 0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 
```

```csv
0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0  
0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0  
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0  
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0  
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0  
(32,32) 
```

```csv
\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 0 0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 
```

可以看到，只需要两个奇异值就能相当精确地对图像实现重构。那么，我们到底需要多少个0-1的数字来重构图像呢？ $\mathbf{v}$ 和 $\mathbf{v}^{\mathrm{T}}$ 都是 $32 \times 2$ 的矩阵，有两个奇异值。因此总数字数目是 $64 + 64 + 2 = 130$ 。和原数目1024相比，我们获得了几乎10倍的压缩比。

# 14.7 本章小结

SVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留矩阵 $80\% \sim 90\%$ 的能量，就可以得到重要的特征并去掉噪声。SVD已经运用到了多个应用中，其中一个成功的应用案例就是推荐引擎。

推荐引擎将物品推荐给用户，协同过滤则是一种基于用户喜好或行为数据的推荐的实现方

法。协同过滤的核心是相似度计算方法；有很多相似度计算方法都可以用于计算物品或用户之间的相似度。通过在低维空间下计算相似度，SVD提高了推荐系引擎的效果。

在大规模数据集上，SVD的计算和推荐可能是一个很困难的工程问题。通过离线方式来进行SVD分解和相似度计算，是一种减少冗余计算和推荐所需时间的办法。在下一章中，我们将介绍在大数据集上进行机器学习的一些工具。

# 第15章

# 大数据与MapReduce

# 本章内容

MapReduce   
Python中Hadoop流的使用  
□ 使用mrjob库将MapReduce自动化  
□利用Pegasos算法并行训练支持向量机

常听人说：“兄弟，你举的例子是不错，但我的数据太大了！”毫无疑问，工作中所使用的数据集将会比本书的例子大很多。随着大量设备连上互联网加上用户也对基于数据的决策很感兴趣，所收集到的数据已经远远超出了我们的处理能力。幸运的是，一些开源的软件项目提供了海量数据处理的解决方案，其中一个项目就是Hadoop，它采用Java语言编写，支持在大量机器上分布式处理数据。

假想你为一家网络购物商店工作，有很多用户来访问网站，其中有一些人会购买商品，有一些人则在随意浏览后离开了网站。对于你来说，可能很想识别那些有购物意愿的用户。如何实现这一点？可以浏览Web服务器日志找出每个人所访问的网页。日志中或许还会记录其他行为，如果这样，就可以基于这些行为来训练分类器。唯一的问题在于数据集可能会非常大，在单机上训练算法可能要运行好几天。本章就将介绍一些实用的工具来解决这样的问题，包括Hadoop以及一些基于Hadoop的Python工具包。

Hadoop是MapReduce框架的一个免费开源实现，本章首先简单介绍MapReduce和Hadoop项目，然后学习如何使用Python编写MapReduce作业①。这些作业先在单机上进行测试，之后将使用亚马逊的Web服务在大量机器上并行执行。一旦能够熟练运行MapReduce作业，本章我们就可以讨论基于MapReduce处理机器学习算法任务的一般解决方案。在本章中还将看到一个可以在Python中自动执行MapReduce作业的mrjob框架。最后，介绍如何用mrjob构建分布式SVM，在大量的机器上并行训练分类器。

# 15.1 MapReduce：分布式计算的框架

# MapReduce

优点：可在短时间内完成大量工作。

缺点：算法必须经过重写，需要对系统工程有一定的理解。

适用数据类型：数值型和标称型数据。

MapReduce是一个软件框架，可以将单个计算作业分配给多台计算机执行。它假定这些作业在单机上需要很长的运行时间，因此使用多台机器缩短运行时间。常见的例子是日常统计数字的汇总，该任务单机上执行时间将超过一整天。

尽管有人声称他们已经独立开发过类似的框架，美国还是把MapReduce的专利颁发给了Google。Google公司的Jeffrey Dean和Sanjay Ghemawat在2004年的一篇论文中第一次提出了这个思想，该论文的题目是“MapReduce：Simplified Data Processing on Large Clusters”①MapReduce的名字由函数式编程中常用的map和reduce两个单词组成。

MapReduce在大量节点组成的集群上运行。它的工作流程是：单个作业被分成很多小份，输入数据也被切片分发到每个节点，各个节点只在本地数据上做运算，对应的运算代码称为mapper，这个过程被称作map阶段。每个mapper的输出通过某种方式组合（一般还会做排序）。排序后的结果再被分成小份分发到各个节点进行下一步处理工作。第二步的处理阶段被称为reduce阶段，对应的运行代码被称为reducer。reducer的输出就是程序的最终执行结果。

MapReduce的优势在于，它使得程序以并行方式执行。如果集群由10个节点组成，而原先的作业需要10个小时来完成，那么应用MapReduce，该作业将在一个多小时之后得到同样的结果。举个例子，给出过去100年内中国每个省每天的正确气温数据，我们想知道近100年中国国内的最高气温。这里的数据格式为：<province><date><temp>。为了统计该时段内的最高温度，可以先将这些数据根据节点数分成很多份，每个节点各自寻找本机数据集上的最高温度。这样每个mapper将产生一个温度，形如“max”><temp>，也就是所有的mapper都会产生相同的key：“max”字符串。最后只需要一个reducer来比较所有mapper的输出，就能得到全局的最高温度值。

注意 在任何时候，每个mapper或reducer之间都不进行通信③。每个节点只处理自己的事务，且在本地分配的数据集上运算。

不同类型的作业可能需要不同数目的reducer。再回到温度统计的例子，虽然这次使用的数据集相同，但不同的是这里要找出每年的最高温度。这样的话，mapper应先找到每年的最大温度并输出，所以中间数据的格式将形如<year><temp>。此外，还需要保证所有同一年的数据传递给同一个reducer，这由map和reduce阶段中间的sort阶段来完成。该例中也给出了MapReduce中值得注意的一点，即数据会以key/value对的形式传递。这里，年代（year）是key，温度（temp）是value。因此sort阶段将按照年代把数据分类，之后合并。最终每个reducer就会收到相同的key值。

从上述例子可以看出，reducer的数量并不是固定的。此外，在MapReduce的框架中还有其他一些灵活的配置选项。MapReduce的整个编配工作由主节点（master node）控制。这些主节点控制整个MapReduce作业编配，包括每份数据存放的节点位置，以及map、sort和reduce等阶段的时序控制等。此外，主节点还要包含容错机制。一般地，每份mapper的输入数据会同时分发到多个节点形成多份副本，用于事务的失效处理。一个MapReduce集群的示意图如图15-1所示。

![](images/8e76e193b341cde322cab4c5ff4eaa56668ba5d1a769533bfc9b0dadbf5d99b7.jpg)  
图15-1 MapReduce框架的示意图。在该集群中有3台双核机器，如果机器0失效，作业仍可以正常继续

图15-1的每台机器都有两个处理器，可以同时处理两个map或者reduce任务。如果机器0在map阶段宕机，主节点将会发现这一点。主节点在发现该问题之后，会将机器0移出集群，并在剩余的节点上继续执行作业。在一些MapReduce的实现中，在多个机器上都保存有数据的多个备份，例如在机器0上存放的输入数据可能还存放在机器1上，以防机器0出现问题。同时，每个节点都必须与主节点通信，表明自己工作正常。如果某节点失效或者工作异常，主节点将重启该节点或者将该节点移出可用机器池。

总结一下上面几个例子中关于MapReduce的学习要点：

□ 主节点控制MapReduce的作业流程；  
- MapReduce的作业可以分成map任务和reduce任务；  
□ map任务之间不做数据交流，reduce任务也一样；  
在map和reduce阶段中间，有一个sort或combine阶段；  
□数据被重复存放在不同的机器上，以防某个机器失效；  
□ mapper和reducer传输的数据形式为key/value对。

Apache的Hadoop项目是MapReduce框架的一个实现。下一节将开始讨论Hadoop项目，并介绍如何在Python中使用它。

# 15.2 Hadoop流

Hadoop是一个开源的Java项目，为运行MapReduce作业提供了大量所需的功能。除了分布式计算之外，Hadoop自带分布式文件系统。

本书既不是Java，也不是Hadoop的教材，因此本节只对Hadoop做简单介绍，只要能满足在Python中用Hadoop来执行MapReduce作业的需求即可。如果读者想对Hadoop做深入理解，可以阅读《Hadoop实战》①或者浏览Hadoop官方网站上的文档（http://hadoop.apache.org/）。此外，Mahout in action②一书也为在MapReduce下实现机器学习算法提供了很好的参考资料。

Hadoop可以运行Java之外的其他语言编写的分布式程序。因为本书以Python为主，所以下面将使用Python编写MapReduce代码，并在Hadoop流中运行。Hadoop流（http://hadoop.apache.org/common/docs/current/Streaming.html）很像Linux系统中的管道（管道使用符号|，可以将一个命令的输出作为另一个命令的输入）。如果用mapper.py调用mapper，用reducer.py调用reducer，那么Hadoop流就可以像Linux命令一样执行，例如：

```txt
cat inputFile.txt | python mapper.py | sort | python reducer.py >outputFile.txt 
```

这样，类似的Hadoop流就可以在多台机器上分布式执行，用户可以通过Linux命令来测试Python语言编写的MapReduce脚本。

# 15.2.1 分布式计算均值和方差的 mapper

接下来我们将构建一个海量数据上分布式计算均值和方差的MapReduce作业。示范起见，这里只选取了一个小数据集。在文本编辑器中创建文件mrMeanMapper.py，并加入如下程序清单中的代码。

程序清单15-1 分布式均值和方差计算的mapper  
import sys   
from numpy import mat, mean, power   
def read_input(file): for line in file: yield line.rstrip()   
input $=$ read_input(sys.stdin)   
input $=$ [float(line) for line in input]   
numInputs $\equiv$ len(input)   
input $=$ mat(input)   
sqInput $=$ power(input,2)   
print"%d\t%f\t%f" %(numInputs, mean(input), mean(sqInput))   
print>>sys.stderr, "report: still alive"

这是一个很简单的例子：该mapper首先按行读取所有的输入并创建一组对应的浮点数，然后得到数组的长度并创建NumPy矩阵。再对所有的值进行平方，最后将均值和平方后的均值发送出去。这些值将用于计算全局的均值和方差。

注意 一个好的习惯是向标准错误输出发送报告。如果某作业10分钟内没有报告输出，则将被Hadoop中止。

下面看看程序清单15-1的运行效果。首先确认一下在下载的源码中有一个文件inputFile.txt，其中包含了100个数。在正式使用Hadoop之前，先来测试一下mapper。在Linux终端执行以下命令：

```batch
cat inputFile.txt | python mrMeanMapper.py 
```

如果在Windows系统下，可在DOS窗口输入以下命令：

```batch
python mrMeanMapper.py <inputFile.txt 
```

运行结果如下：

```txt
100 0.509570 0.344439  
report: still alive 
```

其中第一行是标准输出，也就是reducer的输入；第二行是标准错误输出，即对主节点做出的响应报告，表明本节点工作正常。

# 15.2.2 分布式计算均值和方差的reducer

至此，mapper已经可以工作了，下面介绍reducer。根据前面的介绍，mapper接受原始的输入并产生中间值传递给reducer。很多mapper是并行执行的，所以需要将这些mapper的输出合并成一个值。接下来给出reducer的代码：将中间的key/value对进行组合。打开文本编辑器，建立文件mrMeanReducer.py，然后输入程序清单15-2的代码。

程序清单15-2 分布式均值和方差计算的reducer  
import sys   
from numpy import mat, mean, power   
def read_input(file): for line in file: yield line.rstrip()   
input $=$ read_input(sys.stdin)   
mapperOut $=$ [line.split('t')for line in input]   
cumVal=0.0   
cumSumSq=0.0   
cumN=0.0   
for instance in mapperOut: nj $=$ float(object[O]) cumN $+ =$ nj cumVal $+ =$ nj\*float(object[1]) cumSumSq $+ =$ nj\*float(object[2])   
mean $=$ cumVal/cumN   
varSum $=$ (cumSumSq-2\*mean\*cumVal $^+$ cumN\*mean\*mean)/cumN   
print"%d\t%f\t%f" $\%$ (cumN, mean, varSum)   
print>>sys.stderr,"report:still alive"

程序清单15-2就是reducer的代码，它接收程序清单15-1的输出，并将它们合并成为全局的均值和方差，从而完成任务。

你可以在自己的单机上用下面的命令测试一下：

```txt
%cat inputFile.txt | python mrMeanMapper.py | python mrMeanReduceer.py如果是DOS环境，键入如下命令：
```

```batch
%python mrMeanMapper.py <inputFile.txt | python mrMeanReducer.py 
```

后面的章节将介绍如何在多台机器上分布式运行该代码。你手边或许没有10台机器，没有问题，下节就会介绍如何租用服务器。

# 15.3 在Amazon网络服务上运行Hadoop程序

如果要在100台机器上同时运行MapReduce作业，那么就需要找到100台机器，可以采取购买的方式，或者从其他地方租用。Amazon公司通过Amazon网络服务（Amazon Web Services，AWS，http://aws.amazon.com/），将它的大规模计算基础设施租借给开发者。

AWS提供网站、流媒体、移动应用等类似的服务，其中存储、带宽和计算能力按价收费，用户可以仅为使用的部分按时缴费，无需长期的合同。这种仅为所需买单的形式，使得AWS很有诱惑力。例如，当你临时需要使用1000台机器时，可以在AWS上申请并做几天实验。几天后当你发现当前的方案不可行，就即时关掉它，不需要再为这1000台机器支出任何费用。本节首先介绍几个目前在AWS上可用的服务，然后介绍AWS上运行环境的搭建方法，最后给出了一个在AWS上运行Hadoop流作业的例子。

# 15.3.1 AWS上的可用服务

AWS上提供了大量可用的服务。在行内人士看来，这些服务的名字很容易理解，而在新手看来则比较神秘。目前AWS还在不停地演变，也在不断地添加一些新的服务。下面给出一些基本的稳定的服务。

□S3——简单存储服务，用于在网络上存储数据，需要与其他AWS产品配合使用。用户可以租借一组存储设备，并按照数据量大小及存储时间来付费。  
□ EC2——弹性计算云（Elastic Compute Cloud），是使用服务器镜像的一项服务。它是很多AWS系统的核心，通过配置该服务器可以运行大多数的操作系统。它使得服务器可以以镜像的方式在几分钟内启动，用户可以创建、存储和共享这些镜像。EC2中“弹性”的由来是该服务能够迅速便捷地根据需求增加服务的数量。  
□ Elastic MapReduce（EMR）——弹性MapReduce，它是AWS的MapReduce实现，搭建于稍旧版本的Hadoop之上（Amazon希望保持一个稳定的版本，因此做了些修改，没有使用最新的Hadoop）。它提供了一个很好的GUI，并简化了Hadoop任务的启动方式。用户不需要因为集群琐碎的配置（如Hadoop系统的文件导入或Hadoop机器的参数修改）而多花心思。在EMR上，用户可以运行Java作业或Hadoop流作业，本书将对后者进行介绍。

另外，很多其他服务也是可用的，本书将着重介绍EMR。下面还需要用到S3服务，因为EMR需要从S3上读取文件并启动安装Hadoop的EC2服务器镜像。

# 15.3.2 开启Amazon网络服务之旅

使用AWS之前，首先需要创建AWS账号。开通AWS账号还需要一张信用卡，后面章节中的练习将花费大约1美元的费用。打开http://aws.amazon.com/可以看到如图15-2所示的界面，在右上部有“现在注册”（Sign Up Now）按钮。点击后按照指令进行，经过三个页面就可以完成AWS的注册。注意，你需要注册S3、EC2和EMR三项服务。

建立了AWS账号后，登录进AWS控制台并点击EC2、ElasticMapReduce和S3选项卡，确认你是否已经注册了这些服务。如果你没有注册某项服务，会看到如图15-3所示的提示。

![](images/342e8c42007fe98b1b98e2227013ba9e2e2c433ca787db601f9c19121deee2a4.jpg)  
图15-2 http://aws.amazon.com/页面右上部给出了注册AWS账号的按钮

![](images/55070dbf57d20d4a0a69485c125f33d5df627237502fcffcaa63908f4b1cb647.jpg)  
图15-3 服务未注册时的AWS控制台提示信息。如果你的浏览器在S3、EC2和ElasticMapReduce服务页面也有相应提示，请注册这些服务

这样就做好了在Amazon集群上运行Hadoop作业的准备，下一节将介绍在EMR上运行Hadoop的具体流程。

# 15.3.3 在EMR上运行Hadoop作业

注册了所需的Amazon服务之后，登录AWS控制台并点击S3选项卡。这里需要将文件上传，以便AWS能找到我们提供的文件。

(1) 首先需要创建一个新的bucket（可以将bucket看做是一个驱动器）。例如，创建了一个叫做rustbucket的bucket。注意，bucket的名字是唯一的，所有用户均可使用。你应当为自己的bucket创建独特的名字。  
(2) 然后创建两个文件夹：mrMeanCode和mrMeanInput。将之前用Python编写的MapReduce代码上传到mrMeanCode，另一个目录mrMeanInput用于存放Hadoop作业的输入。  
(3)在已创建的bucket中（如rustbucket）上传文件inputFile.txt到mrMeanInput目录。  
(4) 将文件mrMeanMapper.py和mrMeanReduceer.py上传到mrMeanCode目录。这样就完成了全部所需文件的上传，也做好了在多台机器上启动第一个Hadoop作业的准备。  
(5) 点击Elastic MapReduce选项卡，点击“创建新作业流”（Create New Job Flow）按钮，并将作业流命名为mrMean007。屏幕上可以看到如图15-4所示的页面，在下方还有两个复选框和一个下拉框，选择“运行自己的应用程序”（Run Your Own Application）按钮并点击“继续”（Continue）进入到下一步。

![](images/a87e2dc221d5be3c29e1597e5aaad8c348a815baf88952704973e9a1560d2f6a.jpg)  
图15-4 EMR的新作业流创建页面

(6) 在这一步需要设定Hadoop的输入参数。如果这些参数设置错误，作业将会执行失败。在“指定参数”（Specify Parameters）页面的对应字段上输入下面的内容：

Input Location*: <your bucket name>/mrMeanInput/InputFile.txt

Output Location*: <your bucket name>/mrMean007Log

Mapper": "python s3n:// <your bucket name>/mrMeanCode/mrMeanMapper.py"

Reducecr": "python s3n:// / <your bucket name>/mrMeanCode/mrMeanReducer.py"

可以将“其他参数”（ExtraArgs）字段留空。该字段的作用是指定一些其他参数，如reducer的数量。本页面将如图15-5所示，点击“继续”（Continue）。

![](images/82377e00a0e7213e728113bbb4fcf8491360a42dac16ec6a0e6952de9e703eca.jpg)  
图15-5 EMR的指定参数页面

(7)下一个页面需要设定EC2的服务器镜像，这里将设定用于存储数据的服务器数量，默认值是2，可以改成1。你也可以按需要改变EC2服务器镜像的类型，可以申请一个大内存高运算能力的机器（当然也花费更多）。实际上，大的作业经常在大的服务器镜像上运行，详见http://aws.amazon.com/ec2/#instance。本节的简单例子可以选用很小的机器。本页面如图15-6所示，点击“继续”（Continue）。  
(8) 下一个是“高级选项”（Advanced Options）页面，可以设定有关调试的一些选项。务必打开日志选项，在“亚马逊S3日志路径”（Amazon S3 Log Path）里添加s3n://<your bucket name>/mrMean007DebugLog。只有注册SimpleDB才能开启Hadoop调试服务，它是Amazon的访问非关系数据库的简单工具。虽然开启了该服务，但我们不准备使用它来调试Hadoop流作业。当一个Hadoop作业失败，会有一些失败信息写入上述目录，回头读一下这些信息就可以分析在哪里出了问题。整个页面如图15-7所示，点击“继续”（Continue）。

![](images/fa52cce0cdf86a983d471678bd75adc9e6d75e97537a52302b483fd5c004cdf7.jpg)  
图15-6 设定EMR的EC2服务器镜像的页面，在该页面上可以设定MapReduce作业所需的服务器类型和服务器数量

![](images/3dd3365a3543f96898168e0b15febc8f4c84c8d3112731893104d2f6e4c65ea7.jpg)  
图15-7 EMR的高级选项页面，可以设定调试文件的存放位置，也可以设定继续自动运行机制。作业失败还可以设定登录服务器所需的登录密钥。如果想检查代码的运行环境，登录服务器再查看是个很好的办法

(9) 关键的设置已经完成，可以在接下来的引导页面使用默认的设定，一直点“下一步”（Next）到查看（Review）页面。检查一下所有的配置是否正确，然后点击底部的“创建作业流”

（Create Job Flow）按钮。这样新任务就创建好了，在下一个页面点击“关闭”（Close）按钮将返回EMR控制台。当作业运行的时候，可以在控制台看到其运行状态。读者不必担心运行这样一个小作业花费了这么多时间，因为这里包含了新的服务器镜像的配置。最终页面如图15-8所示（可能你那里不会有这么多的失败作业）。

![](images/7ef204cb8e105c86786f93f97a0d268d951960ec74db079319d5314e52c06ded.jpg)  
图15-8 EMR控制台显示出了一些MapReduce作业，本章的MapReduce作业已经在这张图中启动

新建的任务将在开始运行几分钟之后完成，可以通过点击控制台顶端的S3选项卡来观察S3的输出。选中S3控制台后，点击之前创建的bucket（本例中是rustbucket）。在这个bucket里应该可以看到一个mrMean007Log目录。双击打开该目录，可以看到一个文件part-00000，该文件就是reducer的输出。双击下载该文件到本地机器上，用文本编辑器打开该文件，结果应该是这样的：

100 0.509570 0.344439

这个结果与单机上用管道得到的测试结果一样，所以该结果是正确的。如果结果不正确，应当怎样找到问题所在呢？退回到EMR选项卡，点击“已经完成的任务”（Completed Job），可以看到“调试”（Debug）按钮，上面还有一个绿色小昆虫的动画。点击该按钮将打开调试窗口，可以访问不同的日志文件。另外，点击“控制器”（Controller）超链接，可以看到Hadoop命令和Hadoop版本号。

现在我们已经运行了一个Hadoop流作业，下面将介绍如何在Hadoop上执行机器学习算法。MapReduce可以在多台机器上运行很多程序，但这些程序需要做一些修改。

# 不使用AWS

如果读者不希望使用信用卡，或者怕泄露自己的信用卡信息，也能在本地机器上运行同样的作业。下面的步骤假定你已经安装了Hadoop（http://hadoop.apache.org/common/docs/stable/#Getting+Started）。

(1) 将文件复制到HDFS:   
>hadoop fs -copyFromLocal inputFile.txt mrmean-i   
(2) 启动任务：

```shell
>hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-0.20.2-stream-ing.jar -input mrmean-i -output mrmean-o -mapper "python mrMeanMapper.py" -reducer "python mrMeanReduceer.py" 
```

(3)观察结果：

```txt
>hadoop fs -cat mrmean-o/part-00000 
```

(4) 下载结果：

```txt
>hadoop fs -copyToLocal mrmean-o/part-00000. 
```

完成

# 15.4 MapReduce上的机器学习

在10台机器上使用MapReduce并不能等价于当前机器10倍的处理能力。在MapReduce代码编写合理的情况下，可能会近似达到这样的性能，但不是每个程序都可以直接提速的，map和reduce函数需要正确编写才行。

很多机器学习算法不能直接用在MapReduce框架上。这也没关系，正如老话所说：“需求是发明之母。”科学家和工程师中的一些先驱已完成了大多数常用机器学习算法的MapReduce实现。

下面的清单简要列出了本书常用的机器学习算法和对应的MapReduce实现。

□ 简单贝叶斯——它属于为数不多的可以很自然地使用MapReduce的算法。在MapReduce中计算加法非常容易，而简单贝叶斯正需要统计在某个类别下某特征的概率。因此可以将每个指定类别下的计算作业交由单个的mapper处理，然后使用reducer来将结果加和。  
□ $k$ -近邻算法——该算法首先试图在数据集上找到相似向量，即便数据集很小，这个步骤也将花费大量的时间。在海量数据下，它将极大地影响日常商业周期的运转。一个提速的办法是构建树来存储数据，利用树形结构来缩小搜索范围。该方法在特征数小于10的情况下效果很好。高维数据下（如文本、图像和视频）流行的近邻查找方法是局部敏感哈希算法。  
□ 支持向量机（SVM）——第6章使用的Platt SMO算法在MapReduce框架下难以实现。但有一些其他SVM的实现使用随机梯度下降算法求解，如Pegasos算法。另外，还有一个近似的SVM算法叫做最邻近支持向量机（proximal SVM），求解更快并且易于在MapReduce框架下实现①。  
□奇异值分解——Lanczos算法是一个有效的求解近似特征值的算法。该算法可以应用在一系列MapReduce作业上，从而有效地找到大矩阵的奇异值。另外，该算法还可以应用于主成分分析。  
□K-均值聚类——一个流行的分布式聚类方法叫做canopy聚类，可以先调用canopy聚类法取得初始的k个簇，然后再运行K-均值聚类方法。

如果读者有兴趣了解更多机器学习方法的MapReduce实现，可以访问Apache的Mahout项目主页（http://mahout.apache.org/）以及参考Mahout in Action一书。其中Mahout项目以Java语言编写，该书也对处理大规模数据的实现细节做了很详细的介绍。另一个关于MapReduce很棒的资源是Jimmy Lin和Chris Dyer写的Data Intensive Text Processing with Map/Reduce一书。

接下来将介绍一个可以运行MapReduce作业的Python工具。

# 15.5 在Python中使用mrjob来自动化MapReduce

上面列举的算法大多是迭代的。也就是说，它们不能用一次MapReduce作业来完成，而通常需要多步。在15.3节中，Amazon的EMR上运行MapReduce作业只是一个简例。如果想在大数据集上运行AdaBoost算法该怎么办呢？如果想运行10个MapReduce作业呢？

有一些框架可以将MapReduce作业流自动化，例如Cascading和Oozie，但它们不支持在Amazon的EMR上执行。Pig可以在EMR上执行，也可以使用Python脚本，但需要额外学习一种脚本语言。（Pig是一个Apache项目，为文本处理提供高级编程语言，可以将文本处理命令转换成Hadoop的MapReduce作业。）还有一些工具可以在Python中运行MapReduce作业，如本书将要介绍的mrjob。

mrjob①（http://packages.python.org/mrjob/）之前是Yelp（一个餐厅点评网站）的内部框架，它在2010年底实现了开源。读者可以参考附录A来学习如何安装和使用。本书将介绍如何使用mrjob重写之前的全局均值和方差计算的代码，相信读者能体会到mrjob的方便快捷。（需要指出的是，mrjob是一个很好的学习工具，但仍然使用Python语言编写，如果想获得更好的性能，应该使用Java。）

# 15.5.1 mrjob与EMR的无缝集成

与15.3节介绍的一样，本节将使用mrjob在EMR上运行Hadoop流，区别在于mrjob不需要上传数据到S3，也不需要担心命令输入是否正确，所有这些都由mrjob后台完成。有了mrjob，读者还可以在自己的Hadoop集群上运行MapReduce作业，当然也可以在单机上进行测试。作业在单机执行和在EMR执行之间的切换十分方便。例如，将一个作业在单机执行，可以输入以下命令：

```batch
% python mrMean.py < inputFile.txt > myOut.txt 
```

如果要在EMR上运行同样的任务，可以执行以下命令：

```batch
% python mrMean.py -r emr < inputFile.txt > myOut.txt 
```

在15.3节中，所有的上传以及表单填写全由mrjob自动完成。读者还可以添加一条在本地的Hadoop集群上执行作业的命令②，也可以添加一些命令行参数来指定本作业在EMR上的服务器类型和数目。

另外，15.3节中的mapper和reducer分别存于两个不同的文件中，而mrjob中的mapper和reducer可以写在同一个脚本中。下节将展示该脚本的内容，并分析其工作原理。

# 15.5.2 mrjob 的一个 MapReduce 脚本剖析

用mrjob可以做很多事情，本书仍从最典型的MapReduce作业开始介绍。为了方便阐述，继续沿用前面的例子，计算数据集的均值和方差。这样读者可以更专注于框架的实现细节，所以程序清单15-3的代码与程序清单15-1和程序清单15-2的功能一致。打开文本编辑器，创建一个新文件mrMean.py，并加入下面程序清单的代码。

程序清单15-3 分布式均值方差计算的mrjob实现  
from mrjob.job import MRJob   
class MRmean(MRJob): def__init__(self,\*args，\*\*karges)： super(MRmean,self).__init__(\*args，\*\*karges) self.inCount $= 0$ self.inSum $= 0$ self.inSqSum $= 0$ def map(self,key,val): $\text{一}$ 接收输入数据流 if False: yield inVal $=$ float(val) self.inCount $+ = 1$ self.inSum $+ =$ inVal self.inSqSum $+ =$ inVal\*inVal def map_final(self): mn $=$ self.inSum/self.inCount mnSq $=$ self.inSqSum/self.inCount yield(1,[self.inCount，mn，mnSq]) def reduce(self,key,packedValues）： cumVal $= 0.0$ ;cumSumSq $= 0.0$ ;cumN $= 0.0$ for valArr in packedValues: nj $=$ float(valArr[0]) cumN $+ =$ nj cumVal $+ =$ nj\*float(valArr[1]) cumSumSq $+ =$ nj\*float(valArr[2]) mean $=$ cumVal/cumN var $=$ (cumSumSq-2\*mean\*cumVal $^+$ cumN\*mean\*mean)/cumN yield(mean,var) def steps(self): return ([self.mr(maper $\equiv$ self.map,reducer $\equiv$ self.reduce，\ mapper_final $\equiv$ self.map_final)]） if_name $\equiv =$ 'main'： MRmean.run()

该代码分布式地计算了均值和方差。输入文本分发给很多mappers来计算中间值，这些中间值再通过reducer进行累加，从而计算出全局的均值和方差。

为了使用mrjob库，需要创建一个新的MRjob继承类，在本例中该类的类名为MRmean。代码中的mapper和reducer都是该类的方法，此外还有一个叫做steps()的方法定义了执行的步骤。执行顺序不必完全遵从于map-reduce的模式，也可以是map-reduce-reduce-reduce，或者map-reduce-map-reduce-map-reduce（下节会给出相关例子）。在steps()方法里，需要为mrjob指定mapper和reducer的名称。如果未给出，它将默认调用mapper和reducer方法。

首先来看一下mapper的行为：它类似于for循环，在每行输入上执行同样的步骤。如果想在收到所有的输入之后进行某些处理，可以考虑放在mapper_final中实现。这乍看起来有些古怪，但非常实用。另外在mapper()和mapper_final()中还可以共享状态。所以在上述例子中，首先在mapper()中对输入值进行积累，所有值收集完毕后计算出均值和平方均值，最后把这些值作为中间值通过yield语句传出去。①

中间值以key/value对的形式传递。如果想传出去多个中间值，一个好的办法是将它们打包成一个列表。这些值在map阶段之后会按照key来排序。Hadoop提供了更改排序方法的选项，但默认的排序方法足以应付大多数的常见应用。拥有相同key的中间值将发送给同一个reducer。因此你需要考虑key的设计，使得在sort阶段后相似的值能够收集在一起。这里所有的mapper都使用“1”作为key，因为我希望所有的中间值都在同一个reducer里加和起来。②

mrjob里的reducer与mapper有一些不同之处，reducer的输入存放在迭代器对象里。为了能读取所有的输入，需要使用类似for循环的迭代器。mapper或mapper_final和reducer之间不能共享状态，因为Python脚本在map和reduce阶段中间没有保持活动。如果需要在mapper和reducer之间进行任何通信，那么只能通过key/value对。在reducer的最后有一条输出语句，该语句没有key，因为输出的key值已经固定。如果该reducer之后不是输出而是执行另一个mapper，那么key仍需要赋值。

无须多言，下面看一下实际效果，先运行一下mapper，在Linux/DOS的命令行输入下面的命令（注意不是在Python提示符下）。其中的文件inputFile.txt在第15章的代码里。

```batch
%python mrMean.py --mapper <inputFile.txt 
```

运行该命令后，将得到如下输出：

```json
1 [100，0.50956970000000001，0.34443931307935999] 
```

要运行整个程序，移除--mapper选项。

```batch
%python mrMean.py <inputFile.txt 
```

你将在屏幕上看到很多中间步骤的描述文字，最终的输出如下：

```batch
streaming final output from c:\users\peter\appdata\local
\temp\mrMean.Peter.20110228.172656.279000\output\part-00000
0.50956970000000001 0.34443931307935999
removing tmp directory c:\users\peter\appdata\local\temp\mrMean.Peter.20110228.172656.279000
To stream the valid output into a file, enter the following command:
%python mrMean.py < inputFile.txt > outFile.txt 
```

最后，要在Amazon的EMR上运行本程序，输入如下命令（确保你已经设定了环境变量AWS_ACCESS_KEY_ID和AWS_secret_ACCESS_KEY，这些变量的设定见附录A）。

```batch
python mrMean.py -r emr < inputFile.txt > outFile.txt 
```

完成了mrjob的使用练习，下面将用它来解决一些机器学习问题。上文提到，一些迭代算法仅使用EMR难以完成，因此下一节将介绍如何用mrjob完成这项任务。

# 15.6 示例：分布式SVM的Pegasos算法

第4章介绍过一个文本分类算法：简单贝叶斯。该算法将文本文档看做是词汇空间里的向量。第6章又介绍了效果很好的SVM分类算法，该算法将每个文档看做是成千上万个特征组成的向量。

在机器学习领域，海量文档上做文本分类面临很大的挑战。怎样在如此大的数据上训练分类器呢？如果能将算法分成并行的子任务，那么MapReduce框架有望帮我们实现这一点。回忆第6章，SMO算法一次优化两个支持向量，并在整个数据集上迭代，在需要注意的值上停止。该算法看上去并不容易并行化。

# 在MapReduce框架上使用SVM的一般方法

(1)收集数据：数据按文本格式存放。  
(2) 准备数据：输入数据已经是可用的格式，所以不需任何准备工作。如果你需要解析一个大规模的数据集，建议使用map作业来完成，从而达到并行处理的目的。  
(3) 分析数据：无。  
(4) 训练算法：与普通的SVM一样，在分类器训练上仍需花费大量的时间。  
(5) 测试算法：在二维空间上可视化之后，观察超平面，判断算法是否有效。  
(6) 使用算法：本例不会展示一个完整的应用，但会展示如何在大数据集上训练SVM。该算法其中一个应用场景就是文本分类，通常在文本分类里可能有大量的文档和成千上万的特征。

SMO算法的一个替代品是Pegasos算法，后者可以很容易地写成MapReduce的形式。本节将分析Pegasos算法，介绍如何写出分布式版本的Pegasos算法，最后在mrjob中运行该算法。

# 15.6.1 Pegasos算法

Pegasus是指原始估计梯度求解器（Primal Estimated sub-GrAdient Solver）。该算法使用某种形式的随机梯度下降方法来解决SVM所定义的优化问题，研究表明该算法所需的迭代次数取决于用户所期望的精确度而不是数据集的大小，有关细节可以参考原文①。原文有长文和短文两个版本，推荐阅读长文。

第6章提到，SVM算法的目的是找到一个分类超平面。在二维情况下也就是要找到一条直线，将两类数据分隔开来。Pegasos算法工作流程是：从训练集中随机挑选一些样本点添加到待处理列表中，之后按序判断每个样本点是否被正确分类；如果是则忽略，如果不是则将其加入到待更新集合。批处理完毕后，权重向量按照这些错分的样本进行更新。整个算法循环执行。

上述算法伪代码如下：

将w初始化为0

对每次批处理

随机选择k个样本点（向量）

对每个向量

如果该向量被错分：

更新权重向量w

累加对w的更新

为了解实际效果，Python版本的实现见程序清单15-4。

程序清单15-4 SVM的Pegasos算法  
```python
def predict(w,x):
    return w*x.T
def batchPegasos(dataSet, labels, lam, T, k):
    m,n = shape(dataSet); w = zeros(n);
    dataIndex = range(m)
    for t in range(1, T+1):
        wDelta = mat(zeros(n))
        eta = 1.0/(lam*t)
        randomshuffle(dataIndex)
    for j in range(k):
        i = dataIndex[j]
        p = predict(w, dataSet[i,:])
        if labels[i]*p < 1:
            wDelta += labels[i]*dataSet[i,:].A
        w = (1.0 - 1/t)*w + (eta/k)*wDelta
    return w 
```

程序清单15-4的代码是Pegasos算法的串行版本。输入值T和k分别设定了迭代次数和待处理列表的大小。在T次迭代过程中，每次需要重新计算eta。它是学习率，代表了权重调整幅度的大小。在外循环中，需要选择另一批样本进行下一次批处理；在内循环中执行批处理，将分类错误的值全部累加之后更新权重向量①。

如果想试试它的效果，可以用第6章的数据来运行本例程序。本书不会对该代码做过多分析，它只为Pegasos算法的MapReduce版本做一个铺垫。下节将在mrjob中建立并运行一个MapReduce版本的Pegasos算法。

# 15.6.2 训练算法：用mrjob实现MapReduce版本的SVM

本节将用MapReduce来实现程序清单15-4的Pegasos算法，之后再用15.5节讨论的mrjob框架运行该算法。首先要明白如何将该算法划分成map阶段和reduce阶段，确认哪些可以并行，哪些不能并行。

对程序清单15-4的代码运行情况稍作观察将会发现，大量的时间花费在内积计算上。另外，内积运算可以并行，但创建新的权重变量w是不能并行的。这就是将算法改写为MapReduce作业的一个切入点。在编写mapper和reducer的代码之前，先完成一部分外围代码。打开文本编辑器，创建一个新文件mrSVM.py，然后在该文件中添加下面程序清单的代码。

程序清单15-5 mrjob中分布式Pegasos算法的外围代码  
from mrjob.job import MRJob   
import pickle   
from numpy import \*   
class MRsvm(MRJob): DEFAULT_INPUT_PROTOCOL $=$ 'json_value' def __init__(self,\*args,\*\*kwargs): super(MRsvm,self).__init__(\*args,\*\* kwargs) self.data $=$ pickle.load(open(\'<path to your Ch15 code directory>\svmDat27')) self.w $= 0$ self eta $= 0.69$ self.dataList $= []$ self.k $=$ self(options.batchsize self.numMappers $= 1$ self.t $= 1$ def configure_options(self): super(MRsvm,self).configure_options() self.addpassesthrough_option( --iterations',dest $=$ iterations',default $= 2$ ,type $=$ int', help $=$ 'T: number of iterations to run') self.addpassesthrough_option( --batchsize',dest $=$ batchsize',default $= 100$ ,type $=$ int', help $=$ 'k: number of data points in a batch')

```python
def steps(self):
    return ([self.lr(maper=slow.map, mapper_final=slow.map_fin, reducer=slow.reduce)]*self(options_iterations)
if __name__ == '__main__': 
```

程序清单15-5的代码进行了一些设定，从而保证了map和reduce阶段的正确执行。在程序开头，Mrjob、NumPy和Pickle模块分别通过一条include语句导入。之后创建了一个mrjob类MRsvm，其中__init__(方法初始化了一些在map和reduce阶段用到的变量。Python的模块Pickle在加载不同版本的Python文件时会出现问题。为此，我将Python2.6和2.7两个版本对应的数据文件各自存为svmDat26和svmDat27。

对应于命令行输入的参数，Configure_options()方法建立了一些变量，包括迭代次数（T）、待处理列表的大小（k）。这些参数都是可选的，如果未指定，它们将采用默认值。

最后，steps()方法告诉mrjob应该做什么，以什么顺序来做。它创建了一个Python的列表，包含map、map_fin和reduce这几个步骤，然后将该列表乘以迭代次数，即在每次迭代中重复调用这个列表。为了保证作业里的任务链能正确执行，mapper需要能够正确读取reducer输出的数据。单个MapReduce作业中无须考虑这个因素，这里需要特别注意输入和输出格式的对应。

我们对输入和输出格式进行如下规定：

Mapper Inputs: <mapperNum, valueList> Outputs: nothing   
Mapper_final Inputs: nothing Outputs: $<  1$ ,valueList $\rightharpoondown$ Reducer Inputs: <mapperNum, valueList> Outputs: <mapperNum, valueList>

传入的值是列表数组，valueList的第一个元素是一个字符串，用于表示列表的后面存放的是什么类型的数据，例如{'x',23}和['w',[1,5,6]]。每个mapper_final都将输出同样的key，这是为了保证所有的key/value对都输出给同一个reducer。

定义好了输入和输出之后，下面开始写mapper和reducer方法，打开mrSVM.py文件并在MRsvm类中添加下面的代码。

程序清单15-6 分布式Pegasos算法的mapper和reducer代码

```python
def map(self, mapperId, inVals): if False: yield if inVals[0] == 'w': self.w = inVals[1] elif inVals[0] == 'x': self.dataList.append(inVals[1]) elif inVals[0] == 't': self.t = inVals[1] def map_fin(self): labels = self.data[:, -1]; X= self.data[:, 0:-1] if self.w == 0: self.w = [0.001]*shape(X)[1] for index in self.dataList: p = mat(self.w)*X[index,:].T if labels[index]*p < 1.0: yield (1, ['u', index]) yield (1, ['w', self.w]) yield (1, ['t', self.t]) def reduce(self, _, packedVals): for valArr in packedVals: if valArr[0] == 'u': self.dataList.append(valArr[1]) elif valArr[0] == 'w': self.w = valArr[1] elif valArr[0] == 't': self.t = valArr[1] labels = self.data[:, -1]; X= self.data[:, 0:-1] wMat = mat(self.w); wDelta = mat(zeros(len(self.w))) for index in self.dataList: wDelta += float(labels[index])*X[index,:] eta = 1.0/(2.0*self.t) wMat = (1.0 - 1.0/self.t)*wMat + (eta/self.k)*wDelta for mapperNum in range(1,self.numMappers+1): yield (mapperNum,'wMat.tolist()) [0]) if self.t < self(options_iterations: yield (mapperNum,'t', self.t+1)) for j in range(self.k/self.numMappers): yield (mapperNum,'['x', random.randint(shape(self.data)[0])) 
```

程序清单15-6里的第一个方法是map()，这也是分布式的部分，它得到输入值并存储，以便在map_fin()中处理。该方法支持三种类型的输入：w向量、t或者x。t是迭代次数，在本方法中不参与运算。状态不能保存，因此如果需要在每次迭代时保存任何变量并留给下一次迭代，可以使用key/value对传递该值，抑或是将其保存在磁盘上。显然前者更容易实现，速度也更快。

map_fin()方法在所有输入到达后开始执行。这时已经获得了权重向量w和本次批处理中的一组x值。每个x值是一个整数，它并不是数据本身，而是索引。数据存储在磁盘上，当脚本执行的时候读入到内存中。当map_fin()启动时，它首先将数据分成标签和数据，然后在本次批处理的数据（存储在self.dataList里）上进行迭代，如果有任何值被错分就将其输出给reducer。为了在mapper和reducer之间保存状态，w向量和t值都应被发送给reducer。

最后是reduce()函数，对应本例只有一个reducer执行。该函数首先迭代所有的key/value对并将值解包到一个局部变量datalist里。之后dataList里的值都将用于更新权重向量w，更新量在wDelta中完成累加①。然后，wMat按照wDelta和学习率eta进行更新。在wMat更新完毕后，

又可以重新开始整个过程：一个新的批处理过程开始，随机选择一组向量并输出。注意，这些向量的key是mapper编号。

为了看一下该算法的执行效果，还需要用一些类似于reducer输出的数据作为输入数据启动该任务，我为此附上了一个文件kickStart.txt。在本机上执行前面的代码可以用下面的命令：

```batch
python mrSVM.py < kickStart.txt 
```

```txt
streaming final output from c:\users\peter\appdata\local\temp \mrSVM.Peter.20110301.011916.373000\output\part-00000   
1 ["w",[0.51349820499999987，-0.084934502500000009]]   
removing tmp directory c:\users\peter\appdata\local\temp \mrSVM.Peter.20110301.011916.373000 
```

这样就输出了结果。经过2次和50次迭代后的分类面如图15-9所示。

![](images/0f089cc5ccb5caa9e666aac98caaec4d9bd7cbed0fe98f5443ce98b6bd9f79f4.jpg)  
图15-9 经过多次迭代的分布式Pegasos算法执行结果。该算法收敛迅速，多次迭代后可以得到更好的结果

如果想在EMR上运行该任务，可以添加运行参数：-r emr。该作业默认使用的服务器个数是1。如果要调整的话，添加运行参数：--num-ec2-instances=2（这里的2也可以是其他正整数），整个命令如下：

python mrSVM.py -r emr --num-ec2-classes $= 3$ < kickStart.txt > myLog.txt

要查看所有可用的运行参数，输入%python mrSVM.py -h。

# 调试mrjob

调试一个mrjob脚本将比调试一个简单的Python脚本棘手得多。这里仅给出一些调试建议。

□ 确保已经安装了所有所需的部件：boto、simplejson和可选的PyYAML。  
□ 可以在~/mrjob.conf文件中设定一些参数，确定它们是正确的。  
□在将作业放在EMR上运行之前，尽可能在本地多做调试。能在花费10秒就发现一个错误的情况下，就不要花费10分钟才发现一个错误。  
□检查base_temp_dir目录,它在~/mrjob.conf中设定。例如在我的机器上,该目录的存放位置是/scratch/$USER,其中可以看到作业的输入和输出,它将对程序的调试非常有帮助。  
□一次只运行一个步骤。

到现在为止，读者已经学习了如何编写以及如何在大量机器上运行机器学习作业，下节将分析这样做的必要性。

# 15.7 你真的需要MapReduce吗？

不需要知道你是谁，我可以说，你很可能并不需要使用MapReduce和Hadoop，因为单机的处理能力已经足够强大。这些大数据的工具是Google、Yelp和Facebook等公司开发的，世界上能有多少这样的公司？

充分利用已有资源可以节省时间和精力。如果你的作业花费了太多的时间，先问问自己：代码是否能用更有效率的语言编写（如C或者Java）？如果语言已经足够有效率，那么代码是否经过了充分的优化？影响处理速度的系统瓶颈在哪里，是内存还是处理器？或许你不知道这些问题的答案，找一些人做些咨询或讨论将非常有益。

大多数人意识不到单台机器上可以做多少数字运算。如果没有大数据的问题，一般不需要用到MapReduce和Hadoop。但对MapReduce和Hadoop稍作了解，在面临大数据的问题时知道它们能做些什么，还是很棒的一件事情。

# 15.8 本章小结

当运算需求超出了当前资源的运算能力，可以考虑购买更好的机器。另一个情况是，运算需求超出了合理价位下所能购买到的机器的运算能力。其中一个解决办法是将计算转成并行的作业，MapReduce就提供了这种方案的一个具体实施框架。在MapReduce中，作业被分成map阶段和reduce阶段。

一个典型的作业流程是先使用map阶段并行处理数据，之后将这些数据在reduce阶段合并。这种多对一的模式很典型，但不是唯一的流程方式。mapper和reducer之间传输数据的形式是key/value对。一般地，map阶段后数据还会按照key值进行排序。Hadoop是一个流行的可运行MapReduce作业的Java项目，它同时也提供非Java作业的运行支持，叫做Hadoop流。

Amazon网络服务（AWS）允许用户按时长租借计算资源。弹性MapReduce（EMR）是Amazon网络服务上的一个常用工具，可以帮助用户在AWS上运行Hadoop流作业。简单的单步MapReduce任务可以在EMR管理控制台上实现并运行。更复杂的任务则需要额外的工具。其中一个相对新的开源工具是mrjob，使用该工具可以顺序地执行大量的MapReduce作业。经过很少的配置，mrjob就可以自动完成AWS上的各种繁杂步骤。

很多机器学习算法都可以很容易地写成MapReduce作业。而另一些机器学习算法需要经过创新性的修改，才能在MapReduce上运行。SVM是一个强大的文本分类工具，在大量文档上训练一个分类器需要耗费巨大的计算资源，而Pegasos算法可以分布式地训练SVM分类器。像Pegasos算法一样，需要多次MapReduce作业的机器学习算法可以很方便地使用mrjob来实现。

到这里为止，本书的正文部分就结束了，感谢你的阅读。希望这本书能为你开启新的大门。另外，在机器学习的数学和具体实现方面还有很多东西值得探索。我很期待你能使用在本书里学到的工具和技术开发出一些有趣的应用。
