# A survey of community detection methods in multilayer networks

Xinyu Huang $^{1}$ · Dongming Chen $^{1}$ · Tao Ren $^{1}$ · Dongqi Wang $^{1}$

Received: 24 February 2020 / Accepted: 10 September 2020 / Published online: 13 October 2020  
© The Author(s) 2020

# Abstract

Community detection is one of the most popular researches in a variety of complex systems, ranging from biology to sociology. In recent years, there's an increasing focus on the rapid development of more complicated networks, namely multilayer networks. Communities in a single-layer network are groups of nodes that are more strongly connected among themselves than the others, while in multilayer networks, a group of well-connected nodes are shared in multiple layers. Most traditional algorithms can rarely perform well on a multilayer network without modifications. Thus, in this paper, we offer overall comparisons of existing works and analyze several representative algorithms, providing a comprehensive understanding of community detection methods in multilayer networks. The comparison results indicate that the promoting of algorithm efficiency and the extending for general multilayer networks are also expected in the forthcoming studies.

Keywords Community detection $\cdot$ Multilayer network $\cdot$ Temporal network $\cdot$ Multiplex network $\cdot$ Multilevel network

Responsible editor: Tim Weninger.

$\boxtimes$ Dongming Chen

chendm@mail.neu.edu.cn

Xinyu Huang

neuhxy@163.com

Tao Ren

rent@swc.neu.edu.cn

Dongqi Wang

wangdq@swc.neu.edu.cn

$^{1}$ Software College, Northeastern University, Shenyang 110169, China

# 1 Introduction

Network theory is an important tool for describing and analyzing complex systems throughout a variety of disciplines. Community structures, defined as groups of nodes that are more densely connected than with rest of the network, are widely existed in many real-world complex systems, such as sociology, biology, transportation systems, and so on (Newman 2018). Discovering communities in these systems has become a primary approach to understand how network structure relates to system behaviors. As an effective technique to unveil the underlying structures, community detection has been utilized in many scenarios, such as finding potential friends in social media (Zhu et al. 2017), recommending products for users (Li and Zhang 2020), analyzing social opinions (Wang et al. 2017), and so on.

With the deepening of research, more and more scholars come to realize that simply uncovering communities in a single network is insufficient to analyze the structures and system behaviors in real-life applications. Unlike the community structure in single-layer networks, communities in multilayer networks are comprised of a group of well-connected nodes in all layers. For example, individuals in social networks may have various interactions (e.g. sending emails, participating in the same activity) among them (Ansari et al. 2011). As a result, the conventional studies are encountering with an essential problem of how to utilize the multiple views of the network (Papalexakis et al. 2013). There are also similar scenarios with relevant notations such as multiplex networks (Verbrugge 1979), multilevel networks (Wang et al. 2013), network of networks (Gao et al. 2011), interdependent networks (Buldyrev et al. 2010), multi-dimensional networks (Berlingerio et al. 2011c), which can be generally regarded as multilayer networks (Kivelä et al. 2014). As more interaction information implied, community detection in multilayer networks has been introduced to leverage various relationships to get more accurate results (Liu et al. 2018).

# 1.1 Background

The research of complex networks oriented from graph theory, which is started from the "Seven Bridges of Königsberg" problem in 1736. Although naive in many respects, this approach has been extremely successful in many real-life applications. At the end of the 20th century, by employing the graph model, the famous small-world (Watts and Strogatz 1998) and scale-free (Barabási and Albert 1999) features were discovered (Newman 2018). In 2002, Girvan and Newman uncovered the community structure in networks (Girvan and Newman 2002), which opened up a continuous upsurge of relevant research. The so-called community structures are groups of nodes that are more strongly or frequently connected among themselves than with the others. Therefore, community detection is proposed to find the most reasonable partitions of a network via observed topological structures. Several conventional approaches are listed in Table 1.

In recent years, a sea of improved algorithms based on the above approaches are proposed with fruitful results. However, with the exponential growth of data scale, community detection on large volume data has encountered a serious of problems. For

Table 1 Comparison of some classic community detection algorithms   

<table><tr><td>Name</td><td>Parameters</td><td>Complexity</td><td>Strategy</td></tr><tr><td>GN (Girvan and Newman 2002)</td><td>-</td><td>O(nm2)</td><td>Removal of edges with maximum betweenness</td></tr><tr><td>FN, CNM (Girvan and Newman 2002)</td><td>-</td><td>O(n2), O(n log2n)</td><td>Greedy modularity maximization</td></tr><tr><td>LPA (Raghavan et al. 2007)</td><td>i</td><td>O(n)</td><td>Label propagation</td></tr><tr><td>EM (Newman and Leicht 2007)</td><td>k, i</td><td>O(nki)</td><td>Probabilistic mixture models</td></tr><tr><td>SCAN (Xu et al. 2007)</td><td>ε, μ</td><td>O(n)</td><td>Structural clustering</td></tr><tr><td>Louvain (Blondel et al. 2008)</td><td>-</td><td>O(n log n)</td><td>Multilevel modularity maximization</td></tr><tr><td>LFM (Lancichinetti et al. 2009)</td><td>α</td><td>O(n2 log n)</td><td>Local optimization</td></tr><tr><td>INFOMAP (Rosvall and Bergstrom 2008; Yiapanis et al. 2013)</td><td>τ</td><td>O(m(n+m))</td><td>Information compression</td></tr><tr><td>NMF (Zhang et al. 2007)</td><td>-</td><td>O(hkn2)</td><td>Nonnegative matrix factorization</td></tr></table>

$n$ is the number of nodes and $m$ depicts the number of edges. GN is short for Girvan-Newman Algorithm. FN is from Clauset, Newman, and Moore's work "Finding community structure in very large networks", CNM is an improved version using heap structure and faster updating strategy. Label Propagation Algorithm (i.e. LPA) has nearly linear complexity, where parameter $i$ depicts the number of iterations. Parameter $\alpha$ in LFM algorithm controls the scale of detected communities. In addition, LFM algorithm can be used to detect overlapping communities. EM algorithm, or Expectation-maximization algorithm, requires parameters $k$ and $i$ , where $k$ is the number of communities, $i$ is the iterative times. $\tau$ in INFOMAP algorithm depicts the probability of a random walk, usually assigned $\tau = 0.5$ . $h$ in NMF algorithm represents the number of iterations until convergence, $k$ is the dimensionality value

example, some datasets are changing with time, which brings challenges for traditional research on a static graph. In other words, traditional methods are incapable of dealing with large-scale time-varying networks.

Lancichinetti et al. (2009) raise two problems: The first one is about hierarchical structure, which means large communities are composed of small communities and in turn, large communities group together to form even larger ones. The second is overlapping communities, for example, people belong to more than one community, depending on their families, friends, professions, hobbies, etc. Nodes belong to more than one community is a pervasive scenario in networks.

Besides, the interactions among nodes are becoming more complicated than ever before. The conventional monolayer network (i.e. single-layer network) has provided plentiful cases in which a unit in a complex system is charted into a node, and the interactions between units are straightly represented as edges, no matter what type or weight of the interactions are. With the development of network modeling, we are starting to realize that the existing models cannot fully capture the details existed in some real-life problems, which even leads to incorrect descriptions of some phenomena taking place on real-world networks. Some representative problems occurred are listed as follows.

(1) Multiple interactions among social networks. There has been an increasing focus on social media such as Twitter, Facebook, and Google+, etc. People share their opinions on daily affairs, chat with friends, or even make trades on these platforms. The main problem for analyzing social networks is the multiple interactions among individuals. For example, relationships between two individuals may include friendship, kinship, or schoolmates. If we regard all the relationships as undistinguished edges, the differences will be ignored, which is probably leading to incorrect results.   
(2) Interbank trades. Online payments are replacing traditional cash payments and credit cards, which offers a convenient lifestyle, meanwhile, providing a new way for financial criminals. For example, in money laundering activities (Colladon and Remondi 2017), criminals use different channels to conduct trades. If we simply analyze the transfer records from a single bank with a graph model, the result may be unconvinced. Thus, it is necessary to collect data from all possible trade channels with a multilayer network model, in which each channel can be regarded as a layer.   
(3) Urban transportation system. The study on the urban transportation system has caused wide public concern in the last decade (Black 2018; Sultana et al. 2019). Citizens travel in a variety of ways such as bus, subway, tram, and so on. In analyzing the public transportation system, the characteristics of various modes of transportation should be fully considered, especially for some hub stations (Zhang et al. 2018b) that should be given more attention to solving traffic congestion problems. When bus routes are blocked, numerous passengers select subway alternatively, which results in crowd subway operations. Inherently, the urban transportation system is a multilayer network model.

By tackling the above problems with the multilayer network model (Kivelä et al. 2014; Boccaletti et al. 2014), we are able to get a more reasonable result, i.e., the

community structures in multilayer networks benefit to identify functionally cohesive sub-units and reveal complex interactions and heterogeneous links.

# 1.2 Main contributions

There have been numerous attempts to address community detection problem in multilayer networks through diverse approaches, e.g., identifying communities in temporal networks by modularity-maximization (Bazzi et al. 2016), where the authors emphasize the difference between "null networks" and "null models" in modularity maximization and discuss the effect of interlayer edges on the multilayer modularity-maximization problem. De Bacco et al. (2017) propose a generative model for multilayer networks, which can be used to aggregate layers into clusters or to compress a dataset by identifying especially relevant or redundant layers. The proposed model is capable of incorporating community detection and link prediction for multilayer networks, and experimental results on both synthetic and real-world datasets show its feasibility. Analyzing multilayer networks is of great importance because many interesting patterns cannot be obtained by analyzing single-layer networks. That's our motivation for summarizing these approaches. The contributions of this work are:

(1) We build a taxonomy of community detection methods based on various techniques used.   
(2) We provide a detailed survey of works that come under different categories.   
(3) The evaluation measures for community results are categorized and summarized.   
(4) The applications of community detection in multilayer networks are introduced, as well as interesting directions for future works.

To the best of our knowledge, this is the latest work that provides a comprehensive survey on various community detection methods in multilayer networks.

# 1.3 Outline of the paper

The remainder is organized along the other 5 sections. In the next section, we start by presenting the multilayer network models with several real-world datasets and give brief comparisons on different definitions. Section 3 summarizes the existing community detection methods in multilayer networks and provides some metrics for quality evaluation. We introduce various applications of community detection in multilayer networks in Sect. 4, such as temporal networks, social networks, transportation systems, and biological systems. Section 5 offers concluding remarks and perspective ideas.

# 2 Models

As mentioned above, numerous researchers dedicated to solving the problems in their own situations. In the 1930s, sociograms (Roethlisberger and Dickson 2003) were proposed to represent social relationships in a banking room, which contains 14 individuals via 6 different types of social interactions, as shown in Fig. 1. Such

![](images/8591e20197a740034792cccffa6f9697a6fb73bcf639bb02632d4abbfff270bd.jpg)

![](images/cb9f59adb8abea6edb402281c1c5e9f1c181c315dd914531ca111ca3b73830b2.jpg)  
Fig. 1 The sociograms proposed by Roethlisberger et al. contains 14 individuals via 6 different types of social interactions, observed friendship ties and cliques in a factory. Position reflects the location of their workspace

networks are known as "multiplex networks" (Mucha et al. 2010) or "multi-relational networks" (Cai et al. 2005) in which edges are categorized by their types.

In recent years, great endeavors have been made to unveil the basic mechanisms for the generation of networks with specific structural properties. The analysis of networks has profound implications in very different fields, from social media analytics to biology (Newman 2018). The conventional graph model is incapable when the network is differentiated, multipartite, integrated, and dynamic. Thus, a series of more complicated models are proposed, such as temporal networks (Kostakos 2009), multiplex networks, $k$ -partite networks, and so on (Boccaletti et al. 2014; Kivelä et al. 2014). However, the sudden and immense explosion of research on multilayer networks has also led to a great deal of confusion (Ahmed et al. 2018; Farooq and Zhu 2018; Kivelä et al. 2014; Paolucci 2018). The multilayer network we focus, in this paper, is not a neural network but a mathematic model which can be utilized to represent the complicated network structures.

A multilayer network is a network made of multiple graphs, called layers, which share the same set of nodes, but differ in their edges. To distinguish the definition of a multilayer network from a single-layer network, we intuitively compared the representation of a real-world dataset between the two models. Al Qaeda cell was isolated from a safe hiding place during training and plan terrorist attacks, which

![](images/211ce2e48ce828f314cc2d8262870b9f2df38765ab08a584c3ba8a1ef5e3d798.jpg)  
Fig. 2 The social relationships of 9/11 terrorists represented by a graph model. It is a single-layer network, which consists of 69 nodes and 159 edges. The size of the node represents the number of the neighbors (i.e., the node's degree)

forced the organization to form a relatively dense social network, in which the Hamburg branch planned and eventually participated in the implementation of the September 11 attacks (Silber 2011). The social relationships of 9/11 terrorists are represented in Fig. 2.

If we classify the closeness of the social actions among the terrorists, we can obtain three groups of links, abstracted into a three-layered network, as shown in Fig. 3.

It is obvious that the multilayer networks reveal more detailed information than the monolayer network. The multilayer network model contains nodes and edges from different layers, which represents the different frequencies (or types) of interactions among them.

The nodes in a multilayer network are consistent in that of graph model, which represents individuals across multiple layers likewise. Specifically, some works indicate that the nodes in a multilayer network can be classified into different categories (e.g. bipartite network), thereby the network composed of these nodes is described as a node-colored network (Baltakiene et al. 2018; Brummitt 2014; Kivelä et al. 2014). The different colors represent different types of nodes. For example, the urban transportation system mainly contains buses, subway, and so on. The bus stop and subway station make no difference in a conventional graph model but differ in a multilayer network for the different manners of transportation, thereby they are printed with dif-

![](images/3027fdbd5f8204fb98bef2369c5bfa5920b2f1547d43c1f069b9931f8323eb4a.jpg)  
Fig. 3 The 9/11 terrorists' interactions represented by a multilayer network. $L_{1}$ presents confirmed close contact, $L_{2}$ shows various recorded interactions, $L_{3}$ contains potential or planed or unconfirmed interactions

![](images/d9a563840fac9a53cdc2efd1ea8ae002c4d17ec8ddac5cc40384f99045f5b889.jpg)  
Fig. 4 The transportation system in Chengdu city of China, where the left layer shows the bus lines and the right layer shows the subway system. The nodes from different layers are connected by interlayer edges if the corresponding stations are within $0.5\mathrm{km}$

ferent colors. Inherently, the layers' information has covered the different node colors, i.e., the multilayer network model is a capable solution. The transportation network of Chengdu city is plotted by Muxviz (De Domenico et al. 2015b), as shown in Fig. 4.

The edges in a multilayer network are classified into intralayer edges and interlayer edges (De Domenico et al. 2016). The intralayer edges request the two nodes of this edge are in the same layer, while the interlayer edges (or crossed layer edges) connect nodes among different layers, as illustrated in Fig. 5.

A layer in multilayer networks is composed of a set of nodes and edges, i.e., a graph model. The layers are also organized into two categories:

![](images/b83cca46d3ee7156aa29bc5ab74bfed0073d9a5345c97df88df745657e07ab8d.jpg)  
Fig. 5 A three-layered toy network. The edges in each layer are called intralayer edges, as marked by solid lines, the dashed lines crossed adjacent layers represent interlayer edges

- Ordinal layers. The layers are sorted by a certain order, in which the interlayer edges connect the corresponding nodes in the adjacent layers. Take temporal networks for example, there are numerous layers representing different snapshots, but the order of layers is decided by the time sequence.   
- Categorical layers. The layers are classified into several groups, where each group represents a type of interaction.

# 2.1 Definitions

There are many terms for describing multilayer networks, such as multiplex network, multi-relational network, edge-colored network, node-colored network, multilevel network, multi-dimensional network, independent networks, networks of networks, temporal network and so on (Boccaletti et al. 2014; Kivelä et al. 2014). Table 2 summarizes the main notations used throughout this paper.

As we have known, a graph is a tuple $G = (V, E)$ , where $V$ is a set of nodes and $E \subseteq V \times V$ is the set of edges that connect pairs of nodes (Bollobás 2013). The model of multilayer networks is more complicated and there are mainly two kinds of explanations. One is summarized by Kivelä et al. (2014), described as

$$
\mathcal {M} = \left(V _ {M}, E _ {M}, V, \mathcal {L}\right), \tag {1}
$$

where $V_{M} \subseteq V \times \mathcal{L}_{1} \times \mathcal{L}_{2} \times \dots \times \mathcal{L}_{d}$ , $E_{M} \subseteq V_{M} \times V_{M}$ . They employed a sequence $\mathcal{L}$ , described as

$$
\mathcal {L} = \left\{\mathcal {L} _ {\alpha} \right\} _ {\alpha = 1} ^ {d}, \tag {2}
$$

Table 2 Main symbols used in this paper   

<table><tr><td>Symbol</td><td>Scenario</td><td>Description</td></tr><tr><td>R</td><td>General definitions</td><td>Set of real numbers</td></tr><tr><td>G</td><td>Monolayer</td><td>A graph or a monolayer network</td></tr><tr><td>n</td><td>Monolayer</td><td>The number of nodes</td></tr><tr><td>m</td><td>Monolayer</td><td>The number of edges</td></tr><tr><td>V</td><td>Monolayer</td><td>Node set in a graph or monolayer networks</td></tr><tr><td>E</td><td>Monolayer</td><td>Edge set in a graph or monolayer networks</td></tr><tr><td>ki</td><td>Monolayer</td><td>The degree of node i</td></tr><tr><td>Γ(i)</td><td>Monolayer</td><td>The neighbors of node i</td></tr><tr><td>Q</td><td>Monolayer</td><td>Modularity (Newman and Girvan 2004)</td></tr><tr><td>D</td><td>Monolayer</td><td>Modularity density (Li et al. 2008)</td></tr><tr><td>S</td><td>Monolayer</td><td>Surprise (Aldecoa and Marín 2011)</td></tr><tr><td>L</td><td>Multilayer</td><td>The total number of layers</td></tr><tr><td>G</td><td>Multilayer</td><td>A multilayer network comprised of graphs</td></tr><tr><td>C</td><td>Multilayer</td><td>Collection of interlayer edges</td></tr><tr><td>α</td><td>Multilayer</td><td>Aspect (Kivelä et al. 2014) or layer</td></tr><tr><td>Gα</td><td>Multilayer</td><td>A graph of layer α</td></tr><tr><td>Vα</td><td>Multilayer</td><td>Node set in layer α</td></tr><tr><td>Eα</td><td>Multilayer</td><td>Edge set in a layer α</td></tr><tr><td>L</td><td>Multilayer</td><td>Collection of layers</td></tr><tr><td>Lα</td><td>Multilayer</td><td>Elementary layer α</td></tr><tr><td>M</td><td>Multilayer</td><td>Multilayer network model</td></tr><tr><td>M</td><td>Multilayer</td><td>The supra-adjacency matrix</td></tr><tr><td>Aα</td><td>Multilayer</td><td>The adjacency matrix of layer α</td></tr><tr><td>Iαβ</td><td>multilayer</td><td>The interlayer edges between layer α and β</td></tr><tr><td>kiα</td><td>Multilayer</td><td>The degree of node i in layer α</td></tr><tr><td>QM</td><td>Multilayer</td><td>Modularity for multiplex networks</td></tr><tr><td>ρc</td><td>Miscellaneous</td><td>Redundancy index</td></tr><tr><td>δ</td><td>Miscellaneous</td><td>Kronecker delta</td></tr></table>

where $\alpha$ is called aspect, and $\mathcal{L}_{\alpha}$ depicts an elementary layer. The layer is a product of elementary layers, which can be represented as $\mathcal{L}_1\times \mathcal{L}_2\times \dots \times \mathcal{L}_d$ . The illustration of the multilayer network is given in Fig. 6.

Another model is proposed by Boccaletti et al. (2014), defined as

$$
\mathcal {M} = (\mathcal {G}, \mathcal {C}), \tag {3}
$$

where $\mathcal{G} = \{G_{\alpha}; \alpha \in \{1, \dots, L\}\}$ is a family of (directed or undirected weighted or unweighted) graphs $G_{\alpha} = (V_{\alpha}, E_{\alpha})$ , which represents layers of $\mathcal{M}$ and $\mathcal{C}$ depicts the

![](images/a38c61cce2f566286073a1700f09515bf9423502948b40314cd2e9431e612097.jpg)

![](images/6580260e2c8771b128816a25902d702b77ac5719f4d1a1746a41e375111618ce.jpg)  
Fig. 6 The illustrated multilayer network model proposed by Kivelä et al. As shown in the left panel, the multilayer network has a total of four nodes, so $V = 1,2,3,4$ . There are two aspects, which has corresponding elementary-layer sets $\mathcal{L}_1 = \{A,B\}$ and $\mathcal{L}_2 = \{X,Y\}$ . Therefore, there are four layers: $(A,X),(A,Y),(B,X)$ , and $(B,Y)$ . The right panel shows the representation of the conventional graph model with multiple labels of nodes

interactions between nodes of any two different layer, given by

$$
\mathcal {C} = \left\{E _ {\alpha \beta} \subseteq V _ {\alpha} \times V _ {\beta}; \alpha , \beta \in 1, \dots , L, \alpha \neq \beta \right\}, \tag {4}
$$

The two models differ in the definition of "aspect". Kivelä's model takes into account real-life situations, e.g., a social network contains relations among various types, timeline or situations. Each relation set is regarded as an aspect, namely a classification of layers, providing a comprehensive perspective. Thus, it is more considerate. The structure with aspects concept is much richer than that of ordinary networks. Possible aspects include different types of interactions or communication channels, different subsystems, different spatial locations, different points in time, and so on (De Domenico et al. 2016). However, Boccaletti's model is a general form that easy to understand. Specifically, the supra-adjacency matrix is a distinct tool for representation of a multilayer network, defined as

$$
\mathcal {M} = \left[ \begin{array}{c c c c} A _ {1} & I _ {1 2} & \dots & I _ {1 L} \\ I _ {2 1} & A _ {2} & \dots & I _ {2 L} \\ \vdots & \vdots & \ddots & \dots \\ I _ {L 1} & I _ {L 2} & \dots & A _ {L} \end{array} \right] \in \mathcal {R} ^ {N \times N}, \tag {5}
$$

where $A_{1}, A_{2}, \ldots, A_{L}$ are the adjacency matrix of layer 1, 2, $\ldots, L$ , respectively. $N$ is the total number of nodes, which can be calculated by $N = \sum_{1 \leq l \leq L} |V^{l}|$ . The non-diagonal block $I_{\alpha \beta}$ represents the inter-layer edges of layer $\alpha$ and layer $\beta$ . Thus,

![](images/2ce7abf7b827005f8ad4f60a3f162bc400767a88e9291d8437f0df00b85cae57.jpg)  
Fig. 7 Supra-adjacency representation of 9/11 terrorists' network. The supra-adjacency matrix is represented as a block matrix, where the rows and columns depict the terrorists. The diagonal blocks indicate the interactions, while the non-diagonal blocks represent the that terrorists are simultaneously active on different respects of observed social actions

the interlayer edges can be represented as

$$
I = \bigcup_ {\alpha , \beta = 1, \alpha \neq \beta} ^ {L} I _ {\alpha \beta}. \tag {6}
$$

Take the above-mentioned 9/11 terrorists' network for instance, the supra-adjacency matrix is represented in Fig. 7.

We also list some specific networks that can be represented by a multilayer network manner in the following.

Multiplex network is a special case of multilayer networks (Solá et al. 2013), where all layers share the same set of nodes but may have multiple types of interactions. Some works also use multi-relational networks, multidimensional networks (Berlingerio et al. 2011b) or edge-colored networks (no interlayer edges) for substitution. The underlying limitations exist in the network is node-aligned, i.e. each layer has the same nodes but merely differs in edges. Multiplex network is a special form of multilayer network, where the number of nodes in each layer is consistent, and the nodes are one-to-one correspondence. This network model simplifies the complexity of the general multilayer network form and is therefore widely used to deal with some special problems (Kanawati 2015).

Temporal networks (or time-varying networks) differ with conventional dynamic networks, which focus more on the ordinal variations of connections (Kostakos 2009).

![](images/5adc0ace28b9cc33ee6d4d7ae3c572029b6a14200cc765cdac3c994b96c4f304.jpg)  
Fig. 8 The monolayer network representation of the relationships of characters in "Game of Thrones" of the first five seasons, which contains 796 characters and 2823 links among them. The size of the node depicts the degree centrality, e.g., Tyrion Lannister, Jon Snow, and Daenerys Targaryen, as three key roles in the story, have larger degrees

![](images/43054b0625650498ae2754e764866c65b081742be74eeb311fdffca5bdf53a27.jpg)  
Fig. 9 The multilayer network representation of the relationships of characters in "Game of Thrones" of the first five seasons. Each layer represents a season and the links between the ordinal layers represent the corresponding relationship of characters across different seasons

The temporal network has its own set of research models (Kempe et al. 2002; Kostakos 2009; Tang et al. 2012a), which can illustrate the dynamic characteristics of temporal networks. But it must be mentioned that the power of a multilayer network is that it can be compatible with the representation of the temporal network and can also describe the dynamic characteristics likewise. For example, the multilayer network model we have introduced (Boccaletti et al. 2014) can regard the network structure at each moment as a layer, and the arrangement between different layers is in chronological order. We collected the relationship of characters in the Game of Thrones (the first five seasons), as shown in Figs. 8 and 9.

The study of $k$ -partite networks starts from the complete $k$ -partite graph (i.e., a set of graph vertices decomposed into $k$ disjoint sets such that no two graph vertices within the same set are adjacent) (Brouwer and Haemers 2012). Thus, the $k$ -partite network is also described as node-colored networks (Kivelä et al. 2014), where the nodes are unacquainted in the same layer but have the other layers' common neighbors with other nodes in the same layer. A sample of $k$ -partite networks is shown in Fig. 10.

![](images/6e4a3d9eab61f3dcc70f14aafcc04541ee1088a43babb938747da226362c10dc.jpg)

![](images/3f785c21c3c84bd10b1af72732fd291d5b8a8fe61807a17e011a5155348861b3.jpg)

![](images/da8a2a0c595dc2ae20f4006adb2eb69af26dec25034522e2e795a526e1048812.jpg)  
Fig. 10 Illustration of a $k$ -partite network, where $k = 3$ . There are no intralayer edges in any layer of the network, while the edges are all inter-connected across adjacent layers

![](images/a621ca3051c7fb516565a2ce4c9ecf364f4eaeb9497179a6583ce3d858fbd955.jpg)

![](images/d7b42e3f0793eefe4e79b4ad8d51971acb02fc20b201d7e2f74354db04cf46f0.jpg)  
Fig. 11 Illustration of South women activities network, which is a typical bipartite network. The qualification of the binary network is to check if there are links between nodes in the same type. All the links are connecting nodes with different types of nodes, corresponding with the non-diagonal elements as shown in the middle panel. The right panel presents the corresponding supra-adjacency matrix

However, a special case of $k$ -partite (i.e., $k = 2$ ) networks is the bipartite network (or two-mode network), which is more accessible by us. For example, a customer's transaction records of products can be represented by a "customer-product" network. Fig. 11 presents a classic bipartite network, namely, South women activities network (Davis et al. 2009).

# 2.2 Features

Many basic metrics such as centrality, node similarity, are commonly used by community detection algorithms in monolayer networks. While in multilayer networks, the metrics need to be reformulated and adapted. Thus, in this section, some most important features of multilayer networks are introduced. Studies of structural properties include descriptors to identify the most central nodes according to various notions of importance (Battiston et al. 2014; De Domenico et al. 2015c, 2013; Halu et al. 2013; Solá et al. 2013) and quantify triadic relations such as clustering and transitivity (Battiston et al. 2014; Cozzo et al. 2015; De Domenico et al. 2013).

# 2.2.1 Centrality

The ranking of nodes in multilayer networks is one of the most pressing and challenging tasks that research on complex networks is currently facing (Lü et al. 2016). Many centrality measures have been used for single-layer networks to rank the importance of the nodes, such as degree centrality (Bonacich 1972), betweenness centrality (Freeman 1977), closeness centrality (Freeman 1978), $k$ -shell centrality (Carmi et al. 2007), eigenvector centrality (Bonacich 1987), PageRank (Brin and Page 1998), Leader-Rank (Lü et al. 2011), Local Centrality (Chen et al. 2012), Bridge-Rank (Salavati et al. 2018), and so on. The above measures are widely used in the monolayer network model, while the study on nodes centrality in multilayer networks is still an open issue.

Before introducing the centrality in networks, we will first cover the neighborhood in a multiplex network. There are two definitions for the neighborhood in a multiplex network: One is given by a restrict aggregation concept that $j$ is a neighbor of $i$ if $j$ is connected to $i$ in each layer. The other is loosely defined as $j$ is connected to $i$ in at least one layer (Kazienko et al. 2010). Alhajj and Rokne (2014) give a trade-off definition of the above two methods: $j$ is a neighbor of $i$ if it is connected to $i$ in at least $m$ layers, where $1 \leq m \leq L$ , and $L$ is the total number of layers. This definition may be capable of analyzing the node's centrality in a multiplex network with numerous layers, but it is not ideal enough for introducing another threshold on the number of layers for consideration. However, when applying the concept in a general form of multilayer networks, there are also some problems to be solved. The first problem is the nodes' relationships in a multilayer network, i.e., there may be no corresponding nodes of $i$ in other layers, e.g. in a $k$ -partite network where the nodes of each layer are totally different. The second problem is about the presentation format of the node's neighborhood, the degree centrality can hardly distinguish a node with the same amount of interlayer edges and intralayer edges. The third problem is whether to give equal consideration of weights on interlayer edges and intralayer edges.

In monolayer networks, one of the main centrality measures is the degree of each node: the more links a node has, the more important the node is. The centrality of node $i$ , e.g. the degree of a node $i$ in a multiplex network is the vector (Battiston et al. 2014), defined as

$$
k _ {i} = \left(k _ {i} ^ {1}, k _ {i} ^ {2}, \dots k _ {i} ^ {L}\right), \tag {7}
$$

where $k_{i}^{L}$ is the degree of node $i$ in the $L^{th}$ layer. We can also convert it into another form for simplification, given by

$$
k _ {i} = \sum_ {l = 1} ^ {L} k _ {i} ^ {l}. \tag {8}
$$

Another solution for the centrality of multilayer nodes is through the diffusion across multiple layers. Examples of this measure include influential nodes identification, viral marketing, information diffusion, and so on (Lü et al. 2016). The network structure is more complicated with higher computational complexity. Some scholars

believe that the interlayers edges should be under consideration discriminatively when calculating the node centrality, while others insist on the viewpoint that the interlayer edges make the same contribution to the centrality calculation. Likewise, the degree centrality measures used in multilayer networks can be substituted by other measures, e.g. Eigenvector centrality (Solá et al. 2013).

Above all, in a general form of multilayer network, the nodes may differ in each layer and we cannot obtain the corresponding nodes and calculate the centrality of the nodes directly. This problem has drawn much attention in recent years, including some popular endeavors such as user identification across multiple networks (Carmagnola and Cena 2009; Feng et al. 2017; Liang et al. 2015; Yang et al. 2018), social network coalescence, network alignment (Bayati et al. 2013), and so on.

# 2.2.2 Correlations

Multilayer networks encode significantly more information than their isolated single layers, since they incorporate correlations between the nodes in different layers and between the statistical properties of layers. The correlations of multilayer networks include interlayer degree correlations (Nicosia and Latora 2015), layers overlapping (Kao and Porter 2018), and so on. Some scholars point out that the communities in multilayer networks should consider for overlapping features, while allowing the communities to affect each layer in a different way, including arbitrary mixtures of assortative, disassortative, or directed structure (De Bacco et al. 2017). A definition of local overlap (Cellai et al. 2016) is defined as

$$
o _ {i} ^ {\alpha \beta} = \sum_ {j} \theta \left(\omega_ {i j} ^ {\alpha}\right) \theta \left(\omega_ {i j} ^ {\beta}\right), \tag {9}
$$

where $o_i^{\alpha \beta}$ is the number of overlapping edges that are incident to node $i$ in both layer $\alpha$ and layer $\beta$ , $\omega_{ij}^{\alpha}$ and $\omega_{ij}^{\beta}$ are the weights of intralayer edges $(i,j)$ in layer $\alpha$ and layer $\beta$ , respectively. $\theta(x) = 1$ if $x > 0$ and $\theta(x) = 0$ otherwise. Based on this concept, Kao et al. propose a method that grouping structurally similar layers in multiplex networks and find meaningful groups of layers (Kao and Porter 2018). Considering the degree in undirected multiplex networks, the connection similarity is defined as

$$
\phi^ {\alpha \beta} = \frac {1}{N} \sum_ {i} \phi_ {i} ^ {\alpha \beta} \in [ 0, 1 ], \tag {10}
$$

where $\phi_i^{\alpha \beta}$ is local similarity, defined as

$$
\phi_ {i} ^ {\alpha \beta} = \frac {o _ {i} ^ {\alpha \beta}}{k _ {i} ^ {\alpha} + k _ {i} ^ {\alpha} - o _ {i} ^ {\alpha \beta}}. \tag {11}
$$

The correlations of nodes are of great importance in analyzing multilayer network structures. Zhan et al. improved the community detection algorithms in multi-relational

social networks by utilizing triangles and latent factor cosine similarity prior methods (Zhan et al. 2018). The local topological correlations between any pair of nodes from different layers are also utilized to calculate the centrality of nodes in multilayer networks (Kuncheva and Montana 2015).

# 3 Methods

Communities are known as groups of nodes that are more strongly or frequently connected among themselves than with the remainders (Aldecoa and Marín 2013). Although connecting patterns with other members are possible, they usually have higher linking probability within the group (Fortunato and Hric 2016). Community detection is a fundamental issue in network science, and most existing approaches have been developed for monolayer networks. However, many complex systems are composed of coupled networks through different layers, where each layer represents one of many possible types of interactions.

# 3.1 Problem statement

Communities in single-layer networks comprise a group of well-connected nodes, while in multilayer networks, communities reveal the relationships among nodes in various layers. The comparison of communities on toy examples in single-layer networks and multilayer networks is given in Fig. 12.

The problem of community detection algorithms in multilayer networks is to divide the network into a set of disjoint cohesive modules $C_1, C_2, \ldots, C_k$ where each module $C_k$ is comprised of a group of nodes densely connected inside and loosely connected outside the community. It can be described as

$$
\cup_ {i = 1} ^ {k} C _ {i} = \sum_ {\alpha = 1} ^ {L} V _ {\alpha}, \tag {12}
$$

with $\cap_{i=1}^{k} C_i = \phi$ for non-overlapping community detection and $\cap_{i=1}^{k} C_i \neq \phi$ for overlapping scenarios. Dalibard (Dalibard 2012) gives the three requirements about community detection works as followings:

(1) The community detection should allow for overlapping communities.   
(2) The detected results should be statistically significant, which means applying the algorithm on a random null model should return no communities.   
(3) The detected results should be hierarchical.

Specifically, most of the existing approaches can scarcely satisfy the requirements and hold reasonable efficiency. Besides, the evaluation metrics are also varied from one to another, as introduced in the following subsection.

![](images/37184b51261bb925f914da375718624c3f6c538597dd9aaf97326f79482b051d.jpg)  
Community 1   
Community 2

![](images/523801265414ee152896c0795311681bbc65cea11d1126ceb9af16c2f457c550.jpg)  
(a)   
(b)   
Fig. 12 Comparison of communities in monolayer and multilayer networks. The nodes with different gray levels represent different communities. (a) Communities in a monolayer network. (b) Communities in a two-layered network, each community shares nodes in Layer 1 and Layer 2

# 3.2 Evaluation functions

Quality evaluation for partition results is a complex task due to the lack of a shared and universally accepted definition of community structures. A wide variety of quality functions have been proposed to solve the community detection problem from different perspectives. Several representative metrics are analyzed and classified into three categories (Cazabet et al. 2015):

(1) Single score metrics   
(2) Evaluation on generated networks   
(3) Evaluation on real networks with ground truth

Single score metrics employ quality functions associating a score to the community detection result. For example, the number of edges between partitions can be utilized to evaluate the performance of a given algorithm. Popular metrics include Modularity (Newman and Girvan 2004), Modularity density (Li et al. 2008), surprise (Aldecoa and Marín 2011), and so on. These metrics are universal but often criticized with no consensus of the several meaningful levels of partitions. The comparison with generated networks, e.g., LFR benchmark (Lancichinetti et al. 2008) is widely used to compare the partitions with the community affiliations. It is easy to recognize a good community and capable of evaluating variations in usual communities. How-

ever, sometimes the generated networks are not realistic and differ from the partitions we want. The comparison of real networks with ground truth seems to be a reasonable solution. Normalized Mutual Information (Danon et al. 2005), Precision, Recall, and $F_{1}$ -score (Herlocker et al. 2004; Perry et al. 1955) are representative methods. However, these methods depend on the priori partition labels, which are probably unknown in most of real-world datasets.

# 3.2.1 Modularity, modularity density, performance and surprise

In 2004, modularity (Newman and Girvan 2004) was first proposed by Newman as an evaluation for community partitions, defined as

$$
Q = \frac {1}{2 m} \sum_ {i j} \left(A _ {i j} - \frac {k _ {i} k _ {j}}{2 m}\right) \delta \left(C _ {i}, C _ {j}\right), \tag {13}
$$

where $m$ is the number of edges, $A_{ij}$ is the element of the adjacency matrix, $\delta(C_1, C_2)$ equals 1 if $i$ and $j$ are in the same community, otherwise 0. In 2010, Mucha et al. proposed $Q_M$ (Mucha et al. 2010) for evaluating community in time-dependent, multiscale and multiplex networks, defined as

$$
Q _ {M} = \frac {1}{2 \mu} \left\{\left(A _ {i j \alpha} - \gamma \frac {k _ {i \alpha} k _ {j \alpha}}{2 m _ {\alpha}}\right) \delta_ {\alpha \beta} + \delta_ {i j} C _ {j \alpha \beta} \right\} \delta \left(g _ {i \alpha}, g _ {j \beta}\right), \tag {14}
$$

where $\mu$ denotes the number of links in multiplex networks, $\gamma$ is the resolution parameter. $A_{ij\alpha}$ represents the adjacency matrix of nodes in layer $\alpha$ . $C_{j\alpha \beta}$ represents interlayer edge connecting node $j$ among layer $\alpha$ and layer $\beta$ . This metric introduces a coupling between communities in neighboring layers by allowing interlayer edges, while different $\gamma$ enables the detection of different scale communities. However, the range of $\gamma$ is required to be manually determined, which may be unable to obtain reasonable results without an appropriate $\gamma$ . Afterward, a variational version of $Q_{M}$ is given (Pramanik et al. 2017) as

$$
Q = \frac {1}{2 m} \sum_ {i j} \left(A _ {i j} - P _ {i j}\right) \delta \left(\psi_ {i}, \psi_ {j}\right), \tag {15}
$$

where $\delta(\psi_i, \psi_j)$ is the Kronecker delta function, it equals 1 iff $\psi_i = \psi_j$ , i.e. $i$ and $j$ belong to the same community and 0 otherwise. The penalty term $P_{ij}$ is the expected probability of existing an edge between nodes $i$ and $j$ if edges are placed at random as

$$
P _ {i j} = \left\{ \begin{array}{l l} P _ {i j} ^ {1}, & \text {i f} i \in V ^ {1} \& j \in V ^ {1} \\ P _ {i j} ^ {2}, & \text {i f} i \in V ^ {1} \& j \in V ^ {2} \\ P _ {i j} ^ {1 2}, & \text {i f} i \in V ^ {1} \& j \in V ^ {2} \text {o r} i \in V ^ {2} \& j \in V ^ {1} \end{array} , \right. \tag {16}
$$

where $P_{ij}^{1}$ can be calculated by $P_{ij}^{1} = (h_{i}\times h_{j}) / (2|E^{1}|)$ and $P_{ij}^{2} = (h_{i}\times h_{j}) / (2|E^{2}|)$ and $P_{ij}^{12} = (c_i\times c_j) / (2|I_{12}|)$ . $h_i$ and $h_j$ are the intralayer degrees of nodes $i$ and $j$ , and $c_{i}$ and $c_{j}$ are the respective coupling degrees of $i$ and $j$ . $|I_{12}|$ depicts the amount of all the interlayer edges among layers $L_{1}$ and $L_{2}$ . Likewise, the multiplex modularity is also criticized by resolution limits (Vaiana and Muldoon 2018).

Modularity density (Li et al. 2008) was proposed to solve the resolution limits problem, defined as

$$
D = \sum_ {i = 1} ^ {c} \frac {L \left(V _ {i} , V _ {i}\right) - L \left(V _ {i} , \bar {V} _ {i}\right)}{\left| V _ {i} \right|}, \tag {17}
$$

where $c$ is the total number of communities, $|V_i|$ is the number of nodes of the $i$ -th community. $L(V_i, V_i) = \Sigma_{j \in V_i, k \in V_i} A_{jk}$ denotes the number of edges among $i$ -th community and $L(V_i, \bar{V}_i) = \Sigma_{j \in V_i, k \in \bar{V}_i} A_{jk}$ denotes the number of connections between the $i$ -th community and other communities.

In 2010, Performance (Fortunato 2010) was proposed as a community quality function, which mainly considered the number of correctly "interpreted" pairs of nodes (i.e., the nodes belonging to the same community and connected by an edge, and nodes belonging to different communities and not connected by an edge), defined as

$$
\mathcal {P} = \frac {\left| \left\{(i , j) \in E , C _ {i} = C _ {j} \right\} \right| + \left| \left\{(i , j) \notin E , C _ {i} \neq C _ {j} \right\} \right|}{n (n - 1) / 2}, \tag {18}
$$

where $E$ denotes the edges set, $C_i$ and $C_j$ denote the $i$ -th and $j$ -th communities, respectively, $n$ is the total number of nodes. Specifically, Coverage (Fortunato 2010) was defined as the ratio of the number of intra-community edges by the total number of edges, given as

$$
\mathcal {C} = \frac {\left| \left\{(i , j) \in E , C _ {i} = C _ {j} \right\} \right|}{n (n - 1) / 2}. \tag {19}
$$

In 2011, Aldecoa and Marín (2011) proposed the "Surprise" as a measure for detecting communities. Different from merely considering the number of edges required in a partition, this metric takes the number of nodes into account, and it is capable of resolving the resolution limit problem and detecting small communities (Fortunato and Barthelemy 2007; Lancichinetti and Fortunato 2011). It is an interesting function to measure how impossible is a given partition compared to a null model, defined as

$$
S = - \log \sum_ {j = p} ^ {\min  (M, n)} \frac {\binom {M} {j} \binom {F - M} {n - j}}{\binom {F} {n}}, \tag {20}
$$

where $F$ is the maximum possible number of links in the network (i.e. $k[k - 1] / 2$ , being $k$ the number of nodes), $n$ is the observed number of links, $M$ is the maximum possible number of intracommunity links observed in a partition. The parameter $S$ ,

which stands for Surprise, indeed measures the "surprise" (improbability) of finding by chance a partition with the observed enrichment of intracommunity links to a random graph. The authors declare that surprise implicitly assumes a more complex definition of community: a precise number of units for which it is found a density of link which is statistically unexpected given the features of the network, and in 2013, they also designed surprise maximization methods for detecting community structure in complex networks (Aldecoa and Marín 2013). As an effective metric for evaluating community structures, experiments on the human brain network (Fox and Lancaster 2002; Laird et al. 2005) have also proved its priority to modularity (Nicolini and Bifone 2016).

# 3.2.2 MDL, Pareto frontier and redundancy

The fundamental idea behind the MDL principle is that any regularity in a given set of data can be used to compress the data, i.e. to describe it using fewer symbols than needed to describe the data literally (Grunwald 2004). Rosvall et al. convert the community detection task into solving a coding problem following the MDL principle (Rosvall and Bergstrom 2008). Analogously, the objective function of community detection can be considered as a multi-objective optimization problem. The optimal partitioning for a multilayer network is achieved by maximizing a local evaluation indicator (e.g. local modularity (Chen et al. 2018b)) in each layer, i.e.,

$$
C = \arg \max  _ {C} [ f _ {1} (C), f _ {2} (C), \dots , f _ {k} (C) ], \tag {21}
$$

where $k$ denotes the number of communities. However, calculating an exact Pareto front is, in general, a challenging task. The most popular approximate methods are genetic algorithms, which employ biologically inspired heuristics to attempt to transform randomly selected seed cases into solutions on the Pareto front using propagation Multi-objective Management (Caramia and Dell'Olmo 2008).

Berlingerio et al. define the redundancy index $\rho_{c}$ (Berlingerio et al. 2011a), which captures the phenomenon that a set of nodes constitute a community in a dimension tend to constitute communities also in other dimensions. The redundancy figures out the fraction of redundant links in a multi-dimensional network, defined as

$$
\rho_ {c} = \sum_ {(u, v) \in \tilde {P} _ {c}} \frac {\left| \left\{\alpha : \exists (u , v) ^ {\alpha} \in E \right\} \right|}{L \times \left| P _ {c} \right|}, \tag {22}
$$

where $c$ represents a community, $\alpha$ is a layer of all the layers $\mathcal{L} = \{1,2,\dots ,L\}$ , $P$ is a set of node pairs $(u,v)$ existed at least one layer in a multilayer network; $\bar{P}$ is the set of node pairs existed at least two layers. $\bar{P}_c$ is the subset of $P$ appearing in $c$ ; $\bar{P}_c \subseteq \bar{P}$ is the subset of $\bar{P}$ only containing node pairs in $c$ . The more layers connect each pair of nodes within a community, the higher the redundancy will be.

# 3.2.3 Purity, NMI, and ARI

The clustering accuracy measures are widely utilized to evaluate and compare the performance of community detection algorithms on real-world networks with given ground-truth communities. Suppose the computed clusters $\Omega = \{\omega_1, \omega_2, \dots, \omega_k\}$ with respect to the ground truth classes $C = c_{1}, c_{2}, \dots, c_{k}$ . Purity (Zhao and Karypis 2004) represents the percentage of the total number of nodes classified correctly, defined as

$$
P u r i t y (\Omega , C) = \frac {1}{N} \sum_ {k} \max  _ {j} \left| \omega_ {k} \cap c _ {j} \right|, \tag {23}
$$

where $N$ is the total number of nodes, and $|\omega_k \cap c_j|$ depicts the number of nodes in the intersection of $\omega_k$ and $c_j$ . To compromise the quality of the clustering against the number of clusters, we can utilize normalized mutual information (i.e., NMI) (Danon et al. 2005). The confusion matrix is comprised of ground communities and generated partitions, thereby NMI is defined as

$$
N M I (A, B) = \frac {- 2 \sum_ {i = 1} ^ {C _ {A}} \sum_ {j = 1} ^ {C _ {B}} N _ {i j} \log \frac {N _ {i j} N}{N _ {i} N _ {j}}}{\sum_ {i = 1} ^ {C _ {A}} N _ {i} \log \frac {C _ {i}}{N} + \sum_ {j = 1} ^ {C _ {B}} N _ {j} \log \frac {C _ {j}}{N}}, \tag {24}
$$

where $A$ and $B$ denote the ground-truth communities and the detected partitions. $C_A$ and $C_B$ are the number of groups in partition $A$ and $B$ , respectively. $N_{ij}$ depicts the elements of the confusion matrix. $N_i$ is the sum of the elements in row $i$ , $N_j$ is the sum of elements in column $j$ . $N$ is the number of nodes. The range of NMI is [0, 1]. If $A = B$ , $NMI(A, B) = 1$ . If $A$ and $B$ are completely different, $NMI(A, B) = 0$ . Suppose an approximate size $z$ as the number of community sets, the computation of NMI requires $O(z^2)$ comparisons, which is incapable of evaluating partitions for large-scale networks. In order to cope with the high computational complexity of such method in recent years, several approaches (Cazabet et al. 2015; Rossetti et al. 2016a, b), e.g., the precision, recall, and $F_{1\text{-score}}$ are employed, defined as

$$
P r e c i s i o n = \frac {\mathrm {T P}}{\mathrm {T P} + \mathrm {F P}}, \tag {25}
$$

$$
\text {R e c a l l} = \frac {\mathrm {T P}}{\mathrm {T P} + \mathrm {F N}}, \tag {26}
$$

$$
F _ {1 - \text {s c o r e}} = \frac {2 \times (\text {P r e c i s i o n} \times \text {R e c a l l})}{\text {P r e c i s i o n} + \text {R e c a l l}}, \tag {27}
$$

with $\mathrm{TP} =$ true positive, $\mathrm{FP} =$ false positive, $\mathrm{FN} =$ false negative and $\mathrm{TN} =$ true negative. Besides, Rand Index is also a popular measure, which represents the percentage of TP and TN decisions assigns that are correct (i.e. accuracy), defined as

$$
R I (\Omega , C) = \frac {\mathrm {T P} + \mathrm {T N}}{\mathrm {T P} + \mathrm {F P} + \mathrm {F N} + \mathrm {T N}}. \tag {28}
$$

ARI (Schütze et al. 2008) is RI defined to be scaled in the range [0, 1]. All of the metrics in this subsection are in the range of [0, 1], and the higher the metric, the better the clustering quality is.

# 3.3 Algorithms classification

Early approaches either collapse multilayer networks into a weighted single-layer network (Berlingerio et al. 2011a; Tang et al. 2012b; Taylor et al. 2016b, a), or extend the existing algorithms for each layer and then merge the partitions via consensus clustering (Tang et al. 2009; Papalexakis et al. 2013). However, these approaches have been criticized for ignoring the connections across layers, thereby resulting in low accuracy. Research to date has exhibited some novel algorithms for discovering communities in multilayer networks directly (Ma et al. 2018; Pamfil et al. 2019). Generally speaking, the strategies are mainly classified into three categories (Tagarelli et al. 2017):

- Flattening methods   
- Aggregation methods   
- Direct methods

Flattening methods collapse the layers' information into a single layer and then conduct the traditional monolayer algorithms for detecting communities. This strategy is very common in multiplex networks, where the multiplex network is converted into a multi-relationship network (Rocklin and Pinar 2013) or a monolayer network (Kuncheva and Montana 2015).

Aggregation methods discover the communities in each layer and then merge them by a certain aggregation mechanism, which could be useful for removing redundant information. The aggregation process requires $2^{L}$ comparisons and it's very time-consuming for a temporal network with numerous layers. Thus, "layer communities grouping" is proposed to reduce the redundant layers (Kao and Porter 2018). Dalibard proposed a parameter $P_{C_k}^l$ for each layer $l \in L$ to describe the probability of communities, and then aggregate the communities in each layer in terms of a correlation coefficient $\rho_{\alpha \beta}$ between layers $\alpha$ and $\beta$ (Dalibard 2012). Numerical experiments on synthetic multilayer networks show that the analysis fails in aggregated networks, whereas the multilayer method can accurately identify modules across layers that originate from the same interaction process (De Domenico et al. 2015a). Thus, aggregation is not recommended.

Direct methods aim to detect the community structures directly on the multilayer network by optimizing some quality-assessment criteria without flattening (Oselio et al. 2015). For example, Pramanik et al. defined a multilayer modularity index, i.e., $Q_{M}$ , and combined with the improved GN and Louvain algorithms, namely GN- $Q_{M}$ and Louvain- $Q_{M}$ , respectively (Pramanik et al. 2017).

There has been plenty of endeavors made in the last decades, however, research for multilayer networks is still in infancy (Tagarelli et al. 2017), and it is promising to extract communities without collapsing multilayer networks. Several representative methods are introduced in the following subsections.

Table 3 Similarity measures for MNLPA algorithm   

<table><tr><td>Metric in monolayer networks</td><td>Modification for multilayer networks</td></tr><tr><td>JC(i, j) = |Γ(i)∩Γ(j)|/|Γ(i)∪Γ(j)|</td><td>∑Lα=1(1/2ωα(|N|/|U| + |N&#x27;|/|U|) / ∑Lα=1ωα</td></tr><tr><td>CN(i, j) = |Γ(i)∩Γ(j)|</td><td>∑Lα=1(1/2ωα(|N|+|N|)/ ∑Lα=1ωα</td></tr><tr><td>AA(i, j) = ∑z∈|Γ(i)∩Γ(j)| 1/log kz</td><td>∑kα=1(1/2ωα(∑z∈N 1/√log|Γα|/z)+ ∑z∈N 1/√log|Γα|/z)) / ∑Lα=1ωα</td></tr></table>

$\Gamma (i)$ and $\Gamma (i)$ denote the neighbors of node $i$ and node $j$ , respectively. $\omega_{\alpha}$ denotes the weight of layer $\alpha$ , $N = \Gamma_{\mathrm{out\_j}}^{\alpha} \cap \Gamma_{\mathrm{in\_i}}^{\alpha}$ , $\hat{N} = \Gamma_{\mathrm{in\_j}}^{\alpha} \cap \Gamma_{\mathrm{out\_i}}^{\alpha}$ , $U = \Gamma_{\mathrm{out\_j}}^{\alpha} \cup \Gamma_{\mathrm{in\_i}}^{\alpha}$ and $\hat{U} = \Gamma_{\mathrm{out\_j}}^{\alpha} \cup \Gamma_{\mathrm{in\_i}}^{\alpha}$ . $\Gamma_{\mathrm{out\_j}}^{\alpha}$ and $\Gamma_{\mathrm{in\_i}}^{\alpha}$ denote out-neighbors and in-neighbors of node $j$ and node $i$ in layer $\alpha$ , respectively. $\bar{Z}$ is the intersection of $j$ 's out-neighbors and $i$ 's in-neighbors. $\hat{Z}$ is the intersection of $j$ 's in-neighbors and $i$ 's out-neighbors

# 3.3.1 Improved label propagation algorithm

Label propagation algorithms utilize the propagation features of networks and have linear complexity and reasonable results. These methods allow the nodes to adopt new characteristics depending on the behavior of their neighbors, e.g. adopts labels of the biggest amount of its neighbors. The process of LPA is shown as follows:

Step 1: Traverse the network and assign a unique label to each node.   
Step 2: Establish a random order of the node's revision.   
Step 3: Each node is revised in the assigned order and adopts the most frequent label of its neighbors.   
Step 4: The process is performed iteratively until the algorithm converges and no label changes occur anymore.

Inspired by the traditional LPA process, Alimadadi et al. (2019) redefined the neighborhood in multilayer networks and then proposed MNLPA to detect communities in a weighted and directed Facebook activity network. The algorithms are summarized as followings:

Step 1: Each node $u$ is initiated with a unique label, then the neighbors of $u$ in all layers are obtained, marked as $N_{u}$ .   
Step 2: Calculate the similarity (measures are listed in Table 3) of $u$ between the node $v$ in $N_u$ , and mark $v$ when the similarity score is more than a given threshold $\sigma$ .   
Step 3: Repeat the following steps until the stop criterion is satisfied:

- Nodes are ordered randomly;   
- For each node $u$ , each marked similar neighbor sends out its label to $u$ , and mark the node $u$ with maximum labels.

Step 4: Divide communities by the nodes with the same labels.

The MNLPA is praiseworthy with efficiency, and capable of dealing with weighted and directed networks, but also criticized by instability. The partition result is sensitive

![](images/3a6b727f716b94ab49011fdafef1156cf810e166b523dba5fbd6b41b00d9ea7e.jpg)  
Fig. 13 Comparison of LPA and MNLPA algorithms conducting on Facebook datasets. The homophily obtained by MNLPA algorithm is marked by gray bars, thus the resultant communities are similar and fit the definition of the community to some extent

![](images/00e00e590817e080cc01b74721a26ee6712c3c822dc70302f902605c257c0da0.jpg)

![](images/6582fcbaa211374a5146c9642bf398794f58cf9e8ac4c3501e04f799d81e2960.jpg)

![](images/92b2a1c5af4f7e45edd9b3c18f2372fee0920c85f3288cbd8423a0b2a395faf2.jpg)

![](images/458f1ee8f82f16a505f1cf05b57c01133bbff8bff9beb80d2036846f5e1cf4c7.jpg)

![](images/264c31a2590822368e5857cfb1adf672fdd78e94d07603d2ba1c7971b405a211.jpg)

![](images/b91d7ad6050c5d3d6ad08de06d56d5dffef526d980b268cc53aade36b17a2237.jpg)  
Fig. 14 Three synthetic multilayer networks for evaluating the proposed MNLPA. The top three panels show the multilayer networks, in which each has three layers with community labels generated by LFR benchmark (Lancichinetti et al. 2008). The bottom three panels show the relevant supra-adjacency matrices

to the threshold parameter in MNLPA and the density of the network dataset. As they declared, the MNLPA algorithm is verified by experiments on real-world datasets and the results are reprinted in Fig. 13.

As the Facebook datasets are not provided in their works, we construct several synthetic three-layered networks, as plotted in Fig. 14.

Experiments on the constructed synthetic networks suggest that MNLPA algorithm is fastidious about the parameters. The changing of modularity with the varying threshold $\delta$ is plotted, as shown in Fig. 15.

As shown in Fig. 15, the performance of the proposed MNLPA is not satisfied. With an increasing threshold in [0.2, 0.4], the modularity increased, thus we guess the threshold should be greater than 0.4 approximately. The Facebook datasets utilized in

![](images/4d66dafb4737b2c6202ca06aa35c1eba1361be435626e34478b15fa22e96e735.jpg)  
Fig.15 The modularities tendency obtained by the MNLPA algorithm conducting on three different synthetic networks changes with the varying threshold $\delta$

the experiments are weighted and directed, which may make prominent differences in the experimental results. In brief, the MNLPA is suitable for large-scale networks under certain conditions (e.g., weighted and directed), while for general multilayer networks, it's necessary to make modifications to improve the performance.

# 3.3.2 Nonnegative matrix factorization methods

Nonnegative matrix factorization (NMF) was proposed by Lee and Seung (2001). It aims to factorize the original nonnegative matrix into the product of two other nonnegative matrices. For applications in community detection methods, the original nonnegative matrix can be an adjacency matrix, thereby the objective (loss) function can be presented as

$$
\min  _ {U \geq 0, V \geq 0} L (A, U V ^ {T}) = \left\| A - U V ^ {T} \right\| _ {F} ^ {2}, \tag {29}
$$

where $A$ is a $n \times n$ adjacency matrix, and both $U$ and $V$ are $n \times k$ matrices. The rank $k$ corresponds to the number of divided communities. It has been widely utilized in detecting communities in complex networks (Jiao et al. 2017; Liu et al. 2017; Wu et al. 2018).

Recently, Ma et al. applied this method (S2j-NMF) to community detection for multilayer networks (Ma et al. 2018). They propose a quantitative function (i.e. multilayer network modularity density) and prove the trace optimization of multilayer modularity density is equative to the objective functions of the community detection algorithms (e.g. $k$ -means (MacQueen 1967), NMF, spectral clustering (Ng et al. 2002), multiview clustering for multilayer networks, etc.). The modularity density $Q_{D}$ for $\{V_{c}\}_{c = 1}^{k}$ is defined as

$$
Q _ {D} \left(\left\{V _ {c} \right\} _ {c = 1} ^ {k}\right) = \sum_ {c = 1} ^ {k} \frac {L \left(V _ {c} , V _ {c}\right) - L \left(V _ {c} , \bar {V} _ {c}\right)}{\left| V _ {c} \right|}, \tag {30}
$$

where $Q_{D}(\{V_{c}\}_{c = 1}^{k})$ is the modularity density of community partitions, $k$ is the number of partitions, $\bar{V}_c$ depicts the partitions after removing $V_{c}$ . $L(V_{i},V_{j})$ calculates the connections between $V_{i}$ and $V_{j}$ , defined as

$$
L \left(V _ {i}, V _ {j}\right) = \sum_ {p \in V _ {i}, q \in V _ {j}} w _ {p q}, \tag {31}
$$

where $p$ and $q$ are the nodes of the partition $V_{i}$ and $V_{j}$ , respectively. $w_{pq}$ is the weight of the edge $(p, q)$ and equals 1 in unweighted networks. The objective function is transformed into a multi-objective optimization problem as

$$
Q _ {D} ^ {\mathcal {G}} \left(\left\{V _ {c} \right\} _ {c = 1} ^ {k}\right) = \frac {1}{m} \sum_ {l = 1} ^ {m} \sum_ {c = 1} ^ {k} \frac {L ^ {l} \left(V _ {c} , V _ {c}\right) - L ^ {l} \left(V _ {c} , \bar {V} _ {c}\right)}{\left| V _ {c} \right|}, \tag {32}
$$

where $L^{l}(V_{i}, V_{j})$ is the same with equation (31) applied in $l$ layer. $Q_{D}^{\mathcal{G}}(\{V_{c}\}_{c=1}^{k})$ is the objective function of the partitions in multilayer network $\mathcal{G}$ . Thus, the optimal partitioning $\{V_{c}\}_{c=1}^{k}$ for the multilayer network by maximizing the modularity density in each layer can be represented as

$$
\left\{ \begin{array}{l} \max  \left(Q _ {D} ^ {1} \left(\left\{V _ {c} \right\} _ {c = 1} ^ {k}\right)\right) \\ \max  \left(Q _ {D} ^ {2} \left(\left\{V _ {c} \right\} _ {c = 1} ^ {k}\right)\right) \\ \dots \\ \max  \left(Q _ {D} ^ {m} \left(\left\{V _ {c} \right\} _ {c = 1} ^ {k}\right)\right) \end{array} . \right. \tag {33}
$$

Afterward, dense subgraphs are discovered by employing a greedy search strategy in multilayer networks. The conventional NMF algorithm combined with a factorized basis matrix and various coefficient matrices are applied to each layer. Finally, the experiments are conducted on several datasets, which verifies the proposed method.

The complexity of this method is $O(mn^2 k)$ , where $m$ is the number of layers and $k$ is the number of partitions. Thus, it is probably not acceptable for large-scale networks. Besides, as the authors mentioned, the algorithm is based on multiplex networks, which is not capable of handling the general form of multilayer networks. The algorithm relies on prior information and the number of target communities, and the decomposition process might be time-consuming.

# 3.3.3 Random walk methods

Kuncheva et al. propose a community detection algorithm, namely LART (Locally Adaptive Random Transitions) for the detection of communities that are shared by either some or all the layers in multiplex networks (Kuncheva and Montana 2015). They employ the supra-adjacency matrix $\check{M}$ and define the transition probabilities of four possible moves among the nodes, described as

$$
\left\{ \begin{array}{l} P _ {(i, j) (i, k)} = \frac {\check {M} (i , k) (i , k)}{k _ {i , k}} \\ P _ {(i, j) (j, k)} = \frac {\check {M} (i , k) (j , k)}{k _ {i , k}} \\ P _ {(i, k) (i, l)} = \frac {\check {M} (i , k) (i , l)}{k _ {i , k}} \\ P _ {(i, j) (j, l)} = 0 \end{array} , \right. \tag {34}
$$

where $k_{i,k}$ is the multiplex degree of node $v_{i}^{k}$ in $\check{M}$ defined as $k_{i,k} = \sum_{j,l}\check{M}_{(i,k)(j,l)}$ , $P_{(i,j)(j,k)}$ depicts the transition probability from node $i$ of layer $k$ (i.e., $v_{i}^{k}$ ) to node $j$ of the layer $l$ (i.e., $v_{j}^{l}$ ). The probability to move from node $v_{i}^{k}$ to node $v_{j}^{l}$ is zero when $i \neq j$ and $k \neq l$ since there cannot exist a direct move where there is no connection. The transition probabilities are represented as a matrix $\mathcal{P}$ of the random walk process and written as

$$
\mathcal {P} = \mathcal {D} ^ {- 1} \check {M}, \tag {35}
$$

where $\mathcal{D}$ is the diagonal matrix defined by the multiple node degrees. A dissimilarity matrix $S(t)$ which depends on the multiplex random walk of steps $t$ is defined. The dissimilarity matrix is defined according to the nodes $i$ and $j$ are in the same layer or different layers, denoted by

$$
S (t) _ {(i, k) (j, k)} = \sqrt {\sum_ {h = 1} ^ {N} \sum_ {m = 1} ^ {L} \frac {\left(\mathcal {P} _ {(i , k) (h , m)} ^ {t} - \mathcal {P} _ {(j , k) (h , m)} ^ {t}\right) ^ {2}}{k (h , m)}}, \tag {36}
$$

$$
S (t) _ {(i, k) (j, l)} = \sqrt {s _ {1} + s _ {2} + s _ {3}}, \tag {37}
$$

where $s_1, s_2, s_3$ are defined as

$$
\left\{ \begin{array}{l} s _ {1} = \sum_ {h = 1} ^ {N} \left(\frac {\mathcal {P} _ {(i , k) (h , k)} ^ {t}}{\sqrt {k (h , k)}} - \frac {\mathcal {P} _ {(j , l) (h , l)} ^ {t}}{\sqrt {k (h , l)}}\right) ^ {2} \\ s _ {2} = \sum_ {h = 1} ^ {N} \left(\frac {\mathcal {P} _ {(i , k) (h , l)} ^ {t}}{\sqrt {k (h , l)}} - \frac {\mathcal {P} _ {(j , l) (h , k)} ^ {t}}{\sqrt {k (h , k)}}\right) ^ {2} \\ s _ {3} = \sum_ {h = 1} ^ {N} \sum_ {m = 1; m \neq k, l} ^ {L} \frac {\left(\mathcal {P} _ {(i , k) (h , m)} ^ {t} - \mathcal {P} _ {(j , k) (h , m)} ^ {t}\right) ^ {2}}{k (h , m)} \end{array} . \right. \tag {38}
$$

Afterward, the agglomerative clustering is utilized to merge nodes in communities. The multiplex modularity $Q_{M}$ is employed to evaluate the quality of partitions. The process of LART is shown as follows:

Step 1: Assign each node in each layer to its own community.

Step 2: Merge nodes based on the average linkage criterion using the distance matrix $S$ and obey the principle of the merged community has at least one within-layer or interlayer connection.

![](images/e984a682b8d9b3be8beb0d1599de48a81098b5efdbf102c51a4595245b9e44e0.jpg)  
Fig.16 The comparison of the proposed LART algorithm with MM and PMM algorithms (Kuncheva and Montana 2015) on the five simulated scenarios of the synthetic network

Step 3: Merge the nodes only if the maximum $Q_{M}$ is reached.   
Step 4: Obtain the shared and non-shared communities.

The experiments are conducted on synthetic multiplex networks, and the experimental results are shown in Fig. 16.

The proposed LART algorithm is conducting on five different scenarios and the experimental result demonstrates the performance of the proposed algorithm. However, the algorithm is limited to multiplex networks, and the real-world networks are much more complicated. Hence, the performance of the LART algorithm for real-world datasets is uncertain.

# 3.3.4 Multi-objective optimization methods

Pizzuti and Socievole (2017) proposed the Multi-layer many-objective Optimization algorithm (MLMaOP), in which they formulated the community detection problem in multilayer networks as a many-objective optimization problem and a given objective is contemporarily optimized on all the network layers. In their work, they give the multi-objective optimization problem (MOP) as

$$
\min  _ {x} F (x) = \left(f _ {1} (x), f _ {2} (x), \dots , f _ {d} (x)\right) \text {s u b j e c t} x \in X, \tag {39}
$$

where $d$ is the number of objective functions, $x = (x_{1}, x_{2}, \ldots, x_{n}) \in X$ is the decision vector with a domain of definition $X \subseteq R^{n}$ , $F: X \to Z$ is the mapping from the decision space $X$ to the objective space $Z$ . When $d \geq 3$ , an MOP is referred to as Many Objective Optimization Problem (MaOP) (Farina and Amato 2002). Pareto-dominance relation is used to define a partial ordering in the objective space. Thus, the problem of community detection in multilayer networks using MaOP is defined as

$$
\min  F (P) = \left(F _ {1} (P), F _ {2} (P), \dots , F _ {d} (P)\right) \text {s u b j e c t} P \in \Omega , \tag {40}
$$

where each $F_{\alpha} : \Omega \to R$ computes the value of the objective function only on the layer $G_{\alpha}$ . For the main purpose is to get a maximized $Q$ , so $F_{\alpha}(P) = -Q_{\alpha}(P)$ means that the greater $Q$ , the smaller $F_{\alpha}(P)$ partitions on each layer. The main process of MLMaOP is shown as follows:

Step 1: Initialize a rand partition by using the adjacency matrix of projected $M$ .

![](images/9fd0434c48b697343845d85c1fceff25aab116abc15fa9e5b7b56d3c7ce1fec8.jpg)  
Fig. 17 The comparison of MLMaOP algorithm with competitors on SSRM dataset (Loe and Jensen 2015)

Step 2: Traverse the partition in all the layers, evaluate the objection function on $G_{\alpha}$ to obtain $F_{\alpha}(P)$ . Assign a rank based on Pareto dominance and then combine parents and offspring partition into fronts.

Step 3: Select the best points, and apply the variation operators and create the next partition.

Step 4: Choose a solution from the Pareto front.

The comparison of MLMaOP algorithm with other approaches (Loe and Jensen 2015) is shown in Fig. 17.

The proposed algorithm with three different strategies is competitive on the partitions $P_{1}$ , while on $P_{2}$ and $P_{3}$ , we can see that the NMI results obtained by the other algorithms are better. Besides, the MLMaOP algorithm suffers from a low convergence rate to Pareto front and is likely to be time-consuming for detecting communities in large-scale networks.

# 3.4 Discussion

In the last decade, a plethora of approaches have been proposed to address the community detection problem with enormous network data. We list several representative methods (from 2009 to 2019), as shown in Table 4.

Table 4 shows that most of the presented methods are holding relatively high complexity, where GN- $Q_{M}$ , Louvain- $Q_{M}$ and LART methods are based on multiplex modularity maximization and unfavorable on general multilayer networks. Moreover, the majority are designed for multiplex networks, which require the nodes in each layer should be aligned. As we have introduced in the previous subsection, some improved version of classic monolayer algorithms, e.g., GenLouvain has been regarded as a benchmark and is really worth expecting for general multilayer networks. We can anticipate four prospective directions, i.e., random walk-based method, tensor decomposition, nonnegative matrix factorization, and modularity optimization will receive increasing attention over time.

In addition to the above-mentioned directions, quite a part of algorithms focus on overlapping community detection (Liu et al. 2018) and local community detection (Interdonato et al. 2017; Jeub et al. 2015; Li et al. 2019). On the one hand,

Table 4 A brief comparison of community detection methods in multilayer networks   

<table><tr><td>Name</td><td>Classification</td><td>Strategy</td><td>Complexitya</td><td>Network</td></tr><tr><td>PMM (Tang et al. 2009)</td><td>Aggregation</td><td>Multilayer modularity maximization</td><td>O(n3L)</td><td>Multi-dimensional</td></tr><tr><td>MULTICLUS (Papalexakis et al. 2013)</td><td>Aggregation</td><td>Minimum description length</td><td>O(mnIC2)</td><td>Multiplex, bipartite</td></tr><tr><td>GRAPHFUSE (Papalexakis et al. 2013)</td><td>Aggregation</td><td>Tensor analysis</td><td>O(n3L)</td><td>Multiplex</td></tr><tr><td>ABACUS (Berlingerio et al. 2013)b</td><td>Aggregation</td><td>Multilayer modularity maximization</td><td>-</td><td>Multi-dimensional</td></tr><tr><td>DNMF (Jiao et al. 2017)</td><td>Aggregation</td><td>Nonnegative matrix factorization</td><td>O(n2L)</td><td>Multiplex, temporal</td></tr><tr><td>EMCD (Tagarelli et al. 2017)c</td><td>Aggregation</td><td>Modularity maximization</td><td>O(I(m+LC))</td><td>Multiplex</td></tr><tr><td>Multilink (Mondragon et al. 2018)</td><td>Aggregation</td><td>Multilink similarity</td><td>O(m2)</td><td>Multiplex</td></tr><tr><td>M-EMCD* (Mandaglio et al. 2018)</td><td>Aggregation</td><td>Modularity maximization</td><td>O(I(m+LC))</td><td>Multiplex</td></tr><tr><td>M-Motif (Huang et al. 2019)</td><td>Aggregation</td><td>Merge partitions across layers</td><td>O(n log(n)L2)</td><td>Multilayer</td></tr><tr><td>MEMM (Zhang et al. 2019)</td><td>Aggregation</td><td>Multilayer edge mixture model</td><td>O(n2)</td><td>Multiplex</td></tr><tr><td>HSBM (Paez et al. 2019)</td><td>Aggregation</td><td>Hierarchical SBM and Bayes</td><td>O(n2LC2)</td><td>Multiplex</td></tr><tr><td>Variational-Bayes (Ali et al. 2019)</td><td>Aggregation</td><td>SBM and variational Bayes</td><td>O(n2L2C)</td><td>Multiplex, weighted</td></tr><tr><td>GenLouvain (Jutla et al. 2011)</td><td>Direct</td><td>Multiplex map equation</td><td>O(n2 log n)</td><td>Multiplex, temporal</td></tr><tr><td>MultiGA (Amelio and Pizzuti 2014b)</td><td>Direct</td><td>Genetic representation</td><td>O(In2L)</td><td>Multiplex</td></tr><tr><td>MultiMOGA (Amelio and Pizzuti 2014a)</td><td>Direct</td><td>Multi-objective optimization</td><td>O(LCn2)</td><td>Multiplex</td></tr><tr><td>CLAN (Dabideen et al. 2014)</td><td>Direct</td><td>Variational label propagation</td><td>O(LInK)</td><td>Multiplex, temporal</td></tr><tr><td>LART (Kuncheva and Montana 2015)</td><td>Direct</td><td>Random walk</td><td>O(m3)</td><td>Multiplex</td></tr><tr><td>Multiplex-Infomap (De Domenico et al. 2015a)</td><td>Direct</td><td>Multiplex map equation</td><td>O(n2)</td><td>Multiplex</td></tr><tr><td>LocalNCPs (Jeub et al. 2015)d</td><td>Direct</td><td>Local random walk</td><td>≥ O(nIKL)</td><td>Multiplex</td></tr><tr><td>BAZZI (Bazzi et al. 2016)</td><td>Direct</td><td>Multilayer modularity maximization</td><td>O(nI2L)</td><td>Multiplex, temporal</td></tr><tr><td>ML-LCD (Interdonato et al. 2017)</td><td>Direct</td><td>Local objective function maximization</td><td>≥ O(C3K2L)</td><td>Multiplex</td></tr></table>

Table 4 continued   

<table><tr><td>Name</td><td>Classification</td><td>Strategy</td><td>Complexitya</td><td>Network</td></tr><tr><td>GN-QM (Pramanik et al. 2017)</td><td>Direct</td><td>Maximum betweenness edges removal</td><td>O(nm2)</td><td>Multiplex</td></tr><tr><td>Louvain-QM (Pramanik et al. 2017)</td><td>Direct</td><td>Modularity optimization</td><td>O(m3)</td><td>Multiplex, weighted</td></tr><tr><td>MLMaOP (Pizzuti and Socievole 2017)e</td><td>Direct</td><td>Multi-objective Optimization</td><td>-</td><td>Multiplex</td></tr><tr><td>NFC (Aslak et al. 2018)</td><td>Direct</td><td>Random walk and Infomap</td><td>O(m(n+m))</td><td>Multiplex, temporal</td></tr><tr><td>S2-jNMF (Ma et al. 2018)</td><td>Direct</td><td>Nonnegative matrix factorization</td><td>O(rn2km)</td><td>Multiplex</td></tr><tr><td>MNLPA (Alimadadi et al. 2019)</td><td>Direct</td><td>Label Propagation</td><td>O(nk)</td><td>Multiplex, directed, weighted</td></tr><tr><td>IterModMax (Pamfil et al. 2019)</td><td>Direct</td><td>SBM and Modularity maxminimization</td><td>O(n2L)</td><td>Multiplex, temporal</td></tr><tr><td>TMSCD (Kuncheva and Montana 2017, 2019)</td><td>Direct</td><td>Spectral graph wavelet</td><td>O(n2L)</td><td>Multiplex, temporal</td></tr><tr><td>CMNC (Chen et al. 2019)</td><td>Direct</td><td>Tensor Decomposition</td><td>O(n3L)</td><td>Multiplex</td></tr><tr><td>MCD-Berlingerio (Berlingerio et al. 2011a)f</td><td>Flattening</td><td>Employing monolayer algorithms</td><td>-</td><td>Multi-dimensional</td></tr><tr><td>CDHIA (Tang et al. 2012b)</td><td>Flattening</td><td>network integration and k-means</td><td>≥ O(nICK)</td><td>Multi-dimensional</td></tr><tr><td>AggregationPan (Pan et al. 2018)</td><td>Flattening</td><td>Cutting edges with weight &lt; threshold</td><td>O((m+n)L)</td><td>Multiplex, weighted</td></tr><tr><td>ParticleGao (Gao et al. 2019)</td><td>Flattening</td><td>Particle Competition</td><td>O(nICL2)</td><td>Multiplex, directed, weighted</td></tr></table>

$n$ and $m$ are the number of nodes and edges, respectively. $L$ is the number of layers, $K$ is the average degree of nodes, $I$ denotes the iteration times.   
$b$ The complexity of ABACUS depends on the complexity of the employed monolayer algorithms, e.g., $O(n)$ from LPA (Raghavan et al. 2007) and with total complexity of $O(LCN^2)$ .   
$I$ denotes the number of iterations to convergence at a local optimum; $C$ is the number of communities.   
Local NCPs is a local community detection algorithm. For a given node, the complexity is approximately $O(L\ln K / C)$ . For $C$ partitions, the minimum complexity is $O(L\ln K)$ .   
e The complexity of MLMaOP depends on an uncertain convergence process, thereby marked with $-$   
This algorithm provides a framework via flattening a multi-dimensional network into a weighted network, and then employs the existing monolayer algorithms for community detection, thereby the complexity is uncertain

with the increasing of network scale, global computation becomes time-consuming, which promoting local community detection into our view. Liu et al. (2017) proposed an improved multi-objective evolutionary approach for community detection in multilayer networks. Aiming at solving the local community detection problem, they employ a string-based representative scheme and genetic operation and local search. However, the algorithm adapts the strategy of conducting the Louvain algorithm (Blondel et al. 2008) on each layer and then merges the partitions, which seems to deviate from the multilayer community concept. More than that, comparisons with other competitors are not provided. On the other hand, overlapping communities are also ubiquitous in multilayer networks (De Domenico et al. 2016), i.e., some nodes are attached to multiple partitions simultaneously (Chen et al. 2016). Kao and Porter (2018) proposed a method based on computing pairwise similarities between layers and then executing community detection for grouping structurally similar layers in multiplex networks. The algorithm is verified in both synthetic and empirical multiplex networks. As most of the compared algorithms are designed for multiplex networks, there's still a great deal of works to do in community detection in the general multilayer networks.

In brief, the research on community detection for multilayer networks is just in its infancy. At the time of this writing, there is still no standard algorithms for general multilayer networks and quite a few problems remain to be solved, such as the optimization of algorithm process to avoid time-consuming procedures, the extending of algorithms for applying in general form of multilayer networks, the simplification of mathematical model, and so on.

# 4 Applications

The study of detecting community structures in multilayer networks is experiencing a blossom in the last decade. Relevant researches cover various aspects among our daily life such as analyzing influential users in multiple social platforms (Al-Garadi et al. 2018), finding organization of proteins in a biological system (Gosak et al. 2018) and managing urban transportation system with various traffic manners (Liu et al. 2019), etc. The following subsections summarized applications of community detection via a multilayer network framework.

# 4.1 Temporal networks partition

Community detection in temporal networks, i.e., temporal community detection, is required to find how communities emerge, grow, combine, and decay in an evolving process (Kawadia and Sreenivasan 2012). A common approach to detect temporal communities is to obtain communities independently in each snapshot by utilizing static methods and then map the partitions between two snapshots together as many as possible. Obviously, it fails to achieve the goal of revealing the evolving process because such methods do not adequately use partitions found in past snapshots to inform the identification for the optimal partition on the current snapshot (Jiao et al.

2017). Thus, as the foremost step of modeling, the traditional graph model is incapable of presenting inter-connections in a temporal network.

Multilayer network model is commonly employed in the study of time-varying networks (or temporal networks, multi-slice networks), in which the time snapshots are modeled as layers and the layers are ordered by a certain sequence. However, there's a foundational question: Across how many layers must a community persist in order for layer aggregation to benefit detection? To solve this problem, a layer aggregation approach (De Domenico et al. 2014) is proposed to reduce data size or as a data filter to benefit network-analysis outcomes. Since Mucha et al. (2010) introduced the multiplex modularity optimization method, numerous attempts were made in this field (Drugan et al. 2011; Nguyen et al. 2011; Li and Garcia-Luna-Aceves 2013), which opened up a upsurge in unveiling the communities in time-varying networks. Taylor et al. (2017) proposed the random matrix theory and found layer aggregation to significantly influence detectability. The detectability limitation is described as the ability of network structure to form a community, i.e., if the community structure is too weak, it cannot be found upon inspection of the network (Lancichinetti and Fortunato 2011). When the aggregative network corresponds to the summation of the adjacency matrices encoding the network layers, aggregation always improves detectability. The research is beneficial to understand the contraction of network layers and analyze pairwise-interaction data to obtain sparse network representations. The application of layer aggregation can be used for anomaly detection in network data, e.g., in cybersecurity, detecting harmful events such as attacks, intrusions, and fraud.

# 4.2 Transportation networks optimization

On account of the critical role of transportation system in modern society, the study on traffic dynamics has become one of the most successful applications of complex network theroy. However, the vast majority of researches treat transportation networks as an isolated system, which is inconsistent with the fact that many complex networks are interrelated in a nontrivial way (Du et al. 2016). Analogously, the transportation system has a variety of traffic manners, such as bus, subway, tram, high-speed train, airline, ship, etc, hence a comprehensive study should cover many of such manners. Early researches of traffic networks mainly focus on a single traffic way and ignore the interactions between their counterparts (Calimente 2012; Chen et al. 2014). Du et al. (2016) utilized a two-layered traffic network to study the distribution-based strategy and improved the generating rate of passengers using a particle swarm optimization algorithm. The multilayer network model utilized in this work is an idealized transportation system, in which each layer has a different topology and supports different traveling speeds. The passengers are allowed to travel along the path of minimal traveling time and with the additional cost they can transfer from one layer to another to avoid congestion. The research indicates that a degree centrality-based strategy is not overly beneficial in enhancing the performance of the system. However, starting from such a strategy and reassigning transfer costs using a particle swarm optimization algorithm improve the capacity and several other properties of the system at a reasonable computational cost. The research is rewarding to the selection of traffic manners and

exemplifies how multilayer network models are applied in the urban transportation system.

Inspired by the complex network theory and the multilayer network representation, Hong and Liang (2016) analyze the Chinese airline transportation system with the multilayer network framework, in which each layer is defined by a commercial airline (company) and the weights of links are set by the number of flights, the number of seats and the geographical distance between pairs of airports, respectively. By calculating the clustering coefficient, average shortest path length, and assortativity coefficient of the airports, the research has shown that the Chinese airline is of considerably higher value of a maximal degree and betweenness than the other top airlines. Ding et al. (2018) proposed a method for measurements in areas of Kuala Lumpur (i.e., the national capital of Malaysia) to detect communities. The multilayer network model employed in their research contains the railway layer and urban street layer, which mainly focuses on detecting the changing structures of a rail network and mining in urban network communities. The experimental results suggest that rail network growth triggers structural and community changes, i.e., when an upper-layer rail network grows from a simple tree-like network to a more intricate form, the network diameter and average shortest path length decrease dramatically. The growth of the network allows the remainder of the network to be easily visited, which provides suggestive patterns for city development. Yildirimoglu and Kim (2018) analyzed the urban traffic network by combing bus lines, passenger trajectories, and vehicle trajectories together and formed a three-layered network. By applying the Louvain algorithm (Blondel et al. 2008) independently on the three layers, they found that aggerated patterns can shape geographically well-connected communities in the urban traffic network. The spatial structure is quite alike for the bus and passenger layers, which benefits transit authority in making location decisions. The research is beneficial from a planning perspective that sub-regional borders designate the influential areas around local centers, shopping districts, school zones, etc., and cities can develop policies in order to improve the accessibility to them and enhance network performance.

# 4.3 Social network analysis

Another hot-point of community detection research in multilayer networks is social network analysis (Alhajj and Rokne 2014). Social networks have been studied fairly extensively over the last couple of decades, mainly in the general context of analyzing interactions between people in order to determine important structural patterns in such interactions. With the utilization of plentiful data resources from online social media such as Facebook, Twitter, and Flickr, there's an increasing tendency in discovering community structures in such time-varying social networks (Alimadadi et al. 2019; Rozario et al. 2019; Zhou et al. 2007, 2016). The emergence of online social networks has altered millions of web users' behavior so that their interactions with each other produce huge amounts of data on various activities. Facebook and Twitter, as the top-two popular social media in our daily life, have been widely employed for social network analysis in recent years (Alimadadi et al. 2019; Turker and Sulak 2018).

The analysis of social networks is usually accompanied by various applications such as information propagation, internal trades analysis, influential spreaders identification, and so on. Diffusion processes, like the propagation of information or the spreading of diseases, are fundamental phenomena occurring in social networks. While the study of diffusion processes in single networks has received a great deal of interest from various disciplines for over a decade, diffusion on multilayer networks is still a young and promising research area, presenting many challenging research issues (Salehi et al. 2015). Numerous attempts have been made to uncover the community structures in international trades, typically represented as bipartite networks in which connections can be established between countries and industries (Alves et al. 2019). Biondo et al. (2017) present a multilayer network model with contagion dynamics, which is able to simulate the spreading of information and the transactions phase of a typical financial market. In their two-layered network framework, the first layer comprises the trading decisions of investors, and the second layer is constructed of the information dynamics, which is fruitfully beneficial to explain the aggregate behavior of markets. Basaras et al. (2017) proposed an effective method to detect influential spreaders in multilayer networks based on the underlying community structures. The experimental evaluation shows that the proposed method outperforms the major competitors proposed so far for either single-layer or multilayer networks.

The above-mentioned applications mainly focus on a local structure or some certain context-based community detection for the case of large volume and various dynamic changes of networks. Thus, it is of great significance in designing some smart algorithms to mine the valuable information among plentiful social network resources.

# 4.4 Research on biological systems

Biological systems, from a cell to the human brain, are intrinsically complex (Ma'ayan 2017). Multilayer networks, described by an intricate network of relationships across multiple scales, are most widely employed in representing such systems. The majority of the biological processes are constituted by a group of proteins that are connected densely (Cui et al. 2012). The protein-protein interaction (PPI) network contains the communications among the protein groups that communicate with each other closely, which can be used to predict the complexity of the function of normal proteins (Srihari et al. 2017). In general, there are two typical protein communities: protein complexes and protein functional modules. Protein complexes are sets of proteins that interact with each other to execute a single multimolecular mechanism. Protein functional modules are sets of proteins that participate in a particular biological process, and interact with each other at different time and places (Spirin and Mirny 2003). Recently, several studies are highlighting how simple networks, i.e., obtained by aggregating or neglecting temporal or categorical descriptions of biological data, are not able to account for the richness of information characterizing biological systems (De Domenico 2018). Chen et al. (2018a) proposed an MLPCD algorithm by integrating Gene Expression Data (GED) and a parallel solution of MLPCD using cloud computing technology. They reconstructed the weighted protein-protein interaction (WPPI) network by combining PPI network and related GED, and then defined simplified modularity as the ratio of

in-degrees and out-degrees of proteins in a community. By utilizing an improved Louvain algorithm (Blondel et al. 2008), they have achieved the goal of detecting protein complexes and protein function modules.

Although non-overlapping communities are more commonly studied in network neuroscience, a model of community structure that allows for overlapping networks offers a more realistic presentation of brain network organization (Wu et al. 2011). Taking overlapping communities into consideration, Zhang et al. (2018a) propose a central edge selection (CES) based community detection algorithms for PPI networks. Experimental results on three benchmark networks and two PPI networks indicate the excellent performance of the proposed CES algorithm. Kurmukov et al. (2017) propose a framework to compare both overlapping and non-overlapping community structures of brain networks within the machine learning settings. The performance of the proposed framework is verified in the task of classifying Alzheimer's disease, mild cognitive impairment, and healthy participants. Pan et al. (2018) present an aggregation approach to detect communities in multilayer biological networks, which first constructs a consensus graph form multiple networks and then applies traditional algorithms to detect communities. Inspired by the fact of few shared edges existed among different networks, they merge the weights of edges from different layers and cut off the nodes with low weights. The approach is simple but limited by the application scenarios.

Another notable direction of biological research is about human brain networks. Cantini et al. (2015) propose a multi-network-based strategy to integrate different layers of genomic information and use them in a coordinated way to identify diving cancer genes. The multi-networks they focus, combine transcription factor co-targeting, microRNA, cotargeting, protein-protein interaction, and gene co-expression networks. The combination of different layers benefits extracting from the multi-networks indications on the regulatory pattern and functional role of both the already known and the new candidate diver genes. Sanchez-Rodriguez et al. (2019) introduce an approach for the detection of a modular organization by considering the temporal scales of the information flow over large-scale brain graphs, and several organizational patterns existing in the brain anatomical and functional networks are found. The structures may coexist together, in a dynamical way that is given by the temporal scales of the activity they produce, guaranteeing functional independence and coordination.

In brief, discovering the underlying patterns in biological networks is experiencing a blossom. With the development of network science, multi-biological networks provide plentiful data resources than ever before, which requires us to dedicate more to this promising field.

# 5 Outlook

As interdisciplinary research with a variety of prospective applications, complex network has been receiving increasing attention from the scientific community. Inspired by prosperous real-world scenarios such as social networks, biological networks, and transportation networks, extensive researches have been dedicated to the extraction of non-trivial knowledge from such networks. Along with the further study, scholars

come to realize many systems are inherently represented by a multilayer network, in which edges exist in multiple layers that encode differently but potentially related, types of interactions, and it is important to uncover the interlayer community structures in a complex system.

This paper first presents the various formats of multilayer networks and then introduces the two basic mathematical models. Subsequently, the quality evaluation measures and several typical community detection algorithms are introduced, including label propagation-based algorithm, nonnegative matrix factorization, random walk methods, and multi-objective optimization methods, and so on. After a comprehensive analysis of the above-mentioned methods, we conclude that most of the existing methods are designed for multiplex networks, i.e., the nodes in each layer are aligned, which limits the research on universal multilayer network format. Besides, the algorithms are with high computational complexity and can hardly obtain reasonable partitions among large-scale multilayer networks. A great deal of works remain to be done in the future, such as designing more efficient algorithms for temporal networks with numerous layers and exploring the community structures in special formats of multilayer networks.

Acknowledgements We would like to thank the anonymous reviewers for their careful reading and useful comments that helped us to improve the final version of this paper. This work is partially supported by Liaoning Natural Science Foundation under Grant No. 20170540320, the Science Research Project of Liaoning Provincial Department of Education under Grant Nos. L2015167, L2015173, the Doctoral Scientific Research Foundation of Liaoning Province under Grant No. 20170520358.

# Compliance with ethical standards

Conflicts of interest The authors declare that they have no conflict of interest.

Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.

# References

Ahmed I, Witbooi P, Christoffels A (2018) Prediction of human-bacillus anthracis protein-protein interactions using multi-layer neural network. Bioinformatics 34(24):4159-4164   
Al-Garadi MA, Varathan KD,avana SD, Ahmed E, Mujtaba G, Khan MUS, Khan SU (2018) Analysis of online social network connections for identification of influential users: survey and open research issues. ACM Comput Surv 51(1):1-37   
Aldecoa R, Marín I (2011) Deciphering network community structure by surprise. PLoS ONE 6(9):e24195   
Aldecoa R, Marín I (2013) Surprise maximization reveals the community structure of complex networks. Sci Rep 3(1):1-9   
Alhajj R, Rokne J (2014) Encyclopedia of social network analysis and mining. Springer, Berlin

Ali HT, Liu S, Yilmaz Y, Couillet R, Rajapakse I, Hero A (2019) Latent heterogeneous multilayer community detection. In: ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 8142-8146   
Alimadadi F, Khadangi E, Bagheri A (2019) Community detection in facebook activity networks and presenting a new multilayer label propagation algorithm for community detection. Int J Mod Phys B 33(10):1950089   
Alves LG, Mangioni G, Cingolani I, Rodrigues FA, Panzarasa P, Moreno Y (2019) The nested structural organization of the worldwide trade multi-layer network. Sci Rep 9(1):1-14   
Amelio A, Pizzuti C (2014a) Community detection in multidimensional networks. In: 2014 IEEE 26th international conference on tools with artificial intelligence. IEEE, pp 352-359   
Amelio A, Pizzuti C (2014b) A cooperative evolutionary approach to learn communities in multilayer networks. In: International conference on parallel problem solving from nature. Springer, pp 222-232   
Ansari A, Koenigsberg O, Stahl F (2011) Modeling multiple relationships in social networks. J Mark Res 48(4):713-728   
Aslak U, Rosvall M, Lehmann S (2018) Constrained information flows in temporal networks reveal intermittent communities. Phys Rev E 97(6):062312   
Baltakiene M, Baltakys K, Cardamone D, Parisi F, Radicioni T, Torricelli M, de Jeude J, Saracco F (2018) Maximum entropy approach to link prediction in bipartite networks. arXiv preprint arXiv:1805.04307   
Barabási AL, Albert R (1999) Emergence of scaling in random networks. Science 286(5439):509-512   
Basaras P, Iosifidis G, Katsaros D, Tassiulas L (2017) Identifying influential spreaders in complex multilayer networks: a centrality perspective. IEEE Trans Netw Sci Eng 6(1):31-45   
Battiston F, Nicosia V, Latora V (2014) Structural measures for multiplex networks. Phys Rev E 89(3):032804   
Bayati M, Gleich DF, Saberi A, Wang Y (2013) Message-passing algorithms for sparse network alignment. ACM Trans Knowl Discov Data 7(1):1-31   
Bazzi M, Porter MA, Williams S, McDonald M, Fenn DJ, Howison SD (2016) Community detection in temporal multilayer networks, with an application to correlation networks. Multiscale Model Simul 14(1):1-41   
Berlingerio M, Coscia M, Giannotti F (2011a) Finding and characterizing communities in multidimensional networks. In: 2011 international conference on advances in social networks analysis and mining. IEEE, pp 490-494   
Berlingerio M, Coscia M, Giannotti F (2011b) Finding redundant and complementary communities in multidimensional networks. In: Proceedings of the 20th ACM international conference on Information and knowledge management, pp 2181-2184   
Berlingerio M, Coscia M, Giannotti F, Monreale A, Pedreschi D (2011c) The pursuit of hubbiness: analysis of hubs in large multidimensional networks. J Comput Sci 2(3):223-237   
Berlingerio M, Pinelli F, Calabrese F (2013) Abacus: frequent pattern mining-based community discovery in multidimensional networks. Data Min Knowl Disc 27(3):294-320   
Biondo AE, Pluchino A, Rapisarda A (2017) Informative contagion dynamics in a multilayer network model of financial markets. Ital Econ J 3(3):343-366   
Black J (2018) Urban transport planning: theory and practice, vol 4. Routledge, Abingdon   
Blondel VD, Guillaume JL, Lambiotte R, Lefebvre E (2008) Fast unfolding of communities in large networks. J Stat Mech Theory Exp 2008(10):P10008   
Boccaletti S, Bianconi G, Criado R, Del Genio CI, Gomez-Gardenes J, Romance M, Sendina-Nadal I, Wang Z, Zanin M (2014) The structure and dynamics of multilayer networks. Phys Rep 544(1):1-122   
Bollobás B (2013) Modern graph theory, vol 184. Springer, Berlin   
Bonacich P (1972) Factoring and weighting approaches to status scores and clique identification. J Mathematical Sociology 2(1):113-120   
Bonacich P (1987) Power and centrality: a family of measures. Am J Sociol 92(5):1170-1182   
Brin S, Page L (1998) The anatomy of a large-scale hypertextual web search engine. Comput Netw ISDN Syst 30(1-7):107-117   
Brouwer AE, Haemers WH (2012) Distance-regular graphs. In: Axler S, Capasso V, Casacuberta C, MacIntyre AJ, Ribet K, Sabbah C, Süli E, Woyczynski WA (eds) Spectra of graphs. Springer, pp 177-185   
Brummitt CD (2014) Models of systemic events: interdependence, contagion, and innovation. University of California, Davis   
Buldyrev SV, Parshani R, Paul G, Stanley HE, Havlin S (2010) Catastrophic cascade of failures in interdependent networks. Nature 464(7291):1025-1028

Cai D, Shao Z, He X, Yan X, Han J (2005) Community mining from multi-relational networks. In: European conference on principles of data mining and knowledge discovery. Springer, pp 445-452   
Caliente J (2012) Rail integrated communities in Tokyo. J Trans Land Use 5(1):19-32   
Cantini L, Medico E, Fortunato S, Caselle M (2015) Detection of gene communities in multi-networks reveals cancer drivers. Sci Rep 5:17386   
Caramia M, Dell'Olmo P (2008) Multi-objective management in freight logistics: increasing capacity, service level and safety with optimization algorithms. Springer, Berlin   
Carmagnola F, Cena F (2009) User identification for cross-system personalisation. Inf Sci 179(1-2):16-32   
Carmi S, Havlin S, Kirkpatrick S, Shavitt Y, Shir E (2007) A model of internet topology using k-shell decomposition. Proc Nat Acad Sci 104(27):11150-11154   
Cazabet R, Chawuthai R, Takeda H (2015) Using multiple-criteria methods to evaluate community partitions. arXiv preprint arXiv:1502.05149   
Cellai D, Dorogovtsev SN, Bianconi G (2016) Message passing theory for percolation models on multiplex networks with link overlap. Phys Rev E 94(3):032301   
Chen D, Lu L, Shang MS, Zhang YC, Zhou T (2012) Identifying influential nodes in complex networks. Phys A 391(4):1777-1787   
Chen D, Huang X, Wang D, Jia L (2014) Public transit hubs identification based on complex networks theory. IETE Techn Rev 31(6):440-451   
Chen D, Jia L, Sima D, Huang X, Wang D (2016) Community detection algorithm with membership function. In: Yuan H, Bian F, Geng J (eds) International conference on geo-informatics in resource management and sustainable ecosystem. Springer, pp 185-195   
Chen J, Li K, Bilal K, Metwally AA, Li K, Yu P (2018a) Parallel protein community detection in large-scale PPI networks based on multi-source learning. IEEE/ACM Trans Comput Biol Bioinform. https://doi.org/10.1109/TCBB.2018.2868088   
Chen S, Wang ZZ, Tang L, Tang YN, Gao YY, Li HJ, Xiang J, Zhang Y (2018b) Global vs local modularity for network community detection. PLoS ONE 13(10):e0205284   
Chen Z, Chen C, Zheng Z, Zhu Y (2019) Tensor decomposition for multilayer networks clustering. Proc AAAI Conf Artif Intell 33:3371-3378   
Colladon AF, Remondi E (2017) Using social network analysis to prevent money laundering. Expert Syst Appl 67:49-58   
Cozzo E, Kivelä M, De Domenico M, Solé-Ribalta A, Arenas A, Gómez S, Porter MA, Moreno Y (2015) Structure of triadic relations in multiplex networks. New J Phys 17(7):073029   
Cui G, Shrestha R, Han K (2012) Modulesearch: finding functional modules in a protein-protein interaction network. Comput Methods Biomech Biomed Eng 15(7):691-699   
Dabideen S, Kawadia V, Nelson SC (2014) Clan: An efficient distributed temporal community detection protocol for manets. In: 2014 IEEE 11th international conference on mobile ad hoc and sensor systems. IEEE, pp 91-99   
Dalibard V (2012) Community detection in multi-layer networks. Master's thesis, University of Cambridge   
Danon L, Diaz-Guilera A, Duch J, Arenas A (2005) Comparing community structure identification. J Stat Mech Theory Exp 20055(09):P09008   
Davis A, Gardner BB, Gardner MR (2009) Deep south: a social anthropological study of caste and class. University of South Carolina Press, Columbia   
De Bacco C, Power EA, Larremore DB, Moore C (2017) Community detection, link prediction, and layer interdependence in multilayer networks. Phys Rev E 95(4):042317   
De Domenico M (2018) Multilayer network modeling of integrated biological systems. arXiv preprint arXiv:1802.01523   
De Domenico M, Solé-Ribalta A, Cozzo E, Kivelä M, Moreno Y, Porter MA, Gómez S, Arenas A (2013) Mathematical formulation of multilayer networks. Phys Rev X 3(4):041022   
De Domenico M, Nicosia V, Arenas A, Latora V (2014) Layer aggregation and reducibility of multilayer interconnected networks. arXiv preprint arXiv:1405.0425   
De Domenico M, Lancichinetti A, Arenas A, Rosvall M (2015a) Identifying modular flows on multilayer networks reveals highly overlapping organization in interconnected systems. Phys Rev X 5(1):011027   
De Domenico M, Porter MA, Arenas A (2015b) Muxviz: a tool for multilayer analysis and visualization of networks. J Compl Netw 3(2):159-176   
De Domenico M, Solé-Ribalta A, Omodei E, Gómez S, Arenas A (2015c) Ranking in interconnected multilayer networks reveals versatile nodes. Nat Commun 6(1):1-6

De Domenico M, Granell C, Porter MA, Arenas A (2016) The physics of spreading processes in multilayer networks. Nat Phys 12(10):901-906   
Ding R, Ujang N, bin Hamid H, Abd Manan MS, He Y, Li R, Wu J (2018) Detecting the urban traffic network structure dynamics through the growth and analysis of multi-layer networks. Phys A 503:800-817   
Drugan OV, Plagemann T, Munthe-Kaas E (2011) Detecting communities in sparse manets. IEEE/ACM Trans Netw 19(5):1434-1447   
Du WB, Zhou XL, Jusup M, Wang Z (2016) Physics of transportation: towards optimal capacity using the multilayer network framework. Sci Rep 6:19059   
Farina M, Amato P (2002) On the optimal solution definition for many-criteria optimization problems. In: 2002 annual meeting of the North American fuzzy information processing society proceedings. NAFIPS-FLINT 2002 (Cat. No. 02TH8622). IEEE, pp 233-238   
Farooq MJ, Zhu Q (2018) On the secure and reconfigurable multi-layer network design for critical information dissemination in the internet of battlefield things (iobt). IEEE Trans Wireless Commun 17(4):2618-2632   
Feng S, Wang Q, Shen D, Kou Y, Nie T, Yu G (2017) User identification across social networks based on global view features. In: 2017 14th web information systems and applications conference (WISA). IEEE, pp 93-98   
Fortunato S (2010) Community detection in graphs. Phys Rep 486(3-5):75-174   
Fortunato S, Barthelemy M (2007) Resolution limit in community detection. Proc Nat Acad Sci 104(1):36-41   
Fortunato S, Hric D (2016) Community detection in networks: a user guide. Phys Rep 659:1-44   
Fox PT, Lancaster JL (2002) Mapping context and content: the BrainMap model. Nat Rev Neurosci 3(4):319-321   
Freeman LC (1977) A set of measures of centrality based on betweenness. Sociometry 40:35-41   
Freeman LC (1978) Centrality in social networks conceptual clarification. Soc Netw 1(3):215-239   
Gao J, Buldyrev SV, Havlin S, Stanley HE (2011) Robustness of a network of networks. Phys Rev Lett 107(19):195701   
Gao X, Zheng Q, Verri FA, Rodrigues RD, Zhao L (2019) Particle competition for multilayer network community detection. In: Proceedings of the 2019 11th international conference on machine learning and computing, pp 75-80   
Girvan M, Newman ME (2002) Community structure in social and biological networks. Proc Nat Acad Sci 99(12):7821-7826   
Gosak M, Markovič R, Dolenšek J, Rupnik MS, Marhl M, Stožer A, Perc M (2018) Network science of biological systems at different scales: a review. Phys Life Rev 24:118-135   
Grunwald P (2004) A tutorial introduction to the minimum description length principle. arXiv preprint arXiv:math/0406077   
Halu A, Mondragon RJ, Panzarasa P, Bianconi G (2013) Multiplex pagerank. PLoS ONE 8(10):e78293   
Herlocker JL, Konstan JA, Terveen LG, Riedl JT (2004) Evaluating collaborative filtering recommender systems. ACM Trans Inf Syst 22(1):5-53   
Hong C, Liang B (2016) Analysis of the weighted chinese air transportation multilayer network. In: 2016 12th world congress on intelligent control and automation (WCICA). IEEE, pp 2318-2321   
Huang L, Wang CD, Chao HY (2019) Higher-order multi-layer community detection. Proc AAAI Conf Artif Intell 33:9945-9946   
Interdonato R, Tagarelli A, Ienco D, Sallaberry A, Poncelet P (2017) Local community detection in multilayer networks. Data Min Knowl Disc 31(5):1444-1479   
Jeub LG, Mahoney MW, Mucha PJ, Porter MA (2015) A local perspective on community structure in multilayer networks. arXiv preprint arXiv:1510.05185   
Jiao P, Lyu H, Li X, Yu W, Wang W (2017) Temporal community detection based on symmetric nonnegative matrix factorization. Int J Mod Phys B 31(13):1750102   
Jutla IS, Jeub LG, Mucha PJ (2011) A generalized louvain method for community detection implemented in matlab. http://netwikiamathuncedu/GenLouvain   
Kanawati R (2015) Multiplex network mining: A brief survey. IEEE Intell Informatics Bull 16(1):24-27   
Kao TC, Porter MA (2018) Layer communities in multiplex networks. J Stat Phys 173(3-4):1286-1302   
Kawadia V, Sreenivasan S (2012) Sequential detection of temporal communities by estrangement confinement. Sci Rep 2:794

Kazienko P, Brodka P, Musial K (2010) Individual neighbourhood exploration in complex multi-layered social network. In: 2010 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology. IEEE, vol 3, pp 5-8   
Kempe D, Kleinberg J, Kumar A (2002) Connectivity and inference problems for temporal networks. J Comput Syst Sci 64(4):820-842   
Kivelä M, Arenas A, Barthelemy M, Gleeson JP, Moreno Y, Porter MA (2014) Multilayer networks. J Complex Netw 2(3):203-271   
Kostakos V (2009) Temporal graphs. Phys A 388(6):1007-1023   
Kuncheva Z, Montana G (2015) Community detection in multiplex networks using locally adaptive random walks. In: Proceedings of the 2015 IEEE/ACM international conference on advances in social networks analysis and mining 2015, pp 1308-1315   
Kuncheva Z, Montana G (2017) Multi-scale community detection in temporal networks using spectral graph wavelets. In: International workshop on personal analytics and privacy, Springer, pp 139-154   
Kuncheva Z, Montana G (2019) Spectral multi-scale community detection in temporal networks with an application. arXiv preprint arXiv:1901.10521   
Kurmukov A, Ananyeva M, Dodonova Y, Gutman B, Faskowitz J, Jahanshad N, Thompson P, Zhukov L (2017) Classifying phenotypes based on the community structure of human brain networks. Graphs in biomedical image analysis, computational anatomy and imaging genetics. Springer, Berlin, pp 3-11   
Laird AR, Lancaster JL, Fox PT (2005) The social evolution of a human brain mapping database. Neuroinformatics 3(1):65-78   
Lancichinetti A, Fortunato S (2011) Limits of modularity maximization in community detection. Phys Rev E 84(6):066122   
Lancichinetti A, Fortunato S, Radicchi F (2008) Benchmark graphs for testing community detection algorithms. Phys Rev E 78(4):046110   
Lancichinetti A, Fortunato S, Kertesz J (2009) Detecting the overlapping and hierarchical community structure in complex networks. New J Phys 11(3):033015   
Lee DD, Seung HS (2001) Algorithms for non-negative matrix factorization. In: Leen TK, Dietterich TG, Tresp V (eds) Advances in neural information processing systems. MIT Press, pp 556-562. http:// papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf   
Li C, Zhang Y (2020) A personalized recommendation algorithm based on large-scale real micro-blog data. Neural Comput Appl 32:1-8   
Li Q, Garcia-Luna-Aceves J (2013) Opportunistic routing using prefix ordering and self-reported social groups. In: 2013 international conference on computing, networking and communications (ICNC), IEEE, pp 28-34   
Li X, Tian Q, Tang M, Chen X, Yang X (2019) Local community detection for multi-layer mobile network based on the trust relation. Wirel Netw 26(8):5503-5515   
Li Z, Zhang S, Wang RS, Zhang XS, Chen L (2008) Quantitative function for community detection. Phys Rev E 77(3):036109   
Liang W, Meng B, He X, Zhang X (2015) Gcm: A greedy-based cross-matching algorithm for identifying users across multiple online social networks. In: Pacific-Asia workshop on intelligence and security informatics. Springer, pp 51-70   
Liu RR, Jia CX, Lai YC (2019) Remote control of cascading dynamics on complex multilayer networks. New J Phys 21(4):045002   
Liu W, Suzumura T, Ji H, Hu G (2018) Finding overlapping communities in multilayer networks. PLoS ONE 13(4):e0188747   
Liu X, Wang W, He D, Jiao P, Jin D, Cannistraci CV (2017) Semi-supervised community detection based on non-negative matrix factorization with node popularity. Inf Sci 381:304-321   
Loe CW, Jensen HJ (2015) Comparison of communities detection algorithms for multiplex. Phys A 431:29-45   
Liu L, Zhang YC, Yeung CH, Zhou T (2011) Leaders in social networks, the delicious case. PLoS ONE 6(6):e21202   
Lü L, Chen D, Ren XL, Zhang QM, Zhang YC, Zhou T (2016) Vital nodes identification in complex networks. Phys Rep 650:1-63   
Ma X, Dong D, Wang Q (2018) Community detection in multi-layer networks using joint nonnegative matrix factorization. IEEE Trans Knowl Data Eng 31(2):273-286   
Ma'ayan A (2017) Complex systems biology. J R Soc Interface 14(134):20170391

MacQueen J et al (1967) Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, Oakland, CA, USA 1:281-297   
Mandaglio D, Amelio A, Tagarelli A (2018) Consensus community detection in multilayer networks using parameter-free graph pruning. In: Pacific-Asia conference on knowledge discovery and data mining. Springer, pp 193-205   
Mondragon RJ, Iacovacci J, Bianconi G (2018) Multilink communities of multiplex networks. PLoS ONE 13(3):e0193821   
Mucha PJ, Richardson T, Macon K, Porter MA, Onnela JP (2010) Community structure in time-dependent, multiscale, and multiplex networks. Science 328(5980):876-878   
Newman M (2018) Networks. Oxford University Press,   
Newman ME, Girvan M (2004) Finding and evaluating community structure in networks. Phys Rev E 69(2):026113   
Newman ME, Leicht EA (2007) Mixture models and exploratory analysis in networks. Proc Nat Acad Sci 104(23):9564-9569   
Ng AY, Jordan MI, Weiss Y (2002) On spectral clustering: analysis and an algorithm. In: Advances in neural information processing systems, pp 849-856   
Nguyen NP, Dinh TN, Tokala S, Thai MT (2011) Overlapping communities in dynamic networks: their detection and mobile applications. In: Proceedings of the 17th annual international conference on Mobile computing and networking, pp 85-96   
Nicolini C, Bifone A (2016) Modular structure of brain functional networks: breaking the resolution limit by surprise. Sci Rep 6(1):1-13   
Nicosia V, Latora V (2015) Measuring and modeling correlations in multiplex networks. Phys Rev E 92(3):032805   
Oselio B, Kulesza A, Hero A (2015) Information extraction from large multi-layer social networks. In: 2015 IEEE international conference on acoustics, speech and signal processing (icassp). IEEE, pp 5451-5455   
Paez MS, Amini AA, Lin L (2019) Hierarchical stochastic block model for community detection in multiplex networks. arXiv preprint arXiv:1904.05330   
Pamfil AR, Howison SD, Lambiotte R, Porter MA (2019) Relating modularity maximization and stochastic block models in multilayer networks. SIAM J Math Data Sci 1(4):667-698   
Pan Z, Hu G, Li D (2018) Detecting communities from multilayer networks: an aggregation approach. In: proceedings of the international conference on intelligent science and technology, pp 6-11   
Paolucci F (2018) Network service chaining using segment routing in multi-layer networks. IEEE/OSA J Opt Commun Netw 10(6):582-592   
Papalexakis EE, Akoglu L, Ience D (2013) Do more views of a graph help? community detection and clustering in multi-graphs. In: Proceedings of the 16th international conference on information fusion. IEEE, pp 899-905   
Perry JW, Allen K, Berry MM (1955) Machine literature searching x. Machine language; factors underlying its design and development. Am Doc (pre-1986) 6(4):242   
Pizzuti C, Socievole A (2017) Many-objective optimization for community detection in multi-layer networks. In: 2017 IEEE congress on evolutionary computation (CEC). IEEE, pp 411-418   
Pramanik S, Tackx R, Navelkar A, Guillaume JL, Mitra B (2017) Discovering community structure in multilayer networks. In: 2017 IEEE international conference on data science and advanced analytics (DSAA). IEEE, pp 611-620   
Raghavan UN, Albert R, Kumara S (2007) Near linear time algorithm to detect community structures in large-scale networks. Phys Rev E 76(3):036106   
Rocklin M, Pinar A (2013) On clustering on graphs with multiple edge types. Internet Math 9(1):82-112   
Roethlisberger FJ, Dickson WJ (2003) Management and the worker, vol 5. Psychology Press, New York   
Rossetti G, Guidotti R, Miliou I, Pedreschi D, Giannotti F (2016a) A supervised approach for intra-/intercommunity interaction prediction in dynamic social networks. Soc Netw Anal Mining 6(1):86   
Rossetti G, Pappalardo L, Rinzivillo S (2016b) A novel approach to evaluate community detection algorithms on ground truth. In: Complex networks VII. Springer, pp 133-144   
Rosvall M, Bergstrom CT (2008) Maps of random walks on complex networks reveal community structure. Proc Nat Acad Sci 105(4):1118-1123   
Rozario VS, Chowdhury A, Morshed MSJ (2019) Community detection in social network using temporal data. arXiv preprint arXiv:1904.05291

Salavati C, Abdollahpouri A, Manbari Z (2018) Bridgerank: a novel fast centrality measure based on local structure of the network. Phys A 496:635-653   
Salehi M, Sharma R, Marzolla M, Magnani M, Siyari P, Montesi D (2015) Spreading processes in multilayer networks. IEEE Trans Netw Sci Eng 2(2):65-83   
Sanchez-Rodriguez LM, Iturria-Medina Y, Mouches P, Sotero RC (2019) A method for multiscale community detection in brain networks. bioRxiv p 743732   
Schütze H, Manning CD, Raghavan P (2008) Introduction to information retrieval, vol 39. Cambridge University Press, Cambridge   
Silber MD (2011) The Al Qaeda factor: plots against the West. University of Pennsylvania Press, Philadelphia   
Solá L, Romance M, Criado R, Flores J, García del Amo A, Boccaletti S (2013) Eigenvector centrality of nodes in multiplex networks. Chaos Interdiscip J Nonlinear Sci 23(3):033131   
Spirin V, Mirny LA (2003) Protein complexes and functional modules in molecular networks. Proc Nat Acad Sci 100(21):12123-12128   
Srihari S, Yong CH, Wong L (2017) Computational prediction of protein complexes from protein interaction networks. Morgan & Claypool, Williston   
Sultana S, Salon D, Kuby M (2019) Transportation sustainability in the urban context: a comprehensive review. Urban Geogr 40(3):279-308   
Tagarelli A, Amelio A, Gullo F (2017) Ensemble-based community detection in multilayer networks. Data Min Knowl Disc 31(5):1506-1543   
Tang JK, Leontiadis L, Scellato S, et al. (2012a) Temporal network metrics and their application to real world networks. PhD thesis, CiteSeer   
Tang L, Wang X, Liu H (2009) Uncovering groups via heterogeneous interaction analysis. In: 2009 Ninth IEEE international conference on data mining. IEEE, pp 503-512   
Tang L, Wang X, Liu H (2012b) Community detection via heterogeneous interaction analysis. Data Min Knowl Disc 25(1):1-33   
Taylor D, Caceres RS, Mucha PJ (2016a) Detectability of small communities in multilayer and temporal networks: eigenvector localization, layer aggregation, and time series discretization. Tech. rep   
Taylor D, Shai S, Stanley N, Mucha PJ (2016b) Enhanced detectability of community structure in multilayer networks through layer aggregation. Phys Rev Lett 116(22):228301   
Taylor D, Caceres RS, Mucha PJ (2017) Super-resolution community detection for layer-aggregated multilayer networks. Phys Rev X 7(3):031056   
Turker I, Sulak EE (2018) A multilayer network analysis of hashtags in twitter via co-occurrence and semantic links. Int J Mod Phys B 32(04):1850029   
Vaiana M, Muldoon S (2018) Resolution limits for detecting community changes in multilayer networks. arXiv preprint arXiv:1803.03597   
Verbrugge LM (1979) Multiplexity in adult friendships. Soc Forces 57(4):1286-1309   
Wang D, Li J, Xu K, Wu Y (2017) Sentiment community detection: exploring sentiments and relationships in social networks. Electron Commer Res 17(1):103-132   
Wang P, Robins G, Pattison P, Lazega E (2013) Exponential random graph models for multilevel networks. Soc Netw 35(1):96-115   
Watts DJ, Strogatz SH (1998) Collective dynamics of 'small-world' networks. Nature 393(6684):440-442   
Wu K, Taki Y, Sato K, Sassa Y, Inoue K, Goto R, Okada K, Kawashima R, He Y, Evans AC et al (2011) The overlapping community structure of structural brain network in young healthy individuals. PLoS ONE 6(5):e19608   
Wu W, Kwong S, Zhou Y, Jia Y, Gao W (2018) Nonnegative matrix factorization with mixed hypergraph regularization for community detection. Inf Sci 435:263-281   
Xu X, Yuruk N, Feng Z, Schweiger TA (2007) Scan: a structural clustering algorithm for networks. In: Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pp 824-833   
Yang Y, Yu H, Huang R, Ming T (2018) A fusion information embedding method for user identity matching across social networks. In: 2018 IEEE SmartWorld, ubiquitous intelligence & computing, advanced & trusted computing, scalable computing & communications, cloud & big data computing, internet of people and smart city innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), IEEE, pp 2030-2035   
Yiapanis P, Rosas-Ham D, Brown G, Lujan M (2013) Optimizing software runtime systems for speculative parallelization. ACM Trans Archit Code Optim 9(4):1-27

Yildirimoglu M, Kim J (2018) Identification of communities in urban mobility networks using multi-layer graphs of network traffic. Transp Res Part C Emerg Technol 89:254-267   
Zhan J, Sun M, Wu H, Sun H (2018) Using triangles and latent factor cosine similarity prior to improve community detection in multi-relational social networks. Concurr Comput Practi Exp 30(16):e4453   
Zhang F, Ma A, Wang Z, Ma Q, Liu B, Huang L, Wang Y (2018a) A central edge selection based overlapping community detection algorithm for the detection of overlapping structures in protein-protein interaction networks. Molecules 23(10):2633   
Zhang H, Zhuge C, Yu X (2018b) Identifying hub stations and important lines of bus networks: a case study in xiamen, china. Phys A 502:394-402   
Zhang H, Wang CD, Lai JH, Philip SY (2019) Community detection using multilayer edge mixture model. Knowl Inf Syst 60(2):757-779   
Zhang S, Wang RS, Zhang XS (2007) Uncovering fuzzy community structure in complex networks. Phys Rev E 76(4):046103   
Zhao Y, Karypis G (2004) Empirical and theoretical comparisons of selected criterion functions for document clustering. Mach Learn 55(3):311-331   
Zhou D, Councill I, Zha H, Giles CL (2007) Discovering temporal communities from social network documents. In: Seventh IEEE international conference on data mining (ICDM 2007). IEEE, pp 745-750   
Zhou X, Liang W, Wu B, Lu Z, Nishimura S, Shinomiya T, Jin Q (2016) Dynamic community mining and tracking based on temporal social network analysis. In: 2016 IEEE international conference on computer and information technology (CIT), IEEE, pp 177-182   
Zhu J, Wang B, Wu B, Zhang W (2017) Emotional community detection in social network. IEICE Trans Inf Syst 100(10):2515-2525

Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

# 目录

# 前官… (I)

# 第一章 什么是组合学？ (1)

1.1 例 棋盘的完全覆盖 …………………… (3)  
1.2例 切割立方体 (5)  
1.3 例 幻方 ………………………………………… (6)  
1.4例 四色问题 (8)  
1.5 例36军官问题 ………………………………………… (9)  
1.6 例 最短路问题 ………………………………………… (11)  
练习 (13)

# 第二章 鸽笼原理 ………………………………………… (16)

2.1 铸笼原理的简单形式 ………………………………………… (16)  
2.2鸽笼原理的加强形式 (18)  
2.3 Ramsey定理 ………………………………………… (22)

练习 (25)

# 第三章 基本计数原理：排列与组合 ………………………………………… (28)

3.1 两个基本原理 ………………………………………… (28)  
3.2 集合的排列 ………………………………………… (31)  
3.3集合的组合 (35)  
3.4 重集的排列 ………………………………………… (38)  
3.5 重集的组合 ………………………………………… (40)  
3.6排列的生成 (43)  
3.7排列的逆序 (47)   
3.8 $r$ 组合的生成 ………………………………………… (50)

练习 (52)

# 第四章 二项式系数 ………………………………………… (57)

4.1 Pascal公式 …………………… (57)  
4.2二项式定理 (60)  
4.3 恒等式 ………………………………………… (63)  
4.4二项式系数的单峰性质 (69)   
4.5多项式定理 (71)  
4.8 Newton二项式定理 ………………………………………… (79)

练习 (76)

# 第五章 客斥原理 ………………………………………… (79)

5.1 容斥原理 ……………………(80)  
5.2 重复组合 ………………………………………… (85)  
5.3 错位 ………………………………………… (88)  
5.4 其它禁位问题 ………………………………………… (93)

练习 (96)

# 第六章 遂归关系 ………………………………………… (96)

6.1 Fibonacci序列 ………………………………………… (100)  
6.2 常系数线性齐次递归关系：不同根的情形 ………………………………………… (108)  
6.3常系数线性齐次递归关系：重根的情形 (112)  
6.4迭代与归纳 (116)   
8.5差分表 (120)

练习 (135)

# 第七章 生成函数 ………………………………………… (140)

7.1 生成函数 (140)  
7.2 线性递归关系 ………………………………………… (144)  
7.3 一个几何学的例子 ………………………………………… (153)  
7.4指数型生成函数 (158)

练习 (163)

# 第八章 相异代表组 ………………………………………… (168)

8.1 相异代表组 ………………………………………… (168)

8.2多米诺骨牌、棋盘与偶图 (176)  
8.3一种算法 (182)  
8.4 无限多个集合的情形 ………………………………………… (192)

练习 (195)

# 第九章 组合设计 ………………………………………… (200)

9.1有限域 (200)  
9.2有限几何 (213)   
9.3 拉丁方 ………………………………………… (222)  
9.4 Kirkman女学生问题 ………………………………………… (232)

练习 (240)

# 第十章 图论入门 ………………………………………… (245)

10.1图的基本性质 (245)   
10.2 Euler链与Euler圆 (250)   
10.3 Hamilton链与Hamilton圈 ………………………………………… (255)   
10.4 树 … (259)   
10.5 两个实际问题 ………………………………………… (268)  
10.6 Shannon开关对策 ………………………………………… (272)   
10.7 有向图 ………………………………………… (280)

练习 (284)

# 第十一章 色数、连通度及图的其它参数 ………………………………………… (291)

11.1 色数 (291)  
11.2 平面图的Euler公式 ………………………………………… (300)  
11.3 五色定理 (304)  
11.4 连通度 …………………… (309)  
11.5 图的其它参数 ………………………………………… (317)  
练习 (323)

# 第十二章 优化问题 ………………………………………… (329)

12.1 稳定分配 (330)  
12.2 核心分配 (335)

12.3 Hitchcock运输问题 (339)  
12.4 最优分配问题 ………………………………………… (357)  
12.5 瓶颈问题 ………………………………………… (362)  
练习 (371)

文献目录 (379)

选题解答 (330)

# 第一章 什么是组合学

如果本书的读者从未算过组合问题，那倒是会令人感到意外。你可曾计算或考虑过下列一些问题：如果 $n$ 个队进行比赛（每个队仅与其它各队比赛一次），总共应比赛多少次？怎样构造幻方？怎样使铅笔不离纸面且不重复地描出网络图？为了确定对付“福尔豪斯”①应该如何下赌注，试计算牌型是福尔豪斯的个数是多少？这些问题都是组合问题。正如这些问题所显示的那样，组合学发源于数学消遣和游戏。无论是为了消遣还是由于它们的美学兴趣，过去所研究过的许多问题对于当代的纯粹科学或应用科学都是非常重要的。当代的组合学是数学中非常重要的一个分支，并且它的影响正在迅速扩大。在过去十年里，组合学迅猛发展的一个原因就是计算机已经并且继续地对我们的社会产生巨大的影响。由于计算机闪电般的速度，它已经能够解决以前不敢设想的大规模的问题。但是，计算机没有自主的功能，还需要进行程序设计。这些程序的基础，往往由解决这些问题的组合算法所组成。组合学发展的另一个原因在于它可用于一些以前与数学没有多大关联的学科。我们发现组合学的思想和方法不仅用于数学应用的传统领域即物理学，而且也用于社会科学和生物学。

组合学所研究的就是一组事物安排成各种各样模式的问题。有两类问题经常出现：

（i）安排的存在性 要把一组事物进行安排，使之满足某些条件。当能否这样安排不是那么明显时，就需要讨论存在问题。如果一种安排不总是可能的，那么在怎样的（必要和充分的）条件下，才能使所希望的安排办得到。

（ii）安排的计数和分类 如果某种安排是可能的，可以用许多方式来完成，人们可能要求计算这些方式的个数或者把它们进行分类。

虽然对任何组合问题都可以考虑它的存在和计数，但在实际上常常发生这样的情况：如果存在问题还需要进行深入的研究，那么计数问题就是难于处理的。不过，如果某种安排的存在比较明显时，就有可能计算出完成这种安排的方法的数目。在一些特殊的情况下（当它们的数目很小时），可以把全部安排列举出来。因此，许多组合问题的形式是：“安排……是可能的吗？”或“……存在吗？”或“有多少种方法可以……？”或“计算……的数目”。

与（i）连同出现的第三个组合问题是

（iii）研究一个已知的安排在人们已经构造出了（可能是困难的）满足一定条件的安排之后，那么就可以研究这种安排的性质和结构了，这样的结构可能涉及分类问题（ii）并且也可能牵涉到潜在的应用。

更一般地来说，组合学与离散结构和关系的分析有关。

对于验证一些发现，数学归纳法是组合学的主要工具之一。归纳法常常是有效的方法，尤其在组合学中更是如此。用数学归纳法来证明一个较强的结论往往比证明一个较弱的结论要来得容易些。尽管在归纳论证中需要更多的验证，可是归纳假设却比较强。

可是，一般来说，解决组合问题需要特殊的方法。通常我

们不能只借助于已知的结果或公理，必须研究情况，增长见识，用自己的聪明才智去解决问题。我决不是说没有任何一般原理或方法可以应用。容斥原理、所谓笛笼原理、递归关系以及生成函数的方法等等都是一般原理和方法，在后面一些章节我将要讨论这些原理和方法。不过，我们常常会看到，即使用这些原理和方法，仍需要巧妙地应用它们。在解决组合问题时经验是极为重要的。

为了明白上述讨论的意思，我们现在转到组合问题的几个例子。这些例子从解决比较简单（但其解决却需要机智）的问题入手，将引伸出组合学的一些重大成果。

# 1.1 例棋盘的完全覆盖

考虑一个普通的棋盘，它有8行和8列，总共分成64个方格。假定有一批形状相同的多米诺骨牌，每块骨牌恰好覆盖棋盘的两个相邻的方格。在棋盘上放置32块多米诺骨牌，使得任何两块骨牌都不重叠，每块骨牌都覆盖两个方格，能不能把棋盘的所有方格都覆盖住呢？我们称这种放置为用多米诺骨牌完全覆盖棋盘。这是一个容易的安排问题，人们可以很快地作出许多不同的完全覆盖。要计算不同的完全覆盖的个数虽然是困难的，但是仍可以算出来。在1961年，M.E.Fischer<sup>1)</sup>发现这个数是 $12,988,816 = 2^4 \times (901)^2$ 。我们可以用一个有 $m$ 行 $n$ 列、 $mn$ 个方格的更一般的棋盘来代替普通的棋盘。此时完全覆盖就未必存在。的确，对于 $3 \times 3$ 的棋盘就不能完全覆盖。

对于 $m$ 和 $n$ 的哪些值， $m \times n$ 的棋盘有完全覆盖呢？不难看到，一个 $m \times n$ 的棋盘有完全覆盖的充分必要条件是 $m$ 和 $n$ 至少有一个是偶数，换句话说，棋盘中的方格的个数是偶数。对于 $m \times n$ 的棋盘的不同的完全覆盖的数目，Fischer 已经推导出一个含有三角函数的一般公式。这个问题等价于分子物理学中著名的所谓二聚物问题。它起源于表面上双原子的分子（二聚物）吸收作用的研究。棋盘上的方格相当于分子，而多米诺骨牌相当于二聚物。

再考虑一个 $8 \times 8$ 棋盘，并且用剪刀剪掉对角上两个方格。能不能放置31块多米诺骨牌把这个残缺的棋盘完全覆盖呢？虽然这个残缺棋盘和 $8 \times 8$ 棋盘差不多，可是 $8 \times 8$ 棋盘有一千二百多万个完全覆盖，而这个残缺棋盘却没有完全覆盖。这个结论的证明十分简单，但却是用组合论证的一个出色的例子。在普通的 $8 \times 8$ 棋盘上的方格都是黑白交替着色的，共有32个黑色方格和32个白色方格，如果我们剪掉对角上的两个方格，我们就剪掉了两个同样颜色的方格，比方说是白色的。这样就剩下32个黑色方格和30个白色方格。但是每一块多米诺骨牌覆盖一个黑色方格和一个白色方格，因此31块多米诺骨牌必定覆盖棋盘上31个黑色方格和31个白色方格。所以这个残缺棋盘没有完全覆盖。

更一般地说，可以取一个黑白交替着色的 $m \times n$ 棋盘，并且任意剪掉若干方格。什么样的残缺棋盘有完全覆盖呢？根据上面的论证可知，残缺棋盘具有完全覆盖的必要条件是黑白方格的个数相同。不过，如图1.1所示（图中W表示白，B表示黑），这个条件并不是充分条件。

于是自然要问，残缺棋盘具有完全覆盖的必要充分条件是什么？我们将在第八章问到这一问题上来，并且利用相异代表

![](images/c5be1318a8c5841a6b2daa85b9e98826a4fb548266b5da49a879f9287b194790.jpg)  
图1.1

组的理论给出完整的解答。在那一章里，我们把这个问题具体解释为把一批工作分配给胜任的申请人。

# 1.2 例 切割立方体

考虑一个边长为3的立方体的木块，要把它切割成27个边长为1的立方体。问最小的切割次数是多少？切割这个立方体的一种方法是在每个方向都切割2次，共切割6次，切割时立方体始终保持为一块，如图1.2所示。如果在两次切割之间可以重新堆叠各个小切块，那么能不能少切割几次呢？例如图1.3中，第二次切割的木块数比起在第一次切割后不重新堆叠各小切块时要多。由于每次切割之后小切块的数目以及重新堆叠的方式都增加，看起来这似乎是一个难于分析的问题。

再让我们从另一个角度来看看这个问题。27个小立方体除居中的那一个以外，每个小立方体至少有一面在原来的大立方体的表面上，而居中的那个小立方体每一面都是由切割形成的。因为它有六面，形成它必须有六次切割。这样至少需要切割六次，而且各次切割之间重新堆叠小切块都不能减少这六次

必要的切割精力。旺盛的学生也许会去探求只用6次切割把立方体切割成27个小立方体的不同的切割方法的数目。

![](images/36e9bc1f88da7ba53d1ddb8c15b4c2cc3e50d57a97fdc557c86dc9d90e928584.jpg)  
图1.2

![](images/0dff722ec4c3efb9a3835678654dfc31f110fa4939820365df81d3837b7d177a.jpg)  
图1.3

# 1.3 例幻方

幻方是一种最古老和最流行的数学游戏。 $n$ 阶幻方就是把整数 1, 2, 3, ..., $n^2$ 排列 $n \times n$ 阵列，使得每行中的各数之和、每列中的各数之和以及两条对角线中的各数之和都是同一个数 $S$ 。数 $S$ 称为幻方的幻和。3 阶和 4 阶幻方的例子是

$$
\left( \begin{array}{l l l} 8 & 1 & 6 \\ 3 & 5 & 7 \\ 4 & 9 & 2 \end{array} \right). \left( \begin{array}{l l l l} 1 6 & 3 & 2 & 1 3 \\ 5 & 1 0 & 1 1 & 8 \\ 9 & 6 & 7 & 1 2 \\ 4 & 1 5 & 1 4 & 1 \end{array} \right). \tag {1.3.1}
$$

它们的幻和分别为15和34。在中世纪对幻方有某种神秘观念，人们佩带着幻方以防邪避凶。Benjamin Franklin是一位幻方迷，在他的一些论文中包含有许多有趣的例子。

在 $n$ 阶幻方中所有整数之和是 $1 + 2 + 3 + \dots + n^{2}$ ，由算术级

数的求和公式可知这个和为 $n^2 (n^2 + 1) / 2$ ，由于 $n$ 阶幻方有 $n$ 行且每行和数为S，因此有关系 $nS = n^{2}(n^{2} + 1) / 2$ ，于是任何两个 $n$ 阶幻方有同样的幻和，即 $S = n(n^{2} + 1) / 2$ ，组合学问题就是确定 $n$ 取哪些值时存在 $n$ 阶幻方，并找出构造它的一般方法．不难证明2阶幻方是不存在的（倘若存在的话，幻和必定是5）.不过， $n$ 取其它的数值时， $n$ 阶幻方是可以构造的．有许多构造幻方的特殊方法．这里我们叙述当 $\pmb{n}$ 为奇数时构造 $\pmb{n}$ 阶幻方的方法，这个方法是de la Loubere在十七世纪发现的．首先把1放在顶行正中间的方格上，然后把后继的整数按自然顺序放置在右上斜的对角线上并且作如下的修改：  
（i）当到达顶行时，下一个整数放在底行，好象它在顶行的上面；  
（ii）当到达右端列时，下一个整数放在左端列，好象它是紧靠右端列的右方；  
（iii）当到达的方格已经填上数码或到达右上角的方格时，下一个整数就放在刚填写数码的方格正下方的方格中。  
（1.3.1）的3阶幻方就是用Loubere方法构造的，下面的5阶幻方也是用这个方法构造的：

$$
\left( \begin{array}{l l l l l} 1 7 & 2 4 & 1 & 8 & 1 5 \\ 2 3 & 5 & 7 & 1 4 & 1 6 \\ 4 & 6 & 1 3 & 2 0 & 2 2 \\ 1 0 & 1 2 & 1 9 & 2 1 & 3 \\ 1 1 & 1 8 & 2 5 & 2 & 9 \end{array} \right), \tag {1.3.2}
$$

阶数为非2的偶数的幻方的构造方法以及奇数阶幻方的其它构造方法可以在W.W.RouseBall著、H.S.M.Coxeter校的

《数学消遣和随笔》一书中找到（纽约：Macmillan，1962，193-221）。

# 1.4 例四色问题

考虑平面地图或球面地图，地图上的国家都是连通区域（这样，密歇根州不允许做为这种地图上的国家）。为了能够迅速区分各个国家，需要把它们着色使得具有公共边界的两个国家涂染不同的颜色。要保证每张地图都能如此着色，最少需要多少种颜色？直到最近，它还是数学中最有名的未解决的问题之一。它引起一般人的兴趣在于这个问题的意思简单明白。除了著名的三等分角的问题外，它几乎比其它任何问题引起更多的业余数学家的兴趣。大约在1850年Francis Guthrie首先提出这个问题，当时他还是一名刚毕业的大学生，这个问题引起大量的数学研究。说实在的，O.Ore著的《四色问题》（纽约，科学出版社，1967）整整一本书都是讨论这个问题以及有关的问题。一些地图需要四种颜色。例如图1.4所示的地图就

是如此，因为这张地图的四个国家任何两个都有公共边界，显然把这张地图着色必须有四种颜色。在1890年P.J.Heawood证明了给任何地图着色有五种颜色就足够了（见第十一章）。不难证明“不存在这样的平面地图，其上有五个国家且任何两个国家都

![](images/61f76e7bd9fe6d9bdf17ddb32345ff8964053f9948a296f43b7ac67b4a0784af.jpg)  
图1.4

有公共边界”（见第十一章），倘若存在这种地图的话，就需要五种颜色了。1976年两位数学家K. Appel和W. Haken

宣称1)他们已证明了“用四种颜色就能给任何平面地图着色”。这使数学界为之震惊。他们的证明用计算机计算大约需要1200小时，要作出100亿个独立的逻辑判断！

# 1.5 例36军官问题

设有6种军衔和来自6个团的36名军官，能不能把他们安排成 $6 \times 6$ 的队列使得每行每列里都有每种军衔的一名军官和每个团的一名军官呢？这个问题是十八世纪瑞士数学家Euler（历史上最多产的数学家之一）提出的一个趣味数学问题。它在统计学尤其是在试验设计中有重要的影响（见9.3节）。每个军官可以和一个有序数偶 $(i, j)$ 相对应，其中 $i$ 表示他的军衔 $(i = 1, 2, \dots, 6)$ ， $j$ 表示他所属的团 $(j = 1, 2, \dots, 6)$ 。于是，这个问题就是要把36个有序数偶 $(i, j) (i = 1, 2, \dots, 6)$ 排列成 $6 \times 6$ 阵列，使得在每一行和每一列中，整数1，2，…，6按某种次序在有序数偶的第一个位置上出现和按某种次序在有序数偶的第二个位置上出现。这样的阵列可以分成两个 $6 \times 6$ 阵列，其中一个阵列为各有序数偶的第一个数码所组成的阵列（军衔阵列），另一个阵列为各有序数偶的第二个数码所组成的阵列（团阵列）。于是问题可叙述为：是否存在两个 $6 \times 6$ 阵列，其元素取自整数1，2，…，6，使得（i）在这些阵列的每一行和每一列中都包含整数1，2，…，6；（ii）当两个阵列并置时，全部36个有序数偶 $(i, j) (i, j = 1, 2, \dots, 6)$ 都出现。为了使这个问题具体起

见，假设有3种军衔和来自3个不同的团的9名军官。这时相应的问题的解答是

$$
\left[\begin{array}{l l l}1&2&3\\3&1&2\\2&3&1\end{array}\right], \left[\begin{array}{l l l}1&2&3\\2&3&1\\3&1&2\end{array}\right]\rightarrow \left[\begin{array}{l}(1, 1) (2, 2) (3, 3)\\(3, 2) (1, 3) (2, 1)\\(2, 3) (3, 1) (1, 2)\end{array}\right]. \quad (1. 5, 1)
$$

军衔阵列

团阵列

并置阵列

上面的军衔阵列和团阵列都是所谓3阶拉丁方的例子，下面是2阶和4阶的拉丁方：

$$
\left[ \begin{array}{l l} 1 & 2 \\ 2 & 1 \end{array} \right], \quad \left\{ \begin{array}{l l l l} 1 & 2 & 3 & 4 \\ 4 & 1 & 2 & 3 \\ 3 & 4 & 1 & 2 \\ 2 & 3 & 4 & 1 \end{array} \right\}. \tag {1.5.2}
$$

在（1，5，1）中的两个3阶拉丁方称为正交的拉丁方，因为把它们并置时，由 $i = 1$ ，2，3和 $j = 1$ ，2，3所得到的全部的9种可能的有序数偶 $(i,j)$ 都出现。我们可以把Euler问题重新叙述为：是否存在两个正交的6阶拉丁方呢？更一般地，Euler研究过两个正交的 $n$ 阶拉丁方问题。不难证明，不存在两个正交的2阶拉丁方。因为除了在（1.5.2）中给出的2阶拉丁方外，仅有的另一个2阶拉丁方是

$$
\left[ \begin{array}{c c} 2 & 1 \\ 1 & 2 \end{array} \right],
$$

并且它们不是正交的。Euler指出了当 $n$ 是奇数或4的倍数时，如何构造两个 $n$ 阶正交拉丁方。要注意，这不包含 $n = 6$ 的情形。根据许多次试验，Euler断言不存在两个正交的6阶拉丁方，可是他并未给予证明。他还猜想对于阶数是6，10，14，18，…， $4k + 2$ ，…的拉丁方，任何两个同阶的拉丁方都是不

正交的。G.Tarry1)在1901年用穷举法证明了Euler猜想对于 $n = 6$ 是正确的，大约在1960年三位统计学家R.C.Bose，E.T.Parker和S.S.Shrikhande2)终于证明了对于 $n > 6$ Euler猜想不成立，这就是说，对于 $n = 4k + 2(k = 2,3,4,\dots)$ ，他们指出了如何构造出两个正交的 $n$ 阶拉丁方。这是一个重要成就，从而最终解决了Euler猜想。后面我们将讨论用有限算术构造正交拉丁方的方法以及它们在试验设计中的应用。

# 1.6 例最短路问题

考虑一个街道和交叉路口的系统。有一个人想从一个交叉路口 $A$ 步行到另一个交叉路口 $B$ ，通常从 $A$ 到 $B$ 有很多条路线可走。问题是要确定一条距离尽可能短的路线——最短路。实现它的一种可能的办法是把从 $A$ 到 $B$ 所有的路线列出一张路线表（每条街道最多走过一次，因此这种路线只有有限多条），计算每条路线的距离，然后选择最短的一条路线。但这不是一种行之有效的方法，当系统很大时所需的工作量是难以完成的。我们需要的是一种求最短路的算法，完成这个算法所需的工作量不能随系统增大而增加得太快。我们将在10.5节中叙述这种算法。用这个算法可以实际地找出从 $A$ 到该系统中的其它的每个交叉路口的最短路。寻找两个交叉路口之间的最短路问

题可以抽象地描述如下：设 $X$ 是有限个元素（称为结点）的集合（结点相当于交叉路口）， $E$ 是无序的结点偶（称为边）的集合（边相当于街道）。于是有些结点之间有边连接，有些结点之间没有边连接。有序偶 $(X, E)$ 称为图。在图中连接结点 $x$ 和 $y$ 的路是指第一个结点为 $x$ ，最后的结点为 $y$ 的结点序列，其中相继的结点之间由边连接。使每条边对应一个非负实数——边的长度。一条路的长度就是连接路上相继结点的边的长度之和。给定两个结点 $x$ 和 $y$ ，最短路问题就是寻找从 $x$ 到 $y$ 的长度最小的路。在图1.5中有6个结点10条边。边上的数字表示它的长度。 $x, a, b, d, y$ 是连接 $x$ 和 $y$ 的一条路，其长度为4。另一条路是 $x, b, d, y$ ，其长度为3。不难看到，后一条路是连接 $x$ 和 $y$ 的最短路。

![](images/b02c568e42a06117a171eb69af8306db3e3efae5ba4c44d927b1f49e9c2e6ae6.jpg)  
图1.5

图是一类离散结构，在组合学中已经且将继续对图进行广泛的研究。这种概念的一般性给它在各个不同的领域，如心理学、社会科学、化学、遗传学和通讯科学中提供了广泛应用的可能性。譬如，图的结点可以表示人，而边表示相应的人彼此不信任；又如结点可以表示原子，而边表示原子间的化学键。读者还可以想象其它的方式，通过这些方式，可以用图来描述某些现象。图的一些重要概念和性质将在第八、十和十一章中研究。

# 练习

证明当且仅当 $m$ 和 $n$ 至少有一个是偶数时， $m \times n$ 棋盘有完全覆盖.  
2. 考虑 $m$ 和 $n$ 均为奇数的 $m \times n$ 棋盘。为了确定起见，假定左上角的方格涂白色。证明如果剪掉棋盘上任何一个白色方格，所得到的残缺棋盘有完全覆盖。  
3. 设一所监狱有 84 间囚室，其排列类似于 $8 \times 8$ 棋盘。在所有相邻的囚室之间都有门相通，告示关押在一个角落囚室里的囚犯：只要他能够不重复地通过其它每间囚室后到达对角的囚室，他将被释放。问囚犯能够获得自由吗？  
4. 设 $f(n)$ 表示 $2 \times n$ 棋盘的不同的完全覆盖的个数。计算 $f(1)$ ， $f(2)$ ， $f(3)$ ， $f(4)$ 和 $f(5)$ 。试求这个计数函数 $f$ 所满足的简单关系并加以证明，再利用此关系计算 $f(12)$ 。  
5. 试求 $3 \times 4$ 棋盘的不同的完全覆盖的个数.  
8. 说明如何用6次切割把边长为3的立方体切割成27个边长为1的1小立方体，每次切割时可将小切块重新堆叠。  
7. 考虑下面棋盘问题的三维模型，3维多米诺骨牌是长和宽为1单位长度、高为2单位长度的长方体。证明可以由多米诺骨牌构造边长为 $n$ 个单位长度的立方体的必要充分条件是 $n$ 为偶数。如果 $n$ 是奇数，能不能构造边长为 $n$ 个单位长度的立方体，其正中间有一个边长为1个单位长度的立方体的空洞？（提示：把边长为 $n$ 个单位长度的立方体想象为 $n^3$ 个边长为1单位长度的小立方体所组成，且小立方体交错地涂上黑色和白色）.  
8. 证明2阶幻方是不存在的。  
9. 试用de la Loubere方法构造7阶幻方。  
10. 试用 de la Loubere 方法构造 9 阶幻方.  
11. 试构造6阶幻方  
12. 证明3阶幻方正中间方格上的数码必定是5。推导3阶幻方恰

好有8个.

13. 能不能用下面的部分方阵构造出一个4阶幻方？

![](images/d025249cbad0963f4d36d8f6c5600aa4f25ade29d7a8abfd8bdd1deb69a311b2.jpg)

14. 证明在 $n$ 阶幻方中每个数码 $a$ 换为 $n^2 + 1 - a$ , 所得到的阵列仍是一个 $n$ 阶幻方,  
15. 试说明下面十个国家（1，2，…，10）的地图至少需要三种颜色才能着色。如果使用红、白和蓝三种颜色，试求不同的着色的个数。

![](images/338db95354eb85e87fc8c2b62610c017128f88144c30a0605c949da599b48a03.jpg)

18. 试构造两个正交的4阶拉丁方，  
17. 试构造5阶和6阶拉丁方.   
18. 对于任何 $n$ , 说明构造 $n$ 阶拉丁方的一般方法.  
19. 有没有 2 阶幻六角形？即能不能把整数 1，2，…，7 安放在下面的六角形阵列中使得所有 9 条“线段”上各数之和都相同？

![](images/99f3e12c43ce902cf5f9df74a55084306a29a9b4915e5b2de26e617abd0d262b.jpg)

20.* 若 $6 \times 6$ 棋盘被18块多米诺骨牌完全覆盖，证明可以用水平线或铅直线将棋盘划分成非空的两块，且划分线不穿过任何骨牌。  
21. 在下图所示的街道和交叉路口的系统中，试求从 $A$ 到 $B$ 的全部最短路。街道上的数字表示街道的长度（用某种单位测量）。

![](images/24df3a86114b00724a68ba20d4d6ad379073979f682b61d43fc565d558e1d135.jpg)

# 第二章鸽笼原理

本章我们考虑一个重要的又是基本的组合原理，它可以用来解决各种有趣的问题，并且常常得到一些令人惊异的结果。这个原理有各种称呼，最常用的名称是鸽笼原理、Dirichlet抽屉原理和鞋盒原理。作为鸽笼原理的表述，粗略地说就是：如果许多鸽子飞入不太多的鸽笼，则至少有一个鸽笼被两只或更多的鸽子占据着。下面给出更精确的叙述。

# 2.1 鸽笼原理的简单形式

如果把 $n + 1$ 件东西放入 $n$ 个盒子，则至少有一个盒子含有两件或更多件东西。

这几乎是不需要证明的。可以用反证法加以证明。如果 $n$ 个盒子中每个盒子至多包含一件东西，则东西的总数至多为 $n$ 。由于我们原来有 $n + 1$ 件东西，因此这是不可能的。

要注意，鸽笼原理不能用来寻找究竟是哪个盒子含有两件或更多的东西。要知道是哪个盒子含有两件或更多的东西，最好的办法是逐一检查各个盒子。鸽笼原理只能断定这种盒子的存在。因此，无论什么时候，只能应用鸽笼原理来证明某种安排或某种现象的存在，它并未指出怎样构造这种安排或怎样寻找出现这种现象的场合，除非检查所有的可能情况。

我们从一个简单的应用开始。

应用1 在13个人中必有两个人是在问一个月分出生的。

应用2 给定 $m$ 个整数 $a_{1}, a_{2}, \cdots, a_{p}$ ，存在 $k$ 和 $l$ （ $1 \leqslant k < l \leqslant m$ ），使得 $a_{k+1} + a_{k+2} + \cdots + a_{l}$ 能被 $m$ 整除。概略地说，在序列 $a_{1}, a_{2}, \cdots, a_{m}$ 中，存在依次相继的若干个 $a$ ，它们的和能被 $m$ 整除。

为了看到这个事实，考虑 $m$ 个和： $a_{1}, a_{1} + a_{2}, a_{1} + a_{2} + a_{3}, \cdots, a_{1} + a_{2} + \cdots + a_{m}$ 。如果这些和中有一个能被 $m$ 整除，那么这个结论成立。假定这些和被 $m$ 除时都有非零的余数，那么这些余数为 1，2，…， $m - 1$ 中的数。由于有 $m$ 个和并且只有 $m - 1$ 个余数，因此用 $m$ 除各和时，有两个和的余数是相同的。所以存在整数 $k$ 和 $l (k < l)$ ，使得 $a_{1} + a_{2} + \cdots + a_{k}$ 和 $a_{1} + a_{2} + \cdots + a_{l}$ 被 $m$ 除时有相同的余数 $r$ ：

$$
a _ {1} + a _ {2} + \dots + a _ {k} = b m + r, a _ {1} + a _ {2} + \dots + a _ {I} = c m + r.
$$

两式相减，我们得到 $a_{k+1} + \cdots + a_{1} = (c - b)m$ ，因此 $a_{k+1} + \cdots + a_{1}$ 能被 $m$ 整除。

应用3 有11周的时间准备锦标赛的一位国际象棋大师，决定每天至少比赛一次，但为了不使自己太疲倦，他决定在每一周中比赛不能超过12次。证明存在相继的若干天这位国际象棋大师恰好进行了21次比赛。

设 $a_1$ 是第一天比赛的次数， $a_2$ 是前两天比赛的次数， $a_3$ 是前三天比赛的次数，等等。数列 $a_1, a_2, \cdots, a_{77}$ 是严格递增序列（ $a_1 < a_2 < \cdots < a_{77}$ ），因为每天至少比赛一次。此外， $a_1 \geqslant 1$ ，并且由于在每一周里最多进行12次比赛，所以 $a_{77} \leqslant 132$ 。序列 $a_1 + 21, a_2 + 21, \cdots, a_{77} + 21$ 也是严格递增序列，并且 $a_{77} + 21 \leqslant 132 + 21 = 153$ 。因此154个数 $a_1, a_2, \cdots, a_{77}, a_1 + 21, a_2 + 21, \cdots, a_{77} + 21$ 都是在1和153两个整数之间。由此可见，它们之中有两个是相等的。由于 $a_1, a_2, \cdots, a_{77}$ 之中任何两个数都不相等， $a_1 + 21, a_2 + 21, \cdots, a_7, +21$ 中也是

彼此不相等的，所以一定存在两个数 $i$ 和 $j$ ，使得 $a_{i} = a_{j} + 21$ 。因此在 $j + 1, j + 2, \cdots, i$ 这些天中，国际象棋大师总共比赛了21次。

应用4 从整数1，2，…，200中任选101个整数。证明在选取的这些整数中必存在两个数，其中之一可被另一个整除。

尽量地提出因子2，我们看到任何整数都可以写成 $2^{k} \cdot a$ 的形式，其中 $k \geqslant 0$ 且 $a$ 为奇数。对于1到200之间的每个整数， $a$ 是100个数1，3，5，…，199之中的数。于是在所选取的101个整数中，有两个数在上述表示式中含有相同的 $a$ ，假设这两个数是 $2^{r} \cdot a$ 和 $2^{s} \cdot a$ ，如果 $r < s$ ，则后者可被前者整除。如果 $r > s$ ，则前者可被后者整除。

为了进一步的应用，我们需要鸽笼原理的加强形式。现在我们就来叙述它。

# 2.2 鸽笼原理的加强形式

设 $q_{1}, q_{2}, \cdots, q_{n}$ 是正整数。如果把 $q_{1} + q_{2} + \cdots + q_{n} - n + 1$ 件东西放入 $n$ 个盒子里，则或者第一个盒子至少含有 $q_{1}$ 件东西，或者第二个盒子至少含有 $q_{2}$ 件东西，…，或者第 $n$ 个盒子至少含有 $q_{n}$ 件东西。

鸽笼原理的简单形式是鸽笼原理的加强形式当 $q_{1} = q_{2} = \cdots = q_{n} = 2$ 时的特殊情形。那时， $q_{1} + q_{2} + \cdots + q_{n} - n + 1$ 成为 $2n - n + 1 = n + 1$ 。表达式 $q_{1} + q_{2} + \cdots + q_{n} - n + 1$ 看起来可能有点奇怪。但是我们将会看到它是怎样产生的并且如果用一个较小的数代替它，结论就不成立。

为了证明鸽笼原理的加强形式，我们论证如下：假定我们

把 $q_{1} + q_{2} + \dots + q_{n} - n + 1$ 件东西分配在 $n$ 个盒子里。如果对于每个 $i = 1, 2, \dots, n$ ，第 $i$ 个盒子包含的东西少于 $q$ 件，那么在所有盒子中的东西的总数就不超过

$$
\begin{array}{r l} & (q _ {1} - 1) + (q _ {2} - 1) + \dots + (q _ {n} - 1) = q _ {1} + q _ {2} + \dots \\ & + q _ {n} - n. \end{array}
$$

由于这个数比分配在盒子里的东西的件数小1，我们断定对某个 $i = 1,2,\dots ,n$ ，第 $i$ 个盒子至少含有 $q_{i}$ 件东西．要注意，有可能把 $q_{1} + q_{2} + \dots +q_{n} - n$ 件东西分配在n个盒子，使得第 $i$ 个盒子（ $i = 1,2,\dots ,n$ ）包含的东西少于 $q_{i}$ 件，为此我们只需将 $q_{1} - 1$ 件东西放入第一个盒子， $q_{2} - 1$ 件东西放入第二个盒子，等等．不必待言，如果我们把多于 $q_{1} + q_{2} + \dots +q_{n}- n + 1$ 件的东西放入n个盒子，那么加强形式的结论成立.

在初等数学中，鸽笼原理加强形式的 $q_{1}, q_{2}, \cdots, q_{n}$ 都等于某个整数 $r$ 的特殊情形是最经常被使用的。这时原理表述如下：

如果把 $n(r - 1) + 1$ 件东西放入 $n$ 个盒子中，则至少有一个盒子含有 $r$ 件或更多的东西。

这种情况的另一种提法为：

如果 $n$ 个整数 $m_{1}, m_{2}, \cdots, m_{n}$ 的平均数 $(m_{1} + m_{2} + \cdots + m_{n}) / n$ 大于 $r - 1$ ，则整数 $m_{1}, m_{2}, \cdots, m_{n}$ 中至少有一个大于或等于 $r$ 。

为了弄清这两种叙述之间的联系，我们取 $n(r - 1) + 1$ 件东西并把它们放入 $n$ 个盒子里。对于 $i = 1, 2, \dots, n$ ，令 $m_i$ 是第 $i$ 个盒子里的东西的件数。那么数 $m_1, m_2, \dots, m_n$ 的平均数是

$$
\frac {m _ {1} + m _ {2} + \cdots + m _ {n}}{n} = \frac {n (r - 1) + 1}{n}
$$

$$
= (r - 1) + \dots ,
$$

由于这个平均数大于 $r - 1$ ，诸整数 $m_{i}$ 中至少有一个不小于 $r$ 换言之，这些盒子中至少有一个含有 $r$ 件或更多的东西.

应用5 有大小两个圆盘，每个圆盘都分成200个相等的扇形。在大圆盘中任意选取100个扇形并涂上红色；另外100个扇形涂上蓝色。在小圆盘中每个扇形涂红色或涂蓝色，但红色或蓝色的扇形的个数不限。然后把小圆盘放在大圆盘上使它们的中心重合。证明可以调整两个圆盘的位置使得小圆盘与大圆盘上相对应的扇形中至少有100对是同色的。

为了弄清这一点，我们观察到，如果把大圆盘固定在平面上，那么对于小圆盘来说有200种可能的位置，使得小圆盘的每个扇形都包含在大圆盘的一个扇形中。我们首先计算在这两个圆盘的所有200种可能位置上颜色相同的总数，因为大圆盘有100个扇形涂红色，100个扇形涂蓝色，对于小圆盘的每个扇形，在200种可能的位置上颜色一致者恰有100种，于是在所有位置上颜色相同的总数等于小圆盘扇形的个数乘以100，因此这个数为20,000。所以每个位置上颜色相同的平均数是 $20,000 / 200 = 100$ 。因此一定有一种位置，在这个位置上对应的扇形中至少有100对是同色的。

下面我们介绍一个应用，它首先由两位匈牙利数学家P.Erdős和A.Szekeres1)发现的。

应用6 证明每个长为 $n^2 + 1$ 的实数序列 $a_1, a_2, \dots, a_{n^2 + 1}$ 含有一个长为 $n + 1$ 的递增子序列或长为 $n + 1$ 的递减子

序列, 1)

我们证明如下：假定不存在长为 $n + 1$ 的递增子序列，那么一定存在长为 $n + 1$ 的递减子序列。对于每个 $k = 1, 2, \dots, n^2 + 1$ ，令 $m_{n}$ 是以 $a_{k}$ 开始的最长递增子序列的长度。假定对于每个 $k = 1, 2, \dots, n^2 + 1$ 有 $m \leqslant n$ ，因而不存在长为 $n + 1$ 的递增子序列。由于对于每个 $k = 1, 2, \dots, n^2 + 1$ 有 $m \geqslant 1$ ，因此 $n^2 + 1$ 个数 $m_1, m_2, \dots, m_{n+1}$ 都是 1 和 $n$ 之间的整数。利用鸽笼原理加强形式的 $r = n + 1$ 时的特殊情形，数 $m_1, m_2, \dots, m_{n+1}$ 中必有 $n + 1$ 个数都相等。设

$$
m _ {k 1} = m _ {k 2} = \dots = m _ {k n + 1},
$$

其中 $1 \leqslant k_{1} < k_{2} < \dots < k_{n+1} \leqslant n^{2} + 1$ 。倘若对某个 $i = 1, 2, \dots, n$ ，有 $a_{ki} < a_{ki+1}$ ，那么由于 $k_{i} < k_{i+1}$ ，我们取以 $\dot{a}_{ki+1}$ 开始的一个最长递增子序列，然后把 $a_{ki}$ 放在这个子序列的前面，就得到以 $a_{ki}$ 开始的一个递增子序列。这就意味着 $m_{kp} > m_{ki+1}$ ，因此 $a_{ki} \geqslant a_{ki+1}$ 。由于对每个 $i = 1, 2, \dots, n$ ，这个不等式都成立，我们有

$$
a _ {k 1} \geqslant a _ {k 2} \geqslant \dots \geqslant a _ {k + 1}.
$$

从而 $a_{k1}, a_{k2}, \cdots, a_{kn+1}$ 是长为 $n+1$ 的递减子序列。

应用6的一个有趣的提法如下：假定 $n^2 + 1$ 个人并肩排列成一行，那么总能从他们中挑选出 $n + 1$ 个人并令他们向前走一步，这时他们的身高自左而右递增（或递减）。

# 2.3 Ramsey定理

现在我们叙述鸽笼原理的深刻而且重要的推广，但不予以证明。由于它是由英国逻辑学家F.P.Ramsey提出的，因此把它称为Ramsey定理。集合S的t-元子集是指S的子集，它恰有t个元素。

Ramsey定理 设 $q_{1}, q_{2}, \cdots, q_{n}, t$ 是正整数且 $q_{1} \geqslant t$ , $q_{2} \geqslant t$ , $\cdots$ , $q_{n} \geqslant t$ 。那么存在一个正整数，从而存在最小正整数 $N(q_{1}, q_{2}, \cdots, q_{n}; t)$ ，它仅依赖于 $q_{1}, q_{2}, \cdots, q_{n}$ 和 $t$ ，并具有下面的性质：如果 $m \geqslant N(q_{1}, q_{2}, \cdots, q_{n}; t)$ 且 $S$ 是 $m$ 个元素的集合，把 $S$ 的 $t$ -元子集分布在 $n$ 个盒子中，那么或者有 $q_{1}$ 个元素使它们全部的 $t$ -元子集都分布在第一个盒子里；或者有 $q_{2}$ 个元素使它们全部的 $t$ -元子集都分布在第二个盒子里， $\cdots$ ，或者有 $q_{n}$ 个元素使它们全部的 $t$ -元子集都分布在第 $n$ 个盒子里。

在上面的定理中，当 $t = 1$ 时，定理断定存在一个最小的正整数 $N$ （ $q_1, q_2, \dots, q_{n+1}$ ），具有如下性质：如果 $m \geqslant N$ （ $q_1, q_2, \dots, q_{n+1}$ ），并把一个 $m$ -元集 $S$ 的元素分布在 $n$ 个盒子中，那么或者第一个盒子含有 $q_1$ 个元素，或者第二个盒子含有 $q_2$ 个元素，…，或者第 $n$ 个盒子含有 $q_n$ 个元素。由鸽笼原理的加强形式和随后的讨论可知， $N$ （ $q_1, q_2, \dots, q_{n+1}$ ） $= (q_1 + q_2 + \dots + q_n) - n + 1$ 。

Ramsey定理在数学中有许多完全不同的应用。我们叙述在定理中 $n = 2$ 和 $t = 2$ 所得到的简单情况。考虑三维空间中任何3点都不共线的 $m$ 个点的集合 $S$ ，于是每对点确定唯一的直线段，我们把这些直线段任意涂上红色或蓝色。涂红色的直线段相当于把 $S$ 的2-元子集放入一个“红”盒子里，而涂蓝色的直线段相当于把 $S$ 的2-元子集放入一个“蓝”盒子里。现在设 $q_{1}$ 和 $q_{2}$ 是整数，且 $q_{1} \geqslant 2, q_{2} \geqslant 2$ 。Ramsey定理表明：如果点数 $m$ 充分大（即 $m$ 不小于已断定存在的整数 $N(q_{1}, q_{2}; 2)$ ），那么或者存在 $q_{1}$ 个点使它们之间的全部直线段都是涂红色的（即 $q_{1}$ 个点的所有2-元子集都在“红”盒子中），或者存在 $q_{2}$ 个点使它们之间的全部直线段都是涂蓝色的（即 $q_{2}$ 个点的所有2-元子集都在“蓝”盒子中）。于是，譬如说 $q_{1} = q_{2} = 100$ ，我们选择一个点数充分大的点集 $S$ ，并且把联接 $S$ 的点的直线段任意涂红色或蓝色，那么 $S$ 中必存在100个点使这100个点之间所有的直线段都是涂同一种颜色的。

我们把数 $N(q_{1}, q_{2}, \cdots, q_{n}; t)$ 称为Ramsey数。数 $N(q_{1}, q_{2}, \cdots, q_{n}; t)$ 的确定是一个困难的问题。关于这些数我们了解的非常少，即使象上面所述的 $N(q_{1}, q_{2}; 2)$ 这种特殊情形，也所知甚少。已知 $N(q_{1}, q_{2}; 2)$ 的上界是

$$
N \left(q _ {1}, q _ {2}; 2\right) \leqslant \frac {\left(q _ {1} + q _ {2} - 2\right) !}{\left(q _ {1} - 1\right) ! \left(q _ {2} - 1\right) !},
$$

其中 $k! = k(k - 1)\dots 2.1$ ，不难看到 $N(t,q_2,t) = q_2$ 和 $N(q_{1},t_{1}t) = q_{1}$ ，后者的证明如下：首先 $N(q_{1},t_{1}t)$ $\geqslant q_{1}$ ，如果我们取一个（ $q_{1} - 1$ )-元集 $S$ 并把它的 $t-$ 元子集都放入第一个盒子，那么既不存在 $S$ 的 $q_{1}$ -元子集使它的全部的t-元子集都在第一个盒子里，也不存在 $t-$ 元子集都在第二个

盒子里。其次，如果我们取一个至少有 $q_{1}$ 个元素的集合 $S$ 并把它的t-元子集分布到两个盒子里，那么下列两种情形必居其一：

(i)第二个盒子非空，因而有 $S$ 的t-元子集A在第二个盒子中。  
(ii)第二个盒子为空的，因而 $S$ 的全部 $i$ -元子集都在第一个盒子里，由于 $S$ 至少有 $q_{1}$ 个元素， $S$ 的任何 $q_{1}$ -元子集都有这样一个性质——它的全部 $i$ -元子集都在第一个盒子里。

于是 $N(q_{1},t_{1},t) \leqslant q_{1}$ . 因此我们断定 $N(q_{1},t_{1},t) = q_{1}$ . 类似地可以证明 $N(t,q_{2},t) = q_{2}$ . 此外, 下面一些Ramsey数已经求出来了（注意 $N(q_{1},q_{2},\dots ,q_{n},r) = N(q_{i_{1}},q_{i_{2}},\dots ,q_{i_{n}},r)$ , 其中 $i_1i_2\dots i_n$ 是 $\{1,2,\dots ,n\}$ 的一个排列）：

$$
\begin{array}{l} N (3, 3; 2) = 6, \\ N (3, 4; 2) = 9, \\ N (3, 5; 2) = 1 4, \\ N (3, 6; 2) = 1 8, \\ N (3, 7; 2) = 2 3, \\ N (4, 4, 2) = 1 8, \\ N (3, 3, 3; 2) = 1 7. \\ \end{array}
$$

我们现在证明 $N(3, 3; 2) = 6$ 来结束本章。利用上述线段涂色的方法即可证明。考虑3维空间中一个含6点集合 $S$ ， $S$ 的点确定15条直线段，把这些线段任意涂上红色或蓝色。我们要证明在 $S$ 中必有3点，这3点所确定的三角形的三条边都是涂同一种颜色的。考虑 $S$ 的任意一点 $p$ ，过 $p$ 的5条线段中，或者至少有3条涂红色或者至少有3条涂蓝色（鸽笼原理 $N(3, 3; 1) = 5$ ）。为确定起见，设 $p_1, p_2, p_3$ 为 $S$ 的3个点，且3条线段 $\overline{pp_1}, \overline{pp_2}, \overline{pp_3}$ ，都涂红色。如果 $\overline{p_1p_2}$

已涂红色，则 $\{p, p_1, p_2\}$ 是一个红色三角形的顶点。同样地，如果 $\overline{p_2p_3}$ 或 $\overline{p_1p_3}$ 是涂红色的，则我们也得到一个红色三角形。唯一的另一种可能性是 $\overline{p_1p_2}$ ， $\overline{p_1p_3}$ 和 $\overline{p_2p_3}$ 全是涂蓝色的，这时 $\{p_1, p_2, p_3\}$ 就是一个蓝色三角形的顶点。这就说明了 $N(3, 3; 2) \leqslant 6$ 。另一方面，我们可以举出5个点的

集合，并将这些点所确定的线段涂红、蓝色，使得所有的三角形既没有红色三角形也没有蓝色三角形。这如图2.1所示，其中五边形的线段（实线）涂上红色，而五角星形（虚线）涂上蓝色。由此可见 $N(3,3;2) > 5$ ，所以 $N(3,3;2) = 6$ 。

![](images/5c001b6ae191b18b95329f1d611dcef6f958f41523769308e151d583a061573e.jpg)  
图2.1

# 练习

1. 关于应用3，证明对每个 $k(k = 1,2,\dots ,21)$ ，必存在相继若干天，在这些天中国际象棋大师恰好参加 $\pmb{k}$ 次比赛（ $k = 21$ 的情况在应用中已经讨论），能不能断定：存在相继的若干天，国际象棋大师恰好参加22次比赛？

2*. 关于应用 4. 证明: 从整数 1, 2, …, 200 中可以选取 100 个整数, 使得其中任何一个数都不能被其余的数整除.

3*. 再关于应用 4, 证明: 如果从 1, 2, …, 200 中选取 100 个数, 其中有一个小于 18, 则所选取的数中必存在两个数, 其中一个可被另一个整除.

4. 试从1，2，…，2n的集合中选取整数来推广应用4.  
5*. 证明任意52个整数中必存在两个数, 其和或差能被100整除.  
6. 一个学生用37天准备考试。根据以往的经验她知道需要复习的

时间不超过60小时。她打算每天至少复习一小时，证明不管她如何安排学习时间表（假定每天学习时数为整数），必存在相继的若干天，她恰好学习13小时。

7.（此题不合我国情况，译者删去）  
8. 假定在人数为 6 的一群人中，每两个人或是朋友或是仇敌。证明在这 6 人中，或有 3 人彼此为朋友（每人是其他人的朋友），或有 3 人彼此为仇敌（每人是其他人的仇敌）。举例说明当人数为 5 时结论不真。  
9. 设 $S$ 是 3 维空间中任何 3 点都不共线的 6 个点的集合, 把 $S$ 的点所确定 15 条线段任意涂红色或蓝色. 证明 $S$ 的点所确定的三角形中至少有两个是红色三角形或蓝色三角形 (即两者都是红色的或都是蓝色的, 或一个是红色的而另一个是蓝色的).  
10. 证明在任何 $n$ 个人的小组中有两个人在组里有同样多的熟人.  
11. 证明边长为2的正方形内任意5点必有两点其距离不超过 $\sqrt{2}$   
12. (a) 证明边长为 1 的正三角形内任意 5 点必有两点, 其距离不超过 $-\frac{1}{2}$ .  
(b) 证明边长为 1 的正三角形内任意 10 点必有两点, 其距离不超过 $\frac{1}{3}$ .  
(c) 试确定一个整数 $m_{n}$ ，使得边长为 1 的正三角形内任意 $m_{n}$ 个点必有两点，其距离不超过 $-\frac{1}{n}$ .  
13. 设 $q_{3}$ 和 $t$ 为正整数且 $q_{3} \geqslant t$ . 试确定 Ramsey 数 $N(t, t, q_{3}: t)$ .  
14. 设 $q_{1}, q_{2}, \cdots, q_{n}$ , $t$ 为正整数且 $q_{i} \geqslant t (i = 1, 2, \cdots, n)$ , $r$ 为 $q_{1}, q_{2}, \cdots, q_{n}$ 的最大者. 证明

$$
N (r, r, \dots , r; t) \geqslant N (q _ {1}, q _ {2}, \dots , q _ {n}; t).
$$

由此断定：为证明 Ramsey 定理，只需证明 $q_{1} = q_{2} = \cdots = q_{n}$ 的情形即可。

下面两题不是鸽笼原理或Ramsey定理的应用.

15. 假定 $mn$ 个人的行进乐队排列成 $m$ 行 $n$ 列的矩阵队形，且每一行里每个人都比他左边的那个人高。假定指挥把每一列的人按身高增加的次序从前到后进行调整。证明调整后每一行仍然是从左到右按身高增加的次序排列。  
16. 证明：从集合 $\{1, 2, \dots, 2n\}$ 中任意选取 $n + 1$ 个数，其中必有两个数是互素的。

# 第三章 基本计数原理：排列与组合

本书的大多数读者都有一些解决简单的计数问题的经验，因此，都熟悉“排列”与“组合”这两个术语。计算者都有这种经验，即便看起来相当简单的问题在求解时也可能出现一些困难。一般地说来，人们为了学习数学必须运用数学。对组合数学更应如此，一个要求严格的学生应该力求作大量的习题。

本章我们介绍两个一般原理及其某些推论。这些原理是一目了然的。形式上的验证可以用数学归纳法得到。

# 3.1 两个基本原理

第一个基本原理是非常基本的。它是整体等于其部分之和的一种表述。

加法原理 若集合 $S$ 可以分成子集 $S_{1}, S_{2}, \ldots, S_{m}$ ，则确定 $S$ 中的事物个数，可先求出各子集 $S_{1}, S_{2}, \ldots, S_{m}$ 中的事物个数然后加起来。

这里应该解释一句。如果一个集合 $S$ 可以分成子集 $S_{1}, S_{2}, \ldots, S_{m}$ ，那么任何两个子集自然不会重迭。换句话说， $S$ 的每个事物恰好属于子集 $S_{1}, S_{2}, \ldots, S_{m}$ 中的某一个，因而 $S_{1}, S_{2}, \ldots, S_{m}$ 是 $S$ 的划分（允许集合 $S_{1}, S_{2}, \ldots, S_{m}$ 中某些为空集）。如果这些集合允许重迭，为计算 $S$ 中的事物个数则需要更深奥的原理——所谓容斥原理。这个原理将在第五章中研究。

例 威斯康星大学开设的不同课程门数，当无跨系课程时可以通过求出该校每个系开设的课程门数然后加起来确定（当

同一课程由一个以上的系列入目录，则出现跨系课程）。

利用“选择”这个术语，加法原理的另一种表述为：假若可用 $p$ 种方法从一堆事物中选取一物，而且可用 $q$ 种方法从另一堆事物中选取一物，则不管从哪一堆选取一物可有 $p + q$ 种选法。这种做法可以推广到两堆以上。

例 一个学生想选一门数学课或一门生物学课，但不能同时选两门课。如果该生对四门数学课和三门生物学课具备选课条件，那么该生可以按 $4 + 3 = 7$ 种方式来选择课程。

第二个原理更加复杂，却更加有用。我们叙述两个集合的情况，但是也可以推广到任意有限多个集合上。

乘法原理 如果 $A$ 是 $p$ 个事物的集合且 $B$ 是 $q$ 个事物的集合， $a$ 是 $A$ 的事物， $b$ 是 $B$ 的事物，则形如 $(a, b)$ 的有序对的个数等于 $p \times q$ 。

乘法原理的另一种表述是：如果第一个事物可用 $p$ 种方式选择，并且不论第一个事物怎么选择，第二个事物都有 $q$ 种选择方式。那么同时选择第一个事物和第二个事物的方式可有 $p \times q$ 种。

例 一个学生要选两门课程，第一门课在上午3小时之内任选1小时，第二门课在下午4小时之内也可以任选1小时，那么这个学生有 $3 \times 4 = 12$ 种可能的时间安排。

我们已经提及乘法原理可以推广到3，4或任意有限个集合上。可是与其用 $n$ 个集合来表述它，不如给出 $n = 4$ 的一个例子。

例 从 5 个成年男人, 6 个成年女人, 2 个男孩和 4 个女孩中可选取一个成年男人, 一个成年女人, 一个男孩和一个女孩的方式有 $5 \times 6 \times 2 \times 4 = 240$ 种. 可以选取一个人的方式是 $5 + 6 + 2 + 4 = 17$ 种. 后一个结果从关于 4 堆事物的加法

原理得到。

大量的计数问题为下面几类：

（i）计算事物的有序安排或有序选择数

(a) 不许任何事物重复,  
(b) 允许事物重复。

（ii）计算事物的无序安排或无序选择数

(a) 不许任何事物重复,  
(b) 允许事物重复.

我们将采用集合和重集的安排或选择的说法以区别事物的重复和不重复。重集也类似于集合，只是它的成员不必是不同的。例如，重集 $M = \{a, a, a, b, c, c, d, d, d\}$ 有10个元素，3个 $a, 1$ 个 $b, 2$ 个 $c$ 和4个 $d$ 。我们也采用指明不同元素出现的次数来表示一个重集。于是， $M$ 也可用 $\{3 \cdot a, 1 \cdot b, 2 \cdot c, 4 \cdot d\}$ 来表示。数3，1，2和4分别是重集 $M$ 各元素的重复数。集合就是所有重复数都等于1的重集。上下文足以断定其中的圆点是用来表示重集元素的重复数，还是相乘。为了包括上面的情况(b)，当没有关于事物重复数的限制时（除非对安排的大小提出限制），我们允许重集中的事物出现无限多次（无须区别无限的不同大小的情况）。这样， $a$ 和 $c$ 出现无限多次且 $b$ 和 $d$ 分别出现2次和4次的重集表示为 $\{\infty \cdot a, 2 \cdot b, \infty \cdot c, 4 \cdot d\}$ 。类型(i)的安排或选择叫做排列，类型(ii)的安排或选择称为组合。在以下两节里，将介绍集合和重集的排列和组合个数的某些一般公式。但是，并非所有的排列和组合问题都可以用这些公式解决。经常需要直接求助于加法和乘法基本原理。

例 在1000与9999之间有多少个数字不同的奇数？

在1000与9999之间的数都是4个数字的有序排列。这样，

要求某种排列的个数，我们需要进行4种选择：个位，十位，百位和千位的数字。因为我们要数的数是奇数，个位数字可以是1，3，5，7，9中的任何一个。十位数字和百位数字可以是0，1，2，…，9中的任意一个，而千位数字可以是1，2，…，9中的任意一个。这样，个位数字有5种选择。由于要求四个数字是不相同的，因此，个位数字选定后，千位数字就只有8种选择。以上两个数字选定后，百位数字就剩有8种选择。而以上三个数字都选定了，十位数字就只有7种选择了。于是，应用乘法原理，这个问题的答案是 $5 \times 8 \times 8 \times 7 = 2240$ 种。假若我们按个位、十位、百位和千位数字的次序或按千位，百位，十位和个位数字次序来选择，将会出现什么困难呢？

例 以3种不同的长度，8种不同的颜色和4种不同的直径生产粉笔，试问总共有多少种不同种类的粉笔？

在这个问题中，我们要计算的是组合数，因为粉笔的特性与上面三种数的顺序无关。利用乘法原理有 $3 \times 8 \times 4 = 96$ 种不同种类的粉笔。

例 至多使用 4 位数字可以写成多少个二进制数？

因为数必须考虑其数字的次序，故要计算的是排列问题。有4种选择要做，并且每种都可以独立地选0或1。于是，我们有 $2 \times 2 \times 2 \times 2 = 16$ 种至多4位数字的二进制数。我们也可以把16看做重集 $\{\infty, 0, \infty, 1\}$ （或 $\{4, 0, 4, 1\}$ ）的长度为4的排列个数。

# 3.2 集合的排列

令r是一个正整数。所谓n个元素集合S的r排列我们理解为

$n$ 个元素中 $r$ 个元素的有序安排。如果 $S = \{a, b, c\}$ ，那么 $ab, ac, ba, ca, bc, cb$ 是 $S$ 的 6 个 2 排列。我们用 $P(n, r)$ 表示 $n$ 个元素集合的 $r$ 排列的个数。如果 $r > n$ ，则 $P(n, r) = 0$ 。如果 $r = n$ ，则 $n$ 个元素 $S$ 的 $n$ 排列简称为 $S$ 的排列或 $n$ 个元素的排列。集合 $S = \{a, b, c\}$ 的排列是 $abc,acb, bac, bca, cab, cba$ 。于是 $P(3, 3) = 6$ 。前面我们已经知道 $P(3, 2) = 6$ 。显然对每一个正整数 $n$ ， $P(n, 1) = n$ 。

定理3.2.1 对于正整数 $n$ 和 $r, r \leqslant n$ , $P(n, r) = n(n - 1) \cdots (n - r + 1)$ .

证明 在构造 $n$ 个元素集合的 $r$ 排列时，我们可按 $n$ 种方式选择第一项；只要选择了第一项，就有 $(n - 1)$ 种方式选择第二项；…；而只要选择了前 $r - 1$ 项，则就有 $n - r + 1$ 种方式选择第 $r$ 项。根据乘法原理，这 $r$ 个项可以按 $n(n - 1) \cdots (n - r + 1)$ 种方式选择。

令 $n!$ （读作 $n$ 阶乘）等于乘积 $n \cdot (n - 1) \cdots 2 \cdot 1$ ，为方便起见规定 $0! = 1$ 。则我们可以写

$$
P (n, r) = \frac {n !}{(n - r) !} \cdot
$$

对于 $n \geqslant 0$ ，我们定义 $P(n, 0)$ 为1，并且这与上面的公式当 $n = 0$ 时是一致的。 $n$ 个元素的排列个数是

$$
P (n, \quad n) = n! _ {\bullet}
$$

例 由字母 $a, b, c, d, e$ 组成的 4 个字母的“单词”，每个字母在单词中至多使用一次，这样的单词个数为 $P(5, 4) = 51 / (5 - 4)1 = 120$ 。5 个字母的单词个数 $P(5, 5)$ 也为 120。

例著名的“15难题”是由15个可滑动的方块组成，方块标以号码1至15并且被镶嵌在如图3.1所示的 $4 \times 4$ 的框架

内。这个难题要求从所示的初始位置变成任何特定位置。所谓位置是指框架内的15个编号方块连同一个空方块的一种排列。

难题中的位置个数是多少？

这个问题等价于确定把数1，2，…，15分派到 $4\times 4$ 个格子中的16个方块中并剩下一个方块的方式数。因为我们可以把数16分派给空方块，这个问题也就等价于确定把数1，2，…，16分派给16个方块的方式数。这便有 $P(16,16) = 16!$

![](images/fb47dedc027af0c2cd1971c72bfa9d973bf24f3463914e61de59e2aadea6f29d.jpg)  
图3.1

把数1，2，…，15分派给 $6 \times 6$ 个格子的36个方块并留有21个空方块的方式数是多少？

我们把1，2，…，15分到这36个方块中15个方块的各种分派和36方块的15排列相对应。做法是：首先放置标号1的方块，然后放置标号2的方块，等等。因此，分派的总数是 $P(36, 15) = 36! / 21!$ 。

例 从 $\{1,2,\dots ,9\}$ 中选取不同的数字且使5和6不相邻的7位数有多少？

我们要计算的是集合 $\{1, 2, \dots, 9\}$ 的某种7排列，把这些7排列分成4类：(i)数字5和6都不出现；(ii)数字5出现，但数字6不出现；(iii)数字6出现，但数字5不出现；(iv)数字5和6都出现。类(i)的排列是 $\{1, 2, 3, 4, 7, 8, 9\}$ 的7排列，因而有 $P(7, 7) = 7! = 5040$ 个。类(ii)的排列的个数可如下计算。数字5可以是7位中的任何一位，剩下的6位数字是 $\{1, 2, 3, 4, 7, 8, 9\}$ 的6排列。所以，类(ii)有 $7 \times P(7, 6) = 7 \times (7!) =$

35280个。用同样的方法得到类(iii)有35280个。为了计算类(iv)的排列个数，我们考虑哪一位数字是5，分三种可能性。

第一位数字等于5：

5（÷6）

第二位数字可有7种选择，然后6可有5种选择，剩下的4位数字是余下的6个数字的4排列。所以，在这种情况下有 $7 \times 5 \times P(6, 4) = 12600$ 个排列。

最后一位数字等于5：

（÷6）5.

利用前面同样的论证，断定在这种情况下也有12600个排列。

首位或末位的数字都不是5：

（÷6）5 （÷6）

数字5可以是第二位到第六位中任何一个，因而有5种选择（上面的图示是第4位数字等于5）。紧靠5的左面那一位数字可以有7种选择，从而紧靠5的右面那一位数字可以有6种选择，其余4个位置中6可以任选一位，最后3个位置是5个数字的3排列。因此，在这种情况下共有(5)(7)(6)(4) $P(5, 3) = (5)(7)(6)(4)5! / 21 = 50400$ 个排列。

于是类(iv)有 $2 \times (12600) + 50400 = 75600$ 个排列。利用加法原理，问题的答案是

$$
5 0 4 0 + 2 \times (3 5 2 8 0) + 7 5 6 0 0 = 1 5 1 2 0 0. \tag {①}
$$

更确切地说，我们刚才研究的排列叫做线排列。我们想像把一些事物排成一条直线，如果不把事物排成直线，而排成一

个圆，那么排列个数将会减少。请用这种方式考虑，假定有10个小孩沿圆圈行进，他们构成一个圆圈的不同方式有多少？因为小孩是运动着的，重要的是他们彼此的相对位置，而不是他们相对于环境的相对位置。于是，如果一个圆排列旋转可以得到另一个圆排列，则认为这两个圆排列是相同的。因为小孩的每个线排列产生另外9个相同圆排列，为了得到圆排列个数我们必须把线排列个数除以10。于是，10个小孩的圆排列个数等于 $10! / 10 = 9!$ 。

定理3.2.2 $n$ 个元素集合的 $r$ 圆排列个数是

$$
\frac {P (n , r)}{r} = \frac {n !}{r (n - r) !}.
$$

特别， $n$ 个元素的圆排列个数是 $(n - 1)!$ 。

证明 证明本质上包含在前一段中。 $r$ 线排列可以按 $r$ 排列分组，同一组中的每一个 $r$ 线排列产生相同的 $r$ 圆排列。因为存在 $P(n, r)$ 个线排列，所以 $r$ 圆排列个数是 $P(n, r)/r$ 。

除非会产生二义性，象在讨论圆排列之前那样，我们仍旧使用“排列”代替“线排列”。

例在一个旋转圆筒上画12种不同条纹的方式数是 $P(12,12) / 12 = 111$

例 由20个不同颜色的珠子可以做成多少种项链？

项链是由20个珠子排成一圆周而成。有 $20! / 20 = 19!$ 种这种排列，因为旋转和翻转都不会将一个项链改变，所以项链的个数是 $19! / 2$

# 3.3 集合的组合

设 $r$ 是非负整数。所谓 $n$ 个元素集合 $S$ 的 $r$ 组合，我们理解

为在 $S$ 的 $n$ 个事物中不考虑次序地选择 $r$ 个。换句话说， $S$ 的 $r$ 组合是 $S$ 的 $r$ 元素子集。如果 $S = \{a, b, c, d\}$ ，则 $\{a, b, c\}, \{a, b, d\}, \{a, c, d\}, \{b, c, d\}$ 是 $S$ 的 4 个 3 组合。我们用 $C(n, r)$ 或 $\binom{n}{r}$ 表示 $n$ 个元素集合的 $r$ 组合的个数。于是，如果 $r > n$ ，则 $C(n, r) = 0$ 。如果 $n = 0$ 且 $r$ 是正整数，则 $C(0, r) = 0$ 。为今后方便起见约定 $C(0, 0) = \binom{0}{0} = 1$ 。对于非负整数 $n$ ，易知下列事实是正确的： $C(n, 0) = 1$ ， $C(n, 1) = n$ ， $C(n, n) = 1$ 。

定理3.3.1 对于 $r \leqslant n, P(n, r) = r! C(n, r)$ ，于是

$$
C (n, r) = \binom {n} {r} = \frac {n !}{r ! (n - r) !}.
$$

证明 令 $S$ 是 $n$ 个元素集合。 $S$ 的一个 $r$ 组合可以按 $P(r, r) = r!$ 个方式来排序。由于 $S$ 的每个 $r$ 排列可以通过把 $S$ 的某个 $r$ 组合排序唯一地确定，可见 $P(n, r) = r! C(n, r)$ ，于是，公式 $C(n, r)$ 由3.2节给出的 $P(n, r)$ 的公式得出。

例 已知平面上任3点不共线的25个点，它们能确定多少条直线？能确定多少个三角形？

因为这些点中任何3个点都不在同一条直线上，故每一对点确定唯一的一条直线。于是所确定的直线数等于25个元素集合的2元素子集的个数，由下式给出

$$
C (2 5, 2) = \frac {2 5 !}{2 ! 2 3 !} = 3 0 0.
$$

类似地，每3个点确定唯一的一个三角形，因此所确定的三角形个数为

$$
C (2 5, 3) = \frac {2 5 1}{3 ! 2 2 !} = 2 3 0 0.
$$

例 利用字母表中26个字母来构造8个字母的“单词”，

如果要求每个单词包含3，4或5个元音字母，那么可以构成多少个这种单词？当然在一个词中的字母使用次数不受限制。

我们根据单词中包含元音的个数来计算单词的个数。

3个元音：这些元音所占的3个位置可以按 $C(8,3)$ 种方式来选择；其它5个位置由辅音所占据，那么这些元音的位置可按 $5^{3}$ 种方式来实现，辅音的位置有 $21^{5}$ 种方式来实现，于是，具有3个元音的词的个数是

$$
C (8, 3) 5 ^ {3} 2 1 ^ {5} = \frac {8 !}{3 1 5 !} 5 ^ {3} 2 1 ^ {5}.
$$

4个元音：

$$
C (8, 4) 5 ^ {4} 2 1 ^ {4} = \frac {8 !}{4 ! 4 !} 5 ^ {4} 2 1 ^ {4}.
$$

5个元音：

$$
C (8, 5) 5 ^ {5} 2 1 ^ {3} = \frac {8 !}{5 ! 3 !} 5 ^ {5} 2 1 ^ {3}.
$$

所以，单词的总数是

$$
\frac {8 !}{3 ! 5 !} 5 ^ {3} 2 1 ^ {5} + \frac {8 1}{4 ! 4 !} 5 ^ {4} 2 1 ^ {4} + \frac {8 !}{5 ! 3 !} 5 ^ {5} 2 1 ^ {3}.
$$

由定理3.3.1可直接得到下面的推论。

推论 对于 $r \leqslant n, C(n, r) = C(n, n - r)$ 。

数 $C(n, r)$ 有非常重要又引人注目的一些性质，在第四章将研究其中某些性质。目前，我们只介绍一个更进一层的性质。

定理3.3.2 $n$ 个元素的子集合数是

$$
2 ^ {n} = C (n, 0) + C (n, 1) + \dots + C (n, n).
$$

证明 我们通过指出上面等式两边都计算 $n$ 元集合 $\pmb{S}$ 的子集个数来证明这个定理。首先，我们观察到 $\pmb{S}$ 的每个子集都是对于某个 $r = 0, 1, \dots, n$ 的 $\pmb{S}$ 的 $r$ 元素子集。由于 $C(n, r)$ 等

于 $S$ 的 $r$ 元素子集的个数。根据加法原理可知， $C(n,0) + C(n,1) + \dots + C(n,n)$ 等于 $S$ 的子集的个数。

我们也可以如下计算 $S$ 的子集个数。 $n$ 个元素在形成 $S$ 的各子集时，我们对 $n$ 个元素的每一个都可以做两种选择：它或者在这个子集里或者不在。于是利用乘法原理便有 $2^{n}$ 种方式形成 $S$ 的子集合。

定理3.3.2的这个证明是用两种不同的方法计算同一集合的事物（这时，事物是 $n$ 个元素集合的子集）并使这些结果彼此相等而得到等式的一个例子。这是组合学中的非常有效的方法，并且我们将会看到这一方法的几种其它应用。

# 3.4 重集的排列

如果 $S$ 是重集， $S$ 的 $r$ 排列是 $S$ 的 $r$ 个元素的有序安排。如果 $S$ 的元素个数是 $n$ ，则 $S$ 的 $n$ 排列也称为 $S$ 的排列。例如，如果 $S = \{2 \cdot a, 1 \cdot b, 3 \cdot c\}$ ，则 $abc$ 和 $cbcc$ 是 $S$ 的 4 排列， $abccc$ 是 $S$ 的排列。此重集 $S$ 没有 7 排列，因为 $7 > 2 + 1 + 3 = 6$ ，即 7 大于 $S$ 的元素个数。我们首先计算重集 $S$ 的 $r$ 排列的个数，这里 $S$ 的所有元素的重复数是无限的。

定理3.4.1 设 $S$ 是包含 $k$ 个不同事物而每一事物具有无限重复数的重集，则 $S$ 的 $r$ 排列的个数是 $kr$ 。

证明 在构造 $S$ 的 $r$ 排列时，我们可以按 $k$ 种不同的方式选择第一项，按 $k$ 种不同的方式选择第二项，…，以及按 $k$ 种不同的方式选择第 $r$ 项。由于 $S$ 的所有重复数是无限的，那么对任何项的不同选择个数总是 $k$ ，而不依赖前一项的选择。由乘法原理， $r$ 个项可依 $kr$ 种方式选择。

这个定理的另一种表达方式是：每个都可以无限多次使用

的 $k$ 个不同事物的 $r$ 排列的个数等于 $k^r$ 。我们也注意到，如果 $S$ 的 $k$ 个不同事物的重复数全都至少是 $r$ ，这个定理的结论仍然正确。

例 最多有 4 位数的三进制数的个数是多少？

这一问题的答案是重集 $\{\infty, 0, \infty, 1, \infty, 2\}$ 或重集 $\{4, 0, 4, 1, 4, 2\}$ 的 4 排列的个数。根据定理 3.4.1 这个数等于 $3^4 = 81$ 。

现在我们计算重集的排列，其中每个事物具有有限重复数.

定理3.4.2 设 $S$ 是具有有限重复数 $n_{1}, n_{2}, \cdots, n_{k}$ 的重集且 $n = n_{1} + n_{2} + \cdots + n_{k}$ ，则 $S$ 的排列个数等于

$$
\frac {n !}{n _ {1} \left[ n _ {2} \right] \cdots n _ {k}}.
$$

证明 给定的重集 $S$ 有 $k$ 个不同的事物 $a_{1}, a_{2}, \cdots, a_{k}$ , 其重复数分别为 $n_{1}, n_{2}, \cdots, n_{k}$ , 因此 $S$ 总共有 $n = n_{1} + n_{2} + \cdots + n_{k}$ 个元素。我们需要确定这 $n$ 个元素的排列个数。为此可按下述方式思考, 设有 $n$ 个位置, 要把 $S$ 中的每个元素都恰好置于这 $n$ 个位置中的一个。我们首先决定哪些位置要为 $a_{1}$ 所占据。由于在 $S$ 中有 $n_{1}$ 个 $a_{1}$ , 我们必须从 $n$ 个位置的集合中选择 $n_{1}$ 个位置的子集。我们可以用 $C(n, n_{1})$ 种方式做到这一点。其次要确定哪些位置要由 $a_{1}$ 所占据。这必须从尚剩有 $n - n_{1}$ 个位置中选择 $n_{2}$ 个位置, 这可以有 $C(n - n_{1}, n_{2})$ 种方式做到。接着我们看到有 $C(n - n_{1} - n_{2}, n_{3})$ 种方式选择 $a_{3}$ 所占的位置。我们仿此继续做下去并且引用乘法原理, 便得到 $S$ 的排列个数等于

$$
\begin{array}{l} C (n, n _ {1}) C (n - n _ {1}, n _ {2}) C (n - n _ {1} \dots n _ {2}, n _ {3}) \dots C (n - \\ \left. \left. n _ {1} - n _ {2} - \dots - n _ {k + 1}, n _ {k}\right), \right. \\ \end{array}
$$

应用定理3.3.1，这个数等于

$$
\begin{array}{l} \frac {n !}{n _ {1} ! (n - n _ {1}) !} \cdot \frac {(n - n _ {1}) !}{n _ {2} ! (n - n _ {1} - n _ {2}) !} \cdot \frac {(n - n _ {1} - n _ {2}) !}{n _ {3} ! (n - n _ {1} - n _ {2} - n _ {3}) !} \\ \dots \frac {\left(n - n _ {1} - n _ {2} - \cdots - n _ {i - 1}\right) !}{n _ {k} ! \left(n - n _ {1} - n _ {2} - \cdots - n _ {k} ^ {2}\right) !} \\ \end{array}
$$

化简得

$$
\frac {n !}{n _ {1} ! n _ {2} ! n _ {3} ! \cdots n _ {k} ! 0 !} = \frac {n !}{n _ {1} ! n _ {2} ! \cdots n _ {3} !}.
$$

例单词MISSISSIPPI中的字母的排列的个数是

$$
\frac {1 1 !}{1 1 4 ! 4 1 2 ! ^ {3}}
$$

所以，这个数等于重集 $\{1 \cdot M, 4 \cdot I, 4 \cdot S, 2 \cdot P\}$ 的排列个数。

如果重集 $S$ 只有两个不同的对象 $\alpha_{1}$ 和 $\alpha_{2}$ 且重复数分别为 $n_{1}$ 和 $n_{2}$ ，其中 $n = n_{1} + n_{2}$ ，那么根据定理3.4.2， $S$ 的排列个数为

$$
\frac {n !}{n _ {1} ! n _ {2} !} = \frac {n !}{n _ {1} ! (n - n _ {1}) !} = C (n, n _ {1}) 。
$$

这样，我们可以把 $C(n, n_1)$ 看作 $n$ 个元素集合的 $n_1$ 组合个数或看作 $n$ 个元素的重复数为 $n_2$ 和 $n - n_1$ 的重集的排列个数。

令 $S$ 是具有重复数 $n_{1}, n_{2}, \cdots, n_{k}$ 的 $n$ 个元素的重集，那么 $n = n_{1} + n_{2} + \cdots + n_{k}$ 。定理3.4.2提供了关于 $S$ 的 $n$ 排列个数的简单公式。一般说来，如果 $r < n$ ，不存在关于 $S$ 的 $r$ 排列个数的简单公式。尽管如此，可用生成函数的方法求解，我们将在第七章讨论。

# 3.5 重集的组合

如果 $S$ 是重集，则 $S$ 的 $r$ 组合是不计次序地从 $S$ 中选择 $r$ 个元

素．于是， $S$ 的 $\pmb{r}$ 组合本身是一个重集（ $s$ 的子重集）．如果 $s$ 有 $\pmb{n}$ 个元素，则只存在一个 $s$ 的 $\pmb{n}$ 组合，即 $s$ 自身．如果 $s$ 含有 $\pmb{k}$ 个不同元素，则存在 $\pmb{k}$ 个 $s$ 的1组合.

例 如果 $S = \{2 \cdot a, 1 \cdot b, 3 \cdot c\}$ ，则 $\{a, a\}, \{a, b\}, \{a, c\}, \{b, c\}, \{c, c\}$ 都是 $S$ 的 2 组合。

我们首先计算所有元素重复数都是无限的重集的 $r$ 组合的个数。

定理3.5.1 设 $S$ 是有 $k$ 个不同事物的重集，且每个事物的重复数是无限的，那么 $S$ 的 $r$ 组合个数等于 $C(k - 1 + r, r)$ 。

证明 设 $S$ 的不同事物是 $a_{1}, a_{2}, \cdots, a_{k}$ , 因而 $S = \{\infty \cdot a_{1}, \infty \cdot a_{2}, \cdots, \infty \cdot a_{k}\}$ . $S$ 的任何 $r$ 组合形如 $\{x_{1} \cdot a_{1}, x_{2} \cdot a_{2}, \cdots, x_{k} \cdot a_{k}\}$ , 其中 $x_{1}, x_{2}, \cdots, x_{k}$ 是非负整数且 $x_{1} + x_{2} + \cdots + x_{k} = r$ . 反之, 每个满足 $x_{1} + x_{2} + \cdots + x_{k} = r$ 的非负整数序列 $x_{1}, x_{2}, \cdots, x_{k}$ 对应 $S$ 的一个 $r$ 组合. 于是 $S$ 的 $r$ 组合个数等于方程 $x_{1} + x_{2} + \cdots + x_{k} = r$ 的非负整数解的个数. 我们证明这些解的个数等于重集 $T = \{(k - 1) \cdot 0, r \cdot 1\}$ 的排列的个数. 给定 $T$ 的一个排列, 这 $(k - 1)$ 个零把 $r$ 个 1 分成 $k$ 组. 令 $x_{1}$ 个 1 在第一个零的左边, $x_{2}$ 个 1 在第一个零和第二个零之间, $\cdots, x_{k}$ 个 1 在最后一个零的右边. 那么 $x_{1}, x_{2}, \cdots, x_{k}$ 是非负整数且 $x_{1} + x_{2} + \cdots + x_{k} = r$ . 反之, 给定非负整数 $x_{1}^{\frac{k}{2}}, x_{2}, \cdots, x_{k}$ 且 $x_{1} + x_{2} + \cdots + x_{k} = r$ , 按与上面相反的步骤构造 $T$ 的一个排列 (比如, 如果 $k = 4$ 和 $r = 5$ , 则由 01110011给出的 $T = \{3 \cdot 0, 5 \cdot 1\}$ 的排列对应 $x_{1} + x_{2} + x_{3} + x_{4} = 5$ 的由 $x_{1} = 0, x_{2} = 3, x_{3} = 0, x_{4} = 2$ 组成的一个解), 于是重集 $S$ 的 $r$ 组合个数等于重集 $T = \{(k - 1) \cdot 0, r \cdot 1\}$ 的排列个数. 根据定理 3.4.2, $T$ 的排列个数等于

$$
\frac {(k - 1 + r) !}{(k - 1) ! r !} = C (k - 1 + r, r).
$$

这个定理的另一种表达形式是：给定 $k$ 个不同事物而且每一事物可使用无限多次，其 $r$ 组合个数等于 $C(k + r - 1, r)$ 。我们注意到，如果 $S$ 的 $k$ 个不同事物的重复数全都至少是 $r$ 时，这个定理仍旧成立。

例面包店供应8种面果。如果一盒装十二个面果，能买到多少不同盒的面果？假定面包店现有大量（每种至少12个）各种面果。这是一个组合问题，因为一盒中面果次序对顾客没有什么影响。不同种的盒的样数等于具有8个不同事物且每一事物有无限重复数的重集的12组合个数。根据这个定理，盒的样数等于 $C(8 - 1 + 12, 12) = C(19, 12)$ 。

例 由1，2，…， $k$ 中选取的项组成的长为 $r$ 的递增序列的个数是多少？这样的递增序列由重集 $S=\{\infty,1,\infty,2,\cdots,\infty\}$ 的 $r$ 组合并按增加的次序排列其元素组成。于是这种序列的个数等于 $S$ 的 $r$ 组合个数，然后用定理3.5.1，该数等于 $C(k-1+r,r)$ 。

定理3.5.2 设 $S$ 是具有 $k$ 个不同元素的重集，且每个元素有无限重复数。如果要求 $S$ 的 $k$ 个不同元素的每一个至少在组合中出现一次，则 $S$ 的这种 $r$ 组合个数等于 $C(r - 1, k - 1)$ 。

证明 令 $S$ 的 $k$ 个不同元素是 $a_{1}, a_{2}, \cdots, a_{k}$ 。要计算形如 $\{x_{1} \cdot a_{1}, x_{2} \cdot a_{2}, \cdots, x_{k} \cdot a_{k}\}$ 的 $S$ 的 $r$ 组合个数，其中 $x_{1}, x_{2}, \cdots, x_{k}$ 是正整数。类似于定理3.5.1所给出的论证表明，这种 $r$ 组合的个数等于方程 $x_{1} + x_{2} + \cdots + x_{k} = r$ 的正整数解的个数。计算如下，取 $r$ 个 1 的序列，令 $r$ 个 1 之间的 $(r - 1)$ 个位置用 $p_{1}, p_{2}, \cdots, p_{r - 1}$ 表示。我们证明 $x_{1} + x_{2} + \cdots + x_{r} = r$ 的正整数解的个数等于集合 $T = \{p_{1}, p_{2}, \cdots, p_{r - 1}\}$ 的 $(k - 1)$

组合的个数。选择一个这样的 $(k - 1)$ 组合并且在 $r$ 个 1 的序列中相应的 $k - 1$ 个位置插入 0。如果 $x_{1}$ 是第一个 0 左边的 1 的个数， $x_{2}$ 是第一个 0 与第二个之间的 1 的个数，…，以及 $x_{k}$ 是最后一个 0 的右边的 1 的个数，那么 $x_{1}, x_{2}, \cdots, x_{k}$ 是正整数且 $x_{1} + x_{2} + \cdots + x_{k} = r$ 。反之，给定正整数 $x_{1}, x_{2}, \cdots, x_{k}$ 且 $x_{1} + x_{2} + \cdots + x_{k} = r$ ，可按与上面相反的步骤构造 $T$ 的 $(k - 1)$ 组合（例如，如果 $r = 6$ 和 $k = 4$ ，则 $T = \{p_{1}, p_{2}, p_{3}, p_{4}, p_{5}\}$ 的 3 组合 $\{p_{2}, p_{4}, p_{5}\}$ 对应于 110110101，它对应于 $x_{1} + x_{2} + x_{3} + x_{4} = 6$ 的解 $x_{1} = 2, x_{2} = 2, x_{3} = 1, x_{4} = 1$ ）。于是，重集的 $r$ 组合个数等于集合 $T = \{p_{1}, p_{2}, \cdots, p_{r - 1}\}$ 的 $(k - 1)$ 组合个数。根据定理 3.3.1，后者等于 $C(r - 1, k - 1)$ 。

定理3.5.2也可以直接由定理3.5.1得到（见练习24）.

例 继续考察定理3.5.1下面的例，我们由定理3.5.2可知，这8种面果中每一种至少一个的不同面果盒的个数等于 $C(12 - 1, 8 - 1) = C(11, 7) = 330$ .

计算有限重复数的重集 $r$ 组合个数有满意的方法。在讨论容斥原理之后，我们再在第五章中研究它。

# 3.6排列的生成

在本节和以后各节中，我们考虑与计数无直接关系的排列和组合的某些特性。如果 $S$ 表示集合 $\{1, 2, \dots, n\}$ ，则有 $n!$ 个 $S$ 的排列。即使 $n$ 不算太大， $n!$ 就非常大（例如， $n = 15$ 时， $15!$ 大于 $10^{12}$ ）。

$n!$ 的有效逼近由Stirling公式给出：

$$
n! \sim \sqrt {2 \pi} e ^ {- n} n ^ {n + 1 / 2}.
$$

随增大，逼近的相对误差趋于0：

$$
\lim  _ {n \rightarrow \infty \vee} \frac {n !}{2 \pi e ^ {- n n + 1 / 2}} = 1.
$$

这个公式的证明可以在许多高等微积分中和W.Feller的《Stirling公式的直接证明》一文[American Mathematical Monthly 74（1967），1223—1225]中找到。

无论在理论上还是在许多应用中，排列都是重要的。在计算机科学的分类技术中，它对应于未分类的输入数据。本节我们考虑生成 $\{1, 2, \dots, n\}$ 的所有排列的简单而又巧妙的算法。

因为 $n$ 个元素集合的排列个数相当大，为了使这一算法在计算机上行之有效，它的每一步都必须是简单易行的。下面的这个算法就有这个特点。这个算法最先由 H. Steinhaus1)发现，后又由 S.M. Johnson2)和 H.F. Trotter3)独立地发现。由于我们的兴趣不在于写出能直接应用于计算机上的算法，故只一般地讨论它，就象 M. Gardner 于 1974 年 11 月《科学美国人》杂志上的一篇论文中所做过的那样。这个算法由 $\{1, 2, \dots, n-1\}$ 的排列生成 $\{1, 2, \dots, n\}$ 的排列。于是，由 $\{1\}$ 的唯一排列开始，经过 $(n-1)$ 次例行程序就可以得到 $\{1, 2, \dots, n\}$ 的所有排列。

我们来说明这个算法.

$n = 2$ ：为产生 $\{1,2\}$ 的排列，把 $\{1\}$ 的排列写两次并把2与排列1交叉得

$$
\begin{array}{c} 1 \quad 2 \\ 1 \quad 2 \end{array}
$$

第二个排列是由交换第一个排列的两数得到的。

$n = 3$ ：为了生成 $\{1,2,3\}$ 的排列，按上面已得到的次序把 $\{1,2\}$ 的每一种排列写三次并将3在它们之间穿叉如下：

$$
\begin{array}{c c c} & 1 & 2   3 \\ & 1   3   2 \\ 3   1 & 2 \\ 3   2 & 1 \\ & 2   3   1 \\ & 2 & 1   3 \end{array}
$$

可见每一种排列是由交换前一排列的相邻两数得到的。当3不动，即3在生成序列中从第三个排列到第四个排列时，这个交换是由 $n = 2$ 时对应的交换得到。我们注意到交换最后生成的排列的1与2，便得到第一个排列123。

$n = 4$ ：为生成 $\{1,2,3,4\}$ 的排列，按上面生成次序写出 $\{1,2,3\}$ 的每一种排列4次并将4在它们之间穿叉如下：

$$
\begin{array}{r r r r} 1 & 2 & 3 & 4 \\ 1 & 2 & 4 & 3 \\ 1 & 4 & 2 & 3 \\ 4 & 1 & 2 & 3 \\ 4 & 1 & 3 & 2 \\ 1 & 4 & 3 & 2 \end{array} \qquad \qquad \begin{array}{r r r r} 1 & 3 & 4 & 2 \\ 1 & 3 & 2 & 4 \\ 3 & 1 & 2 & 4 \\ 3 & 1 & 4 & 2 \\ 3 & 4 & 1 & 2 \\ 4 & 3 & 1 & 2 \end{array}
$$

<table><tr><td>4 3</td><td>2</td><td>1</td><td></td><td>2 4</td><td>3</td><td>1</td></tr><tr><td>3 4</td><td>2</td><td>1</td><td></td><td>4 2</td><td>3</td><td>1</td></tr><tr><td>3</td><td>2</td><td>4</td><td>1</td><td>4 2</td><td>1</td><td>3</td></tr><tr><td>3</td><td>2</td><td>1</td><td>4</td><td>2 4</td><td>1</td><td>3</td></tr><tr><td>2</td><td>3</td><td>1</td><td>4</td><td>2</td><td>1</td><td>4 3</td></tr><tr><td>2</td><td>3</td><td>4</td><td>1</td><td>2</td><td>1</td><td>3 4</td></tr></table>

我们再次看到，每一个排列由交换前一个排列的相邻两数得到。当4不动时，即它在生成序列中的第四与第五，第八与第九，第十二与第十三，第十六与第十七以及第二十与第二十一的排列之间，这时交换由 $n = 3$ 时对应的交换得到。交换最后生成的排列的1与2，我们也得到第一个排列1234。

对任何 $n$ ，怎样生成 $\{1, 2, \dots, n\}$ 的所有排列现在应该清楚了。我们把证明下面问题的归纳法细节留作练习。

（i）这个算法生成 $\{1, 2, \dots, n\}$ 的全部排列。

（ii）每一个排列由交换前一个排列相邻两数得到。

（iii）第一个排列是 $123\dots n,$ ，最后一个排列是21. $\dots n$ ，于是，交换最后一个排列中的1与2，便得到第一个排列。

为了利用归纳法验证（i），只要注意到从 $\{1,2,\dots ,n\}$ 的排列删除 $n$ 剩下的就是 $\{1,2,\dots ,n - 1\}$ 的排列并且 $\{1,2,\dots ,n\}$ 的每一个排列是在 $\{1,2,\dots ,n - 1\}$ 的排列中唯一插入一个 $n$ 得到就够了。（ii）的验证上面业已指出。为验证（iii），必须注意到至少有2个元素集合的排列个数是偶数（对于 $n\geqslant 2$ ， $n!$ 是偶数）。

对任何 $n$ ，给定 $\{1,2,\dots ,n\}$ 的一个排列，可以确定它是在生成序列中哪一步出现的。反之，也可以确定在给定的一步中出现哪一个排列。为清楚地分析它，有兴趣的学生可参

考S.Even著的《算法组合学》一书（New York: Macmillan, 1973）.

# 3.7 排列的逆序

利用逆序来描述排列的方法是M.Hall，Jr1)发现的。现在我们就来讨论它。逆序的概念是一个旧概念，它在矩阵的行列式理论中起着重要作用。令 $i_1i_2\dots i_n$ 是集合 $\{1,2,\dots ,n\}$ 的一个排列，如果 $k < l$ 而 $i_k > i$ ，（ $i_k,i_l$ ）称为一个逆序。于是，逆序对应一对数，它们在排列中失去自然次序。例如，排列31524中有4个逆序，即（3，1），（3，2），（5，2），（5，4）。{1，2，…，n}的唯一没有逆序的排列是123…n。对于排列 $i_1i_2\dots i_n$ ，我们令 $a_{j}$ 表示第二个分量为 $j$ 的逆序个数。换句话说， $a_{j}$ 等于排列中先于 $j$ 且又大于 $j$ 的那些整数的个数。它度量 $j$ 的失序有多少。数列 $a_1,a_2,\dots ,a_n$ 称为排列 $i_1i_2\dots i_n$ 的逆序数列。上例给出的数列的逆序数列是1，2，0，1，0。

排列 $i_1 i_2 \cdots i_n$ 的逆序数列必须满足条件

$$
0 \leqslant a _ {1} \leqslant n - 1, \quad 0 \leqslant a _ {2} \leqslant n - 2, \dots , \quad 0 \leqslant a _ {n - 1} \leqslant 1,
$$

$$
a _ {n} = 0.
$$

这是显然的，因为对每一个 $k = 1, 2, \dots, n$ ，在集合 $\{1, 2, \dots, n\}$ 中都有 $(n - k)$ 个整数大于 $k$ 。利用乘法原理，可知满足 $0 \leqslant b_{1} \leqslant n - 1, 0 \leqslant b_{2} \leqslant n - 2, \dots, 0 \leqslant b_{n - 1} \leqslant 1, b_{n} = 0$ 的整数 $b_{1}, b_{2}, \dots, b_{n}$ 的个数等于 $n(n - 1) \cdots 2 \cdot 1 = n!$ 。我们将证

明这些数列中每一个都唯一地对应 $\{1, 2, \dots, n\}$ 的一个排列。并且指出怎样来构造这种排列。

定理3.7.1 设 $b_{1}, b_{2}, \cdots, b_{n}$ 是整数数列且 $0 \leqslant b_{1} \leqslant n - 1$ , $0 \leqslant b_{2} \leqslant n - 2$ , $\cdots$ , $0 \leqslant b_{n - 1} \leqslant 1$ , $b_{n} = 0$ ，则存在 $\{1, 2, \cdots, n\}$ 的唯一排列，其逆序数列是 $b_{1}, b_{2}, \cdots, b_{n}$ .

证明 我们介绍一种方法，可用来唯一地构造其逆序数列为 $b_{1}, b_{2}, \cdots, b_{n}$ 的一个数列。

$n$ ：写出 $n_{\bullet}$

$n - 1$ ：考察 $b_{n - 1}$ ，已知 $0\leqslant b_{n - 1}\leqslant 1$ ，如果 $b_{n - 1} = 0$ ，则 $(n - 1)$ 必在 $n$ 的前面，如果 $b_{n - 1} = 1$ ，则 $(n - 1)$ 必在 $n$ 的后面.

$n - 2$ ：考察 $b_{n - 2}$ ，已知 $0\leqslant b_{n - 2}\leqslant 2$ ，如果 $b_{n - 2} = 0$ ，则 $(n - 2)$ 必在由 $n - 1$ 步得到的两个数之前，如果 $b_{n - 2} = 1$ ，则 $(n - 2)$ 必在由 $n - 1$ 步得到的两个数之间，如果 $b_{n - 2} = 2$ ，则 $(n - 2)$ 必在由 $n - 1$ 步得到的两个数之后.

#

$n - k$ ：（一般步）考察 $b_{n - k}$ ，已知 $0\leqslant b_{n - k}\leqslant k$ ，在由 $\pmb{n}$ 到 $n - k + 1$ 的各步中， $k$ 个数 $n,n - 1,\dots ,n - k + 1$ 已按要求的次序放好。如果 $b_{n - k} = 0$ ，则 $n - k$ 必在由 $n - k + 1$ 步得到的所有数之前。如果 $b_{n - k} = 1$ ，则 $n - k$ 必在头两个数之间。如果 $b_{n - k} = k$ ，则 $n - k$ 必在所有数之后。

#

1:

作完 $n, n - 1, \dots, 2, 1$ 诸步之后，就唯一确定了 $\{1, 2, \dots, n\}$ 的排列，其逆序数列是 $b_{1}, b_{2}, \dots, b_{n}$ 。

例试确定 $\{1, 2, 3, 4, 5, 6, 7, 8\}$ 的排列，使其逆序数列为5，3，4，0，2，1，1，0。对于给定的逆序数列，按定理3.7.1证明中的步骤，得到下面结

果：

$$
\begin{array}{l} \text {8 : 8} \\ \text {7 : 8 7} \\ \text {6 : 8 6 7} \\ \text {5 : 8 6 5 7} \\ \text {4 : 4 8 6 5 7} \\ \text {3 : 4 8 6 5 3 7} \\ \text {2 : 4 8 6 2 5 3 7} \\ \text {1 : 4 8 6 2 5 1 3 7} \end{array}
$$

于是，所求排列为48625137.

习惯上，根据 $\{1, 2, \dots, n\}$ 的排列 $i_1 i_2 \dots i_n$ 的逆序个数是偶数还是奇数，称它为偶排列或奇排列。排列的符号则根据它是否为偶或奇定义为 +1 或 -1。在矩阵的行列式理论中，排列符号的概念是重要的，这里 $n \times n$ 阶矩阵 $A[a_{ij}]$ 的行列式定义为 $\sum e(i_1 i_2 \dots i_n) a_{i_1 i_2 a_{i_2 \dots} a_{i_n}}$ 。这里求和是指对 $\{1, 2, \dots, n\}$ 的所有排列求和， $e(i_1 i_2 \dots i_n)$ 为 $i_1 i_2 \dots i_n$ 的符号。

若排列 $i_1 i_2 \cdots i_n$ 具有逆序个数为 $k = b_1 + b_2 + \cdots + b_n$ 的逆序数列 $b_1, b_2, \cdots, b_n$ ，则 $i_1 i_2 \cdots i_n$ 可以相继经过 $k$ 次交换相邻两数得到 $1 2 3 \cdots n$ 。我们首先把 1 逐次与 $b_1$ 个在它左边的数进行交换，然后把 2 逐次与 $b_2$ 个在它左边的大于 2 的数交换，等等。用这种方法经 $b_1 + b_2 + \cdots + b_n$ 次交换后，我们得到 $1 2 3 \cdots n$ 。

例 逐次交换相邻两数把排列 $361245$ 变成 $123456$ 。这个逆序数列是 $220110$ 。逐次交换的结果是：

361245

<table><tr><td>3</td><td>1</td><td>6</td><td>2</td><td>4</td><td>5</td></tr><tr><td>1</td><td>3</td><td>6</td><td>2</td><td>4</td><td>5</td></tr><tr><td>1</td><td>3</td><td>2</td><td>6</td><td>4</td><td>5</td></tr><tr><td>1</td><td>2</td><td>3</td><td>6</td><td>4</td><td>5</td></tr><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>6</td><td>5</td></tr><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td></tr></table>

这个过程是计算机科学中通用分类过程的一个例子。排列 $i_{1}i_{2}\dots i_{n}$ 的元素对应着未分类的数据，关于更有效的分类技术和对它们的分析，可参见D.E.Knuth的《分类与搜索》(Volume 3 of The Art of Computer Programming; Addison-Wesley, 1973).

# 3.8 r 组合的生成

现在描述一个生成 $n$ 个元素的集合 $S$ 的所有 $r$ 组合的标准算法。假设 $S$ 的元素已按某种方式排序，不失一般性取 $S$ 为 $\{1, 2, \dots, n\}$ 。我们可以用词典排序法给 $S$ 的 $r$ 组合的集合排序。回想一下，词combination在词典中先于词combinatorics，因为从左到右计算，它们的第一个不同的字母是在词中的第九个位置，其中combination是i而combinatorics是o。在字母表的顺序中由于i先于o，因此combination先于combinatorics。我们在 $S$ 的 $r$ 组合中引入类似的顺序，由整数1，2，…，n的自然顺序代替字母表的顺序。 $\mathbf{\Delta}S = \{1, 2, \dots, n\}$ 的每个 $r$ 组合可取为 $a_1, a_2, \dots, a_r$ ，这里 $1 \leqslant a_1 < a_2 < \dots < a_r \leqslant n$ （即 $r$ 组合对应于取自 $\{1, 2, \dots, n\}$ 的长为 $r$ 的严格增加序列）。若 $b_1, b_2, \dots, b_r$ 是不同的 $r$ 组合且 $1 \leqslant b_1 < b_2 < \dots < b_r \leqslant n$ ，只要存在j使得 $a_1 = b_1, \dots, a_{j-1} = b_{j-1},$

$a_{j} \neq b_{j}$ , 且 $a_{j} < b_{j}$ , 则说 $a_{1}, a_{2}, \cdots, a_{r}$ 按词典序先于 $b_{1}, b_{2}, \cdots, b_{r}$ . 换句话说, $a_{1}, a_{2}, \cdots, a_{r}$ 按词典序先于 $b_{1}, b_{2}, \cdots, b_{r}$ , 只要在两个词中头一个不同的字母位置上（从左往右计算）, $a_{1}, a_{2}, \cdots, a_{r}$ 有比 $b_{1}, b_{2}, \cdots, b_{r}$ 小的数字.

例 考察 $S = \{1, 2, 3, 4, 5, 6, 7, 8\}$ 的 5 组合的词典序。2，3，4，6，8 先于 2，3，5，6，7。第一个 5 组合是 1，2，3，4，5；最后一个 5 组合是 4，5，6，7，8。

定理3.8.1 设 $a_1, a_2, \cdots, a_r$ 是 $S = \{1, 2, \cdots, n\}$ 的一个 $r$ 组合。若 $a_1, a_2, \cdots, a_r$ 等于 $n - r + 1, n - r + 2, \cdots, n$ ，则按词典序 $a_1, a_2, \cdots, a_r$ 是最后的 $r$ 组合。如果 $a_1, a_2, \cdots, a_r$ 不等于 $n - r + 1, n - r + 2, \cdots, n$ ，并令 $j$ 是使 $a_j + 1 \leqslant n$ 的最大整数且 $a_j + 1$ 不是 $a_1, a_2, \cdots, a_r$ 中的一个，则按词典序紧跟 $a_1, a_2, \cdots, a_r$ 的 $r$ 组合是

$$
a _ {1}, \dots , a _ {j - 1}, a _ {j} + 1, a _ {j} + 2, \dots , a _ {j} + (r - j + 1).
$$

证明 由词典序的定义可知， $n - r + 1$ ， $n - r + 2$ ， $\cdots$ ， $n$ 是最后一个 $r$ 组合。现在令 $a_1, a_2, \cdots, a_r$ 不同于 $n - r + 1$ ， $-r + 2$ ， $\cdots$ ， $n$ ，并寻找定理所指明的 $j$ 。从词典序的定义，我们断定 $a_1, a_2, \cdots, a_{j-1}, a_j + 1, a_j + 2, \cdots, a_j + (r - j + 1)$ 在 $a_1, a_2, \cdots, a_r$ 之后。而且，由 $j$ 的定义得出，这两个 $r$ 组合之间的任意 $r$ 组合 $b_1, b_2, \cdots, br$ 必须满足 $b_1 = a_1$ ， $\cdots$ ， $b_{j-1} = a_{j-1}$ 。因为 $a_1, a_2, \cdots, a_r$ 是以 $a_1, a_2, \cdots, a_j$ 开始的最后一个 $r$ 组合，所以任何一个比 $a_1, a_2, \cdots, a_r$ 大，但是在 $a_1, a_2, \cdots, a_r$ 和 $a_1, \cdots, a_{j-1}, a_j + 1, a_j + 2, \cdots, a_j + (r - j + 1)$ 之间的 $r$ 组合必以 $a_1, \cdots, a_{j-1}, a_j + 1$ 开始。因为 $a_1, \cdots, a_{j-1}, a_j + 1, a_j + 2, \cdots, a_j + (r - j + 1)$ 显然是以 $a_1, \cdots, a_{j-1}, a_j + 1$ 开始的最小 $r$ 组合，定理证毕。

从这个定理我们得出按词典序生成 $S = \{1, 2, \dots, n\}$ 的 $r$ 组合的下面算法。

（i）初始步。由1，2，…， $r$ 开始。

（ii）一般步。假定已生成 $r$ 组合 $a_1, a_2, \cdots, a_r$ 。若 $a_1 = n - r + 1, a_2 = n - r + 2, \cdots, a_r = n$ ，则停止。否则令 $j$ 是使 $a_j + 1 \leqslant n$ 并且 $a_j + 1$ 不是 $a_1, a_2, \cdots, a_r$ 中任何一个数的最大整数。构造 $r$ 组合 $a_1, a_2, \cdots, a_{j-1}, a_j + 1, a_j + 2, \cdots, a_j + (r - j + 1)$ 。

例 假若我们应用前面的算法来生成 $S = \{1, 2, 3, 4, 5, 6\}$ 的 4 组合，便得到：

<table><tr><td>1 2 3 4</td><td>1 2 5 6</td><td>2 3 4 5</td></tr><tr><td>1 2 3 5</td><td>1 3 4 5</td><td>2 3 4 6</td></tr><tr><td>1 2 3 6</td><td>1 3 4 6</td><td>2 3 5 6</td></tr><tr><td>1 2 4 5</td><td>1 3 5 6</td><td>2 4 5 6</td></tr><tr><td>1 2 4 6</td><td>1 4 5 6</td><td>3 4 5 6</td></tr></table>

如果把生成一个集合的排列算法与生成 $n$ 元素集合的 $r$ 组合算法结合起来，我们就得到了生成 $n$ 元素集合 $r$ 排列的算法。

# 练习

1. 对于性质 (a) 与 (b) 的 4 个组合的每一种, 计算数字为 1, 2, 3, 4, 5 的 4 位数个数.

（a）数字全不同.

（b）数为偶数.

2. 洗一付52张纸牌有多少种方式？

3. 给一个人分五张牌有多少种分法？五张牌都不同的人有多少？

4. 下列各数中有多少不同的正因子？

(a) $3^{4} \times 5^{2} \times 7^{3} \times 11$ .

(b) 620.

5. 作为50!的因子的10的最高次幂是多少？  
6. 比5400大并有下列性质的数有多少？

(a) 数字全不同.  
（b）不出现数字2与7.

7. 如果先生与女士交替就坐，问6位先生与6位女士围圆桌就坐有多少种方式？  
8. 若某两人拒绝相邻而坐，问12个人围圆桌就坐有多少种方式？  
9. 拥有10名男人和12名妇女的俱乐部选出4人组成一个委员会，若委员会中至少有两名妇女，有多少种组成方式？此外，若Mr.与Ms. Math拒绝在同一个委员会工作，那么委员会的组成又有多少种？  
10. 如果没有两邻的两个数在同一个集合里，由 $\{1, 2, \dots, 20\}$ 中的数可形成三个数的集合有多少种？  
11. 有15名选手，其中5名只能打后卫，8名只能打前锋，2名能打后卫或打前锋，要选出11人组成一支足球队。若一支足球队有7人打前锋4人打后卫，试确定可能的球队数？  
12. 一教室有两排，每排八个坐位，今有14名学生，5人总坐在前排，4人总在后排。问学生入座有多少种方式？  
13. 给定空间的25个点，任何四点不共面，它们可以确定多少个三角形？可以确定多少个四面体？  
14. 不用定理3.3.1中给出的公式，使用组合论证，证明

$$
C (n, r) = C (n, n - r).
$$

15. 把 8 个同样棋子摆在普通棋盘上使一个不能攻击另一个（即没有两个是在同行或同列上）有多少种方式？如果每个棋子有不同的标记，问有多少种方式？  
16. 试确定你的全名中的字母排列个数？  
17. 给定 8 个棋子, 其中 5 个是红色的而且一样, 3 个为兰色的而且一样.  
(a) 把 8 个棋子摆在 $8 \times 8$ 棋盘上使得一个不能攻击另一个, 问有多少种方式?

（b）把8个棋子摆在 $12 \times 12$ 棋盘上使得一个不能攻击另一个，问有多少种方式？

18. 一位秘书在位于她的住房东9个街段北7个街段的大厦中工作，每天她要步行16个街段上班（见下图），她有多少条可能的上班路线？若在她住房东四个街段北3个街段处开始沿东西方向的一个街段被水淹没（并且她不会游泳），那么她又有多少可能的上班路线？

![](images/5cc3b45b13b8e490b4b42e0bdd403eaf3a2f3f7087666ad5a074c4a880bb57c3.jpg)

19. 令 $S$ 是具有重复数为 $n_{1}, n_{2}, \cdots, n_{k}$ 的重集，其中 $n_{1} = 1$ 。令 $n = n_{1} + n_{2} + \cdots + n_{k}$ ，证明 $S$ 的圆排列个数等于

$$
\frac {n !}{n _ {2} ! \cdots n _ {k} ! ^ {*}}
$$

20. 在有15个队参加的足球锦标赛中，前三名授予金杯、银杯和铜杯，而最后三个队被降级。只要分别获得金、银和铜杯的足球队与降级的球队是一样的，我们就认为比赛的结果相同。试问这次锦标赛中有多少种不同的结果。  
21. 试列出重集 $\{2 \cdot a, 1 \cdot b, 3 \cdot c\}$ 的所有 3 组合和 4 组合.  
22. 试确定 $k$ 个不同元素且有限重复数分别为 $n_{1}, n_{2}, \cdots, n_{k}$ 的重集的组合（任意大小）的总数.  
23. 面包店出售六种点心，你能买多少打不同的点心？  
24. 试从定理3.5.1直接导出定理3.5.2.  
25. 试确定重集 $\{1 \cdot a_{1}, \infty \cdot a_{1}, \dots, \infty \cdot a_{k}\}$ 的 $r$ 组合个数，更一般

地，推出重复数或为1或为∞的重集的r组合个数的公式

26. 证明把 $n$ 元素集合 $S$ 划分为 $k$ 个有序集合的个数等于 $k^{n}$   
27. 证明把重集 $S = \{n \cdot a\}$ 划分成 $k$ 个有序重集个数等于 $C(k + n - 1, n)$ .  
28. 把 $n$ 个元素集合 $S$ 划分成 $k$ 个有序集合，其中第一个集合含有 $n_1$ 个元素，第二个集合含有 $n_2$ 个元素，…，第 $k$ 个集合含有 $n_k$ 个元素，证明划分的个数等于

$$
\frac {n !}{n _ {1} ! n _ {2} ! \cdots n _ {k} !}.
$$

29. 用3.6节中所叙述的算法，试确定{1.2.3.4.5}的哪一个排列在31524之后？哪个排列在31524之前？  
30. 利用数学归纳法证明在 3.6 节中描述过的生成 $\{1, 2, \dots, n\}$ 的排列的算法满足该节所提到的三个性质 (i), (ii) 和 (iii).  
31. 确定{1, 2, ..., 8}的两个排列35168274和83476125的逆序数列.   
32. 构造 $\{1, 2, \cdots, 8\}$ 的两个排列使其逆序数列为2，5，5，8，2，1，1，0与6，6，1，4，2，1，0，0。

33. {1, 2, 3, 4, 5, 6} 有多少种排列，使其恰有

(a) 15个逆序？  
(b) 14个逆序？  
（c）13个逆序？

试构造 $\{1, 2, 3, 4, 5, 6\}$ 的一个排列使其恰好有13个逆序.

34. 证明 $\{1, 2, \dots, n\}$ 的排列的最大逆序数等于 $n(n - 1) / 2$ 。试确定具有 $n(n - 1) / 2$ 个逆序的唯一排列，再确定所有比它少一个逆序的排列  
35. 试逐次交换相邻两数，由排列256143与436251生成123458.  
36. 使用3.8节中所描述的算法按词典序生成{1.2.3.4, 5}的全部2组合.  
37. 使用3.6节中所描述的算法生成{1, 2, 3, 4, 6, 8}的全部3组合.  
38. 确定{1, 2, ..., 10}的按词典序紧跟2, 3, 4, 8, 9, 10后面

的一个8组合，哪个6组合直接在2，3，4，6，9，10之前？

39. 确定 $\{1, 2, \cdots, 15\}$ 的按词典序紧跟在 $1, 2, 4, 8, 8, 14, 15$ 之后的7组合，哪一个7组合直接在 $1, 2, 4, 8, 8, 14, 15$ 之前？  
40. 按词典序生成{1，2，3}各排列的逆序数列，并写出对应的排列。再按词典序生成{1，2，3，4}的排列的逆序数列。  
41. 构造{1，2，3，4}的全部3排列  
42. 构造 $\{1, 2, 3, 4, 5, 6\}$ 的全部 4 排列.  
43. 考虑 $\{1, 2, \dots, n\}$ 的 $r$ 组合 $a_1, a_2, \dots, a_r$ ，证明当它按词典序生成 $\{1, 2, \dots, n\}$ 的 $r$ 组合在步数为

$$
\begin{array}{l} C (n, r) - C (n - a _ {1}, r) - C (n - a _ {2}, r - 1) - \dots \\ - C (n - a _ {r}, 1) \\ \end{array}
$$

处出现.

# 第四章 二项式系数

表示 $n$ 元素集合的 $k$ 组合个数 $C(n, k)$ 或 $\binom{n}{k}$ 有许多引人入胜的性质并且涉及到各种恒等式。因为它们出现在二项式定理之中，所以把它们叫做二项式系数。在计算机科学中算法分析的一些公式里，二项式系数一再出现，因此对它熟练地进行运算是必要的。本章只讨论它们的某些性质。但是，可望学得包含在这一类恒等式中的好思路和获得这类恒等式的某些方法。

# 4.1 Pascal公式

在3.3节中，对所有非负整数 $k$ 和 $n$ ，已定义了二项式系数 $\binom{n}{k}$ 。若 $k > n$ ，则 $\binom{n}{k}=0$ 。对所有的 $n, \binom{n}{0}=1$ 。若 $n$ 是正整数且 $1 \leqslant k \leqslant n$ ，则

$$
\binom {n} {k} = \frac {n !}{k ! (n - k) !} = \frac {n (n - 1) \cdots (n - k + 1)}{k (k - 1) \cdots 2 \cdot 1}. \tag {4.1.1}
$$

在定理3.3.1的推论中，我们已经注意到

$$
\left( \begin{array}{c} n \\ k \end{array} \right) = \left( \begin{array}{c} n \\ n - k \end{array} \right).
$$

这个关系对满足 $0 \leqslant k \leqslant n$ 的整数 $k$ 和 $n$ 都有效。

定理4.1.1（Pascal公式）。对于满足 $1 \leqslant k \leqslant n - 1$ 的整数 $n$ 和 $k$ ，有

$$
\left( \begin{array}{l} n \\ k \end{array} \right) = \binom {n - 1} {k} + \binom {n - 1} {k - 1}.
$$

证明 证明这个等式的一种方法是用它们的值代入二项式系数并且检查其两端是否相等。我们把这个直截了当的验证留给学生。

有一个组合论证如下。设 $S$ 是 $n$ 个事物的集合，挑出 $S$ 的一个事物，比如说 $x$ 。 $S$ 的 $k$ 组合可划分成 $A$ 和 $B$ 两类，把不含有 $x$ 的所有 $S$ 的 $k$ 组合放在 $A$ 类，其它的放在 $B$ 类中，即是 $S$ 的含有 $x$ 的 $k$ 组合。在 $A$ 类中 $S$ 的 $k$ 组合个数等于 $(n - 1)$ 个元素集合 $S - \{x\}$ 的 $k$ 组合个数，因此等于 $\binom{n-1}{k}$ ，在 $B$ 类中 $S$ 的 $k$ 组合个数等于 $(n - 1)$ 个元素集合 $S - \{x\}$ 的 $(k - 1)$ 组合个数 $\binom{n-1}{k-1}$ 。于是，由加法原理有

$$
\binom {n} {k} = \binom {n - 1} {k} + \binom {n - 1} {k - 1}.
$$

为了解释这个证明，令 $n = 5$ ， $k = 3$ 和 $S = \{x,y,z,u,v\}$ 。那么在A类中的3组合是

$$
\begin{array}{c} \left\{y, z, u \right\}, \left\{y, z, v \right\}, \left\{y, u, v \right\}, \\ \left\{z, u, v \right\}. \end{array}
$$

这些正是集合 $\{y, z, u, v\}$ 的那些3组合。在B类中的S的3组合是

$$
\begin{array}{l} \left\{x, y, z \right\}, \left\{x, y, u \right\}, \left\{x, y, v \right\}, \\ \left\{x, z, u \right\}, \left\{x, z, v \right\}, \left\{x, u, v \right\}. \end{array}
$$

在这些3组合中删除元素 $x$ ，我们得到 $\{y, z, u, v\}$ 的2组合：

$$
\begin{array}{r l} \{y, z \}, & \{y, u \}, \{y, v \}, \{z, u \}, \\ & \{z, v \}, \{u, v \}. \end{array}
$$

于是， $\left( \begin{array}{l}5\\ 3 \end{array} \right) = 10 = 4 + 6 = \left( \begin{array}{l}4\\ 3 \end{array} \right) + \left( \begin{array}{l}4\\ 2 \end{array} \right).$

利用公式

$$
\binom {n} {k} = \binom {n - 1} {k} + \binom {n - 1} {k - 1}
$$

和初始值

$$
\left( \begin{array}{c} n \\ 0 \end{array} \right) = \left( \begin{array}{c} n \\ n \end{array} \right) = 1,
$$

对所有非负整数 $n$ ，不必用公式（4.1.1），就能够计算二项式系数。用这种方法计算二项式系数时，结果往往表现为阵列形式，称之为Pascal三角形。出现在1653年Blaise Pascal Traite的《三角形算术》中的这个阵列如图4.1所示。在三角形中除了是1以外的每一项都是由上一行的两项相加得到：其一是正上方的项，另一项是紧靠它的左上方的项。这与定理4.1.1所给出的Pascal公式一致。例如，

$$
\left( \begin{array}{l} 8 \\ 3 \end{array} \right) = 5 6 = 3 5 + 2 1 = \left( \begin{array}{l} 7 \\ 3 \end{array} \right) + \left( \begin{array}{l} 7 \\ 2 \end{array} \right).
$$

$$
\begin{array}{c c c c c c c c c c c} n & \binom {n} {0} & \binom {n} {1} & \binom {n} {2} & \binom {n} {3} & \binom {n} {4} & \binom {n} {5} & \binom {n} {6} & \binom {n} {7} & \binom {n} {8} \\ \hline 0 & 1 & & & & & & & & \\ 1 & 1 & 1 & & & & & & & \\ 2 & 1 & 2 & 1 & & & & & & \\ 3 & 1 & 3 & 3 & 1 & & & & & \\ 4 & 1 & 4 & 6 & 4 & 1 & & & & \\ 5 & 1 & 5 & 1 0 & 1 0 & 5 & 1 & & & \\ 6 & 1 & 6 & 1 5 & 2 0 & 1 5 & 6 & 1 & & \\ 7 & 1 & 7 & 2 1 & 3 5 & 3 5 & 2 1 & 7 & 1 \\ 8 & 1 & 8 & 2 8 & 5 8 & 7 0 & 5 8 & 2 8 & 8 & 1 \\ ; & ; & ; & ; & ; & ; & ; & ; & ; \end{array}
$$

图4.1

有关二项式系数的许多关系，只要仔细地考察Pascal三角

形都可以发现，对称关系 $\binom{n}{k}=\binom{n}{n-k}$ 容易从这个三角形发现，定理3.3.2的恒等式 $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}+\cdots+\binom{n}{n}=2^{n}$ ，只要把Pascal三角形的一行中的数加起来就可以发现。对应于二项式系数 $\binom{n}{1}$ 的这列中的数为计数的数。对应于二项式系数 $\binom{n}{2}$ 的这一列中的数称为三角形数，它对应于图4.2所示的三角形点阵中的点数。对应于二项式系数 $\binom{n}{3}$ 的这列中的数叫做四面体数，它对应四面体点阵中的点数，希望你也来考察Pascal三角形，以得到含有二项式系数的其它关系式。很可能你所发现的某些关系在4.3节将被证明。

![](images/a0580787a24cbd1786fd80bd6861417cfb4f0ef196302e23176fbb9488babd49.jpg)  
图4.2

# 4.2 二项式定理

二项式系数是因为它出现在二项式定理中得名的。这个定理开头的一些情形，对于一切学习过一些代数的学生来说都应该知道。

定理4.2.1 设 $n$ 是一正整数，则对所有的 $x$ 和 $y$ ，有

$$
(x + y) ^ {n} = y ^ {n} + \binom {n} {1} x y ^ {n - 1} + \binom {n} {2} x ^ {2} y ^ {n - 2} + \dots
$$

$$
\begin{array}{l} + \binom {n} {n - 1} x ^ {n - 1} y + x ^ {n} \\ = \sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {k} y ^ {n - k}. \\ \end{array}
$$

第一种证明 把 $(x + y)^n$ 写成每个因子都等于 $(x + y)$ 的 $n$ 个因子连乘积. 假设我们把这个乘积展开直到没有括号为止. 由于对每个因子 $(x + y)$ 可以选择 $x$ 或选择 $y$ , 其结果有 $2^n$ 个项并且每项都可以写成 $x^k y^{n-k}$ 形式, $k = 0, 1, \ldots, n$ . 在 $k$ 个因子中选取 $x$ , 在剩下的因子中选取 $y$ 便得到 $x^k y^{n-k}$ . 于是, 项 $x^k y^{n-k}$ 在已展开的乘积中出现的次数等于 $n$ 个因子集合的 $k$ 组合个数, 即 $\binom{n}{k}$ . 因此,

$$
(x + y) ^ {n} = \sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {k} y ^ {n - k}.
$$

第二个证明 这个证明是对 $n$ 使用归纳法。如果 $n = 1$ ，公式变为

$$
\begin{array}{l} (x + y) ^ {1} = \sum_ {k = 0} ^ {1} \left( \begin{array}{l} 1 \\ k \end{array} \right) x ^ {k} y ^ {1 - k} = \left( \begin{array}{l} 1 \\ 0 \end{array} \right) x ^ {0} y ^ {1} \\ + \left( \begin{array}{c} 1 \\ 1 \end{array} \right) x ^ {1} y ^ {0} = y + x, \\ \end{array}
$$

这显然是正确的。现在假定该公式对正整数 $n$ 成立，并证明当由 $n + 1$ 代替 $n$ 时，定理也成立。我们写

$$
(x + y) ^ {n + 1} = (x + y) (x + y) ^ {n} = (y + x) (x + y) ^ {n}
$$

由归纳法假设，它可写为

$$
\begin{array}{l} (x + y) ^ {n + 1} = (y + x) \left(\sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {k} y ^ {n - k}\right) \\ = y \left(\sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {k} y ^ {n - k}\right) \\ \end{array}
$$

$$
\begin{array}{l} + x \left(\sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {i} y ^ {n - k}\right) \\ = \binom {n} {0} y ^ {n + 1} + \sum_ {k = 1} ^ {n} \binom {n} {k} x ^ {k} y ^ {n + 1 - k} \\ + \sum_ {k = 0} ^ {n - 1} \binom {n} {k} x ^ {k + 1} y ^ {n - k} + \binom {n} {n} x ^ {n + 1}. \\ \end{array}
$$

在上面第二个和式中用 $k - 1$ 代替 $k$ ，它变为

$$
\sum_ {k = 1} ^ {n} \binom {n} {k - 1} x ^ {i} y ^ {n + 1 - k}
$$

所以，

$$
\begin{array}{l} (x + y) ^ {n + 1} = y ^ {n + 1} + \sum_ {k = 1} ^ {n} \left[ \binom {n} {k} \right. \\ + \left( \begin{array}{c} n \\ k - 1 \end{array} \right) ] x: y ^ {n + 1 - k} + x ^ {n + 1} \\ \end{array}
$$

利用Pascal恒等式，上式成为

$$
(x + y) ^ {n + 1} = y ^ {n + 1} + \sum_ {k = 1} ^ {n} \binom {n + 1} {k} x ^ {k} y ^ {n + 1 - k} + x n + 1
$$

或

$$
(x + y) ^ {n + 1} = \sum_ {k = 0} ^ {n + 1} \binom {n + 1} {k} x ^ {k} y ^ {n + 1 - k}.
$$

于是，由 $n + 1$ 代替 $n$ 时，公式为真。因此，这个定理用归纳法得到了证明。

二项式定理可以写成几种其它等价形式：

$$
(x + y) ^ {n} = \sum_ {k = 0} ^ {n} \binom {n} {n - k} x ^ {k} y ^ {n - k},
$$

$$
(x + y) ^ {n} = \sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {n - k} y ^ {i},
$$

$$
(x + y) ^ {n} = \sum_ {k = 0} ^ {n} \binom {n} {n - k} x ^ {n - k} y ^ {3},
$$

其中，第一个式子由定理4.2.1和下面事实

$\binom{n}{k}=\binom{n}{n-k}$ 对于 $k=0,1,\cdots,n$

得到；其余两个通过交换 $x$ 与 $y$ 便可得到。

$y = 1$ 的情况经常出现，把它作为一种特殊情况写下来。

推论 令 $n$ 是正整数，则对所有的 $x$

$$
(1 + x) ^ {n} = \sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {k} = \sum_ {k = 0} ^ {n} \binom {n} {n - k} x ^ {k}.
$$

二项式定理的 $n = 2,3,4$ 的情况，学生们可能都熟悉：

$$
\begin{array}{l} (x + y) ^ {2} = y ^ {2} + 2 x y + x ^ {2} = x ^ {2} + 2 x y + y ^ {2} \\ (x + y) ^ {3} = y ^ {3} + 3 x y ^ {2} + 3 x ^ {2} y + x ^ {3} = x ^ {3} + 3 x ^ {2} y + 3 x y ^ {2} \\ + y ^ {3} \\ \end{array}
$$

$$
\begin{array}{l} (x + y) ^ {4} = y ^ {4} + 4 x y ^ {3} + 6 x ^ {2} y ^ {2} + 4 x ^ {3} y + x ^ {4} \\ = x ^ {4} + 4 x ^ {3} y + 6 x ^ {2} y ^ {2} + 4 x y ^ {3} + y ^ {4}. \\ \end{array}
$$

我们注意到，这些展开式中的系数都是Pascal三角形的某些行的数。由定理4.2.1和Pascal三角形的构造可知，它们是正确的。

# 4.3 恒等式

正如我们已经指出，二项式系数满足很多恒等式。恒等式

$\binom{n}{k}=\frac{n}{k}\binom{n-1}{k-1}$ , 对于正整数 $n$ 和 $k$ (4.3.1)

直接由当 $k > n$ 时， $\binom{n}{k}=0$ 与对于 $1 \leqslant k \leqslant n$ 时，

$$
\binom {n} {k} = \frac {n (n - 1) \cdots (n - k + 1)}{k (k - 1) \cdots 1}
$$

得到.

恒等式

$$
\binom {n} {0} + \binom {n} {1} + \binom {n} {2} + \dots + \binom {n} {n} = 2 ^ {n} \quad \text {对 于 正 整 数} n \tag {4.3.2}
$$

已经作为定理3.3.2证明过，它也能从二项式定理中令 $x = y = 1$ 得出。如果我们在二项式定理中令 $y = 1$ ， $x = -1$ ，则有

$$
\binom {n} {0} - \binom {n} {1} + \binom {n} {2} - \dots + (- 1) ^ {n} \binom {n} {n} = 0 \text {对 于 正 整 数} n. \tag {4.3.3}
$$

也可以把它写成

$$
\binom {n} {0} + \binom {n} {2} + \dots = \binom {n} {1} + \binom {n} {3} + \dots \quad \text {对 于 正 整 数} n. \tag {4.3.4}
$$

这个恒等式可以解释如下：如果 $S$ 是 $n$ 个元素的集合，则 $S$ 的具有偶数个元素的子集个数等于 $S$ 的具有奇数个元素的子集个数。

利用恒等式（4.3.1）和（4.3.2），可以导出下面的恒等式：

$$
1 \binom {n} {1} + 2 \binom {n} {2} + \dots + n \binom {n} {n} = n 2 ^ {n - 1} \quad \text {对 于 正 整 数} n. \tag {4.3.5}
$$

为弄清楚这一点，我们注意到恒等式（4.3.1）及（4.3.2），那么（4.3.5）左端等于

$$
\begin{array}{l} n \binom {n - 1} {0} + n \binom {n - 1} {1} + \dots + n \binom {n - 1} {n - 1} = n \left[ \binom {n - 1} {0} \right. \\ \left. + \binom {n - 1} {1} + \dots + \binom {n - 1} {n - 1} \right] = n \cdot 2 ^ {n - 1}. \\ \end{array}
$$

导出（4.3.5）的另一种方法如下：由二项式定理的推论

$$
(1 + x) ^ {n} = \binom {n} {0} + \binom {n} {1} x + \binom {n} {2} x ^ {2} + \dots + \binom {n} {n} x ^ {n}.
$$

若对 $x$ 微分其两端，则得

$$
\begin{array}{l} n (1 + x) ^ {n - 1} = \binom {n} {1} + 2 \binom {n} {2} x + 3 \binom {n} {3} x ^ {2} + \dots \\ + n \left( \begin{array}{c} n \\ n \end{array} \right) x ^ {n - 1}, \\ \end{array}
$$

令 $x = 1$ ，即得到（4.3.5）.

许多有趣的恒等式都可以利用逐次微分和二项级数乘法导出。为简化起见，现在我们使用和号。从

$$
(1 + x) ^ {n} = \sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {k}
$$

开始. 对 $x$ 微分其两端得

$$
n (1 + x) ^ {n - 1} = \sum_ {k = 1} ^ {n} k \binom {n} {k} x ^ {k = 1}.
$$

两端乘以 $x$ 得

$$
n x (1 + x) ^ {k - 1} = \sum_ {k = 1} ^ {n} k \binom {n} {k} x ^ {k}.
$$

再对 $x$ 微分其两端得

$$
n [ (1 + x) ^ {n - 1} + (n - 1) x (1 + x) ^ {n - 2} ] = \sum_ {k = 1} ^ {n} k ^ {2} \binom {n} {k} x ^ {k - 1}.
$$

令 $x = 1$ ，得到

$$
n [ 2 ^ {n - 1} + (n - 1) 2 ^ {n - 2} ] = \sum_ {k = 1} ^ {n} k ^ {2} \binom {n} {k},
$$

因而有

$$
n (n \div 1) 2 ^ {n - 2} = \sum_ {k = 1} ^ {n} k ^ {2} \binom {n} {k} \quad \text {对 正 整 数} n. \tag {4.3.6}
$$

其它恒等式可以利用组合推理导出，例如

$$
\sum_ {k = 0} ^ {n} {\binom {n} {k}} ^ {2} = {\binom {2 n} {n}} \qquad {\text {对 于 正 整 数}} n. \tag {4.3.7}
$$

为了证明这个恒等式，令 $S$ 是 $2n$ 个元素的集合，则（4.3.7）的右端为 $S$ 的 $n$ 组合个数。现在把 $S$ 划分成两个 $n$ 元素子集 $A$ 和 $B$ 。 $S$ 的每个 $n$ 组合是 $A$ 的 $k$ 组合与 $B$ 的 $(n - k)$ 组合的并集， $k = 0, 1, \dots, n$ 。 $A$ 的 $k$ 组合个数是 $\binom{n}{k}$ ， $B$ 的 $(n - k)$ 组合个数是 $\binom{n}{n - k}$ 。于是，应用乘法原理， $S$ 的由 $A$ 的 $k$ 组合与 $B$ 的 $(n - k)$ 组合的并集形成的 $n$ 组合个数是

$$
\binom {n} {k} \binom {n} {n - k} = \binom {n} {k} ^ {2}.
$$

因此，由加法原理， $S$ 的n组合个数等于 $\sum_{k = 0}^{n}\binom{n}{k}^2$ .由于 $\binom{2n}{n}$ 也等于 $S$ 的n组合个数，这就证明了（4.3.7）.

通常把数 $\binom{n}{k}$ 的定义域扩充到 $n$ 是任何实数与 $k$ 是任何整数（包括负整数）。这只需要定义

$$
\binom {r} {k} = \frac {r (r - 1) \cdots (r - k + 1)}{k (k - 1) \cdots 1} \text {对 于 实 数} r \text {和 正 整 数} k,
$$

$$
\left( \begin{array}{c} {{r}} \\ {{0}} \end{array} \right) = 1 \qquad \qquad \text {对 于 实 数} r,
$$

$$
\left( \begin{array}{c} {r} \\ {k} \end{array} \right) = 0 \quad \text {对 于 实 数} r \text {和 负 整 数} k,
$$

就行了，于是，

$$
\left(\frac {7}{2}\right) = \frac {\left(\frac {7}{2}\right) \left(\frac {5}{2}\right) \left(\frac {3}{2}\right) \left(\frac {1}{2}\right) \left(- \frac {1}{2}\right)}{5 \cdot 4 \cdot 3 \cdot 2 \cdot 1}.
$$

对于：是实数，只要 $k \neq 0$ 恒等式（4.3.1）仍然有效。

同样，Pascal公式

$$
\left( \begin{array}{c} r \\ k \end{array} \right) = \left( \begin{array}{c} r - 1 \\ h \end{array} \right) + \left( \begin{array}{c} r - 1 \\ k - 1 \end{array} \right)
$$

对于所有实数 $r$ 和整数 $k$ 也有效。使用推广的二项式系数，可以对所有实数 $r$ 和非负整数 $k$ 导出下面求和公式：

$$
\begin{array}{l} \left( \begin{array}{c} r \\ 0 \end{array} \right) + \left( \begin{array}{c} r + 1 \\ 1 \end{array} \right) + \left( \begin{array}{c} r + 2 \\ 2 \end{array} \right) + \dots + \left( \begin{array}{c} r + k \\ k \end{array} \right) \\ = \left( \begin{array}{c} r + k + 1 \\ k \end{array} \right). \tag {4.3.8} \\ \end{array}
$$

重复应用Pascal公式可以证明这个恒等式：

$$
\begin{array}{l} \left( \begin{array}{c} r + k + 1 \\ k \end{array} \right) = \left( \begin{array}{c} r + k \\ k \end{array} \right) + \left( \begin{array}{c} r + k \\ k - 1 \end{array} \right) \\ = \binom {r + k} {k} + \binom {r + k - 1} {k - 1} + \binom {r + k - 1} {k - 2} \\ = \dots \\ = \binom {r + k} {k} + \binom {r + k - 1} {k - 1} + \binom {r + k - 2} {k - 2} \\ + \dots + \binom {r + 1} {1} + \binom {r + 1} {0} \\ = \binom {r + k} {k} + \binom {r + k - 1} {k - 1} + \binom {r + k - 2} {k - 2} \\ + \dots + \left( \begin{array}{c} r + 1 \\ 1 \end{array} \right) + \left( \begin{array}{c} r \\ 0 \end{array} \right) + \left( \begin{array}{c} r \\ - 1 \end{array} \right). \\ \end{array}
$$

由于 $\binom{r}{-1}=0$ ，（4.3.8）得证。

对于非负整数 $n$ 和 $k$ ，与（4.3.8）伴随的求和公式如下：

$$
\left( \begin{array}{l} 0 \\ h \end{array} \right) + \left( \begin{array}{l} 1 \\ h \end{array} \right) + \left( \begin{array}{l} 2 \\ h \end{array} \right) + \dots + \left( \begin{array}{l} n \\ k \end{array} \right) = \left( \begin{array}{l} n + 1 \\ h + 1 \end{array} \right). \tag {4.3.9}
$$

对于固定的 $h$ ，我们对 $\pmb{n}$ 使用归纳法证明这个公式。如果 $n = 0$ 公式变为

$$
\left( \begin{array}{c} 0 \\ k \end{array} \right) = \left( \begin{array}{c} 1 \\ k + 1 \end{array} \right),
$$

对于 $k = 0$ ，两端都是1。对于 $k > 0$ ，两端都是0。于是，(4.3.9)对于 $n = 0$ 为真。现在假定它对于 $n$ 为真，往证当由 $n + 1$ 代替 $n$ 时，它也为真。由归纳法假设与Pascal公式，得知

$$
\begin{array}{l} \left( \begin{array}{c} 0 \\ k \end{array} \right) \div \left( \begin{array}{c} 1 \\ k \end{array} \right) + \left( \begin{array}{c} 2 \\ k \end{array} \right) + \dots + \left( \begin{array}{c} n \\ k \end{array} \right) + \left( \begin{array}{c} n + 1 \\ k \end{array} \right) \\ = \binom {n + 1} {k + 1} + \binom {n + 1} {k} \\ = \left( \begin{array}{c} n + 2 \\ k + 1 \end{array} \right). \\ \end{array}
$$

于是，当由 $n + 1$ 代替 $n$ 时，（4.3.9）为真。因此，根据归纳法公式普遍成立。

如果在（4.3.9）中令 $k = 1$ ，则得到

$$
\left( \begin{array}{l} 0 \\ 1 \end{array} \right) + \left( \begin{array}{l} 1 \\ 1 \end{array} \right) + \left( \begin{array}{l} 2 \\ 1 \end{array} \right) + \dots + \left( \begin{array}{l} n \\ 1 \end{array} \right) = \left( \begin{array}{l} n + 1 \\ 2 \end{array} \right)
$$

或

$$
1 + 2 + \dots + n = \frac {(n + 1) n}{2},
$$

这就是有名的关于前 $n$ 个正整数的求和公式。在第六章中，我们将研究怎样求前 $n$ 个正整数的各种幂之和。

许多其它有趣和有用的恒等式在练习中给出，其中包括含有二项式系数乘积的某些重要恒等式。我们把本节用到的推导恒等式的一些方法概括如下：

（i）利用二项式系数公式。  
（ii）利用二项式系数的Pascal公式。  
（iii）数学归纳法  
（iv）利用二项式定理，包括微分和积分。  
（v）组合论证方法。

# 4.4 二项式系数的单峰性质

如果检查位于Pascal三角形一行中的二项式系数，会发现开始这些数增加而后减少。具有这种性质的数列称为单峰的。于是，数序 $s_0, s_1, \cdots, s_{it}$ 是单峰的，只要存在整数 $t$ 满足 $0 \leqslant t \leqslant n$ ，使得

$$
s _ {0} \leqslant s _ {1} \leqslant \dots \leqslant s _ {l}, \quad s _ {l} \geqslant s _ {l + 1} \geqslant \dots \geqslant s _ {n}.
$$

数 $s_{i}$ 称为数列中的最大数。整数 $t$ 不必唯一。

例如，如果 $s_0 = 1, s_1 = 3, s_2 = 3, s_3 = 2$ ，则

$$
s _ {0} \leqslant s _ {1} \leqslant s _ {2}, s _ {2} \geqslant s _ {3},
$$

但是也有

$$
s _ {0} \leqslant s _ {1}, s _ {1} \geqslant s _ {2} \geqslant s _ {3}.
$$

定理4.4.1 令 $n$ 是正整数，二项式系数数列

$$
\left( \begin{array}{c} n \\ 0 \end{array} \right), \left( \begin{array}{c} n \\ 1 \end{array} \right), \left( \begin{array}{c} n \\ 2 \end{array} \right), \dots , \left( \begin{array}{c} n \\ n \end{array} \right)
$$

是单峰数列，若 $n$ 是偶数，则

$$
\begin{array}{l} \binom {n} {0} <   \binom {n} {1} <   \dots <   \binom {n} {n / 2}, \binom {n} {n / 2} > \dots > \\ \left( \begin{array}{c} n \\ n - 1 \end{array} \right) > \left( \begin{array}{c} n \\ n \end{array} \right). \\ \end{array}
$$

若 $n$ 是奇数，则

$$
\begin{array}{l} \binom {n} {0} <   \binom {n} {1} <   \dots <   \binom {n} {(n - 1) / 2} = \binom {n} {(n + 1) / 2} > \dots > \\ \left( \begin{array}{c} n \\ n - 1 \end{array} \right) > \left( \begin{array}{c} n \\ n \end{array} \right). \\ \end{array}
$$

证明 我们考察这个数列中的相邻的二项式系数之比。令 $1 \leqslant k \leqslant n$ ，有

$$
\frac {\binom {n} {k}}{\binom {n} {k - 1}} = \frac {\frac {n !}{k ! (n - k) !}}{\frac {n !}{(k - 1) ! (n - k + 1) !}} = \frac {n - k + 1}{k}.
$$

因此，根据 $k < n - k + 1$ ， $k = n - k + 1$ 或 $k > n - k + 1$ 有

$$
\left( \begin{array}{c} n \\ k - 1 \end{array} \right) <   \left( \begin{array}{c} n \\ k \end{array} \right), \left( \begin{array}{c} n \\ k - 1 \end{array} \right) = \left( \begin{array}{c} n \\ k \end{array} \right),
$$

或 $\binom{n}{k-1} > \binom{n}{k}$ .

于是，当且仅当 $k < (n + 1) / 2$ 时 $k < n - k + 1$ 。如果 $n$ 是偶数，则 $\frac{k}{n + 1} / 2$ 等价于 $k \leqslant n / 2$ 。若 $n$ 是奇数，则 $k < (n + 1) / 2$ 等价于 $k \leqslant (n - 1) / 2$ 。因此，如同定理所叙述的那样，二项式系数是增加的。我们还注意到当且仅当 $2k = n + 1$ 时， $k = n - k + 1$ 。若 $n$ 是偶数，对于任何 $k$ ， $2k \div n + 1$ 。若 $n$ 是奇数，则对于 $k = (n + 1) / 2$ ，有 $2k = n + 1$ 。因此，对于 $n$ 是偶数，在数列中没有两个相邻的二项式系数是相等的。对于 $n$ 是奇数，仅有两个相邻的二项式系数相等，它们是

$$
\binom {n} {(n + 1) / 2} \text {和} \binom {n} {(n - 1) / 2}.
$$

用类似的方法得出象定理叙述的那样二项式系数是减少的。

对于任意实数 $x$ ，令 $[x]$ 表示小于或等于 $x$ 的最大整数。例如， $\left[2\frac{1}{2}\right] = 2, [3] = 3$ 和 $\left[-1\frac{1}{2}\right] = -2.$

推论 对于正整数 $n$ ，二项式系数

$$
\left( \begin{array}{c} n \\ 0 \end{array} \right), \left( \begin{array}{c} n \\ 1 \end{array} \right), \left( \begin{array}{c} n \\ 2 \end{array} \right), \dots , \left( \begin{array}{c} n \\ n \end{array} \right)
$$

的最大数是 $\binom{n}{n/2}$ 。

证明 我们注意到：在定理4.4.1中对于 $n$ 是偶数 $[n / 2] =$

$n / 2$ ；对于 $n$ 是奇数， $[n / 2] = (n - 1) / 2$ ，便可得出这个推论。

# 4.5多项式定理

当 $n$ 是一正整数时，二项式定理提供了关于 $(x + y)^{n}$ 的一个公式。它可以推广到公式 $(x + y + z)^{n}$ 或更一般地推广到 $t$ 个实数之和的 $n$ 次幂，即 $(x_{1} + x_{2} + \dots + x_{t})^{n}$ 的公式上去。在一般的公式中，二项式系数由

$$
\frac {n !}{n _ {1} ! n _ {2} ! \cdots n _ {i} !}
$$

取代，其中 $n_1, n_2, \dots, n_l$ 是非负整数且 $n_1 + n_2 + \dots + n_l = n$ . 这个数记为

$$
\left( \begin{array}{c} n \\ n _ {1} n _ {2} \dots n _ {l} \end{array} \right),
$$

并且称为多项式系数。回想在3.4节中这个数表示重复数为 $n_1, n_2, \cdots, n_l$ 的重集的排列数。对于 $k = 0, 1, \cdots, n$ 的二项式系数 $\binom{n}{k}$ 的值为

$$
\frac {n !}{k ! (n - k) !}
$$

用上述记号它变为

$$
\left( \begin{array}{c} n \\ k n - k \end{array} \right)
$$

并且表示重复数为 $k$ 与 $n - k$ 的重集的排列个数。

在叙述一般定理之前，让我们考虑一个特殊情况。令 $x_{1}, x_{2}, x_{3}$ 是实数。若我们把 $(x_{1} + x_{2} + x_{3})^{3}$ 乘开直到没有括号为止（建议读者去完成），得到其和

$$
x _ {1} ^ {3} + x _ {2} ^ {3} + x _ {3} ^ {3} + 3 x _ {1} ^ {2} x _ {2} + 3 x _ {1} ^ {2} x _ {3} + 3 x _ {1} x _ {2} ^ {2} + 3 x _ {2} ^ {2} x _ {3} +
$$

$$
3 x _ {1} x _ {3} ^ {2} + 3 x _ {2} x _ {3} ^ {2} + 6 x _ {1} x _ {2} x _ {3}.
$$

在上面的和式中出现的项全都形如 $x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}$ ，其中 $n_1, n_2, n_3$ 是非负整数且 $n_1 + n_2 + n_3 = 3$ 。 $x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}$ 的系数恰好是数

$$
\left( \begin{array}{c} 3 \\ n _ {1} n _ {2} n _ {3} \end{array} \right) = \frac {3 !}{n _ {1} ! n _ {2} ! n _ {3} !}.
$$

定理4.5.1 设 $n$ 是一正整数，则对所有 $x_{1}, x_{2}, \cdots, x_{t}$ 有

$$
(x _ {1} + x _ {2} + \dots + x _ {t}) ^ {n} = \sum \left( \begin{array}{c} n \\ n _ {1} n _ {2} \dots n _ {t} \end{array} \right) x _ {1} ^ {n _ {1}} x _ {2} ^ {n _ {2}} \dots x _ {t} ^ {n _ {t}},
$$

其中求和是在使 $n_1 + n_2 + \dots + n_i = n$ 的所有非负整数数列 $n_1, n_2, \dots, n_i$ 上进行的。

证明 为证明这个定理，我们推广二项式定理的第一个证明。把 $(x_{1} + x_{2} + \dots + x_{t})^{n}$ 写成 $n$ 个相同因子 $(x_{1} + x_{2} + \dots + x_{t})$ 的乘积。假定把这个乘积展开直到没有括号为止。因为对于每个因子可以选择 $x_{1}, x_{2}, \dots, x_{t}$ 中的任何一个，故总计有 $t^{n}$ 项，并且每项可写成 $x_{1}^{n} x_{2}^{n} \dots x_{t}^{n}$ 形式，其中 $n_{1}, n_{2}, \dots, n_{t}$ 是其和等于 $n$ 的非负整数。总计有 $n$ 个因子，在 $n_{1}$ 个因子中选择 $x_{1}$ ，剩下 $n - n_{1}$ 个因子，在其中的 $n_{2}$ 个因子中选择 $x_{2}, \dots$ 在剩下的 $n - n_{1} - n_{2} - \dots - n_{i-1}$ 个因子中选择 $n_{i}$ 个，在其中取 $x_{i}$ 以得到项 $x_{1}^{n_1} x_{2}^{n_2} \dots x_{i}^{n_i}$ 。于是，由乘法原理出现 $x_{1}^{n_1} x_{2}^{n_2} \dots x_{i}^{n_i}$ 的次数为

$$
\binom {n} {n _ {1}} \binom {n - n _ {1}} {n _ {2}} \dots \binom {n - n _ {1} - n _ {2} - \dots - n _ {i - 1}} {n _ {i}}.
$$

在3.4节中我们已经看到，这个数等于

$$
\frac {n !}{n _ {1} ! n _ {2} ! \cdots n _ {l} !}.
$$

这就证明了该定理。

例 当展开 $(x_{1} + x_{2} + x_{3} + x_{4} + x_{5})^{7}$ 时， $x_{1}^{2}x_{3}x_{4}^{3}x_{6}$ 的系数等于

$$
\left( \begin{array}{l l l l} 7 & \\ 2, & 0, & 1, & 3, & 1 \end{array} \right) = \frac {7 !}{2 ! 0 ! 1 ! 3 ! 1 !} = 4 2 0
$$

例当展开 $(2x_{1} - 3x_{2} + 5x_{3})^{0}$ 时， $x_{1}^{3}x_{2}x_{3}^{2}$ 的系数等于

$$
\left( \begin{array}{l} 6 \\ 3, 1, 2 \end{array} \right) 2 ^ {3} (- 3) (5) ^ {2} = - 3 6, 0 0 0.
$$

# 4.6 Newton 二项式定理

在1676年Newton推广了4.2节中的二项式定理得到 $(x + y)^{\alpha}$ 的展开式，其中 $\alpha$ 是任意实数。不过，对于一般的指数，展开式成为一个无限级数从而需要研究收敛问题。我们只叙述这个定理和讨论某些特殊情况。该定理的证明可以在许多高等微积分教程中找到。

定理4.6.1 设 $\alpha$ 是一个实数，则对于满足 $|x / y| < 1$ 的所有 $x$ 和 $y$ 有

$$
\cdot (x + y) ^ {a} = \sum_ {h = 0} ^ {\infty} \binom {a} {h} x ^ {h} y ^ {a - k},
$$

这里

$$
\binom {a} {k} = \frac {a (a - 1) \cdots (a - k + 1)}{k !}.
$$

如果 $\alpha$ 是正整数 $n$ ，因为对于 $k > n, \binom{n}{k} = 0$ ，上面展开式就变为

$$
(x + y) ^ {n} = \sum_ {k = 0} ^ {n} \binom {n} {k} x ^ {k} y ^ {n - k}.
$$

这与4.2节的二项式定理一致。

若令 $z = x / y$ ，则 $(x + y)^{\alpha} = y^{\alpha}(z + 1)^{\alpha}$ ，于是，定理4.6.1可以等价地表示为：对于 $|z| < 1$ 的任何 $z$

$$
(z + 1) ^ {\alpha} = \sum_ {k = 0} ^ {\infty} \left( \begin{array}{c} \alpha \\ k \end{array} \right) z k _ {\bullet}
$$

如果 $n$ 是正整数而且 $\alpha = -n$ ，则 $\alpha$ 为负指数，那么，

$$
\begin{array}{l} \binom {a} {k} = \binom {- n} {k} = \frac {- n (- n - 1) \cdots (- n - k + 1)}{k !} \\ = (- 1) ^ {\frac {k n (n + 1) \cdots (n + k - 1)}{k !}} \\ = (- 1) ^ {k} \binom {n + k - 1} {k}. \\ \end{array}
$$

这样，对于 $|z| < 1$

$$
(z + 1) ^ {- n} = \frac {1}{(z + 1) ^ {n}} = \sum_ {k = 0} ^ {\infty} (- 1) ^ {k} \binom {n + k - 1} {k} z ^ {k}.
$$

对于 $n = 1$

$$
\left( \begin{array}{c} n + k - 1 \\ k \end{array} \right) = \left( \begin{array}{c} k \\ k \end{array} \right) = 1,
$$

从而得到

$$
\frac {1}{z + 1} = \sum_ {k = 0} ^ {\infty} (- 1) ^ {k} z ^ {k}, \quad | z | <   1.
$$

如果用 $-z$ 代替 $z$ ，就得到一个熟知的无限几何级数的和，

$$
\frac {1}{1 - z} = \sum_ {h = 0} ^ {\infty} z ^ {2} = 1 + z + z ^ {2} + \dots , | z | <   1.
$$

二项式定理可用来得到所要求的任何精度的平方根。如果取 $\alpha = \frac{1}{2}$ ，则

$$
\left( \begin{array}{c} \alpha \\ 0 \end{array} \right) = 1,
$$

而对于 $k > 0$

$$
\begin{array}{l} \left( \begin{array}{l} \alpha \\ k \end{array} \right) = \left( \begin{array}{l} \frac {1}{2} \\ k \end{array} \right) = \frac {\frac {1}{2} \left(- \frac {1}{2} - 1\right) \cdots \left(\frac {1}{2} - k + 1\right)}{k !} \\ = \frac {(- 1) ^ {k - 1}}{2 ^ {3}} \cdot \frac {1 \cdot 3 \cdots (2 k - 3)}{k !} \\ = \frac {(- 1) ^ {k - 1}}{2 ^ {k}} \frac {1 \cdot 2 \cdot 3 \cdot 4 \cdots (2 k - 3) (2 k - 2)}{2 \cdot 4 \cdots (2 k - 2) \cdot k !}. \\ = \frac {(- 1) ^ {k - 1}}{k \cdot 2 ^ {2 k - 1}} \cdot \frac {(2 k - 2) !}{(k - 1) ! ^ {2}} \\ = \frac {(- 1) ^ {k - 1}}{k \cdot 2 ^ {2 k - 1}} \left( \begin{array}{c} 2 k - 2 \\ k - 1 \end{array} \right). \\ \end{array}
$$

于是，对于 $|z| < 1$

$$
\begin{array}{l} \sqrt {1 + z} = (1 + z) ^ {1 / 2} = 1 + \sum_ {k = 1} ^ {\infty} \frac {(- 1) ^ {k - 1}}{k \cdot 2 ^ {2 k - 1}} \binom {2 k - 2} {k - 1} z ^ {k} \\ = 1 + \frac {1}{2} z - \frac {1}{2 \cdot 2 ^ {3}} \binom {2} {1} z ^ {2} \\ + \frac {1}{3 * 2 ^ {5}} \left(\frac {4}{2}\right) z ^ {3} - \dots \\ \end{array}
$$

例如，

$$
\begin{array}{l} \sqrt {2 0} = \sqrt {1 6 + 4} = 4 \sqrt {1 + 0 . 2 5} \\ = 4 [ 1 + \frac {1}{2} (0. 2 5) - \frac {1}{8} (0. 2 5) ^ {2} \\ + \frac {1}{1 6} (0. 2 5) ^ {3} - \dots ] \\ = 4, 4 7 2 \dots \\ \end{array}
$$

# 练习

1. 利用二项式系数的公式证明Pascal公式.  
2. 填上Pascal三角形的对应于 $n = 9$ 与 $n = 10$ 的两行  
3. 考察位于Pascal三角形由左向右上方的所有对角线上的二项式系数之和，前面几个是：1，1， $i+1=2$ ， $1+2=3$ ， $1+3+1=5$ ， $1+4+3=8$ 。多计算几个这些对角线之和，然后确定这些和有什么关系（把它与第一章练习4的计数函数 $f$ 的值相比较）。  
4. 利用二项式定理展开 $(x + y)^{5}$ 和 $(x + y)^{6}$ .  
5. 利用二项式定理展开 $(2x - 7)^{2}$   
6. 在 $(3x - 2y)^{18}$ 展开式中 $x^{5}y^{13}$ 的系数是什么？ $x^{8}y^{9}$ 的系数是什么？  
7. 使用二项式定理证明

$$
3 n - \sum_ {k = 0} ^ {n} \binom {n} {k} 2 ^ {k}.
$$

试推广到对于任何实数 $r$ ，求 $\sum_{k=0}^{n} \binom{n}{k} r^k$ 的和。

8. 运用二项式定理证明

$$
2 ^ {n} = \sum_ {k = 0} ^ {n} (- 1) ^ {k} \binom {n} {k} 3 ^ {n - k}.
$$

9. 证明对于 $r \neq k$ 的实数 $r$ 和整数 $k$ 有

$$
\left( \begin{array}{l} r \\ k \end{array} \right) = \frac {r}{r - k} \left( \begin{array}{l} r - 1 \\ k \end{array} \right).
$$

10. 证明对于至少为2的正整数 $n$

$$
\binom {n} {1} - 2 \binom {n} {2} + 3 \binom {n} {3} - 4 \binom {n} {4} + \dots +
$$

$$
+ (- 1) ^ {r - 1} n \binom {n} {n} = 0.
$$

11. 证明对于正整数 $n$

$$
\begin{array}{l} 1 + \frac {1}{2} \binom {n} {1} + \frac {1}{3} \binom {n} {2} + \frac {1}{4} \binom {n} {3} + \dots + \frac {1}{n + 1} \binom {n} {n} \\ = \frac {2 ^ {n + 1} - 1}{n + 1}. \\ \end{array}
$$

12. 求下式之和

$$
\begin{array}{l} 1 - \frac {1}{2} \binom {n} {1} + \frac {1}{3} \binom {n} {2} - \frac {1}{4} \binom {n} {3} + \dots + \\ + (- 1) ^ {n} \frac {1}{n + 1} \binom {n} {n}. \\ \end{array}
$$

13. 试用

$$
m ^ {2} = 2 \binom {m} {2} + \binom {m} {1}.
$$

和恒等式（4.3.9）求级数 $1^{2} + 2^{2} + 3^{2} + \dots + n^{2}$ 之和

14. 试求整数 $a, b$ 和 $c$ 使对所有的 $m$ 有

$$
m ^ {3} = a \binom {m} {3} + b \binom {m} {2} + c \binom {m} {1},
$$

再求级数 $1^{3} + 2^{3} + 3^{3} + \dots + n^{3}$ 之和.

15. 证明对所有实数 $r$ 和整数 $k$ ,

$$
\left( \begin{array}{c} - r \\ k \end{array} \right) = (- 1) ^ {k} \left( \begin{array}{c} r + k - 1 \\ k \end{array} \right).
$$

16. 证明对所有的实数 $r$ 和所有的整数 $k$ 和 $m$ ,

$$
\left( \begin{array}{c} r \\ m \end{array} \right) \left( \begin{array}{c} m \\ k \end{array} \right) = \left( \begin{array}{c} r \\ k \end{array} \right) \left( \begin{array}{c} r - k \\ m - k \end{array} \right).
$$

17. 利用组合论证，证明对所有正整数 $m_{1}, m_{2}$ 和 $n$ 有

$$
\sum_ {k = 0} ^ {n} \binom {m _ {1}} {k} \binom {m _ {2}} {n - k} = \binom {m _ {1} + m _ {2}} {n}.
$$

作为特殊情况导出恒等式（4.3.7）

18. 使用二项式定理和关系

$$
(1 + x) ^ {m _ {1}} (1 + x) ^ {m _ {2}} = (1 + x) ^ {m _ {1} + m _ {2}},
$$

证明练习17的恒等式，

19. 利用二项式定理进行试验（微分，积分等），试寻找含有二项式系数的有趣的恒等式。

20. 利用多项式定理，证明对于正整数 $n$ 和 $t$ 有

$$
t ^ {n} = \sum \left( \begin{array}{c} n \\ n _ {1} n _ {2} \dots n _ {i} \end{array} \right),
$$

这里求和是对使 $n_1 + n_2 + \dots + n_t = n$ 的非负整数 $n_1, n_2, \dots, n_t$ 的所有数列进行的.

21. 试用多项式定理展开 $(x_{1} + x_{2} + x_{3})^{4}$ .

22. 试确定在 $(x_{1} + x_{2} + x_{3} + x_{4} + x_{5})^{10}$ 展开式中的 $x_{1}^{3}x_{2}^{3}x_{3}^{3}x_{5}^{3}$ 的系数.

23. 在 $(x_{1} - x_{2} + 2x_{3} - 2x_{4})^{8}$ 展开式中的 $x_{1}^{2}x_{2}^{3}x_{3}x_{4}^{2}$ 的系数是什么？

24. 试用 $(x_{1} + x_{2} + x_{3})^{n} = [(x_{1} + x_{2}) + x_{3}]^{n}$ 与二项式定理展开 $(x_{1} + x_{2} + x_{3})^{n}$ .

25. 对正整数 $n$ 使用归纳法，证明对任何正整数 $n$ ，

$$
\frac {1}{(z + 1) ^ {n}} = \sum_ {k = 0} ^ {\infty} (- 1) ^ {n} \binom {n + k - 1} {k} z ^ {h}, | z | <   1.
$$

这里我们假定

$$
\frac {1}{z + 1} = \sum_ {k = 0} ^ {\infty} (- 1) ^ {k} z ^ {k}, | z | <   1.
$$

28. 利用Newton二项式定理求 $\sqrt{39}$ 的近似值。

# 第五章 容斥原理

在一些计数问题中，经常是间接计算一个集合中的元素个数比起直接计算来得简单。例如，要计算 $\{1, 2, \dots, n\}$ 的1不在第一个位置上的排列 $i_1 i_2 \dots i_n$ （即 $i_1 \neq 1$ ）的个数。注意到1不在第一个位置上的排列可以根据 $\{2, 3, \dots, n\}$ 中的哪个整数k是在第一个位置上面而分成n-1个组这个事实，我们就能够直接计算。k在第一个位置上的排列由n-1个元素集合 $\{1, 2, \dots, k-1, k+1, \dots, n\}$ 的一个排列置于k的后面所组成。因此，存在 $(n-1)!$ 个k在第一个位置上的 $\{1, 2, \dots, n\}$ 的排列。依据加法原理，1不在第一个位置上的 $\{1, 2, \dots, n\}$ 的排列有 $(n-1)!$ （n-1）个。另一方面，注意到1在第一个位置上 $\{1, 2, \dots, n\}$ 的排列个数与 $\{2, 3, \dots, n\}$ 的排列个数 $(n-1)!$ 一样多之后，我们便可间接地计算。因为 $\{1, 2, \dots, n\}$ 的排列总数是n！，因而l不在第一个位置上的 $\{1, 2, \dots, n\}$ 的排列个数是n!-(n-1)!=(n-1)!(n-1)。

再举个例子，要计算在1到600之间不能被6整除的整数个数。我们可以间接地作如下计算，在1到600之间能被6整除的整数个数是 $600 / 6 = 100$ ，这是由于从1数起每隔6个位置的整数都能被6整除。所以，在1到600之间不能被6整除的个数是 $600 - 100 = 500$ 。

这两个例子是利用如下原理进行间接计数的。如果 $A$ 是集合 $S$ 的子集合，则 $A$ 中的元素个数等于 $S$ 中的元素个数减去不在 $A$ 中的元素个数。对于 $S$ 中的子集 $A$ ，用 $\overline{A}$ 表示 $A$ 相对于 $S$ 的补集和用 $|A|$ 表示 $A$ 中的元素个数。上面规则便可写成 $|A|$

$= |S| - |\overline{A}|$ 或 $|\overline{A}| = |S| - |A|$ ，本章的目的是讨论这个原理的一个重要推广，称之为容斥原理，且并将它运用到若干问题上去。

# 5.1 容斥原理

我们将采取便于使用的形式来表达容斥原理。作为上面已用过的原理的第一步推广，令 $S$ 是含有有限个元素的集合， $P_{1}$ 和 $P_{2}$ 是 $S$ 中的每个元素可能具有或可能不具有的两个性质。我们要计算 $S$ 中既不具有性质 $P_{1}$ 也不具有性质 $P_{2}$ 的元素个数。首先计算 $S$ 的所有元素，其次从中去掉具有性质 $P_{1}$ 的所有元素和具有性质 $P_{2}$ 的所有元素，注意到已两次去掉那些同时具有 $P_{1}$ 和 $P_{2}$ 性质的元素，因此再重新计入所有这种元素一次。我们可以用符号把它表示如下，令 $A_{1}$ 是 $S$ 的具有性质 $P_{1}$ 的元素的子集合， $A_{2}$ 是 $S$ 的具有性质 $P_{2}$ 的元素的子集合。那么， $\overline{A}_{1}$ 是由 $S$ 的不具有性质 $P_{1}$ 的那些元素组成， $\overline{A}_{2}$ 是由 $S$ 的不具有性质 $P_{2}$ 的那些元素组成。集合 $\overline{A}_{1} \cap \overline{A}_{2}$ 的元素是既不具有性质 $P_{1}$ 也不具有性质 $P_{2}$ 的那些元素。因而，

$$
\left| \overline {{A}} _ {1} \cap \overline {{A}} _ {2} \right| = | S | - | A _ {1} | - | A _ {2} | + | A _ {1} \cap A _ {2} |.
$$

由于上面等式的左端计算 $S$ 中既不具有性质 $P_{1}$ , 也不具有性质 $P_{2}$ 的那些元素的个数, 我们证明既不具有性质 $P_{1}$ 又不具有性质 $P_{2}$ 的一个元素在等式右端被计算的次数的净值为 1 , 而其它每个元素在右端被计算的次数的净值为 0 , 这样就可以说明这个等式是成立的。如果 $x$ 是既不具有性质 $P_{1}$ 又不具有性质 $P_{2}$ 的元素, 就把它计算在 $S$ 的元素之内, 而不计算在 $A_{1}$ 或 $A_{2}$ 的元素之内, 也不计算在 $A_{1} \cap A_{2}$ 的元素之内。因而它在等式右端被计算的次数的净值为 $1 - 0 - 0 + 0 = 1$ 。如果 $x$ 只具有性质 $P_{1}$ ,

它在等式右端被计算的次数的净值为 $1 - 1 - 0 + 0 = 0$ 。若 $x$ 只具有性质 $P_{2}$ ，它在等式右端被计算的次数的净值为 $1 - 0 - 1 + 0 = 0$ ，最后，如果 $x$ 同时具有性质 $P_{1}$ 和 $P_{2}$ ，它在等式右端被计算的次数的净值为 $1 - 1 - 1 + 1 = 0$ 。因此，等式右端也是计算 $S$ 的既不具有性质 $P_{1}$ 也不具有性质 $P_{2}$ 的元素个数。

更一般地，令 $P_{1}, P_{2}, \cdots, P_{m}$ 是 $S$ 中的每个元素可能具有或可能不具有的 $m$ 个性质。对于 $i = 1, 2, \cdots, m$ ，令 $A_{i}$ 是 $S$ 的具有性质 $P_{i}$ （可能还有其它性质）的元素子集合，则 $A_{i} \cap A_{j}$ 是同时具有性质 $P_{i}$ 和 $P_{j}$ （可能还有其它性质）的那些元素的子集合， $A_{i} \cap A_{j} \cap A_{k}$ 是同时具有性质 $P_{i}$ ， $P_{j}$ 和 $P_{k}$ 的那些元素的子集合，等等。不具有所有这些性质的任何一个的元素的子集合是 $\overline{A}_{1} \cap \overline{A}_{2} \cap \cdots \cap \overline{A}_{m}$ 。容斥原理指出了怎样通过那些随着所具有的性质而定的元素的计数去计算这个集合中元素的个数。

定理5.1.1 不具有性质 $P_{1}, P_{2}, \ldots, P_{m}$ 的任何一个 $S$ 的元素个数由下式给出

$$
\begin{array}{l} \left| \bar {A} _ {1} \cap \bar {A} _ {2} \cap \dots \cap \bar {A} _ {m} \right| = | S | - \sum | A _ {i} | + \sum | A _ {i} \cap A _ {j} | \\ - \sum \left| A _ {i} \cap A _ {j} \cap A _ {k} \right| + \dots + (- 1) ^ {m} \left| A _ {1} \cap A _ {2} \cap \dots \cap A _ {m} \right|, \\ \end{array}
$$

这里第一项求和是对 $\{1,2,\dots ,m\}$ 中的所有整数i进行的，第二项求和是对 $\{1,2,\dots ,m\}$ 中的所有2组合 $\{i,j\}$ 进行的，第三项求和是对 $\{1,2,\dots ,m\}$ 中的所有3组合 $\{i,j,k\}$ 进行的，等等。

对于 $m = 3$ ，上面等式成为

$$
\begin{array}{l} \left| \overline {{A}} _ {1} \cap \overline {{A}} _ {2} \cap \overline {{A}} _ {3} \right| = | S | - (| A _ {1} | + | A _ {2} | + | A _ {3} |) \\ + \left(\left| A _ {1} \cap A _ {2} \right| + \left| A _ {1} \cap A _ {3} \right| + \left| A _ {2} \cap A _ {3} \right|\right) \\ - \left| A _ {1} \cap A _ {2} \cap A _ {3} \right|. \\ \end{array}
$$

注意在右端有 $1 + 3 + 3 + 1 = 8$ 项。对于 $m = 4$ ，这个等式变为

$$
\begin{array}{l} \left| \overline {{A _ {1}}} \cap \overline {{A _ {2}}} \cap \overline {{A _ {3}}} \cap \overline {{A _ {4}}} \right| = | S | - (| A _ {1} | + | A _ {2} | + | A _ {3} | \\ + \left| A _ {4} \right|) + \left(\left| A _ {1} \cap A _ {2} \right| + \left| A _ {1} \cap A _ {3} \right|\right) \\ + \left| A _ {1} \cap A _ {4} \right| + \left| A _ {2} \cap A _ {3} \right| \\ + \left| A _ {2} \cap A _ {4} \right| + \left| A _ {3} \cap A _ {4} \right|) \\ - \left(| A _ {1} \cap A _ {2} \cap A _ {3} | + | A _ {1} \cap A _ {2} \cap A _ {4} |\right. \\ + \left| A _ {1} \cap A _ {3} \cap A _ {4} \right| + \left| A _ {2} \cap A _ {3} \cap A _ {4} \right|) \\ + \left| A _ {1} \cap A _ {2} \cap A _ {3} \cap A _ {4} \right|. \\ \end{array}
$$

在这种情况下，右端有 $1 + 4 + 6 + 4 + 1 = 16$ 项。一般地说来，右端有

$$
1 + \binom {m} {1} + \binom {m} {2} + \binom {m} {3} + \dots + \binom {m} {m}
$$

项，並且我们已经知道这个数等于 $2^{m}$ （见定理3.3.2）.

定理5.1.1的证明等式（5.1.1）的左端计算 $S$ 中不具有 $m$ 个性质中任何一个的元素个数，我们证明不具有 $m$ 个性质 $P_{1}$ ， $P_{2}, \ldots, P_{m}$ 的任何一种性质的一个元素在等式右端被计算的次数的净值为1，而至少具有这些性质之一的元素在等式右端被计算的次数的净值为0。这样，就可以确认等式的正确性。首先考虑不具这些性质中任何一种性质的一个元素 $x$ ，它在（5.1.1）的右端被计算的次数的净值为

$$
1 - 0 + 0 - 0 + \dots + (- 1) ^ {m} \times 0 = 1,
$$

因为它在 $S$ 中，但不在其它任何集合中。现在考虑一个恰好具有 $n \geqslant 1$ 个性质的元素 $y$ 。它在 $|S|$ 中被计算的次数为

$$
1 = \left( \begin{array}{c} n \\ 0 \end{array} \right).
$$

它在 $\sum |A_{i}|$ 中被计算的次数是

$$
n = \left( \begin{array}{c} n \\ 1 \end{array} \right),
$$

因为它恰好具有 $n$ 个性质，所以它恰好是 $A_{1}, A_{2}, \cdots, A_{m}$ 中 $n$ 个集合的成员。它在 $\sum |A_{i} \cap A_{j}|$ 中被计算的次数是 $\binom{n}{2}$ ，因为我们有 $\binom{n}{2}$ 种方式选择它的一对性质，所以它恰好是 $\binom{n}{2}$ 个集合 $A_{i} \cap A_{j}$ 的成员。它在 $\sum |A_{i} \cap A_{j} \cap A_{k}|$ 被计算的次数是 $\binom{n}{3}$ ，等等。于是， $y$ 在右端被计算的次数的净值是

$$
\begin{array}{l} \binom {n} {0} - \binom {n} {1} + \binom {n} {2} - \binom {n} {3} + \dots + (- 1) ^ {m} \binom {n} {m} \\ = \binom {n} {0} - \binom {n} {1} + \binom {n} {2} - \binom {n} {3} + \dots + (- 1) ^ {n} \binom {n} {n}, \\ \end{array}
$$

因为 $n \leqslant m$ 并且当 $k > n$ 时有 $\binom{n}{k}=0$ . 根据恒等式（4.3.3）这个最后表达式等于0，所以当 $y$ 至少具有这些性质之一时，它在等式（5.1.1）的右端被计算的次数的净值为0。这就证明了这个定理。

推论 $S$ 的至少具有性质 $P_{1}, P_{2}, \dots, P_{m}$ 中的一种性质的那些元素的个数由下式给出

$$
\begin{array}{l} \left| A _ {1} \cup A _ {2} \cup \dots \cup A _ {m} \right| = \Sigma \left| A _ {i} \right| - \Sigma \left| A _ {i} \cap A _ {j} \right| \\ + \sum | A _ {i} \cap A _ {j} \cap A _ {k} | - \dots \\ + (- 1) ^ {m + 1} \left| A _ {1} \cap A _ {2} \cap \dots \cap A _ {m} \right|, \tag {5.1.2} \\ \end{array}
$$

这里的求和如同定理（5.1.1）中所指明的那样。

证明 集合 $A_{1} \cup A_{2} \cup \cdots \cup A_{n}$ 是由至少具有这些性质中一种性质的那些元素组成，而且

$$
\left| A _ {1} \bigcup A _ {2} \bigcup \dots \bigcup A _ {m} \right| = | S | - \left| \overline {{A _ {1} \bigcup A _ {2} \bigcup \dots \bigcup A _ {m}}} \right|.
$$

但是

$$
\overline {{A _ {1} \bigcup A _ {2} \bigcup \cdots \bigcup A _ {n}}} = \overline {{A _ {1}}} \cap \overline {{A _ {2}}} \cap \dots \cap \overline {{A _ {m}}},
$$

这是集合论的一个著名规则，因而

$$
\left| A _ {1} \cup A _ {2} \cup \dots \cup A _ {m} \right| = | S | - \left| \overline {{A}} _ {1} \cap \overline {{A}} _ {2} \cap \dots \cap \overline {{A}} _ {m} \right|.
$$

把它与等式（5.1.1）结合起来，就得到了等式（5.1.2）。

应用 试求在 1 到 1000 之间那些不能被 5, 6 和 8 中任何一个整除的整数个数。

为解决这个问题我们引入一些记号。对于实数 $r$ ，[r] 代表不超过 $r$ 的最大整数。还把两个整数 $a$ ， $b$ 或三个整数 $a$ ， $b$ 和 $c$ 的最小公倍数分别简记为 $\operatorname{lcm} \{a, b\}$ 或 $\operatorname{lcm} \{a, b, c\}$ 。令 $P_1$ 表示一个整数能被 5 整除的性质， $P_2$ 表示一个整数能被 6 整除的性质和 $P_3$ 表示一个整数能被 8 整除的性质。令 $S$ 是由前 1000 个正整数组成的集合。对于 $i = 1, 2, 3$ ，令 $A_i$ 是由 $S$ 中具有性质 $P_i$ 的那些整数组成的集合。要求的是 $A_1 \cap A_2 \cap A_3$ 中的整数的个数。

我们看到，

$$
\left| A _ {1} \right| = \left[ \frac {1 0 0 0}{5} \right] = 2 0 0, \left| A _ {2} \right| = \left[ \frac {1 0 0 0}{6} \right] = 1 6 6,
$$

$$
\left| A _ {3} \right| = \left[ \frac {1 0 0 0}{8} \right] = 1 2 5.
$$

在集合 $A_{1} \cap A_{2}$ 中的能被 5 也能被 6 整除。但是，一个整数当且仅当能被 $\operatorname{lcm}\{5, 6\}$ 整除时，才能既被 5 也被 6 整除。由于 $\operatorname{lcm}\{5, 6\} = 30$ ， $\operatorname{lcm}\{5, 8\} = 40$ 和 $\operatorname{lcm}\{6, 8\} = 24$ ，便得到

$$
\left| A _ {1} \cap A _ {3} \right| = \left[ \frac {1 0 0 0}{3 0} \right] = 3 3,
$$

$$
\left| A _ {1} \cap A _ {2} \right| = \left[ \frac {1 0 0 0}{4 0} \right] = 2 5,
$$

$$
\left| A _ {2} \cap A _ {3} \right| = \left[ \frac {1 0 0 0}{2 4} \right] = 4 1.
$$

由于 $\operatorname{lcm}\{5,6,8\} = 120$ ，可用类似方法断定

$$
\left| A _ {1} \cap A _ {2} \cap A _ {3} \right| = \left[ - \frac {1 0 0 0}{1 2 0} \right] = 8.
$$

于是根据容斥原理可知，在1和1000之间不能被5，6和8中任何一个整除的整数个数等于

$$
\begin{array}{l} \left| \overrightarrow {A _ {1}} \cap \overrightarrow {A _ {2}} \cap \overrightarrow {A _ {3}} \right| = 1 0 0 0 - (2 0 0 + 1 6 6 + 1 2 5) + (3 3 + 2 5 \\ + 4 1) - 8 = 6 0 0. \\ \end{array}
$$

在下面几节中，我们讨论容斥原理对某些一般问题的应用。

# 5.2 重复组合

在3.3和3.5节中，已经确定了 $n$ 个不同元素集合的 $r$ 组合个数是

$$
\binom {n} {r} = \frac {n !}{r ! (n - r) !}
$$

和 $k$ 个不同元素而每个元素有无限重复数的重集的 $r$ 组合个数等于 $\binom{k+r-1}{r}$ 。本节将指出，如果一个重集的元素具有任意给定的重复数，如何使用公式 $\binom{k+r-1}{r}$ 和容斥原理来求出这个重集的 $r$ 组合个数。假设 $S$ 是重集且 $S$ 的一个元素 $x$ 有无限重复数，那么 $S$ 的 $r$ 组合个数等于 $S$ 中 $x$ 的重复数改为 $r$ 而得到的重集 $T$ 的 $r$ 组合个数。这样做的原因，是由于 $S$ 的 $r$ 组合重复数显然不会超过 $r$ ，因而大于 $r$ 的任何重复数可以用 $r$ 代替。例如，重集 $\{3 \cdot a, \infty \cdot b, 6 \cdot c, 10 \cdot d, \infty \cdot e\}$ 的8组合个数与重集 $\{3 \cdot a, 8 \cdot b, 6 \cdot c, 8 \cdot d, 8 \cdot e\}$ 的8组合个数是相同的。总结一下，就

可以说我们已经确定了重集 $S = \{n_{1} \cdot a_{1}, n_{2} \cdot a_{2}, \dots, n_{k} \cdot a_{k}\}$ 在两种极端情况下的 $r$ 组合个数：(i) $n_{1} - n_{2} = \dots = n_{k} = 1$ 即 $S$ 是集合；(ii) $n_{1} = n_{2} = \dots = n_{k} = r$ ，我们将举例说明如何应用容斥原理来得到处在这两者之间的情况下的解。尽管是就一个特殊的例子来讨论的，但在一般情况下这种方法显然也是有效的。

例 试确定重集 $B = \{3 \cdot a, 4 \cdot b, 5 \cdot c\}$ 的10组合个数。

我们要把容斥原理应用到重集 $C = \{\infty, a, \infty, b, \infty, c\}$ 的所有10组合的集合 $S$ 上。令 $P_{1}$ 表示 $C$ 的10组合多于3个 $a$ 这一性质，令 $P_{2}$ 表示 $C$ 的10组合多于4个 $b$ 这一性质，最后令 $P_{3}$ 表示 $C$ 的10组合多于5个 $c$ 这一性质。那么， $B$ 的10组合个数等于不具有 $P_{1}$ ， $P_{2}$ ，和 $P_{3}$ 中任何一个性质的 $C$ 的10组合个数。对于 $i = 1, 2, 3$ ，令 $A_{i}$ 是由 $C$ 的具有性质 $P_{i}$ 的那些10组合组成。例如， $A_{2}$ 是由 $C$ 的且 $b$ 出现4次以上的那些10组合组成。 $B$ 的10组合个数等于 $\overline{A}_{1} \cap \overline{A}_{2} \cap \overline{A}_{3}$ 中的元素个数。利用容斥原理得

$$
\begin{array}{l} \left| \overline {{A}} _ {1} \cap \overline {{A}} _ {2} \cap \overline {{A}} _ {3} \right| = | S | - \left(\left| A _ {1} \right| + \left| A _ {2} \right| + \left| A _ {3} \right|\right) \\ + \left(\left| A _ {1} \cap A _ {2} \right| + \left| A _ {1} \cap A _ {3} \right| + \left| A _ {2} \cap A _ {3} \right|\right) \\ - \left| A _ {1} \cap A _ {2} \cap A _ {3} \right|. \\ \end{array}
$$

根据定理3.5.1,

$$
| S | = \left( \begin{array}{c} 3 + 1 0 - 1 \\ 1 0 \end{array} \right) = \left( \begin{array}{c} 1 2 \\ 1 0 \end{array} \right) = 6 6.
$$

$A_{1}$ 是由 $C$ 的且 $\alpha$ 至少出现 4 次的所有 10 组合组成。如果从 $A_{1}$ 这些 10 组合中任取一个，並去掉 4 个 $\alpha$ ，就剩下 $C$ 的一个 6 组合。反之，如果取 $C$ 的一个 6 组合並且把 4 个 $\alpha$ 加进去，便得到了 $C$ 的且 $\alpha$ 至少出现 4 次的 10 组合。于是， $A_{1}$ 中 10 组合个数等于 $C$ 的 6 组合个数。因而，

$$
\left| A _ {1} \right| = \binom {3 + 6 - 1} {6} = \binom {8} {6} = 2 8.
$$

用类似方法可知， $A_{2}$ 中的10组合个数等于C的5组合个数， $A_{3}$ 中的10组合个数等于C的4组合个数。于是

$$
\left| A _ {2} \right| = \binom {3 + 5 - 1} {5} = \binom {7} {5} = 2 1,
$$

$$
\left\{A _ {3} \right\} = \binom {3 + 4 - 1} {4} = \binom {6} {4} - 1 5.
$$

$A_{1} \cap A_{2}$ 是由 $C$ 的且 $a$ 至少出现 4 次同时 $b$ 至少出现 5 次的所有 10 组合组成。如果从这些 10 组合中的任一个去掉 4 个 $a$ 和 5 个 $b$ ，就剩有 $C$ 的一个 1 组合。反之，若对于 $C$ 的一个 1 组合加进 4 个 $a$ 和 5 个 $b$ ，就得到了一个 $a$ 至少出现 4 次并且 $b$ 至少出现 5 次的 10 组合。于是， $A_{1} \cap A_{2}$ 中的 10 组合个数等于 $C$ 的 1 组合个数，因而

$$
\left| A _ {1} \cap A _ {2} \right| = \binom {3 + 1 - 1} {1} = \binom {3} {1} = 3.
$$

用类似方法可以导出 $A_{1} \cap A_{3}$ 中的10组合个数等于 $C$ 中的 $\bullet$ 组合个数，而且 $A_{2} \cap A_{3}$ 中不存在10组合，这样

$$
\mid A _ {1} \cap A _ {3} \mid = \binom {3 + 0 - 1} {0} = \binom {2} {0} = 1 \text {和} \mid A _ {2} \cap A _ {3} \mid = 0.
$$

同样也有

$$
\left| A _ {1} \cap A _ {2} \cap A _ {3} \right| = 0.
$$

于是

$$
\begin{array}{l} \left| \overline {{A}} _ {1} \cap \overline {{A}} _ {2} \cap \overline {{A}} _ {3} \right| = 6 6 - (2 8 + 2 1 + 1 5) + (3 + 1 \div 0) - 0 \\ = 6. \\ \end{array}
$$

在定理3.5.1的证明中，已经指出了 $r$ 组合和方程整数解之间的联系。重集 $\{n_1 \cdot a_1, n_2 \cdot a_2, \dots, n_k \cdot a_k\}$ 的 $r$ 组合个数等于方程

$$
\boldsymbol {x} _ {1} + \boldsymbol {x} _ {2} + \dots + \boldsymbol {x} _ {5} = \boldsymbol {r}
$$

在 $x_{1}$ 是整数并且

$$
0 \leqslant x _ {i} \leqslant n. \quad (i = 1, 2, \dots , k)
$$

的约束下的解的个数。这些解的个数可以用刚才举例说明的方法进行计算。

# 5.3 错 位

在一次聚会中，有10个人寄存他们的帽子，问有多少方法交还他们的帽子，使得没有一个人得到他来时戴的那一顶？一台V-8发动机的8个火花塞由于清理把它们从所在的汽缸体取出，问有多少种方法把它们放回汽缸体但没有一个火花塞回到它原来的汽缸体？有多少种方法写下M，A，D，I，S，O，N这些字母使得拼成的“词”与词MADISON的拼法完全不一样？这个意思是指没有任何一个字母在所拼成的词中与在词MADISON中占有同样位置。这些问题中的每一个都是下面一般问题的一个例子。给定 $n$ 个元素集合 $X$ ，其中每个元素有一指定位置，我们要求找出集合 $X$ 中每个元素都不在原指定位置的排列个数。在第一个问题中， $X$ 是10顶帽子的集合，帽子的指定位置是这顶帽子的主人（的头部）。在第二个问题中， $X$ 是火花塞的集合，火花塞的指定位置是它原来所在的汽缸体。在第三个问题中， $X = \{M, A, D, I, S, O, N\}$ ，一个字母的位置是由词MADISON所规定。

由于这些元素的实际性质是无关紧要的，我们可以取集合 $\{1,2,\dots ,n\}$ 作为 $X$ ，其中每个整数的位置由它在序列1， $2,\dots ,n$ 中的位置来规定。因而， $\{1,2,\dots ,n\}$ 的一个错位是使 $i_1\mp 1,i_2\mp 2,\dots ,i_n\mp n$ 的 $\{1,2,\dots ,n\}$ 的一个排列 $i_1i_2\dots i_n$ 。于是， $\{1,2,\dots ,n\}$ 的一个错位就是 $\{1,2,\dots ,n\}$ 的每个元素都不在其自然位置的一个排列。我们用 $D_{n}$ 表示 $\{1,2,\dots ,n\}$ 的错位的个数。上面的问题分别要求计

算 $D_{10}$ ， $D_{8}$ 和 $D_{7}$ ，对于 $\pmb {n} = 1$ ，不存在错位，对于 $\pmb {n} = 2$ ，21是唯一的错位，对于 $\pmb {n} = 3$ ，存在两个错位，即231和312. $\pmb {n} = 4$ 的错位列表如下：

<table><tr><td>2 1 4 3</td><td>3 1 4 2</td><td>4 1 2 3</td></tr><tr><td>2 3 4 1</td><td>3 4 1 2</td><td>4 3 1 2</td></tr><tr><td>2 4 1 3</td><td>3 4 2 1</td><td>4 3 2 1.</td></tr></table>

这样，有 $D_{1} = 0$ ， $D_{2} = 1$ ， $D_{3} = 2$ ， $D_{4} = 9$

定理5.3.1 对于 $n \geqslant 1$

$$
D _ {n} = n! \left(1 - \frac {1}{1 !} + \frac {1}{2 !} - \frac {1}{3 !} + \dots + (- 1) ^ {n} \frac {1}{n !}\right).
$$

证明 令 $S$ 是 $\{1,2,\dots ,n\}$ 的所有 $n!$ 个排列的集合。对于 $j = 1,2,\dots ,n$ ，令 $P_{j}$ 表示排列中 $j$ 是位于它的自然位置上这一性质。于是，只要 $i_{1} = j$ ， $\{1,2,\dots ,n\}$ 的排列 $i_1i_2\dots i^n$ 就具有性质 $P_{j}$ 。 $\{1,2,\dots n\}$ 的排列当且仅当它不具有性质 $P_{1},P_{2},\dots ,P_{n}$ 中的任一性质时才是一个错位。如果对于 $j = 1,2,\dots ,n,A_{j}$ 表示具有性质 $P_{j}$ 的 $\{1,2,\dots ,n\}$ 的排列集合，那么 $\{1,2,\dots ,n\}$ 的错位恰好就是在 $\overline{A}_1\bigcap \overline{A}_2\bigcap \dots$ $\bigcap \overline{A}_n$ 中的那些排列。于是 $D_{n} = \{\overline{A}_{1}\cap \overline{A}_{2}\cap \dots \cap \overline{A}_{n}|$ ，然后用容斥原理计算 $D_{i}$ 。 $A_{i}$ 中的排列就是那些形如 $1i_{2}i_{3}\dots i_{j}$ 的排列，这里 $i_2i_3\dots i_n$ 是 $\{2,3,\dots ,n\}$ 的一个排列。这样， $|A_{1}| = (n - 1)!$ 。用同样的方法，可知

$$
\left| A j \right| = (n - 1)! \quad (j = 1, 2, \dots , n).
$$

在 $A_{1} \cap A_{2}$ 中的排列是那些形如 $12i_{3}i_{4}\cdots i$ 的排列，这里 $i_{3}i_{4}\cdots i$ 是 $\{3, 4, \cdots, n\}$ 的一个排列。于是， $\{A_{1} \cap A_{2}\} = (n - 2)!$ ，并且用类似的方法可以看出，对于 $\{1, 2, \cdots, n\}$ 的2组合 $\{i, j\}$ 有

$$
\left| A _ {1} \cap A _ {1} \right| = (n - 2) 1.
$$

对于任意整数 $k$ 且 $1 \leqslant k \leqslant n$ , $A_{1} \cap A_{2} \cap \cdots \cap A_{k}$ 中的排列就是那些形如 $12 \cdots k i_{k+1} \cdots i_{n}$ 的排列, 其中 $i_{k+1} \cdots i_{n}$ 是 $\{k+1, \cdots, n\}$ 的一个排列. 因而, $|A_{1} \cap A_{2} \cap \cdots \cap A_{k}| = (n-k)!$ , 并且更一般地来说, 对于 $\{1, 2, \cdots, n\}$ 的一个 $k$ 组合 $\{i_{1}, i_{2}, \cdots, i_{k}\}$ 有

$$
\left| A i _ {1} \cap A i _ {2} \cap \dots \cap A i _ {k} \right| = (n - k)!.
$$

因为 $\{1,2,\dots ,n\}$ 的 $k$ 组合有 $\binom{n}{k}$ 个，应用容斥原理我们断定

$$
\begin{array}{l} D _ {n} = n! - \left( \begin{array}{l} n \\ 1 \end{array} \right) (n - 1)! + \left( \begin{array}{l} n \\ 2 \end{array} \right) (n - 2)! - \left( \begin{array}{l} n \\ 3 \end{array} \right) (n - 3)! \\ + \dots + (- 1) ^ {n} \left( \begin{array}{c} n \\ n \end{array} \right) 0! \\ = n! - \frac {n !}{1 !} + \frac {n !}{2 !} - \frac {n !}{3 !} + \dots + (- 1) ^ {n} \frac {n !}{n !} \\ = n! \left(1 - \frac {1}{1 1} + \frac {1}{2 1} - \frac {1}{3 1} + \dots + (- 1) ^ {n} \frac {1}{n !}\right). \\ \end{array}
$$

于是，定理证毕.

我们可以应用所得到的这个公式来计算

$$
D _ {5} = 5! \left(1 - \frac {1}{1 !} + \frac {1}{2 !} - \frac {1}{3 !} + \frac {1}{4 !} - \frac {1}{5 !}\right) = 4 4.
$$

用同样的方法可以计算 $D_{6} = 265$ ， $D_{7} = 1854$ 和 $D_{3} = 14833$

回想一下 $e^{-1}$ 的交错级数

$$
e ^ {- 1} = 1 - \frac {1}{1 !} + \frac {1}{2 !} - \frac {1}{3 !} + \frac {1}{4 !} - \dots ,
$$

可以写成

$$
\begin{array}{l} e ^ {- 1} = \frac {D _ {n}}{n !} + (- 1) ^ {n + 1} \frac {1}{(n + 1) !} + (- 1) ^ {n + 2} \frac {1}{(n + 2) !} \\ + \dots , \\ \end{array}
$$

由无穷级数的基本事实，我们断言 $e^{-1}$ 与 $D_{n} / n!$ 相差比

$1 / (n + 1)!$ 还小。稍加计算便可知，对于 $n \geqslant 7$ 来说， $e^{-1}$ 与 $D_{n} / n!$ 至少有3个小数位相同。于是，从实用观点看，对于 $n \geqslant 7$ ， $e^{-1}$ 与 $D_{n} / n!$ 可认为是一样的。数 $D_{n} / n!$ 是 $\{1,2,\dots,n\}$ 的错位个数与 $\{1,2,\dots,n\}$ 的排列个数之比。于是 $D_{n} / n!$ 表示我们随机地选择 $\{1,2,\dots,n\}$ 的一个排列是一个错的概率。就本节一开始提到的帽子问题来说，如果这些帽子随位机地交还给这些人，那么没有任何一个人戴上自己帽子的概率是 $D_{10} / 10!$ ，并且这实际上就是 $e^{-1}$ 。从前面的一些说明可知，如果有1,000,000个人，其概率基本上是相同的。

错位数 $D_{n}$ 有若干有趣的性质。例如

$$
D _ {n} = (n - 1) \left(D _ {n - 2} + D _ {n - 1}\right) \quad (n = 3, 4, 5, \dots). \tag {5.3.1}
$$

这个公式是线性递归关系之一例1)。从数据 $D_{1} = 0$ ， $D_{2} = 1$ 出发，我们可以应用它计算相应于任何整数 $n$ 的 $D_{n}$ 。例如，

$$
\begin{array}{l} D _ {3} = 2 \left(D _ {1} + D _ {2}\right) = 2 (0 + 1) = 2, \\ D _ {4} = 3 \left(D _ {2} + D _ {3}\right) = 3 (1 + 2) = 9, \\ D _ {6} = 4 \left(D _ {3} + D _ {4}\right) = 4 (2 + 9) = 4 4, \\ D _ {8} = 5 \left(D _ {4} + D _ {5}\right) = 5 (9 + 4 4) = 2 6 5. \\ \end{array}
$$

下一章要指出怎样去解常系数的线性递归关系，不过那些方法在这里并不适用，因为公式(5.3.1)有变系数 $n - 1$

我们可以用组合方法证明这个公式如下。考虑 $\{1, 2, \dots, n\}$ 的所有 $D_{n}$ 个错位，这里 $n \geqslant 3$ 。根据整数2，3，…， $n$ 中哪一个是在错位的第一个位置，可将这些错位划分成 $n - 1$ 类。显然，每类包含有同样多的错位。因此， $D_{n}$ 等于 $(n - 1)d_{n}$ ，其中 $d_{n}$ 表示2在第一个位置的错位个数。这样的错位

是形如

$$
2 i _ {2} i _ {3} \dots i _ {n}
$$

的一些排列，这里 $i_2 \div 2, i_3 \div 3, \dots, i_n \div n$ 。这 $dn$ 个错位可以根据 $i_2 = 1$ 或 $i_2 \div 1$ 进一步划分成两个子类。令 $d_n'$ 是形如

$$
2 1 i _ {2} i _ {4} \dots i n
$$

的错位个数，这里 $i_3 \div 3, \dots, i_n \div n$ 。令 $d_n^{\prime \prime}$ 是形如

$$
2 i _ {2} i _ {3} \dots i _ {n}
$$

的错位个数，其中 $i_2 \neq 1, i_3 \neq 3, \dots, i_n \neq n$ 。这样 $d_n = d_1^* + d_n^*$ ，因而

$$
D _ {n} = (n - 1) d _ {n} = (n - 1) \left(d _ {n} ^ {\prime} + d _ {n} ^ {\prime \prime}\right).
$$

我们首先注意到 $d_{n}^{\prime}$ 等于 $\{3,4,\dots ,n\}$ 的排列 $i_3i_4\dots i_n$ 的个数，其中 $i_3\neq 3,\dots ,i_n\neq n$ ，换句话说， $d_{n}^{\prime}$ 是 $\{3,4,\dots ,n\}$ 的排列个数，其中3不在第一个位置上，4不在第二个位置上，等等。于是 $d_{n}^{\prime} = D_{n - 2}$ 。其次注意到 $d_{n}^{\prime \prime}$ 等于 $\{1,3,\dots ,n\}$ 的排列 $i_2i_3\dots i_n$ 的个数，其中 $i_2\neq 1,i_3\neq 3,$ $\dots ,i_{n}\neq n$ 。换言之， $d_{n}^{\prime \prime}$ 是 $\{1,3,\dots ,n\}$ 的排列个数，这里1不在第一个位置上，3不在第二个位置上，…， $n$ 不在第 $n - 1$ 个位置上。这样， $d_{n}^{\prime \prime} = D_{n - 1}$ ，由此断定

$$
D _ {n} = (n - 1) \left(d _ {n} ^ {\prime} + d _ {n} ^ {\prime \prime}\right) = (n - 1) \left(D _ {n - 2} + D _ {n - 1}\right),
$$

这与(5.3.1)是一致的。这个证明应归于Euler。

我们可以把(5.3.1)重新写成

$$
D _ {n} - n D _ {n - 1} = - [ D _ {n - 1} - (n - 1) D _ {n - 2} ].
$$

右端括号中的表达式与左端用 $n - 1$ 代替 $n$ 后的表达式完全一样。于是，可以迭代计算得到

$$
D _ {n - 1} - (n - 1) D _ {n - 2} = - \left(D _ {n - 2} - (n - 2) D _ {n - 3}\right).
$$

重复迭代便得到下面式子：

$$
D _ {n} - n D _ {n - 1} = - [ D _ {n - 1} - (n - 1) D _ {n - 2} ] =
$$

$$
\begin{array}{l} = (- 1) ^ {2} [ D _ {n - 2} - (n - 2) D _ {n - 3} ] \\ = (- 1) ^ {3} [ D _ {n - 3} - (n - 3) D _ {n - 4} ] = \dots \\ = (- 1) ^ {n - 2} \left(D _ {2} - 2 D _ {1}\right). \\ \end{array}
$$

由于 $D_{2} = 1$ ， $D_{1} = 0$ ，我们得到公式

$$
D _ {n} = n D _ {n - 1} + (- 1) ^ {n - 2}
$$

或

$$
D _ {n} = n D _ {n - 1} + (- 1) ^ {n} \quad (n = 2, 3, 4, \dots). \tag {5.3.2}
$$

[严格地说，我们的论证仅适用于 $n = 3, 4, \cdots$ ，但容易验证(5.3.2)在 $n = 2$ 时也成立。]

重复迭代公式(5.3.2)，或使用它与数学归纳法，可以得到定理5.3.1的一个不同证明（见练习18）。由于(5.3.2)是从(5.3.1)得到的，而(5.3.1)已给出一个独立的组合证明，这就为定理5.3.1提供了一种毋须使用容斥原理的证明。

公式(5.3.1)和(5.3.2)相似于下面两个关于阶乘的公式：

$$
n! = (n - 1) [ (n - 2)! + (n - 1)! ] \quad (n = 2, 3, 4, \dots), \tag {5.3.3}
$$

$$
n! = n (n - 1)! \quad (n = 1, 2, 3, \dots). \tag {5.3.4}
$$

# 5.4 其它禁位问题

上一节计算了 $\{1, 2, \dots, n\}$ 的且有一些绝对禁用位置的排列个数。更准确地说，我们计算了 $\{1, 2, \dots, n\}$ 的且第 $j$ 个位置上禁止出现 $j (j = 1, 2, \dots, n)$ 的排列个数。本节研究有某些相对禁用位置的排列的计数问题，并且指出怎样应用容斥原理来计算这种排列的个数。

这个问题如下。假设一个班级的8个男孩每天散步一次，按8人一列行走，因而除第一个男孩外每个男孩前面都有另一个男孩。由于孩子总是看到同一个人在他的前面感到很乏味，在第二天他们决定变换一下位置，使得没有一个男孩的前面是第一天在他前面的男孩。试问有多少种方法变换他们的位置？

一种可能性是将这些男孩的次序反转过来，使第一个现在变换到最后一个，等等。但还有许多其它可能性。如果我们把这些男孩用1，2，…，8进行编号，第一天在行列中最后的男孩得到编号为1，…，第一个男孩得到编号为8，那么我们要求的就是去确定{1，2，…，8}的且不出现式样12，23，…，78的排列个数。例如，31542876是容许的排列，但25834761①则不是，因为式样34在其中出现了。一般地，给定一个正整数n，我们计算式样12，23，…，(n-1)n中任何一个都不出现的{1，2，…，n}的排列个数Qn。

对于 $n = 1$ ，1是容许的排列，对于 $n = 2$ ，21是容许的排列，对于 $n = 3$ ，容许的排列是213，321和132，而对于 $n = 4$ ，它们是

$$
\begin{array}{l} \begin{array}{c c c} 4 1 3 2 & 4 3 2 1 & 4 2 1 3 \end{array} \\ \begin{array}{c c c} 3 2 1 4 & 3 2 4 1 & 3 1 4 2 \end{array} \\ \begin{array}{l l l} 2 4 3 1 & 2 4 1 3 & 2 1 4 3 \end{array} \\ 1 3 2 4 \quad 1 4 3 2. \\ \end{array}
$$

于是， $Q_{1} = 1, Q_{2} = 1, Q_{3} = 3, Q_{4} = 11.$

定理5.4.1 对于 $n \geqslant 1$

$$
Q _ {n} = n! - \binom {n - 1} {1} (n - 1)! + \binom {n - 1} {2} (n - 2)! -
$$

$$
\binom {n - 1} {3} (n - 3)! + \dots + (- 1) ^ {n - 1} \binom {n - 1} {n - 1} 1 1.
$$

证明 令 $S$ 是 $\{1,2,\dots ,n\}$ 的所有 $n!$ 个排列的集合，因而 $|S| = n!$ 。对于 $j = 1,2,\dots ,n - 1$ ，令 $P_{j}$ 表示在排列中出现式样 $j(j + 1)$ 这一性质。于是， $\{1,2,\dots ,n\}$ 的排列当且仅当不具有 $P_{1},P_{2},\dots ,P_{n - 1}$ 的任何一个性质时，才计入数 $Q_{n}$ 之内。对于 $j = 1,2,\dots ,n - 1$ ，令 $A_{j}$ 表示满足性质 $P_{j}$ 的 $\{1,2,\dots ,n\}$ 的排列的集合。那么

$$
Q _ {n} = \left\{\overline {{A _ {1}}} \cap \overline {{A _ {2}}} \cap \dots \cap \overline {{A _ {n - 1}}} \right\},
$$

并且我们使用容斥原理计算 $Q_{n}$ 。首先计算 $A_{1}$ 中排列的个数。一个排列当且仅当式样12在其中出现时才在 $A_{1}$ 中。于是， $A_{1}$ 中的一个排列可以视为 $n - 1$ 个符号 $\{12, 3, 4, \dots, n\}$ 的一个排列。我们断定 $\left|A_{1}\right| = (n - 1)!$ ，并且一般地可得

$$
\left| A _ {j} \right| = (n - 1)! \quad (j = 1, 2, \dots , n - 1).
$$

属于集合 $A_{1}$ ， $A_{2}$ ，…， $A_{n - 1}$ 的两个集合的那些排列含有两种式样。这些式样或共有一个元素，如12与23，或没有公共元素，如12与34。一个含有12与34两种式样的排列可以视为 $(n - 2)$ 个符号 $\{12,34,5,\dots ,n\}$ 的一个排列，于是 $\left|A_1\bigcap A_3\right| = (n - 2)!.$ 一个包含12与23两种式样的排列含有式样123，于是可以视为 $(n - 2)$ 个符号 $\{123,4,\dots ,n\}$ 的一个排列，从而 $\left|A_{1}\bigcap A_{2}\right| = (n - 2)!.$ 。一般地，对于 $\{1,2,\dots ,n - 1\}$ 的一个2组合 $\{i,j\}$ 有

$$
\left| A _ {i} \cap A _ {j} \right| = (n - 2) 1.
$$

更一般地，一个含有表12，23，…， $(n - 1)n$ 中的 $k$ 个指定式样的排列可以视为 $n - k$ 个符号的一个排列，从而对于 $\{1,2,\dots ,n - 1\}$ 的一个 $k$ 组合 $\{i_1,i_2,\dots i_k\}$ 有

$$
\left| A i _ {1} \cap A i _ {2} \cap \dots \cap A i _ {k} \right| = (n - k)!
$$

因为对于 $k = 1,2,\dots ,n - 1$ 有 $\binom{n-1}{k}$ 个 $\{1,2,\dots ,n - 1\}$ 的 $k$ 组合，应用容斥原理得到

$$
\begin{array}{l} Q _ {n} = n! - \binom {n - 1} {1} (n - 1)! + \binom {n - 1} {2} (n - 2)! \\ - \binom {n - 1} {3} (n - 3)! + \dots + (- 1) ^ {n - 1} \binom {n - 1} {n - 1} 1 1. \\ \end{array}
$$

利用定理5.4.1的公式，我们算出

$$
Q _ {5} = 5! - \binom {4} {1} 4! + \binom {4} {2} 3! - \binom {4} {3} 2! + \binom {4} {4} 1! = 5 3.
$$

数 $Q_{1}$ ， $Q_{2}$ ， $Q_{3}$ …与错位数有密切关系，事实上，对于 $n = 3$ 4，…，有 $Q_{n} = D_{n} + D_{n - 1}$ （见练习23）．于是，知道了错位数，便可以计算 $Q_{1}$ ， $Q_{2}$ ， $Q_{3}$ ，…，前节中已经知道 $D_{5} = 44$ ， $D_{6} = 265$ ，我们就得到 $Q_{1} = D_{8} + D_{5} = 265 + 44 = 309.$

# 练习

1. 试求在 1 到 10,000 之间不能被 4，5 或 6 整除的那些整数的个数。  
2. 试求1到10,000之间不能被4，6，7或10整除的那些整数的个数.  
3. 试求1到10,000之间既不是完全平方也不是完全立方的那些整数的个数.  
4. 试确定重集 $S = \{4 \cdot a, 3 \cdot b, 4 \cdot c, 5 \cdot d\}$ 的12组合的个数.  
5. 试确定重集 $S = \{\infty, a, 3, 5, c, 7, d\}$ 的10组合的个数.  
6. 面包点心店出售巧克力的、肉桂的和普通的面果，并且在某一特定时间有6个巧克力的、7个肉桂的以及3个普通的面果。如果一盒装12个面果，可以有多少种装有不尽相同的面果的装法？  
7. 确定方程 $x_{1} + x_{2} + x_{3} = 14$ 在不超过8的非负整数中解的个数  
8. 确定方程 $x_{1} + x_{2} + x_{3} = 14$ 在不大于8的正整数中解的个数

9. 确定方程 $x_{1} + x_{2} + x_{3} + x_{4} = 20$ 在 $1 \leqslant x_{1} \leqslant 6, 0 \leqslant x_{3} \leqslant 7, 4 \leqslant x_{3} \leqslant 8$ 和 $2 \leqslant x_{4} \leqslant 6$ 条件下整数解的个数.

10. 令 $S$ 是 $k$ 个不同元素而重复数分别为 $n_1, n_2, \dots, n_k$ 的重集，令 $r$ 是一个正整数使得至少有一个 $S$ 的 $r$ 组合。证明在应用容斥原理确定 $S$ 的 $r$ 组合的个数时，有 $A_1 \cap A_2 \cap \dots \cap A_k = \emptyset$ .

11. 试确定 $\{1, 2, \dots, 8\}$ 的且没有任何一个偶数出现在其自然顺序的位置上的排列的个数.

12. 确定 $\{1, 2, \dots, 8\}$ 的且恰有 4 个整数出现在它们的自然顺序的位置上的排列的个数.

13. 确定集合 $\{1, 2, \dots, n\}$ 的且恰有 $k$ 个整数出现在它们的自然顺序的位置上的排列个数的一般公式.

14. 在一聚会中，7个人寄存帽子，问有多少种方法交还他们的帽子使得

(a) 没有任何一个人得到他自己的帽子

(b) 至少有一个人得到了他自己的帽子.

(c) 至少有两人得到他们自己的帽子.

15. 试用组合论证导出下面的恒等式

$$
\begin{array}{l} n! = \binom {n} {0} D _ {n} + \binom {n} {1} D _ {n - 1} + \binom {n} {2} D _ {n - 2} + \dots + \binom {n} {n - 1} D _ {1} \\ + \binom {n} {n} D _ {\theta_ {1}} (\text {这 里} D _ {\theta_ {2}} \text {定 义 为} 1.) \\ \end{array}
$$

18. 确定重集 $S = \{3 \cdot a, 4 \cdot b, 2 \cdot c\}$ 的且不允许任何一类的全部字母连续出现的排列的个数（例如，abbbbaca是不容许的，但abbbacab是可以的）.  
17. 通过类似于定理5.3.1所给出的对错位数的计算，证明对于 $n = 3, 4, 5, \dots, Q_{n} = (n - 1)(D_{n - 2} + D_{n - 1})$   
18. 从递归公式 $D_{n} = n D_{n-1} + (-1)^{n} (n = 2, 3, 4, \cdots)$ 出发，给出定理5.3.1的一种证明。  
19. 证明阶乘公式 $n! = (n - 1) \left[ (n - 2)! + (n - 1)! \right] \quad (n = 2, 3,$

4.）

20. 证明：当且仅当 $n$ 是奇数时， $D_{n}$ 是偶数。

21. 令 $S$ 是 $k$ 个元素的集合 $\{a_{1}, a_{2}, \dots, a_{k}\}$ ，利用容斥原理确定 $S$ 的 $r$ 组合个数，从而导出一个关于 $\binom{k}{r}$ 的恒等式。

22. 证明5.4节中的数 $Q_{n}$ 可以写成如下形式：

$$
Q _ {n} = (n - 1)! \left(n - \frac {n - 1}{1 !} + \frac {n - 2}{2 !} - \frac {n - 3}{3 !} + \dots + \frac {(- 1) ^ {n - 1}}{(n - 1) !}\right).
$$

23.（续练习22）证明恒等式

$$
(- 1) ^ {k} \frac {n - k}{k !} = (- 1) ^ {k} \frac {n}{k !} + (- 1) ^ {k} \frac {1}{(k - 1) !},
$$

并且利用它证明 $Q_{n} = D_{n} + E_{n - 1}$ $(n = 1,2,3,\dots)$

24. 八名女孩围坐在一个旋转木马上，问有多少种方法改变其座位，使每个女孩有一个不同的女孩在她的前面。

# 第六章 递归关系

在5.3节关于整数集合 $\{1, 2, \dots, n\}$ 的错位数 $D_{n}$ 的研究中，我们导出了下面一些关系：

$$
D _ {n} = (n - 1) \left(D _ {n - 2} + D _ {n - 1}\right) \quad (n = 3, 4, \dots), \tag {6.0.1}
$$

$$
D _ {n} = n D _ {n - 1} + (- 1) ^ {n} \quad (n = 2, 3, \dots). \tag {6.0.2}
$$

这两个关系的每一个都是递归关系的例子。方程(6.0.1)指出怎样由第 $n - 1$ 个和第 $n - 2$ 个错位数计算第 $n$ 个错位数。这个关系连同初始值 $D_{1} = 0$ 和 $D_{2} = 1$ 唯一地确定了错位数序列 $D_{1}, D_{2}, \ldots$ 。由于知道 $D_{1}$ 与 $D_{2}$ ，便可由(6.0.1)确定 $D_{3}$ 。从 $D_{2}$ 与 $D_{3}$ 就能计算 $D_{4}$ ，等等。方程(6.0.2)指出了怎样由第 $n - 1$ 个错位数计算出第 $n$ 个错位数。于是，这个关系连同初始值 $D_{1} = 0$ 也唯一地确定错位数序列。

给定一个数列 $H(0), H(1), H(2), \ldots, H(n), \ldots$ ，如果 $H(n)$ 和数列中在它前面的若干项联系起来的一个方程对所有大于某个整数 $n_0$ 的整数 $n$ 都是有效的，则称这个方程为递归关系。递归关系提供了计算序列中各项的有效手段，这些项在我们的课文中通常表示一系列问题的不同解的个数。在许多情况下，有可能从递归关系得到序列的通项显式。其方法是对这个关系用迭代法推测出一个公式，然后借助于这个递归关系使用归纳法证明它成立。正如在第五章练习18中看到的那样，递归关系(6.0.2)可以用来导出定理5.3.1所给出的公式 $D_n$ 。即使不能得到一个公式，递归关系仍是一个有效的计算工具。实际上，从计算观点看，一个公式可能还不如一个递归关系那

样有价值。

例 对于 $n \geqslant 1$ ，令 $p(n) = p(n, n)$ 是 $\{1, 2, \dots, n\}$ 的排列个数。 $\{1, 2, \dots, n\}$ 的每个排列可以在 $\{1, 2, \dots, n-1\}$ 的一个排列的项之间、开头或末尾的 $n$ 个位置中的某一个位置插入 $n$ 而唯一地得到。因此，

$$
p (n) = n p (n - 1) \quad (n = 2, 3, \dots). \tag {6.0.3}
$$

这个递归关系，连同初始值 $p(1) = 1$ 一起，唯一地确定序列 $p(1), p(2), p(3), \cdots$ 的各项。迭代这个关系，得到

$$
\begin{array}{l} p (n) = n p (n - 1) = n (n - 1) p (n - 2) = \dots \\ = n (n - 1) \dots \times 3 \times 2 \times p (1) \\ = n (n - 1) \dots \times 3 \times 2 \times 1 = n! \\ \end{array}
$$

这便给出关于 $\{1, 2, \dots, n\}$ 的排列个数的公式 $p(n) = n!$ 的另一种推导。另一个方法是由(6.0.3)，与初始值 $p(1) = 1$ 一起，用归纳法证明 $p(n) = n!$ 。

# 6.1 Fibonacci 序列

我们从相当详细地研究一个由递归关系所定义的特殊的计数序列开始递归关系的讨论。Pisa(比萨)的Leonardo在他的1202年出版的Liber Abacci(照字义上说是一本关于算盘的书)中，提出了一对兔在一年里能产出多少对兔子的问题。Leonardo以Fibonacci(Filius Bonacci的缩写)这个名字而闻名于世，他的主要工作是在西欧引入现代计数系统。

Fibonacci提出的问题如下：在一年开始时把一对兔子放入围场中，雌兔每月产雌雄各一的一对新兔子。第二个月开始，每对新兔子也是每月产一对兔子。在一年后围场中有多少对兔子？

在第一个月中，所给的一对兔子要产一对新兔子，因而在第一个月结束时，围场中有两对兔子。在第二个月里只有原来的一对兔子产一对新兔子，于是在第二个月结束时将有3对兔子。在第三个月里，原来的一对兔子和在第一个月里生的一对兔子都要产息，所以在第三个月结束时，围场中将有 $2 + 3 = 5$ 对兔子。对于每个 $n = 1, 2, \dots$ ，令 $f(n)$ 是第 $n$ 个月开始时（等价于第 $n - 1$ 个月结束）围场中兔子的对数。我们已计算了 $f(1) = 1$ ， $f(2) = 2$ ， $f(3) = 3$ 和 $f(4) = 5$ ，而要求的是计算 $f(13)$ 。我们先来导出 $f(n)$ 的一个递归关系，由此就不难算出 $f(13)$ 。在第 $n$ 个月开始时有第 $n - 1$ 个月开始时围场中的全部兔子。此外，在第 $n - 2$ 个月开始时就存在的各对兔子在第 $n - 1$ 个月里都产新的一对。于是，在第 $n$ 个月开始时有 $f(n - 1) + f(n - 2)$ 对兔子。所以

$$
f (n) = f (n - 1) + f (n - 2) \quad (n = 3, 4, \dots).
$$

利用这个关系和已算出的 $f(1)$ ， $f(2)$ ， $f(3)$ 及 $f(4)$ 的值，就可得到

$$
f (5) = f (4) + f (3) = 5 + 3 = 8,
$$

$$
f (6) = f (5) + f (4) = 8 + 5 = 1 3,
$$

$$
f (7) = f (6) + f (5) = 1 3 + 8 = 2 1,
$$

$$
f (8) = f (7) + f (6) = 2 1 + 1 3 = 3 4,
$$

$$
f (9) = f (8) + f (7) = 3 4 + 2 1 = 5 5,
$$

$$
f (1 0) = f (9) + f (8) = 5 5 + 3 4 = 8 9,
$$

$$
f (1 1) = f (1 0) + f (9) = 8 9 + 5 5 = 1 4 4,
$$

$$
f (1 2) = f (1 1) + f (1 0) = 1 4 4 + 8 9 = 2 3 3,
$$

$$
f (1 3) = f (1 2) + f (1 1) = 2 3 3 + 1 4 4 = 3 7 7.
$$

因而在一年后围场中有377对兔子.

令 $f(0) = 1$ ，则 $f(2) = 2 = 1 + 1 = f(1) + f(0)$ ，我们把

满足递归关系

$$
f (n) = f (n - 1) + f (n - 2) \quad (n = 2, 3, 4, \dots),
$$

及初始值 $f(0) = 1, f(1) = 1$ 的数列 $f(0), f(1), f(2), f(3), \cdots$ 称为Fibonacci序列，并且把序列的各个项称为Fibonacci数。通过计算，Fibonacci序列具有下列形式

$$
1, 1, 2, 3, 5, 8, 1 3, 2 1, 3 4, 5 5, 8 9, 1 4 4, 2 3 3, 3 7 7, \dots .
$$

Fibonacci序列有许多值得注意的性质。例如，关于Fibonacci序列 $f(0), f(1), f(2), \ldots$ 各项的部分和公式是

$$
f (0) + f (1) + f (2) + \dots + f (n) = f (n + 2) - 1.
$$

对于 $n = 0$ ，这个关系化为 $f(0) = f(2) - 1$ ，它自然是正确的，因为 $1 = 2 - 1$ 。对于 $n > 0$ ，这个关系可以利用数学归纳法证明。假定该公式用 $n - 1$ 代替 $n$ 为真，则

$$
\begin{array}{l} f (0) + f (1) + f (2) + \dots + f (n) = [ f (0) + f (1) + f (2) \\ \left. + \dots + f (n - 1) \right] + f (n) \\ = [ f (n + 1) - 1 ] + f (n) \\ = [ f (n + 1) + f (n) ] - 1 \\ = f (n + 2) - 1. \\ \end{array}
$$

Fibonacci数的其它一些性质在练习中给出。我们在这里只是要求出关于Fibonacci数的一个公式，并且顺便举例说明求解递归关系的一些技巧。在6.2节和6.3节要更详细地研究它。

考虑Fibonacci递归关系，对于 $n = 2,3,4,\dots ,F(n) =$ $F(n - 1) + F(n - 2)$ ，并暂且不去考虑 $F(0)$ 与 $F(1)$ 的任一个初值。解这个递归关系的一种方法是寻找形如 $F(n) = q^{n}$ 的解，这里 $\pmb{q}$ 是一个不为零的数。于是，我们在熟知的首项为 $q^0 = 1$ 的几何级数中找出一个解来。注意到当且仅当

或 $q^{n - 2}(q^2 - q - 1) = 0 \quad (n = 2, 3, 4, \dots)$

$$
q ^ {n} = q ^ {n - 1} + q ^ {n - 2}
$$

成立时， $F(n) = q^{n}$ 满足递归关系，由于假定 $q$ 不为零，我们断定当且仅当 $q^2 - q - 1 = 0$ 时，或等价地，当且仅当 $q$ 是二次方程 $x^2 - x - 1 = 0$ 的根时， $F(n) = q^n$ 是递归关系的解。利用二次求根公式，得到这个方程的两个根为

$$
q _ {1} = \frac {1 + \sqrt {5}}{2}, q _ {2} = \frac {1 - \sqrt {5}}{2}.
$$

于是

$$
F (n) = \left(\frac {1 + \sqrt {5}}{2}\right) ^ {n} \quad \text {与} \quad F (n) = \left(\frac {1 - \sqrt {5}}{2}\right) ^ {n}
$$

都是递归关系 $F(n) = F(n - 1) + F(n - 2)$ 的解，因为这个递归关系是线性的（不存在 $F$ 的异于一的幂）和齐次的（不存在常数项），由此得到对任何常数 $c_{1}$ 和 $c_{2}$

$$
F (n) = c _ {1} \left(- \frac {1 + \sqrt {5}}{2}\right) ^ {n} + c _ {2} \left(- \frac {1 - \sqrt {5}}{2}\right) ^ {n}
$$

是递归关系的解（读者可以验证一下）。

对于Fibonacci序列，有初始值 $F(0) = F(1) = 1$ ，我们能够选择 $c_{1}$ 与 $c_{2}$ 以便取得这些初始值吗？要满足这些初始值，必须有

$$
\begin{array}{l} (n = 0) \quad c _ {1} + c _ {2} = 1, \\ (n = 1) \quad c _ {1} \left(\frac {1 + \sqrt {5}}{2}\right) + c _ {2} \left(\frac {1 - \sqrt {5}}{2}\right) = 1. \\ \end{array}
$$

这是一个未知数为 $c_{1}$ 和 $c_{2}$ 的两个线性方程的联立方程组，其解为

$$
c _ {1} = \frac {1}{\sqrt {5}} \cdot \frac {1 + \sqrt {5}}{2}, c _ {2} = \frac {- 1}{\sqrt {5}} \cdot \frac {1 - \sqrt {5}}{2}.
$$

我们可以把所得到的结果总结成下面的定理。

定理6.1.1 Fibonacci数满足公式

$$
\begin{array}{l} f (n) = \frac {1}{\sqrt {5}} \left(\frac {1 + \sqrt {5}}{2}\right) ^ {n + 1} - \frac {1}{\sqrt {5}} \left(\frac {1 - \sqrt {5}}{2}\right) ^ {n + 1} \\ (n = 0, 1, 2, \dots). \\ \end{array}
$$

这个解

$$
F (n) = c _ {1} \left(\frac {1 + \sqrt {5}}{2}\right) ^ {n} + c _ {2} \left(\frac {1 - \sqrt {5}}{2}\right) ^ {n}
$$

是递归关系 $F(n) = F(n - 1) \div F(n - 2) (n = 2, 3, 4, \dots)$ 在下述意义下的一般解，即无论什么初始值 $F(0) = a$ 与 $F(1) = b$ 都可以确定常数 $c_{1}$ 与 $c_{2}$ 使上面的表达式满足初始值。这是因为线性方程组

$$
\begin{array}{l} c _ {1} + c _ {2} = a \\ c _ {1} \left(\frac {1 + \sqrt {5}}{2}\right) + c _ {2} \left(\frac {1 - \sqrt {5}}{2}\right) = b \\ \end{array}
$$

的系数矩阵行列式不等于零，即

$$
\det  \left( \begin{array}{c c} 1 & 1 \\ \frac {1 + \sqrt {5}}{2} & \frac {1 - \sqrt {5}}{2} \end{array} \right) = - \sqrt {5}.
$$

因而， $a$ 与 $b$ 无论是什么值，线性方程组都能唯一地解出 $c_{1}$ 与 $c_{2}$

例 考虑数列 $g(0), g(1), g(2), \cdots$ ，使得当 $n = 2, 3, 4, \cdots$ 时， $F(n) = g(n)$ 满足递归关系 $F(n) = F(n - 1) + F(n - 2)$ 且其初始值 $g(0) = 2, g(1) = -1$ 。我们需要确定 $c_{1}$ 与 $c_{2}$ ，于是

$$
\begin{array}{l} c _ {1} + c _ {2} = 2, \\ c _ {1} \frac {1 + \sqrt {5}}{2} + c _ {2} \frac {1 - \sqrt {5}}{2} = - 1, \\ \end{array}
$$

解这个方程组，得到

$$
c _ {1} = \frac {\sqrt {5} - 2}{\sqrt {5}}, c _ {2} = \frac {\sqrt {5} + 2}{\sqrt {5}}.
$$

因而这个数列满足公式

$$
g (n) = \frac {\sqrt {5} - 2}{\sqrt {5}} \left(\frac {1 + \sqrt {5}}{2}\right) ^ {n} + \frac {\sqrt {5} + 2}{\sqrt {5}} \left(\frac {1 - \sqrt {5}}{2}\right) ^ {n}.
$$

Fibonacci数在组合问题中经常出现。用多米诺骨牌覆盖 $2 \times n$ 棋盘的完全覆盖个数等于第 $n$ 个Fibonacci数（见第一章练习4）。把Fibonacci数表为二项式系数之和是一种值得注意的表示法。这一点在第四章练习3已作了提示。

定理6.1.2 位于Pascal三角形由左向右上方的对角线上的二项式系数之和是Fibonacci数。更精确地说，对于 $n \geqslant 0$ ，第 $n$ 个Fibonacci数 $f(n)$ 满足

$$
f (n) = \left( \begin{array}{c} n \\ 0 \end{array} \right) + \left( \begin{array}{c} n - 1 \\ 1 \end{array} \right) + \left( \begin{array}{c} n - 2 \\ 2 \end{array} \right) + \dots + \left( \begin{array}{c} n - k \\ k \end{array} \right),
$$

这里 $k = \lfloor n / 2\rfloor$

证明 对于 $n \geqslant 0$ ，定义

$$
g (n) = \left( \begin{array}{c} n \\ 0 \end{array} \right) + \left( \begin{array}{c} n - 1 \\ 1 \end{array} \right) + \left( \begin{array}{c} n - 2 \\ 2 \end{array} \right) + \dots + \left( \begin{array}{c} n - k \\ k \end{array} \right),
$$

其中 $k = \lfloor n / 2\rfloor$ ，因为对于整数 $p > n$ 有 $\binom{n}{p}=0$ ，我们可以写成

$$
g (n) = \binom {n} {0} + \binom {n - 1} {1} + \binom {n - 2} {2} + \dots + \binom {0} {n}.
$$

需要证明的是：对于所有 $n \geqslant 0$ 有 $f(n) = g(n)$ 。假定我们能证明 $f(0) = g(0)$ ， $f(1) = g(1)$ 以及 $g(n)$ 是递归关系 $F(n) = F(n - 1) + F(n - 2)$ 的一个解，则由于初始值连同递归关系一起唯一地确定了数列，便能断定 $f(n) = g(n)$ 对于所有 $n \geqslant 0$ 成立。但是

$$
g (0) = \left( \begin{array}{c} 0 \\ 0 \end{array} \right) = 1 \div f (0),
$$

$$
g (1) = \left( \begin{array}{c} 1 \\ 0 \end{array} \right) + \left( \begin{array}{c} 0 \\ 1 \end{array} \right) = 1 = f (1).
$$

利用Pascal公式，对于 $n \geqslant 2$ 我们得

$$
\begin{array}{l} g (n - 1) + g (n - 2) = \binom {n - 1} {0} + \binom {n - 2} {1} + \binom {n - 3} {2} + \dots \\ + \binom {0} {n - 1} + \binom {n - 2} {0} + \binom {n - 3} {1} + \dots \\ + \left( \begin{array}{c} 0 \\ n - 2 \end{array} \right) \\ = \binom {n - 1} {0} + \left[ \binom {n - 2} {1} + \binom {n - 2} {0} \right] \\ + \left[ \binom {n - 3} {2} + \binom {n - 3} {1} \right] + \dots \\ + \left[ \binom {0} {n - 1} + \binom {0} {n - 2} \right] \\ = \left( \begin{array}{c} n - 1 \\ 0 \end{array} \right) + \left( \begin{array}{c} n - 1 \\ 1 \end{array} \right) + \left( \begin{array}{c} n - 2 \\ 2 \end{array} \right) + \dots \\ + \left( \begin{array}{c} 1 \\ n - 1 \end{array} \right) \\ = \binom {n} {0} + \binom {n - 1} {1} + \binom {n - 2} {2} + \dots \\ + \left( \begin{array}{c} 1 \\ n - 1 \end{array} \right) + \left( \begin{array}{c} 0 \\ n \end{array} \right) \\ = g (n). \\ \end{array}
$$

于是， $g(n)$ 是对于 $n \geqslant 2$ 的递归关系 $F(n) = F(n - 1) + F(n - 2)$ 的一个解，并且对于所有 $n \geqslant 0$ 有 $f(n) = g(n)$ 。

# 6.2 常系数线性齐次递归关系：不同根的情形

尚没有一般规则可以用来解所有的递归关系。不过，却有一些规则适用于某些类的递归关系，常系数线性齐次递归关系就是其中的一类。这就是形如

$$
H (n) = a _ {1} H (n - 1) + a _ {2} H (n - 2) + \dots + a _ {k} H (n - k)
$$

$$
(n = k, k + 1, \dots). \tag {6.2.1}
$$

的递归关系，这里 $a_1, a_2, \dots, a_n$ 是常数。因为 $H(n)$ 是根据 $H$ 的前面 $k$ 个值通过这个关系来确定的，递归关系(6.2.1)称为 $k$ 阶的。可以假定 $a_k \neq 0$ ，因为否则(6.2.1)将化成较低阶的递归关系。形容词“线性”指的是(6.2.1)中出现 $H$ 的所有值都是一次幂，而形容词“齐次”指的是没有常数项。例如，

$$
H (n) = 3 H (n - 1) ^ {2} + H (n - 2) \quad (n - 2, 3, \dots)
$$

就不是线性递归关系。而

$$
H (n) = 2 H (n - 1) + 3 \quad (n = 1, 2, \dots)
$$

不是齐次递归关系。同样，

$$
H (n) = (n + 2) H (n - 1) + 2 H (n - 2) \quad (n = 2, 3, \dots)
$$

也不是常系数的递归关系，因为 $H(n - 1)$ 的系数 $(n + 2)$ 随 $n$ 而变化。

假定 $H(n) = h(n), n = 0, 1, 2, \cdots$ 满足递归关系(6.2.1)，也可以把它写成如下形式

$$
\begin{array}{l} H (n) - a _ {1} H (n - 1) - a _ {2} H (n - 2) - \dots - a _ {k} H (n - k) \\ = 0 \quad (n = k, k + 1, \dots). \tag {6.2.2} \\ \end{array}
$$

那么，只要 $k$ 个初始值 $h(0), h(1), \cdots, h(k - 1)$ 已经给定，数列 $h(0), h(1), h(2), \cdots$ 就唯一地确定。我们把与递归关系 (6.2.1) 或 (6.2.2) 相联系的方程

$$
x ^ {k} - a _ {1} x ^ {k - 1} - a _ {2} x ^ {k - 2} - \dots - a _ {5} = 0 \tag {6.2.3}
$$

称为(6.2.1)的特征方程。方程(6.2.3)有 $k$ 个根 $q_{1}$ ， $q_{2}$ ，…， $q_{k}$ ，这些根称为(6.2.1)的特征根。特征根可能是复数并且不一定是相异的。不过，因为 $a_{k} \neq 0$ ，它们必定全不为零。

例以Fibonacci序列为一个解的递归关系 $F(n) = F(n - 1) + F(n - 2)$ 有特征方程 $x^{2} - x \pm 1 = 0$ ，正如在6.1节中确定过的那样，这个方程有两个根

$$
q _ {1} = \frac {1 + \sqrt {5}}{2} \quad \text {与} \quad q _ {2} = \frac {1 - \sqrt {5}}{2},
$$

这两个根就是特征根.

定理6.2.1 令 $q$ 是非零实数或复数，则当且仅当 $q$ 是一个特征根时， $H(n) = q^n$ 是逆归关系(6.2.1)的一个解。

证明 可以看出，当且仅当对所有 $n \geqslant k$ 均有

$$
q ^ {n} = a _ {1} q ^ {n - 1} + a _ {2} q ^ {n - 2} + \dots + a _ {k} q ^ {n - k}
$$

或等价地， $q^{n - k}(q^k -a_1q^{k - 1} - a_2q^{k - 2} - \dots -a_k) = 0$

成立时， $H(n) = q^n$ 是(6.2.1)的解．由于 $q\neq 0$ ，于是对所有 $n\geq k$ ，上面的方程等价于方程

$$
q ^ {k} - a _ {1} q ^ {k - 1} - a _ {2} q ^ {k - 2} - \dots - a _ {k} = 0.
$$

因而当且仅当 $q$ 是特征根时， $H(n) = q^{n}$ 是递归关系（6.2.1）的一个解。

现在设 $h_1(n)$ 与 $h_2(n)$ 是递归关系(6.2.1)的两个解，又 $c_1$ 与 $c_2$ 是常数。那么 $c_1 h_1(n) + c_2 h_2(n)$ 也是(6.2.1)的解。为弄清这一点，先注意到，由于 $h_1(n)$ 与 $h_2(n)$ $(n = k, k + 1, \dots)$ 都是解，便有

$$
h _ {1} (n) = a _ {1} h _ {1} (n - 1) + a _ {2} h _ {1} (n - 2) + \dots + a _ {k} h _ {1} (n - k),
$$

$$
h _ {2} (n) = a _ {1} h _ {2} (n - 1) + a _ {2} h _ {2} (n - 2) + \dots + a _ {k} h _ {2} (n - k).
$$

第一个方程乘以 $c_{1}$ ，第二个方程乘以 $c_{2}$ ，然后再相加，得到

$$
\begin{array}{l} c _ {1} h _ {1} (n) + c _ {2} h _ {2} (n) = \left[ c _ {1} a _ {1} h _ {1} (n - 1) + c _ {2} a _ {1} h _ {2} (n - 1) \right] \\ + \left[ c _ {1} a _ {2} h _ {1} (n - 2) + c _ {2} a _ {2} h _ {2} (n - 2) \right] + \dots \\ + \left[ c _ {1} a _ {k} h _ {1} (n - k) + c _ {2} a _ {i} h _ {2} (n - k) \right] \\ = a _ {1} \left[ c _ {1} h _ {1} (n - 1) + c _ {2} h _ {2} (n - 1) \right] + a _ {2} \left[ c _ {1} h _ {1} (n - 2) \right. \\ + c _ {2} h _ {2} (n - 2) ] + \dots + a _ {h} [ c _ {1} h _ {1} (n - k) \\ + c _ {2} h _ {2} (n - k) ] \cdot \\ \end{array}
$$

因而， $H(n) - c_{1}h_{1}(n) + c_{2}h_{2}(n)$ 也是(6.2.1)的解，更一般

地来说，利用归纳法可以证明，如果 $h_1(n), h_2(n), \dots, h_i(n)$ 是(6.2.1)的解， $c_1, c_2, \dots, c_i$ 是常数，则

$$
H (n) = c _ {1} h _ {1} (n) + c _ {2} h _ {2} (n) + \dots + c _ {t} h. (n)
$$

也是一个解。如果把这个结果与定理6.2.1结合起来，就得如下的结论。如果 $q_{1}, q_{2}, \cdots, q_{k}$ 是递归关系(6.2.1)的特征根， $c_{1}, c_{2}, \cdots, c_{k}$ 是任意常数，则

$$
H (n) = c _ {1} q _ {1} ^ {n} + c _ {2} q _ {2} ^ {n} + \dots + c _ {k} q _ {k} ^ {n} \tag {6.2.3}
$$

也是一个解。我们说(6.2.3)是递归关系(6.2.1)的一般解，是假定对每个解 $H(n)$ ，都能适当选取常数 $c_{1}, c_{2}, \cdots, c_{k}$ 使 $H(n)$ 可以表示成(6.2.3)的形式。

定理6.2.2 设递归关系

$$
\begin{array}{l} H (n) = a _ {1} H (n - 1) + a _ {2} H (n - 2) + \dots + a _ {k} H (n - k), \\ a _ {k} \neq 0 (n = k, k + 1, \dots) \\ \end{array}
$$

的特征根 $q_{1}, q_{2}, \cdots, q_{k}$ 互不相同，则

$$
H (n) = c _ {1} q _ {1} ^ {n} + c _ {2} q _ {2} ^ {n} + \dots + c _ {k} q _ {k} ^ {n}
$$

是一般解。

证明 令 $h(n)$ 是递归关系的任一个解，那么， $h(n)$ 由它的初始值 $h(0) = b_0$ ， $h(1) = b_1$ ，…， $h(k - 1) = bk - 1$ 完全地确定。于是，要证明的是，可以选取常数 $c_1, c_2, \dots, c_k$ 使得

$$
\begin{array}{l} c _ {1} + c _ {2} + \dots + c _ {k} = b _ {0}, \\ c _ {1} q _ {1} + c _ {2} q _ {2} + \dots + c _ {k} q _ {k} = b _ {1}, \\ \end{array}
$$

$$
\begin{array}{l} c _ {1} + c _ {2} + \dots + c _ {k} = b _ {0}, \\ c _ {1} q _ {1} + c _ {2} q _ {2} + \dots + c _ {k} q _ {k} = b _ {1}, \\ \dots \tag {6.2.4} \\ c _ {1} q _ {1} ^ {k - 1} + c _ {2} q _ {2} ^ {k - 1} + \dots + c _ {5} q _ {5} ^ {k - 1} = b _ {k} - 1. \\ \end{array}
$$

在(6.2.4)中，我们得到一个具有 $k$ 个未知数 $c_{1}, c_{2}, \dots, c_{k}$ 的 $k$ 个线性方程的方程组。方程组的系数矩阵是

$$
\left[ \begin{array}{c c c c} 1 & 1 & \dots & 1 \\ q _ {1} & q _ {2} & \dots & q _ {3} \\ \dots & \dots & \dots & \dots \\ q _ {1} ^ {i - 1} & q _ {2} ^ {i - 1} & \dots & q _ {4} ^ {i - 1} \end{array} \right].
$$

这个矩阵是矩阵论中众所周知的Vandermonde阵。它的行列式由

$$
\prod_ {1 \leq i = j \leq k} (q _ {j} - q _ {i})
$$

给出，它是形如 $qj - qi$ 且 $1 \leqslant i < j \leqslant k$ 的所有 $\binom{k}{2}$ 个项的乘积（这个结果的证明可在许多关于矩阵论或线性代数的书中找到）。因为所有根 $q_1, q_2, \cdots, q_k$ 都是不同的，所以这个行列式不为零。于是，方程组(6.2.4)关于 $c_1, c_2, \cdots, c_k$ 有唯一解。

例在初始值 $H(0) = 1$ ， $H(1) = 2$ ， $H(2) = 0$ 的条件下，求解递归关系 $H(n) = 2H(n - 1) + H(n - 2) - 2H(n - 3)$ $(n = 3,4,\dots)$

这个递归关系的特征方程是

$$
x ^ {3} - 2 x ^ {2} - x + 2 = 0,
$$

它有根1，-1，2.于是

$$
\begin{array}{l} H (n) = c _ {1} 1 ^ {n} + c _ {2} (- 1) ^ {n} + c _ {3} 2 ^ {n} \\ = c _ {1} + c _ {2} (- 1) ^ {n} + c _ {3} 2 ^ {n} \\ \end{array}
$$

是一般解，我们需要选择 $c_{1}, c_{2}$ 和 $c_{3}$ ，使得

$$
\begin{array}{l} c _ {1} + c _ {2} + c _ {3} = 1, \\ c _ {1} - c _ {2} + 2 c _ {3} = 2, \\ c _ {1} + c _ {2} + 4 c _ {3} = 0. \\ \end{array}
$$

可以求得这个方程组的唯一解是 $c_{1} = 2, c_{2} = -\frac{2}{3}, c_{3} =$

$\frac{1}{3}$ 因而所求的解为

$$
h (n) = 2 - \frac {2}{3} (- 1) ^ {n} - \frac {1}{3} 2 ^ {n}.
$$

例 在信道上传输仅用3个字母 $a, b, c$ 且长度为 $n$ 的词，规定有两个 $a$ 连续出现的词不能传输。试确定这个信道容许传输的词的个数。

令 $h(n)$ 表示容许传输且长度为 $\pmb{n}$ 的词的个数， $n = 1$ ，2,3，…。通过简单计算便可求得 $h(1) = 3$ 与 $h(2) = 8$ 。令 $n\geqslant 3$ 如果词的第一个字母是 $\textit{\textbf{b}}$ 或c，则词可按 $h(n - 1)$ 种方式完成。如果词的第一个字母是a，则第二个字母是b或c，这时这个词可按 $h(n - 2)$ 种方式完成。所以， $h(n) = 2h(n - 1) + 2h(n - 2)$ 这个递归关系

$$
H (n) = 2 H (n - 1) + 2 H (n - 2) \quad (n = 3, 4, \dots)
$$

的特征方程是

$$
x ^ {2} - 2 x - 2 = 0,
$$

其根是

$$
q _ {1} = 1 + \sqrt {3}, q _ {2} = 1 - \sqrt {3}.
$$

所以一般解是

$$
\begin{array}{l} H (n) = c _ {1} \left(1 + \sqrt {3}\right) ^ {n} + c _ {2} \left(1 - \sqrt {3}\right) ^ {n} \\ (n = 3, 4, \dots), \\ \end{array}
$$

为了确定 $h(n)$ ，我们求出 $c_{1}$ 与 $c_{2}$ 使得初始值 $H(1) = 3$ ， $H(2)$ $= 8$ ，这便导出方程组

$$
\begin{array}{l} c _ {1} (1 + \sqrt {3}) + c _ {2} (1 - \sqrt {3}) = 3, \\ c _ {1} (1 + \sqrt {3}) ^ {2} + c _ {2} (1 - \sqrt {3}) ^ {2} = 8. \\ \end{array}
$$

可以证明该方程组的解是

$$
c _ {1} = \frac {\dot {2} + \sqrt {3}}{2 \sqrt {3}}, c _ {2} = \frac {- 2 + \sqrt {3}}{2 \sqrt {3}}.
$$

因此，

$$
\begin{array}{l} h (n) = \frac {2 + \sqrt {3}}{2 \sqrt {3}} (1 + \sqrt {3}) ^ {n} + \frac {- 2 + \sqrt {3}}{2 \sqrt {3}} \\ \times (1 - \sqrt {3}) ^ {n} \quad (n = 1, 2, \dots). \\ \end{array}
$$

如果特征方程的根 $q_{1}, q_{2}, \cdots, q_{k}$ 不是相异的，则

$$
H (n) = c _ {1} q _ {1} ^ {n} + c _ {2} q _ {2} ^ {n} + \dots + c _ {k} q _ {k} ^ {n} \tag {6.2.5}
$$

不是递归关系的通解。例如，递归关系

$$
H (n) = 4 H (n - 1) - 4 H (n - 2)
$$

有特征方程

$$
x ^ {2} - 4 x + 4 = (x - 2) ^ {2} = 0.
$$

这样，2是二重特征根。这时(6.2.5)成为

$$
H (n) = c _ {1} 2 ^ {n} + c _ {2} 2 ^ {n} = \left(c _ {1} + c _ {2}\right) 2 ^ {n} = c 2 ^ {n},
$$

这里 $c = c_{1} + c_{2}$ 是新常数，所以，我们仅有一个常数，要选择 $c$ 使 $H$ 的两个初始值都满足就不会总是可能的了。比如，规定初始值 $H(0) = 1$ ， $H(1) = 3$ 。为满足这些初始值必须有

$$
\begin{array}{l} c = 1, \\ 2 c = 3, \\ \end{array}
$$

这显然是不可能的。因而， $H(n) = c2^n$ 不是递归关系 $H(n) = 4H(n - 1) - 4H(n - 2)$ 的一般解。在下节我们将看到怎样去求特征方程有重根的递归关系的一般解。

# 6.3 常系数线性齐次递归关系：重根的情形

刚在上一节考虑过的递归关系 $H(n) = 4H(n - 1) - 4H(n - 2)$ ，问题在于它的特征方程的两个根是相等的，因而与这两

个根2，2相联系的两个解化为一个解，在这种情况下，我们需要找出一个解，它不同于与重根相联系的解。除解 $H(n) = 2^{n}$ 外， $H(n) = n2^{n}$ 也是一个解。为弄清这一点，考察

$$
\begin{array}{l} 4 (n - 1) 2 ^ {n - 1} - 4 (n - 2) 2 ^ {n - 2} \\ = 4 [ (n - 1) 2 - (n - 2) ] 2 n - 2 \\ = 4 (2 n - 2 - n + 2) 2 ^ {n - 2} \\ = 4 (n) 2 ^ {n - 2} = n 2 ^ {1}. \\ \end{array}
$$

于是， $H(n) = n2^{n}$ 满足递归关系 $H(n) = 4H(n - 1) - 4H(n - 2)$ ，我们断定

$$
H (n) = c _ {1} 2 ^ {n} + c _ {2} n 2 ^ {n}
$$

是这个递归关系的一般解。为了证实这一点，令 $H(0) = a$ ， $H(1) = b$ 是任意初始值。要满足这些初始值必须有

$$
\begin{array}{l} \mathbf {c} _ {1} = \mathbf {a}, \\ 2 c _ {1} \div 2 c _ {2} = b. \\ \end{array}
$$

这个线性方程组有唯一解 $c_{1} = a, c_{2} = (b - 2a) / 2$ . 因而， $H(n) = c_{1}2^{n} + c_{2}n2^{n}$ 是一般解.

现在我们推广这些思想。考虑递归关系

$$
\begin{array}{l} H (n) = a _ {1} H (n - 1) + a _ {2} H (n - 2) + \dots \\ + a _ {k} H (n - k), \quad a _ {k} \neq 0 \\ (n = k, k + 1, \dots) \tag {6.3.1} \\ \end{array}
$$

及其特征方程

$$
p (x) = x ^ {k} - a _ {1} x ^ {k - 1} - a _ {2} x ^ {k - 2} - \dots - a _ {k} = 0.
$$

为确定起见，假定 $q$ 是 $p(x)$ 的一个三重根，则 $p(x) = (x - q)^3 a(x)$ ，并且对于每个 $n = k, k + 1, \dots, q$ 是 $p_n(x)$ 的一个三重根，这里 $p_n(x)$ 定义为

$$
\begin{array}{l} p _ {n} (x) = x ^ {n - k} p (x) \\ = x ^ {n} - a _ {1} x ^ {n - 1} - a _ {2} x ^ {n - 2} - \dots - a _ {n} x ^ {n - k}. \\ \end{array}
$$

于是， $q$ 是 $\pmb{p}_{n}(\pmb{x})$ 的导数

$$
\begin{array}{l} p _ {n} ^ {\prime} (x) = n x ^ {n - 1} - a _ {1} (n - 1) x ^ {n - 2} - a _ {2} (n - 2) x ^ {n - 3} \\ \ldots - a _ {k} (n - k) x ^ {n - k - 1} \\ \end{array}
$$

的一个二重根，因而也是

$$
\begin{array}{l} x p _ {n} ^ {\prime} (x) = n x ^ {n} - a _ {1} (n - 1) x ^ {n - 1} - a _ {2} (n - 2) x ^ {n - 2} \\ - \dots - a _ {k} (n - k) x ^ {n - k} \\ \end{array}
$$

的一个二重根。具体地说，

$$
\begin{array}{l} n q ^ {n} = a _ {1} (n - 1) q ^ {n - 1} + a _ {2} (n - 2) q ^ {n - 2} \\ + \dots + a _ {k} (n - k) q ^ {n - k}, \\ \end{array}
$$

换句话说， $H(n) = nq^n$ 是递归关系(6.3.1)的一个解，因为 $\pmb{q}$ 是 $xp_n^{\prime}(x)$ 的一个二重根， $\pmb{q}$ 也是它的导数

$$
\begin{array}{l} n ^ {2} x ^ {n - 1} - a _ {1} (n - 1) ^ {2} x ^ {n - 2} - a _ {2} (n - 2) ^ {2} x ^ {n - 3} \\ - \dots - a _ {k} (n - k) ^ {2} x ^ {n - k - 1} \\ \end{array}
$$

的一个根，于是也是由上式乘以 $x$ 所得到的多项式

$$
\begin{array}{l} n ^ {2} x ^ {n} - a _ {1} (n - 1) ^ {2} x ^ {n - 1} - a _ {2} (n - 2) ^ {2} x ^ {n - 2} \\ \dots - \sigma_ {k} (n - k) ^ {2} x ^ {n - k} \\ \end{array}
$$

的一个根。因此

$$
\begin{array}{l} n ^ {2} q ^ {n} = a _ {1} (n - 1) ^ {2} q ^ {n - 1} + a _ {2} (n - 2) ^ {2} q ^ {n - 2} \\ + \dots + a _ {k} (n - k) ^ {2} q ^ {n - k}, \\ \end{array}
$$

于是 $H(n) = n^{2}q^{n}$ 是递归关系(6.3.1)的一个解。总之，如果 $q$ 是特征方程的三重根，则 $H(n) = q^{n}$ ， $H(n) = nq^{n}$ 以及 $H(n) = n^{2}q^{n}$ 都是递归关系(6.3.1)的解。更一般地，若 $q$ 是特征方程的 $e$ 重根，则 $H(n) = q^{n}$ ， $H(n) = nq^{n}$ ，…， $H(n) = n^{e - 1}q^{n}$ 都是(6.3.1)的解。

定理6.3.1 设 $q_{1}, q_{2}, \cdots, q_{t}$ 是递归关系

$$
\begin{array}{l} H (n) = a _ {1} H (n - 1) + a _ {2} H (n - 2) + \dots \\ + a _ {k} H (n - k), a _ {k} \neq 0 (n = k, k + 1, \dots) \\ \end{array}
$$

的特征方程相异的根。那么，这个递归关系对应于 $q_{i}$ 的部分一般解是

$$
\begin{array}{l} H _ {i} (n) = c _ {1} q _ {i} ^ {n} + c _ {2} q _ {i} ^ {n} + \dots + c _ {r} n ^ {c _ {i} - 1} q _ {i} ^ {n} \\ = \left(c _ {1} + c _ {2} \pi + \dots + c _ {c _ {i}} n ^ {e _ {i} - 1}\right) q _ {i} ^ {n}, \tag {6.3.2} \\ \end{array}
$$

这里 $q_{i}$ 是特征方程的 $e_i$ 重根（ $i = 1, 2, \dots, t$ ）。

$$
H (n) = H _ {1} (n) + H _ {2} (n) + \dots + H _ {i} (n) \tag {6.3.3}
$$

是这递归关系的一般解。

证明 根据我们在叙述这个定理之前的分析以及由于一个数与一个解之积或两个解之和同样是解可知，(6.3.2)所定义的 $H_{i}(n)(i = 1,2,\dots ,t)$ 都是解。从而 $H(n) = H_{1}(n) + H_{2}(n) + \dots +H_{i}(n)$ 也是一个解。要证明它是一般解，和定理6.2.2的证明一样，需要证明某个系数矩阵行列式不为零。这个矩阵是Vandermonde矩阵的一个推广并且其行列式之值为

$$
\prod_ {i = 1} ^ {t} (- q _ {i}) \binom {e _ {i}} {2} \prod_ {1 \leq i <   j \leq t} (q _ {j} - q _ {i}) ^ {e _ {j} e _ {i}},
$$

因为 $q_{1}, \cdots, q_{l}$ 都是互异的不为零的数，这个行列式不等于零。

例假定初始值为 $H(0) = 1$ ， $H(1) = 0$ ， $H(2) = 1$ 和 $H(3) = 2$ ，求解递归关系

$$
\begin{array}{l} H (n) = - H (n - 1) + 3 H (n - 2) + 5 H (n - 3) \\ + 2 H (n - 4), (n = 4, 5, \dots) \\ \end{array}
$$

这个递归关系的特征方程是

$$
x ^ {4} + x ^ {3} - 3 x ^ {2} - 5 x - 2 = 0.
$$

其根为-1，-1，-1，2。于是一般解中对应于-1这个根的部分是

$$
H _ {1} (n) = c _ {1} (- 1) ^ {n} + c _ {2} n (- 1) ^ {n} + c _ {3} n ^ {2} (- 1) ^ {n},
$$

而一般解对应于2这个根的部分是

$$
H _ {2} (n) = c _ {4} 2 ^ {\text {日}}
$$

因此，

$$
\begin{array}{l} H (n) = H _ {1} (n) + H _ {2} (n) \\ = c _ {1} (- 1) ^ {n} + c _ {2} n (- 1) ^ {n} + c _ {3} n ^ {2} (- 1) ^ {n} + c _ {4} 2 ^ {n} \\ \end{array}
$$

是一般解。我们必须选择 $c_{1}, c_{2}, c_{3}$ 和 $c_{4}$ ，使得

$$
\begin{array}{l} c _ {1} + c _ {4} = 1, \\ - c _ {1} - c _ {2} - c _ {3} + 2 c _ {4} = 0, \\ c _ {1} + 2 c _ {2} + 4 c _ {3} + 4 c _ {4} = 1, \\ - c _ {1} - 3 c _ {2} - 9 c _ {3} + 8 c _ {4} = 2. \\ \end{array}
$$

可以求出这个方程组的唯一解是 $c_{1} = \frac{42}{52}, c_{2} = -\frac{29}{52}$ ,

$c_{3} = \frac{7}{52}, c_{4} = \frac{10}{52}$ . 因此所求的解是

$$
\begin{array}{l} h (n) = \frac {4 2}{5 2} (- 1) ^ {n} - \frac {2 9}{5 2} n (- 1) ^ {n} \\ + \frac {7}{5 2} n ^ {2} (- 1) ^ {n} + \frac {1 0}{5 2} 2 ^ {n}. \\ \end{array}
$$

# 6.4 迭代与归纳

在上面两节中，我们已指出怎样解任意常系数线性齐次递归关系。不过，其成功是依赖于能找到特征方程的根，而这一点并非总是可能的。一旦知道这些根后，为了满足初始值还必须解一个线性方程组。如果递归关系的阶是 $k$ ，那么就有 $k$ 个未知数的 $k$ 个方程。实际上，利用特征根去求递归关系的解即使不是不可能也是非常困难的。若递归关系不是常系数线性和齐次的，还没给出任何一种求解的方法。我们将利用几个例子来

说明求出递归关系解的两种可能的一般方法。

例 假定初始值为 $H(0) = 0$ ，求解递归关系 $H(n) = H(n - 1) + n^3 \quad (n = 1, 2, 3 \cdots)$ .

如果迭代这个递归关系，可求得

$$
\begin{array}{l} H (n) = H (n - 1) + n ^ {3} = H (n - 2) + (n - 1) ^ {3} + n ^ {3} \\ = \dots \\ = H (1) + 2 ^ {3} + \dots + (n - 1) ^ {3} + n ^ {3} \\ = H (0) + 1 ^ {3} + 2 ^ {3} + \dots + (n - 1) ^ {2} + n ^ {3} \\ = 0 + 1 ^ {3} + 2 ^ {3} + \dots + (n - 1) ^ {3} + n ^ {3} \\ = 1 ^ {3} + 2 ^ {3} + \dots + (n - 1) ^ {3} + n ^ {3}. \\ \end{array}
$$

于是，对于 $n \geqslant 1$ ， $H(n)$ 是前 $n$ 项立方和。我们能找到这个和的简单表达式吗？利用这个递归关系，计算 $H(n)$ 的前面少数几项并且寻找其结构

$$
\begin{array}{l} H (0) = 0, \\ H (1) = 0 + 1 ^ {3} = 1, \\ H (2) = H (1) + 2 ^ {3} = 1 + 2 ^ {3} = 9, \\ H (3) = H (2) + 3 ^ {3} = 9 + 2 7 = 3 6, \\ H (4) = H (3) + 4 ^ {3} = 3 6 + 6 4 = 1 0 0, \\ H (5) = H (4) + 5 ^ {3} = 1 0 0 + 1 2 5 = 2 2 5. \\ \end{array}
$$

我们观察到

$$
\begin{array}{l} H (0) = 0 ^ {2}, \\ \bar {H} (1) = 1 ^ {2} = (1) ^ {2}, \\ H (2) = 3 ^ {2} = (1 + 2) ^ {2}, \\ H (3) = 6 ^ {2} = (1 + 2 + 3) ^ {2}, \\ H (4) = 1 0 ^ {2} = (1 + 2 + 3 + 4) ^ {2}, \\ H (5) = 1 5 ^ {2} = (1 + 2 + 3 + 4 + 5) ^ {2}. \\ \end{array}
$$

在4.3节已经证明

$$
1 + 2 + 3 + \dots + n = \frac {n (n + 1)}{2}.
$$

这样，上面算出的值适合于公式

$$
H (n) = \frac {n ^ {2} (n + 1) ^ {2}}{4}. \tag {6.4.1}
$$

为了证实（6.4.1）确是这个递归关系具有已知初始值的解，我们对 $n$ 施行归纳证明。已经注意到，该公式对于 $n = 0, 1, 2, \dots, 5$ 是正确的。假定它对于 $n$ 是正确的，我们往证它对于 $n + 1$ 也是正确的。使用递归关系并注意到（6.4.1）就可推出

$$
\begin{array}{l} H (n + 1) = H (n) + (n + 1) ^ {3} \\ = \frac {n ^ {2} (n + 1) ^ {2}}{4} + (n + 1) ^ {3} \\ = \frac {n ^ {2} (n + 1) ^ {2} + 4 (n + 1) ^ {3}}{4} \\ = \frac {(n + 1) ^ {2} (n ^ {2} + 4 n + 4)}{4} \\ = \frac {(n + 1) ^ {2} (n + 2) ^ {2}}{4} \\ \end{array}
$$

它正是用 $(n + 1)$ 代替 $n$ 时的公式。因而，根据归纳法这公式对所有 $n$ 均成立。

例（河内塔谜）有三个竹桩，把 $n$ 个圆盘按照由小到大的尺寸穿在一个竹桩上，最大的在底下。打算一次一个地搬动这些圆盘从这个竹桩转移到另一个上，并规定任何时候都不容许把较大圆盘放在较小圆盘的顶上。我们的问题就是去确定完成这个转移而必须搬动的次数。

令 $h(n)$ 为转移这 $\pmb{n}$ 个盘子而所需要搬动的次数，这里

$n = 0, 1, 2, \cdots$ 。容易证明 $h(0) = 0, h(1) = 1$ 和 $h(2) = 3$ ，我们能找到 $h(n)$ 满足的递归关系吗？为了把 $n$ 个圆盘从一个桩移动到另一个桩上，必须先把顶上 $n - 1$ 个圆盘移到一个桩上，把最大圆盘移到空着的桩上，然后再把 $n - 1$ 个圆盘移到有最大圆盘的桩上。于是， $h(n)$ 是递归关系

$$
H (n) = 2 H (n - 1) + 1 \quad (n = 1, 2, \dots) (6, 4, 2)
$$

的一个解[严格地说，我们的论证只适用于当 $n \geqslant 2$ 时 $h(n)$ 满足这个递归关系，但是容易看到对于 $n = 1$ 时它也成立，因为 $h(0) = 0, h(1) = 1$ ]。这是一个常系数线性递归关系，但它不是齐次的，因为存在为1的项。为了求 $h(n)$ ，我们利用迭代法解具有初始值 $H(0) = 0$ 的（6.4.2）如下：

$$
\begin{array}{l} H (n) = 2 H (n - 1) + 1 \\ = 2 [ 2 H (n - 2) + 1 ] + 1 = 2 ^ {2} H (n - 2) + 2 + 1 \\ = 2 ^ {2} [ 2 H (n - 3) + 1 ] \div 2 + 1 \\ = 2 ^ {3} H (n - 3) + 2 ^ {2} \quad 2 + 1 \\ = \dots \\ = 2 ^ {n} H (0) + 2 ^ {1 - 1} + \dots + 2 ^ {2} + 2 + 1 \\ = 2 ^ {n - 1} + \dots + 2 ^ {2} + 2 + 1. \\ \end{array}
$$

因为 $H(0) = 0$ ，所以

$$
H (n) = 2 ^ {n - 1} + 2 ^ {n - 2} + \dots + 2 ^ {2} + 2 + 1,
$$

并且使用几何级数的求和公式便知，（6.4.2）具有初始值 $H(0) = 0$ 的解是

$$
H (n) = \frac {2 ^ {n} - 1}{2 - 1} \approx 2 ^ {n} - 1 \quad (n = 0, 1, 2, \dots). \tag {6.4.3}
$$

因此，对于 $n = 0, 1, 2, \dots, h(n) = 2^n - 1$ ， $h(n)$ 这个公式也可以用数学归纳法和递归关系来证明。

例 给定 $n$ 个实数 $a_{1}, a_{2}, \cdots, a_{n}$ , 有多种方法构造它们的乘积, 问共有多少种不同方法?

对于 $n = 1,2,3,\dots$ ，令 $h(n)$ 表示构造 $\pmb{n}$ 个数乘积的方法的个数，我们首先对于较小的 $\pmb{n}$ 计算 $h(n)$ ，注意到 $h(1) = 1$ [这可作为 $h(1)$ 的定义]．对于 $\pmb {n} = \pmb{2}$ ，有两种方法：

$$
a _ {1} \times a _ {2}, \quad a _ {2} \times a _ {1}.
$$

于是 $h(2) = 2$ ，对于 $n = 3$ ，有12种方法：

$$
\begin{array}{l} a _ {1} \times (a _ {2} \times a _ {3}), a _ {2} \times (a _ {1} \times a _ {3}), a _ {3} \times (a _ {1} \times a _ {2}), \\ a _ {1} \times \left(a _ {3} \times a _ {2}\right), a _ {2} \times \left(a _ {3} \times a _ {1}\right), a _ {3} \times \left(a _ {2} \times a _ {1}\right), \\ (a _ {2} \times a _ {3}) \times a _ {1}, (a _ {1} \times a _ {3}) \times a _ {2}, (a _ {1} \times a _ {2}) \times a _ {3}, \\ \left(a _ {3} \times a _ {2}\right) \times a _ {1}, \left(a _ {3} \times a _ {1}\right) \times a _ {2}, \left(a _ {2} \times a _ {1}\right) \times a _ {3}. \\ \end{array}
$$

因而 $h(3) = 12$ 。事实上， $h(3)$ 可由 $h(2)$ 确定如下。我们选取数 $a_1, a_2, a_3$ 中两个先作乘积。这存在3种选择，并且每一对可按2种方法作乘积。剩下的那个数可左乘或右乘所选取的数对之积（在上表中的第一列包含着先选择作乘积的一对数是 $a_2$ 与 $a_3$ 时产生的那些乘积）。因此 $h(3) = (3 \times 2)h(2) = 12$ 。现在我们推广这种思想求出 $h(n)$ 的一个递归关系。令 $n \geqslant 2$ 。 $n$ 个数 $a_1, a_2, \cdots, a_n$ 的乘积需要对两个数的相乘相继进行 $n - 1$ 次。考虑 $n - 1$ 个数 $a_1, a_2, \cdots, a_{n - 1}$ 的 $h(n - 1)$ 个乘积中的任何一个。这样的一个乘积需要进行 $n - 2$ 次乘法。我们可以把 $a_n$ 与这种乘积用下列任意一个方法联接起来：

（i）对 $n - 2$ 次乘法之一的任一因子的任一边乘以 $\pmb{a}_{\lambda}$   
（ii）用 $a_{n}$ 左乘 $a_1$ ， $a_2$ ，…， $a_{n - 1}$ 之积.  
（iii）用 $a_{n}$ 右乘 $a_1, a_2, \dots, a_{n-1}$ 之积.

例如，设 $n = 6$ 并考虑数 $a_1, a_2, a_3, a_4, a_5$ 的 $h(5)$ 个乘积之一，比方说

$$
(a _ {1} \times a _ {2}) \times [ (a _ {3} \times a _ {4}) \times a _ {5} ].
$$

在这个乘积中作了4次乘法。我们可在这些乘法中任选一次乘法，如 $(a_{3} \times a_{4}) \times a_{5}$ ，即 $(a_{3} \times a_{4})$ 与 $a_{5}$ 相乘，并且把 $a_{6}$ 按4种方法的任一种插入：

$$
\left[ a _ {6} \times \left(a _ {3} \times a _ {4}\right) \right] \times a _ {8}, \left(a _ {3} \times a _ {4}\right) \times \left(a _ {6} \times a _ {5}\right),
$$

$$
\left[ \left(a _ {3} \times a _ {4}\right) \times a _ {6} \right] \times a _ {6}, \left(a _ {3} \times a _ {4}\right) \times \left(a _ {6} \times a _ {8}\right).
$$

对应于 $a_1, a_2, a_3, a_4, a_5, a_6$ 的乘积是

$$
\left(a _ {1} \times a _ {2}\right) \times \left(\left[ a _ {6} \times \left(a _ {3} \times a _ {4}\right) \right] \times a _ {5}\right),
$$

$$
\left(a _ {1} \times a _ {2}\right) \times \left[ \left(a _ {3} \times a _ {4}\right) \times \left(a _ {6} \times a _ {5}\right) \right],
$$

$$
\left(a _ {1} \times a _ {2}\right) \times \left(\left[ \left(a _ {3} \times a _ {4}\right) \times a _ {8} \right] \times a _ {5}\right),
$$

$$
\left(a _ {1} \times a _ {2}\right) \times \left[ \left(a _ {3} \times a _ {4}\right) \times \left(a _ {5} \times a _ {6}\right) \right].
$$

此外，用 $a_{3}$ 左乘或右乘所给的 $a_{1}, a_{2}, a_{3}, a_{4}, a_{5}$ 的乘积；

$$
\begin{array}{l} a _ {5} \times \left(\left(a _ {1} \times a _ {2}\right) \times \left[ \left(a _ {3} \times a _ {4}\right) \times a _ {5} \right]\right), \\ \left(\left(a _ {1} \times a _ {2}\right) \times \left[ \left(a _ {8} \times a _ {4}\right) \times a _ {5} \right]\right) \times a _ {6}. \\ \end{array}
$$

由上面的分析我们断定，存在 $4(n - 2) + 1 + 1 = 4n - 6$ 种方法把 $a_{n}$ 与 $a_1, a_2, \dots, a_{r-1}$ 的 $h(n - 1)$ 个乘积的任何一个积联接起来。因而，我们已确定 $h(n)$ 是递归关系

$$
H (n) = (4 n - 6) H (n - 1) \quad (n = 2, 3, \dots) \tag {6.4.4}
$$

的一个解。为了求出 $h(n)$ ，利用迭代法解这个具有初始值 $H(1) = 1$ 的关系如下：

$$
\begin{array}{l} H (n) = (4 n - 6) H (n - 1) \\ = (4 n - 6) (4 (n - 1) - 6) H (n - 2) \\ = (4 n - 6) (4 n - 1 0) H (n - 2) \\ = (4 n - 6) (4 n - 1 0) (4 n - 1 4) H (n - 3) \\ = \dots \\ = (4 n - 6) (4 n - 1 0) (4 n - 1 4) \dots \times 6 \times 2 H (1) \\ = (4 n - 6) (4 n - 1 0) (4 n - 1 4) \dots \times 6 \times 2, \\ \end{array}
$$

因为 $H(1) = 1$ ，我们可以把上式改写为

$$
\begin{array}{l} H (n) = 2 (2 n - 3) 2 (2 n - 5) 2 (2 n - 7) \dots 2 (3) 2 (1) \\ = 2 ^ {n - 1} (1 \times 3 \times 5 \times \dots (2 n - 7) (2 n - 5) (2 n - 3)) \\ = - \frac {2 ^ {n - 1} (2 n - 2) !}{2 \times 4 \times \cdots (2 n - 6) (2 n - 4) (2 n - 2)} \\ = - \frac {2 ^ {n - 1} (2 n - 2) !}{2 ^ {n - 1} \times 1 \times 2 \times \cdots (n - 3) (n - 2) (n - 1)} \\ = \frac {(2 n - 2) !}{(n - 1) !} = (n - 1)! \binom {2 n - 2} {n - 1}. \\ \end{array}
$$

所以（6.4.4）具有初始值 $H(1) = 1$ 的解是

$$
\begin{array}{l} H (n) = (n - 1)! \left( \begin{array}{l} 2 n - 2 \\ n - 1 \end{array} \right) \\ (n = 1, 2, 3, \dots), \tag {6.4.5} \\ \end{array}
$$

于是

$$
h (n) = (n - 1)! \left( \begin{array}{c} 2 n - 2 \\ n - 1 \end{array} \right) \quad (n = 1, 2, 3, \dots).
$$

作为最后的说明，我们进行如下的考察。假定在形成 $n$ 个数 $a_1, a_2, \dots, a_n$ 的乘积时，需要保持它们的给定次序。例如，对于 $n = 3, a_1 \times (a_2 \times a_3)$ 是容许的乘积，但不容许 $(a_2 \times a_1) \times a_3$ ，对于 $n = 1, 2, 3, \dots$ ，令 $g(n)$ 表示 $a_1, a_2, \dots, a_n$ 保持给定次序所形成的乘积的个数。那么，显然有

$$
g (n) = \frac {1}{n !} h (n).
$$

于是使用关于 $h(n)$ 的递归关系，便知 $g(n)$ 满足递归关系

$$
G (n) = - \frac {4 n - 6}{n} - G (n - 1) \quad (n = 2, 3, \dots)
$$

利用关于 $h(n)$ 的公式，则有

$$
g (n) = \frac {1}{n} \left( \begin{array}{c} 2 n - 2 \\ n - 1 \end{array} \right) \quad (n = 1, 2, 3, \dots).
$$

上面所给定的数 $g(n)$ 称为Catalan数。它们在各种不同范围里出现。在第七章它们将作为一个几何问题解的个数出现。

另一方面， $g(n)$ 也满足一个比上面给出的更为复杂的递归关系。为确定它，我们注意到在形成 $n$ 个数 $a_1, a_2, \dots, a_n$ 按给定次序的乘积时，最后一个乘法是某个 $a_1, a_2, \dots, a_k$ 之积与某个 $a_{k+1}, \dots, a_n$ 之积相乘，其中 $k$ 是数 1，2，…， $n-1$ 中的任意一个数。例如，在乘积

$$
\left[ a _ {1} \times \left(a _ {2} \times a _ {3}\right) \right] \times \left[ \left(a _ {4} \times a _ {5}\right) \times \left(a _ {8} \times a _ {7}\right) \right]
$$

中的最后一次乘法是这两个方括号内的表达式之间相乘。由于 $g(k)$ 个 $a_1, \cdots, a_k$ 之积中的每一个积可以与 $g(n - k)$ 个 $a_{k+1}, \cdots, a_n$ 之积中的每一个积联结起来，我们断定

$$
\begin{array}{l} g (n) = \sum_ {k = 1} ^ {n - 1} g (k) g (n - k) \\ \begin{array}{r l} & = g (1) g (n - 1) + g (2) g (n - 2) + \dots + g (n - 1) g (1) \\ & \quad (n = 2, 3, \dots). \end{array} \\ \end{array}
$$

所以，对于 $n = 2,3,\dots ,g(n)$ 是递归关系 $H(n) = H(1)H(n - 1)$ $+H(2)H(n - 2) + \dots +H(n - 1)H(1)$ 的一个解，在第七章中，我们将借助于Newton二项式定理直接解这个关系。

# 6.5 差分表

通常在数值分析课程中研究差分表。假设由实验得到一个函数性态的某些数据，利用差分表得到一个多项式函数符合这些数据。这里我们对差分表的兴趣主要在于利用它可以把数 $x$ 的 $k$ 次幂（ $k$ 是正整数）表示为广义二项式系数之和。这样一种表示法可以用来计算前 $n$ 个正整数的 $k$ 次幂之和。我们也将应用差分表解一个组合几何问题。

考虑一个对于所有实数 $x$ 定义的函数 $p$ 。照顾到讨论的效果，我们取一个由 $p(x) = 2x^2 + 3x + 1$ 定义的特殊的多项式函数。对 $x = 0, 1, 2, \cdots$ 计算 $p(x)$ 并把这些值列成一行，称为第0行。

$$
1 \quad 6 \quad 1 5 \quad 2 8 \quad 4 5 \quad 6 6 \quad 9 1 \quad \dots .
$$

在下面的一行，称为第1行，列出第0行的各相邻项之差。由 $\Delta p(x) = p(x + 1) - p(x)$ 定义一个新的函数 $\Delta p$ 。于是这些差就是对于 $x = 0, 1, 2, \dots$ 的函数值 $\Delta p(x)$ ，我们把它们称为 $p$ 的第一阶差分：

$$
\begin{array}{c c c c c c c c c} 1 & 6 & 1 5 & 2 8 & 4 5 & 6 6 & 9 1 & \dots \\ & 5 & 9 & 1 3 & 1 7 & 2 1 & 2 5 & \dots \end{array}
$$

在第2行中我们列入第一行中各相邻项之间的差，它们就是函数 $\Delta p$ 的差分或对于 $x = 0$ ，1，2，…的函数值 $\Delta^2 p(x)$ ，这里 $\Delta^2 p$ 定义为

$$
\begin{array}{l} \Delta^ {2} p (x) = \Delta (\Delta p) (x) = \Delta p (x + 1) - \Delta p (x) \\ = p (x + 2) - p (x + 1) - [ p (x + 1) - p (x) ] \\ = p (x + 2) - 2 p (x + 1) + p (x) \\ \end{array}
$$

这些值称为 $p$ 的第二阶差分：

$$
\begin{array}{c c c c c c c c c} 1 & 6 & 1 5 & 2 8 & 4 5 & 6 6 & 9 1 & \dots \\ & 5 & 9 & 1 3 & 1 7 & 2 1 & 2 5 & \dots \\ & & 4 & 4 & 4 & 4 & 4 & \dots \end{array}
$$

我们无限地继续把每一行各相邻两数之差列入新的一行中。这样排成的表称为函数 $p$ 的差分表。对于 $k \geqslant 2$ ，由 $\Delta^k p(x) = \Delta^{k-1} p(x+1) - \Delta^{k-1} p(x)$ 归纳地定义函数 $\Delta^k p$ 。 $\Delta^k p(x)$ 对于 $x = 0, 1, 2, \cdots$ 的这些值称为 $p$ 的第 $k$ 阶差分。此外，我们定义 $\Delta^0 p = p$ ，并把对于 $x = 0, 1, 2, \cdots$ 的函数值 $p(x)$ 称为 $p$ 的第 0 阶差分。如果差分表的任一行全由 0 组成，则所

有以下各行也全由0组成并且毋须把它们列入表中，例如，由 $p(x) = 2x^2 + 3x + 1$ 定义的函数 $p$ 的差分表由下表确定：

<table><tr><td>1</td><td>6</td><td>.15</td><td>28</td><td>45</td><td>66</td><td>91</td><td>...</td></tr><tr><td>5</td><td>9</td><td>13</td><td>17</td><td>21</td><td>25</td><td>...</td><td></td></tr><tr><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>...</td><td></td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>...</td><td></td><td></td></tr></table>

下面读者应记住多项式的两个基本论据。

（i）如果多项式 $p(x)$ 对于无限多个 $x$ 的值都等于0，则 $p(x)$ 恒等于0.

（ii）若 $p(x)$ 与 $g(x)$ 是两个至多为 $n$ 次的多项式并且在 $n + 1$ 个不同的数上它们取相同的值，则对所有的 $x, p(x) = g(x)$ 。

（ii）的一个直接推论是，如果两个多项式有相同的差分表，则这两个多项式是相同的。

定理6.5.1 设 $p(x) = a_{n}x^{n} + a_{n - 1}x^{n - 1} + \dots +a_{1}x + a_{0}$ 是 $n$ 次多项式，则 $p(x)$ 的第 $n + 1$ 阶差分全为零。

证明 我们对 $n$ 施行归纳证明。如果 $n = 0, p(x) = a_0$ 是一个常数，那么显然 $p(x)$ 的第1阶差分全为零。令 $n \geqslant 1$ ，又假定对于次数小于 $n$ 的多项式定理成立，即假定对于 $k < n, k$ 次多项式的差分表的第 $k + 1$ 行全由0组成。我们注意到，若删掉 $p(x)$ 的差分表的第0行，便得到 $\Delta p(x)$ 的差分表。于是， $p(x)$ 的差分表的第 $n + 1$ 行与 $\Delta p(x)$ 的差分表的第 $n$ 行相同。现在

$$
\begin{array}{l} \Delta p (x) = p (x + 1) - p (x) \\ = \left[ a _ {n} (x + 1) ^ {n} + a _ {3 - 1} (x + 1) ^ {n - 1} + \dots + a _ {1} (x + 1) + a _ {0} \right] \\ - \left[ a _ {1} x ^ {n} + a _ {n - 1} x ^ {n - 1} + \dots + a _ {1} x + a _ {0} \right] _ {\bullet} \\ \end{array}
$$

利用二项式定理，

$$
\begin{array}{l} a _ {n} (x + 1) ^ {n} - a _ {n} x ^ {n} = a _ {1} \left[ x ^ {n} + \binom {n} {n - 1} x ^ {n - 1} + \dots \right. \\ \left. + \binom {n} {1} x + 1 \right] - a _ {n} x ^ {n} \\ = a _ {n} \left[ \binom {n} {n - 1} x ^ {n - 1} + \dots + \binom {n} {1} x + 1 \right]. \\ \end{array}
$$

由此与上面给出的 $\Delta p(x)$ 表示式，我们断定 $\Delta p(x)$ 至多为 $n - 1$ 次多项式。根据归纳法假设， $\Delta p(x)$ 的差分表的第 $n$ 行全由0组成，因而， $p(x)$ 的差分表的第 $n + 1$ 行全由0组成。于是，根据归纳法这个定理是正确的。

现在考虑两个函数 $p$ 与 $q$ 并由 $f(x) = p(x) + q(x)$ 定义 $f_{\bullet}$ 因为

$$
\begin{array}{l} f (x + 1) - f (x) = [ p (x + 1) + q (x + 1) ] \\ - [ p (x) \div q (x) ] \\ = [ p (x + 1) - p (x) ] \\ + [ q (x + 1) - q (x) ], \\ \end{array}
$$

我们断定 $f$ 的差分表可以由 $p$ 与 $q$ 的差分表对应项相加得到。这时可以说 $f$ 的差分表是 $p$ 与 $q$ 的差分表的和。此外，若 $c$ 与 $d$ 是常数，则由 $g(x) = cp(x) + dq(x)$ 定义的函数 $g$ 的差分表可以由 $p$ 与 $q$ 的差分表的各项分别乘以 $c$ 与 $d$ ，然后把对应项相加得到。例如， $p(x) = x^2 + x + 1$ 的差分表是

![](images/f79bd8e47df324c5fb1e58c3519932aab48aee1c95c054f331f27c8de4dc9c3f.jpg)

而 $g(x) = x^{2} - x - 2$ 的差分表是

$$
\begin{array}{c c c c c c} - 2 & - 2 & 6 & 4 & 1 0 & \dots \\ 0 & 2 & 4 & 6 & \dots \\ 2 & 2 & 2 & 2 & \dots \\ 0 & 0 & \dots \end{array}
$$

由 $f(x) = 2(x^{2} + x + 1) + 3(x^{2} - x - 2) = 5x^{2} - x - 4$ 定义的 $f$ 的差分表可以将第一张表的各项乘以2，第二张表的各项乘以3，然后把对应项相加得到：

$$
\begin{array}{c c c c c c} - 4 & 0 & 1 4 & 3 8 & 7 2 & \dots \\ 4 & 1 4 & 2 4 & 3 4 & \dots \\ 1 0 & 1 0 & 1 0 & \dots \\ 0 & 0 & \dots \end{array}
$$

更一般地，若 $p, q, \dots, r$ 是函数而 $c, d, \dots, e$ 是常数，则由 $f(x) = cp(x) + dq(x) + \dots + er(x)$ 定义的函数 $f$ 的差分表可以从 $p, q, \dots, r$ 的差分表用类似方法求出。

此外，我们注意到函数 $f$ 的差分表可由它的左边沿上的各数确定，即由 $f(0), \Delta f(0), \Delta^2 f(0), \cdots$ 之值确定。例如，考虑差分表

$$
\begin{array}{r l} 2 & a b c d \dots \\ - 1 & r s \quad / \dots \\ 3 & u v \\ 0 & w \quad \dots \\ 0 & \dots \end{array}
$$

因为 $a - 2 = -1$ ，得 $a = 1$ .由于 $r - (-1) = 3$ ， $r = 2$ .同样， $u = 3$ 等等。用这种方法能确定所有的数 $a, r, u, w, \cdots$ ，而它们便是 $f(1), \Delta f(1), \Delta^2 f(1), \Delta^3 f(1), \cdots$ 。用同样方法，能确定 $b, s, v, \cdots$ ，它们就是 $f(2), \Delta f(2), \Delta^2 f(2), \cdots$ 等等。于是，整个差分表由它的左边沿上的数确定。

现在考虑 $n$ 次多项式 $\pmb{p}(\pmb{x})$ ，并设它的差分表的左边沿上是

$c_{0}, c_{1}, \cdots, c_{n}, 0, 0, \cdots.$ 对于 $i = 0, 1, 2, \cdots$ 令 $p_{r}(x)$ 是一个多项式，它的差分表左边沿上为 0，0，…，1，0，0，…，其中 1 出现在此差分表的第 $i$ 行。那么， $c_{0} p_{0}(x) + c_{1} p_{1}(x) + \cdots + c_{n} p_{n}(x)$ 的差分表的左边沿上是 $c_{0}, c_{1}, \cdots, c_{n}, 0, 0, \cdots.$ 于是， $p(x)$ 的差分表的左边沿与 $c_{0} p_{0}(x) + c_{1} p_{1}(x) + \cdots + c_{n} p_{n}(x)$ 的差分表的左边沿相同。由于差分表的左边沿确定了整个差分表，我们断定 $p(x)$ 与 $c_{0} p_{0}(x) + c_{1} p_{1}(x) + \cdots + c_{n} p_{n}(x)$ 有同样的差分表。因而，对所有的 $x$

$$
p (x) = c _ {0} p _ {0} (x) + c _ {1} p _ {1} (x) + \dots + c _ {n} p _ {n} (x).
$$

上面方程提供了 $p(x)$ 的一个由多项式 $p_0(x), p_1(x), \dots, p_n(x)$ 组成的表达式。尚须去定出这些多项式，我们来确定 $p_3(x)$ 。其它的多项式可以用类似方法确定。 $p_3(x)$ 的差分表的左边沿是 0，0，0，1，0，0……所以，用已经叙述过的方法我们得 $p_3(x)$ 的差分表是

$$
\begin{array}{c c c c c c c c} \hline 0 & 0 & 1 & 4 & 1 0 & 2 0 & \dots \\ 0 & 0 & 1 & 3 & 6 & 1 0 & \dots \\ 0 & 1 & 2 & 3 & 4 & \dots \\ 1 & 1 & 1 & 1 & \dots \\ 0 & 0 & 0 & \dots \\ 0 & 0 & \dots \\ 0 & \dots \end{array}
$$

请注意，除一些0外，差分表的头4行与Pascal三角形头4列是相同的。从这个差分表看到 $p_3(x)$ 是3次的且 $p_3(0) = p_3(1) = p_3(2) = 0$ ， $p_3(3) = 1$ 。考虑多项式

$$
q _ {3} (x) = \frac {x (x - 1) (x - 2)}{3 !} = \binom {x} {3},
$$

有 $q_{3}(0) = q_{3}(1) = q_{3}(2) = 0, q_{3}(3) = 1$ ，由这些值可以得到 $q_{3}(x)$ 的差分表的左边沿是 $0, 0, 0, 1, 0, 0, \dots$ ，在1之后

出现的这些0是因为 $q_{3}(x)$ 是3次多项式，因此：

$$
p _ {3} (x) = q _ {3} (x) = \left( \begin{array}{c} x \\ 3 \end{array} \right).
$$

更一般地，我们有

$$
\begin{array}{l} p _ {0} (x) = \left( \begin{array}{c} x \\ 0 \end{array} \right) = 1, \\ p _ {1} (x) = \left( \begin{array}{l} x \\ 1 \end{array} \right) = \frac {x}{1 !}, \\ p _ {2} (x) = \binom {x} {2} = \frac {x (x - 1)}{2 !}, \\ \end{array}
$$

#

$$
p _ {h} (x) = \left( \begin{array}{c} x \\ h \end{array} \right) = \frac {x (x - 1) \cdots (x - k + 1)}{k !},
$$

中

所以，如果 $p(x)$ 是 $n$ 次多项式且其差分表的左边沿等于 $c_{0}, c_{1}, \dots, c_{n}, 0, 0, \dots,$ 则

$$
p (x) = c _ {0} \left( \begin{array}{c} x \\ 0 \end{array} \right) + c _ {1} \left( \begin{array}{c} x \\ 1 \end{array} \right) + \dots + c _ {n} \left( \begin{array}{c} x \\ n \end{array} \right).
$$

例 考虑多项式 $p(x) = x^3 + 2x^2 - 3x + 2$ ，它的差分表是

2 2 12 38 86 …

0 10 26 48 …

10 16 22 ...

6 6 …

0

根据这个差分表的左边沿，我们断定

$$
\begin{array}{l} p (x) = 2 p _ {0} (x) + 0 \cdot p _ {1} (x) + 1 0 p _ {2} (x) + 6 p _ {3} (x) \\ = 2 + 1 0 \binom {x} {2} + 6 \binom {x} {3}. \\ \end{array}
$$

对任意的非负整数 $m$ 与 $k$ ，从公式(4.3.9)可得

$$
p _ {\underline {{\hat {k}}}} (0) + \overline {{p _ {\underline {{\hat {k}}}} (1)}} + \dots + p _ {\underline {{\hat {k}}}} (m)
$$

$$
\begin{array}{l} = \binom {0} {k} + \binom {1} {k} + \dots + \binom {m} {k} = \binom {m + 1} {k + 1} \\ = p _ {i + 1} (m + 1) \\ \end{array}
$$

利用这个公式，我们便对任何多项式 $p(x)$ 有一个计算和式

$$
p (0) + p (1) + \dots + p (m) = \sum_ {t = 0} p (t)
$$

的方法。设 $p(x)$ 是 $n$ 次多项式，求 $c_{0}, c_{1}, \cdots, c_{n}$ ，使得

$$
p (x) = c _ {0} p _ {0} (x) + c _ {1} p _ {1} (x) + \dots + c _ {n} p _ {n} (x).
$$

那么

$$
\begin{array}{l} \sum_ {t = 0} ^ {m} p (t) = \sum_ {t = 0} ^ {m} \left[ c _ {0} p _ {0} (t) + c _ {1} p _ {1} (t) + \dots + c _ {n} p _ {n} (t) \right] \\ = c _ {0} \left(\sum_ {t = 0} ^ {m} p _ {0} (t)\right) + c _ {1} \left(\sum_ {t = 0} ^ {m} p _ {1} (t)\right) \\ + \dots + c _ {n} \left(\sum_ {t = 0} ^ {m} p _ {n} (t)\right) \\ = c _ {0} p _ {1} (m + 1) + c _ {1} p _ {2} (m + 1) \\ + \dots + c _ {n} p _ {n + 1} (m + 1) \\ = c _ {0} \binom {m + 1} {1} + c _ {1} \binom {m + 1} {2} + \dots + c _ {n} \binom {m + 1} {n + 1}. \\ \end{array}
$$

于是，如果 $p(x)$ 是 $n$ 次多项式且其差分表的左边沿等于 $c_{0}, c_{1}, \dots, c_{n}, 0, 0, \dots,$ 则

$$
\sum_ {l = 0} ^ {m} p (t) = c _ {0} \binom {m + 1} {1} + c _ {1} \binom {m + 1} {2}
$$

$$
+ \dots + c _ {n} \left( \begin{array}{c} m + 1 \\ n + 1 \end{array} \right).
$$

例 对正整数 $m$ ，求和 $1^4 + 2^4 + \dots + m^4$ .

令 $p(x) = x^4$ ， $p(x)$ 的差分表为

$$
\begin{array}{c c c c c c} 0 & 1 & 1 5 & 8 1 & 2 5 8 & 6 2 5 \dots \\ & 1 & 1 5 & 6 5 & 1 7 5 & 3 8 9 \dots \\ & & 1 4 & 5 0 & 1 1 0 & 1 9 4 \dots \\ & & & 3 6 & 6 0 & 8 4 \dots \\ & & & 2 4 & 2 4 & \dots \\ & & & 0 & \dots \end{array}
$$

于是 $x^4 = p(x) = 0\left( \begin{array}{l}x\\ 0 \end{array} \right) + 1\left( \begin{array}{l}x\\ 1 \end{array} \right) + 14\left( \begin{array}{l}x\\ 2 \end{array} \right) + 36\left( \begin{array}{l}x\\ 3 \end{array} \right)$ $+24\left(\frac{x}{4}\right)$

因而

$$
\begin{array}{l} 1 ^ {4} + 2 ^ {4} + \dots + m ^ {4} = \sum_ {t = 0} ^ {m} p (t) = 0 \binom {m + 1} {1} + 1 \binom {m + 1} {2} \\ + 1 4 \binom {m + 1} {3} + 3 6 \binom {m + 1} {4} + 2 4 \binom {m + 1} {5}. \\ \end{array}
$$

用类似方法我们对任意正整数 $n$ 与 $m$ 可以求出 $1^{n} + 2^{n} + \dots + m^{n}$ . 先要确定常数 $c(n, t)$ , 这里 $n = 1, 2, 3, \dots$ 与 $t = 0, 1, 2, \dots$ , 使得

$$
x ^ {n} = \sum_ {t = 0} ^ {n} c (n, t) \left( \begin{array}{c} x \\ t \end{array} \right).
$$

再注意到，对所有正整数 $n$ ， $c(n,0) = 0$ ，由下式定义的数 $S(n,t)$

$$
S (n, t) = \frac {c (n , t)}{t !} \quad (n = 1, 2, 3, \dots , t = 0, 1, 2, \dots , n),
$$

称为第二类的Stirling数。由于有了它们，可以把上面关于 $x^n$ 的方程写成

$$
x ^ {n} = \sum_ {t = 0} ^ {n} S (n, t) [ x ] _ {t},
$$

这里 $[x]_0 = 1$ ，且对正整数 $t, [x]_t = x(x - 1) \cdots (x - t + 1)$ 。注意因为对于 $n = 1, 2, 3, \dots, c(n, 0) = 0$ ，故 $S(n, 0) = 0$ 。第二类的Stirling数有许多有趣的性质并且有纯组合解释（见练习23）。它们满足递归关系

$S(n, t) = t S(n - 1, t) + S(n - 1, t - 1) \quad (1 \leqslant t \leqslant n - 1)$ ，这类似于二项式系数的Pascal公式。利用这个递归关系以及对所有正整数 $n$ ， $S(n, 0) = 0$ ， $S(n, n) = 1$ 就可以确定所有的第二类Stirling数。

我们用一个几何问题结束本章。

应用 对于 $n = 0, 1, 2, \dots$ ，令 $h_2(n)$ 表示由 $n$ 条处于一般位置的直线（即每对直线恰好交于一点，但没有三条直线交于一点）把平面划分成区域的数目。对于 $n = 0, 1, 2, \dots$ ，令 $h_3(n)$ 表示由 $n$ 个处于一般位置的平面（每对平面交于一直线，但是没有三个平面交于一直线；每三个平面交于一点，但没有四个平面相交于一点）把空间划分成区域的数目。我们希望得到 $h_2(n)$ 与 $h_3(n)$ 的公式。一个比较简单的问题是确定由 $n$ 个不同的点把直线分成“线段”的个数 $h_1(n)$ 的公式。我们有 $h_1(0) = 1$ ， $h_1(1) = 2$ ， $h_1(2) = 3$ 。一般地， $h_1(n) = n + 1$ 。关于 $h_1(n)$ 的公式可以证明如下。令 $n \geqslant 1$ ，考虑把直线用 $n - 1$ 个点①

划分成 $h_1(n - 1)$ 个“线段”。如果把第 $n$ 个点插入这条直线上，则把现有的“线段”之一划分为两段。因而

$$
h _ {1} (n) = h _ {1} (n - 1) + 1.
$$

利用迭代得到

$$
h _ {1} (n) = h _ {1} (0) + 1 + 1 + \dots + 1 = 1 + n.
$$

#

我们可以推广上面论证方法去确定 $h_2(n)$ 与 $h_3(n)$ 的公式。令 $n \geqslant 1$ ，考虑使用 $n - 1$ 条处于一般位置的直线把一平面划分成 $h_2(n - 1)$ 个区域，在这平面内插入第 $n$ 条直线，使得这 $n$ 条直线是处于一般位置。头 $n - 1$ 条直线与第 $n$ 条直线相交于 $n - 1$ 个不同的点，它们把第 $n$ 条直线划分成 $h_1(n - 1)$ 个“线段”。第 $n$ 条直线的这些段中的每一段都把平面上的原有区域之一分成两个区域（见图6.1， $n = 4$ 的情形）。所以

$$
h _ {2} (n) = h _ {2} (n - 1) + h _ {1} (n - 1)
$$

或

$$
h _ {2} (n) - h _ {2} (n - 1) = h _ {1} (n - 1).
$$

![](images/faa1a649ad1642e774f75af2a344bdaa91d09a4100cc9c34d6c3be5e5557b874.jpg)  
图6.1

上述方程告诉我们 $h_2(n)$ 的差分表可以在 $h_1(n)$ 的差分表的顶上添上其值为 $h_2(0), h_2(1), h_1(2), \cdots$ 的新行得到。现在考虑用 $n - 1$ 个处于一般位置的平面把空间划分成 $h_3(n - 1)$ 个区域，在空间中插入第 $n$ 个平面使得这 $n$ 个平面是处于一般位置。头 $n - 1$ 个平面与第 $n$ 个平面相交于 $n - 1$ 条直线，这些直线在第 $n$ 个平面上处于一般位置。这 $n - 1$ 条直线把第 $n$ 个平面划分成 $h_2(n - 1)$ 个平面区域，这些平面区域的每一个把现有的空间区域之一划分成两个区域。因此

$$
h _ {3} (n) = h _ {3} (n - 1) + h _ {2} (n - 1)
$$

或

$$
h _ {3} (n) - h _ {3} (n - 1) = h _ {2} (n - 1).
$$

这后一个方程告诉我们 $h_3(n)$ 的差分表可以在 $h_2(n)$ 的差分表顶上添上其值为 $h_3(0), h_3(1), h_3(2), \cdots$ 的新行得到。使用公式 $h_1(n) = n + 1$ ，我们便知 $h_2(n)$ 的差分表是

$$
\begin{array}{c c c c c} h _ {3} (0) & h _ {2} (1) & h _ {2} (2) & h _ {2} (3) & h _ {2} (4) \dots \\ 1 & 2 & 3 & 4 & \dots \\ 1 & 1 & 1 & \dots \\ 0 & 0 & 0 & \dots \end{array}
$$

而 $h_3(n)$ 的差分表是

$$
\begin{array}{c c c c c} h _ {3} (0) & h _ {3} (1) & h _ {3} (2) & h _ {3} (3) & h _ {3} (4) \\ k _ {2} (0) & k _ {2} (1) & h _ {2} (2) & k _ {2} (3) & \dots \\ 1 & 2 & 3 & \dots \\ 1 & 1 & & \dots \\ 0 & & \dots \end{array}
$$

由于 $h_2(0) = 1$ ， $h_2(n)$ 的差分表的左边沿上是1，1，1，0，0，…，因此

$$
\hat {h} _ {2} (n) = \binom {n} {0} + \binom {n} {1} + \binom {n} {2}.
$$

又由于 $h_3(0) = 1$ ， $h_3(n)$ 的差分表的左边沿上是1，1，1，1,0，0，…，所以

$$
h _ {3} (n) = \left( \begin{array}{c} n \\ 0 \end{array} \right) + \left( \begin{array}{c} n \\ 1 \end{array} \right) + \left( \begin{array}{c} n \\ 2 \end{array} \right) + \left( \begin{array}{c} n \\ 3 \end{array} \right).
$$

注意 $h_1(n)$ 的公式可以写成

$$
h _ {1} (n) = \left( \begin{array}{c} n \\ 0 \end{array} \right) + \left( \begin{array}{c} n \\ 1 \end{array} \right).
$$

这些结果可以推广到高维空间上。对于 $i \geqslant 1$ ， $h_i(n)$ 表示用 $n$ 个处于一般位置的 $(i-1)$ 维超平面把 $i$ 维空间划分成区域的数目。那么，对 $n \geqslant 1$ ，

$$
h _ {i} (n) - h _ {i} (n - 1) = h _ {i = 1} (n - 1),
$$

并且 $h_i(n)$ 的差分表的左边沿上是1，1，…，1，0，0，…，其中共有 $i + 1$ 个1。因此

$$
h _ {i} (n) = \left( \begin{array}{l} n \\ 0 \end{array} \right) + \left( \begin{array}{l} n \\ 1 \end{array} \right) + \dots + \left( \begin{array}{l} n \\ i \end{array} \right).
$$

# 练习

1. 设 $f(0), f(1), f(2), \ldots$ 是 Fibonacci 序列，对下列每个表达式用较小的 $n$ 值计算几个，猜测其一般公式，然后用数学归纳法证明它。

（a） $f(1) + f(3) + \dots +f(2n - 1).$   
(b） $f(0) + f(2) + \dots +f(2n).$   
（c）f（0）-f（1）+f（2）-+（-1）n f(n).  
（d）f（0）²+（1）²++f（n）²  
（e） $f(n)f(n + 2) + (-1)^{n}$

2. 考虑一个 $1 \times n$ 拼盘。假定我们对棋盘的每个方格用红或兰两个颜色之一去着色，对 $n = 1, 2, 3, \cdots$ ，令 $g(n)$ 表示没有任何两个着红色的方格是相邻的着色的个数。建立并且证明 $g(n)$ 所满足的递归关系。再求 $g(n)$ 的公式。

3. 用红、白和兰三色将 $1 \times n$ 棋盘上的方格着色，对于 $n = 1, 2, 3, \cdots$ ，令 $h(n)$ 等于没有两相邻方格都着红色这种着色的个数，建立并且证明 $h(n)$ 所满足的递归关系。再求 $h(n)$ 的公式。

4. 假设在 Fibonacci 问题中，在一年开始时把 2 对兔子放入围场，一年后围场中有多少对兔子？更一般地，n 个月后围场中有多少对兔子？

5. 设初始值 $H(0) = 0$ 与 $H(1) = 1$ ，试解递归关系 $H(n) = 4H(n - 2)$ （ $n = 2, 3, 4, \cdots$ ）

6. 假设初始值 $H(0) = 2$ ，解递归关系 $H(n) = (n + 2)H(n - 1)$ （ $n = 1, 2, 3, \cdots$ ）

7. 设初始值为 $H(0) = 0$ ， $H(1) = 1$ 和 $H(2) = 2$ ，解递归关系 $H(n) = H(n - 1) + \theta H(n - 2) - \theta H(n - 3) \quad (n = 3, 4, 5, \dots)$ .

8. 设初始值为 $H(0) = -1$ 和 $H(1) = 0$ ，解递归关系 $H(n) = 8H(n - 1) - 16H(n - 2) \quad (n = 2, 3, 4, \cdots)$ .

9. 设初始值为 $H(0) = 1$ ， $H(1) = 0$ ， $H(2) = 0$ ，解递归关系 $H(n) = 3H(n - 2) - 2H(n - 3)$ （ $n = 3, 4, 5, \cdots$ ）.

10. 设初始值为 $H(0) = 0$ ， $H(1) = 1$ ， $H(2) = 1$ 和 $H(3) = 2$ ，解递归关系 $H(n) = 5H(n - 1) - 6H(n - 2) - 4H(n - 3) + 8H(n - 4)$ （ $n = 4, 5, 6, \cdots$ ）

11. 解下列递归关系。先考察开头几个数值，并推测解的公式，然后用归纳法证明所推测的公式。

(a) $H(n) = 3H(n - 1)$ （ $n = 1, 2, 3, \dots$ ）； $H(0) = 1$ .  
(b) $H(n) = H(n - 1) - n + 3$ $(n = 1,2,3,\dots)$ , $H(0) = 2$ .   
(c) $H(n) = -H(n - 1) + 1$ （ $n = 1, 2, 3, \cdots$ )

$$
\begin{array}{l} H (0) = 0. \\ \begin{array}{r} (d) H (n) = - H (n - 1) + 2 \quad (n = 1, 2, 3, \dots); \\ H (0) = 1. \end{array} \\ \begin{array}{r} (e) H (n) = 2 H (n - 1) + 1 \quad (n = 1, 2, 3, \dots), \\ H (0) = 1. \end{array} \\ \end{array}
$$

12*. 假设在圆周上选取 $2\pi$ 个等距离点，对于 $n = 0, 1, 2, \cdots$ ，令 $h(n)$ 表示把这些点成对连接使所得直线段都不相交的连接方式的个数。试建立 $h(n)$ 的递归关系。

13. 设 $p(x) = 2x^2 + x + 3$ ，找出关于 $\sum_{i=0}^{m} p(i)$ 的求和公式。  
14. 设 $p(x) = 2x^3 - 3x^2 + 2x + 1$ ，找出关于 $\sum_{t=0}^{m} p(t)$ 的求和公式：  
15. 3次多项式 $p(x)$ 的差分表第0行头4项是1，-1，3，19.

(a) 确定 $p(x)$ .  
（b）计算p(0)+p(1)+…+p(m).  
16. 找出 $\sum_{t=0}^{m} t^{5}$ 的求和公式.  
17. 证明所有第 $n + 1$ 阶差分都等于0的差分表是某个至多为 $n$ 次的多项式函数的差分表。  
18. 设 $p$ 是对所有实数 $x$ 定义的函数， $k$ 是正整数，用归纳法证明

$$
\Delta^ {k} p (x) = \sum_ {j = 0} ^ {k} (- 1) ^ {j} \binom {k} {k - j} p (x + (k - j)) _ {\bullet}
$$

19. 设 $p(x)$ 是 $n$ 次多项次，证明使得满足

$$
p (x) = c _ {0} \left( \begin{array}{c} x \\ 0 \end{array} \right) + c _ {1} \left( \begin{array}{c} x \\ 1 \end{array} \right) + \dots + c _ {n} \left( \begin{array}{c} x \\ n \end{array} \right)
$$

的常数 $c_{0}, c_{1}, c_{2}, \cdots C_{n}$ 是唯一的.

$20^{\circ}$ ，给定函数 $f$ 的差分表，证明这张表由任何一个来自表中且适合下述条件的数列 $a_0, a_1, a_2, \cdots$ 所确定。 $c_{0} = f(0)$ ，并且对每个 $i = 0, 1, 2, \cdots, a_{i+1}$ 或者是在 $a$ 同一行紧靠 $a$ 的右边或者是在 $a_i$ 的下一行紧靠 $a$ 的右边.  
21. 制作一张第二类Stirling数 $S(n, t)$ （ $n = 1, 2, \cdots, 6$ ）的表.  
22. 证明第二类Stirling数 $S(n, t)$ 满足递归关系

$$
S (n, t) = t S (n - 1, t) + S (n - 1, t - 1) \quad (t = 1, 2, \dots ,
$$

n-1).

23. 设 $P(n, t)$ 等于 $n$ 元集合分成 $t$ 个非空无序子集合的划分的个数。证明 $P(n, t)$ 与第二类Stirling数 $S(n, t)$ 满足相同的递归关系，并且 $P(n, t)$ 与 $S(n, t)$ 有同样的初始值。由此断定 $P(n, t) = S(n, t)$ 。  
24. 设 $T(n, t)$ 等于 $n$ 元集合分成 $t$ 个非空有序子集合的划分的个数（这样，在每种划分下都有第1。第2，…，以及第7个子集合）。

(a) 证明 $T(n, t) = t! S(n, t)$ .

（b）利用 $(a)$ 与 $S(n, t)$ 所满足的递归关系，建立 $T(n, t)$ 所满足的递归关系.  
25.（续练习24）用容斥原理证明

$$
T (n, t) = \sum_ {i = 0} (- 1) ^ {i} \binom {t} {i} (t - i) ^ {n}.
$$

28. 用练习24（a）及练习25，证明

$$
S (n, t) = \frac {1}{t !} \sum_ {i = 0} ^ {t} (- 1) ^ {i} \binom {t} {i} (t - i) ^ {n}.
$$

27. 设 $X$ 是 $n$ 元集合, $Y$ 是 $t$ 元集合, 证明 $X$ 到 $Y$ 上的映射的个数等于 $t! S(n, t)$ .  
28°. 求出并且证明含有第二类Stirling数的 $1^{n} + 2^{n} + \cdots + m^{n}$ 的一般公式.  
29. 对于非负整数 $n$ ，记 $[x]_n = s(n, 0) + s(n, 1)x + s(n, 2)x^2$

+…+s(n，n)x"，用这种方法确定的数s(n，t）（t=0，1，2，…，n），称为第一类Stirling数，证明这些数满足递归关系s(n，t)=s(n-1，t-1)-(n-1)s(n-1，t）（t=1，2，…，n-1），并且对于n=1，2，3，…有初始值s(n，0)=0；对于n=0，1、2，…有初始值s(n,n)=1.

30. 对于 $n = 1, 2, \cdots, 6$ 制作第一类Stirling数的表.

31. 对于 $n = 0, 1, 2, \cdots$ , Bell数或指数数 $B_{n}$ 定义为 $n$ 元集合分成非空子集合的划分的个数. 证明

$$
B _ {n} = S (n, 0) + S (n, 1) + S (n, 2) + \dots + S (n, n).
$$

# 第七章 生成函数

迄今，我们已经知道了解决组合的枚举问题的几种方法。从基本的加法原理和乘法原理开始，导出了排列与组合的各种公式。证明了容斥原理，并且已用它来解决某些枚举问题。在上一章中，讨论了寻找递归关系的重要思想，并且介绍了求解递归关系的某些方法。不过，要找出给定问题中的递归关系并不容易。正象已经指出的那样，即使成功地找到了这种递归关系，要求解也是非常困难的，甚至是不可能的。重要的是我们有许多方法能用来解决枚举问题。本章讨论的最后一个方法是属于Euler的生成函数法。

# §7.1 生成函数

设 $a_0, a_1, a_2, \dots, a_n, \dots$ 是一数列。定义它的生成函数①为幂级数

$$
f (x) = a _ {0} + a _ {1} x ^ {1} + a _ {2} x ^ {2} + \dots + a _ {n} x ^ {n} + \dots .
$$

虽然上面幂级数有收敛问题，但对我们的目的来说，可以把它想象为形式幂级数。这里 $x^0 = 1$ ， $x^1$ ， $x^2$ ，…， $x^n$ ，…解释为代数符号或者是区分序列中不同项 $a_0, a_1, a_2, \dots, a_n$ ，…的形式记号。在适当的时候，我们将对幂级数的收敛作某些说明，但不证明。由于有限序列 $a_0, a_1, a_2, \dots, a_n$ 可以看成无限序列 $a_0, a_1, a_2, \dots, a_{n+1}, a_{n+2}, \dots,$ 其中 $a_{n+1} = 0$ ， $a_{n+2} = 0$ ，…。因此，上面的定义把生成函数和每一个有限序

列也联系起来了。

例 令 $m$ 是正整数，二项式系数序列

$$
\left( \begin{array}{c} m \\ 0 \end{array} \right), \quad \left( \begin{array}{c} m \\ 1 \end{array} \right), \quad \left( \begin{array}{c} m \\ 2 \end{array} \right), \dots , \quad \left( \begin{array}{c} m \\ m \end{array} \right)
$$

的生成函数 $f(x) = \binom{m}{0} + \binom{m}{1}x + \binom{m}{2}x^2 + \cdots$

+（m）x"。根据二项式定理，f（x）=（1+x）m，更一般地，令α是实数，二项式系数序列

$$
\left( \begin{array}{c} \alpha \\ 0 \end{array} \right), \quad \left( \begin{array}{c} \alpha \\ 1 \end{array} \right), \quad \left( \begin{array}{c} \alpha \\ 2 \end{array} \right), \dots \left( \begin{array}{c} \alpha \\ n \end{array} \right), \dots
$$

的生成函数 $f_{\alpha}(x)$ 为

$$
\begin{array}{l} f _ {\alpha} (x) = \left( \begin{array}{l} \alpha \\ 0 \end{array} \right) + \left( \begin{array}{l} \alpha \\ 1 \end{array} \right) x + \left( \begin{array}{l} \alpha \\ 2 \end{array} \right) x ^ {2} + \dots \\ + \left( \begin{array}{c} \alpha \\ n \end{array} \right) x ^ {n} + \dots . \\ \end{array}
$$

根据Newton二项式定理

$$
f _ {\alpha} (x) = (1 + x) ^ {\alpha}.
$$

例求数列 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 的生成函数 $g_{k}(x)$ 其中 $a_{2}$ 是 $k > 0$ 个不同元素的多重集合的 $n$ 组合个数，而每个元素都有无限重复数。在3.5节中我们看到

$$
a _ {n} = \left( \begin{array}{c} k + n - 1 \\ n \end{array} \right).
$$

于是

$$
\begin{array}{l} g _ {k} (x) = \binom {k - 1} {0} + \binom {k} {1} x + \binom {k + 1} {2} x ^ {2} + \dots \\ + \binom {k + n - 1} {n} x ^ {r} + \dots , \\ \end{array}
$$

利用Newton二项式定理，便知

$$
g _ {k} (x) = \frac {1}{(1 - x) ^ {k}} = (1 - x) ^ {- k}.
$$

作为特殊情形，序列1，1，1，…，1，1，…的生成函数是

$$
g _ {1} (x) = \frac {1}{1 - x},
$$

而序列 $1, 2, 3, \dots, n,$ …的生成函数是

$$
g _ {2} (x) = \frac {1}{(1 - x) ^ {2}}.
$$

虽然在前面一些例子中，对明确地给定的数列我们找到这个数列生成函数的闭公式。但是，生成函数在枚举问题中的用途通常在于反演这些步骤。例如利用递归关系定义数列 $a_{0}, a_{1}, \cdots, a_{n}, \cdots$ 或简单地说， $a_{n}$ 表示计数问题的所求解。无须清楚地知道序列 $a_{0}, a_{1}, \cdots, a_{n}, \cdots$ 的各项，就可设法求出它的生成函数 $f(x)$ 的闭公式。把 $f(x)$ 展成幂级数，便能够得到序列各项的值。如果我们是幸运的话，某个模式将出现，然后可得到通项 $a_{n}$ 的公式。

例令 $S$ 是多重集合 $\{\infty \cdot e_1,\infty \cdot e_2,\dots ,\infty \cdot e_k\}$ ， $a_{n}$ 是 $S$ 的 $n$ 组合个数，其中 $s$ 的每个不同元素出现偶数次。试求数列 $a_0,a_1,a_2,\dots ,a_n,\dots$ 的生成函数 $f_{n}(x)$ 。（由这个序列项的定义易知 $0 = a_{1} = a_{3} = a_{5} = \dots .a_{n}$ 的一个等价定义是： $a_{n}$ 是方程 $z_{1} + z_{2} + \dots +z_{k} = n$ 的非负、偶整数 $z_{1},z_{2},\dots ,z_{k}$ 的解的个数。）

考虑幂级数 $1 + x^{2} + x^{4} + x^{8} + \dots$ 及其 $k$ 重乘积

$$
\begin{array}{l} \left(1 + x ^ {2} + x ^ {4} + x ^ {6} + \dots\right) \left(1 + x ^ {2} + x ^ {4} + x ^ {6} + \dots\right) \dots \\ \left(1 + x ^ {2} + x ^ {4} + x ^ {6} + \dots\right) \tag {7.1.1} \\ \end{array}
$$

当作完乘法时，上式中出现形如

$$
x ^ {2} 1 x ^ {2} 2 \dots x ^ {2 k} = x ^ {1} + ^ {+} 2 + \dots + 2 k
$$

的项，这里 $x^{2}$ 是第一个幂级数的项， $x^{2}$ 是第二个幂级数的

项，等等。特别地， $z_{1}, z_{2}, \cdots, z_{k}$ 是非负偶整数。于是，作完乘法后的表达式产生另一个幂级数，其中 $x^{n}$ 的系数是 $z_{1} + z_{2} + \cdots + z_{k} = n$ 的非负偶整数 $z_{1}, z_{2}, \cdots, z_{k}$ 的解的个数。

这样，在（7.1.1）中 $x^{n}$ 的系数是 $a_{n}$ ，并且（7.1.1）是 $a_{0}, a_{1}, \cdots, a_{n}, \cdots$ 的生成函数 $f_{k}(x)$ 。因为

$$
1 + x ^ {2} + x ^ {4} + x ^ {6} + \dots = \frac {1}{1 - x ^ {2}},
$$

我们便知

$$
\begin{array}{l} f _ {k} (x) \approx \frac {1}{(1 - x ^ {2}) ^ {\frac {1}{k}}} = 1 + k x ^ {2} + \frac {k (k + 1)}{2} x ^ {4} \\ + \frac {k (k + 1) (k + 2)}{6} x ^ {0} + \dots . \\ \end{array}
$$

从出现的形式使我们相信：对于 $n = 0$ ，1，2，…

$$
a _ {2 n} = \frac {k (k + 1) (k + 2) \cdots (k + n - 1)}{n !} = \binom {k + n - 1} {n} \tag {7.1.2}
$$

是一个正确公式。现在可以利用更直接的方法证明。依据一对一方式，可以认为每个 $e_i$ 出现偶数次的 $S = \{\infty \cdot e_1, \infty \cdot e_2, \dots, \infty \cdot e_k\}$ 的 $2n$ 组合，与 $S$ 的 $n$ 组合相同，如下所述。给定一个 $2n$ 组合 $\{m_1 \cdot e_1, m_2 \cdot e_2, \dots, m_k \cdot e_k\}$ ，其中 $m_1, m_2, \dots, m_k$ 为偶数。我们把它与 $n$ 组合 $\{(m_1 / 2) \cdot e_1, (m_2 / 2) \cdot e_2, \dots, (m_k / 2) \cdot e_k\}$ 联系起来。反之，给定 $n$ 组合 $\{p_1 \cdot e_1, p_2 \cdot e_2, \dots, p_k \cdot e_k\}$ ，把它与 $2n$ 组合 $\{2p_1 \cdot e_1, 2p_2 \cdot e_2, \dots, 2p_k \cdot e_k\}$ 联系起来，其中每个 $e_i$ 出现偶数次。这便确立了（7.1.2）的正确性。

利用类似求解上例的论证，可以建立与组合问题有关的各种序列的生成函数。例如：

$$
\left(1 + x + x ^ {2} + x ^ {3} + \dots\right) \left(1 + x ^ {2} + x ^ {4} + x ^ {3} + \dots\right) \times
$$

$$
\begin{array}{l} \times (1 + x ^ {3} + x ^ {8} + x ^ {9} + \dots) \\ = \left(\frac {1}{1 - x ^ {-}}\right) \left(\frac {1}{1 - x ^ {2}}\right) \left(\frac {1}{1 - x ^ {3}}\right) \tag {7.1.3} \\ \end{array}
$$

是序列 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 的生成函数，这里 $a_{n}$ 是多重集合 $S = \{\infty \cdot e_{1}, \infty \cdot e_{2}, \infty \cdot e_{3}\}$ 的 $n$ 组合个数，而其中 $e_{2}$ 出现偶数次， $e_{3}$ 出现 3 的倍数次。

$$
\begin{array}{l} \left(1 + x + x ^ {2} + \dots + x ^ {r} 2\right) \left(1 + x + x ^ {2} + \dots + x ^ {r} 2\right) \dots \\ \times (1 + x + x ^ {2} + \dots + x ^ {r _ {k}}) \tag {7.1.4} \\ \end{array}
$$

是序列 $a_{0}, a_{1}, a_{2}, a_{3}, \cdots, a_{n}, \cdots$ 的生成函数，这里 $a_{n}$ 是多重集合 $S = \{e_{1}, e_{2}, e_{3}, \cdots, e_{k}\}$ 的 $k$ 组合个数。在第五章我们已看到怎样利用容斥原理计算 $a_{n}$ 。

$$
\frac {x ^ {k}}{\left(1 - x ^ {2}\right) ^ {k}} \tag {7.1.5}
$$

是序列 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 的生成函数，其中 $a_{n}$ 是多重集合 $S = \{\infty * e_{1}, \infty * e_{2}, \cdots, \infty * e_{k}\}$ 的 $n$ 组合个数，而每个 $e_{i}$ 出现奇数次。请注意

$$
a _ {0} = a _ {i} = \dots = a _ {k - 1} = 0.
$$

# 7.2 线性递归关系

本节说明怎样利用生成函数求解常系数线性齐次递归关系。这将提供解6.2和6.3节中给出的递归关系的另一种方法。Newton二项式定理（定理4.6.1）在这个方法中起着重要作用。特别地，要使用Newton二项式定理的下述情形：如果 $n$ 是正整数和 $r$ 是非零实数，那么对于 $|x| < \frac{1}{|r|}$ ，有

$$
(1 - r x) ^ {- n} = (- r x + 1) ^ {- n} = \sum_ {k = 0} ^ {\infty} \binom {- n} {k} (- r x) ^ {k},
$$

或 $\frac{1}{(1 - rx)^n} = \sum_{k=0}^{\infty} (-1)^k \binom{n}{k} r^k x^k.$

正如在4.6节看到的那样，

$$
\binom {- n} {k} = (- 1) ^ {k} \binom {n + k - 1} {k},
$$

我们可以把上面公式记为

$\frac{1}{(1 - rx)^n} = \sum_{h=0}^{\infty}\binom{n+k-1}{k}r^k x^k,$ (7.2.1)

$$
| x | <   \frac {1}{| r |}.
$$

例 确定平方序列 $0, 1, 4, \dots, n^{2}, \dots$ 的生成函数。利用（7.2.1）及 $n = 2, r = 1$ ，

$$
\frac {1}{(1 - x) ^ {2}} = 1 + 2 x + 3 x ^ {2} + \dots + n x ^ {n - 1} + \dots ,
$$

因此

$$
\frac {x}{(1 - x) ^ {2}} = x + 2 x ^ {2} + 3 x ^ {3} + \dots + n x ^ {n} + \dots .
$$

微分，我们得到

$$
\frac {1 + x}{(1 - x) ^ {3}} = 1 + 2 ^ {2} x + 3 ^ {2} x ^ {2} + \dots + n ^ {2} x ^ {n - 1} + \dots .
$$

乘以 $x$ ，得到

$$
\frac {x (1 + x)}{(1 - x) ^ {3}} = x + 2 ^ {2} x ^ {2} + 3 ^ {2} x ^ {3} + \dots + n ^ {2} x ^ {n} + \dots .
$$

所以， $x(1 + x) / (1 - x)^{2}$ 是我们要求的生成函数。

例 对于 $n = 2, 3, 4, \cdots$ ，附有初始值为 $H(0) = 1$ ， $H(1) = -2$ ，解递归关系 $H(n) = 5H(n-1) - 6H(n-2)$ ，为了简化记号，在处理生成函数时，令 $a_0 = H(0)$ ， $a_1 = H(1)$ ， $a_2 = H(2)$ ，等等。这样，上面的递归关系成为

$-6a_{n-2}$ 或 $a_{n} - 5a_{n-1} + 6a_{n-2} = 0$ ，对于 $n = 2, 3, 4, \cdots$ 及其 $a_{0} = 1$ ， $a_{1} = -2$ 。

令 $f(x) = a_{0} + a_{1}x + a_{2}x^{2} + \dots +a_{n}x^{n} + \dots$ 是 $a_0,a_1,\dots ,$ $a_{n},$ …的生成函数，那么有下列方程：

$$
\begin{array}{l} f (x) = a _ {0} + a _ {1} x + a _ {2} x ^ {2} + \dots + a _ {n} x ^ {n} + \dots , \\ - 5 x f (x) = - 5 a _ {0} x - 5 a _ {1} x ^ {2} - 5 a _ {2} x ^ {3} - \dots - 5 a _ {n - 1} x ^ {n} - \dots , \\ 6 x ^ {2} f (x) = 6 a _ {0} x ^ {2} + \dots + 6 a _ {n - 2} x ^ {n} + \dots . \\ \end{array}
$$

这三个方程相加，得到

$$
\begin{array}{l} (1 - 5 x + 6 x ^ {2}) f (x) = a _ {0} + \left(a _ {1} - 5 a _ {0}\right) x ^ {+} \left(a _ {2} - 5 a _ {1} + 6 a _ {0}\right) x ^ {2} \\ + \dots + (a _ {n} - 5 a _ {n - 1} + 6 a _ {n - 2}) x ^ {n} + \dots \\ \end{array}
$$

因为对于 $n = 2, 3, 4, \cdots$ ，有 $a_{n} - 5a_{n-1} + 6a_{n-2} = 0$ 以及 $a_{0} = 1, a_{1} = -2$ ，我们得到

$$
(1 - 5 x + 6 x ^ {2}) f (x) = a _ {0} + \left(a _ {1} - 5 a _ {0}\right) x = 1 - 7 x.
$$

所以 $f(x) = \frac{1 - 7x}{1 - 5x + 6x^2}.$

我们想从生成函数 $f(x)$ 的这个闭公式确定序列 $a_0, a_1, a_2, \dots, a_n, \dots$ 的各项。为此，一种方法是使用 $1 - 7x$ 除以 $1 - 5x + 6x^2$ 的如下长除法：

$$
\begin{array}{l} 1 - 5 x + 6 x ^ {2} \frac {1 - 2 x - 1 6 x ^ {2} - 6 8 x ^ {3} - \cdots}{1 - 7 x} \\ \frac {1 - 5 x + 6 x ^ {2}}{- 2 x - 6 x ^ {2}} \\ \frac {- 2 x + 1 0 x ^ {2} - 1 2 x ^ {3}}{- 1 6 x ^ {2} + 1 2 x ^ {3}} \\ \frac {- 1 6 x ^ {2} + 8 0 x ^ {3} - 9 6 x ^ {4}}{- 6 8 x ^ {3} + 9 6 x ^ {4}} \\ \frac {- 6 8 x ^ {3} + 3 4 0 x ^ {4} - 4 0 8 x ^ {5}}{- 2 4 4 x ^ {4} + 4 0 8 x ^ {5}} \\ \end{array}
$$

#

因而 $a_0 = 1$ ， $a_1 = -2$ ， $a_2 = -16$ ， $a_3 = -68$ ，…，但是，这个方法不能产生序列的通项公式。为了得到这样的公式，我们可以用部分分式以及Newton二项式定理。注意到 $1 - 5x + 6x^2 = (1 - 2x)(1 - 3x)$ ，于是可写

$$
\frac {1 - 7 x}{1 - 5 x + 6 x ^ {2}} = \frac {A}{1 - 2 x} + \frac {B}{1 - 3 x}
$$

这里， $A$ 与 $\pmb{B}$ 为待定常数。用 $(1 - 5x + 6x^{2})$ 乘方程的两端，可以确定 $A$ 与 $\pmb{B}$ ，其办法如下：

$$
1 - 7 x = (1 - 3 x) A + (1 - 2 x) B
$$

或 $1 - 7x = (A + B) + (-3A - 2B)x,$

因而

$$
\begin{array}{l} A + B = 1, \\ - 3 A - 2 B = - 7. \\ \end{array}
$$

联立解这两个方程，得到 $A = 5$ 和 $B = -4$ 。

因此

$$
f (x) = \frac {1 - 7 x}{1 - 5 x + 6 x ^ {2}} = \frac {5}{1 - 2 x} - \frac {4}{1 - 3 x}.
$$

由（7.2.1）

$$
\frac {1}{1 - 2 x} = 1 + 2 x + 2 ^ {2} x ^ {2} + \dots + 2 ^ {n} x ^ {n} + \dots
$$

以及 $\frac{1}{1 - 3x} = 1 + 3x + 3^2 x^2 +\dots +3^n x^n +\dots .$

从而 $f(x) = 5(1 + 2x + 2^2 x^2 + \dots + 2^n x^n + \dots) - 4(1 + 3x)$

$$
\begin{array}{l} + 3 ^ {2} x ^ {2} + \dots + 3 ^ {n} x ^ {n} + \dots) \\ = 1 + (- 2) x + (- 1 6) x ^ {2} + \dots + (5 \times 2 ^ {n} - 4 \times 3 ^ {n}) x ^ {4} \\ + \dots_ {q} \\ \end{array}
$$

因为这是 $a_{0}, a_{1}, \dots, a_{n}, \dots$ 的生成函数，我们断定

$$
a _ {n} = 5 \times 2 ^ {n} - 4 \times 3 ^ {n}, \text {对 于} n = 0, 1, 2, \dots ,
$$

推广上例中的方法，能在理论上解任何常系数 $k$ 阶线性齐次递归关系。与之相连的生成函数具有形式 $\frac{p(x)}{q(x)}$ ，其中 $\frac{p(x)}{q(x)}$ 为低于 $k$ 次的多项式， $q(x)$ 是 $k$ 次多项式且常数项为1。为了求出序列的通项公式，首先使用部分分式方法，把 $\frac{p(x)}{q(x)}$ 表示为形如

$$
\frac {A}{(1 - r x) ^ {t}}
$$

的代数式之和，此处 $t$ 为正整数， $r$ 是实数， $A$ 是常数。然后用（7.2.1）去求 $1 / (1 - rx)^{i}$ 的幂级数。合并同类项，得到生成函数的幂级数，由此能够读出序列的各项。

例 令 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 是满足对于 $n = 3, 4, 5 \cdots$ 附有初始值 $a_{0} = 0, a_{1} = 1, a_{2} = -1$ 的递归关系: $a_{n} + a_{n-1} - 16a_{n-2} + 20a_{n-3} = 0$ 的数列, 试求该序列的通项公式.

令 $f(x) = a_0 + a_1x + a_2x^2 + \dots + a_nx^n + \dots$ 是 $a_0, a_1, a_2, \dots, a_n, \dots$ 的生成函数，把下面四个方程相加.

$$
\begin{array}{l} f (x) = a _ {0} + a _ {1} x + a _ {2} x ^ {2} + a _ {3} x ^ {3} + \dots + a _ {n} x ^ {n} + \dots , \\ x f (x) = a _ {0} x + a _ {1} x ^ {2} + a _ {2} x ^ {3} + \dots + a _ {n + 1} x ^ {n} + \dots , \\ - 1 6 x ^ {2} f (x) = - 1 6 a _ {0} x ^ {2} - 1 6 a _ {1} x ^ {3} - \dots - 1 6 a _ {4 - 2} x ^ {n} \dots , \\ 2 0 x ^ {3} f (x) = 2 0 a _ {0} x ^ {3} + \dots + 2 0 a _ {\pi - 3} x ^ {n} + \dots , \\ \end{array}
$$

得到

$$
\begin{array}{l} (1 + x - 1 6 x ^ {2} + 2 0 x ^ {3}) f (x) = a _ {0} + \left(a _ {1} + a _ {0}\right) x + \left(a _ {2} + a _ {1} \right. \\ - 1 6 a _ {0}) x ^ {2} + \left(a _ {8} + a _ {2} - 1 6 a _ {1} + 2 0 a _ {0}\right) \\ \times x ^ {3} + \dots + (a _ {n} + a _ {n - 1} - 1 6 a _ {n - 2} + \\ \end{array}
$$

$$
+ 2 0 a _ {n - 3}) x ^ {n} + \dots
$$

因为对于 $n = 3,4,5,\dots$ ，有 $a_{n} + a_{n - 1} - 16a_{n - 2} + 20a_{n - 3} = 0$ 及 $a_0 = 0,a_1 = 1,a_2 = -1$ ，上式化简为

$$
(1 + x - 1 6 x ^ {2} + 2 0 x ^ {3}) f (x) = x _ {\bullet}
$$

于是 $f(x) = \frac{x}{1 + x - 16x^2 + 20x^3}.$

注意到 $(1 + x - 16x^{2} + 20x^{3}) = (1 - 2x)^{2}(1 + 5x)$ ，因而对某些常数 $A$ ， $B$ 和 $C$

$$
\frac {x}{1 + x - 1 6 x ^ {2} + 2 0 x ^ {3}} = \frac {A}{1 - 2 x} + \frac {B}{(1 - 2 x) ^ {2}} + \frac {C}{1 + 5 x}.
$$

为了确定 $A, B$ 和 $C$ , 用 $(1 + x - 16x^{2} + 20x^{3})$ 乘这个方程两端, 得到

$$
x = (1 - 2 x) (1 + 5 x) A + (1 + 5 x) B + (1 - 2 x) ^ {2} C
$$

或 $x = (A + B + C) + (3A + 5B - 4C)x + (-10A + 4C)x^2.$

因而

$$
\begin{array}{l} A + B + C = 0, \\ 3 A + 5 B - 4 C = 1, \\ - 1 0 A + 4 C = 0. \\ \end{array}
$$

联立解这些方程，我们得到 $A = -\frac{2}{49}, B = \frac{7}{49}, C = -\frac{5}{49}$ 于是

$$
\begin{array}{l} f (x) = \frac {x}{1 + x - 1 6 x ^ {2} + 2 0 x ^ {3}} = - \frac {2 / 4 9}{1 - 2 x} + \frac {7 / 4 9}{(1 - 2 x) ^ {2}} \\ - \frac {5 / 4 9}{1 + 5 x}. \\ \end{array}
$$

由（7.2.1） $\frac{1}{1 - 2x} = \sum_{k=0}^{\infty} 2^{k} x^{k},$

$$
\frac {1}{(1 - 2 x) ^ {2}} = \sum_ {k = 0} ^ {\infty} \binom {k + 1} {k} 2 ^ {k} x ^ {k} = \sum_ {k = 0} ^ {\infty} (k + 1) 2 ^ {k} x ^ {k},
$$

$$
\frac {1}{1 + 5 x} = \sum_ {k = 0} ^ {\infty} (- 5) ^ {k} x ^ {k}.
$$

所以

$$
\begin{array}{l} f (x) = - \frac {2}{4 9} \cdot \left(\sum_ {k = 0} ^ {\infty} 2 ^ {k} x ^ {2}\right) \div \frac {7}{4 9} \left(\sum_ {k = 0} ^ {\infty} (k + 1) 2 ^ {k} x ^ {k}\right) \\ - \frac {5}{4 9} \left(\sum_ {k = 0} ^ {\infty} (- 5) ^ {k} x ^ {k}\right) = \sum_ {k = 0} ^ {\infty} \left[ - \frac {2}{4 9} 2 ^ {k} + \frac {7}{4 9} (k + 1) 2 ^ {k} - \frac {5}{4 9} \right. \\ \left. \times (- 5) k \right] x k _ {*} \\ \end{array}
$$

因为这是 $a_0, a_1, a_2, \dots, a_n, \dots$ 的生成函数，便有

$$
a _ {n} = - \frac {2}{4 9} 2 ^ {n} + \frac {7}{4 9} (n + 1) 2 ^ {n} - \frac {5}{4 9} (- 5) ^ {n}, \text {对 于} n = 0, 1, 2, \dots .
$$

上面 $a_{n}$ 的公式会使我们想起6.2和6.3节所叙述过的用特征方程的根求解递归关系。的确，上面公式提供了给定递归关系的特征方程的根为 $+2$ ， $+2$ ，和-5，下面的讨论将阐明这两种方法之间的关系。在前面一个例子中，已把这生成函数表示成形式

$$
f (x) = \frac {p (x)}{q (x)}.
$$

这里， $q(x) = 1 + x - 16x^{2} + 20x^{3}$ ，因为对于 $n = 3,4,5,\dots$ ，递归关系是 $a_{n} + a_{n - 1} - 16a_{n - 2} + 20a_{n - 3} = 0$ ，相应的特征方程是 $r(x) = 0$ ，这里 $r(x) = x^3 +x^2 -16x + 20$ ，若用 $\frac{1}{x}$ 代替 $r(x)$ 中的 $x$ （这相当于作变量置换 $y = \frac{1}{x}$ ），得到

$$
r (1 / x) = \frac {1}{x ^ {3}} + \frac {1}{x ^ {2}} - 1 6 \frac {1}{x} + 2 0
$$

或

$$
x ^ {3} r (1 / x) = 1 + x - 1 6 x ^ {2} + 2 0 x ^ {3} = q (x).
$$

特征方程 $r(x) = 0$ 的根是2,2，和-5，因为 $r(x) = (x - 2)^{2}(x + 5)$ ，由此可见

$$
q (x) = x ^ {3} \left(\frac {1}{x} - 2\right) ^ {2} \left(\frac {1}{x} + 5\right) = (1 - 2 x) ^ {2} (1 + 5 x),
$$

这正是所要检验的.

以上关系一般成立。设数列 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 是由 $k$ 阶递归关系 $a_{n} + c_{1}a_{n-1} + \cdots + c_{k}a_{n-k} = 0$ 所确定，其中 $n = k, k+1, \cdots$ ，初始值为 $a_{0}, a_{1}, a_{2}, \cdots, a_{k-1}$ 。由于递归关系是 $k$ 阶的，可假定 $c_{k}$ 不为零，令 $f(x)$ 是 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 的生成函数。利用例子中给定的方法，可以求得多项式 $p(x)$ 和 $q(x)$ ，使得

$$
\boldsymbol {f} (\boldsymbol {x}) = \frac {\boldsymbol {p} (\boldsymbol {x})}{q (\boldsymbol {x})},
$$

这里 $q(x)$ 是 $k$ 次多项式，而 $p(x)$ 是低于 $k$ 次的多项式。实际上

而

$$
q (x) = 1 + c _ {1} x + c _ {2} x ^ {2} + \dots + c _ {k} x ^ {k},
$$

$$
\begin{array}{l} p (x) = a _ {0} + \left(a _ {1} + c _ {1} a _ {0}\right) x + \left(a _ {2} + c _ {1} a _ {1} + c _ {2} a _ {0}\right) x ^ {2} + \dots + \\ + \left(a _ {k - 1} + c _ {1} a _ {k - 2} + \dots + c _ {k - 1} a _ {0}\right) x ^ {k - 1}. \\ \end{array}
$$

与此递归关系相联系的特征方程为 $r(x) = 0$ ，其中

$$
r (x) = x ^ {k} + c _ {1} x ^ {k - 1} + \dots + c _ {k}.
$$

因而 $q(x) = x^{k}r(1 / x).$

于是，若 $r(x) = 0$ 的根为 $q_{1}, q_{2}, \dots, q_{k}$ ，则

$$
\boldsymbol {r} (\boldsymbol {x}) = (x - q _ {1}) (x - q _ {2}) \dots (x - q _ {k})
$$

且

$$
q (x) = (1 - q _ {1} x) (1 - q _ {2} x) \dots (1 - q _ {k} x).
$$

反之，若已知 $k$ 次多项式 $q(x) = b_{0} + b_{1}x + \dots +b_{k}x^{k}$ 且 $b_{0}$ $\neq 0$ ，以及次数低于 $k$ 的多项式 $p(x) = d_0 + d_1x + \dots +d_{k - 1}$ $\cdot x^{k - 1}$ ，那么利用部分分式和Newton二项式定理，找到幂级数 $a_0 + a_1x^1 +\dots +a_nx^n +\dots$ ，使得

$$
\frac {p (x)}{q (x)} = a _ {0} + a _ {1} x + \dots + a _ {n} x ^ {n} \dots .
$$

[这个幂级数对所有 $|x| < r$ 的 $x$ 收敛于 $p(x) / q(x)$ ，其中 $r$ 是 $q(x) = 0$ 的根的最小绝对值，因为 $b_0 \neq 0$ ，且 $0$ 不是 $q(x) = 0$ 的根。] 可以把上面的方程写成下面形式：

$$
\begin{array}{l} d _ {0} + d _ {1} x + \dots + d _ {k - 1} x ^ {k - 1} = (b _ {0} + b _ {1} x + \dots + b _ {k} x ^ {k}) (a _ {0} + a _ {1} x \\ + \dots + a _ {n} x ^ {n} + \dots) _ {\bullet} \end{array}
$$

作完右端乘法并比较系数，我们得到

$$
\boldsymbol {b} _ {\theta} \boldsymbol {a} _ {\theta} = \boldsymbol {d} _ {\theta},
$$

$$
b _ {0} a _ {1} + b _ {1} a _ {0} = d _ {i}, \tag {7.2.2}
$$

··

$$
\boldsymbol {b} _ {0} \boldsymbol {a} _ {k - 1} + \boldsymbol {b} _ {1} \boldsymbol {a} _ {k - 2} + \dots + \boldsymbol {b} _ {k - 1} \boldsymbol {a} _ {0} = d _ {k - 1}
$$

以及

$$
b _ {0} a _ {n} + b _ {1} a _ {n - 1} + \dots + b _ {k} a _ {n - k} = 0, \text {当} n = k, k + 1, \dots \text {时}. \tag {7.2.3}
$$

由于 $b_{0} \neq 0$ ，方程（7.2.3）可以写成下面形式

$a_{n} + \frac{b_{1}}{b_{0}} a_{n - 1} + \dots +\frac{b_{k}}{b_{0}} a_{n - k} = 0$ ，对于 $n = k,k + 1,\dots ,$ 这是 $a_0,a_1,a_2,\dots ,a_n,\dots$ 所满足的常系数线性齐次递归关系。初始值 $a_0,a_1,\dots a_{k - 1}$ 可由方程(7.2.2)解出，现在把我们讨论过的内容总结如下：

定理7.2.1 令 $a_0, a_1, a_2, \dots, a_{n+1}$ 是满足 $k$ 阶常系数线性齐次递归关系

$a_{3} + c_{1}a_{n - 1} + \dots +c_{k}a_{n - k} = 0$ ， $c_{k}\neq 0,n = k,k + 1,\dots (7,2,4)$ 的数列，则 $a_0,a_1,a_2,\dots ,a,\dots$ 的生成函数是形如

$$
\frac {p (x)}{q (x)}, \tag {7.2.5}
$$

这里 $q(x)$ 是具有非零常数项的 $k$ 次多项式和 $p(x)$ 是低于 $k$ 次的多项式。反之，给定多项式 $p(x)$ 和 $q(x)$ ，则存在满足形如(7.2.4)阶常系数线性齐次递归关系的数列 $a_0, a_1, a_2, \dots, a_n, \dots$ ，其生成函数由(7.2.5)给定。

# 7.3 一个几何学的例子

设 $K$ 为平面或空间中点的集合，只要连结 $K$ 中的任何两点 $\pmb{p}$ 与 $\pmb{q}$ 的直线上的所有点都在 $K$ 内，则称 $K$ 为凸集。平面内的三角形区域、圆形区域和矩形区域都是点的凸集。另一方面，图7.1中的区域不是凸的，因为对于已标出的两点 $\pmb{p}$ 与 $\pmb{q}$ ，连接 $\pmb{p}$ 和 $\pmb{q}$ 的直线段要通过这区域的外面。

这样的区域属于多角区域，即它们的边界是由有限条直线段所组成，这些直线段称为区域的边。三角形区域和矩形区域都是多角形的，但圆形区域则不是。任何多角形区域至少必须有三条边。图7.2中的区域是有七条边的凸多角形区域。

![](images/229cf51d92fd6123223923a9b9e9094d73cab815485c9388d689ee4f76ee7310.jpg)  
图7.1

![](images/63d91fb0834d0ca699bd32be8196fe512b67c75d4bb92de01e20c41c02fa5d35.jpg)  
图7.2

在多角形区域中，那些边的交点叫做顶点。连接两个不相邻顶点的直线段称为对角线。

给定一个 $n$ 边多角形区域，那么它的对角线的数目可以如下计算。每个顶点由对角线连接其他 $n - 3$ 个顶点，于是，计算每个顶点的对角线数目，再求和，得到 $n(n - 3)$ 。因为每条对角线有两个顶点，每条对角线在这个和中算过两次。因而对角线的数目应为 $n(n - 3) / 2$ ，还可以用下面的方法间接地得到这个数目。我们知道，连接 $n$ 个顶点的直线段共有

$$
\binom {n} {2} = \frac {n (n - 1)}{2}
$$

条，其中有 $n$ 条是多角形的边，剩下是对角线。所以有

$$
\frac {n (n - 1)}{2} - n = \frac {n (n - 3)}{2}.
$$

条对角线. 对于凸多角形区域, 每条对角线都位于区域的内部. 于是, 有 $n$ 条边的凸多角形区域的每条对角线把该凸多角形区域分成有 $k$ 条边的与 $n - k + 2$ 条边的两个凸多角形区域,

其中 $k = 3,4,\dots ,n - 1$

给定 $n$ 边凸多角形区域，可由一顶点引出 $n - 3$ 条对角线，把区域分成 $n - 2$ 个三角形区域，还有其它的一些方法，在区域内画出 $n - 3$ 条不相交的对角线把区域分成三角形区域，如在图7.3中给出 $n = 8$ 的一个例子。

利用下面定理，我们可以确定有多少种不同方法画

![](images/cbfce82de3aa8dd0514215344f1a0280aff36c8f59a4df20f4fe1a189ac41ced.jpg)  
图7.3

出 $n - 3$ 条在区域内不相交的对角线把 $n$ 边凸多角形区域分成三角形区域.

定理7.3.1 令 $h(n)$ 表示拓 $\pmb{n + 1}$ 条边的凸多角形区域由引入在区域内不相交的对角线分成三角形区域的方法的个数，若定义 $h(1) = 1$ ，那么 $h(n)$ 是下列递归关系的解。

$$
\begin{array}{l} H (n) = \sum_ {k = 1} ^ {n - 1} H (k) H (n - k) \\ = H (1) \cdot H (n - 1) + H (2) \cdot H (n - 2) + \dots \\ + H (n - 1) \cdot H (1), \quad n = 2, 3, 4, \dots . \tag {7.3.1} \\ \end{array}
$$

这个递归关系具有初值 $H(1) = 1$ 的解为

$$
h (n) = \underbrace {\frac {1}{n} \binom {2 n - 2} {n - 1}} _ {n - 1}, n = 1, 2, 3, \dots . \tag {7.3.2}
$$

证明 如果 $n = 1$ ，已定义 $h(1) = 1$ ，我们可把一个直线段想象为具有2条边但无内部的多角形区域．如果 $n = 2$ ，则 $h(2) = 1$ ，因为一个三角形区域没有对角线并且它不能进一步分划．此外，因为

$$
\sum_ {k = 1} ^ {2 - 1} h (k) h (2 - k) = \sum_ {k = 1} ^ {1} h (k) h (2 - k) = h (1) * h (1) = 1,
$$

递归关系（7.3.1）对于 $n = 2$ 成立。现在令 $n \geqslant 3$ 。考虑有 $n + 1 \geqslant 4$ 条边的凸多角形区域 $R$ 。我们把区域的一条边区分出来且称它为基，在将 $R$ 分成三角形区域的每一种分法中，这个基是这些三角形区域 $T$ 之一的一条边，并且这个三角形区域把 $R$ 的剩余部分分成 $k + 1$ 条边的多角形区域 $R_{1}$ 与 $n - k + 1$ 条边的多角形区域 $R_{2}$ ， $k = 1, 2, 3, \dots, n - 1$ （见图7.4）。

$R$ 的进一步细分，是分别在 $R_{1}$ 与 $R_{2}$ 内引入一些不相交的对角线，把 $R_{1}$ 与 $R_{2}$ 分成若干三角形区域来实现的。因为 $R_{1}$ 有 $k + 1$

条边，所以 $R_{1}$ 可以按 $h(k)$ 种方法分成三角形区域； $R_{2}$ 有 $n - k + 1$ 条边， $R_{2}$ 可以按 $h(n - k)$ 种方法分成三角形区域。因而，对于选择含有基的某特定三角形区域 $T$ ，利用在区域内引入不相交的对角线有 $h(k)$ 和 $h(n - k)$ 种方法把 $R$ 分成三角形区域。所以总共有

![](images/32ee1eb907fc921dcbb41f595849c264333035d70f409b05c43f7f8242fa700a.jpg)  
图7.4

$$
\sum_ {k = 1} ^ {n - 1} h (k) h (n - k)
$$

种方法把 $R$ 分成三角形区域.这就建立了递归关系（7.3.1）

现在着手考虑它的解，递归关系（7.3.1）不是线性的。此外， $h(n)$ 不依赖于事先给出的若干个固定值，而是依赖所有值 $h(1), h(2), \cdots, h(n-1)$ 。于是，一般方法对于解这递归关系并不适用。为了方便起见，令 $a_1 = h(1)$ ， $a_2 = h(2)$ ， $a_3 = h(3)$ ， $\cdots$ ， $a_n = h(n)$ ， $\cdots$ 。因此，（7.3.1）成为

$$
a _ {n} = a _ {1} a _ {n - 1} + a _ {2} a _ {n - 2} + \dots + a _ {n - 1} a _ {1}, \tag {7.3.3}
$$

这里 $a_{1} \neq 1$ ，令

$$
f (x) = a _ {1} x + a _ {2} x ^ {2} + \dots + a _ {n} x ^ {n} + \dots \tag {7.3.4}
$$

是序列 $a_1, a_2, \dots, a_n, \dots$ 的生成函数，那么，用 $f(x)$ 乘其自身，得到

$$
\begin{array}{l} f (x) ^ {2} = a _ {1} ^ {2} x ^ {2} + \left(a _ {1} a _ {2} + a _ {2} a _ {1}\right) x ^ {3} + \left(a _ {1} a _ {3} + a _ {2} a _ {2} + a _ {3} a _ {1}\right) x ^ {4} + \dots \\ + \left(a _ {1} a _ {i - 1} + a _ {2} a _ {i - 2} + \dots + a _ {n - 1} a _ {1}\right) x ^ {n} + \dots . \\ \end{array}
$$

用（7.3.3）和 $a_{1} = a_{2} = 1$ ，我们可以把它记为

$$
\begin{array}{l} f (x) ^ {2} = a _ {1} ^ {2} x ^ {2} + a _ {3} x ^ {3} + a _ {4} x ^ {4} + \dots + a _ {n} x ^ {n} + \dots \\ = a _ {2} x ^ {2} + a _ {3} x ^ {3} + a _ {4} x ^ {4} + \dots + a _ {n} x ^ {n} + \dots \\ \end{array}
$$

$$
= f (x) - a _ {1} x = f (x) - x.
$$

于是， $f(x)$ 满足方程

$$
f (x) ^ {2} - f (x) + x = 0.
$$

这是关于 $f(x)$ 的二次方程，因此由二次求根公式得 $f(x) = f_{1}(x)$ ，或 $f_{2}(x)$ 。这里

$$
f _ {1} (x) = \frac {1 + \sqrt {1 - 4 x}}{2}, f _ {2} (x) = \frac {1 - \sqrt {1 - 4 x}}{2}.
$$

由 $f(x)$ 的定义可得 $f(0) = 0$ ，又因 $f_{1}(0) = 1$ 和 $f_{2}(0) = 0$ ，我们断定

$$
f (x) = f _ {2} (x) = \frac {1 - \sqrt {1 - 4 x}}{2} = \frac {1}{2} - \frac {1}{2} (1 - 4 x) ^ {1 / 2}.
$$

根据Newton二项式定理（特别是参看4.6节末所作过的结论），

$$
(1 + z) ^ {1 / 2} = 1 + \sum_ {n = 1} ^ {\infty} \frac {(- 1) ^ {n - 1}}{n \times 2 ^ {2 n - 1}} \binom {2 n - 2} {n - 1} z ^ {n},
$$

对于 $|z| < 1$ ，如果我们用 $-4x$ 替代 $z$ ，得到

$$
\begin{array}{l} (1 - 4 x) ^ {1 / 2} = 1 + \sum_ {n = 1} ^ {\infty} \frac {(- 1) ^ {n - 1}}{n \times 2 ^ {2 n - 1}} \binom {2 n - 2} {n - 1} (- 1) ^ {n} 4 ^ {n} x ^ {n} \\ = 1 + \sum_ {n = 1} ^ {\infty} (- 1) ^ {2 n - 1} \frac {2}{n} \binom {2 n - 2} {n - 1} x ^ {n} \\ = 1 - 2 \sum_ {n = 1} ^ {\infty} \frac {1}{n} \binom {2 n - 2} {n - 1} x ^ {n} \quad \text {对 于} | x | <   \frac {1}{4}. \\ \end{array}
$$

于是

$$
f (x) = \frac {1}{2} - \frac {1}{2} (1 - 4 x) ^ {1 / 2} = \sum_ {n = 1} ^ {\infty} \frac {1}{n} \binom {2 n - 2} {n - 1} x ^ {n}. \tag {7.3.5}
$$

比较方程（7.3.4）与（7.3.5）便知

$$
a _ {n} = \frac {1}{n} \binom {2 n - 2} {n - 1} \quad \text {对 于} n = 1, 2, 3, \dots
$$

因为 $h(n) = a_{n}$ ，定理证毕。

正如6.4节说明过的那样，数

$$
\frac {1}{n} \binom {2 n - 2} {n - 1} \quad n = 1, 2, 3, \dots ,
$$

叫做Catalan数。上述定理和证明对于6.4节研究的问题提供了另一种解法。那里，我们计算了按序给定的 $n$ 个数乘法的方法个数，并求出它是等于第 $n$ 个Catalen数。我们在那里注意到递归关系（7.3.1）对这个问题也成立。

# 7.4 指数型生成函数

在7.1节中，利用单项式集合 $\{1, x, x^2, \dots, x^k, \dots\}$ 定义数列 $a_0, a_1, a_2, \dots, a_k, \dots$ 的生成函数。这个特别适用于某些序列，尤其是包含二项式系数的序列，因为它是Newton二项式定理的格式，不过，对于用其项来计算排列的那些序列，考虑关于单项式 $\{1, x, x^2/2!, \dots, x^n/n!, \dots\}$ 的生成函数更加有用。这些单项式产生 $e^x$ 的幂级数，

$$
e ^ {x} = \sum_ {n = 0} ^ {\infty} \frac {x ^ {n}}{n !} = 1 + x + \frac {x ^ {2}}{2 !} + \dots + \frac {x ^ {n}}{n !} + \dots .
$$

把与单项式 $\{1, x, x^2/2!, \cdots, x^n/n!$ , $\cdots\}$ 有关的生成函数称为指数生成函数（对于单项式 $\{1, x, x^2, \cdots, x^n, \cdots\}$ 的情形，我们仍用短语“生成函数”或“普通生成函数”）。已知一数列 $a_0, a_1, a_2, \cdots, a_n, \cdots$ ，我们定义它的指数生成函数是

$$
f _ {e} (x) = \sum_ {n = 0} ^ {\infty} a _ {n} \frac {x ^ {n}}{n !} = a _ {0} + a _ {1} x + a _ {2} \frac {x _ {2}}{2 !} + \dots + a _ {n} \frac {x ^ {n}}{n !} + \dots .
$$

例令 $n$ 是正整数. 试确定数列 $P(n,0), P(n,1), P(n,2), \dots, P(n,n)$ 的指数生成函数，如前所述，这里 $P(n,k)$ 表示 $n$ 元素集合的 $k$ 排列的个数，并且其值为 $n! / (n - k)!, k = 0, 1, 2, \dots, n.$ 指数生成函数是

$$
\begin{array}{l} f _ {e} (x) = P (n, 0) + P (n, 1) x + P (n, 2) \frac {x ^ {2}}{2 !} + \dots + P (n, n) \frac {x ^ {n}}{n !} \\ = 1 + x + \frac {n !}{2 ! (n - 2) !} x ^ {2} + \dots + \frac {n !}{n ! 0 !} x ^ {n} = (1 + x) ^ {n}. \\ \end{array}
$$

因此， $(1 + x)^n$ 是数列 $P(n,0)$ ， $P(n,1), \dots, P(n,n)$ 的指数生成函数。在7.1节中，我们已经看到 $(1 + x)^n$ 是数 $C(n,0), C(n,1), \dots, C(n,n)$ 的普通生成函数。

例 序列 $1, 1, 1, \dots, 1$ ，…的指数生成函数是

$$
f _ {e} (x) = \sum_ {n = 0} ^ {\infty} \frac {x ^ {n}}{n !} = e ^ {x}.
$$

更一般地，如果 $\pmb{\alpha}$ 是任意实数，序列 $a^0 = 1$ ，α， $\pmb{a}^{2}$ ，…， $\pmb{\alpha}^{n}$ …的指数生成函数是

$$
f _ {e} (x) = \sum_ {n = 0} ^ {\infty} a ^ {n} \frac {x ^ {n}}{n !} = \sum_ {n = 0} ^ {\infty} \frac {(a x) ^ {n}}{n !} = e ^ {a x}.
$$

如果 $k$ 是正整数，回忆3.4节中 $k^n$ 表示有 $k$ 个不同元素且每个元素都有无限重复数的多重集合的 $n$ 排列个数。于是，对于这个计数的序列的指数函数是 $e^{k^x}$ 。

对于 $k$ 个不同元素且每个元素都有有限重复数的多重集合 $S$ ，下面定理确定了 $S$ 的 $n$ 排列个数的指数生成函数。这是在3.4节末已约定的指数生成函数的形式中的解。我们定义多重集合的0排列个数是等于1。

定理7.4.1 令 $S$ 是多重集合 $\{n_{1} \cdot e_{1}, n_{2} \cdot e_{2}, \dots, n_{k} \cdot e_{k}\}$ ，这里 $n_{1}, n_{2}, \dots, n_{k}$ 是非负整数。对于 $n = 0, 1, 2, \dots$ ，令 $a_{n}$ 是 $S$ 的 $n$ 排列的个数。那么， $a_{0}, a_{1}, a_{2}, \dots, a_{n}, \dots$ 的生成函数 $f_{e}(x)$ 是

$$
f n _ {1} (x) f n _ {2} (x) \dots f n _ {t} (x), \tag {7.4.1}
$$

其中对于 $i = 1,2,\dots ,k_{\bullet}$

$$
f n _ {i} (x) = 1 + x + \frac {x ^ {2}}{2 !} + \dots + \frac {x ^ {n _ {i}}}{n _ {i} !}. \tag {7.4.2}
$$

证明 令

$$
f _ {e} (x) = a _ {0} + a _ {1} x + a _ {2} \frac {x ^ {2}}{2 !} + \dots + a _ {n} \frac {x ^ {n}}{n !} + \dots
$$

是 $a_{0}, a_{1}, a_{2}, \cdots, a, \cdots$ 的指数生成函数。注意对于 $n > n_{1} + n_{2} + \cdots + n_{k}$ ， $a = 0$ ，因而 $f_{e}(x)$ 确实是有限和。从（7.4.2）我们看到，当（7.4.1）作完乘法时，得到形如

$$
\frac {x ^ {m _ {1}}}{m _ {1} !} - \frac {x ^ {m _ {2}}}{m _ {2} !} \dots \frac {x ^ {m _ {k}}}{m !} = \frac {x ^ {m _ {1} + m _ {2} + \cdots + m k}}{m _ {1} ! m _ {2} ! \cdots m _ {k} !} \tag {7.4.3}
$$

的项，这里 $0 \leqslant m_{1} \leqslant n_{1}, 0 \leqslant m_{2} \leqslant n_{2}, \dots, 0 \leqslant m_{k} \leqslant n_{k}$

令 $n = m_{1} + m_{2} + \dots +m_{k}$ 那么（7.4.3）中的表达式可以记为

$$
\frac {x ^ {n}}{m _ {1} ! m _ {2} ! \cdots m _ {k} !},
$$

它也等于

$$
\frac {n !}{m _ {1} ! m _ {2} ! \cdots m _ {k} !} \cdot \frac {x ^ {n}}{n !}.
$$

这样，（7.4.1）中的 $x^{3} / n!$ 的系数是

$$
\sum \frac {n !}{m _ {1} ! m _ {2} ! \cdots m _ {k} !}. \tag {7.4.4}
$$

这里求和是对所有整数 $m_{1}, m_{2}, \cdots, m_{k}$ 且 $0 \leqslant m_{1} \leqslant n_{1}, 0 \leqslant m_{2} \leqslant n_{2}, \cdots, 0 \leqslant m_{k} \leqslant n_{k}$ 以及 $m_{1} + m_{2} + \cdots + m_{k} = n$ 进行的。但是，从3.4节我们断定在（7.4.4）求和中的项

$$
\frac {n !}{m _ {1} ! m _ {2} ! \cdots m _ {k} !}
$$

等于 $S$ 的 $n$ 排列个数或是 $S$ 的子多重集合 $\{m_1 * e_1, m_2 * e_2, \dots, m_k * e_k\}$ 的排列个数。因为 $S$ 的 $n$ 排列个数等于使 $m_1 + m_2 + \dots + m_k = n$ 的所有子多重集合的排列个数， $S$ 的 $n$ 排列个数 $a_i$ 等于（7.4.4）中的数。由于这也是（7.4.1）乘积中 $x^n / n!$ 的系数，我们断定

$$
f _ {e} (x) = f n _ {1} (x) f n _ {2} (x) \dots f n _ {k} (x).
$$

利用在定理7.4.1证明中所用过的同样论证，可以计算数列的生成函数，而这数列中的每个数是计算带附加限制的多重集合的 $n$ 排列数。让我们注意，如果在（7.4.2）式中，用下式来定义 $f_{\infty}(x)$

$$
f _ {\infty} (x) = 1 + x + \frac {x ^ {2}}{2 !} + \dots + \frac {x ^ {k}}{k !} + \dots = e ^ {x}, \tag {7.4.5}
$$

那么定理7.4.1对于重复数 $n_{1}, n_{2}, \cdots, n_{k}$ 都是 $\infty$ 时仍然有效。的确，若它们中的某些是有限数而其余等于 $\infty$ ，定理也是有效的。另外，有下面的指数生成函数的例子。令 $S$ 是多重集合

$$
\{\infty \bullet e _ {1}, \infty \bullet e _ {2}, \dots , \infty e _ {k} \}.
$$

$$
(e ^ {x} - 1) ^ {k} \tag {7.4.6}
$$

是序列 $a_0, a_1, a_2, \dots, a_n, \dots$ 的指数生成函数，其中 $a_n$ 是 $S$ 的每个不同元素至少出现一次的 $n$ 排列个数。

$$
\begin{array}{l} \left(1 + \frac {x ^ {2}}{2 !} + \frac {x ^ {4}}{4 !} + \dots + \frac {x ^ {2 m}}{(2 m) !} + \dots\right) ^ {k} \\ = \left[ \frac {1}{2} \left(e ^ {x} + e ^ {- x}\right) \right] ^ {k} \tag {7.4.7} \\ \end{array}
$$

是 $S$ 的 $\pmb{n}$ 排列个数序列的指数生成函数，其中的排列要求每个不同元素出现偶数次。

$$
\begin{array}{l} \left(1 + \frac {x ^ {2}}{2 !} + \frac {x ^ {5}}{5 !}\right) \left(x + \frac {x ^ {2}}{2 !} + \frac {x ^ {6}}{6 !}\right) \left(1 + x + \frac {x ^ {2}}{2 !} + \dots \right. \\ \left. + \frac {x ^ {n}}{m !} + \dots\right) ^ {k = 2} \tag {7.4.8} \\ \end{array}
$$

是序列 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}$ , $\cdots$ 的指数生成函数。这里 $a_{n}$ 是 $S$ 的 $n$ 排列个数，其中 $e_{i}$ 出现 0 次，2 次或 5 次， $e_{2}$ 出现 1 次，2 次和 6 次，其余 $e_{3}, \cdots, e_{8}$ 出现次数不加限制。

例 一个 $1 \times n$ 棋盘中的正方形着红、白和蓝三色，假定偶数正方形着白色，试求着色的方法个数。

令 $a_{n}$ 表示这种着色的个数，这里我们规定 $a_{0}$ 是1，那么 $a_{n}$ 等于3种颜色（红、白和蓝）的多重集合的 $n$ 排列个数，而其中每个有无限重复数，白色出现偶数次。于是， $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 的指数生成函数是

$$
\begin{array}{l} f _ {e} (x) = \left(1 + \frac {x ^ {2}}{2 !} + \frac {x ^ {4}}{4 !} + \dots\right) \left(1 + x + \frac {x ^ {2}}{2 !} + \dots\right) ^ {2} \\ = \frac {1}{2} \left(e ^ {x} + e ^ {- x}\right) e ^ {3 x} = \frac {1}{2} \left(e ^ {3 x} + e ^ {x}\right) \\ = \frac {1}{2} \left[ \sum_ {n = 0} ^ {\infty} 3 ^ {n} \frac {x ^ {n}}{n !} + \sum_ {n = 0} ^ {\infty} \frac {x ^ {n}}{n !} \right] \\ = \frac {1}{2} \sum_ {n = 0} ^ {\infty} (3 ^ {n} + 1) \frac {x ^ {n}}{n !}. \\ \end{array}
$$

因此， $a_{n} = (3^{n} + 1) / 2,n = 0,1,2,\dots .$

例 确定 $n$ 位数码全是奇数且数字1和3出现偶数次的数码个数。

令 $a_{n}$ 表示这种数码个数，这里定义 $a_0 = 1$ ，那么 $a_{n}$ 等于多重集合 $S = \{\infty, 1, \infty, 3, \infty, 5, \infty, 7, \infty, 9\}$ ，其中1和3是出现偶数次的 $n$ 排列个数。 $a_0, a_1, a_2, \dots, a_n$ ，…的指数

生成函数是

$$
\begin{array}{l} f _ {e} (x) = \left(1 + \frac {x ^ {2}}{2 !} + \frac {x ^ {4}}{4 !} + \dots\right) ^ {2} \left(1 + x + \frac {x ^ {2}}{2 !} + \dots\right) ^ {3} \\ = \left(\frac {e ^ {x} + e ^ {- x}}{2}\right) ^ {2} e ^ {3 x} = \left(\frac {e ^ {2 x} + 1}{2}\right) ^ {2} e ^ {x} \\ = - \frac {1}{4} \left(e ^ {4 x} + 2 e ^ {2 x} + 1\right) e ^ {x} \\ = \frac {1}{4} \left(e ^ {5 x} + 2 e ^ {3 x} + e ^ {x}\right) \\ = \frac {1}{4} \cdot \left[ \sum_ {n = 0} ^ {\infty} 5 \frac {x ^ {n}}{n !} + 2 \sum_ {n = 0} ^ {\infty} 3 ^ {n} \frac {x ^ {n}}{n !} + \sum_ {n = 0} ^ {\infty} \frac {x ^ {n}}{n !} \right] \\ = \sum_ {n = 0} ^ {\infty} \frac {5 ^ {n} + 2 \times 3 ^ {n} + 1}{4} \cdot \frac {x ^ {7}}{n !}. \\ \end{array}
$$

所以

$$
a _ {n} = \frac {5 ^ {n} + 2 \times 3 ^ {n} + 1}{4}, \quad n = 0, 1, 2, \dots .
$$

# 练习

1. 令 $c$ 的实数，确定数列 $c^0 = 1, c^1, c^2, \dots, c^n, \dots$ 的生成函数。

2. 确定下列数列的生成函数

(a) 1, -1, 1, ..., (-1)\( ^n \), \).

（b） $\binom{a}{0}, -\binom{a}{1}, \binom{a}{3}, \cdots, (-1)^n \binom{a}{n}, \cdots$

（ $\pmb{\alpha}$ 是实数）.

(c) 0, 0, -1, 1, -1, 1, -1, 1, …

(d) 5, 6, 7, ..., n+5, ...

(e) $1 / 0!$ , $1 / 1!$ , $1 / 2!$ , ..., $1 / n!$ , ...   
$(f)1 / 0!, - 1 / 1!,1 / 2!,\dots ,(-1)^{n} / n!,\dots .$

3. 令 $S$ 是多重集合 $\{\infty, e_1, \infty, e_2, \infty, e_3, \infty, e_4\}$ . 确定序列 $a_0, a_1, a_2, \cdots, a_n, \cdots$ 的生成函数，其中 $a_n$ 是 $S$ 的附有下面限制的 $n$ 组合个数.

(a) 每个 $e_i$ 出现奇数次.  
（b）每个 $e_i$ 出现3的倍数次  
（c）元素 $\pmb{e}_{1}$ 不出现， $\pmb{\epsilon_{2}}$ 至多出现一次.  
（d）元素 $e_1$ 出现 1 次，3 次或 11 次，元素 $e_2$ 出现 2 次，4 次或 5 次.  
(e) 每个 $e$ ; 至少出现 10 次.

4. 确定第一类 Stirling 数列: $s(n, 0)$ , $s(n, 1)$ , $s(n, 2)$ , $\cdots$ , $s(n, n)$ (见第六章练习29) 的生成函数.

5*. 确定第二类Stirling数列: $S(n,0), S(n,1), S(n,2), \cdots, S(n,n)$ (见6.5节) 的生成函数.  
6. 利用7.2节中所叙述的方法，用生成函数解下列递归关系

(a) $a_{n} = 4a_{n - 2}$ 对于 $n = 2,3,4,\dots ,a_{t} = 0$ $a_1 = 1.$   
(b) $a_{n} = a_{n - 1} + a_{n - 2}$ 对于 $n = 2, 3, 4, \dots; a_0 = 1$ , $a_1 = 3$   
$(c) a_{n} = a_{n-1} + 9a_{n-2} - 9a_{n-3}$ 对于 $n = 3, 4, 5, \cdots$ ， $a_{0} = 0, a_{1} = 1, a_{2} = 2$ .  
(d) $a_{n} = 8a_{n - 1} - 16a_{n - 2}$ 对于 $n = 2,3,4,\dots ,a_0 = -1,$ $a_1 = 0$   
（e）an=3a-2-2a-3 对于n=3，4，5，…；α=1，a1 $= 0$ ， $a_{2} = 0$   
$(f) a_{1} = 5a_{n - 1} - 6a_{n - 2} - 4a_{n - 3} + 8a_{n - 4}$ 对于 $n = 4, 5, 6, \dots; a_{0} = 0, a_{1} = 1, a_{2} = 1, a_{3} = 2$ 。

$(g)a_{n} = 2a_{n - 1} - 4a_{n - 2} + 8a_{n - 3} + 16a_{n - 4}$ 对于 $\pmb {n} = \pmb{4}$ ，5， $6,\dots ,a_0 = 1,a_1 = 2,a_2 = 1,a_3 = 2.$

7. 利用7.2节中所描述的除法，计算生成函数是下列式子的序列的头6项。

（a）1+2x 1+3x+x  
（b）1+3x² 1+2x+x²+5x4.  
（c）+3x+x² 3+2x4

8. 确定立方序列 $0, 1, 8, \cdots, n^{3}, \cdots$ 的生成函数.

9. 令 $a_0, a_1, a_2, \cdots, a_n$ 是对于 $n = 0, 1, 2, \cdots$ 由 $a_n = n^8$ 定义的序列。对于 $n = 1, 2, 3, \cdots$ ，证明递归关系 $a_n = a_{n-1} + 3n^2 + 3n + 1$ ，并且利用这个递归关系确定该序列的生成函数。  
$10^{*}$ 令 $k$ 是正整数，且 $a_0, a_1, a_2, \dots, a_n$ ，…是对于 $n = 0, 1, 2, \dots$ 由 $a_n = n^k$ 定义的序列，确定这个序列所满足的递归关系，然后再确定它的生成函数.  
11. 令 $a_0, a_1, a_2, \dots, a_n, \dots$ 是由 $a_n = \binom{n}{2}$ , $n = 0, 1, 2, \dots$ 定义的序列，确定该序列的生成函数。  
12. 令 $a_{0}, a_{1}, a_{2}, \cdots, a_{n}, \cdots$ 是由 $a_{n}=\binom{n}{3}, n=0,1,2, \cdots$ 定义的序列，确定这个序列的生成函数。  
13°. 令 $h(n)$ 表示 $n + 2$ 条边的凸多角形区域被它的对角线（假设没有3条对角线有公共点）分成的区域个数，定义 $h(0) = 0$ ，证明对于 $n = 1,2,3,\dots$ 有

$$
h (n) = h (n - 1) + \binom {n + 1} {3} + n.
$$

确定序列 $h(0), h(1), h(2), \dots, h(n), \dots$ 的生成函数，并由此求出 $h(n)$ 的公式.

14. 确定序列 $1!$ ，2！，3！，…，n！，的指数生成函数

15. 令 $a$ 是实数，序列 $a_0, a_1, a_2, \dots, a_n, \dots$ 是由 $a_0 = 1$ ， $a_n = a(a - 1) \dots (a - n + 1)$ ， $n = 1, 2, 3, \dots$ 定义的序列。确定该序列的指数生成函数。

16. 令 $S$ 是多重集合 $\{\infty, e_1, \infty, e_2, \dots, \infty, e_k\}$ . 确定序列 $a_0, a_1, a_2, \dots, a_n, \dots$ 的指数生成函数，其中 $a_0 = 1$ 和对于 $n = 1, 2, 3, \dots$ 有

（a） $\sigma_{n}$ 等于 $S$ 的每个元素出现奇数次的 $\pmb{n}$ 排列个数  
（b） $a_{n}$ 等于 $S$ 的每个元素至少出现4次的 $n$ 排列个数.  
（c）an等于S的n排列个数，其中e至少出现一次，至少出现二次，…， $\pmb{e_{k}}$ 至少出现 $k$ 次.  
（d）an等于S的n排列个数，其中e至多出现一次，至多出现二次，…，a至多出现k次.

17. 正整数 $n$ 的划分是把 $n$ 作为一些正整数之和的一种表示。在和中的整数次序是不加考虑的，例如， $6 = 3 + 3$ ， $6 = 6$ ， $6 = 1 + 1$ ， $2 + 2$ 都是 $6$ 的划分。令 $p(n)$ 等于 $n$ 的划分个数，定义 $p(0) = 1$ 。

（a）证明 $p(n)$ 等于多重集合 $S = \{n\cdot a\}$ 分成无序多重集合（其个数不作规定）的划分个数.  
（b）证明 $p(n)$ 等于方程 $n = 1 \cdot x_{1} + 2 \cdot x_{2} + \dots + n \cdot x_{n}$ 在非负整数 $x_{1}, x_{2}, \dots, x_{n}$ 中解的个数.  
（c）证明序列p(0)，p(1)，p(2)，，p(n)，…的生成函数由无限乘积

$$
\prod_ {k = 1} ^ {\infty} (1 - x ^ {k}) ^ {- 1}
$$

给定.

$(d)$ 令 $t_1, t_2, \dots, t_m$ 是不同的正整数。令 $q(n)$ 等于 $n$ 的划分个数，其中所有加数取自 $t_1, t_2, \dots, t_m$ ，定义 $q(0) = 1$ ，证明 $q(0), q(1), \dots, q(n)$ ，…的生成函数是

$$
\prod_ {k = 1} ^ {m} \left(1 - x ^ {t _ {k}}\right) ^ {- 1}.
$$

18. 一个 $1 \times n$ 棋盘的各正方形着红、兰、绿和橙四色，若偶数个正方形着红色和偶数个正方形着绿色，试确定上述着色方法的个数。  
19. 试确定有 $n$ 位数码全是奇数，使 1 与 3 每个都出现正偶数次的数码个数。  
20. 设至少有 4 个数字组成的 $n$ 位数码, 其中 4 与 6 每个出现偶数次, 5 与 7 至少出现一次, 8 与 9 不作限制. 试确定这种数码的个数.

# 第八章 相异代表组

本章所研究的一般问题是个典型问题，并且可用下面所谓配偶问题的例子引入。

在某地区全体居民中，有很多未婚小姐与先生。这些小姐都急于结婚。如果没有任何其它限制，为了满足这些小姐的心愿应必备唯一的条件是现有的先生人数至少和小姐人数一样多。可是，即使很着急的小姐也不会那样轻率地结婚。实际上，每个女人可能排除一些可与其结婚的男人，列出她认为合意的那些先生的名单。那个地区的每一位小姐和她认为合意的先生是否都可能结婚？当然，这不总是可能的。比如，或许有三位小姐，每个人列出名单仅有两位先生，恰好又都是相同的两位先生。既然不总是可能的，其可能条件是什么？当这些条件不满足，找到对象的小姐的最多数人数是多少？最后，我们将如何确定哪位小姐和哪位先生结婚，以便有配偶的人数最多？

以上的问题，可能纯属消遣，而无重大应用。但是，我们将见到这个问题和它的解对实际问题，有着重要的联系。例如把职业分配给胜任的申请者。它的解将使我们能够解决第一章中涉及的完全覆盖“剪过”棋盘所表述的问题，并且它将对于在第一章中已讨论过的和第九章中将要进一步讨论的构造拉丁方有某种应用。

# 8.1 相异代表组

在上面所表述的配偶问题中，我们的每一位小姐都准备一张她合意的先生名单。令 $G = \{g_1, g_2, \dots, g_m\}$ 是供选取

的先生集合， $L = \{l_1, l_2, \dots, l_n\}$ 是小姐的集合。那么对应于每一位小姐 $l_i$ ，有 $G$ 的子集 $G_i$ （ $l_i$ 的名单）。为使每位小姐能和她的名单中的一位先生结婚，其必要和充分条件是可以从 $G_1, G_2, \dots, G_n$ 的每一个集合中选取一个成员，且没有任何两次选取是相同的（没有两位小姐要给同一位先生作配偶）。从每个集合选取一个相异元素称为集合 $G_1, G_2, \dots, G_n$ 的相异代表组。

例在 $G = \{g_{1},g_{2},g_{3},g_{4},g_{5}\}$ 和 $L = \{l_1,l_2,l_3,$ （20 $l_{4}\}$ 情形中，其中小姐的名单是 $G_{1} = \{g_{2},g_{3}\} ,G_{2} = \{g_{1},$ $g_{4},g_{5}\} ,G_{3} = \{g_{1},g_{2},g_{5}\} ,G_{4} = \{g_{2},g_{4}\}$ 、一个相异代表组可以由 $G_{1}$ 中选取 $g_{2}$ ， $G_{2}$ 中选取 $g_{1}$ ， $G_{3}$ 中选取 $g_{5}$ $G_{4}$ 中选取 $g_{4}$ 得到。因此，若 $l_{1}$ 与 $g_{2}$ 结婚， $l_{2}$ 与 $g_{1}$ 结婚， $l_{3}$ 与 $g_{5}$ 结婚， $l_{4}$ 与 $g_{4}$ 结婚，则满足这些小姐们的要求.

相异代表的问题可以考虑为与配偶无关的任意集合族问题。于是，我们有如下说法：

令 $E$ 是集合， $A_{1}, A_{2}, \cdots, A_{n}$ 是 $E$ 的 $n$ 个子集合族。 $A_{1}, A_{2}, \cdots, A_{n}$ 不要求是相异集合。 $E$ 的一族元素 $e_{1}, e_{2}, \cdots, e_{n}$ 是 $A_{1}, A_{2}, \cdots, A_{n}$ 的代表组，只要 $e_{1}$ 在 $A_{1}$ 中， $e_{2}$ 在 $A_{2}$ 中， $\cdots$ 在 $A_{n}$ 中。此外，若 $e_{1}, e_{2}, \cdots, e_{n}$ 是相异元素，则 $e_{1}, e_{2}, \cdots, e_{n}$ 是 $A_{1}, A_{2}, \cdots, A_{n}$ 的相异代表组。

例令 $A_{1} = \{1,4\}$ ， $A_{2} = \{1,2,3\}$ ， $A_{3} = \{3$ 4，5}， $A_{4} = \{3,4,5\}$ 那么4，1，4，3是 $A_{1}$ $A_{2},A_{3},A_{4}$ 的代表组，而1，2，3，4和4，2，3，5是相异代表组.

一个非空集合族 $A_{1}, A_{2}, \cdots, A_{n}$ 总有代表组。为构造一个代表组，我们只需从每个集合中按任意方式选取一个元素就行了。不过，这个集合族未必有相异代表组。

例令 $A_{1} = \{1,4\}$ ， $A_{2} = \{1,2,3,4,5\}$ ， $A_{3} = \{4\}$ 和 $A_{4} = \{1,4\}$ 。那么， $A_{1},A_{2},A_{3},A_{4}$ 没有相异代表组。为了看清这一点，考虑三个集合 $A_{1},A_{3},A_{4}$ 。它们之间只有两个元素，即1和4。因而如果我们要从 $A_{1},A_{3},A_{4}$ 中选取相异的三个元素，这是不可能的。

更一般地，我们可以表述关于集合族 $A_{1}$ ， $A_{2}$ ，…， $A_{n}$ 有相异代表组的必要条件。假设 $e_{1}, e_{2}, \cdots, e_{n}$ 是 $A_{1}, A_{2}, \cdots, A_{n}$ 的相异代表组。令 $k$ 是满足 $1 \leqslant k \leqslant n$ 的整数。如果我们任意取 $k$ 个集合，比方说 $A_{i_{1}}, A_{i_{2}}, \cdots, A_{i_{k}}$ ，其中 $1 \leqslant i_{1} < i_{2} < \cdots < i_{k} \leqslant n$ ，那么 $\{e_{i_{1}}, e_{i_{2}}, \cdots, e_{i_{k}}\}$ 是 $A_{i_{1}} \cup A_{i_{2}} \cup \cdots \cup A_{i_{k}}$ 的 $k$ 元子集。于是， $A_{i_{1}} \cup A_{i_{2}} \cup \cdots \cup A_{i_{k}}$ 至少含有 $k$ 个相异元素。若集合族 $A_{1}$ ， $A_{2}$ ，…， $A_{n}$ 有相异代表组，这是一个必须成立的条件。这类条件的个数是 $2^{n} - 1$ （ $\{1, 2, \cdots, n\}$ 的非空子集的个数）。每一个都有相异代表组的必要条件。出乎意料地这些条件又提供了关于一个集合族 $A_{1}$ ， $A_{2}$ ，…， $A_{n}$ 的相异代表组的充分条件。这是在1935年由P.Hall证明的，并且通常称为配偶问题，因为它回答了配偶的第一个问题。

定理8.1.1（配偶定理）集合族 $A_{1}, A_{2}, \cdots, A_{n}$ 有相异代表组当且仅当满足下面条件：

(i) 配偶条件，对于每个 $k = 1, 2, \dots, n$ 和对于每次选取 $i_1, i_2, \dots, i_k$ 其中 $(1 \leqslant i_1 < i_2 < \dots < i_k \leqslant n)$ ，有

$$
\left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| \geqslant k.
$$

证明 我们已注意到配偶条件(i)是 $A_{1}, A_{2}, \cdots, A_{n}$ 有相异代表组的必要条件，现在我们对 $n$ 归纳证明：如果配偶条

件成立，则 $A_{1}, A_{2}, \cdots, A_{n}$ 有相异代表组。首先令 $n = 1$ ，因而我们只要有一个集合 $A_{1}$ ，配偶条件说 $A_{1}$ 至少有一个元素，比方说 $e_{1}$ ，并且 $e_{1}$ 本身就是 $A_{1}$ 的相异代表组。现在令 $n > 1$ ，归纳假设是少于 $n$ 个集合且满足配偶条件的任何集合族有相异代表组。现在假设 $A_{1}, A_{2}, \cdots, A_{n}$ 满足配偶条件。我们区分两种情况。

情况1 对于 $1 \leqslant k \leqslant n - 1$ 与 $i_1, i_2, \dots, i_k$ 的每个选择，其中 $1 \leqslant i_1 < i_2 < \dots < i_k \leqslant n$ ，我们有

$$
\left| A _ {i _ {1}} \bigcup A _ {i _ {2}} \bigcup \dots \bigcup A _ {i _ {k}} \right| \geqslant k + 1.
$$

这时配偶条件满足并且有空余。由于配偶条件满足， $A_{n}$ 不空。选取 $A_{n}$ 的任何元素 $e_{n}$ 以及考虑从 $A_{1}, A_{2}, \cdots, A_{n-1}$ 的每一集合中去掉 $e_{n}$ （如果存在的话），而得到的集合族 $A_{1} - \{e_{n}\}, A_{2} - \{e_{n}\}, \cdots, A_{n-1} - \{e_{n}\}$ 。令 $1 \leqslant k \leqslant n-1$ ，考虑 $i_{1}, i_{2}, \cdots, i_{k}$ ，其中 $1 \leqslant i_{1} < i_{2} < \cdots < i_{k} \leqslant n-1$ ，则有

$$
\begin{array}{l} \left. \right.\left| \right.\left(A i _ {1} - \left\{e _ {n} \right\}\right) \cup \left(A i _ {2} - \left\{e _ {n} \right\}\right) \cup \dots \cup \left(A i _ {k} - \left\{e _ {n} \right\}\right)\left. \right] \\ = \left| \left(A i _ {1} \bigcup A i _ {2} \bigcup \dots \bigcup A i _ {k}\right) - \left\{e _ {n} \right\} \right| \\ \geqslant \left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| - 1 \\ \geqslant (k + 1) - 1 = k. \\ \end{array}
$$

于是， $A_{1} - \{e_{n}\}$ ， $A_{2} - \{e_{n}\}$ ，…， $A_{n - 1} - \{e_{n}\}$ 满足配偶条件．由归纳假设可知，该族有相异代表组，比方说， $e_1$ $e_2$ ，…， $e_{n - 1}$ ，因而 $e_1$ ， $e_2$ ，…， $e_{n - 1}$ ， $e_n$ 是关于 $A_{1}$ ， $A_{2}$ ，…， $A_{n - 1}$ ， $A_{n}$ 的相异代表组.

情况2（这是情况1的否定）对于某个整数 $p$ ，且 $1 \leqslant p \leqslant n - 1$ ，以及对于 $i_1, i_2, \dots, i_p$ 的某个选择，其中 $1 \leqslant i_1 < i_2 < \dots < i_p \leqslant n$ ，我们有

$$
\left| A _ {i _ {1}} \bigcup A _ {i _ {2}} \bigcup \dots \bigcup A _ {i p} \right| = p.
$$

这时有该族的 $p$ 个集合，而这些集合之中恰好含 $p$ 个元素。为

了简化记号，记 $p$ 个集合为 $A_{1}, A_{2}, \cdots, A_{p}$ （利用该族集合重新编号，总是可以做到的）。于是，对于某个 $p$ ，这里 $1 \leqslant p \leqslant n - 1$ ，就有 $\{A_{1} \cup A_{2} \cup \cdots \cup A_{p}\} = p$ 。

首先考虑集合族 $A_{1}$ ， $A_{2}$ ，…， $A_{p}$ 。由于这些集合 $A_{1}$ ， $A_{2}$ ，…， $A_{p}$ 是从满足配偶条件的集合族 $A_{1}$ ， $A_{2}$ ，…， $A_{p}$ 中选取的，所以 $A_{1}$ ， $A_{2}$ ，…， $A_{p}$ 满足配偶条件。因为 $p \leqslant n - 1$ ，由归纳假设知， $A_{1}$ ， $A_{2}$ ，…， $A_{p}$ 有相异代表组，比方说 $e_{1}$ ， $e_{2}$ ，…， $ep$ 。注意 $A_{1} \cup A_{2} \cup \dots \cup A_{p}$ 恰好含有 $p$ 个元素，我们便有 $A_{1} \cup A_{2} \cup \dots \cup A_{p} = \{e_{1}, e_{2}, \dots, e_{p}\}$ 。

令 $F = \{e_1, e_2, \dots, e_p\}$ 。现在考虑有 $n - p$ 个集合的集合族 $A_{p + 1} - F, A_{p + 2} - F, \dots, A_n - F$ ，由于 $p \geqslant 1$ ，我们有 $n - p \leqslant n - 1$ 。我们证明这个集合族满足配偶条件。令 $k$ 是整数并且 $1 \leqslant k \leqslant n - p$ ，以及 $j_1, j_2, \dots, j_k$ 是整数，并且 $p + 1 \leqslant j_1 < j_2 < \dots < j_k \leqslant n$ 。则

$$
\begin{array}{l} \left| \left(A _ {j _ {1}} - F\right) \cup \left(A _ {j _ {2}} - F\right) \cup \dots \cup \left(A _ {j _ {k}} - F\right) \right| \\ = \left| \left(A _ {j _ {1}} \cup A _ {j _ {2}} \cup \dots \cup A _ {j _ {k}}\right) - F \right| \\ = \left\{\left(F \cup \bigcup A _ {j _ {1}} \cup A _ {j _ {2}} \cup \dots \cup \left(A _ {j _ {k}}\right) - F \right] \right. \\ = \left| \left(A _ {1} \cup A _ {2} \cup \dots \cup A _ {p} \cup A _ {j _ {1}} \cup A _ {j _ {2}} \cup \dots \cup A _ {j _ {k}}\right) - F \right| \\ = \left| A _ {1} \cup A _ {2} \cup \dots \cup A _ {p} \cup A _ {j _ {1}} \cup A _ {j _ {2}} \cup \dots \cup A _ {j _ {k}} \right| - | F | \\ \geqslant (p + k) - p = k. \tag {①} \\ \end{array}
$$

这个不等式可以从 $A_{1}, A_{2}, \cdots, A_{n}$ 满足配偶条件和 $|F| = p$ 得到。上面结论的其余部分，从集合论性质和 $F = A_{1} \cup A_{2} \cup \cdots \cup A_{s}$ 得到。我们断定集合族 $A_{p+1} - F, A_{p+2} - F, \cdots, A_{s} - F$ 满足配偶条件，并由此根据归纳假设它有相异代表组，比方说 $e_{p+1}, e_{p+2}, \cdots, e_{i}$ 。但是， $e_{1}, e_{2}, \cdots, e_{p}, e_{p+1}, e_{p+2}, \cdots, e_{n}$ 是 $A_{1}, A_{2}, \cdots, A_{p}, A_{p+1}, A_{p+2}, \cdots, A_{n}$

的相异代表组.

这就完成了定理的证明。

例 令集合族 $A_{1}, A_{2}, A_{3}, A_{4}, A_{5}$ 由下式定义: $A_{1} = \{1, 2\}$ , $A_{2} = \{1, 4\}$ , $A_{3} = \{1, 3, 4, 5\}$ , $A_{4} = \{2, 4\}$ , $A_{5} = \{1, 4\}$ . 则 $A_{1} \cup A_{2} \cup A_{4} \cup A_{5} = \{1, 2, 4\}$ , 于是 $|A_{1} \cup A_{2} \cup A_{4} \cup A_{5}| = 3$ . 根据配偶定理, 集合族 $A_{1}, A_{2}, A_{3}, A_{4}, A_{5}$ 没有相异代表组, 然而集合族 $A_{1}, A_{2}, A_{3}, A_{4}$ 却有相异代表组, 如1, 4, 5, 2. 因此, 可以用相异元素代表的给定集合族的集合, 最多个数是4. 若 $A_{1}, A_{2}, A_{3}, A_{4}, A_{5}$ 这些集合是5位小姐的五张名单, 这意味着, 每位小姐可以和她的名单巾的先生结婚的人数最多是4.

给定一个集合族，我们希望确定集合族中有相异代表组的集合的最大数目。首先证明下面定理。

定理8.1.2 令 $A_{1}, A_{2}, \cdots, A_{n}$ 是集合 $E$ 的子集合族。令 $r$ 是正整数且 $r \leqslant n$ ，则该集合族中 $r$ 个集合有相异代表组当且仅当满足下面条件：

(ii)对每个 $k = 1,2,\dots ,n$ 以及对于 $i_1,i_2,\dots ,i_k$ 的每次选择（这里 $1\leqslant i_{1} <   i_{2} <   \dots <  i_{k}\leqslant n)$ ，有

$$
\left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| \geqslant k - (n - r)
$$

在证明定理之前，注意到定理8.1.1是这个定理当 $r = n$ 时所得到的特殊情形。又注意到，若 $k = 1, 2, \dots, n - r$ ，则(ii)中的不等式自动满足，因为这时右端不比零大。

证明我们用定理8.1.1和一点技巧，令 $F$ 是 $\pmb{n} - \pmb{r}$ 个元素的任何集合，并且与 $\pmb{E}$ 没有公共元素。于是，由于每个 $A_{i}$ 是 $\pmb{E}$ 的子集， $F$ 与每个 $A_{i}$ 没有公共元素。考虑由集合 $A_{1}, A_{2}, \dots, A_{n}$ 中的每一个加上 $\pmb{F}$ 的元素，得到集合族 $A_{1} \cup F, A_{2} \cup F, \dots, A_{n} \cup F$ 。

首先证明集合族 $A_{1}, A_{2}, \cdots, A_{n}$ 中的 $r$ 个集合有相异代表组，当且仅当 $A_{1} \cup F, A_{2} \cup F, \cdots, A_{n} \cup F$ 有相异代表组。为了使记号简化，假定 $r$ 个集合 $A_{1}, A_{2}, \cdots, A_{r}$ ，有相异代表组，比方说 $e_{1}, e_{2}, \cdots, e_{r}$ 。令 $F = \{f_{1}, f_{2}, \cdots, f_{n-r}\}$ 。那么， $e_{1}, e_{2}, \cdots, e_{r}, f_{1}, f_{2}, \cdots, f_{n-r}$ 是 $A_{1} \cup F, A_{2} \cup F, \cdots, A_{n} \cup F$ 的相异代表组。这是因为 $F$ 的元素都在这个族的所有集合中，并且没有 $F$ 中的任何元素与 $e_{1}, e_{2}, \cdots, e_{r}$ 中的任何元素相同。现在假定 $A_{1} \cup F, A_{2} \cup F, \cdots, A_{n} \cup F$ 有相异代表组，比方说 $x_{1}, x_{2}, \cdots, x_{n}$ 。因为 $F$ 只有 $n-r$ 个元素，至多有 $n-r$ 个 $x_{i}$ 属于 $F$ ，所以至少有 $r$ 个 $x_{i}$ 不属于 $F$ 。为简化起见，设 $x_{1}, x_{2}, \cdots, x_{r}$ 不属于 $F$ 。那么， $x_{1}$ 在 $A_{1}$ 中， $x_{2}$ 在 $A_{2}$ 中， $\cdots$ ， $x_{r}$ 在 $A_{r}$ 中。由于 $x_{1}, x_{2}, \cdots, x_{r}$ 是相异的，故 $x_{1}, x_{2}, \cdots, x_{r}$ 是 $A_{1}, A_{2}, \cdots, A_{r}$ 的相异代表组。因而， $A_{1}, A_{2}, \cdots, A_{n}$ 中的 $r$ 个集合有相异代表组。

由定理8.1.1可知， $A_{1}\cup F,A_{2}\cup F,\dots ,A_{n}\cup F$ 有相异代表组，当且仅当对每个 $k = 1,2,\dots ,n$ 和对于 $\pmb {i}_1,\pmb {i}_2,\dots ,$ （20 $i_k$ 且 $1\leqslant i_1 <   i_2 <   \dots <  i_k\leqslant n$ 的每次选择，有

$$
\left| \left(A i _ {1} \bigcup F\right) \bigcup \left(A i _ {2} \bigcup F\right) \bigcup \dots \bigcup \left(A i _ {k} \bigcup F\right) \right| \geqslant k.
$$

但是， $(A_{i_1} \cup F) \cup (A_{i_2} \cup F) \cup \dots \cup (A_{i_k} \cup F) = (A_{i_1} \cup A_{i_2} \cup \dots \cup A_{i_k}) \cup F$ ，又由于没有 $F$ 的元素在任何 $A_i$ 中，所以

$$
\begin{array}{l} \left| \left(A _ {i _ {1}} \bigcup A _ {i _ {2}} \bigcup \dots \bigcup A _ {i _ {k}} \bigcup F \right| = \left| \left(A _ {i _ {1}} \bigcup A _ {i _ {2}} \bigcup \dots \right. \right. \right. \\ \left. \bigcup A _ {i _ {1}} \right| + | F | = | A _ {i _ {1}} \bigcup A _ {i _ {2}} \bigcup \dots \bigcup A _ {i _ {k}} | \\ + (n - r) _ {\bullet} \\ \end{array}
$$

于是上述不等式化为

$$
\left. \left. A _ {i _ {1}} \bigcup A _ {i _ {2}} \bigcup \dots \bigcup A _ {i _ {k}} \right| + (n - r) \geqslant k _ {\bullet} \right.
$$

这就意味着当且仅当满足条件(ii)时， $A_{1}\bigcup F$ ， $A_{2}\bigcup F$ ，…， $A_{n}\bigcup F$ 有相异代表组。因为， $A_{1}$ ， $A_{2}$ ，…， $A_{n}$ 中的r个集合

有相异代表组，当且仅当 $A_{1} \cup F, A_{2} \cup F, \dots, A_{n} \cup F$ 有相异代表组。定理证毕。

定理8.1.3 令 $A_{1}, A_{2}, \cdots, A_{n}$ 是集合族，则该集合族中有相异代表组的集合的最大数目等于表达式

$$
\left| A i _ {1} \cup A i _ {2} \overbrace {\cdots} ^ {\infty} \cup A i _ {k} \right| + (n - k)
$$

对于 $k = 1,2,\dots ,n$ 和 $i_1,i_2,\dots ,i_n$ ，其中 $\mathbf{1}\leqslant i_{1} <   i_{2} <   \dots <$ $i_k\leqslant n$ 的所有选择的最小值.

证明 从定理8.1.2可得，集合族 $A_{1}, A_{2}, \cdots, A_{n}$ 中的 $r$ 个集合有相异代表组，当且仅当对于 $k = 1, 2, \cdots, n$ 和 $i_{1}, i_{2}, \cdots, i_{k} (1 \leqslant i_{1} < i_{2} < \cdots < i_{k} \leqslant n)$ 的所有选择，有

$$
\left| A i _ {1} \cup A i _ {2} \cup \dots \cup A i _ {k} \right| + (n - k) \geqslant r _ {\bullet}
$$

于是，使这个不等式总是成立的 $r$ 的最大值等于表达式

$$
\left| A i _ {1} \cup A i _ {2} \cup \dots \cup A i _ {k} \right| + (n - k)
$$

的最小值.

上述定理解决了确定每位小姐可以与她合意的名单中某位先生结婚的最多小姐人数的问题。到目前我们还没有确立“最好配偶方式”的有效方法。这一点将在8.3节中考虑。

例令集合族 $A_{1}, A_{2}, \dots, A_{7}$ 由

$$
\begin{array}{l} A _ {1} = \{1, 6 \}, A _ {2} = \{4, 5, 6 \}, A _ {3} = \{2, 6 \}, \\ A _ {4} = \{1, 2 \}, A _ {5} = \{1, 2, 4, 6 \}, A _ {6} = \{1 \}, \\ A _ {7} = \{1, 2, 6 \} \\ \end{array}
$$

定义. 则

$$
\begin{array}{l} \left. \left| A _ {1} \cup A _ {3} \cup A _ {4} \cup A _ {6} \cup A _ {7} \right| + (7 - 5) \right. \\ = \left| \left\{1, 2, 6 \right\} \right| + 2 = 5. \\ \end{array}
$$

因为6，5，2，1，4是 $A_{1}$ ， $A_{2}$ ， $A_{3}$ ， $A_{4}$ ， $A_{5}$ 的相异代表组，可见5是这个集合族中有相异代表组的集合的最大数目。

# 8.2 多米诺骨牌、棋盘与偶图

我们由回忆在第一章已介绍的下列问题开始。从 $m \times n$ 棋盘着手，其 $mn$ 个正方形位于 $m$ 行 $n$ 列。这些正方形交替地着黑色 $(b)$ 和白色 $(w)$ 。因此，如果两个正方形有公共边，则它们的着色不同。我们用一把剪刀剪掉棋盘上的某些个正方形，剩下剪过的棋盘。剪过的完全覆盖是多米诺骨牌在棋盘上的一种排列，使得

(i) 每个多米诺骨牌盖住剪过棋盘上的两个正方形.

(ii)每个剪过棋盘的正方形由某个多米诺骨牌盖住.

(iii)没有两个多米诺骨牌重迭（盖住同一个正方形）。把(ii)与(iii)联结起来，要求剪过棋盘的每个正方形恰好有一个多米诺骨牌盖住。问题是确定剪过棋盘何时有完全覆盖。更一般地，我们要问可以把多米诺骨牌放在剪过棋盘上使(i)与(iii)满足的最多骨牌是多少。由于每个骨牌应当盖住一个黑色正方形和一个白色正方形，显然剪过棋盘必须有相等的黑色和白色正方形以便能够有一个完全覆盖。但是，正如在第一章中指出过的那样，靠这一点还不能保证存在一个完全覆盖。可以按下面方法使用配偶定理来确定一个完全覆盖何时是可能的。令剪过棋盘上的黑色正方形对应于小姐，而白色正方形对应于先生。黑色正方形的名单是由所有的与它有公共边的白色正方形组成。于是，黑色正方形的名单中至多有4个白色正方形；它可能含有的个数比4少，因为这个黑色正方形可能是在棋盘的边上，或者与它有公共边的白色正方形可能被剪掉了。可见白色正方形是在黑色正方形的名单上，只要用一个骨牌同时能盖住这个黑色和这个白色的正方形。这样一来，假定在剪

过棋盘上有等数个白色和黑色正方形，一个完全覆盖对应于黑色正方形与白色正方形的配偶，并且每一个黑色正方形与它的名单上的白色正方形相配偶（等价地说，由黑色正方形的名单组成的集合族有相异代表组）。根据配偶定理的结论，我们则有下面定理。

定理8.2.1 令 $B$ 与 $W$ 分别表示剪过棋盘上的黑色和白色正方形集合。对于黑色正方形集合 $B$ 的每个子集 $A$ ，令 $W(A)$ 表示至少与 $A$ 中的一个黑色正方形有公共边的白色正方形集合。那么完全覆盖存在当且仅当 $|B| = |W|$ 和对于 $B$ 的每个子集 $A$ ，有

$$
\left| W (A) \right| \geqslant | A |.
$$

定理中的集合 $W(A)$ 恰好是与 $A$ 中的黑色正方形相连系的那些名单的并集。

例 考虑图8.1中剪过的 $5 \times 6$ 棋盘，其中画影线的正方形是被剪掉的。为了区分各个正方形，我们在剪过棋盘的白色与黑色正方形内标有脚码。

![](images/93d0f6dad37a829ef2d08ea765994c0805ad8fd16693fccad20564cda92f83ad.jpg)  
图8.1

剪过棋盘有9个黑色和9个白色的正方形.如果对于 $i = 1,2,3,\dots ,9$ ，我们令 $\pmb{W_{i}}$ 表示与黑色正方形b有公共边的白色正方形集合，那么 $W_{1} = \{\pmb {w}_{1},\pmb {w}_{2}\} ,\pmb{W_{2}} = \{\pmb{w_{2}},\pmb {w_{3}},\pmb{w_{4}}\} ,$ $W_{3} = \{\overbrace{w_{2},w_{3},w_{4}}\} ,W_{4} = \{w_{3},w_{0},w_{7}\} ,W_{6} = \{w_{4},$ （20 $w_{7}\} ,W_{6} = \{w_{5}\} ,W_{7} = \{w_{8},w_{8}\} ,W_{8} = \{w_{7},w_{8}\}$ 和 $W_{9} = \{w_{6},w_{9}\} .W_{1},W_{2},\dots ,W_{9}$ 有相异代表组是 $w_{1},w_{3},w_{2},w_{8},w_{4},w_{5},w_{3},w_{7},w_{9}$ 剪过棋盘的完全覆盖是放一个骨牌盖住 $b_{1}$ 与 $w_{1}$ ，放一个骨牌盖住 $b_{2}$ 与 $w_{3}$ ，放一个骨牌盖住 $b_{3}$ 与 $w_{2}$ ，等等.

更一般地，可以使用定理8.1.3来确定在剪过棋盘上按着下面方法放置骨牌的最多个数。这个方法是使得(i)每个骨牌盖住剪过棋盘的两个正方形和(iii)没有两个骨牌重迭。上述的讨论和例子应该能够把这一点弄清楚。对于剪过棋盘的每一个黑色正方形有一个与它有公共边的白色正方形的子集合。这就定义了白色正方形的子集合族。使(i)与(iii)成立的可放置在剪过棋盘上的骨牌最多个数等于该集合族中有相异代表组的集合的最大数目。

存在另外一种方法研究多米诺骨牌——棋盘问题。更一般地，研究相异代表组问题。令 $X$ 和 $Y$ 是没有公共元素的有限集合。令 $\Delta$ 是形如 $[x, y]$ 的元素对集合，其中 $x$ 在 $X$ 中和 $y$ 在 $Y$ 中， $X$ 与 $Y$ 的元素叫做结点， $\Delta$ 中的元素对 $[x, y]$ 叫做边。所得到的组合对象叫做偶图，并用 $\langle X, \Delta, Y \rangle$ 表示它。正如这个名字所表示的那样，偶图是称为图的更一般的组合对象的特殊情形。这将在第十章中研究，现在我们不去管它。如果 $e = [x, y]$ 是一条边，则 $x$ 和 $y$ 称为这条边 $e$ 的结点，并且说 $e$ 联结 $x$ 与 $y$ 和 $x$ 与 $y$ 相交于 $e$ 。若两条边有一个公共点，也说它们是相交的。我们可以用下面例子中指出的方式画出偶图。

例令 $X = \{a,b,c\}$ 和 $Y = \{r,s,t,u\}$ ，偶图 $\langle X,\Delta ,Y\rangle$ 的边是 $[a,r],[a,t],[b,s],[b,t],[b,u],[c,u]$ 。用点表示 $X$ 的每一结点，这些点被排成一列。同样地，有对应于 $Y$ 的结点的一列点。把一列中的点与另外一列中的点用直线段连接起来，只要对应的这些点的结点，在偶图中由边连接着。得到几何图形如图8.2中所示。

对于剪过棋盘， $B$ 和 $W$ 分别是其上的白色与黑色正方形的集合，把它与偶图 $\langle B,\Delta ,W\rangle$ 联系起来，这里用一条边联结黑色正方形与白色正方形，只要它们有一个公共边。于是，我们想象把一条边对应成剪过棋盘上放置的一个多米诺骨牌。更一般地，令 $A_{1},A_{2},\dots ,$ $A_{n}$ 是集合 $Y$ 的子集合族。设 $X = \{1,2,\dots ,n\}$ ，我们把这个子集合族与偶图 $\langle X,\Delta ,Y\rangle$ 连系起来，这里对于 $X$ 中的i和Y中的y，由边把i与y连接，只

![](images/ca2369624706a2bfc77d11c735bc3123325deda46e6a23514d594a4b4042e85b.jpg)  
图8.2

要 $y$ 是集合 $A_{i}$ 的元素。于是， $X$ 中的结点对应着集合 $A_{1}, A_{2}, \ldots, A_{n}$ ，并且对应于每个集合中的每个元素都有一条边。

例令 $Y = \{a,b,c,d,e\}$ 的子集合族 $A_{1},A_{2},A_{3},$ $A_{4}$ 是由 $A_{1} = \{a,b\}$ ， $A_{2} = \{a,c\}$ ， $A_{3} = \{b,c,d\}$ ， $A_{4} = \{d,e\}$ 定义，那么 $X = \{1,2,3,4\}$ 和在图8.3中画出相连系的偶图。显然，集合族 $A_{1},A_{2},A_{3},A_{4}$ 可以从偶图重新构造出来，因为后者包含着有关哪个元素在哪个集合的信息。 $A_{1},A_{2},A_{3},A_{4}$ 的相异代表组是 $a,c,b,e$ 。在偶图中这对应着什么？因为 $a$ 在 $A_{1}$ 中，有连接1与 $a$ 的边[1,a]。同样，在偶图中有边[2，c]，[3，b]，[4，e]。这些边中没有两

边有一公共点，所以 $a, c, b, e$ 是 $A_{1}$ ， $A_{2}$ ， $A_{3}$ ， $A_{4}$ 的相异代表组。反之，任何两边没有公共点，4条不同边的集合给出 $A_{1}$ ， $A_{2}$ ， $A_{3}$ ， $A_{4}$ 的相异代表组。例如，4条边 $[1, b]$ ， $[2, a]$ ， $[3, d]$ ， $[4, e]$ 中没有两边有一公共点，这些边对应相异代表组 $b$ ， $a$ ， $d$ ， $e$ 。

![](images/66aeb2ff68795d221ca2b5e3ea51bec61bb62ea40dbac0f2db0a477615946c8b.jpg)  
图8.3

给定偶图 $\langle X, \Delta, Y \rangle$ ，边的集合是匹配的，只要这些边中任何两边没有公共点。于是， $\Delta$ 中的边集合 $\left[x_{i_1}, y_{i_1}\right]$ ， $\left[x_{i_2}, y_{i_2}\right]$ ，…， $\left[x_{i_k}, y_{i_k}\right]$ 在偶图中是匹配的，只要结点 $x_{i_1}, x_{i_2}, \ldots, x_{i_k}$ 是不同的，并且结点 $y_{i_1}, y_{i_2}, \ldots, y_{i_k}$ 也是不同的。从上述例子加以概括，我们得到下面结果。令 $A_1, A_2, \ldots, A_n$ 是集合 $Y$ 的子集合族， $\langle X, \Delta, Y \rangle$ 是相连系的偶图，其中 $X = \{1, 2, \ldots, n\}$ 。

(i) 如果 $\{[i_1, y_{i_1}], [i_2, y_{i_2}], \dots, [i_k, y_{ik}]\}$ 是匹配的，则 $y_{i_1}, y_{i_2}, \dots, y_{ik}$ 是 $A_{i_1}, A_{i_2}, \dots, A_{ik}$ 的相异代表组。  
(ii) $A_{1}$ , $A_{2}$ , $\cdots$ , $A_{n}$ 有相异代表组当且仅当 $\langle X, \Delta, Y \rangle$ 中有 $n$ 条边是匹配的。  
(iii)在集合族 $A_{1}$ ， $A_{2}$ ，…， $A_{n}$ 中，有相异代表组的集合的最大数目是 $\langle X, \Delta, Y \rangle$ 中相匹配的边的最大个数。

令 $\langle X, \Delta, Y \rangle$ 是偶图。说结点集合 $S$ 盖住边只要 $\Delta$ 中的每条边在 $S$ 中至少有它的一个结点。例如，在图8.3的偶图中，结点集合 $\{3, 4, a, b, c\}$ 盖住所有边。下面定理是由D.Ko-nig1)证明的。

定理8.2.2 令 $\langle X, \Delta, Y \rangle$ 是偶图，则相匹配边的最大个数等于盖住这些边的结点的最小个数。

证明 我们将应用定理8.1.3。令 $\alpha$ 等于匹配边的最大个数， $\beta$ 等于能够盖住这些边的结点最小个数。需要证明 $\alpha = \beta$ 。令 $M$ 是 $\alpha$ 条匹配边，而 $S$ 是盖住这些边的 $\beta$ 个结点集合。因为 $M$ 的两条边无公共点，又由于 $M$ 的每条边至少在 $S$ 中有它的一个结点，根据鸽笼原理可知 $\alpha \leqslant \beta$ 。现在我们指出存在盖住这些边的 $\alpha$ 个结点的集合。这时将有 $\beta \leqslant \alpha$ 。把不等式 $\alpha \leqslant \beta$ 和 $\alpha \geqslant \beta$ 合并起来，我们便得到 $\alpha = \beta$ 。假设 $X = \{1, 2, \dots, n\}$ ，考虑 $Y$ 的子集合族 $A_{1}, A_{2}, \dots, A_{n}$ ，其中 $A_{i}$ 由 $Y$ 中使 $[1, y]$ 是 $\Delta$ 中的边所有那些 $y$ 组成， $A_{2}$ 由 $Y$ 中使 $[2, y]$ 是 $\Delta$ 中的边的所有那些 $y$ 组成，等等。那么，在偶图 $\langle X, \Delta, Y \rangle$ 中匹配边的最大个数 $\alpha$ 等于集合族 $A_{1}, A_{2}, \dots, A_{n}$ 中有相异代表组的集合的最大数目。于是，由定理8.1.3可知有整数 $k$ ， $1 \leqslant k \leqslant n$ 以及存在整数 $i_{1}, i_{2}, \dots, i_{s}$ 且 $1 \leqslant i_{1} < i_{2} < \dots < i_{s} \leqslant n$ ，使得

$$
a = \left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| + n - k.
$$

考虑 $\alpha$ 个结点集合

$$
T = (A _ {1} \cup A _ {2} \cup \dots \cup A _ {i _ {k}}) \cup \left\{i _ {k + 1}, i _ {k + 2}, \dots , i _ {n} \right\}
$$

这里 $\{i_{k+1}, i_{k+2}, \cdots, i_n\}$ 是 $\{i_1, i_2, \cdots, i_k\}$ 在 $X$ 中的补集。令 $[x, y]$ 是 $\Delta$ 中一条边。如果 $x$ 是 $i_{k+1}, i_{k+2}, \cdots, i_n$ 中之一，则 $[x, y]$ 在 $T$ 中有它的一个结点。否则， $x$ 是 $i_1, i_2, \cdots, i_k$ 中之一。这时，由于 $[x, y]$ 是一条边， $y$ 是 $A_x$ 的一个元素。因为 $x$ 是 $i_1, i_2, \cdots, i_k$ 之一，可见 $y$ 在 $T$ 中。所以 $T$ 盖住这些边。由于 $T$ 恰好含有 $\alpha$ 个结点，这个定理的证明便完成了。

例 考虑图8.4中画出的偶图。容易检验 $\{1, 2, e, f\}$ 盖住偶图中的边。于是，由定理8.2.2知，任何匹配至多有4条边。因为边 $[1, a]$ ， $[2, b]$ ， $[3, e]$ ， $[4, f]$ 形成匹配，我

们断定在这个偶图中匹配边的最多条数是4.

![](images/15fc73f18ac4a7b9e3f71443d4f0b86a8df0afea6809324cf0126628d231f9c5.jpg)  
图8.4

# 8.3 一种算法

本节我们考虑下列问题。已知一个偶图 $\langle X, \Delta, Y \rangle$ ，用有效的方法确定一种匹配，在这个偶图的所有匹配中它包含的边数最多。如果 $M$ 是在 $\langle X, \Delta, Y \rangle$ 中匹配，我们将描述一种算法，它或能确定 $M$ 有最大可能的边数，或指出怎样修改 $M$ 得到比 $M$ 边数更大的匹配。重复应用这种算法，最终会确定最大边数的匹配。

令 $\langle X, \Delta, Y \rangle$ 是偶图。 $\langle X, \Delta, Y \rangle$ 中的基本链 $\gamma$ 定义为两个或更多个相异结点的序列使得相邻结点由一边连接。两个相邻的结点必须一个在 $X$ 中和另一个在 $Y$ 中。若 $u$ 和 $\sigma$ 分别是基本链 $\gamma$ 的第一个结点和最末一个结点，则说 $\gamma$ 连接 $u$ 和 $v$ 。一个基本链不是连接 $X$ 的两结点或 $Y$ 的两结点，就是连接 $X$ 的一个结点与 $Y$ 的一个结点。基本链 $\gamma$ 的边是连接链上的相邻结点的那

些边。 $Y$ 上边的个数称为它的长度。奇数长的链必须是连接 $X$ 的结点与 $Y$ 的结点，而偶数长的链必须连接 $X$ 的两结点或 $Y$ 的两结点。基本圈定义为 5 个或更多个结点序列，这些结点除第一个结点和最末一个结点是相同外，其它结点是相异的，并且相邻结点由边连接。偶图中基本圈的边的个数（它的长度）必是偶数。因而，偶图中的基本圈有至少为 4 的偶数长度。

例 考虑图8.5中所画的偶图。那么， $x_{1}, y_{2}, x_{3}, y_{4}$ 是连接 $x_{1}$ 与 $y_{4}$ 的长度为3的链。这个链的边是 $[x_{1}, y_{2}]$ ， $[x_{4}, y_{2}]$ ，和 $[x_{3}, y_{4}]$ ， $y_{4}, x_{4}, y_{3}, x_{2}, y_{1}$ 是连接 $y_{4}$ 与 $y_{1}$ 长度为4的链，其边是 $[x_{4}, y_{4}]$ ， $[x_{4}, y_{3}]$ ， $[x_{2}, y_{3}]$ ，和 $[x_{2}, y_{1}]$ 。一些圈的例子是 $x_{1}, y_{2}, x_{2}, y_{1}, x_{1}$ 与 $x_{1}, y_{2}, x_{3}, y_{4}, x_{4}, y_{3}, x_{2}, y_{1}, x_{1}$ 。

![](images/3ab4882d01290d74339b9b56bd54633dafaac6a134050ca6875d6afa26427183.jpg)  
图8.5

![](images/402a448aef8d036408416db4e8b59bba02b87fb502c7fb0bdbff8179fe7f2b4b.jpg)  
图8.6

令 $M$ 是偶图 $\langle X, \Delta, Y \rangle$ 中的匹配。连接相异结点 $u$ 和 $v$ 的奇数长的基本链 $\gamma$ 称为关于 $M$ 的交错链，只要

(i) $\pmb{\gamma}$ 的第一，第三，第五，…边不在 $M$ 中；  
(ii) $\pmb{\gamma}$ 的第二，第四，…边在 $M$ 中；  
（iii） $\pmb{u}$ 与 $\pmb{v}$ 都不与 $M$ 的边相交.

注意到在关于匹配 $M$ 的交错链 $\gamma$ 中，不在 $M$ 中的 $\gamma$ 的边数比在 $M$ 中的 $\gamma$ 的边数多出一条，并且 $\gamma$ 连接 $X$ 的结点和 $Y$ 的结点。长为 1 的链 $u$ ， $v$ 是关于 $M$ 的交错链，只要边 $[u, v]$ 不在 $M$ 中，并且 $u$ 与 $v$ 都不与 $M$ 的边相交。

例 考虑图8.6中所画出的偶图。令 $M$ 是匹配的，其边在图中用粗体标出。那么， $x_{2}, y_{3}, x_{5}, y_{6}, x_{3}, y_{2}$ 是关于 $M$ 的交错链。注意如果用不在 $M$ 中 $\gamma$ 的边代替 $M$ 的在 $\gamma$ 中的那些边，得到比 $M$ 多一条边的新匹配 $M'$ 。我们有 $M = \{[x_{1}, y_{1}], [x_{3}, y_{6}], [x_{6}, y_{3}]\}$ 。用 $[x_{2}, y_{3}], [x_{6}, y_{6}]$ 和 $[x_{3}, y_{2}]$ 代替 $[x_{3}, y_{5}]$ 和 $[x_{5}, y_{3}]$ ，我们得到匹配 $M' = \{[x_{1}, y_{1}], [x_{2}, y_{3}], [x_{6}, y_{5}], [x_{3}, y_{2}]\}$ 。此外， $x_{2}, y_{2}$ 是关于 $M$ 的交错链 $\gamma'$ ，由于这个链长为 1， $M$ 的任何边都不是 $\gamma'$ 的边。如果增添 $[x_{2}, y_{2}]$ 而扩大 $M$ ，于是得到 4 条边的匹配 $M''$ 。

现在我们推广这些想法.

定理8.3.1 令 $M$ 是偶图 $\langle X, \Delta, Y \rangle$ 中的一个匹配。那么， $M$ 在 $\langle X, \Delta, Y \rangle$ 的所有匹配中有最多边数当且仅当不存在关于 $M$ 的交错链。

证明 正如上面例子那样，首先证明如果存在关于 $M$ 的交错链，则可以修改 $M$ 产生比 $M$ 多一条边的匹配。令 $\gamma$ 是关于 $M$ 的交错链。令 $M_0$ 是 $\gamma$ （属于 $M$ ）的边，又令 $N_0$ 是 $\gamma$ 的其余的边。由交错链的定义，断定 $|N_0| = |M| + 1$ 。令 $M' = (M - M_0) \cup N_0$ 。于是，利用 $N_0$ 中的边代替 $M_0$ 中的边由 $M$ 得到 $M'$ ，特别是 $|M'| = |M| + 1$ 。断言 $M'$ 是匹配的，因为 $M$ 是匹配的并且 $M - M_0$ 是 $M$ 的子集合， $M - M_0$ 一定是匹配的，又因为 $N_0$ 中的边是链 $\gamma$ 的第一，第三，第五，…边，所以 $N_0$ 也是匹配的。这样 $M'$ 将是匹配的，只要 $M - M_0$ 中的边都不与 $N_0$ 中的边相交。

考虑 $N_{0}$ 的一条边 $[x, y]$ 。首先假设 $[x, y]$ 既不是 $\gamma$ 的第一条边也不是 $\gamma$ 的最末一条边。那么， $x$ 是 $M_{0}$ 中某条边的结点。由于 $M$ 是匹配的， $x$ 不是 $M - M_{0}$ 中任何边的结点。类似地， $y$ 是 $M_{0}$ 中某条边的结点，不是 $M - M_{0}$ 中任何边的结点。所以， $[x, y]$ 不与 $M - M_{0}$ 中的任何边相交。现在假定 $[x, y]$ 是 $\gamma$ 的第一条边，并且说 $x$ 是 $\gamma$ 的第一个结点。那么， $y$ 是 $M_{0}$ 中某条边的结点，由此它不是 $M - M_{0}$ 中边的结点。不过，从交错链的定义中的部分(iii)，得到 $x$ 不是 $M$ 中边的结点，因而不是 $M - M_{0}$ 中边的结点。所以， $[x, y]$ 与 $M - M_{0}$ 中任何边都不相交。若 $[x, y]$ 是 $\gamma$ 的最末边，可用类似论证。于是， $N_{0}$ 中的任何边与 $M - M_{0}$ 中的边不相交。所以， $M' = (M - M_{0}) \cup N_{0}$ 是匹配的并且比 $M$ 多一条边。这便证明了定理第一部分。

相反，假设 $M^{*}$ 是匹配的，并且比 $M$ 的边多。我们证明存在关于 $M$ 的交错链。现在考虑偶图 $\langle X, M \cup M^{*}, Y \rangle$ ，其边是 $M$ 或 $M^{*}$ 中的那些边。这样所有不在 $M$ 或 $M^{*}$ 中的边就可以不予考虑。因为 $M$ 与 $M^{*}$ 都是匹配的，这个偶图的每个结点至多与两条边相交。所以把偶图分成很多不相交的基本链和圈（读者应该确信这一点）。对于每个基本链和圈，由于 $M$ 和 $M^{*}$ 都是匹配的，因而没有两条相邻边是在 $M$ 中，也没有任何两条相邻边在 $M^{*}$ 中。因为圈有偶数长，每个圈在 $M$ 中有的边数与它在 $M^{*}$ 中是一样多。所以，由于 $M^{*}$ 比 $M$ 边多，存在一个在 $M^{*}$ 中的边数比在 $M$ 中多的基本链 $\gamma$ 。这样，链 $\gamma$ 含有 $M^{*}$ 的边恰好比 $M$ 多一条，并且 $\gamma$ 是关于 $M$ 的交错链。这就完成了这个定理的证明。

例 考虑图8.7中画出的图。令 $M$ 是匹配 $\{[x_1, y_1], [x_2, y_2], [x_4, y_3], [x_5, y_4]\}$ ，其边在图中用粗体标出。匹配 $M^* = \{[x_1, y_2], [x_2, y_1], [x_3, y_3], [x_4, y_4], [x_5, y_6]\}$ 的边比 $M$ 的边多。偶图 $\langle X, M \cup M^*, Y \rangle$ （这时，

偶图与已知图是一样的）分成不相交基本圈 $x_{1}, y_{2}, x_{2}, y_{1}, x_{1}$ 和链 $x_{3}, y_{3}, x_{4}, y_{4}, x_{5}, y_{6}$ 。后者是关于 $M$ 的交错链。

![](images/082421e224147d9d1c25de80cef1f0794568e89f684c9abc545b77cc159f62ec.jpg)  
图8.7

定理8.3.1有下面推论，假设已知一个匹配 $M$ ，要想确定它是否可能有最多边数，根据这个定理，只要寻找关于 $M$ 的交错链。如果一个也不存在，则 $M$ 有最多可能的边数。不过，假若找到一个，则定理8.3.1的证明指出怎样利用交

错链去修改 $M$ 产生一个匹配，并且比 $M$ 多一条边。已知匹配 $M$ ，现在描述一个系统搜索关于 $M$ 的交错链的算法。这个算法或者产生这种交错链，或者从它断定 $M$ 在匹配中有最多可能的边数。这个算法是 L.R. Ford, Jr. 和 D.R. Fulkerson 1) 的更一般网络算法的特殊情形。

算法 令 $\langle X, \Delta, Y \rangle$ 是偶图，这里 $X$ 的 $m$ 个结点是 $\{x_1, x_2, \dots, x_m\}$ ， $Y$ 的 $n$ 个结点是 $\{y_1, y_2, \dots, y_n\}$ 。令 $M$ 是任意一匹配。用 (*) 标记不与 $M$ 中的任何边相交的 $X$ 的所有结点，采用下面两种标记过程 I 和 II，以 I 开始并交替使用 I 与 II 直至不能再进行标记为止：

1. 选取 $X$ 的新近标记的结点，比方说 $x_{i}$ ，用 $(x_{i})$ 标记 $Y$ 的所有结点，它们由不在 $M$ 中的边连接着，并且在前面未曾标记过。对 $X$ 的所有新近标记结点重复进行。  
I. 选取 $Y$ 的新近标记的结点，比方说 $y_{i}$ ，用 $(y_{i})$ 标记

$X$ 的所有结点，它们由 $M$ 中的边连接着，并且在前面未曾标记过。对 $Y$ 的所有新标记结点重复进行。

这样，在用 $(^{*})$ 标记 $X$ 的某些结点的初始步骤后，我们依次考虑这些结点的每一个，并且根据I中给出的规则标记Y的某些结点。然后，依次考虑Y的这些标记过的每一个结点，并且根据I中给出的规则标记 $X$ 的某些结点。再依次考虑 $X$ 的新近标记过的每一个结点，并且根据I中给出的规则标记Y的某些结点，等等。注意， $X$ 或 $Y$ 的任何结点不会得到多于一次的标记。我们继续交替使用I和I直至或者

（i） $Y$ 中已标记过的结点不与 $M$ 中任何边相交（这叫做临界），或者  
（ii）不可能标记更多的结点，并且临界已不出现（这叫做非临界）。

在临界情况下，我们成功地找到了关于 $M$ 的交错链。以 $Y$ 的达到临界结点开始，我们可以按相反次序，通过这些标记直至达到 $X$ 的以 $(*)$ 标记的结点为止，来构造交错链。如同在算法中我们使用过的那样，标记的作用是使我们能够在临界情况下构造交错链。现在指出在非临界情况下，匹配 $M$ 会有偶图所有匹配中最多的边数。

定理6.3.2 假设在上述算法中非临界已出现。令 $X_{1}$ 是由 $X$ 中所有未标记结点组成， $Y_{1}$ 是由 $Y$ 中所有标记过结点组成，则 $S = X_{1} \cup Y_{1}$ 覆盖偶图 $\langle X, \Delta, Y \rangle$ 的边， $|S| = |M|$ ，并且 $M$ 有匹配中最多可能的边数。

证明 $S$ 不能覆盖这些边的唯一原因是由于存在着连接 $X - X_{1}$ 中的一个结点与 $Y - Y_{1}$ 中的一个结点的一条边。假设存在这样一条边 $e = [x, y]$ ，且 $x$ 在 $X - X_{1}$ 中和 $y$ 在 $Y - Y_{1}$ 中。如果 $e$ 不在 $M$ 中，则由于 $x$ 已标记过，从算法 I 可得 $y$ 已标记

过，这与 $Y_{1}$ 含有 $Y$ 中所有标记过的结点相矛盾。于是， $e$ 在 $M$ 中。因为 $x$ 与 $M$ 中一边相交，从算法Ⅰ可知， $x$ 被标 $(y)$ 。但是，这从该算法也可得，结点 $y$ 自身谅必标记过，因为这个结点 $y$ 必在 $x$ 得到标记 $y$ 之前已经标记过。这又与 $Y_{1}$ 含有 $Y$ 的所有标记过结点相矛盾。我们断定不可能存在任何连接 $X - X_{1}$ 的结点与 $Y - Y_{1}$ 的结点的边，所以， $S = X_{1} \cup Y_{1}$ 覆盖偶图的所有边。

现在考虑 $Y_{1}$ 中一结点 $y$ 。由于 $y$ 已标记过且临界还未出现，可得 $y$ 与 $M$ 的一边相交。因为 $M$ 是一个匹配， $y$ 恰好与 $M$ 的一边（比方说 $[x, y]$ ）相交。根据算法 I，结点 $x$ 已标记过，因而 $x$ 不在 $X_{1}$ 中。现在考虑 $X_{1}$ 中一结点 $x'$ 。由于 $x'$ 未标记， $x'$ 与 $M$ 的一边相交，否则 $x'$ 在初始步应得到标记 $(\bullet)$ 。因为 $M$ 匹配， $x'$ 恰与 $M$ 的一边（比方说 $[x', y']$ ）相交。若 $y'$ 在 $Y_{1}$ 中，则 $y'$ 应标记过，并由算法 I，所以 $x'$ 也应标记过。由于 $x'$ 在 $X_{1}$ 中， $x'$ 未标记过，因而 $y'$ 在 $Y - Y_{1}$ 中。这样， $M$ 与 $x_{1}$ 相交的边中，没有一条是和 $M$ 与 $Y_{1}$ 相交的边相同。因为 $M$ 的每条边或与 $X_{1}$ 相交或与 $Y_{1}$ 相交，这就意味着（见图8.8） $M$ 中的边数与 $S = X_{1} \cup Y_{1}$ 中的结点一样多，即是 $\{S\} = |M|$ ，由于 $S$ 覆盖 $\langle X, \Delta, Y \rangle$ 的边，现在从定理8.2.2可得 $M$ 有匹配中最多可能边数。这便完成这个定理的证明。

我们可以应用上述算法，确定偶图中的具有最多边数匹配如下。先任选一条边 $e_1$ ，然后选不与 $e_1$ 相交的任何边 $e_2$ ，接下去选不与 $e_1$ 或 $e_2$ 相交的任意边 $e_3$ ，等等，直至不可能再选出边为止。这就给出一个匹配 $M$ ，不可能利用加入更多的边使其进一步扩大。对 $M$ 使用这个算法，如果非临界出现，则 $M$ 有最多可能的边数。若临界出现，则利用关于 $M$ 的交错链，我们构造一个匹配 $M_1$ ，并且它比 $M$ 多一条边。现在对 $M_1$ 再应用这

个算法，等等。最后，我们得到一匹配，对于这个匹配该算法产生非临界。这时匹配有最多可能的边数。我们用一个例子说

明之.

![](images/87b3c405e212a7e3313ba8cac39630ed362f4f963300c46dcb0cc8f9cb5c98ad.jpg)  
图8.8

例 考虑图8.9中所画出的偶图。我们选取边 $[x_2, y_2]$ ，边 $[x_3, y_3]$ 和 $[x_4, y_4]$ 。这三条边形成了不能扩大的匹配 $M$ 。 $M$ 的边在图8.9中用粗体标出。现在我们对 $M$ 应用这个算

法，并分下面几步：

![](images/0f3e94b181bee5d7419f25d2622b09b7f5e58414532bfc99884096f317ec6fe3.jpg)

步1用 $(\ast)$ 标记 $X$ 中所有那些与 $M$ 不相交的结点： $x_{1}, x_{5}, x_{6}$ .

步2 考虑在步1中已标记过的结点 $x_{1}$ ， $x_{5}$ ， $x_{6}$ 。用 $(x_{1})$ 标记 $Y$ 的连接 $x_{1}$ 的所有结点： $y_{3}$ ，用 $(x_{6})$ 标记 $Y$ 的连接 $x_{5}$ 的所有未标记的结点： $y_{4}$ ，用 $(x_{6})$ 标记 $Y$ 的连接 $x_{8}$ 的未标记结点：一个也没有。

步3 考虑在步2中标记过的结点 $y_{3}$ ， $y_{4}$ 。用 $(y_{3})$ 标记 $X$ 的由 $M$ 中的边连接 $y_{3}$ 的所有未标记的结点： $x_{3}$ 。用 $(y_{4})$ 标记 $X$ 的由 $M$ 中的边连接 $y_{4}$ 的所有未标记的结点： $x_{4}$ 。

步4 考虑在步3中标记过结点 $x_{3}, x_{4}$ 。用 $(x_{3})$ 标记 $Y$ 的由不在 $M$ 中的边连接 $x_{3}$ 的所有未标记的结点： $y_{2}$ ，用 $(x_{4})$ 标记 $Y$ 的由不在 $M$ 中的边连接 $x_{4}$ 的所有未标记的结点：一个也没有。

步5 考虑在步4中标记过的结点 $y_{2}$ 。用 $(y_{2})$ 标记 $X$ 的由 $M$ 中的边连接 $y_{2}$ 的所有未标记的结点： $x_{1}$ 。

步6 考虑在步5中标记过的结点 $x_{2}$ ，用 $(x_{2})$ 标记 $Y$ 的由 $M$ 中的边连接 $x_{2}$ 的所有未标记的结点： $y_{1}, y_{6}, y_{8}$

在步6结束时，我们达到临界，因为 $y_{1}, y_{5}, y_{9}$ 中任一个都不与 $M$ 的边相交。考虑这些结点中的一个，比方说 $y_{1}$ 。如果用标记作向导，由 $y_{1}$ 按相反次序追踪我们各步，得到关于 $M$ 的交错链 $y_{1}, x_{2}, y_{2}, x_{3}, y_{3}, x_{1}$ 。若在 $M$ 中用边 $[x_{2}, y_{1}]$ ， $[x_{3}, y_{2}]$ ， $[x_{1}, y_{3}]$ 代替边 $[x_{2}, y_{2}]$ ， $[x_{3}, y_{3}]$ ，得到匹配 $M_{1} = \{ [x_{4}, y_{4}]$ ， $[x_{2}, y_{1}]$ ， $[x_{3}, y_{2}]$ ， $[x_{1}, y_{3}] \}$ 。现在对 $M_{1}$ 应用这个算法。这样作时，就得到图8.10中指出的标记（在任何一步中，在指定标记时都有几个结点要考虑，按下标递增次序进行）。

这时非临界出现，因为进一步标记不可能，并且 $Y$ 的标记

![](images/dabb38e96629bcb42e6e16a75c747afb89189a05320abf96b286a016533fc0cc.jpg)  
图8.10

过的这些结点与 $M_{1}$ 的边相交。 $X$ 的未标记的集合 $X_{1}$ 是 $\{x_{2}, x_{3}\}$ ，而 $Y$ 的已标记的集合 $Y_{1}$ 是 $\{y_{3}, y_{4}\}$ ，集合 $S = X_{1} \cup Y_{1} = \{x_{2}, x_{3}, y_{3}, y_{4}\}$ 覆盖偶图的所有边。 $M_{1}$ 的边数与 $S$ 的结点是一样多。于是， $M_{1}$ 在偶图的所有匹配中有最多可能的边数。

用上述算法的实例来结束本节。考虑一个公司要补充 $n$ 个职位和有 $m$ 个申请者。每个申请者胜任某些职位，但未必胜任所有的职位。每个职位由胜任的申请者担任，可能被承担的职位数最多是多少？当然，没有一个申请者可以担任两个职位。令申请者的集合 $A = \{a_{1}, a_{2}, \dots, a_{m}\}$ 和职位的集合是 $P = \{p_{1}, p_{2}, \dots, p_{n}\}$ ，我们构造一个偶图 $\langle A, \Delta, P \rangle$ ，这里对于 $i = 1, 2, \dots, m$ 和 $j = 1, 2, \dots, n$ ， $a_{i}$ 由边连接到 $p_{j}$ ，当且仅当申请者 $a_{i}$ 胜任职位 $p_{j}$ 。把申请者委派给他胜任的职位，对应于偶图中的一个匹配。于是可被承担的最多职位数等于一个匹配中最多边数。上述算法对于作出最佳委派给出了

一种有效程序.

# 8.4 无限多个集合的情形

在考虑相异代表组时，我们仅仅涉及有限集合族，即是有限个集合的集合族。同样，作为偶图 $\langle X, \Delta, Y \rangle$ 的定义的一部分， $X$ 与 $Y$ 也是有限集合，因而结点个数是有限的。本节简要讨论无限集合族的相异代表组和无限偶图中的匹配。不给出证明<sup>1)</sup>。

令 $I$ 是无限集合，并令 $(A_{i}: i \in I)$ 是由集合 $I$ 标记的集合族。于是，对于 $I$ 的每个元素 $i$ 有对应的集合 $A_{i}$ 。对于这个集合族的相异代表组也是由 $I$ 标记的元素族 $(e_{i}: i \in I)$ ，使得对于 $I$ 中的每个 $i$ ， $e_{i}$ 是在 $A_{i}$ 中，并且只要 $i$ 与 $j$ 不同， $e_{i}$ 与 $e_{j}$ 也不同。这就推广了我们的有限集合族的相异代表组。如果集合族 $(A_{i}: i \in I)$ 要有相异代表组，则下面推广的配偶条件必须成立：对于每个正整数 $k$ 和 $l$ 的 $k$ 个不同元素 $i_{1}, i_{2}, \cdots, i_{k}$ 的每个选择，有

$$
\left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| \geqslant k.
$$

对于无限族来说，保证相异代表组存在的这个条件不再是充分的了。

例 考虑无限集合族 $A_{1}, A_{2}, \cdots, A_{n}, \cdots$ ，这里 $A_{1} = \{1, 2, 3, \cdots\}$ ， $A_{2} = \{1\}$ ， $A_{3} = \{2\}$ ， $\cdots$ ， $A_{i} = \{n - 1\}$ ， $\cdots$ （这个集合族由正整数集合加以标记）。容易看到推广的配偶条件成立。可是，这个集合族仍没有相异

代表组。为验证这一点，注意在任何相异代表组中，1必须代表 $A_{2}$ ，2必须代表 $A_{3}$ ，3必须代表 $A_{4}$ ，等等。但是，没有留下任何元素用来代表 $A_{1}$ 。

尽管这样，我们仍有下面定理，它是由M.Hall，Jr.在1948年发现的1）.

定理8.4.1 令 $I$ 是一个无限集，并令 $(A_{i}; i \in I)$ 是有限集合族，则 $(A_{i}; i \in I)$ 具有一个相异代表组，当且仅当推广的配偶条件成立。

注意到，前面的例子不能应用这个定理，因为集 $A_{1}$ 是无限集。如果用偶图阐述，这个定理成为

定理8.4.2 令 $\langle X, \Delta, Y \rangle$ 是无限偶图，其中 $X$ 的每个结点只与有限条边相交。那么存在匹配 $M$ ，使得 $X$ 中每个结点与 $M$ 的一条边相交，当且仅当对于每个正整数 $k$ ，以及 $X$ 的 $k$ 个结点的每个集合 $A$ ， $Y$ 的至少连接 $A$ 中一个结点的结点数不小于 $k$ 。

现在令 $\langle X, \Delta, Y \rangle$ 是这种无限偶图，使得 $X$ 和 $Y$ 中的每个结点只与有限多条边相交。假设对于每个正整数 $k$ 和 $X$ 的每 $k$ 个结点集合 $A$ ， $Y$ 的连接 $A$ 中某个结点的结点数至少是 $k$ 。同时假定对于每个正整数 $k$ 和 $Y$ 的每 $k$ 个结点的集合 $B$ ， $X$ 的连接 $B$ 中某个结点的结点数至少是 $k$ 。由定理8.4.2可得，存在匹配 $M_1$ 与 $M_2$ 使 $X$ 的每个结点与 $M_1$ 的边相交，以及 $Y$ 的每个结点与 $M_2$ 的边相交。能够找到“匹配” $X$ 和 $Y$ 的这种匹配吗？即是找到匹配 $M$ ，使 $X$ 的每个结点与 $M$ 的一边相交，并且 $Y$ 的每个结点也与 $M$ 的一边相交。在这种匹配中 $X$ 与 $Y$ 的结点是对

偶的.

定理8.4.3 令 $\langle X, \Delta, Y \rangle$ 是无限偶图。假设存在匹配 $M_{1}$ 使 $X$ 的每个结点与 $M_{1}$ 的一边相交，匹配 $M_{2}$ 使 $Y$ 的每个结点与 $M_{2}$ 的一边相交。那么存在一个匹配 $M$ 使得 $X$ 的每个结点和 $Y$ 的每个结点与 $M$ 的一边相交。

如果在这个定理中，偶图 $\langle X, \Delta, Y \rangle$ 是有限的，那么匹配 $M_{1}$ 和 $M_{2}$ 的存在，意味着 $X$ 和 $Y$ 有同样多个结点。于是，在这种情况下，匹配 $M_{1}$ 或 $M_{2}$ 可取作这个定理中的 $M_{\bullet}$

与无限的剪过棋盘有关的偶图，满足在定理8.4.3以前讨论过的有限条件。考虑把一平板分成一些单位正方形，并交替地着黑色和白色。剪掉某些正方形，留下一个无限剪过的板。每个白色正方形至多与4个黑色正方形有公共边。同样，每个黑色正方形至多与4个白色正方形有公共边。于是，在有关的偶图中，每个结点至多与4个边相交。如果我们应用定理8.4.3我们得到下列结果：

如果能够不重迭地把多米诺骨牌放置在剪过的无限棋盘上，使得覆盖所有白色正方形；并且如果能够不重迭地把多米诺骨牌放置在剪过的无限棋盘上，使得覆盖所有黑色正方形。则剪过的无限棋盘有一个完全覆盖。

把它与定理8.4.2结合起来，得到下面结果。

定理8.4.4 一个剪过的无限棋盘有多米诺骨牌的完全覆盖，当且仅当满足下面两个条件：

(i) 对每个正整数 $k$ 和每 $k$ 个白色正方形集合 $A_{0}$ , 至少与 $A$ 中一个白色正方形有公共边的黑色正方形的数目不小于 $k$ .  
(ii）对每个正整数 $k$ 和每 $k$ 个黑色正方形的集合 $C$ ，至少与 $C$ 中一个黑色正方形有公共边的白色正方形的数目不小于 $k$

# 练习

1. 对下列每个集合族，确定其代表组。若不存在代表组，说明为什么？对于相异代表组重复解答上面问题。

(a) $A_{1} = \{1,2,3\}$ , $A_{2} = \{2,3,4\}$ , $A_{3} = \emptyset$ , $A_{4} = \{1\}$ .   
(b) $A_{1} = \{1, 2, 3\}$ , $A_{2} = \{2, 3\}$ , $A_{3} = \{4\}$ , $A_{5} = \{1, 2\}$ .   
(c) $A_{1} = \{1, 2, 3\}$ , $A_{2} = \{1, 2, 3\}$ , $A_{3} = \{1, 2, 3\}$ , $A_{4} = \{1, 2, 3\}$ .   
(d) $A_{1} = \{1, 2, 3, 4, 5\}$ , $A_{2} = \{1, 3\}$ , $A_{3} = \{1, 3\}$ , $A_{4} = \{1, 2, 3, 4, 5\}$ , $A_{5} = \{1, 3\}$ .

2. 确定集合族 $A_{1} = \{1, 2\}$ , $A_{2} = \{2, 3\}$ , $A_{3} = \{3, 4\}$ , $A_{4} = \{4, 5\}$ , $A_{5} = \{5, 1\}$ 的不同的相异代表组的数目，并推广到 $n$ 个集合的情形。  
3. 令 $A_{1}, A_{2}, \cdots, A_{n}$ 是由 $A_{1} = A_{2} = \cdots = A_{k} = \{1, 2, \cdots, k - 1\}$ 和 $A_{k - 1} = \cdots = A_{n} = \{1, 2, \cdots, n\}$ 定义为 $\{1, 2, \cdots, n\}$ 的子集合族，这里 $k$ 是某个整数且 $1 \leqslant k \leqslant n$ 。证明配偶条件的 $2^{n - 1}$ 个不等式中恰有一个不满足。  
4. 对下列集合族中的每一个，使用定理8.1.3，确定有相异代表组的那个集合族中的集合的最大数目。

$$
A _ {5} = \{1, 2, 3, 4, 5 \}, A _ {6} = \{1, 2 \}.
$$

(a) $A_{1} = \{1,2\}$ , $A_{2} = \{3,4\}$ , $A_{3} = \{1,2\}$ , $A_{4} = \{1\}$ ,   
$\sim (4)$ $A_{1} = \{1,2\} ,A_{2} = \{2,3\} ,A_{3} = \{4,5\} ,A_{4} = \{2,3,4\} ,$ $A_{6} = \{1,2,3\} ,A_{8} = \{1,3\} .$   
(c) $A_{1} = \{1, 2, 3\}$ , $A_{2} = \{1, 2, 3\}$ , $A_{3} = \{1, 2, 3\}$ , $A_{4} = \{1, 2, 3\}$ , $A_{5} = \{1, 2, 3, 4, 5, 6\}$ , $A_{6} = \{1, 2, 3, 4, 5, 6\}$ .   
(d) $A_{1} = \{1, 2, 3\}$ , $A_{2} = \{2, 3\}$ , $A_{3} = \{2\}$ , $A_{4} = \{2, 5, 6\}$ , $A_{5} = \{1, 5\}$ , $A_{6} = \{3, 6\}$ .

5. 令 $A_{1}, A_{2}, \cdots, A_{n}$ 是一集合族，并假定这个集合族有相异代表组，令 $x$ 至少是这些集合之一中的元素。证明 $A_{1}, A_{2}, \cdots, A_{n}$ 有一个含有元素 $x$ 的相异代表组，举例说明，一般地规定含有 $x$ 的集合要由 $x$

![](images/3526330559cdd77d6d6ebe211465186d3ff119b3c8e7cde02e78f40bf8597d8e.jpg)

代表是不可能的.

6. 令 $A_{1}, A_{2}, \cdots, A_{n}$ 是一集合族，使得对每个 $k = 1, 2, \cdots, n$ 和对每次选择 $i_{1}, i_{2}, \cdots, i_{k}$ ，其中 $1 \leqslant i_{1} < i_{2} < \cdots < i_{k} \leqslant n$ ，有

$$
\left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| \geqslant k + 1.
$$

令 $x$ 是 $A_{1}$ 的任何元素。证明 $A_{1}, A_{2}, \cdots, A_{n}$ 有一个 $x$ 代表 $A_{1}$ 的相异代表组，即是有形如 $x, e_{2}, \cdots, e_{n}$ 的相异代表组。

7. 令 $A_{1}, A_{2}, \cdots, A_{n}$ 是一集合族，这里对所有的 $i = 1, 2, \cdots, n$ 。 $A_{i} = \{1, 2, \cdots, m\}$ ，证明 $A_{1}, A_{2}, \cdots, A_{n}$ 有相异代表组当且仅当 $m \geqslant n$ 。如果 $m \geqslant n$ ，证明不同的相异代表组的个数等于

$$
P (m, n) = m (m - 1) \dots (m - n + 1).
$$

8. 令 $A_{1}, A_{2}, \cdots, A_{n}$ 是 $A_{1} = \{2, 3, \cdots, n\}, A_{2} = \{1, 3, 4, \cdots, n\}, A_{3} = \{1, 2, 4, 5, \cdots, n\}, \cdots, A_{n} = \{1, 2, \cdots, n - 1\}$ 的集合族，一般地， $A_{i} = \{1, 2, \cdots, n\} - \{i\}$ ，证明 $A_{1}, A_{2}, \cdots, A_{n}$ 有相异代表组，并且不同的相异代表组的个数等于第 $n$ 个重排列数 $D_{n}$ 。

9. 构造一个集合族，它有相异代表组当且仅当如下图所示的剪过棋盘有一完全覆盖，试求相异代表组及相应的完全覆盖。

10. 在 $m \times n$ 棋盘中，这里 $m$ 与 $n$ 都是奇数，着一种颜色（比如说白色）的正方形比别的颜色（黑色）多一个。证明如果恰好把一个白色正方形从棋盘上剪掉，得到剪过棋盘有一完全覆盖。

11*. 在 $m$ 与 $n$ 至少有一个为偶数的 $m \times n$ 棋盘中, 有相等个数的白色与黑色正方形. 证明若 $m$ 与 $u$ 至少为 2, 那么从棋盘上无论怎样剪掉一

![](images/fcda682f0a1942d33917e1350fe07618342c40044d8ed7c837e1320b869e8db0.jpg)

个白色和一个黑色正方形，得到剪过的棋盘有一完全覆盖。

12. 构造与练习9中剪过棋盘有关的偶图，确定对应那里所求出的完全覆盖的匹配。  
13. 构造对应于图8.4中偶图的集合族，确定有相异代表组的这些集合中四个集合的所有集合族。   
14. 考虑下面画出的偶图。证明在匹配中最多边数是 3，证明匹配 $M = \{(x_{1}, y_{1}), (x_{2}, y_{2})\}$ 不能扩大到由 3 条边组成的匹配。求关于 $M$ 的交错链，并用它修改 $M$ 给出 3 条边的一个匹配。

![](images/5431239a02d31fecc1510382b466dea4e99cc72bb183e53f297c9356826cf2d1.jpg)

15. 令 $M$ 是偶图中一匹配。令 $S$ 由与 $M$ 相交的所有结点组成，证明下面说法等价：（i）不存在包含 $M$ 且 $|M'| > |M|$ 的匹配 $M'$ 。（ii） $S$ 覆盖偶图的边。（iii）不存在长为 1 的关于 $M$ 的交错链。  
16. 令 $\langle X_{\mathfrak{q}}, \Delta_{\mathfrak{q}}, Y_{\mathfrak{q}} \rangle$ 是一偶图。假设有正整数 $p$ ，使 $X_{\mathfrak{q}}$ 中的每个结点至少与 $p$ 条边相交，而 $Y_{\mathfrak{q}}$ 中的每个结点至多与 $p$ 条边相交，用两种不同方法计算偶图中的边，证明 $|X_{\mathfrak{q}}| \leqslant |Y_{\mathfrak{q}}|$ 。  
17. 令 $\langle X, \Delta, Y \rangle$ 是一偶图，使 $X$ 含有 $m$ 个结点， $Y$ 含有 $n$ 个结点。假定有正整数 $p$ ，使 $X$ 的每个结点至少与 $p$ 条边相交，而 $Y$ 的每个结点至多与 $p$ 条边相交。利用练习16，证明覆盖偶图的边的结点集合不能少于 $m$ 个结点，断定存在 $m$ 条边的匹配，并且 $m$ 是匹配中最多边数。  
18. 在一次晚会中，有 $n$ 个男孩和 $n$ 个女孩。假定存在整数 $p$ ，使得每个男孩恰好熟识 $p$ 个女孩，而每个女孩恰好熟识 $p$ 个男孩。利用练习17证明这些男孩和女孩可以配对，使得在每对中男孩和女孩都是熟识

的.

19\*、令 $A_{1},A_{2},\dots ,A_{n}$ 是一集合族，又令 $r_1,r_2,\dots ,r_n$ 是正整数.证明存在集合族 $B_{1}$ ， $B_{2}$ ，…， $B_{n}$ 其中对于 $i = 1,2,\dots ,n$ 有 $B_{i}\subseteq A_{i}$ ， $\mid B_i\mid = r_i$ 以及对于所有不同于j的i,有 $B_{i}\cap B_{j} =$ $\varnothing$ ，当且仅当对于每个 $k = 1,2,\dots ,n$ 和对每次选择 $i_1,i_2,\dots ,i_k$ 其中 $1\leqslant i_1 <   i_2 <   \dots <  i_k\leqslant n$ ，有

$$
\left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| \geqslant r _ {i _ {1}} + r _ {i _ {2}} + \dots + r _ {i _ {k}}.
$$

指出配偶定理是这个结果的特殊情形。（提示：证明集合族 $B_{1}$ ， $B_{2}$ ，…， $B_{n}$ 存在，当且仅当集族

$$
\underbrace {A _ {1} , \cdots , A _ {1}} _ {r _ {1}}, \underbrace {A _ {2} , \cdots , A _ {2}} _ {r _ {2}}, \dots , \underbrace {A _ {n} , \cdots , A _ {n}} _ {r _ {n}}
$$

有相异代表组，然后再应用配偶定理。）

$20^{*}$ 令 $A_{1}, A_{2}, \cdots, A_{n}$ 和 $B_{1}, B_{2}, \cdots, B_{n}$ 是集合 $E$ 的两个划分。证明存在 $\{1, 2, \cdots, n\}$ 的排列 $j_{1}, j_{2}, \cdots, j_{n}$ ，使

$$
A _ {1} \cap B _ {j _ {1}} \neq \emptyset , A _ {2} \cap B _ {j _ {2}} \neq \emptyset , \dots , A _ {n} \cap B _ {j _ {n}} \neq \emptyset
$$

当且仅当对于每个 $k = 1,2,\dots ,n$ ，集合 $A_{1},A_{2},\dots ,A_{n}$ 中的任何 $k$ 个集合不包含在集合 $B_{1},B_{2},\dots ,B_{n}$ 中的 $k - 1$ 个集合的并．（提示：构造一个偶图，其结点对应于 $A_{1},A_{2},\dots ,A_{n}$ 与 $B_{1},B_{2},\dots ,$ $B_{n}$ ，当且仅当 $A_{i}\cap B_{j}\neq \emptyset$ 时，有一条边连接 $A_{i}$ 与 $B_{j}$ ：）

21*.（续练习20）假定有正整数 $p$ ，使得对于 $i = 1, 2, \dots, n$ 有 $|A_i| = p$ 及 $|B_i| = p$ 。证明存在 $\{1, 2, \dots, n\}$ 的排列 $j_1, j_2, \dots, j_n$ 使得

$$
A _ {1} \cap B _ {j _ {1}} \neq \emptyset , A _ {2} \cap B _ {j _ {2}} \neq \emptyset , \dots , A _ {n} \cap B _ {j _ {n}} \neq \emptyset .
$$

22. 求集合族 $A_{1}, A_{2}, \cdots, A_{10}$ 中有相异代表组的集合的最多数目，这里

$$
A _ {1} = \{1, 8, 1 0, 1 3 \} \quad A _ {6} = \{1, 4, 5, 7, 1 1 \},
$$

$$
A _ {2} = \{5, 8 \}. \quad A _ {7} = \{8, 1 3 \},
$$

$$
A _ {3} = \{2, 3, 4, 1 1, 1 2 \}, \quad A _ {8} = \{5, 6, 1 0, 1 3 \},
$$

$$
A _ {4} = \{1 0, 1 3 \}, \quad A _ {9} = \{5, 8, 1 0, 1 3 \},
$$

$$
A _ {5} = \{1, 5, 8 \}, \quad A _ {1 0} = \{1, 5, 8, 1 0, 1 3 \}.
$$

23. 用8.3节中讨论的方法确定在下面偶图中匹配 $M$ 的最多边数，在每种情况下，确定覆盖边的结点集合 $S$ ，这里 $|S| = |M|$

![](images/80ba4c5aebd2b38aada4ffa497acc24a7f3605d6d91aa6d8b9d03447dc0e9323.jpg)  
(a)

![](images/957e5adb1e9ed8f414ad0e911ad6ffadfbe65770b24a7ec2a365001a28958afc.jpg)  
（b）

24. 一公司有7个缺位 $p_1, p_2, \cdots, p_7$ 和10个申请者 $a_1, a_2, \cdots, a_{10}$ . 每个申请者胜任职位集合分别为 $\{p_1, p_6, p_8\}, \{p_2, p_6, p_7\}, \{p_3, p_4\}, \{p_1, p_8\}, \{p_6, p_7\}, \{p_3\}, \{p_2, p_3\}, \{p_1, p_3\}, \{p_1\}, \{p_6\}$ . 确定由胜任的申请者可以担任的最多职位数.  
25*. 令 $\langle X, \Delta, Y \rangle$ 是无限偶图，又令 $M$ 是匹配。无限链是这种不同结点的无限序列，使相邻结点由一条边连接。关于 $M$ 的无限交错链 $\gamma$ 是一无限链，使 $\gamma$ 的第一、第三、第五、一条边不在 $M$ 中， $\gamma$ 的第二、第四、一条边在 $M$ 中，并且 $\gamma$ 的第一个结点不与 $M$ 的一边相交。（注意，因为 $\gamma$ 是无限序列，故无最末结点。）用交错链和无限交错链来证明定理8.4.3。  
26. 试给出一例，在剪过的无限棋盘上使所有白色正方形由不重迭的多米诺骨牌覆盖，而黑色正方形不由不重迭的多米诺骨牌所覆盖。

# 第九章 组合设计

本章我们研究几个称为组合设计，或简称设计的例子，所谓设计就是安排某些事物满足所规定的要求。设计的研究是组合论中重要的且范围很广的一部分。在第一章扼要地讨论过组合设计的两个例子，即幻方和拉丁方。本章我们将更全面地研究拉丁方和正交拉丁方。相异代表组（见第八章）可以认为是某一类设计的研究。求集合 $A_{1}$ ， $A_{2}$ ，…， $A_{n}$ 的相异代表组最终说来是求 $n$ 个不同元素 $e_{1}$ ， $e_{2}$ ，…， $e_{n}$ 的一种安排。其中 $e_{1}$ 在 $A_{1}$ 中， $e_{2}$ 在 $A_{2}$ 中，等等。不过，本章感兴趣的一类设计，拉丁方尤其是正交拉丁方是这方面较好的例子。这类设计与将要提出的一些几何概念有着密切的联系。

构造组合设计的许多方法依赖于有限多个数的算术的某些知识。开始对这方面简略地进行一些讨论。学过近世代数的读者只要浏览一下甚至可以跳过9.1节。

# 9.1 有限域

我们要定义的代数结构的原型是具有加法和乘法两个二元运算的有理数集合。这两种二元运算具有许多性质，例如结合性、交换性和分配性。0 和 1 这两个有理数分别在加法和乘法方面起着特殊的作用。用这些基本性质去定义称为域的代数结构。虽然有理数是一个无限域，将会看到也存在只具有有限多个元素的域。

设 $F$ 是非空集合， $F$ 上的二元运算。是 $F$ 的函数，它使 $F$ 的元素的每一有序对 $a, b, F$ 有一个元素 $a \circ b$ 与之相应，通常

的加法 $(+)$ 、乘法 $(\times)$ 和减法 $(-)$ 都是有理数集合上的二元运算。一个域是指一个至少有两个元素的集合 $F$ 且在其上定义了两个二元运算，用 $+$ 和 $\times$ 表示，分别称为加法和乘法。它们具有下面诸性质。

# 1. 结合性

对于 $F$ 中所有的 $a, b, c$ 有

$$
a + (b + c) = (a + b) + c \text {和} a \times (b \times c) = (a \times b) \times c.
$$

# 2. 交换性

对于 $F$ 中所有 $\pmb{a}$ ， $\textit{\textbf{b}}$ 有

$$
a + b = b + a \text {和} a \times b = b \times a.
$$

# 3. 存在幺元

(i) 存在一个特殊元素，用 0 表示并称为加法幺元，使得对于 $F$ 中所有 $a$

$$
a + 0 = 0 + a = a _ {\bullet}
$$

(ii) 存在一个特殊元素，用1表示并称为乘法幺元，使得对于 $F$ 中所有 $a$

$$
a \times 1 = 1 \times a = a _ {\bullet}
$$

# 4. 存在逆元

(i) 对 $F$ 中每个 $a, F$ 中存在元素 $a^{*}$ , 使得

$$
a + a ^ {*} = a ^ {*} + a = 0.
$$

元素 $a^*$ 叫做 $\pmb{a}$ 的加法逆元.

(ii）对 $F$ 中每个非零 $\pmb{a}$ ， $F$ 中存在元素 $\hat{\alpha}$ ，使得

$$
a \times \hat {a} = \hat {a} \times a = 1.
$$

元素 $\hat{a}$ 称为 $a$ 的乘法逆元。

# 5. 分配性

对于 $F$ 中所有 $a, b, c$ 有

$$
a \times (b + c) = (a \times b) + (a \times a)
$$

和 $(b + c) \times a = (b \times a) + (c \times a)$ .

请注意，分配性是联系加法和乘法两运算的唯一性质。对于有理数、实数和复数说来，上面这些性质都是熟知的。每一个这样的集合都构成一个域。这里加法和乘法是这些数的通常加法和乘法。具有通常加法和乘法的整数集合 $Z$ 不形成一个域。例如2在 $Z$ 中没有乘法逆元。

从定义一个域的一些性质得到许多初等的但又是重要的事实，现在把它们集在一起，在几乎任何一本近世代数的书中都可查到它们，或者读者也可以自己去证明。对一个域通常用它的元素集合 $F$ 表示，虽然这种表示是含糊不清的，因为一般说来在 $F$ 上有一种以上的方法定义加法和乘法使它形成一个域，但如果我们已将一个固定的加法和乘法记在心里，这就不会引起任何混乱。

定理9.1.1 下列性质在任何域 $F$ 中成立：

6 $0 \neq 1$

7 递元的唯一性

(i) $F$ 的每个元素a有唯一加法逆元，现在用 $-a$ 表示它。

(ii) $F$ 的每个非零元素a有唯一乘法逆元，现在用 $\pmb{a}^{-1}$ 或 $1 / a$ 表示它.

8 对于 $F$ 中所有 $a, b$ ,

$$
\begin{array}{l} (- a) \times b = a \times (- b) = - (a \times b) \text {和} (- a) \times (- b) \\ = a \times b. \\ \end{array}
$$

9 对于 $F$ 中的所有 $\pmb{a}$

$$
- (- a) = a.
$$

10 对于 $F$ 中的所有 $a$ ,

$$
a \times 0 = 0 \times a = 0.
$$

11 逆元的分配性

(i) 对于 $F$ 中的所有 $a, b$ ,

$$
- (a + b) = (- a) + (- b).
$$

(ii) 对于 $F$ 中所有非零的 $a, b$ ,

$$
(a \times b) ^ {- 1} = a ^ {- 1} \times b ^ {- 1},
$$

12 方程的解

(i) 对于 $F$ 中的所有 $a, b$ , 方程

$$
\boldsymbol {a} + \boldsymbol {x} = \boldsymbol {b}
$$

在 $F$ 中有唯一的解， $x = (-a) + b$ .

(ii) 对于 $F$ 中所有的 $a, b$ 且 $a \neq 0$ , 方程

$$
a \times y = b
$$

在 $F$ 中有唯一解， $y = a^{-1} \times b$ .

13 相消性

(i) 对于 $F$ 中所有 $a, b, c, a + b = a + c$ 蕴含 $b = c$ .

(ii) 对于 $F$ 中所有 $a, b, c$ 且 $a \neq 0, a \times b = a \times c$ 推出 $b = c$ .

14 对于 $F$ 中所有 $a, b$ ,

$$
a \times b = 0 \quad \text {推 出} \quad a = 0 \quad \text {或} \quad b = 0.
$$

这些性质很多是相互有联系的。例如，14不难从性质13(ii)或12(ii)得到。关于性质12(i)，注意到交换性， $(-a) + b = b + (-a)$ ，并且对于 $b + (-a)$ 通常写成 $b - a$ 。后者恰可作为 $\pmb{F}$ 中减法的定义，同样， $b / a = a^{-1} \times b = b \times a^{-1}$ 是 $\pmb{F}$ 中的除法定义，而且常常略去乘积 $a \times b$ 中的乘法符号写成 $ab$ 。

就组合学的应用来说，涉及的域，其集合的元素个数是有限的，称它们为有限域。由定义，一个域至少有两个元素。从性质6得知，0与1是 $F$ 的不同元素。

例 设 $F$ 是由两个元素 0 与 1 组成的集合并且加法和乘法由下表定义：

$$
\begin{array}{c c c} + & 0 & 1 \\ \hline 0 & 0 & 1 \\ 1 & 1 & 0 \end{array} \qquad \qquad \begin{array}{c c c} \times & 0 & 1 \\ \hline 0 & 0 & 0 \\ 1 & 0 & 1 \end{array} .
$$

注意，这些规则除 $1 + 1 = 0$ 外和整数 0 与 1 的通常相加及相乘的规则一样。显然对于 $1 + 1$ 不能使用通常的加法，因为 2 不在 $F$ 中。不难证明 $F$ 连同这些规则构成一个域。由于 $1 + 1 = 0$ 故推得 $\cdots 1 = 1$ 。

这个例子可以推广去构造无限多个有限域。令 $n$ 是至少为 2 的整数，又令 $Z_{1} = \{0, 1, 2, \cdots, n - 1\}$ ，我们希望在 $Z_{n}$ 上定义加法和乘法。有时可以使用通常的加法和乘法，但在另外一些场合通常的加法和乘法所得到的整数不在 $Z_{n}$ 中。例如，如果 $n = 7$ ，则 $2 + 3 = 5$ 是在 $Z_{7}$ 中但 $4 + 5 = 9$ 就不在 $Z_{7}$ 内。可以修改如下。设 $a, b$ 是在 $Z_{n}$ 中， $c$ 是对 $a$ 与 $b$ 施行通常的整数加法所得到的结果。若 $c$ 在 $Z_{n}$ 中，就定义 $a + b = c$ 。否则 $c \geqslant n$ ，就减去 $n$ 的足够大倍数得到一个在 $Z_{n}$ 中的整数 $r$ 。换句话说，用 $n$ 除 $c$ 得到 0 与 $n - 1$ 之间的余数 $r$ ：

$$
c = q n + r, \quad 0 \leqslant r \leqslant n - 1.
$$

这时定义 $a + b = r$ 。于是 $Z_{n}$ 中的加法定义为 $a$ 与 $b$ 的通常的整数之和除以 $n$ 的余数。同样，设 $s$ 是 $a$ 与 $b$ 的通常整数乘积除以 $n$ 的余数， $s$ 在 $Z_{n}$ 中，定义 $a \times b = s$ 。根据定义，这个加法和乘法都是 $Z_{n}$ 上的二元运算。 $Z_{n}$ 的这个加法和乘法常常分别称为模 $n$ 加法和模 $n$ 乘法。

例 $Z_{11}$ 中的加法和乘法满足：

（i） $4 + 9 = 2$ ，因为4与9的通常整数和是13，而13除以11的余数为2.

(ii) $3 + 8 = 0$ ，因为3与8的通常整数和是11，而11除

以11的余数为0。注意，在 $Z_{11}$ 中这个方程意味着 $-3 = 8$ 和 $-8 = 3$ 。

（iii） $5 \times 7 = 2$ ，因为5与7的通常整数乘积是35，而35除以11的余数是2。  
(iv) $3 \times 4 = 1$ ，因为3与4的通常整数乘积是12，而12除以11的余数为1。注意，这个方程意味着 $3^{-1} = 4$ 和 $4^{-1} = 3$ 或 $1/3 = 4$ 和 $1/4 = 3$ 。

虽然整数集合 $Z$ 使用通常的加法和乘法不能形成域，但设想 $Z_{n}$ 是一个域是合理的。从上面例子我们看到，尽管3在 $Z$ 中无乘法逆元，在 $Z_{11}$ 中3是有乘法逆元的，即4。但是，并不能期望对所有整数 $n \geqslant 2$ ， $Z_{n}$ 都是域。例如 $Z_{6}$ ，我们有 $2 \times 3 = 0$ ，因为2与3的通常的整数乘积是6，它除以6余数为0，于是根据性质14， $Z_{6}$ 不能是域。事实上，从方程 $2 \times 3 = 0$ 可得2和3在 $Z_{6}$ 中都没有乘法逆元。（假若在 $Z_{6}$ 中 $2^{-1}$ ，则 $2^{-1} \times (2 \times 3) = 2^{-1} \times 0 = 0$ 。利用性质1和3，得到 $3 = 0$ ，这是不合理的。）或许读者从这些讨论中已猜到 $Z_{n}$ 是否形成一个域依赖于 $n$ 的因子分解。确实是如此，并且有下面的定理成立。

定理9.1.2 设 $n$ 是不小于 2 的整数。那么用模 $n$ 加法与模 $n$ 系乘法运算， $Z_{n}$ 是一个域当且仅当 $n$ 是一个素数。

证明 这个定理的详细证明是相当冗长的。模 $n$ 加法与乘法的结合性，交换性和分配性从整数加法和乘法的相应性质得到。同样，整数 0 与 1 分别是 $Z_{n}$ 的加法和乘法幺元。如果 $a \neq 0$ 在 $Z_{n}$ 中，整数 $n - a$ 也同样在 $Z_{n}$ 中。于是，对于模 $n$ 加法， $a + (n - a) = 0$ 并且 $n - a$ 是 $a$ 在 $Z_{n}$ 中的加法逆元。所有这些性质不管 $n$ 是否为素数都成立。这样，唯一还要考虑的性质是乘法逆元的存在。

首先假定 $n$ 不是素数，则 $n$ 能分解成两个整数 $a$ 与 $b$ 之积，

这里 $1 < a, b < n$ 。整数 $a$ 与 $b$ 都属于 $Z_{n}$ ，利用模 $n$ 乘法， $ab = 0$ 。因而， $Z_{i}$ 不是域。

现在设 $n$ 是一素数并令 $a$ 是 $Z_{n}$ 的一个非零元素. 于是, $1 \leqslant a \leqslant n - 1$ . 因为 $n$ 是素数, 可见 $a$ 与 $n$ 的最大公因子是 1. 利用 Euclidean 算法 (见练习 5), 便知有整数 $x$ 和 $y$ 使得

$$
a x + n y = 1. \tag {9.1.1}
$$

但 $x$ 不可能是 $n$ 的倍数，因为否则的话，从上面方程我们断定1是 $n$ 的倍数。这是不可能的，因为 $n \geqslant 2$ 。因而当 $x$ 除以 $n$ 时有非零余数，并且存在整数 $q$ 与 $r$ 使 $x = qn + r$ ，这里 $1 \leqslant r < n - 1$ 。将它代入方程（9.1.1）得

$$
a r + (q a + y) n = 1,
$$

或

$$
a r = (- q a - y) n + 1.
$$

这就意味着，利用模 $n$ 乘法， $ar = 1$ ，从而 $r$ 是 $a$ 在 $Z_{n}$ 中的乘法逆元。因此 $Z_{n}$ 是一个域。

例求在 $Z_{23}$ 中的乘法逆元.

Encludean 算法给出如下等式：

$$
2 3 = 4 (5) + 3, \quad 5 = 1 (3) + 2, \quad 3 = 1 (2) + 1.
$$

按相反次序使用这些方程，得到

$$
\begin{array}{l} 1 = 3 - 1 (2) = 3 - 1 [ 5 - 1 (3) ] = 2 (3) - 5 \\ = 2 [ 2 3 - 4 (5) ] - 5 = 2 (2 3) - 9 (5). \\ \end{array}
$$

因而5在 $Z_{23}$ 中的乘法逆元是 $-9 = 14$ ，对此验证如下，我们看到，对5与14施行整数乘法是 $5 \times 14 = 70 = 3 \times 23 + 1$ ，这样5与14的整数模-23乘法之积是1。

根据定理9.1.2，如果 $n$ 不是素数，则用模 $n$ 加法和乘法 $Z_{n}$ 不能形成一个域。但这并不意味着无法在 $n$ 个元素的集合上去定义一个加法和乘法使它成为一个域。我们将会看到存在与 $Z_{p}$

不同的有限域，这里 $p$ 是素数，但是一个有限域中的元素个数总是一个素数的正整数幂。例如，不存在只有6个元素的域。

如何构造与 $Z_{p}$ （ $p$ 为素数）不同的有限域呢？在讨论这个问题之前，想一下怎样从实数域 $\mathbb{R}$ 构造复数域 $\mathbb{C}$ ，多项式 $z^2 + 1$ 无实根。令 $i$ 是一个符号，并形式地认为它是 $z^2 + 1$ 的根： $i^2 + 1 = 0$ 或 $i^2 = -1$ 。然后再考虑形如 $a + bi$ 的所有表达式，这里 $a$ 与 $b$ 都是实数。这些表达式的加法由

$$
(a + b i) + (c + d i) = (a + c) + (b + d) i
$$

来定义。由于 $a + c$ 与 $b + d$ 是实数之和，这个加法是利用实数的加法来定义的。在假定结合性、交换性和分配性都成立的前提下，乘法由 $i^3 = -1$ 确定：

$$
\begin{array}{l} (a + b i) (c + d i) = a (c + d i) + (b i) (c + d i) \\ = a c + a (d i) + (b i) c + (b i) (d i) \\ = a c + (a d) i + (b c) i + (b d) i ^ {2} \\ = a c + (a d + b c) i - b d \\ = (a c - b d) + (a d + b c) i. \\ \end{array}
$$

这样导致我们得到乘法定义

$$
(a + b i) (c + d i) = (a c - b d) + (a d + b c) i.
$$

当然，尚未证明形如 $a + bi$ 的符号即复数的集合使用这些加法和乘法构成一个域，还要验证定义一个域的那些性质是成立的。可以推广上面的思想从域 $Z_{p}$ （ $p$ 是素数）去构造新的域。

对于一个域 $F$ ，我们用 $F[z]$ 表示系数在 $F$ 中的所有 $\pmb{z}$ 的多项式集合，回顾一下关于 $F[z]$ 的一些基本性质。 $F[z]$ 中的每一个多项式可以写成 $a_0 + a_1z + a_2z^2 + \dots + a_nz^n$ ，的形式，这里 $a_0, a_1, a_2, \dots, a_n$ 在 $F$ 中，这不是一个多项式的唯一表达式，因为如果 $m \geqslant n$ ，则 $a_0 + a_1z^1 + a_2z^2 + \dots + a_mz^m$ ，这里 $a_{n+1} = \dots = a_m = 0$ ，是与 $a_0 + a_1z + a_2z^2 + \dots + a_nb$ 表示同一个多

项式。如果 $p(z) = a_0 + a_1z + \dots +a_nz^n$ 是 $F(z)$ 中的一个多项式且 $a_{n}\neq 0$ ，则 $n$ 称为 $p(z)$ 的次数并记为 $\deg p(z)$ 。这样，对每一个非零多项式都确定了一个次数。零多项式 $p(z) = 0$ 的次数定义为-1.于是，常数多项式是次数为0或小于0的那些多项式；非零常数多项式是次数为0的那些多项式。 $F(z)$ 中的多项式加法和乘法定义同实系数多项式加法和乘法一样。设 $f(z)$ 和 $g(z)$ 是 $F(z)$ 中的多项式，因为我们总能包括更多的零系数，故可设 $f(z) = a_{0} + a_{1}z + a_{2}z^{2} + \dots +a_{n}z^{n}$ 和 $g(z) = b_{0}+$ $b_{1}z + b_{2}z^{2} + \dots +b_{n}z^{n}$ 。那么

$$
\begin{array}{l} f (z) + g (z) = \left(a _ {0} + b _ {0}\right) + \left(a _ {1} + b _ {1}\right) z + \left(a _ {2} + b _ {2}\right) z ^ {2} \\ + \dots + (a _ {n} + b _ {n}) z ^ {n}, \\ \end{array}
$$

和

$f(z)g(z) = a_{0}b_{0} + (a_{0}b_{1} + a_{1}b_{0})z + (a_{0}b_{2} + a_{1}b_{1} + a_{2}b_{0})z^{2}+\cdots + a_{n}b_{n}z^{n}$ . 对于非零多项式 $f(z)$ 和 $g(z)$ 不难推出 $\deg f(z) \times g(z) = \deg f(z) + \deg g(z)$ . 对于所定义的加法和乘法，一个域的所有规定的性质除了乘法的逆元存在性外都为 $F[z]$ 所满足，验证这一点虽然很繁琐却是容易做到的。正如整数一样， $F[z]$ 也有一个带余除法。

如果 $f(z)$ 和 $g(z)$ 是 $F[z]$ 中的两个多项式且 $g(z) \neq 0$ ，则在 $F[z]$ 中存在多项式 $a(z)$ 和 $r(z)$ 使得

$$
f (z) = a (z) g (z) + r (z),
$$

其中 $\deg r(z) <   \deg g(z)$

多项式 $r(z)$ 称为 $f(z)$ 除以 $g(z)$ 的余式。这个证明留作一个练习，也可以在几乎任何一本近世代数书中找到它。

设 $f(z)$ 是 $F[z]$ 中一个非常数多项式，如果 $f(z)$ 在 $F[z]$ 中不能分解成两个非常数多项式，则 $f(z)$ 称为在 $F[z]$ 中不可约的或在 $F$ 上不可约的。于是， $f(z)$ 在 $F[z]$ 中是不可约的当且

仅当对于 $F[z]$ 中的多项式 $a(z)$ 和 $b(z)$ ，由 $f(z) = a(z)b(z)$ 推出 $\deg a(z) = 0$ 或 $\deg b(z) = 0$ 。 $F[z]$ 的1次多项式总是不可约的， $F[z]$ 中2次多项式是不可约的当且仅当它在 $F$ 中无根。同样， $F[z]$ 中3次多项式是不可约的当且仅当它在 $F$ 中无根。事实上，假定 $\deg f(z) = 3$ 和 $f(z)$ 不是不可约的，则 $f(z) = a(z)b(z)$ ，这里 $\deg a(z) \geqslant 1, \deg b(z) \geqslant 1$ 而且 $\deg a(z) + \deg b(z) = 3$ 。这两个多项式之一，比方说 $a(z)$ 的次数为1。于是 $a(z) = cz + d$ ， $c \neq 0$ 和

$$
f (z) = (c z + d) b (z).
$$

因而 $-c^{-1}d = -d / c$ 是 $f(z)$ 在 $F$ 中的根。反之，如果 $f(z)$ 在 $F$ 中有根 $r$ ，则不难从带余除法推得， $F[z]$ 中存在某多个多项式 $a(z)$ 使 $f(z) = (z - r)a(z)$ ，于是 $f(z)$ 在 $F[z]$ 中不是不可约的。对于4次或更高次的多项式，可能在 $F[z]$ 中不是不可约的但在 $F$ 中却是无根的。作为一个例子，考虑实系数多项式 $z^4 + 2z^2 + 1$ 。我们有 $z^4 + 2z^2 + 1 = (z^2 + 1)(z^2 + 1)$ ，因而 $z^4 + 2z^2 + 1$ 在 $F[z]$ 中不是不可约的；但 $z^4 + 2z^2 + 1$ 却无实根。

关于 $F[z]$ 方面最后所需要的一点是最大公因子的存在性及其性质。设 $f(z)$ 和 $b(z)$ 是在 $F(z)$ 中。若在 $F[z]$ 中存在多项式 $a(z)$ 使得 $f(z) = a(z)b(z)$ ，则 $b(z)$ 导 $f(z)$ 的除数或因子。设 $g(z)$ 是 $F[z]$ 中的另一个多项式并假定 $f(z)$ 和 $g(z)$ 都不为0，若 $b(z)$ 是 $f(z)$ 同时也是 $g(z)$ 的一个因子，那么 $b(z)$ 是 $f(z)$ 和 $g(z)$ 的一个公因子。若 $b(z)$ 是 $f(z)$ 和 $g(z)$ 的公因子并且次数是最大的，则多项式 $b(z)$ 称为 $f(z)$ 和 $g(z)$ 的最大公因子。 $f(z)$ 和 $g(z)$ 的任何两个最大公因子中的一个为另一个的常数倍。

如果 $b(z)$ 是 $f(z)$ 和 $g(z)$ 的最大公因子，则在 $F[z]$ 中存在

多项式 $s(z)$ 和 $t(z)$ ，使得

$$
s (z) f (z) + i (z) g (z) = b (z).
$$

这个命题可以用 $F[z]$ 的Euclidean算法来证明（见练习14）.

多项式 $f(z)$ 和 $g(z)$ 的最大公因子是 1（等价地，是一个非零常数），则它们称为互素多项式。于是

如果 $f(z)$ 和 $g(z)$ 是互素多项式，则存在多项式 $s(z)$ 和 $t(z)$ 使得

$$
s (z) f (z) + t (z) g (z) = 1.
$$

对于正整数 $n$ 和域 $F$ ，令 $F[z; n]$ 表示 $F[z]$ 中次数小于 $n$ 的所有多项式 $a_{0} + a_{1}z + a_{2}z^{2} + \dots + a_{n-1}z^{n-1}$ 集合。如果把两个次数小于 $n$ 的多项式相加，其结果仍然是次数小于 $n$ 的多项式。于是，多项式的通常加法是 $F[z; n]$ 上的一个二元运算。与此不同，次数小于 $n$ 的两个多项式之积很可能是 $n$ 次或 $n$ 次以上的多项式。这样，多项式的通常乘法不是 $F[z; n]$ 上的二元运算。设 $f(z)$ 是 $F[z]$ 中的一个 $n$ 次不可约多项式。利用这个多项式 $f(z)$ ，在 $F[z; n]$ 上定义一个所谓模 $f(z)$ 乘法的二元运算如下：设 $g(z)$ 和 $h(z)$ 是 $F[z; n]$ 中的两个多项式，令 $k(z)$ 是 $g(z)$ 与 $h(z)$ 的通常多项式乘法的乘积。定义 $g(z)$ 和 $h(z)$ 的模 $f(z)$ 乘积为多项式 $k(z)$ 除以 $f(z)$ 的余式。因为 $f(z)$ 是 $n$ 次多项式，当一个多项式除以 $f(z)$ ，其余式是一个次数小于 $n$ 的多项式。因此这就定义了 $F[z; n]$ 上的一个二元运算。

例令 $F = R$ ， $\pmb{R}$ 是实数域，又令 $f(z) = z^{2} + 2z + 3$ 。则 $f(z)$ 无实根，因而 $f(z)$ 在 $\mathbb{R}[z]$ 中是不可约多项式，考虑形如 $a_0 + a_1z$ 的所有多项式集合 $\mathbb{R}[z;2]$ ，这里 $a_0$ 和 $a_1$ 是实数。设 $g(z) = 3z + 2$ 和 $h(z) = 2z + 1$ ，则 $g(z)$ 与 $h(z)$ 的通常乘积是 $k(z) = 6z^2 +7z + 2$ ，因为

$$
6 z ^ {2} + 7 z + 2 = 6 (z ^ {2} + 2 z + 3) + (- 5 z - 1 6),
$$

$g(z)$ 与 $h(z)$ 的模 $f(z)$ 乘积是 $-5z - 16$

例令 $F = Z_{2},Z_{2}$ 是整数模2的域.又令 $f(z) = z^{3} + z^{2} + 1$ 由于0与1都不是 $f(z)$ 的根且 $f(z)$ 是3次的，故 $f(z)$ 是 $Z_{2}[z]$ 中的不可约多项式，考虑形如 $a_2z^2 +a_1z + a_0$ 的所有多项集合 $Z_{2}[z;3]$ ，这里 $\pmb{a}_0$ ， $\pmb{a}_{1}$ ， $\pmb{a}_{2}$ 在 $Z_{2}$ 中，令 $g(z) = z^{2} + 1$ 和 $h(z)$ $= z^{2} + z$ ，那么 $g(z)$ 和 $h(z)$ 的通常乘积是 $k(z) = z^4 +z^3 +z^2 +z.$ 因为

$$
z ^ {4} + z ^ {3} + z ^ {2} + z = z \left(z ^ {3} + z ^ {2} + 1\right) \div z ^ {2},
$$

$g(z)$ 和 $h(z)$ 的模 $f(z)$ 乘积是 $z^2$

定理9.1.3 设 $f(z)$ 是 $F[z]$ 中的一个不可约多项式，其次数 $n \geqslant 1$ ，那么，使用通常多项式加法和模 $f(z)$ 乘法， $F[z; n]$ 是一个域。

证明 这个定理的详细证明将是冗长的。对于加法和模 $f(z)$ 乘法的结合性、交换性和分配性从多项式加法和乘法的对应性质得到，常数多项式0与1分别是 $F[z; n]$ 的加法和乘法元。如果多项式 $g(z)$ 在 $F[z; n]$ 中，则多项式 $-g(z)$ 也在 $F[z; n]$ 中；因而 $-g(z)$ 是 $g(z)$ 在 $F[z; n]$ 中的加法逆元。尚需证明的是在 $F[z; n]$ 中每个非零多项式有乘法逆元。

设 $g(z)$ 是一个次数小于 $n$ 的非零多项式，因为 $f(z)$ 是 $n$ 次的并且是 $F[z]$ 中的不可约多项式，所以1是 $f(z)$ 和 $g(z)$ 的最大公因子。于是，在 $F[z]$ 中有多项式 $s(z)$ 和 $t(z)$ ，使得

$$
s (z) f (z) + i (z) g (z) = 1.
$$

令 $r(z)$ 是 $t(z)$ 除以 $f(z)$ 的余式，于是， $\deg r(z) < n$ ，并且存在多项式 $q(z)$ 使得 $t(z) = q(z)f(z) + r(z)$ ，把它代入上面的方程，改写后得到

$$
r (z) g (z) = \left[ - s (z) + q (z) g (z) \right] f (z) + 1.
$$

因而，1是 $r(z)$ 与 $g(z)$ 通常多项式乘积除以 $f(z)$ 的余式。所

以 $r(z)g(z)$ 的模 $f(z)$ 乘积是1，从而 $r(z)$ 是 $g(z)$ 在 $F[z; n]$ 中的乘法逆元。这样，域的所有性质都满足。

例 我们来构造一个具有4个元素的域。从域 $Z_{2} = \{0,1\}$ 且模2加法和乘法出发， $Z_{2}[z]$ 中多项式 $f(z) = z^{2} + z + 1$ 在 $Z_{2}$ 中无根，因为 $f(0) = 1$ 和 $f(1) = 1$ 。于是 $f(z)$ 在 $Z_{2}[z]$ 中是不可约多项式。 $Z_{2}[z;2]$ 中的多项式是所有形如 $a_{0} + a_{1}z$ 的多项式，这里 $a_{0}$ 和 $a_{1}$ 都在 $Z_{2}$ 中。于是， $Z_{2}[z;2]$ 恰好含有4个多项式，即 $0 = 0 + 0z$ ， $1 = 1 + 0z$ ， $z = 0 + z$ 和 $1 + z$ 。 $Z_{2}[z;2]$ 的加法和模 $f(z)$ 乘法表给出如下：

<table><tr><td>+</td><td>0</td><td>1</td><td>z</td><td>1+z</td></tr><tr><td>0</td><td>0</td><td>1</td><td>z</td><td>1+z</td></tr><tr><td>1</td><td>1</td><td>0</td><td>1+z</td><td>z</td></tr><tr><td>z</td><td>z</td><td>1+z</td><td>0</td><td>1</td></tr><tr><td>1+z</td><td>1+z</td><td>z</td><td>1</td><td>0</td></tr></table>

<table><tr><td>X</td><td>0</td><td>1</td><td>z</td><td>1+z</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>1</td><td>z</td><td>1+z</td></tr><tr><td>z</td><td>0</td><td>z</td><td>1+z</td><td>1</td></tr><tr><td>1+z</td><td>0</td><td>1+z</td><td>1</td><td>z</td></tr></table>

例如 $z(z + 1) = 1$ ，因而在这个域中 $1 + z$ 是 $z$ 的乘法逆元。

例 使用域 $Z_{3} = \{0, 1, 2\}$ 构造含有 $3^{3} = 27$ 个元素的域。考虑 $Z_{3}[z]$ 中的多项式 $f(z) = z^{3} + 2z + 1$ ，因为 $f(0) = 1$ ， $f(1) = 1$ ， $f(2) = 1$ ，由此可见 $f(z)$ 在 $Z_{3}$ 中无根。由于 $\deg f(z) = 3$ ， $f(z)$ 在 $Z_{3}[z]$ 中是不可约多项式。 $Z_{3}[z;3]$ 中的多项式是形如 $a_{0} + a_{1}z + a_{2}z^{2}$ 的多项式，其中 $a_{0}, a_{1}, a_{2}$ 的每一个或是 0 或是 1 或是 2。因而， $Z_{3}[z;3]$ 中有 $3^{3} = 27$ 个多项式且使用通常多项式加法和模 $f(z)$ 乘法， $Z_{3}[z;3]$ 是一个域。

一般地，可以说明如下。考虑域 $Z_{p}$ 。假定找到一个 $n$ 次多项式 $f(z)$ ，它在 $Z_{p}[z]$ 中是不可约的。那么根据定理9.1.3，使用通常多项式加法和模 $f(z)$ 乘法， $Z_{p}[z; n]$ 是一个域。

$\tilde{Z}_{\ell}[z;\pi]$ 中的多项式是形如 $a_0 + a_1z + a_2z^2 + \dots + a_{n-1}z^{n-1}$ 的多项式，这里 $a_0, a_1, a_2, \dots, a_{n-1}$ 都在 $Z_{\ell}$ 中。于是，对于 $a_0, a_1, a_2, \dots, a_{n-1}$ 中每一个有 $p$ 种选择，因而这个域有 $p^n$ 个元素。下面的定理补充了这个讨论的结果，其证明在大多数近世代数的书中都可以找到。

定理9.1.4 如果 $F$ 是一个有限域，则存在一个素数 $p$ 和一个正整数 $n$ ，使得 $F$ 中的元素个数是 $p^n$ 。对于每个素数 $p$ 和正整数 $n$ ，存在（本质上是唯一的）含有 $p^n$ 个元素的域。这个定理的一个推论是不存在含有6个元素的域。

# 9.2 有限几何

首先，我们对Euclidean平面解析几何的某些方面作一些回顾。通过建立两条垂直直线的坐标系统，可以把平面上的每一点与唯一的一对有序实数等同起来。平面上的点与这些有序对是一一对应的。建立了这种坐标之后，便发现平面上的直线对应着形如 $cx + dy + e = 0$ 的线性方程，这里 $c, d, e$ 是实数且 $c$ 与 $d$ 不同时为 0。对应于方程 $cx + dy + e = 0$ 的直线是由坐标 $(a, b)$ 满足方程 $cx + dy + e = 0$ 的所有点组成。直线与线性方程之间的这种对应不是一一对应，因为对任何非零实数 $k, cx + dy + e = 0$ 和 $(kc)x + (kd)y + ke = 0$ 都对应着同一条直线。事实上，两个线性方程对应着同一直线当且仅当一个是另一个的非零常数倍。

考虑线性方程 $cx + dy + e = 0$ ，这里 $c$ 和 $d$ 不同时等于0。如果 $d \neq 0$ ，可以用 $d^{-1}$ 乘这个方程，得到形如 $y = mx + b$ 的方程，其中 $m$ 和 $b$ 都是实数。这就是所谓的斜截式直线方程。如是 $d = 0$ ，则 $c \neq 0$ ，并用 $c^{-1}$ 乘方程，得到形如 $x = k$ 的方程，

这里 $k$ 是实数。形如 $x = k$ 的方程的直线是具有“无限”斜率的垂直直线。这种简化使我们能马上断定平面上的直线与形如 $y = mx + b$ 和 $x = k$ 的线性方程之间存在一一对应。把点与有序实数对和直线与线性方程等同起来的重要性在于可以用代数方法描述和确定一些几何性质。例如，假若两条不同直线不平行，它们必须相交于唯一的一点，这个交点可以通过求出对应于这两直线的线性方程的联立解来确定。

我们可以倒转上面讨论的推理，对任何域——特别是已经指出怎样构造的有限域——确立新的几何结构。在这样做之前，我们抽出Enclidean平面上的点与线所满足的两个基本的关联性质。

A1 两个不同的点位于一条且只位于一条公共直线上。

上面的性质经常说成两个不同的点确定唯一的一条直线。

A2 已知一点 $P$ 和一直线且 $P$ 不在 $l$ 上，存在一条且只存在一条直线 $l'$ ，使 $P$ 位于 $l'$ 上且 $l'$ 不与 $l$ 相交。

上面A2中的直线 $l^{\prime}$ 是通过 $P$ 平行于 $l$ 的直线，我们处处都将随意地采用标准的几何术语。例如，两条直线相交是指有一点位于它们两者之上。不相交的两条直线叫做平行直线。一个仿射平面是定义为一个这样的系统，它由一个其元素称为点的集合 $\mathcal{P}$ 和一个其元素称为直线的集合 $\mathcal{L}$ 组成，而直线是满足性质A1，A2和下述的A3的点的集合。

A3 存在4个点其中任何3点不共线。

![](images/88c5535c4f8e2e42d2ec54629d0490a12285b72af06dc37d97eb12713d507f74.jpg)  
图9.1

加进性质A3是避免某些平凡情形。例如图9.1中的“几何图形”虽然满足A1和A2，但并不是一个仿射平面。（仅

仅图中那些圆点表示点）.

对于Euclidean平面性质A3自然是满足的。与这性质相联系的几何构形可用图9.2来说明。给出任何3点都不共线的4点，由性质A1它们确定了6条直线。

图9.2中的构形是有限仿射平面的一个例子。取点的集合是 $\mathcal{S} = \{P, Q, R, S\}$ 和直线的集合是 $\mathcal{L} = \{l_1, l_2, l_3, l_4, l_5, l_6\}$ 。于是，如图所示每条直线过两点，不难看出性质A1和A2是满足的。一般地说来，一个有限仿射平面可以认为是一个某种组合设计。它是把元素称为点的有限集分成一些称为直线的子集合，使得满足性质A1，A2和A3的一种安排。

![](images/fba3b6679d6a191d4bf664dea0d3c00811cb03c3d127db23da7f0dcb66a94934.jpg)  
图9.2

设 $F$ 是任何一个域，可以用 $F$ 定义一个仿射平面 $\mathrm{AP}(F)$ 如下。含点的集合 $\mathcal{S}$ 是所有 $F$ 中 $a, b$ 的有序对 $(a, b)$ 。直线集合 $\mathcal{L}$ 是对应于线性方程 $cx + dy + e = 0$ 的直线集合，这里 $c, d, e$ 都在 $F$ 中且 $c$ 与 $d$ 不同时为 0 。具体地说，对应于这样一个

线性方程的直线是所有点 $(a, b)$ 的集合，其坐标满足 $cx + dy + e = 0$ 。正如 $F$ 等于实数域的情形，两个线性方程确定同一直线当且仅当一个方程是另一个方程的非零常数倍。每条直线对应唯一的一个形如 $y = mx + b$ 或 $x = k$ 的方程。如同欧氏平面坐标化的方法一样，不难验证仿射平面的性质A1和A2成立。所要用到的性质仅仅是域的性质。不过，如果 $F$ 是有限域，可以得到一些特殊的组合计数的推论。因为这些推论引出了将要用到的某些重要的事实，这就是我们将要采取的方法。

定理9.2.1 设 $F$ 是有 $n$ 个元素的有限域，则对于 $\mathrm{AP}(F)$ 下列性质成立。

（i） 点的个数是 $n^2$   
（ii）有 $n^2 + n$ 条直线；  
（iii）每条直线恰好通过 $n$ 个点；  
（iv）每个点恰好位于 $n + 1$ 条直线上.

证明 点的集合 $\mathcal{SP}$ 是所有有序对 $(a, b)$ 的集合，这里 $a, b$ 在 $F$ 中。因为 $F$ 有 $n$ 个元素，存在 $n \cdot n = n^2$ 个点。曾经注意到，直线的集合 $\mathcal{L}$ 与形如 $y = mx + b$ 和 $x = k$ 的线性方程集合是一一对应的，这里 $m, b$ 和 $k$ 都在 $F$ 中。因为 $F$ 有 $n$ 个元素，存在 $n \cdot n = n^2$ 个第一种类型方程和 $n$ 个第二种类型方程。所以直线的个数是 $n^2 + n$ 。计算在一条直线上点的个数如下。方程 $x = k$ 的直线通过 $n$ 个点，即点 $(k, r)$ ，这里 $r$ 是 $F$ 的任意一个元素。考虑方程是 $y = mx + b$ 的直线。它是由坐标满足这方程的所有点组成。但对于 $F$ 中的任何 $x$ 值，方程确定唯一的 $y$ 值。因为可令 $x$ 取 $F$ 中 $n$ 个值的任意一个，所以恰好有 $n$ 个点位于这条直线上。于是每条直线恰好通过 $n$ 个点。

令 $P = (a, b)$ 是一点，计算 $\mathcal{L}$ 中通过 $P$ 点的直线个数，方程为 $x = a$ 的直线是过 $P$ 的一条直线。过 $P$ 的其余直线是对于某

个选定的 $m$ 与 $k$ ，形为 $y = mx + k$ 的直线，但是，给定 $F$ 中的任何 $m$ 的值，恰好存在 $F$ 中的一个 $k$ 值，即 $k = b - ma$ ，使直线 $y = mx + k$ 包含 $P$ 。因而，过 $P$ 点有 $n$ 条方程为 $y = mx + k$ 的直线，于是过 $P$ 的直线共有 $n + 1$ 条。

定理9.2.2 令 $F$ 是有 $n$ 个元素的有限域，则 $\mathbf{AP}(F)$ 是一仿射平面，称之为由 $F$ 确定的仿射平面。

证明 令 $P = (a, b)$ 和 $Q = (c, d)$ 是两个不同的点，用Euclidean平面中通常的直线斜率公式确定过 $P$ 与 $Q$ 的一条直线：

$$
(y - b) (c - a) = (x - a) (d - b), \tag {9.2.5}
$$

或

$$
(c - a) y + (b - d) x + [ a (d - b) - b (c - a) ] = 0. \tag {9.2.6}
$$

后者是系数在 $F$ 中的线性方程。因为 $P \neq Q$ ，则不能同时有 $c - a = 0$ 和 $b - d = 0$ ，故这个方程确定一直线。从这个方程的形式（9.2.5）看， $P$ 与 $Q$ 显然都在这直线上。于是，两个不同的点至少有一条公共直线。两个不同的点恰好有一条公共直线等价于两条不同的直线的交点不能多于一个。考虑一点 $P$ 。由定理9.2.1，恰有 $n + 1$ 条过 $P$ 的直线 $l_{1}, l_{2}, \ldots, l_{n + 1}$ ，并且这些直线的每一条都含有与 $P$ 不同的 $n - 1$ 个点。因为已经知道每点与 $P$ 有一条公共直线，可见每点至少是在 $l_{1}, l_{2}, \ldots, l_{n + 1}$ 这些直线中的一条直线上（见图9.3）。

这样计算点 $P$ 和其它位于 $n + 1$ 条直线中每一条上的 $n - 1$ 个点，得到 $1 + (n + 1)(n - 1) = n^2$ 。假定除 $P$ 外，有另外一点 $Q$ 位于这些直线的两条上，比方说 $l_1$ 和 $l_2$ 。那么， $Q$ 就计算了两次，因而位于直线 $l_1, l_2, \dots, l_{n + 1}$ 上的点的个数至多是 $n^2 - 1$ 。由

![](images/6211493414205728ab44794cbacb75e45bb7cba23f7d5c8f77e3bf2c5b1a4f86.jpg)  
图9.3

于每一个点是位于这些直线中的一条直线上，这与定理9.2.1的（i）矛盾。所以 $l_{1}, l_{2}, \cdots, l_{n+1}$ 中不存在两条直线在异于 $P$ 的点处相交。我们断定两条在 $P$ 点相交的不同直线不能在任何其它的点处相交。这就完成了A1的证明。

为了证明A2, 令 $P$ 是一点且 $l$ 是不过 $P$ 的一条直线。设 $Q_{1}, Q_{2}, \cdots, Q_{n}$ 是 $l$ 上的 $n$ 个点。对于每个 $i = 1, 2, \cdots, n$ 恰好存在过 $P$ 和 $Q_{i}$ 的一条直线。这些直线成为过 $P$ 的 $n$ 条直线。根据定理9.2.1的（iv）有 $n + 1$ 条直线过 $P$ 。因而，恰好存在一条过 $P$ 的直线 $l'$ ，它不与 $l$ 相交（见图9.4）。

![](images/a8c261903444cdcfdda62764ac0868476ae17fd9bb776ce2abad4e48a58a8699.jpg)  
图9.4

最后证明A3. 考虑4个点 $(0,0)$ , $(0,1)$ , $(1,0)$ 和 $(1,1)$ . 假定有一条直线 $l$ 过这些点中的三个点, 则 $l$ 或同时过 $(0,0)$ 和 $(0,1)$ , 或同时过 $(1,0)$ 和 $(1,1)$ . 但是, 过 $(0,0)$ 和 $(0,1)$ 的直线有方程 $x = 0$ , 因而不过点 $(1,0)$ 与 $(1,1)$ 的任何一个. 过 $(1,0)$ 和 $(1,1)$ 的直线有方程 $x = 1$ , 因而不过点 $(0,0)$ 与 $(0,1)$ 的任何一个. 于是过点 $(0,0)$ , $(0,1)$ , $(1,0)$ 和 $(1,1)$ 中任何三个点的直线是不存在的, 故满足A3. 这样, 一个仿射平面的所有性质都满足, $\mathrm{AP}(F)$ 是一个仿射平面.

例 构造仿射平面 $\mathrm{AP}(Z_2)$ 。存在 $2^2 = 4$ 个点，即 $(0,0)$ ， $(0,1)$ ， $(1,0)$ 和 $(1,1)$ 。 $2^2 + 2 = 6$ 条直线的方程是 $x = 0$ ， $x = 1$ ， $y = 0$ ， $y = 1$ ， $y = x$ 和 $y = x + 1$ 。每条直线恰好过两个点，而每个点恰好位于 $2 + 1 = 3$ 条直线上。可以画出这个仿射平面的点和直线如图9.5所示。注意，这个图与图9.2是表示同一几何结构。

![](images/2e5beff182f274006e00918005ad22d86643ab855fbbd366364474982773467c.jpg)

例 构造仿射平面 $\mathrm{AP}(Z_3)$ 。它有 $3^2 = 9$ 个点和 $3^2 + 3 = 12$ 条直线。我们可以画出这个平面的点和直线如图9.6所示。

![](images/9e9753fb503c294d66713b0b79c0c702416dab76438bf1f879f0bed28e1bf697.jpg)  
图9.6

注意 $\mathrm{AP}(Z_3)$ 的12条直线分成4个平行类，每类有3条直线。在同一个平行类中的两条直线是平行的（不相交），而在不同的平行类中两条直线相交于一点。这4个平行类是：

（i）垂直类是由具有方程 $x = 0$ ， $x = 1$ 和 $x = 2$ 的直线组成，这些直线方程都非斜截式方程，我们用Euclidean平面术语，说它们有无限斜率.  
（ii）水平类是由具有方程 $y = 0$ ， $y = 1$ 和 $y = 2$ 的直线

组成。这些直线有形如 $y = 0x + b$ 的方程并构成斜率为 0 的平行直线类。

（iii）斜率为1的平行直线类，其方程为 $y = x$ ， $y = x + 1$ 和 $y = x + 2$ 。

（iv）斜率为2的平行直线类，其方程为 $y = 2x$ ， $y = 2x + 1$ 和 $y = 2x + 2$ 。

一般地说来，

如果 $F$ 有 $n$ 个元素，仿射平面 $\mathrm{AP}(F)$ 有 $n + 1$ 个平行直线类：垂直类（无限斜率）和对于 $F$ 中每个 $m$ 值有斜率为 $m$ 的平行直线类。后者中包括一个水平类，即斜率为0的平行直线类。

例 使用9.1节中构造的4个元素0，1， $z$ 和 $1 + z$ 的域，可以构成一个有16个点和20条直线的仿射平面 $\mathbf{AP}(\pmb{F})$ 。这个仿射平面有5个平行类：垂直类（无限斜率），水平类（0斜率）和3个斜率分别为1， $z, 1 + z$ 的平行类。图9.7说明斜率为 $z$ 的平行类。这些直线的方程是 $(1)y = zx, (2)y = zx + 1, (3)y = zx + z$ 和 $(4)y = zx + (1 + z)$ 。

![](images/3b157aa50ed846bf3cfaaa0f4148251eced845c18456a3c62d93fd1ec32b7ad9.jpg)  
图9.7

利用在9.1节中所给出 $F$ 的加法和乘法表，算出这些直线中每一条上的点。

(1)

$$
\begin{array}{c c} y & z x \\ 0 & 0 \\ z & 1 \\ 1 + z & z \\ 1 & 1 + z \end{array}
$$

(2）

$$
\begin{array}{c c} y & z x + 1 \\ \hline 1 & 0 \\ 1 + z & 1 \\ z & z \\ 0 & 1 + z \end{array}
$$

(3）

$$
\begin{array}{c c} y & z x + z \\ \hline z & 0 \\ 0 & 1 \\ 1 & z \\ 1 + z & 1 + z \end{array}
$$

（4）

$$
\begin{array}{c c} y & z x + (1 + z) \\ \hline 1 + z & 0 \\ 1 & 1 \\ 0 & z \\ z & 1 + z \end{array}
$$

我们已在图9.7中标明了这些点是位于（1），（2），（3）和（4）的哪一条直线上。注意由于所考虑的是斜率 $z \neq 0$ 的平行直线类，这类直线中每一条与水平直线相交于唯一的一点，与垂直直线相交于唯一的一点。反映在9.7中就是每个标号(1)，(2)，（3）和（4）在每个水平行与每个垂直行出现一次。

# 9.3 拉丁方

$n$ 阶拉丁方定义为由 $n$ 个不同符号（通常取整数 $1,2,\dots ,n$ ）构成的 $n \times n$ 阵列，其中每个符号在每行与每列恰好出现一次。我们已在1.5节里简要地讨论过拉丁方和Euler著名的36名军官问题，而且读者可能希望在进一步讨论之前复习一下那一节的内容。

$$
\left( \begin{array}{l l} 2 & 1 \\ 1 & 2 \end{array} \right), \left( \begin{array}{l l l} 3 & 2 & 1 \\ 2 & 1 & 3 \\ 1 & 3 & 2 \end{array} \right), \left( \begin{array}{l l l l} 4 & 2 & 1 & 3 \\ 3 & 1 & 2 & 4 \\ 2 & 4 & 3 & 1 \\ 1 & 3 & 4 & 2 \end{array} \right). \tag {9.3.1}
$$

分别是2，3和4阶拉丁方的例子。设 $A$ 是一个 $\pmb{n}$ 阶拉丁方，位于 $A$ 的第i行和第j列的元素记为 $a_{ij} (i = 1,2,\dots ,n,j = 1,$

2，…，n）。令B是另一个n阶拉丁方，其各项为bij，（i=1,2，…，n），如果n个有序对（aij,bij）（i=1，2，…，n和j=1，2，…，n）都是不同的，则拉丁方A与B称为正交的。假设A与B是由整数1，2，…，n组成的。因为由整数1，2，…，n恰好可以构成n²个有序对（i，j），拉丁方A与B是正交的只要由整数1，2，…，n做成π²个有序对的每一个在有序对（aij，bjj）（i=1,2,…，n和j=1，2，…，n）的集合中出现。如下例所示，可以把这些有序对（aij，bjj）认为是A与B并置在一起得到的。

例 我们取两个3阶拉丁方 $A$ 和 $B$ ，并置它们得到一个有序对阵列。

$$
\left(\begin{array}{l l l}3&2&1\\2&1&3\\1&3&2\end{array}\right), \left(\begin{array}{l l l}2&3&1\\1&2&3\\3&1&2\end{array}\right)\rightarrow \left(\begin{array}{l l l}(3, 2)&(2, 3)&(1, 1)\\(2, 1)&(1, 2)&(3, 3)\\(1, 3)&(3, 1)&(2, 2)\end{array}\right).
$$

由于并置得到的有序对全是不同的（等价地说，用整数1，2，3做成的9个可能有序对的每一个在并置阵列中出现），所以拉丁方A与B是正交的。

例 下面两个3阶拉丁方 $A$ 与 $B$ 不是正交的，因为当它们并置后，所得到的阵列中的有序对不是全不同的。

$$
\left[\begin{array}{l l l}3&2&1\\2&1&3\\1&3&2\end{array}\right], \left[\begin{array}{l l l}1&3&2\\3&2&1\\2&1&3\end{array}\right]\rightarrow \left\{\begin{array}{l}(3, 1) (2, 3) (1, 2)\\(2, 3) (1, 2) (3, 1)\\(1, 2) (3, 1) (2, 3)\end{array}\right\}.
$$

正交拉丁方对试验设计有许多应用，在继续进行研究拉丁方之前，举一个应用它们的例子。假设希望试验各种混水量和各种肥料对某类土壤上种植小麦产量的影响。若有 $n$ 种混水量和 $n$ 种肥料作试验，则有 $n^2$ 种水与肥料的可能组合。把所要进行试验的矩形场地分成 $n^2$ 个地块，每个地块对应着混水量与肥

料的 $n^2$ 种组合的一种试验。没有任何理由认为土壤肥力在整个区域上都是一样的。例如，完全可能第一行肥力高，因此小麦获得较高产量不单单是由于所用的水量或所用的某种肥料引起的。假若要求每种肥料在区域的任何一行或任何一列出现不多于一次，同样，每种水量在任何一行或任何一列也不多于一次，便有可能做到使土壤肥力对小麦产量的影响达到最低限度。于是，把 $n$ 种肥料使用到 $n^2$ 个地块决定一个 $n$ 阶拉丁方 $A$ ，同样使用 $n$ 种混水量也确定一个 $n$ 阶拉丁方 $B$ 。由于有 $n^2$ 个混水量与肥料的组合要作试验，两个拉丁方 $A$ 与 $B$ 应当是正交的。总之，一对 $n$ 阶正交拉丁方决定着一个关于试验 $n$ 种肥料和 $n$ 种混水量对小麦产量的试验设计。例如，有三种标记为1，2，3的肥料与三种也标记为1，2，3的混水量一起作试，那么一种适用的试验设计是

![](images/905066b9a3903558fadab25ad2c3183b58c6c53bee7c29f780e4a89768b4d26f.jpg)

对于任何正整数 $n$ ，都毫无困难地可以构造出一个 $n$ 阶拉丁方。这里有一个6阶拉丁方，其构造方法对每个 $n$ 都行得通：

$$
\left( \begin{array}{c c c c c c} 1 & 2 & 3 & 4 & 5 & 6 \\ 6 & 1 & 2 & 3 & 4 & 5 \\ 5 & 6 & 1 & 2 & 3 & 4 \\ 4 & 5 & 6 & 1 & 2 & 3 \\ 3 & 4 & 5 & 6 & 1 & 2 \\ 2 & 3 & 4 & 5 & 6 & 1 \end{array} \right).
$$

为了构造它，在第一行写整数1，2，3，4，5，6。由第一行移动6，使它成为第一项构成第二行，由第二行移动5使它成为第一项构成第三行，等等。为了构造一对 $n$ 阶正交拉丁方，或者甚至三个 $n$ 阶拉丁方使它们的每一对都是正交的，一般地说来，需要不同于试凑法的一些技巧。现在说明怎样用9.2节中所讨论的仿射平面 $\mathrm{AP}(F)$ 构造正交拉丁方。

设 $F$ 是 $\pmb{n}$ 个元素的有限域。（根据定理9.1.4， $\pmb{n}$ 是某素数的乘方）。考虑由 $F$ 确定的仿射平面 $\mathrm{AP}(F)$ ，和以非零 $m_0$ 为斜率的平行直线类。这个平行类是由 $\mathrm{AP}(F)$ 的具有形如

$$
y = m _ {0} x + k
$$

的方程的 $n$ 条直线组成，这里 $k$ 是 $F$ 的 $n$ 个元素的任何一个。我们注意到这个平行直线类既不是水平类也不是垂直类。于是，斜率为 $m_0$ 的 $n$ 条平行直线类中的每一条与每条水平线相交唯一的一点，与每条垂直线也相交唯一的一点。正如9.2节那样，用圆点表示平面 $\mathrm{AP}(F)$ 的点并把它们列成一个方形格式。把斜率为 $m_0$ 的平行类的 $n$ 条直线任意地安排一个次序，从而有第一、第二、…、第 $n$ 条直线。把第一条直线上的所有点用1标记，第二条直线上的所有点用2标记，…，第 $n$ 条直线上的所有点用 $n$ 标记。因为 $n$ 条直线是平行的， $\mathrm{AP}(F)$ 的每个点恰好位于一直线上，所以每个点恰好得到一个标记。由于平行类的每条直线与每条水平直线相交唯一一点，整数1，2，…， $n$ 的每一个恰好在每行出现一次。同样，由于平行类的每条直线与垂直直线相交唯一一点，整数1，2，…， $n$ 的每一个恰好在每列出现一次。这种标记的结果就是一个 $n$ 阶拉丁方。我们断定， $\mathrm{AP}(F)$ 中斜率不为0的 $n - 1$ 个平行直线类的每一个产生一个 $n$ 阶拉丁方。关于4个元素的域 $F$ ，已在9.2节末给出用上述方法构造的一个例子。

现在我们来研究对应于 $\mathrm{AP}(F)$ 的 $n - 1$ 个非零斜率平行类所产生的 $n - 1$ 个 $n$ 阶拉丁方之间的关系。令 $m_{1}$ 与 $m_{2}$ 是不为零的斜率且 $m_{1} \neq m_{2}$ ，又令 $L_{1}$ 和 $L_{2}$ 分别是由斜率为 $m_{1}$ 和 $m_{2}$ 的平行直线类所构造的拉丁方。我们肯定 $L_{1}$ 和 $L_{2}$ 是正交的，假若不然，那么当 $L_{1}$ 与 $L_{2}$ 并置时必有某个有序整数对，比方说 $(i, j)$ ，出现在不同的位置上。这就意味着斜率为 $m_{1}$ 的平行类的第 $i$ 条直线与斜率为 $m_{2}$ 的平行类的第 $j$ 条直线相交于不同的两点。因为在仿射平面里两条不同直线至多相交于一点（性质A1）。因而，断定 $L_{1}$ 与 $L_{2}$ 是正交拉丁方。这种论证适用于由 $\mathrm{AP}(F)$ 的 $n - 1$ 个非零斜率的平行类所产生的每一对拉丁方，因此我们证明了如下的定理。

定理9.3.1 设 $F$ 是 $n \geqslant 3$ 个元素的域，又令 $L_{1}, L_{2}, \ldots, L_{n-1}$ 是对应于仿射平面 $\mathrm{AP}(F)$ 的 $n-1$ 个非零斜率的平行直线类所产生的拉丁方。那么对于 $i \neq j$ ， $L_{i}$ 与 $L_{j}$ 是正交拉丁方。

在定理9.3.1中，没有包含2个元素的域的原因是，如果 $n = 2$ ，拉丁方 $L_{1}, L_{2}, \dots, L_{n-1}$ 的集合只有一个 $L_{k}$ 。根据定理9.1.4，若 $F$ 是有 $n$ 个元素的域，则存在素数 $p$ 和正整数 $k$ 使 $n = p^{k}$ 。于是，构造 $n$ 阶正交拉丁方的方法只适用于 $n$ 是某素数的乘方。

例 构造四个两两正交的5阶拉丁方，我们使用由5个元素0，1，2，3和4组成的域 $Z_{5}$ 。考虑斜率为1的平行类。这个平行类的直线方程是：（1） $y = x$ ，（2） $y = x + 1$ ，（3） $y = x + 2$ ，（4） $y = x + 3$ ，（5） $y = x + 4$ 。把直线(1)上的所有点用1标记，直线(2)上的所有点用2标记，…，和直线(5)上的所有点用5标记。这样，得到拉丁方

$$
\dot {L} _ {1} = \left[ \begin{array}{l l l l l} 5 & 4 & 3 & 2 & 1 \\ 4 & 3 & 2 & 1 & 5 \\ 3 & 2 & 1 & 5 & 4 \\ 2 & 1 & 5 & 4 & 3 \\ 1 & 5 & 4 & 3 & 2 \end{array} \right]
$$

（斜率为1）

用类似方法构造拉丁方

$$
L _ {2} = \left\{ \begin{array}{l l l l l} 5 & 3 & 1 & 4 & 2 \\ 4 & 2 & 5 & 3 & 1 \\ 3 & 1 & 4 & 2 & 5 \\ 2 & 5 & 3 & 1 & 4 \\ 1 & 4 & 2 & 5 & 3 \end{array} \right\}
$$

（斜率为2），

$$
L _ {3} = \left( \begin{array}{l l l l l} 5 & 2 & 4 & 1 & 3 \\ 4 & 1 & 3 & 5 & 2 \\ 3 & 5 & 2 & 4 & 1 \\ 2 & 4 & 1 & 3 & 5 \\ 1 & 3 & 5 & 2 & 4 \end{array} \right)
$$

（斜率为3）.

$$
L _ {4} = \left( \begin{array}{c c c c c} 5 & 1 & 2 & 3 & 4 \\ 4 & 5 & 1 & 2 & 3 \\ 3 & 4 & 5 & 1 & 2 \\ 2 & 3 & 4 & 5 & 1 \\ 1 & 2 & 3 & 4 & 5 \end{array} \right)
$$

（斜率为4）.

这些拉丁方 $L_{1}, L_{2}, L_{3}, L_{4}$ 中每两个都是正交的。

是否存在五个两两正交的5阶拉丁方？对于这个问题的回答是否定的，更一般地我们有下面定理。

定理9.3.2 设 $L_{1}, L_{2}, \ldots, L_{k}$ 是两两正交的 $n$ 阶柱丁方，则 $k \leqslant n - 1$ 。

证明 首先考察下面情况。假定在拉丁方 $L_{1}$ 中，我们重新标记各项，使得所有的 1 由 $i_{1}$ 代替，所有的 2 由 $i_{2}$ 代替，…，和所有的 $n$ 由 $i_{n}$ 代替，这里 $i_{1}i_{2}\dots i_{n}$ 是 1，2，…， $n$ 的一个排

列。这样得到的阵列 $L_{1}^{\prime}$ 显然也是一个拉丁方，同样清楚的是， $L_{1}^{\prime}$ 正交于 $L_{2}, L_{3}, \ldots, L_{n}$ 中的每一个。假若不然，比如说并置 $L_{1}^{\prime}$ 与 $L_{2}$ ，有序对 $(r, s)$ 出现两次，那么并置 $L_{1}$ ，与 $L_{2}$ 时有序对 $(j, s)$ 出现两次，这里 $j$ 是使 $i_{j} = r$ 的整数。这与假设 $L_{1}$ 和 $L_{2}$ 是正交的相矛盾。于是，允许我们按指出的方式重新标记 $L_{1}$ 的各项。同样，能够重新标记 $L_{2}, L_{3}, \ldots, L_{k}$ 中每一个的各项。所以，可以重记拉丁方 $L_{1}, L_{2}, \ldots, L_{k}$ 中每一个的各项，使整数 $1, 2, \ldots, n$ 依自然顺序出现在第一行。假定已做到这点，并对这些拉丁方仍旧记为 $L_{1}, L_{2}, \ldots, L_{k}$ 。于是，当 $L_{1}, L_{2}, \ldots, L_{k}$ 中任意两个拉丁方并置时，有序对 $(1, 1), (2, 2), \ldots, (n, n)$ 在第一行出现。现在考虑 $L_{1}, L_{2}, \ldots, L_{k}$ 中每个拉丁方第二行第一列位置上的项。这些项没有一个是1，因为1已经在每个拉丁方的第一行第一列位置上出现。例如，假定 $L_{1}$ 与 $L_{2}$ 在第二行第一列位置上都是 $t$ ，则当 $L_{1}$ 与 $L_{2}$ 并置时，有序对 $(t, t)$ 就会出现两次，因为它也出现在并置阵列的第一行。因此， $L_{1}$ 与 $L_{2}$ 在第二行第一列位置上的项是不同的。用同样的方法，可以证明 $L_{1}, L_{2}, \ldots, L_{k}$ 在第二行第一列位置上的各项都是不同的。因为这些项中没有一个是1，根据鸽笼原理可得 $k \leqslant n^{-1}$ 。

这样，当 $n$ 是某素数的乘方， $n \neq 2$ ，用仿射平面 $\mathrm{AP}(F)$ 中平行直线类构造正交拉丁方的方法（这里 $F$ 是 $n$ 个元素的域），得到两两正交的 $n$ 阶拉丁方最多是 $n - 1$ 个。如果 $n$ 不是某素数的乘方将会怎样呢？在第一章已提到没有一对 2 阶和 6 阶正交拉丁方，但是对于其它的正整数 $n$ ，存在一对 $n$ 阶正交拉丁方，它的证明已超出本书的范围。不过，如果 $n$ 不是某奇数的 2 倍，定理9.3.1以及进一步的构造可以用来求出一对 $n$ 阶正交拉丁方。举例来说明这一点，读者可参看由H.J.Ryser著的“组

合数学”（Carus Mathematical Monograph No.14，Mathematical Association of America，1964）。考虑 $n = 12 = 3 \times 4$ 。因为3与4都是素数幂，我们知道存在3阶和4阶正交拉丁方。下面的阵列 $A_{1}$ 与 $A_{2}$ 是3阶正交拉丁方，而阵列 $B_{1}$ 与 $B_{2}$ 是4阶正交拉丁方。

$$
\begin{array}{l} A _ {1} = \left( \begin{array}{l l l} 3 & 2 & 1 \\ 2 & 1 & 3 \\ 1 & 3 & 2 \end{array} \right), \quad A _ {2} = \left( \begin{array}{l l l} 3 & 1 & 2 \\ 2 & 3 & 1 \\ 1 & 2 & 3 \end{array} \right); \\ B _ {1} = \left[ \begin{array}{l l l l} 4 & 3 & 2 & 1 \\ 3 & 4 & 1 & 2 \\ 2 & 1 & 4 & 3 \\ 1 & 2 & 3 & 4 \end{array} \right], \quad B _ {2} = \left[ \begin{array}{l l l l} 4 & 2 & 1 & 3 \\ 3 & 1 & 2 & 4 \\ 2 & 4 & 3 & 1 \\ 1 & 3 & 4 & 2 \end{array} \right]. \\ \end{array}
$$

用这两对去构造一对12阶正交拉丁方。令 $C_1$ 是由 $4 \times 4$ 阵列

$$
(a, B _ {1}) = \left\{ \begin{array}{l} (a, 4) (a, 3) (a, 2) (a, 1) \\ (a, 3) (a, 4) (a, 1) (a, 2) \\ (a, 2) (a, 1) (a, 4) (a, 3) \\ (a, 1) (a, 2) (a, 3) (a, 4) \end{array} \right\}
$$

代替 $A_{1}$ 的每一项 $a$ 而得到的 $12 \times 12$ 阵列。同样，令 $C_{2}$ 是由 $4 \times 4$ 阵列 $(b, B_{2})$ 代替 $A_{2}$ 的每一项 $b$ 而得到的 $12 \times 12$ 阵列。 $C_{1}$ 与 $C_{2}$ 的各项从形如 $(i, j)$ （ $1 \leqslant i \leqslant 3$ 和 $1 \leqslant j \leqslant 4$ ）的12个有序对集合取值。现在把这12个有序对的每一个指派整数 $1, 2, \cdots, 12$ 中的一个，使得不同的有序对指派不同的整数，并且 $C_{1}$ 与 $C_{2}$ 的各项都用指派它们的整数代替。这样，得到两个 $12 \times 12$ 阵列 $L_{1}$ 和 $L_{2}$ ，并且不难证明它们是12阶拉丁方。借助读者去完成这种构造作为一个练习。而且可以证明 $L_{1}$ 与 $L_{2}$ 是正交的。类似的构造方法是普遍行得通的，即从一对 $n_{1}$ 阶正交拉丁方和另一对 $n_{2}$ 阶正交拉丁方，用这种方法可以得到一对

$n_1 n_2$ 阶正交拉丁方。这种构造可以重复任意多次。设 $n$ 是一个大于 1 而且不是素数幂的整数，令 $n = p_1^{a_1} p_2^{a_2} \cdots p_i^{a_i}$ 是 $n$ 的素数因子分解。这里 $p_1, p_2, \cdots, p_i$ 是不同的素数， $a_1, a_2, \cdots, a_i$ 是正整数。如果没有 $p_i^{a_i} = 2$ ，那么我们知道存在一对 $p_i^{a_i}$ 阶正交拉丁方（ $i = 1, 2, \cdots, t$ ）。使用上面的构造方法，相继地得到一对 $p_i^{a_1} p_i^{a_2}$ 阶， $p_i^{a_1} p_i^{a_2} p_i^{a_3}$ 阶， $\cdots$ ， $p_i^{a_1} p_i^{a_2} \cdots p_i^{a_t} = n$ 阶的正交拉丁方。这样便可以构造一对 $n$ 阶正交拉丁方。没有 $p_i^{a_i} = 2$ 这个条件等价于 $n$ 不是一个奇数的 2 倍。因而，这个方法指出，对于除了 2，6，10，14， $\cdots$ ， $4k + 2$ ， $\cdots$ 以外且比 1 大的每个 $n$ 如何构造一对 $n$ 阶正交拉丁方。业已指出不存在一对 2 阶正交拉丁方，约在 1900 年 G. Tarry 证明了也不存在一对 6 阶正交拉丁方，由此证实了 L. Euler 的一个猜想。但是，Euler 又推测不存在一对以 10，14， $\cdots$ ， $4k + 2$ ， $\cdots$ 中任何一个为阶的正交拉丁方。在 1959 年 R. C. Bose，E. T. Parker 和 S. S. Shrikhandel 证明了 Euler 猜想完全是错误的，他们指明了对于 $n$ 等于整数 10，14， $\cdots$ ， $4k + 2$ ， $\cdots$ 中任何一个时怎样去构造一对 $n$ 阶正交拉丁方。有兴趣的读者可以查阅 Martin Gar-dner 在 1959 年 11 月发表在《科学的美国人》上关于推翻 Euler 猜想的研究论文。

我们考虑一个深入一层的问题来结束拉丁方的讨论，它的解依赖于配偶定理。假定试图用一次加入一行来构造 $n$ 阶拉丁方，按照每列不重复任何整数的方法来做这一点。使用这种方法总能保证得到一个拉丁方吗？或者能否推出到某一步我们再

不能继续做下去？定义一个 $r \times n$ 拉丁矩形是由整数 1，2，…， $n$ 作成的 $r \times n$ 阵列，这些整数在每一行和每一列都不重复出现。于是，在 $r \times n$ 拉丁矩形中，1，2，…， $n$ 中的每个整数在每行恰好出现一次，在每列至多出现一次，而且 $r \leqslant n$ 。例如，

$$
L = \left[ \begin{array}{c c c c c} 3 & 1 & 2 & 5 & 4 \\ 1 & 3 & 5 & 4 & 2 \\ 4 & 5 & 3 & 2 & 1 \end{array} \right]
$$

是一个 $3 \times 5$ 拉丁矩形。我们说一个 $r \times n$ 拉丁矩形 $L$ 可以补足成一个 $n$ 阶拉丁方只要有可能对 $L$ 加添 $n - r$ 行后得到一个 $n$ 阶拉丁方。上面 $3 \times 5$ 拉丁矩形可以补足成拉丁方

$$
\left( \begin{array}{c c c c c} 3 & 1 & 2 & 5 & 4 \\ 1 & 3 & 5 & 4 & 2 \\ 4 & 5 & 3 & 2 & 1 \\ 5 & 2 & 4 & 1 & 3 \\ 2 & 4 & 1 & 3 & 5 \end{array} \right).
$$

定理9.3.3 一个 $r \times n$ 柱丁矩形总可以补足成一个 $n$ 阶拉丁方。

证明 设 $L$ 是一个 $r \times n$ 拉丁矩形。如果 $r = n$ ，毋须证明。于是假定 $r < n$ ，我们定义 $\{1, 2, \dots, n\}$ 的子集合族 $A_{1}, A_{2}, \dots, A_{n}$ ，其中 $A_{i}$ 是由 1 与 $n$ 之间且在 $L$ 的 $i$ 列中没有出现的整数组成。这样，集合 $A_{1}, A_{2}, \dots, A_{n}$ 中的每一个恰好包含 $n - r$ 个元素。因为 $L$ 有 $r$ 行，整数 1，2，…， $n$ 中的每一个在 $L$ 中出现 $r$ 次。由于没有整数在任何列中重复，整数 1，2，…， $n$ 中每一个是集合 $A_{1}, A_{2}, \dots, A_{n}$ 中恰好 $n - r$ 个的元素。假定 $a_{1}, a_{2}, \dots, a_{n}$ 是 $A_{1}, A_{2}, \dots, A_{n}$ 的相异代表组。那么，可以把 $a_{1}, a_{2}, \dots, a_{n}$ 加添到 $L$ 中作为新行而得到一个 $(r + 1) \times n$ 阵列 $L'$ 。因为 $a_{1}, a_{2}, \dots, a_{n}$ 是 $A_{1}, A_{2}, \dots, A_{n}$ 的相异代表

组，没有整数在这一行中重复，也没有任何整数在 $L^{\prime}$ 的任何列中重复。现在证明 $A_{1}, A_{2}, \cdots, A_{n}$ 有相异代表组，对此，由定理8.1.1只要证明配偶条件成立。令 $k$ 是 $1, 2, \cdots, n$ 中的一个整数并选取整数 $i_{1}, i_{2}, \cdots, i_{k}$ ，其中 $1 \leqslant i_{1} < i_{2} < \cdots < i_{k} \leqslant n$ 。令

$$
\left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| = t.
$$

考虑和式 $\left|A_{i_1}\right| + \left|A_{i_2}\right| + \dots +\left|A_{i_k}\right|$ 。因为集合 $A_{1}, A_{2}, \dots, A_{n}$ 中每一个恰含 $n - r$ 个整数，所以

$$
\left| A _ {i _ {1}} \right| + \left| A _ {i _ {2}} \right| + \dots + \left| A _ {i _ {k}} \right| = k (n - r) 。
$$

另一方面， $A_{i_1} \cup A_{i_2} \cup \dots \cup A_{i_k}$ 中七个整数的每一个恰在 $A_1, A_2, \dots, A_n$ 的 $n-r$ 个集合中出现，因而至多在 $A_{i_1}, A_{i_2}, \dots, A_i$ 中 $n-r$ 个集合出现。这意味着

$$
\left| A _ {i _ {1}} \right| + \left| A _ {i _ {2}} \right| + \dots + \left| A _ {i _ {k}} \right| \leqslant t (n - r) _ {\bullet}
$$

故

$$
t \geqslant k _ {\bullet}
$$

于是

$$
\left| A _ {i _ {1}} \cup A _ {i _ {2}} \cup \dots \cup A _ {i _ {k}} \right| \geqslant k,
$$

从而满足配偶条件。所以可以扩充 $L$ 成为一个 $(r + 1) \times n$ 拉丁矩形。如果 $r + 1 = n$ ，就完成了证明。如果不然，还能重复论证直至得到一个 $n \times n$ 拉丁矩形，即一个 $n$ 阶拉丁方。

# 9.4 Kirkman女学生问题

在1850年Rev.ThomasP.Kirkman提出下面一个问题。一位女教师每天带领她班上的15名女生去散步，她把这些女生按三人一行排成5行，这样每个女生有2名同伴。能否作出一个连续散步七天的计划，使得没有一个女生和她的任何同学同

在一个三人组的次数多于一次？这个问题的解是由15名女生集合的35个（无序）三人组（3组合）组成，并且每一对女生同在一个且仅一个三人组中。此外，可以把35个三人组分成7批，每批有5个三人组（一批对应7天中的一天），而且每名女生恰好在7批的每一批中的一个三人组内（每批的那些三人组形成15名女生集合的一个划分）。

首先考虑Kirkman女学生问题有解必须具备的首要性质。15名女生集合一定有35个三人组，使得每对女生同在一个且仅一个三人组中。更一般地，考虑下面的问题。令 $v$ 与 $b$ 是正整数，又令 $X$ 是 $v$ 个元素的集合。Steiner三元组系 $\mathcal{S}$ 定义为 $X$ 的 $b$ 个三元组的集合，使得对 $X$ 的每一对元素同在一个且仅一个三元组中。对于哪些 $v$ 和 $b$ 的值，Steiner三元组系存在呢？在Kirkman问题中 $v = 15$ 与 $b = 35$ 。

假定有一个Steiner三元组系 $\mathcal{S}$ ，它是 $v$ 个元素集合 $X$ 的 $b$ 个三元组组成，则有

$$
\left( \begin{array}{l} v \\ 2 \end{array} \right) = v \frac {(v - 1)}{2}
$$

个 $X$ 的元素对。这些对中的每一对在 $\mathcal{S}$ 的一个且仅一个三元组中出现。另一方面， $\mathcal{S}$ 的每个三元组 $\{x, y, z\}$ ，含有 $X$ 的3对元素，即 $\{y, x\}$ ， $\{x, z\}$ 和 $\{y, z\}$ 。因而， $\mathcal{S}$ 的b个三元组含有3b个 $X$ 的元素对。所以

$$
3 b = \frac {v (v - 1)}{2}, \quad \text {或} b = \frac {v (v - 1)}{6}.
$$

具体说，就是用 $v$ 个元素集合构造的Steiner三元组系中三元组的个数 $b$ 由 $v$ 确定。我们把用 $v$ 个元素的集合构造的Steiner三元组系称为 $v$ 阶Steiner三元组系。于是，要确定存在 $v$ 阶Steiner三元组系的哪些 $v$ 的值。注意到15阶的Steiner三元组系有 $b =$

$15(15 - 1) / 6 = 35$ 个三元组，它是Kirkman女学生问题的解所要求的个数。现在令 $x$ 是 $X$ 的一个元素。由于 $X$ 有 $\pmb{\nu}$ 个元素， $\pmb{x}$ 恰好是 $X$ 的 $\pmb{\upsilon} - 1$ 个对的成员。设 $\{x,y,z\}$ 是 $\mathcal{S}$ 中含有 $\pmb{x}$ 的一个三元组，那么这个三元组包含以 $\pmb{x}$ 为成员的两个对，即 $\{x,y\}$ 与 $\{x,z\}$ 。由此断定 $\pmb{x}$ 恰好在 $\mathcal{S}$ 的 $(v - 1) / 2$ 个三元组中。由于这个断言对 $X$ 的每个元素 $\pmb{x}$ 都是正确的，因此 $X$ 的每个元素恰在 $r = (v - 1) / 2$ 个三元组中。由于 $\pmb{\tau}$ 必须是一个整数，从 $r = (v - 1) / 2$ 可得 $\pmb{\nu}$ 必须是个奇数。因此，存在 $\pmb{\upsilon}$ 阶Steiner三元组系的必要条件是 $\pmb{\nu}$ 为奇数。但是，我们还可以再说几句。从 $b = v(v - 1) / 6$ 是个整数（因为 $\pmb{\delta}$ 表示 $\mathcal{S}$ 的三元组的个数），可知，或者 $\pmb{\upsilon}$ 被3整除，或者 $v - 1$ 被3整除。先假定 $\pmb{\nu}$ 能被3整除，那么对某个整数 $k$ 有 $v = 3k$ 。因为 $\pmb{\nu}$ 是奇数， $k$ 必须是奇数，因此对某整数 $n$ 有 $k = 2n + 1$ 。代入后便得

$$
v = 6 n + 3.
$$

现在假定 $v - 1$ 可被3整除，则对某整数 $k$ ，有 $v - 1 = 3k$ 或 $v = 3k + 1$ ，因为 $v$ 是奇数，则 $k$ 必为偶数，因而对某整数 $n$ ，有 $k = 2n$ 代入后有

$$
v = 6 n + 1.
$$

我们把讨论的结果概括起来表述如下：

定理9.4.1 存在一个 $v$ 阶 Steiner 三元组系的必要条件是对某个非负整数 $n$ 有 $v = 6n + 1$ 或 $v = 6n + 3$ .

注意，如果 $v = 6n + 1$ 或 $v = 6n + 3$ ，则 $\pmb{v}$ 自然是奇数。再注意到， $v = 6n + 1$ 意味着除以6余数是3。对于定理9.4.1所说的可能存在阶Steiner三元组系的前面几个值是3,7,9,13,15,19,21，…。（我们排除 $v = 1$ 的可能性，因为这时甚至形成一个三元组都是不可能的。）

例设 $v = 3$ 和取 $X = \{1,2,3\}$ .那么一个3阶Steiner三元

组系有 $b = 3(3 - 1) / 6 = 1$ 个三元组，于是三元组

$$
\{1, 2, 3 \}
$$

自身构成3阶Steiner三元组系，

例令 $b = 7$ 且取 $X = \{1,2,3,4,5,6,7\}$ 。一个7阶Steiner三元组系有 $b = 7(7 - 1) / 6 = 7$ 个三元组。下面就是一个例子。 $\{1,2,4\}, \{2,3,5\}, \{3,4,6\}, \{4,5,7\}, \{5,6,1\}, \{6,7,2\}, \{7,1,3\}$ 。这个系可表为图9.8所示的几何图形。不难验证 $X$ 的每对元素恰在这些三元组的一个三元组中。

![](images/98a823d7d0ee029a03ab3336aa8095d9d258f03dff261c3fbaa70366ab78a22f.jpg)  
图9.8

E. Steiner 在 1853 年曾问过, 如果 $v = 6n + 1$ 或 $v = 6n + 3$ 是否存在现在称为 $v$ 阶 Steiner 三元组系? 当时他不知道, Kirkman 在 1847 年已经解决了这个问题。若 $v = 6n + 1$ 或 $v = 6n + 3$ , Kirkman 指出了怎样去构造 $v$ 阶 Steiner 三元组系。在这里我们不想给出 Kirkman 定理的证明。只满足于提供一种从给定的两个 Steiner 三元组系构造出一个新的 Steiner 三元组系的方法。

定理9.4.2 如果存在一个 $v$ 阶Steiner三元组系和一个 $\pmb{w}$

阶Steiner三元组系，则存在一个 $vw$ 阶Steiner三元组系。

证明设 $X = \{a_{1}, a_{2}, \dots, a_{v}\}$ 是 $\pmb{\vartheta}$ 个元素的集合， $\mathcal{S}_1$ 是一个用 $X$ 的元素构造的 $\pmb{\nu}$ 阶Steiner三元组系.设 $Y = \{b_{1}, b_{2}, \dots, b_{w}\}$ 是 $\pmb{\varpi}$ 个元素的集合， $\mathcal{S}_2$ 是一个用 $Y$ 的元素构造的 $\pmb{\varpi}$ 阶Steiner三元组系.令 $Z = \{c_{ij} : 1 \leqslant i \leqslant v, 1 \leqslant j \leqslant w\}$ 是 $\pmb{\nu}\pmb{\varpi}$ 个元素的集合.如下所示，我们把 $\pmb{Z}$ 的元素排成一个 $\pmb{\nu} \times \pmb{\varpi}$ 阵列，其中每一行对应于 $X$ 的一个元素和每一列对应于 $Y$ 的一个元素.

$$
\begin{array}{c} b _ {1} \quad b _ {2} \dots b _ {w} \\ a _ {1} \left\{ \begin{array}{l l l l} c _ {1 1} & c _ {1 2} & \dots & c _ {1 w} \\ c _ {2 1} & c _ {2 2} & \dots & c _ {2 w} \\ \vdots & \vdots & \vdots \\ c _ {v 1} & c _ {v 2} & c _ {v w} \end{array} \right\}, \end{array} \tag {9.4.1}
$$

我们利用 $Z$ 的元素定义一个三元组的集合 $\mathcal{S}$ 如下。设 $\{c_{ir}, c_{j\epsilon}, c_{k\ell}\}$ 是 $Z$ 的元素的一个三元组，那么， $\{c_{ir}, c_{j\epsilon}, c_{k\ell}\}$ 是 $\mathcal{S}$ 中一个三元组当且仅当下列条件之一成立：

(i) $v = s = t$ ，而且 $\{a_{i}, a_{j}, a_{k}\}$ 是 $\mathcal{S}_{1}$ 中的一个三元组。等价地，元素 $c_{ir}, c_{js}$ 和 $c_{kt}$ 是在(9.4.1)阵列的同一列中，它们所在的那些行对应于 $\mathcal{S}_{1}$ 中的一个三元组。

(ii) $i = j = k$ ，而且 $\{b_r, b_s, b_t\}$ 是 $\mathcal{S}_2$ 中的一个三元组。等价地，元素 $c_{ir}, c_{js}$ 和 $c_{kt}$ 是在（9.4.1）阵列的同一行中，它们所在的那些列对应于 $\mathcal{S}_2$ 中的一个三元组。

(iii) $i$ ， $j$ 和 $k$ 全不相同，并且 $\{a_i,a_j,a_k\}$ 是 $\mathcal{S}_1$ 中的一个三元组，同样， $r,s$ 和 $t$ 也不相同，并且 $\{b_r,b_s,b_t\}$ 是 $\mathcal{S}_2$ 中的一个三元组。等价地，(9.4.1)阵列的元素 $c_{ir},c_{js}$ 和 $c_{kt}$ 位于3个不同的行和3个不同的列中，它们所在的那些行对应于 $\mathcal{S}_1$ 中的一个三元组，而它们所在的那些列对应于 $\mathcal{S}_2$ 中的一个三元组。

注意到没有 $\mathcal{S}$ 中的三元组恰好位于(9.4.1)阵列的2个行，也没有 $\mathcal{S}$ 中任何三元组恰好位于(9.4.1)阵列的2个列。现在证

明这个 $Z$ 元素的三元组集合是一个 $\pmb{v}\pmb{w}$ 阶Steiner三元组系，设 $\{c_{i}r,c_{j}s\}$ ，是一个 $z$ 的不同元素对，考虑三种情况.

情况1 $r = s$ ，由此 $\pmb {i}\neq \pmb{j}$ ，因 $\mathcal{S}_1$ 是Steiner三元组系，则 $\mathcal{S}_1$ 中存在唯一的包含元素对 $\{a_i,a_j\}$ 的三元组 $\{a_{i},a_{j},a_{k}\}$ 因而， $\{c_{ir},c_{jr},c_{kr}\}$ 是 $\mathcal{S}$ 中包含元素对 $\{c_{ir},c_{jr}\}$ 的唯一三元组.

情况2 $i = j$ ，由此 $r \neq s$ ，因为 $\mathcal{S}_2$ 是Steiner三元组系，则 $\mathcal{S}_2$ 中存在唯一包含元素对 $\{b_r, b_s\}$ 的三元组 $\{b_r, b_s, b_t\}$ 。从而 $\{c_{ir}, c_{is}, c_{it}\}$ 是 $\mathcal{S}$ 中包含元素对 $\{c_{ir}, c_{is}\}$ 的唯一的三元组。

情况3 $i \div j$ 和 $r \div s$ 。因为 $\mathcal{S}_1$ 是Steiner三元组系，则 $\mathcal{S}_1$ 中存在包含 $\{a_i, a_j\}$ 的唯一的三元组 $\{a_i, a_j, a_k\}$ ，因为 $\mathcal{S}_2$ 是Steiner三元组系，那么 $\mathcal{S}_2$ 中存在包含 $\{b_r, b_s\}$ 的唯一的三元组 $\{b_r, b_s, b_t\}$ 。所以 $\{c_{ir}, c_{js}, c_{ki}\}$ 是 $\mathcal{S}$ 中包含元素对 $\{c_{ir}, c_{js}\}$ 的唯一三元组。

因此， $\mathcal{S}$ 是一个vw阶Steiner三元组系。

例用上面定理构造一个9阶Steiner三元组系.设 $X = \{a_{1}, a_{2}, a_{3}\}$ , $\mathcal{S}_{1}$ 是仅有一个三元组 $\{a_{1}, a_{2}, a_{3}\}$ 的3阶Steiner三元组系.设 $Y = \{b_{1}, b_{2}, b_{3}\}$ , $\mathcal{S}_{2}$ 是仅有一个三元组 $\{b_{1}, b_{2}, b_{3}\}$ 的3阶Steiner三元组系.令 $Z = \{c_{i} : 1 \leqslant i \leqslant 3, 1 \leqslant j \leqslant 3\}$ 是9个元素的集合，其元素排列成如下阵列：

$$
\begin{array}{l l l} b _ {1} & b _ {2} & b _ {3} \\ a _ {1} & c _ {1 1} & c _ {1 2} \\ a _ {2} & c _ {2 1} & c _ {2 2} \\ a _ {3} & c _ {3 1} & c _ {3 2} \end{array}
$$

使用定理9.4.2证明中的构造方法，得到下面12个三元组：

(i) $\{c_{11}, c_{21}, c_{31}\}, \{c_{12}, c_{22}, c_{32}\}, \{c_{13}, c_{23}, c_{33}\}$ ;

(ii) $\{c_{11}, c_{12}, c_{13}\}, \{c_{21}, c_{22}, c_{23}\}, \{c_{31}, c_{32}, c_{33}\}$ ;   
(iii) $\{c_{11}, c_{22}, c_{33}\}$ , $\{c_{12}, c_{23}, c_{31}\}$ , $\{c_{13}, c_{21}, c_{32}\}$ ,

$$
\left\{c _ {1 2}, c _ {2 2}, c _ {3 1} \right\}, \quad \left\{c _ {1 2}, c _ {2 1}, c _ {3 3} \right\}, \quad \left\{c _ {1 1}, c _ {2 3}, c _ {3 2} \right\}.
$$

(iii)中的三元组是 $Z$ 元素的所有这样的三元组，它恰有阵列每行的一个元素和每列的一个元素。根据定理9.4.2，上面的12个三元组形成一个9阶Steiner三元组系。如果分别以1,2,3,4,5,6,7,8,9代替 $c_{11}, c_{21}, c_{31}, c_{12}, c_{22}, c_{32}, c_{13}, c_{23}, c_{33}$ ，则S的这些三元组是

{1，2，3} {1.4.7} {1.5.9} {3.5.7}  
{4，5，6} {2，5，8} {3，4，8} {2，4，9}   
{7,8,9} {3,6,9} {2,6,7} {1,6,8}.

这个9阶Steiner三元组系为下面的问题提供了一个解，它是Kirkman女学生问题的一个较小型式。9名女学生天天按3人一行排成3行进行散步，能否作出一个连续4天的散步计划，使得没有一个女生和她的任何同学同在一个三人组散步的次数多于一次？注意，由于一名女生每天有2名同伴并且其他的女生只有8名，不可能作出一个连续4天以上的散步计划，使得没有一个女生和她的任何同学同在一个三人组中散步多于一次。但是对于连续4天，上面的9阶Steiner三元组系给出了一个解。4个列的每一列中的三元组给出4天中一天的安排。

设 $X = \{1,2,\dots ,v\}$ 是 $v = 6n + 3$ 个元素的集合， $\mathcal{S}$ 是用 $X$ 的元素构造的 $v$ 阶Steiner三元组系.如果 $\mathcal{S}$ 的

$$
b = \frac {v (v - 1)}{6} = (2 n + 1) (3 n + 1)
$$

个三元组可以划分成 $3n + 1$ 个且每一个包含 $2n + 1$ 个三元组的集合 $\tau_{1},\tau_{2},\dots ,\tau_{3n + 1}$ ，使得 $X$ 的 $v$ 个元素的每一个都恰好出现在

$\tau_{1}, \tau_{2}, \cdots, \tau_{3n+1}$ 中每一个集合内的一个三元组里，则 $\mathcal{S}$ 称为一个 $v$ 阶Kirkman三元组系。注意由于 $X$ 的每个元素 $x$ 恰好出现在 $\mathcal{S}$ 的

$$
r = \frac {v - 1}{2} = 3 n + 1
$$

个三元组中，用上述方式划分 $\mathcal{S}$ 在数量上是可能的。如果 $n = 0$ ，则 $v = 3$ ，并且 $\{1,2,3\}$ 是一个 $v$ 阶Kirkman三元组系。（这时 $3n + 1 = 1$ ，只有一个 $\tau_{i}$ 。）如果 $n = 1$ ，则 $v = 9$ ，并且上例中构造的9阶Steiner三元组系就是一个9阶Kirkman三元组系。 $b = 12$ 个三元组被划分成 $3n + 1 = 4$ 个集合，而每个集合有 $2n + 1 = 3$ 个三元组，使得每个元素恰在4人集合中每个集合的一个三元组里出现。若 $n = 2$ ，则 $v = 15$ ，并且一个15阶Kirkman三元组系等价于Kirkman女学生问题的一个解—— $b = 35$ 个三元组要划分成 $3n + 1 = 7$ 个集合，而每个集合有 $2n + 1 = 5$ 个三元组，使得每个元素恰好在每个集合中的一个三元组里出现。

对于每个 $n = 0,1,2,3,\dots$ ，是否存在 $\pmb {v} = 6\pmb{n} + 3$ 阶Kirkman三元组系，曾经是一个百余年都未解决的问题.在1971年D.R.Ray-Chaudhuri和D.M.Wilson)对所有 $\pmb {n} = 0,1,2,3,\dots$ 指出了怎样构造 $\pmb {v} = 6\pmb {n} + 3$ 阶Kirkman三元组系，从而解决了这个问题，在Kirkman提出他的女学生问题的同年（1850）就有了如下的解：

<table><tr><td>τ1</td><td>τ2</td><td>τ3</td><td>τ4</td></tr><tr><td>{1, 2, 3}</td><td>{1, 4, 5}</td><td>{1, 6, 7}</td><td>{1, 8, 9}</td></tr><tr><td>{4, 8, 12}</td><td>{2, 8, 10}</td><td>{2, 9, 11}</td><td>{2, 12, 14}</td></tr><tr><td>{5, 10, 15}</td><td>{3, 13, 14}</td><td>{3, 12, 15}</td><td>{3, 5, 6}</td></tr><tr><td>{6, 11, 13}</td><td>{6, 9, 15}</td><td>{4, 10, 14}</td><td>{4, 11, 15}</td></tr><tr><td>{7, 9, 14}</td><td>{7, 11, 12}</td><td>{5, 8, 13}</td><td>{7, 10, 13}</td></tr></table>

1) ——Kirkman 女学生问题, 见 American Mathematical Society Proceedings, Symposium on Pure Mathematics 19 (1971), 187-204.

<table><tr><td>τθ</td><td>τθ</td><td>τγ</td></tr><tr><td>{1, 10, 11}</td><td>{1, 12, 13}</td><td>{1, 14, 15}</td></tr><tr><td>{2, 13, 15}</td><td>{2, 4, 6}</td><td>{2, 5, 7}</td></tr><tr><td>{3, 4, 7}</td><td>{3, 9, 10}</td><td>{3, 8, 11}</td></tr><tr><td>{5, 9, 12}</td><td>{5, 11, 14}</td><td>{4, 9, 13}</td></tr><tr><td>{6, 8, 14}</td><td>{7, 8, 15}</td><td>{6, 10, 12}</td></tr></table>

这些三元组的集合 $S$ 是一个15阶Steiner三元组系，并划分成7个集合而每个集合含有所指明的5个三元组。在 $\tau_{1}$ 中的三元组给出第一天散步的一种安排，在 $\tau_{2}$ 中的三元组给出第二天散步的一种安排，等等。

# 练习

1. 证明一个域满足定理9.1.1中所给出的性质6至性质13.  
2. 试将域 $Z_{3}$ 的加法和乘法表完整地写出来，确定每个元素的加法逆元和每个非零元素的乘法逆元。  
3. 试将域 $Z_{\mathfrak{p}}$ 的加法和乘法表完整地写出来，确定每个元素的加法逆元和每个非零元素的乘法逆元。  
4. 设 $a$ 与 $b$ 是整数且 $b \neq 0$ . 考虑下面一串等式：

$$
a = q _ {1} b + r _ {1} \quad 0 <   r _ {1} <   b
$$

$$
b = q _ {2} r _ {1} + r _ {2} \quad 0 <   r _ {2} <   r _ {1}
$$

$$
r _ {1} = q _ {3} r _ {2} + r _ {3} \quad 0 <   r _ {3} <   r _ {2}
$$

$$
r _ {2} = q _ {4} r _ {3} + r _ {4} \quad 0 <   r _ {4} <   r _ {3}
$$

#

$$
\begin{array}{l} r _ {k} = q _ {k + 2} r _ {k + 1} + r _ {k + 2} \quad 0 <   r _ {k + 2} <   r _ {k + 1} \\ r _ {k + 1} = q _ {k + 3} r _ {k + 2}. \\ \end{array}
$$

于是，在a除以b时得到商q和余数r，然后用r除b，得到商q和余数

$r_{\pm}$ ，仿此继续下去直到第一次作除法时所得的余数为零，证明在这一串等式中最后那个非零余数 $r_{k + 2}$ 是 $\pmb{a}$ 与 $\pmb{b}$ 的最大公因子.

5.（续练习4）按相反次序利用练习4那一串等式，证明存在整数 $x$ 与 $y$ 使得 $a$ 与 $b$ 的最大公因子可以表为 $ax + by$ 。特别，如果 $a$ 与 $b$ 的最大公因子是1，则存在整数 $x$ 与 $y$ 使得 $ax + by = 1$ 。

6. 如果 $n$ 是一个比 1 大且不为素数的整数, 那么在定理 9.1.2 中已经看到, 使用模 $n$ 加法和乘法, $Zn$ 不是一个域, 确定 $Z_{\mathfrak{p}}$ 的哪个元素有乘法逆元. 同样, 确定 $Z_{\mathfrak{p}}$ 的哪个元素有乘法逆元.

7. 假若 $n$ 是一个大于 1 且不为素数的整数，描述 $Zn$ 中有乘法逆元的元素的特征。

8. 在域 $Z_{19}$ 中，计算

(a) -3

(e) 3-1

(i) $5 \times 12$

(b) -11

(f) 7-1

(j) 5/7

(c) 13+17

（g） $3\times 5$

（k） 2/9

(d）3-12

（h）T×8

(I) $1 / 8$

9. 在域 $Z_{7}$ 中用两种不同的方法计算 $2 \times (3 - 4)$ .

10. 设 $F$ 是一个域，又令 $f(z)$ 与 $g(z)$ 是 $F(z)$ 中的非零多项式。证明 $\deg f(z)g(z) = \deg f(z) + \deg g(z)$ .

11. 设 $F$ 是一个域，又令 $f(z)$ 和 $g(z) \neq 0$ 是 $F(z)$ 中的多项式。证明在 $F(z)$ 中存在多项式 $a(z)$ 和 $r(z)$ 且 $\deg r(z) < \deg g(z)$ ，使得 $f(z) = a(z)g(z) + r(z)$

12. 设 $F$ 是一个域，又令 $f(z)$ 和 $g(z)$ 是 $F(z)$ 中两个不全为 0 的多项式。证明 $f(z)$ 与 $g(z)$ 的任意两个最大公因子彼此互为常数倍。

13. 设 $F$ 是一个域，又令 $f(z)$ 和 $g(z) \neq 0$ 是 $F(z)$ 中的多项式，考虑下面的一串等式：

$$
f (z) = q _ {1} (z) g (z) \div r _ {1} (z), \quad 0 <   \deg r _ {1} (z) <   \deg g (z),
$$

$$
g (z) = q _ {2} (z) r _ {1} (z) + r _ {2} (z), \quad 0 <   \deg r _ {2} (z) <   \deg r _ {1} (z),
$$

$$
r _ {1} (z) = q _ {3} (z) r _ {2} (z) + r _ {3} (z), \quad 0 <   \deg r _ {3} (z) <   \deg r _ {2} (z),
$$

$$
r _ {2} (z) = q _ {4} (z) r _ {3} (z) + r _ {4} (z), \quad 0 <   \deg r _ {4} (z) <   \deg r _ {3} (z),
$$

$$
\begin{array}{l} r _ {k} (z) = q _ {k + 2} (z) r _ {k + 1} (z) + r _ {k + 1} (z), \\ 0 <   \deg r _ {k + 2} (z) <   \deg r _ {k + 1} (z), \\ \end{array}
$$

$$
r _ {k + 1} (z) = q _ {k + 3} (z) r _ {k + 2} (z).
$$

证明 $r_{k+2}(z)$ 是 $f(z)$ 和 $g(z)$ 的最大公因子.

14.（续练习13）按相反次序使用练习13的那一串等式，证明如果 $d(z)$ 是 $f(z)$ 与 $g(z)$ 的最大公因子，则在 $F(z)$ 中有多项式 $s(z)$ 和 $t(z)$ ，使 $d(z) = s(z)f(z) + t(z)g(z)$ ，特别如果 $f(z)$ 和 $g(z)$ 是互素时，则存在多项式 $s(z)$ 与 $t(z)$ ，使得 $1 = s(z)f(z) + t(z)g(z)$ .  
15. 设 $F$ 是实数域 $\mathbb{R}$ . 又令 $z^2 + z + 2$ 是 $\mathbb{R}(z)$ 中的一个多项式, 证明 $z^3 + z + 2$ 在 $\mathbb{R}(z)$ 中是不可约的. 在具有模 $z^3 + z + 2$ 通常多项式加法和乘法的域 $\mathbb{R}(z; 2)$ 中, 计算

(a) $(2z + 3)\times (z - 1).$ （20 (d） $5^{-1}$   
(6) $(3z + 2) / (2z + 3)$ .   
(c） $(2z + 3)^{-1}$

16. 设 $F = Z_{2}$ ，考虑 $Z_{2}(z)$ 中的多项式 $z^{3} + z + 1$ ，证明 $z^{3} + z + 1$ 在 $Z_{2}[z]$ 中是不可约的，在具有模 $z^{3} + z + 1$ 通常多项式加法和乘法的 $Z_{2}[z;3]$ 中计算

(a) $(z^{2} + 1) + (z^{2} + z + 1)$ . (d) $(z + 1)^{-1}$ .   
(b) $z \times (z^2 + z + 1)$ . (e) $(z^2 + z + 1)^{-1}$ .   
(c） $(z^{2} + 1)\times (z^{2} + z).$ （20 $(f)(z + 1) / (z^2 +z + 1),$

这个域有多少个元素？

17. 在 $Z_{2}(z)$ 中确定一个 2 次不可约多项式，从而构造一个有 9 个元素的域。求这个域中的 $z^{-1}$ 。  
18. 证明 $z^2 + z + 4$ 是 $Z_{11}(z)$ 中的一个不可约多项式，从而构造一个有121个元素的域，在这个域中计算

(a) $(3 + 7z)\times (2 + 4z)$ （20 (c） $(4 + 5z) / (2 + 3z).$   
(b) $(2 + 3z)^{-1}$

19. 证明两个方程 $cx + dy + e = 0$ 与 $c'x + d'y + e' = 0$ 确定 $\mathrm{AP}(F)$

中的同一直线当且仅当 $F$ 中存在一个元素 $k$ 使 $c^{\prime} = kc$ , $d^{\prime} = kd$ 和 $e^{\prime} = ke$ . 由此推出直线 $y = mx + b$ 与 $y = m^{\prime}x + b^{\prime}$ 是同一条直线当且仅当 $m^{\prime} = m$ 与 $b^{\prime} = b$ .

20. 证明对于每一个域 $F$ ，包括无限域， $\mathbf{AP}(F)$ 是一个仿射平面。（对于无限域，计数论断不能应用，例如在直线上有无限个点。）

21. 假定有人采取我们对于域所用过的方法去定义仿射平面 $\mathrm{AP}(Z_{4})$ . 试考察仿射平面（9.2节）的性质A1，A2和A3哪些不成立，试确定在给定直线上有多少点和多少条直线过已知点.

22. 设 $F$ 是一个域，证明仿射平面 $\mathrm{AP}(F)$ 的两直线 $y = mx + b$ 与 $y = m^{\prime}x + b^{\prime}$ 平行当且仅当 $m = m^{\prime}$

23. 构造仿射平面 $\mathrm{AP}(Z_k)$ . 确定平行直线类

24. 用 4 个元素的域 $F$ , 确定 $\mathbf{AP}(F)$ 的所有平行直线类 (这是 9.2 节最后一例的继续).

25. 在 $\mathrm{AP}(Z_7)$ 中确定过点（2，6）且平行于直线 $y = 3x + 4$ 的直线方程.

26. 在 $\mathrm{AP}(Z_{11})$ 中确定平行于直线 $y = 9x + 5$ 且过点（3.7）的直线方程.

27. 利用练习16的域 $Z_{2}(z;3)$ ，在 $\mathrm{AP}(Z_2(z;3))$ 中确定平行于直线 $y = zx + z^2$ 且过点 $(z, z + 1)$ 。

28. 利用练习15的域 $\mathbb{R}(z; 2)$ ，在 $\mathrm{AP}(\mathbb{R}(z; 2))$ 中确定平行于直线 $y = zx + 3$ 且过点（1，2）的直线方程。

29. 采用练习24和9.3节的方法构造3个两两正交的4阶拉丁方，

30.构造两个7阶正交拉丁方，

31.构造6个两两正交的7阶拉丁方，

32. 构造两个11阶正交拉丁方.

33. 构造两个8阶正交拉丁方.

34. 设 $A$ 是一个 $n$ 阶拉丁方，且存在一个 $n$ 阶拉丁方 $B$ ，使 $A$ 与 $B$ 是正交的。证明 $A$ 中存在位于不同行、不同列上的 $n$ 个位置上的元素的集合是 $\{1, 2, \dots, n\}$ 。

35. 构造两个15阶正交拉丁方.  
36. 构造两个12阶正交拉丁方.  
37. 利用练习30的结果和一对3阶拉丁方构造出一对21阶正交拉丁方.  
38. 设 $n \geqslant 3$ 是整数, 并假定 $L_{1}, L_{2}, \ldots, L_{n-1}$ 是两两正交的拉丁方. 证明怎样用这些拉丁方去构造一个 $n$ 阶仿射平面.  
39. 把下面 $3 \times 6$ 拉丁矩形补足为一个6阶拉丁方。

$$
\left( \begin{array}{c c c c c c} 1 & 3 & 4 & 6 & 5 & 2 \\ 3 & 4 & 1 & 5 & 2 & 6 \\ 5 & 6 & 3 & 2 & 4 & 1 \end{array} \right).
$$

40. 把下面 $3 \times 7$ 拉丁矩形补足为一个7阶拉丁方，

$$
\left( \begin{array}{c c c c c c c} 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ 3 & 4 & 1 & 7 & 6 & 5 & 2 \\ 2 & 5 & 7 & 1 & 3 & 4 & 6 \end{array} \right).
$$

41. 说明怎样构造一个 $21 = 7 \times 3$ 阶Steiner三元组系。   
42°.构造一个19阶Steiner三元组系，  
43. 设 $\mathcal{S}$ 是一个13阶Steiner三元组系, 试从数值上考察, 证明把 $\mathcal{S}$ 的三元组集合划分一些集合, 使每个元素恰好在每个集合内的一个三元组中是不可能的.  
44. 证明由定理9.4.2能推出，对于每个正整数r存在一个3阶Steiner三元组系。  
45. 证明由定理9.4.2能推出，如果存在一个 $v$ 阶Steiner三元组系，则对于每个正整数 $r$ ，存在一个 $v^r$ 阶Steiner三元组系。  
46. 证明（9.4.2）给出了Kirkman女学生问题的一个解。

# 第十章 图论入门

图论是组合数学的一个重要课题。图论为离散事物集合提供数学模型，其中某些事物对是由某种方式联系着的。这些事物可以是依据相互友谊彼此联系着的人们，或者是电网络中由电气元件连结的一些节点，或者是有机分子中的彼此用化学方法联系着的一些原子，等等。图论的第一篇论文是由瑞士数学家L. Euler于1736年写成的，解决了当时著名的哥尼斯堡桥问题。图论起源于智力游戏。当今，它为研究任何一类离散事物的关系结构提供了一种本质的框架。人们一直在使用着各种图，它的理论已应用于经济学、心理学、社会学、遗传学、运筹学等方面。

在第八章中，在讨论相异代表组与匹配时，已经介绍了一类特殊图，即偶图。但是，本章不假定已经学过第八章内容。每章都可独立地阅读。

# 10.1 图的基本性质

图 $G$ 是由称为结点的事物的有限集合 $N$ 以及结点的一些无序对的重集 $E$ 组成，结点的无序对称为边，记为 $[x, y]$ 。我们用 $G = (N, E)$ 表示这种图。如果 $e = [x, y]$ 是一条边，则称 $x$ 与 $y$ 为边 $e$ 的结点，且说 $e$ 连接 $x$ 和 $y$ 。我们也说结点 $x$ 和 $y$ 交于边 $e = [x, y]$ ，如果两条边有一个公共结点，也说它们是相交的，按下列方式把图 $G$ 想象为几何实体是方便的。对于 $G$ 的每一结点，选取平面上的一点与之相对应，且用不同的点来表示不同的结点，对于每条边 $[x, y]$ ，将表示结点 $x$ 和 $y$ 的点用

直线段或曲线连接起来。这些曲线除了通过对应的曲线的两个端点外，不再通过表示图的其它结点的点。

例设 $G = (N,E)$ 是一个图，它的结点集合是 $N = \{x,y,z,u,v\}$ ，其边是 $[x,y],[x,z],[x,v],[y,z],[y,u],[z,v],[u,v]$ 。这个图可以画在平面上，如图10.1所示。

由于边是由无序结点对组成，边 $e = [x, y]$ 与边 $[y, x]$ 是一样的。于是，用什么次序表示边的结点无关紧要。还可能一条边的两个结点是相同的。这样的边 $e = [x, x]$ 称为环。环是一个结点与它自身连结。

![](images/a56a8fb499f710b55f728193984b0ca78a6bf76fdc5492a9611f1bc8b98ae32d.jpg)  
图10.1

例图10.2中画出的图 $G$ 有4个结点和9条边，其中4条

边为环。图10.3中画出的图 $H$ 有13个结点和21条边，其中2条边是环。注意，对于 $H$ 的某些结点对，连接它们的边不只一条。这是允许的，因为图的边形成

重集.

![](images/92d6536a754e98b61e4a2705d1384ea82163269e63311b08dacb03c0ddaf0a0d.jpg)  
图10.2

![](images/78b73934742bbbf65f5eb76b3a6585f8666d40502ded25387657fb1c9476ff0d.jpg)  
图10.3

当然，我们不总能够在平面上如此容易地画出一个图。有时我们不得不使两条曲线相交于一点，这个交点并不表示图的结点。

例 考虑具有 5 个结点 $x, y, z, u$ 和 $v$ 的图 $G$ ，使得每一对不同的结点都有一条边连接，如图 10.4 所示。这里，表示边 $[x, u]$ 与 $[z, v]$ 的曲线相交于一点，该点不对应于图 $G$ 的结点。

![](images/4ecc88283bf0e194cb233f6dfbfa1ff6ac41cb21778a771e0f75e699d1a11030.jpg)  
图10.4

我们也可以把图 $G$ 画成具有5条对角线的五角形，如图10.5所示。这时，对应于这些边的直线段有5个交点，它们都不是 $G$ 的结点。不可能在平面内画出这个图，使得任何两条曲线的交点都对应图 $G$ 的结点。读者可以验证这一点（参阅定理

11.2.3).

一个图称为可平面的，只要它可以画在平面上，使任何两条曲线只能在对应于图的结点的那些点上相交。这样得到的图叫做平面图。前面的两个例子的图都是平面图，而上面例子的图是不可平面的。

![](images/01cbcf82cda0e8bacaf573f3bf8455eb071b80576a9b41eb9b46258aad6909f3.jpg)  
图10.5

在一个图中，结点 $x$ 的度是相交于 $x$ 的边的条数。因为环 $[x, x]$ 连接 $x$ 自身，在计算 $x$ 的度时，我们把环 $[x, x]$ 计算两次。图10.1的图中的结点 $x, y, z, u$ 和 $v$ 的度分别为3，3，3，2和3。

定理10.1.1 设 $G$ 是图，则度为奇数的结点的个数为偶数。

证明 设图 $G$ 的结点是 $x_{1}, x_{2}, \cdots, x_{n}$ , 结点的度分别是 $d_{1}, d_{2}, \cdots, d_{n}$ . 设图 $G$ 的边的总数是 $m$ . 因为每条边连接两个结点, 每条边使两个结点的度各增加 1 , 或者在环的情况下, 对一个结点的度增加 2 . 于是, 和 $d_{1} + d_{2} + \cdots + d_{n}$ 对每条边都计算 2 次. 因此,

$$
d _ {1} + d _ {2} + \dots + d _ {n} = 2 m,
$$

为偶数。由这个方程可知， $d_{i}$ 是奇数的结点的个数必是偶数。

这个定理可以给出如下有趣的解释。假定在一个晚会上有 $n$ 个人，并且在这个晚会中间，某些人要握手。那么在晚会结束时，握手为奇数次的人的总数是偶数。为弄清楚这一点，我们作一个图，其结点分别表示 $n$ 个人，并且当且仅当两个人在晚会上握过手时，相应的两个结点就用一条边连结起来。这时就可以使用这个定理。

设 $G = (N, E)$ 是图， $G$ 中的链 $\gamma$ 是 $k \geqslant 2$ 个结点 $x_1, x_2, \dots, x_k$ 的序列，使得 $\{[x_1, x_2], [x_2, x_3], \dots, [x_{k-1}, x_k]\}$ 是重集 $E$ 的子集。于是，链中相邻的结点由边连结，并且每条边的重复次数不多于它在 $E$ 中的重复数。我们称 $\gamma$ 为连接 $x_1$ 和 $x_k$ 的链。边 $[x_1, x_2], [x_2, x_3], \dots, [x_{p-1}, x_k]$ 叫做链 $\gamma$ 的边。 $\gamma$ 的长度是它的边的个数，每条边按它在链中的重复数进行计数。我们可以把连接 $x_1$ 和 $x_k$ 的链 $\gamma$ 想象为由图的边组成的一条路，使得沿路从 $x_1$ 到 $x_k$ 没有一条边被通过一次以

上。允许多次通过一个结点，只要是沿着不同的边1)通过它，基本链是结点 $x_{1}, x_{2}, \cdots, x_{k}$ 都不相同的链。圈的定义同链 $y$ 的定义一样，只是结点 $x_{1}$ 与 $x_{k}$ 相同。基本圈是除了第一个和最末一个结点外所有结点都不相同的圈。一个圈的每个结点与圈的偶数个边相交，而基本圈的每个结点恰好与圈的两条边相交。最后，若图 $G$ 仅有一个结点，或者多于一个结点且每对不同结点有链连接，则称 $G$ 为连通的。

![](images/9bc0aedaa33f9da1efad9fcfbf193dfab76c2918102fe93fc5031245c5868a86.jpg)  
图10.6

![](images/b8737bc66a92ab24c9f444bdb3bb5a5b5c3f45d690626bc8e6400e63ca89bd86.jpg)

![](images/326260da6627036467ab23ad0683ac611a620c28570f47bf9a28208786c85042.jpg)  
图10.7

例 考虑图10.6中所示的图。 $x, y, v, w, y, z, u$ 是连接 $x$ 和 $u$ 的链，但不是基本链，因为结点 $y$ 是重复的。从这个链中去掉圈 $y, v, w, y$ ，我们就得到连接 $x$ 和 $u$ 的基本链 $x, y, z, u$ 。在这个图中，显然每对不同的结点有一链连接，因而是连通的。

不连通的图例如图10.7所示。显然在该图中不存在联接 $x$ 和 $u$ 的链。

如果图 $G = (N, E)$ 是不连通的，则可以把结点集合 $N$ 唯一地划分为子集 $N_{1}, N_{2}, \cdots, N_{k}$ ，使得同一子集的任何两个结点有链连接它们，而不同子集的两个结点没有连接它们的链（见练习7，8，9）。对于 $i = 1, 2, \cdots, k$ ，设 $G_{i} = (N_{i}, E_{i})$ 是图，其结点的集合是 $N_{i}$ ， $E_{i}$ 是由 $E$ 的所有连接 $N_{i}$ 中两结点的边组成。则称 $G_{1}$ ， $G_{2}$ ， $\cdots$ ， $G_{k}$ 为 $G$ 的连通分图。当且仅当一个图恰好有一个连通分图时，这个图是连通的。图10.7中所示的图有两个连通分图“三角形”和“矩形”。

本节中我们讨论图的某些基本性质留在练习中进一步研究。

# 10.2 Euler链与Euler圈

Euler在1736年关于图论的一篇论文中解决了当时著名的哥尼斯堡桥问题。东普鲁士的古城哥尼斯堡（现在的加里宁格勒）座落在普莱格尔河两岸和河中两个孤岛上，城的各个部分由七座桥连接，如图10.8所示。每当星期天，哥尼斯堡的市民们喜欢在城中散步，于是产生这样一个问题，能否设计出一条

![](images/37bfc89b19d487a5662292bdfa04655e1d925083e0806a69288e5448eca1d4f6.jpg)  
图10.8

路线，使得由家门出发通过七座桥的每一桥恰好一次之后，返回原地呢？

Euler用图10.9所示的图代替图10.8所示的哥尼斯堡地图。按照这个图，问题就变成为确定是否存在由图的全部边所组成的圈。

![](images/3e5157c1c2dd49f62465733b42e38ebf906f1d22189de6194c0ce586c92406a1.jpg)

我们要解决任意图的这种问题。假设 $G$ 是图， $G$ 中的 Euler 圈是包含 $G$ 的全部边的图。于是，Euler 圈是这样的一种圈，其中每条边出现的次数等于在边的重集1)中它的重复数。连接两个结点 $x$ 和 $y$ 的Euler链是连接 $x$ 和 $y$ 且 $G$ 的全部边都出现的一条链。

例 在图10.10的左图中， $x, y, v, u, w, y, z, u, x$ 是Euler圈。右图中， $x, y, z, s, r, v, w, s, v, x, u, z$ 是连接 $x$ 和 $z$ 的Euler链。

![](images/898db7c2f77ede3f85c390628ffb60d032a6df41575e7a20f97c497450f429bd.jpg)

![](images/01dd6e065819a142a89fded85abcf5ae1337029783e0809544d636283df4320f.jpg)  
图10.10

定理10.2.1 设 $G$ 是连通图，当且仅当所有结点的度为偶数时 $G$ 有Euler图。

证明 首先假设 $G$ 有 Euler 圈。当我们沿一个圈行进时，每次经过一条边进入一个结点，又必须经过另外一条边离开它。因为 $G$ 的每条边是圈的一条边，可见 $G$ 的每个结点的度是偶数。其次假设 $G$ 的全部结点的度是偶数。令 $x$ 是 $G$ 的任何一个结点，又令 $\gamma$ 是最长的链，它连接 $x$ 到某结点 $y$ 。如果 $x \neq y$ ，则 $y$ 与 $\gamma$ 的奇数个边相交。由于 $y$ 的度为偶数，存在 $G$ 的一条边 $[y, z]$ ，它不是 $\gamma^{1}$ 的边。因而存在连接 $x$ 到 $z$ 的链 $\gamma'$ ，它的长度大于 $\gamma$ 的长度。这是矛盾的。所以， $y = x$ 从而 $\gamma$ 是个圈。若 $G$ 的每条边都是 $\gamma$ 的边，则 $\gamma$ 是 Euler 圈。假若存在 $G$ 的一条边，它不是 $\gamma$ 的边。由于 $G$ 是连通的，存在 $G$ 的一条边 $e$ ，它不属于 $\gamma$ 使得 $e$ 连接圈 $\gamma$ 的一个结点 $u$ 与某个其它结点（也许是 $\gamma$ 的结点）。令 $G^{*}$ 是由 $G$ 去掉 $\gamma$ 的所有边而得到的图。那么，由于 $G$ 的每个结点与 $\gamma$ 的偶数条边相交（可能为零），在图 $G^{*}$ 中每个结点的度是偶数。考虑 $G^{*}$ 的包含边 $e$ 的连通分图。如上所述，我们发现图 $G^{*}$ 中的圈 $\gamma^{*} = u, v, \cdots, u$ 。我们可以把这个圈同图 10.11 中所表示的那种圈 $\gamma$ 连接起来，便得到 $G$ 中的一个圈，它的长度大于 $\gamma$ 的长度。这是矛盾的，因此 $G$ 的每条边是 $\gamma$ 的边，从而 $\gamma$ 是 Euler 圈。

![](images/29c04aeef2a6dcb75293725875b49d4ae5a4b3717de8110920cf31767fe24648.jpg)  
图10.11

例 定理10.2.1的证明提供了在全部结点的度为偶数的连通图中构造Euler圈的一种算法。现在举例说明。

考虑图10.12中所示图 $G^{*}$ . 比如说由结点 $\pmb{x}$ 开始，沿一条链前进，在每个结点选择一条未用过的边直到不再可能为止。结果必定是一个圈 $\gamma$ ，但未必是Euler圈。图10.13中的圈 $\pmb{x}, \pmb{y}, z, w, s, r, v, u, z, v, x, y, u, x^{(1)}$ 是一种可能的 $\gamma$ 。

![](images/9df56819bab3893b1c7db978067103b089cf6d36c22a66411db610a7db47491e.jpg)  
图10.12

![](images/1a859b7f69c919cfa7a67f6d9762679d6b0c5e489357c6d1343a240c1db27401.jpg)  
图10.13

删掉这个圈的边，并且用同样方法来构造圈 $\gamma'$ ，使其始、末结点是 $\gamma$ 的某个点。图10.14中的圈 $v, z, s, r, w, v$ 是一种可能的 $\gamma'$ ，它在 $v$ 点开始并在 $v$ 点结束。

![](images/678e33798bccbc2e070e84cf649b00d99ce723c20ac4706e5498742c547cd6ac.jpg)  
图10.14

然后，可以把 $\gamma$ 与 $\gamma^{\prime}$ 连接起来，得到Euler圈 $\gamma'' = x, y, z, w, s, r, v, z, s, r, w, v, u, z, v, x, y, u, x$ .一般地来说，需要这样做几步才能得到Euler圈.

现在，我们用定理10.2.1来证明关于图中Euler链的存在性定理。

定理10.2.2 设 $G$ 是连通图，又设 $x$ 和 $y$ 是 $G$ 的不同结点，则当且仅当 (i) 结点 $x$ 与 $y$ 的度为奇数，(ii) 所有不同于 $x$ 和 $y$ 的结点的度为偶数时， $G$ 有连接 $x$ 与 $y$ 的 Euler 链。

证明首先假设 $G$ 有连接 $\pmb{x}$ 与 $\pmb{y}$ 的Euler链.因为 $\pmb{x}$ 与 $\pmb{y}$ 是不同的，故 $\pmb{x}$ 和 $\pmb{y}$ 的度都为奇数。另外，所有不同于 $\pmb{x}$ 和 $\pmb{y}$ 的结点的度为偶数。其次假定 $G$ 是连通的，结点 $\pmb{x}$ 和 $\pmb{y}$ 的度是奇数，并且不同于 $\pmb{x}$ 和 $\pmb{y}$ 的所有结点的度是偶数。令 $G^{*}$ 是由 $G$ 加入连接 $\pmb{x}$ 和 $\pmb{y}$ 的新边而得到的图。（这条边是除 $G$ 中可能已经连接 $\pmb{x}$ 和 $\pmb{y}$ 的边之外的新边。）则 $G^{*}$ 是连通的，并且 $G^{*}$ 的所有结点的度是偶数。应用定理10.2.1， $G^{*}$ 有Euler圈 $\pmb{\gamma}$ ，如果我们从 $G^{*}$ 中删掉连接 $\pmb{x}$ 与 $\pmb{y}$ 的新边，便得到 $G$ 中连接 $\pmb{x}$ 与 $\pmb{y}$ 的Euler链。

设 $G$ 是平面图，定理10.2.1和定理10.2.2可以用来回答下面的问题。何时才能用铅笔描出平面图 $G$ ，要求任何曲线都不多于一次，铅笔又决不离开纸。根据这两个定理，当且仅当 $G$ 没有或者恰有两个度为奇数的结点时，才能描出图 $G$ 。而且，当且仅当所有结点的度为偶数时，可以在同一个位置开始和结束。

例 图10.15中的平面图可铅笔不离开纸一笔描出它，因为全部结点的度都是4。图10.16中的图只要我们在结点 $x$ 开始和在结点 $y$ 结束，可以铅笔不离开纸一笔描出它，或者反之以结点 $y$ 开始以结点 $x$ 结束。图10.17中所画的图不能铅笔不离开纸一笔描出它，因为它有4个度为奇数的结点。

![](images/4485873fbec46dd72cf5f6fccb38e3f73e93015c6a73eb845d5a039917806460.jpg)  
图10.15

![](images/17b5a692be5bc8cfc665b457be94a636303199204accb5ffb70640b43afc32d9.jpg)  
图10.16

![](images/df5096cb462f4807a614fdbe5748ffe62f7ae32cd25bf4fe920b4e0da95308c2.jpg)  
图10.17

# 10.3 Hamilton链与Hamilton圈

1857年，爱尔兰数学家W.R.Hamilton提出一个难题，要沿着十二面体的棱确定一条路线，经过每个顶点恰好一次。十二面体的这些顶点与棱如图10.18所示。粗体的边确定了一条路线，它能在任何顶点开始，经过其余每个顶点恰好一次之后，又回到原处。

![](images/c0c61265f976286d2ac8325042db68ad45e41b1b63f42b9ebc1de76b99035db9.jpg)  
图10.18

对任何图都可以提出Hamilton难题。设 $G$ 是图， $G$ 中的 Hamilton圈是 $G$ 的每个结点都是其结点的基本圈 $\gamma$ 。如果 $G$ 的结点个数是 $n$ ，则 Hamilton圈是长为 $n$ 的基本圈。可以把 Hamilton圈方便地想象为沿着 $G$ 的边，从任何结点开始能够走遍其余各结点恰好一次后又返回原处的一条路线。连接两个不同结点 $x$ 和 $y$ 的 Hamilton链是 $G$ 的每个结点都是其结点的连接 $x$ 和 $y$ 的基本链 $\gamma$ 。如果 $G$ 的结点个数是 $n$ ，则连接 $x$ 与 $y$ 的 Hamilton链是连接 $x$ 和 $y$ 长为 $n - 1$ 的基本链。显然，有 Hamilton 圈或者有 Hamilton 链的图是连通图。

例 在图10.19的图 $G$ 中， $x, y, z, w, v, u, x$ 是Hamilton圈，而 $x, y, z, u, v, w$ 是连接 $x$ 和 $w$ 的Hamilton链。图10.20所示的图既没有Hamilton圈也没有Hamilton链。

![](images/82e0cfa456623a6f9b5b3d94585f020a51422e0bd73cb6f50d88cb5b49b81575.jpg)  
图10.19

![](images/37cadedd93e8a4193bdeb4dc989bf1ba7ff8459c24276216b53b55e4ee91d7f7.jpg)  
图10.20

尽管Hamilton圈与Euler圈类似，但是尚不知道图具有Hamilton圈的充分必要条件是什么。不过，已经知道几个充分条件，现在我们就来研究其中之一。粗略地说，这个条件保证了有一些边恰当地分布在图的结点上。1960年，O.Ore证明了下面定理。所谓简单图指的是不存在连接结点到自身的边，并且每对不同的结点至多有一条边连接的图<sup>1)</sup>。图10.19和10.20所示的图都是简单图。

定理10.3.1 设 $G$ 是有 $n \geqslant 3$ 个结点的简单图。假设满足下列条件：

(i)只要 $x$ 和 $y$ 是无边连接的不同结点并且 $x$ 的度加上 $y$ 的度至少是 $n_{\bullet}$

则 $G$ 有Hamilton图.

证明 首先证明 $G$ 是连通的。如果不然， $G$ 至少有两个连通分图。令一个连通分图有 $n_{1}$ 个结点，另一个有 $n_{2}$ 个结点。那么， $n_{1} + n_{2} \leqslant n$ 。设 $x$ 是第一个连通分图的结点，又设 $y$ 是第二个连通分图的结点，则 $x$ 和 $y$ 没有边连接，而且 $x$ 的度至多为 $n_{1} - 1$ ， $y$ 的度至多为 $n_{2} - 1$ 。由于 $(n_{1} - 1) + (n_{2} - 1) = (n_{1} + n_{2}) - 2 \leqslant n - 2$ ，这与给定的条件(i)矛盾，因此 $G$ 是连通的。

设 $\gamma = x_{1}, x_{2}, \cdots, x_{r}$ 是 $G$ 中最长的基本链，我们根据 $r = n$ 或 $r < n$ 分两种情况讨论。

情况1 $r = n$ 。那么 $\gamma$ 是连接 $x_{1}$ 和 $x_{2}$ 的Hamilton链。如果 $x_{1}$ 和 $x_{n}$ 有边连接，则 $x_{1}, x_{2}, \ldots, x_{i}, x_{1}$ 是Hamilton圈。假设 $x_{1}$ 和 $x_{n}$ 没有边连接，设 $x_{1}$ 和 $p \geqslant 1$ 个结点由边连接，又设 $x_{n}$ 和 $q \geqslant 1$ 个结点由边连接。从条件(i)我们知道， $p + q \geqslant n$ 。如果在 $x_{2}, x_{3}, \ldots, x_{i}$ 中存在某个结点 $x_{i}$ ，使得 $x_{1}$ 和 $x_{i}$ 连接，并且 $x_{n}$ 和 $x_{i-1}$ 连接，则 $x_{1}, x_{i}, x_{i+1}, \ldots, x_{n}, x_{i-1}, x_{i-2}, \ldots, x_{2}, x_{1}$ 是Hamilton圈（见图10.21）。现在，我们证明一定存在这样的结点 $x_{i}$ ，从而就完成这种情况的定理证明。既然 $x_{1}$ 连接结点 $x_{2}, x_{3}, \ldots, x_{n}$ 中的 $p$ 个结点，如果 $x_{n}$ 不连接 $p$ 个由 $x_{1}$ 所连接的结点的前一个结点中任何一个结点，那么连接 $x_{n}$ 的 $q$ 个结点必取自于 $(n-1)-p$ 个结点的集合。因而

$$
(n - 1) - p \geqslant q \text {或} n - 1 \geqslant p + q.
$$

这与 $p + q \geqslant n$ 这个事实矛盾。于是，第一情况证毕。

![](images/a4d241135a6e8a65fc7dcd4e0f3d9f458667c43a3e180419c021a9be3a8adaaf.jpg)  
图10.21

情况2 $r < n$ 。假设 $x_{1}$ 连接不在 $\gamma$ 上的结点 $y$ 。那么， $y, x_{1}, x_{2}, \cdots, x_{r}$ 是比 $\gamma$ 长的基本链。于是， $x_{i}$ 仅能与 $\gamma$ 上的一些结点连接。用同样的方法可以证明， $x_{i}$ 仅能与 $\gamma$ 上的一些结点连接。类似于情况1所用的论证，证明存在长为 $r$ 的基本圈 $\gamma' = y_{1}, y_{2}, \cdots, y_{r}, y_{1}$ 。由于 $G$ 是连通的，存在不是 $\gamma'$ 的结点 $z$ ，使得对某个 $j$ ， $z$ 连接 $y_{j}$ ，那么 $z, y_{j}, \cdots, y_{r}, y_{1}, \cdots, y_{j-1}$ 是长为 $r$ 的基本链，因而这与 $\gamma$ 是 $G$ 中的最长的基本链并且 $\gamma$ 的长为 $r - 1$ 的事实矛盾。这样，情况2不能出现。这便完成了定理的证明。

作为定理10.3.1的直接推论，有下面G。Dirac于1952年证明的结果。

推论设 $G$ 是有 $n \geqslant 3$ 个结点的简单图，其中每个结点的度至少是 $n / 2$ 。则 $G$ 有Hamilton圈。

例 考虑图10.22中所示的图。易见定理10.3.1的假设是满足的。因此， $G$ 有Hamilton圈。我们留给读者自己去找出它。

图的Hamilton圈的存在与构造问题是与所谓推销员的实

![](images/187dfebf181d0cf90e25d8482bde33a323a30304b6284e37d0b7d565a41a145e.jpg)  
图10.22

际问题有关。推销员的家在某个城市，并且希望作一次能够访问和他的公司有商务联系的 $m$ 个其它城市的旅行。这些城市中某些城间有航班，另一些没有。设 $G$ 是一个图，它的结点是 $m + 1$ 个城市，并且两个城间有航班时，用一条边连接起来。 $G$ 中的Hamilton圈对应于一条路，从他家开始，在访问其它 $m$ 个城市恰好一次之后又返回原地。更实际的问题是确定推销员的最经济（按时间或按钱）的路线。关于确定最经济路线还没有通用的好算法。

# 10.4 树

对于连通图的任何一对不同的结点，至少存在一个连接它们的基本链（见练习1）。每对不同的结点恰好有一个基本链连接的简单图称为树。图10.23中的图便是树的一例。

在图10.24中表示出分别有1，2，3，4，5个结点的

构造不同的树。图10.25表示出具有6个结点的结构不同的树。

![](images/6b8546cf22d3d4aac6da429ce5ea2559876794a97981ce1c10bf614109fef11e.jpg)  
图10.23

![](images/1e4252094b1818fc7d767029d93d6a8fc09276eb077f13b61ac9c57022755291.jpg)  
图10.24

![](images/ad49ade4a181ac31638e91e82fa5872e0c546babcde79137180d8893ae01adcc.jpg)  
6个结点  
图10.25

某些有机物分子可以表示为树——原子是结点，原子间的键是边。链烷烃系 $\mathrm{C}_{2} \mathrm{H}_{2 k + 2}$ 的碳氢化合物可以表示为树，有 $k$ 个碳原子并且每个碳原子有 4 个键，有 $2 k + 2$ 个氢原子并且每个氢原子有一个键。对于 $k = 4$ ，我们得到如图 10.26 所示的丁烷系。

![](images/36c6fdbc2fb7192d834e8ecdf5520bdef496ee388fe622f1de3e0092ec328982.jpg)

![](images/993a0b8e3eecdd01e18736db62acab849a22dfc816afe096d8e49f4be9cc99df.jpg)  
图10.26

在分子结构中氢原子不起作用，它是由碳原子间的键所决定。在丁烷系这种情况下，碳原子及其键对应者具有4个结点的二种不同的树。有5个结点的三种树中每一种都产生戊烷系（ $k = 5$ ）的三种碳氢化合物的一种。如果我们加入足够的氢键直至每个碳原子有4个键，便给出所需要氢原子的个数。图10.25中六个结点树的前五种的每一种树都给出己烷系 $(k = 6)$ 的五种碳氢化合物的一种；最末的一种树不对应任何一种己烷系的碳氢化合物，因为它有度为5的一个结点。

A. Cayley 于 1875 年在试图枚举化合物时首先研究了树。在这个过程中，他利用寻找结构不同的树预言了未知化合物的存在。

分类过程也确定了一种树。例如，假设有一组物体有三种颜色、两种形状和三种大小，要根据共同特征进行分类。为此，首先依据颜色把物体分类，其次依据形状分类，最后依据尺寸大小分类。这个过程确定了如图10.27中所示的树。

现在，我们研究树的某些性质。如果图 $G$ 有圈，则它有基本圈 $\gamma$ （见练习3）。一个基本圈的任何两个结点由两个不同

的基本链连接，即对应两个“半圈”的基本链。因此，树不可能有任何圈。

![](images/c85beef23913ea4a0888a2402b15d07e07ca7d7a7a830ab4b31d34b15c7e81d7.jpg)  
图10.27

定理10.4.1 当且仅当图 $G$ 是连通的并且没有图时， $G$ 是树。

证明 显然，树是连通的并且我们刚刚已经证明树没有圈。反之，设 $G$ 是没有圈的连通图。假设 $G$ 不是树，那么存在两个不同结点 $a$ 和 $b$ ，并且有连接 $a$ 和 $b$ 的两个不同的基本链 $x_{1} = a, x_{2}, \cdots, x_{m} = b$ 和 $y_{1} = a, y_{2}, \cdots, y_{n} = b$ 。由于这两个基本链是不同的，存在第一个整数 $i > 1$ ，使得 $x_{1} = y_{1}, \cdots, x_{i-1} = y_{i-1}, x_{i} \neq y_{i}$ 。而且，由于 $x_{m} = b = y_{n}$ ，存在第一个整数 $j > i$ ，使得对某个 $k, x_{j} = y_{k}$ （见图10.28）。这样一来， $x_{i-1}, x_{i}, \cdots, x_{j} = y_{i}, y_{k-1}, \cdots, y_{i}, y_{i-1}$ 是圈。这与 $G$ 没有圈的假设矛盾。所以， $G$ 是树。

![](images/705ddfb8b8fbd7ab98435078e5107b81cd8e34e3bf26803b7d5e8432e8f6697c.jpg)  
图10.28

在图 $G$ 中，悬挂结点是度为1的结点。如果 $\pmb{x}$ 是悬挂结点，则存在唯一边 $\pmb{e}$ ，使 $\pmb{x}$ 与 $\pmb{e}$ 相交，边 $\pmb{e}$ 称为悬挂边。

例 在图10.29所示的图 $G$ 中， $x$ 和 $y$ 是悬挂结点，而 $e$ 和 $f$ 是悬挂边。

![](images/01a13c3da4da93bb236fd92d14da86bf2e3c406899a1a5b04ebf432e5811e1ab.jpg)  
图10.29

定理10.4.2 设 $G$ 是有 $n \geqslant 2$ 个结点的树，则 $G$ 至少有一个悬挂结点。

证明 假若不然，令 $G$ 的每个结点的度至少为 2。以任何结点 $x_{1}$ 开始，并令 $x_{2}$ 连接 $x_{1}$ 。那么， $x_{2}$ 也连接一个结点 $x_{3} \neq x_{1}$ ，同样， $x_{3}$ 连接 $x_{4} \neq x_{3}$ 。如果如此继续下去，由于只有有限个结点，最终必然与某个结点重合。一旦与一个结点重合就得到一个圈。根据定理 10.4.1，树没有圈，这便得到矛盾。

根据这个定理我们断定，有 $n$ 个结点的任何两个树有同样多的边。

定理10.4.3 有 $n$ 个结点的树恰有 $n - 1$ 条边。

证明 我们对 $n$ 进行归纳来证明这个定理。显然，这个定理对于 $n = 1$ 时成立。设 $n > 1$ ，又设 $G$ 是有 $n$ 个结点的树。由定理10.4.2知， $G$ 有悬挂结点 $x$ 。设 $e$ 是与 $x$ 相交的悬挂边。设 $G'$ 是从 $G$ 中删掉结点 $x$ 和边 $e$ 而得到的图。显然， $G'$ 是连通的。但是，由于 $G$ 没有圈， $G'$ 也就没有圈。因而根据定理10.4.1， $G'$ 是树。因为 $G'$ 有 $n - 1$ 个结点，由归纳假设可知， $G'$ 有 $(n - 1) - 1 = n - 2$ 条边。所以， $G$ 有 $(n - 2) + 1 = n - 1$ 条边。于是，定理为真。

设 $G$ 是结点集合为 $N$ 的图。 $G$ 的生成树是这样的一种树，其结点与 $G$ 的结点相同，并且边是 $G$ 中的某些边。

例 对于图10.30中的图 $G$ ，图10.31所示的两个树都是生成树。

![](images/0a9caae2e438064fdc97f8097f996a3b52459e9c11067b9f0ff05230382e6471.jpg)  
图10.30

![](images/98da60b7da698f7e06d0bc930b72ac6021b036bec38c1fd5c66507a0465f09d5.jpg)

![](images/d620ecba7562681227594ea333aff3440f8e8d042d7859a51aa3ed797be1abac.jpg)  
图10.31

有 $n$ 个结点和恰好有 $n - 1$ 条边的图未必都是树。但是，我们有下面的定理。

定理10.4.4 设 $G$ 是有 $n$ 个结点和恰好 $n - 1$ 条边又不含有圈的图，则 $G$ 是树。

证明 假设 $G$ 不是连通的，那么 $G$ 有 $k \geqslant 2$ 个连通分图 $G_{1}, G_{2}, \cdots, G_{k}$ ，分别有 $n_{1}, n_{2}, \cdots, n_{k}$ 个结点，这里 $n = n_{1} + n_{2} + \cdots + n_{k}$ ，这些连通分图的每一个都没有圈，根据定理10.4.1，它们都是树。因此，根据定理10.4.3，对于 $i = 1, 2, \cdots, k$ ， $G_{i}$ 有 $n_{i} - 1$ 条边，于是， $G$ 的边的总数是

$$
\begin{array}{l} \left(n _ {1} - 1\right) + \left(n _ {2} - 1\right) + \dots + \left(n _ {b} - 1\right) = \left(n _ {1} + n _ {2} + \dots \right. \\ + n _ {5}) - k = n - k \leqslant n - 2 \\ \end{array}
$$

这与 $G$ 有 $n - 1$ 条边的假设矛盾。所以， $G$ 是连通的，并且由于 $G$ 没有圈，从定理10.4.1可知， $G$ 是树。

显然，不是连通的图不可能有生成树。对于连通图有下面定理。

定理10.4.5 设 $G$ 是连通图，则 $G$ 有生成树。如果相继地

删掉圈上的边直到不再有圈为止，结果确实是 $G$ 的生成树。

证明 如果 $G$ 没有圈，因为 $G$ 是连通的，从定理10.4.1可知， $G$ 本身是树。因此，这时 $G$ 本身就是生成树。现在假设 $G$ 至少有一个圈。如果我们删掉 $G$ 的圈上的一条边，所得图 $G_{1}$ 是连通的并且有与 $G$ 同样的结点集合。若 $G_{1}$ 没有圈，则 $G_{1}$ 便是 $G$ 的生成树。如果 $G_{1}$ 还有圈，再删掉 $G_{1}$ 的圈上的一条边，等等。用这种方法，最终得到没有圈的连通图 $H$ ，而且有与 $G$ 相同的结点集合，所以，根据定理10.4.1， $H$ 是 $G$ 的生成树。

如果在图10.30的图中，逐次删除边2，3，和5，得到图10.31中左边的生成树。

假设 $G$ 是有 $n$ 个结点和 $m$ 条边的连通图。那么， $G$ 的生成树恰有 $n - 1$ 条边。于是，在用逐次删除圈的边的方法确定 $G$ 的生成树时，我们必须删掉 $m - (n - 1) = m - n + 1$ 条边。数 $m - n + 1$ 称为 $G$ 的独立圈数。连通图 $G$ 的独立圈数总是非负整数。当且仅当 $G$ 是树时独立圈数为零。独立圈数给出一个图的连通程度的粗略量度。

从图10.32所示的图 $G$ 可以看出粗略到何等程度。这个图的独立圈数是 $13 - 8 + 1 = 6$ 。但是，该图的连通程度不好，只要删除一条边 $e$ ，得到的图就不再是连通的了。图的连通性的进一步讨论可见11.4节。

![](images/f6697fa86e950ca8a41479ee383c6ecb104b22097027fbc5a7b61f8a74e92ee8.jpg)  
图10.32

现在，我们来研究有同样结点集合的生成树的两个性质。定理10.4.6 设 $T$ 是图 $G$ 的生成树，又设 $e$ 是 $G$ 的边，它连

接不同的结点 $a$ 和 $b$ 但不属于 $T$ ，则存在唯一的基本圈 $\gamma$ ，其边是 $e$ 和 $T$ 的某些边。如果 $f$ 是 $\gamma$ 的与 $e$ 不同的任何边，当用 $e$ 代替 $T$ 的边 $f$ ，则得到的新树 $T'$ 是 $G$ 的生成树。

证明 在树 $T$ 中，存在连接 $a$ 和 $b$ 的唯一基本链。这个链连同边 $e$ 一起确定了唯一的基本圈 $\gamma$ （见图10.33）。设 $f$ 是圈 $\gamma$ 的任何边，并且假定我们从 $T$ 删除边 $f$ 而加进 $e$ ，得到图 $T'$ ，则 $T'$ 没有圈。设 $x$ 和 $y$ 是 $G$ 的任何两个结点，则 $T$ 中存在连接 $x$ 和 $y$ 的链 $\lambda$ 。如果 $f$ 不是 $\lambda$ 的边，则 $\lambda$ 是 $T'$ 中连接 $x$ 和 $y$ 的链。如果 $f$ 是 $\lambda$ 中的边，那么我们可以利用 $\gamma$ 的边修改 $\lambda$ 得到连接 $x$ 和 $y$ 的链 $\lambda'$ ，使得 $f$ 不是 $\lambda'$ 的边。此时， $\lambda'$ 是 $T'$ 中的连接 $x$ 和 $y$ 的链。所以， $T'$ 是连通的。由于 $T'$ 没有圈，根据定理10.4.1可知 $T'$ 是树。因为 $T'$ 有与 $G$ 同样的结点集合， $T'$ 是 $G$ 的生成树。

![](images/f1cb491ddcba0d8048406d652883a760e6c6ca36c6aa0855a6fc8304b0ef0063.jpg)  
图10.33

定理10.4.7 设 $T_{1}$ 和 $T_{2}$ 是图 $G$ 的生成树。设 $f$ 是 $T_{1}$ 的边，但不是 $T_{2}$ 的边，则存在一个 $T_{2}$ 的边 $e$ 。它不是 $T_{1}$ 的边，用 $e$ 代替 $T_{1}$ 的边 $f$ 时，则得到 $G$ 的生成树 $T_{3}$ 。

证明 我们想要找到 $T_{2}$ 的边 $\pmb{e}$ , 它不是 $T_{1}$ 的边, 使得 $\pmb{f}$ 是把 $\pmb{e}$ 加入 $T_{1}$ 后得到的唯一的基本圈 $\pmb{\rho}$ 的边。再根据定理10.4.6, 若用 $\pmb{e}$ 代替 $T_{1}$ 的边 $\pmb{f}$ , 则所得到的图是 $\pmb{G}$ 的生成树 $T_{3}$ . 这样一来, 就证明了这个定理。

设边 $f$ 连接 $a$ 和 $b$ ，我们从 $T_{1}$ 删掉 $f$ ，得到两个较小的树 $T_{1}^{\prime}$ 和 $T_{1}^{\prime \prime}$ ，使得 $a$ 是 $T_{1}^{\prime}$ 的结点而 $b$ 是 $T_{1}^{\prime \prime}$ 的结点（见图10.34）。因为 $T_{2}$ 是有与 $T_{1}$ 相同的结点集合的树，一定存在 $T_{2}$ 的边 $e$ ，它连接 $T_{1}^{\prime}$ 的结点 $c$ 和 $T_{1}^{\prime \prime}$ 的结点 $d$ 。由于 $a$ 和 $c$ 是 $T_{1}^{\prime}$ 的结点，存在连接 $c$ 与 $a$ 的唯一基本链 $\gamma_{1}^{\prime} = c, x, \cdots, a$ 。由于 $b$ 和 $d$ 是 $T_{1}^{\prime \prime}$ 的结点，存在连接 $b$ 与 $d$ 的唯一基本链 $\gamma_{1}^{\prime \prime} = b, y, \cdots, d$ 。因为 $e$ 连接 $c$ 与 $d$ 以及 $f$ 连接 $a$ 与 $b$ ，经合并 $\gamma_{1}^{\prime}$ 和 $\gamma_{1}^{\prime \prime}$ ，我们便得到所求的基本圈 $\gamma = c, x, \cdots, a, b, y, \cdots, d, c$ 。

![](images/a6ac456363efabee055856238550fee11788b2773b906ebf7536f3abec30b5ff.jpg)  
图10.34

例 考虑图10.35中图 $G$ 和边分别为 $\{1, 2, 3, 4, 5\}$ 和 $\{1, 7, 5, 8, 3\}$ 的两个生成树 $T_{1}$ 和 $T_{2}$ . 考虑 $T_{1}$ 的边4. 因为把边7加入到 $T_{1}$ 得到边为 $\{2, 3, 4, 7\}$ 的圈，故可用边7代替 $T_{1}$ 的边4而得到边为 $\{1, 2, 3, 5, 7\}$ 的生成树.

![](images/62444707f60011de253a79950aa227bf3da69a5bf758e1cd88b2484155d288ba.jpg)  
图10.35

# 10.5 两个实际问题

在本节中，我们考虑两个重要的实际问题。其中之一有时称为极小连接问题。有 $n$ 个城市 $A_{1}, A_{2}, \cdots, A_{n}$ ，并且希望建造这些城市间的公路系统，以便旅行者能从每个城市到达其余诸城市。已知建造城市 $A_{i}$ 和 $A_{j}$ 之间的公路成本为 $c(A_{i}, A_{j})$ ，怎样建造公路系统使得总成本尽可能小呢？显然，具有最小成本的公路系统对应于树。于是，我们寻求构造其结点是城市 $A_{1}, A_{2}, \cdots, A_{s}$ 的树，使得树的边的总成本尽可能小。

我们用下面方法可以推广这个问题。假设 $G$ 是有 $n$ 个结点的连通图，并且假设 $G$ 的每条边 $e$ 对应一个正数 $c(e)$ 。我们将把 $c(e)$ 想象为边 $e$ 的成本，但是 $c(e)$ 可能是 $e$ 的长度或 $e$ 的某种其它的度量。定义 $G$ 的生成树 $T$ 的成本 $c(T)$ 是 $T$ 的所有边的成本的总和。问题是要确定有最小成本的 $G$ 的生成树。当 $G$ 是有 $n$ 个结点，其中每对不同结点由一条边连接的图时，便出现前面讨论的该问题的特殊情形。

定理10.5.1 令 $G$ 是有 $n$ 个结点的连通图。下面算法给出 $G$ 的成本最小的生成树。

(i) 选取最小成本的边 $e_{i}$ .

假设 $i < n - 1$ 并且已选出边 $e_1, \dots, e_i$ .

(ii) 选取不同于边 $e_1, \cdots, e_i$ 的边 $e_{i+1}$ , 使得不存在这样的圈它的所有的边都包含在 $\{e_1, e_2, \cdots, e_i, e_{i+1}\}$ 中, 并且 $e_{i+1}$ 是满足这个条件的最小成本的边。重复这样作, 直至选出 $e_1, e_2, \cdots, e_{n-1}$ 为止。

证明 设 $T_{0}$ 是图，其结点为 $G$ 的 $n$ 个结点，且其边为 $e_{1}, e_{2}, \dots, e_{n-1}$ 。由 $T_{0}$ 的构造知，它没有圈。应用定理10.4.4，我们

可知 $T_{0}$ 是树，并且是 $G$ 的生成树。令 $T$ 是其成本为最小的 $G$ 的生成树。如果 $T$ 与 $T_{0}$ 相同，则 $T_{0}$ 是有最小成本的 $G$ 的生成树。假设 $T$ 与 $T_{0}$ 不同，则存在 $T_{0}$ 的边 $e_{i+1}$ ，使 $e_{i+1}$ 不是 $T$ 的边，而 $e_{1}, \cdots, e_{i}$ 是 $T$ 的边。如果我们把边 $e_{i+1}$ 加到 $T$ 中去，根据定理 10.4.6 必须产生唯一的基本圈 $\gamma$ 。由于 $T_{0}$ 是树并且 $e_{i+1}$ 是 $T_{0}$ 的一边，便存在 $\gamma$ 的某条边 $f$ ，它不是 $T_{0}$ 的边，用边 $e_{i+1}$ 代替 $T$ 的边 $f$ ，产生新树 $T'$ 成本为

$$
c \left(T ^ {\prime}\right) = c (T) - c (f) + c \left(e _ {i + 1}\right).
$$

因为在 $G$ 的所有生成树中， $T$ 的成本最小，从上面方程断定

$$
c \left(e _ {i + 1}\right) \geqslant c (f).
$$

由于 $e_1, \cdots, e_i, f$ 是 $T'$ 的边，可见没有任何圈其边包含于 $\{e_1, \cdots, e_i, f\}$ 中。因而，从上面不等式及 $e_{i+1}$ 的选择可得到 $c(e_{i+1}) = c(f)$ ，所以 $T'$ 也是其成本为最小的 $G$ 的生成树。但是， $T'$ 与 $T_0$ 有公共边的个数比 $T$ 与 $T_0$ 公共边数多1。我们可以用 $T'$ 代替 $T$ 重复上面的论证。最后，我们得到最小成本 $G$ 的生成树，它与 $T_0$ 有 $n-1$ 条公共边。这时我们断定， $T_0$ 是最小成本的 $G$ 的生成树。

例 考虑图10.36中的图 $G$ 假设 $c[x_1, x_2] = 1$ ， $c[x_2, x_3] = 2$ ， $c[x_3, x_4] = 2$ ， $c[x_4, x_5] = 3$ ， $c[x_1, x_5] = 2$ ， $c[x_2, x_5] = 1$ ， $c[x_1, x_4] = 3$ ， $c[x_2, x_4] = 2$ 。正如定理10.5.1所解的那样， $G$ 的有最小成本的生成树，通过选取边 $[x_1, x_2]$ ， $[x_2, x_3]$ ， $[x_3, x_4]$ ， $[x_2, x_4]$ 得到 $T_{0}$ 的后

![](images/cc4db5e82e5e72f70255b6a072e7a99b21180c1b9cbc22ee1d1e5480621111bf.jpg)  
图10.36

再令 $G$ 是连通图，对应于 $G$ 的每边 $\pmb{e}$ 存在正整数 $c(e)$ 。现在

我们把 $c(e)$ 看作边 $e$ 的长，如果 $a$ 与 $b$ 是结点，则连接 $a$ 与 $b$ 的链 $\gamma: x_1 = a, x_2, x_3, \dots, x_i = b$ 的长等于它的各边长之和：

$$
c (\gamma) = c \left[ x _ {1}, x _ {2} \right] + c \left[ x _ {2}, x _ {3} \right] + \dots + c \left[ x _ {k - 1}, x _ {k} \right].
$$

连接 $a$ 与 $b$ 的链称为最短链，只要连接 $a$ 与 $b$ 的链中，没有比 $\gamma$ 的长更小的链。显然，连接 $a$ 与 $b$ 的最短链是基本链。我们将叙述由G.Dantzig提出的一个算法，对给定 $G$ 的结点 $\pmb{x_0}$ ，它确定连接 $\pmb{x_0}$ 与 $G$ 中任意结点 $\pmb{x}$ 的最短链。这个算法的结论如下：

定理10.5.2 设 $G$ 是连通图， $x_0$ 是 $G$ 的一个结点，则存在 $G$ 的生成树 $T$ ，对于 $G$ 的与 $x_0$ 不同的每个结点 $x$ ，使得 $T$ 的连接 $x_0$ 与 $x$ 的唯一基本链是 $G$ 的连接 $x_0$ 与 $x$ 的最短链。

证明 我们来描述确定该定理中所说的树 $T$ 的算法。从结点 $x_0$ 开始，一步一步地构造一个越来越大的树，直至包含 $G$ 的全部结点，成为 $G$ 的生成树为止。令 $X_0 = \{x_0\}$ 和定义 $i(x_0) = 0$ 。

(i) 选择连接 $x_0$ 的结点 $x_1$ , 使 $c[x_0, x_1]$ 尽可能小. 令 $l(x_1) = c[x_0, x_1]$ . 那么, 显然 $x_0, x_1$ 是连接 $x_0$ 和 $x_1$ 其长为 $l(x_1)$ 的最短链. 令 $X_1 = \{x_0, x_1\}$ 和 $E_1 = \{[x_0, x_1]\}$ .

(ii）一般步骤，假设对于 $k\geqslant 1$ 已确定了结点集合 $X_{k} = \{x_{0},x_{1},\dots ,x_{k}\}$ 和边集合 $E_{k}$ ，对于 $X_{k}$ 中结点 $\pmb{x}_i$ ，选取不在 $X_{k}$ 中的结点 $y_{i}$ ，使 $[x_i,y_i]$ 是 $G$ 的边且 $c[x_i,y_i]$ 尽可能小.（对于 $X_{k}$ 中的某个 $x_{i}$ ，在 $X_{k}$ 之外可能没有与 $\pmb{x}_{i}$ 连接的结点 $\pmb{y}_{i}$ 这时我们便干脆不考虑 $x_{i}$ ，但是，由于 $G$ 是连通的，必然至少有不在 $X_{k}$ 中的一个结点，它与 $X_{k}$ 中的结点相连接，除非 $X_{k}$ 已经是由 $G$ 的全部结点组成。）选取 $X_{k}$ 中的结点 $\pmb{x}_{P}$ ，使得对于 $i = 0,1,2,\dots ,k$ ，有

$$
l (x _ {p}) + c [ x _ {p}, y _ {p} ] \leqslant l (x _ {i}) + c [ x _ {i}, y _ {i} ].
$$

令 $x_{k + 1} = y_{i}$ 及 $l(x_{k + 1}) = l(x_{p}) + c[x_{p},y_{p}]$ 令 $X_{k + 1} = \{x_0,$

$x_{1},\dots ,x_{k},x_{k + 1}\}$ 及 $E_{k + 1} = E_k\cup \{[x_p,x_{p + 1}]\}$

令 $T$ 是结点集合为 $G$ 的结点集合 $X_{n-1} = \{x_0, x_1, \dots, x_{n-1}\}$ ，边为集合 $E_{n-1}$ 的图。由于 $T$ 的构造可知，它是连通的和没有圈。因而 $T$ 是 $G$ 的生成树。而且对任意 $x_i$ ，其中 $1 \leqslant k \leqslant n-1$ ，从构造可知，连接 $x_0$ 和 $x_k$ 的 $T$ 的基本链 $\gamma_h$ 的长是 $l(x_i)$ 。尚待证明 $\gamma_h$ 是 $G$ 的连接 $x_0$ 和 $x_k$ 的最短基本链。我们通过对 $k$ 归纳来证明这一点。对于 $k=1$ 由 (i) 这是显然的。假定对于 $j \leqslant k$ ， $\gamma_j$ 是 $G$ 的连接 $x_0$ 与 $x_j$ 的最短基本链。考虑 $k+1$ 。

令 $\gamma^{\prime}$ 是 $G$ 的联 $x_0$ 与 $x_{k + 1}$ 的基本链，令 $x_{i}$ 是 $X_{k}$ 中 $\gamma^\prime$ 的最末的结点（一定存在这种结点，因为 $x_0$ 是 $X_{k}$ 中 $\gamma^\prime$ 的结点），又令 $y$ 是 $\gamma^{\prime}$ 的 $x_{i}$ 的后一个点。利用归纳假设， $\gamma_{i}$ 是 $G$ 中连接 $x_0$ 和 $x_{i}$ 的最短的基本链，并且 $\gamma_{i}$ 的长为 $l(x_{i})$ 。因此， $\gamma^{\prime}$ 至少有长度

$$
\begin{array}{l} l \left(x _ {i}\right) + c \left[ x _ {i}, y \right] \geqslant l \left(x _ {i}\right) + c \left[ x _ {i}, y _ {i} \right] \\ \geqslant l (x _ {p}) + c [ x _ {p}, y _ {p} ] \\ = l \left(x _ {5 + 1}\right), \\ \end{array}
$$

$\gamma_{k+1}$ 的长度。这样， $\gamma_{k+1}$ 是 $G$ 的连接 $x_0$ 和 $x_{k+1}$ 的最短基本链且长为 $l(x_{2+1})$ 。由归纳定理得到证明。

在人们希望找到连接 $x_0$ 与特定结点 $z$ 的最短基本链的情况下，上述算法一旦得到结点 $z$ 便结束。

例 考虑在图10.37中的图，其边上的数字表示 $G$ 的边长。

如果我们以 $x_0 = \alpha$ 开始，遵循定理的证明中给出的构造，依次得到下列结点及边：

$$
\begin{array}{l} a, b, d, c, e, f, \\ [ a, b ], [ b, d ], [ a, c ], [ d, e ], [ c, f ]. \\ \end{array}
$$

所得到的树在图10.38中画出。

![](images/cf134822d2a6578993a4ae3d80c72b0414f85fb65fb712a877a44ad77f95cf75.jpg)  
图10.37

![](images/21bd5a047555372a9e2cf21c736ebd1a9b04357a7db5e31bf39261600070acc2.jpg)  
图10.38

# 10.6 Shannon开关对策①

本节中我们叙述能在任何图上进行的对策。它是由C. Shannon发明的，又由A. Lehman 找到其巧妙的解答1)。该解依赖于在10.4节介绍过的树的某些性质。我们的叙述和对策的解是根据著者的论文“网络和Shannon开关对策”的第三节，（Delta 4 (1974), 1-23）。

Shannon 对策进行如下。令 $G$ 是任意图，标出其中两个结点 $u$ 和 $v$ 。有两个对手，我们将他们称为正对手 $(P)$ 和负对手 $(N)$ ，并且他们轮流进行对策。正对手的目标是构造连接两结点 $u$ 与 $v$ 的链，负对手的目标是阻止连接 $u$ 和 $v$ 的所有链，这样阻止正对手是他的目标。对策玩法进行如下：当轮到 $P$ 时， $P$ 标出图的某条边，比如说标以“+”符号， $N$ 就不能再使用它。当轮到 $N$ 时， $N$ 标出图的某条边，比如说标以“-”符号，则 $P$ 就不能使用它。（如果图是画在纸上，可以让 $N$ 擦掉没有标出“+”符号的边。）比赛继续直到一个对手达到他的目标为止：或者在标以“+”的一些边之中有一条连接 $u$ 和 $v$ 的链（这时正对手 $P$ 赢）或者标以“-”的边阻碍连接 $u$ 与 $v$ 的全部链，即是说，连接 $u$ 和 $v$ 的每条链其边中至少有一条边标以“-”号（这时负对手赢），只可能有一个胜者，因为 $P$ 和 $N$ 的目标彼此是矛盾的，并且不可能有平局。有可能对策进行到每条边都标以“+”或“-”，然后决定哪个对手胜。

下面提出一些问题。是否存在 $P$ 所遵循的策略，不管 $N$ 玩得怎样好，都将保证 $P$ 获胜呢？如果这样，对于 $P$ 来说，这种获胜的策略是什么？是否存在 $N$ 所遵循的策略，不管 $P$ 玩得如何好，都将保证 $N$ 获胜呢？如果这样，使 $N$ 获胜的这种策略又是什么？对于某种特定的对策， $P$ 和 $N$ 不可能都有取胜的策略，但是当我们改变结点 $u$ 和 $v$ 或改变图时，我们可以希望有时 $P$ 有取胜的策略，或者有时 $N$ 有取胜的策略。此外，无论是 $P$ 或 $N$ ，取胜的策略依赖于谁先行动。

考虑图10.39中由第一个图给出的对策。在该对策中，正对手可以取胜，不管他是先或后玩，因为两边中任一条边标以“+”符号都确定连接 $u$ 和 $v$ 的链。在继续之前，我们注意到，如果当 $P$ 后行动时，有正对手 $P$ 的取胜策略，则当 $P$ 先行动时也

有 $P$ 的取胜策略，这是显然的，因为对 $P$ 来说，有额外一条边标以“+”号，决不是不利条件，最坏不起任何作用，何况也许有些好处。当正对手后行动有取胜的策略（当先行动时更是如此）时，把这种对策称为正对策。例如，由图10.39中第一图所给出的对策便是正对策。在图10.39中，第二图所确定的对策，无论负对手是先行动或后行动都有取胜的策略，因为在任何边上标以“-”符号都能阻止正对手构成链。当负对手后行动时有取胜的策略时（当先行动时更是如此），我们把这种对策称为负对策。例如，图10.39中的第二图所确定的对策就是负对策。既不是正的也不是负的对策称为中对策。在中对策中，先行动的对手，无论是正对手还是负对手，有取胜的策略。图10.39中第三个图所确定的对策便是中对策的例子。

![](images/b6580dc0c8208e01d5946fd1819028f9ce3052668fb728f0dceddda0f6076b87.jpg)  
图10.39

例 考察图10.40中的图所决定的对策。如果正对手先行动，并且在边5上标以“+”号，然后以在边2上置“+”号与边1上置“-”号相对抗或者反之，并且以在边4上置“+”号与边3上置“-”号相对抗或者反之，则正对手将取胜。同样，如果负对手先行动，并且在边5上置“-”号，然后在边3上置“-”号与边1上置“+”号相对抗或者反之，再在边4上置“-”号与在边2上置“+”号，则负对手将取胜。因而，这是一个中对策。

例 考察在10.41中的图所确定的对策。这个图由图10.40加入连接 $u^{\text{与}}v$ 的一条新边而组成。假设在这个对策中，负对手先行动。如果他不在边6上置“-”号，那么对手可以置“-”号，因而正对手获胜，若负对手在边6上置“-”号，那么剩下的对策和前面的例子正对手先行动的情况一样。因而，正对手有获胜的策略，故是正对策。

![](images/af3387a7ecc0a563d3acd7dd0e9e5e690c82e4c862257b435bf85c662db9940d.jpg)  
图10.40

![](images/71766c85cc3c861c373b98331a3038b4a01833ace3104f9344e37e4edc615bbb.jpg)  
图10.41

前面两个例子说明了一般原则：

一个中对策，当插入连接不同的结点u和v的新边时，它就变成正对策。

在叙述由Lehman证明的关于正对策的基本定理之前，我们引入一个更深入的概念。设 $G = (X, E)$ 是图，并且令 $A$ 是 $X$ 的子集。那么，由 $A$ 诱导的子图是图 $G_A = (A, E_A)$ ，其结点集合为 $A$ ，其边集合是 $G$ 中的所有连接 $A$ 中的结点的边。图10.42说明一个图 $G$ 和由集合 $A = \{a, c, d\}$ 诱导的子图。

![](images/15ab11c25c021a0254a0c0834967996751af4acc3486210c93b6fcf6d38dc8ef.jpg)

![](images/bfcc0896491a788ef66b79469187cdee5a71e8de39d1b8037f12bf5c9266684f.jpg)  
图10.42

定理10.6.1 令 $G$ 是具有标出结点 $u$ 和 $v$ 的图。那么，当且仅当存在包含 $u$ 与 $v$ 的结点集合 $A$ ，使图 $G_A$ 有两个没有公共边的生成树时， $G$ 上关于 $u$ 与 $v$ 所进行的对策是正对策。

解释一下这个定理，当且仅当存在 $G$ 中的两个树，包含 $\pmb{u}$ 和 $\pmb{v}$ 在内的相同结点集合，并且它们没有公共边时， $G$ 上关于 $\pmb{u}$ 与 $\pmb{v}$ 所进行的对策是正对策。已经证明了图10.41中的图所确定的对策是正对策。在图10.43中画出了具有所需性质的两个树。这时，该树为生成树（结点集合A是整个结点集合）。

![](images/fe0ff9f0b00e8bdb8b278a45f06bf9e60e87a16b86985699a474042826c88e60.jpg)

![](images/3c44c431f1ef3973f25236fa51f8f2ddd2a8899833ab054b95227e1323e44077.jpg)  
图10.43

我们将不给出该定理的完整证明。满足于在存在包含 $u$ 和 $v$ 的结点集合 $A$ ，使得图 $G_{A}$ 是没有公共边的两个生成树的情况下，指出正对手后行动取胜的策略。令 $T_{1}^{(0)}$ 和 $T_{2}^{(0)}$ 是这样的两个树。

比赛的第一阶段。对手 $N$ 先行动，并且在 $G$ 的一条边 $\pmb{x}$ 上标以“-”号，存在两种可能性。

(i) 假设 $x$ 是树 $T_{2}^{(0)}$ 、 $T_{2}^{(0)}$ 之一，比方说 $T_{1}^{(0)}$ 的一条边。由于 $T_{1}^{(0)}$ 和 $T_{2}^{(0)}$ 是 $G_{A}$ 的生成树，从定理10.4.7可知，存在 $T_{2}^{(0)}$ 的一条边 $y$ ，使由 $y$ 代替 $T_{1}^{(0)}$ 的边 $x$ 得到的树，也是 $G_{A}$ 的生成树。用 $T_{1}^{(1)}$ 表示这个新树。我们指示对手 $P$ 在边 $y$ 上标以“+”号。令 $T_{2}^{(1)} = T_{2}^{(0)}$ ，树 $T_{1}^{(1)}$ 和 $T_{2}^{(1)}$ 是恰有一公共

边即标以“+”号的边 $y$ 的 $G_{A}$ 的两生成树。

（ii）假定 $x$ 既不是 $T_{1}^{(0)}$ 的边，也不是 $T_{2}^{(0)}$ 的边。我们指示对手 $P$ 在 $T_{1}^{(0)}$ 或 $T_{2}^{(0)}$ 的任何边 $y$ 上标以“+”号：比如说 $y$ 是 $T_{1}^{(0)}$ 的边。因为 $T_{1}^{(0)}$ 和 $T_{2}^{(0)}$ 是 $G_{A}$ 的生成树，边 $y$ 联接 $T_{2}^{(0)}$ 的两个结点。由定理10.4.6可知，存在 $T_{2}^{(0)}$ 的某条边 $z$ ，使得由 $y$ 代替 $T_{2}^{(0)}$ 的边 $z$ 而得到的树，也是 $G_{A}$ 的生成树。用 $T_{2}^{(1)}$ 表示这个新树，并令 $T_{1}^{(1)} = T_{1}^{(0)}$ 。树 $T_{1}^{(1)}$ 和 $T_{2}^{(1)}$ 是 $G_{A}$ 的恰有一条公共边，即标以“+”号边 $y$ 的生成树。

我们断定作为第一阶段的结果，得到两个树 $T_{1}^{(1)}$ 和 $T_{2}^{(1)}$ 它们都是 $G_{A}$ 的生成树，并且恰有一个标以“+”号的公共边。在比赛第二阶段中，由 $T_{1}^{(1)}$ 和 $T_{2}^{(1)}$ 来充当 $T_{1}^{(0)}$ 和 $T_{2}^{(0)}$ 的角色。

比赛的第二阶段。这里的描述非常类似于第一阶段，因而我们将简要些。假设对手 $N$ 在 $G$ 的一边 $x$ 上标以“-”号。边 $x$ 不可能是树 $T_{1}^{(1)}$ 和 $T_{2}^{(1)}$ 的公共边，因为该公共边已标有“+”号。

(i) 假定 $x$ 是树 $T_{1}^{(1)}$ , $T_{2}^{(1)}$ 之一的边, 比如说 $T_{1}^{(1)}$ 的边. 则存在 $T_{2}^{(1)}$ 的一边 $y$ , 使得用 $y$ 代替 $T_{1}^{(1)}$ 的边 $x$ 而得到树 $T_{1}^{(2)}$ , 也是 $G_{A}$ 的生成树. 我们指示对手 $P$ , 在边 $y$ 上放置“+”号, 令 $T_{2}^{(2)} = T_{2}^{(1)}$ .  
（ii）假设 $x$ 既不是 $T_{1}^{(1)}$ 也不是 $T_{2}^{(1)}$ 的边。我们指示对手 $P$ 在 $T_{1}^{(1)}$ 或 $T_{2}^{(1)}$ 的任何可用的边 $y$ 上放置“+”号；比如说 $y$ 是 $T_{1}^{(1)}$ 的边，存在 $T_{2}^{(1)}$ 的边 $z$ ，使得由 $y$ 代替 $T_{2}^{(1)}$ 的边 $z$ 而得到树 $T_{2}^{(2)}$ ，它是 $G_{A}$ 的生成树。令 $T_{1}^{(2)} = T_{1}^{(1)}$ 。

我们断定，作为第二阶段比赛的结果，得到两个树 $T_{1}(2)$ 和 $T_{2}(2)$ ，它们是 $G_{A}$ 的生成树，并且恰有标以“+”号的两个公共边。

策略的余下部分，非常类似于上面的两个阶段。在第 $k$ 阶段比赛结束时，存在 $G_{A}$ 的两个树 $T_{1}^{(k)}$ 和 $T_{2}^{(k)}$ ，它们恰有 $k$ 条标以“+”号的公共边。假设 $A$ 恰包含 $n$ 个结点，则在第 $n-1$ 阶段比赛结束时，存在 $G_{A}$ 的生成树 $T_{1}^{(n-1)}$ 和 $T_{2}^{(n-1)}$ ，它们恰有 $n-1$ 条标以“+”号的公共边。因为 $T_{1}^{(n-1)}$ 和 $T_{2}^{(n-1)}$ 都是 $G_{A}$ 的生成树，由定理10.4.3，我们断定 $T_{1}^{(n-1)}$ 和 $T_{2}^{(n-1)}$ 各自恰有 $n-1$ 条边。于是，树 $T_{1}^{(n-1)}$ 和 $T_{2}^{(n-1)}$ 是相同的，并且标以“+”号的那些边都是 $G_{A}$ 的生成树的边。由于 $A$ 包含点 $u$ 和 $v$ ，故存在连接 $u$ 与 $v$ 的链，其全部边都标有“+”号。所以，我们断定若正对手遵循我们的指示，在第 $n-1$ 阶段比赛结束时，甚至还在这以前，正对手就能在包含有连接 $u$ 和 $v$ 的链上的所有边标以“+”号。于是，正对手定获胜，我们对于正对手的指示是使他取胜的策略。

可以使用定理10.6.1对中对策和负对策分类如下。令 $G$ 是标出结点 $u$ 和 $v$ 的图，又令 $G^{*}$ 是由 $G$ 加入连结 $u$ 和 $v$ 的新边 $e^{*}$ 而得到的图。那么当且仅当(i)它不是正对策；可是(ii)在 $G^{*}$ 上关于 $u$ 和 $v$ 进行的对策是正对策时，在 $G$ 上关于 $u$ 和 $v$ 进行的对策是中对策。当且仅当它不是正对策，并且在 $G^{*}$ 上关于 $u$ 和 $v$ 也不是正对策时，在 $G$ 上关于 $u$ 和 $v$ 进行的对策是负对策。于是，当且仅当(i)不存在包含 $u$ 和 $v$ 的结点集合 $A$ ，使图 $G_{A}$ 有两个没有公共边的生成树，但是，(ii)存在包含 $u$ 和 $v$ 的结点集合 $A$ ，使图 $G_{A}^{*}$ 有两个没有公共边的生成树时，在 $G$ 上关于 $u$ 和 $v$ 进行的对策是中对策。当且仅当在 $G$ 上或 $G^{*}$ 上找不到这样的子集 $A$ 时，在 $G$ 上关于 $u$ 和 $v$ 进行的对策是负对策。在中对策中，正对手可以靠在 $G$ 上关于 $u$ 和 $v$ 进行的对策中先走一步获胜，可假想对策是在 $G^{*}$ 上关于 $u$ ， $v$ 进行的，并且负对手已走了第一步，在 $e^{*}$ 上标以“-”号。不幸的是，一般地说，不容易找到一

个策略，使得负对手在负对策时后走一步，或在中对策时先走一步取胜。

为结束本节，我们提一下由D.Gale设计在1960年发表的搭桥对策，它等价于图10.44中的图 $G$ 上进行的关于结点 $\pmb{u}$ 和 $\pmb{v}$ 的Shannon开关对策。暂且，除连接 $\pmb{u}$ 和 $\pmb{v}$ 的实曲线 $e^{*}$ 外，不管是否虚线，全表示 $G$ 的边。

![](images/847d8516d79c2abb782f6f6b63a4e5e25cb596f8f18915008d32d657ff14bbbf.jpg)  
图10.44

图 $G$ 有22个结点和41条边，不存在包含 $\pmb{u}$ 和 $\pmb{\sigma}$ 的 $G$ 的结点集合 $A$ ，使图 $G_{A}$ 具有没有公共边的两个生成树，但是，如果我们向 $G$ 加入边 $e^{*}$ ，我们便得到有两个生成树的图 $G^{*}$ ，其中实线表示树 $T_{1}$ 的边，虚线表示另一个树 $T_{2}$ 的边。这意味着，搭桥对策是中对策，在该对策中，正对手取胜策略变得简单。图10.44中这些箭头记号把 $T_{1}$ 的边和 $T_{2}$ 的边配成对。正对手先行动时搭桥对策可当着在 $G^{*}$ 上关于 $\pmb{u}$ 和 $\pmb{\sigma}$ 的对策处理，此时，负对于先行动并且在 $e^{*}$ 上放置“-”号。在这种对策中，使正对于取

胜的策略是在负对手标“-”号的边配对的边上标“+”号。于是，正对手首次应在连接 $v$ 与 $u$ 的边上标以“+”号。这种配对策略是由O.Gross设计的。

# 10.7 有向图

有向图是一种图，其中每条边都指定了方向。更准确地说，有向图 $D$ 是由称为结点的有限事物集合 $N$ 连同有序结点对的重集 $A$ 所组成。有序结点对叫做弧，并记为 $(x, y)$ 。结点 $x$ 称为弧 $(x, y)$ 的始点，结点 $y$ 称为弧 $(x, y)$ 的终点。我们用 $D = (N, A)$ 表示这种有向图。图的许多概念稍加改变或不变就可以搬到有向图上来。这是因为对应于每一个有向图 $D = (N, A)$ 存在着由删掉每条弧上的方向而得到一个基础图 $G = (N, E)$ 。更准确说， $D$ 的每条弧 $(x, y)$ 对应于 $G$ 的每条边 $[x, y]$ 。

例设 $D = (N, A)$ 是个有向图。其结点集合为 $N = \{x, y, z, u, v\}$ ，其弧为 $(x, y), (y, z), (z, y), (z, u), (u, x), (v, u), (x, v)$ 。这个有向图可以画在平面上，如图10.45所示，其中我们用箭头表示弧的顶点顺序。基础图如图10.46所示。

![](images/5669bb6b6aec7a861e270273d1ed19ce46f4def86c901279b0e90ef5b89b80c0.jpg)  
图10.45

![](images/7c87b2517166d624931610ea6b622d24cfdc80d3862499fb3be3a779b33d539d.jpg)  
图10.46

对于有向图 $D$ 的每个结点，定义两种度。结点 $x$ 的入度是以 $x$ 为终点的弧的个数。 $x$ 的出度是以 $x$ 为始点的弧的个数。如果 $G$ 是 $D$ 的基础图，则 $G$ 的结点 $x$ 的度是 $D$ 中 $x$ 的入度与出度之和。对于上面例子的有向图 $D$ ， $u$ 的入度为 3， $u$ 的出度为 1；在基础图 $G$ 中的 $u$ 的度为 $3 + 1 = 4$ 。由于每条弧有一个始点和终点，在有向图中，结点的入度之和等于出度之和。

设 $D = (N, A)$ 是有向图， $D$ 中的路 $\gamma$ 是 $k \geqslant 2$ 个结点 $x_1, x_2, \cdots, x_k$ 序列，使得 $(x_1, x_2), (x_2, x_3), \cdots, (x_{k-1}, x_k)$ 是 $D$ 的弧，且其中没有一条弧重复次数比 $A$ 中它的重复数多。路 $\gamma$ 是由 $x_1$ 到 $x_k$ 的路。基本路是其结点都不相同的路 $x_1, x_2, \cdots, x_k$ 。回路的定义除了结点 $x_1$ 与 $x_k$ 相同外，与路的定义一样。基本回路是除了第一个和最末一个结点相同外，其余所有结点都不同。

例 对于图10.45中所示的有向图， $x, y, z, y$ 是从 $x$ 到 $y$ 的路，而 $x, y, z, u$ 是从 $x$ 到 $u$ 的基本路。而且， $x, v, u, x$ 是基本回路， $y, z, y$ 也是基本回路。

有向图是连通的，只要它的基础图是连通的。有向图是强连通的，只要对于每对不同结点 $x, y$ ，既存在从 $x$ 到 $y$ 的路，也存在从 $y$ 到 $x$ 的路，图10.45中所示的有向图是强连通的。图10.47中所示的有向图是连通的，但不是强连通的，因为不存在从 $x$ 到 $y$ 的路。

![](images/d2ea42252d1bce95eaaf9ab3bd150e56f697d3f2b0dc0721e233b1c1bc035a35.jpg)  
图10.47

有向图 $D$ 中的 Euler回路是这样的回路，其弧是 $D$ 的所有弧。有向图 $D$ 的从结点 $x$ 到结点 $y$ 的 Euler路是从 $x$ 到 $y$ 的，其弧是 $D$ 的所有弧的路。对应于定理10.2.1和10.2.2我们有下面定理。

定理10.7.1 连通有向图 $D$ 当且仅当对每个结点 $x$ ，其入度等于出度时才有Euler回路。

定理10.7.2 设 $D$ 是连通有向图，又设 $x$ 和 $y$ 是 $D$ 的不同结点。那么，当且仅当 $x$ 的出度比 $y$ 的入度大 1， $y$ 的入度比 $y$ 的出度大 1，并且对于所有与 $x$ 和 $y$ 不同的结点，其入度等于出度时，存在从 $x$ 到 $y$ 的 Euler 路。

由于这些定理的证明类似于定理10.2.1和10.2.2的证明，所以留给读者作练习。

设 $D$ 是有向图， $D$ 中Hamilton回路是一基本回路 $\gamma$ ，使得 $D$ 的每个结点都为 $\gamma$ 的一个结点。如果 $x$ 和 $y$ 是不同的两个结点，从 $x$ 到 $y$ 的Hamilton路是从 $x$ 到 $y$ 的基本路 $\gamma$ ，使得 $D$ 的每个结点都是 $\gamma$ 的结点。如果 $D$ 有Hamilton回路，则 $D$ 是强连通的。若 $D$ 存在从结点 $x$ 到结点 $y$ 的Hamilton路，则 $D$ 是连通的。有向图称为简单的，只要每对不同结点 $x$ 和 $y$ 至多存在一条以 $x$ 为始点，以 $y$ 为终点的弧，并且不存在始点和终点相同的弧。对应于定理10.3.1，有下面的Ghoula-Houri定理。

定理10.7.3 设 $D$ 是有 $n$ 个结点的强连通简单有向图。如果对每个结点 $x, y$ 的入度与出度之和至少为 $n$ ，则 $D$ 有Hamilton回路。

尽管这个定理类似于定理10.3.1，但它的证明困难得多。因而，证明从略。有向图的其它一些与图中已证明过的相应的定理，我们在练习中叙述并且留给读者自己去证明。

我们考虑一个实际上相当重要的问题来结束本节。设 $G$ 是

表示城市街道系统的图，结点表示交叉口。如果图 $G$ 是连通的，则可以从城市中的任意点到其它任何点。我们考虑的问题如下：在什么条件下可以把街道变成单行道后，使从城市中的任何点仍有可能沿规定单行方向达到任意其他点。假若城市被河流分成两部分，并且在河上仅有一座桥，则它显然是不可能的。问题抽象为确定何时才可能把图所有边赋予方向后，使得到的有向图是强连通的。

设 $G$ 是连通图， $G$ 的峡边是这样的一条边，从 $G$ 中删除它，剩下的图就不再是连通的了。对于图10.48中所画出的图，边 $[x, y]$ 是峡边，如果连通图 $G$ 有峡边，则显然不可

![](images/979fc58436ade39456abfa5063d69b869fa8f3e9c2b931de9997769f19b82f7d.jpg)  
图10.48

能给它的边标出方向得到有向图是强连通的。反之亦真，在证明它以前，我们首先给出图的边是峡边的特征。

定理10.7.4 令 $G$ 是连通图，当且仅当 $G$ 的某边不是任何基本圈的边时， $G$ 的这条边是峡边。

证明 令 $e = [x, y]$ 是 $G$ 的一边，如果 $e$ 是某基本圈的边，则删去 $e$ 剩下一连通图。因而 $e$ 不是峡边。反之，假定 $e$ 不是峡边，则移去 $e$ 剩下连通图 $G'$ 。因此在 $G'$ 中，存在连接 $x$ 和 $y$ 的基本链 $x, z, \cdots, y$ 。但是， $x, z, \cdots, y, x$ 是 $G$ 中的基本圈，并且它的一边是 $e$ 。

下面定理是H.E.Robbins的结果.

定理10.7.5 令 $G = (X, E)$ 是没有峡边的连通图，则可以给 $G$ 的边赋予方向，使得到有向图 $D$ 是强连通的。

证明 因为 $G$ 没有峡边，由定理10.7.4可知， $G$ 的每条边都是基本圈的边。我们给出一个对 $G$ 的边赋予方向的算法，使

得所产生的有向图是强连通的。令 $\gamma_{1}$ 是 $G$ 的基本圈，其结点集合为 $A_{1}$ 。给 $\gamma_{1}$ 的边标明方向，使它成为回路。再把两结点都在 $A_{1}$ 中的其它所有边任意标出方向。如果 $X = A_{1}$ ，则为作完。如果 $X \neq A_{1}$ ，则因为 $G$ 是连通的，必存在边 $e_{1}$ ，其结点之一在 $A_{1}$ 中。边 $e_{1}$ 是基本圈的边，且这个基本圈包含连接 $A_{1}$ 中两结点的链 $\gamma_{2}$ ，使得没有 $\gamma_{2}$ 的任何边已被标出方向。给 $\gamma_{2}$ 的边标明方向，使它成为路。令 $A_{2}$ 是由 $A_{1}$ 和和 $\gamma_{2}$ 的所有其它结点组成的结点集合。把两个结点全在 $A_{2}$ 中的 $G$ 的其它所有边任意标出方向。如果 $X = A_{2}$ ，则 $G$ 的边已赋予方向，结果是强连通的有向图。若 $X \neq A_{2}$ ，我们可仿此继续作，直至我们给 $G$ 的所有边标明方向形成强连通的有向图为止。

# 练习

1. 证明如果图的两个结点是由一链连接，则它们由基本链连接。  
2. 如果 $x$ 和 $y$ 是圈 $y$ 的结点， $x$ 和 $y$ 一定为基本圈的结点吗？  
3. 试证图中圈的边集合，可以划分成集合 $A_{1}, A_{2}, \ldots, A_{k}$ ，使每个 $A_{i}$ 是基本圈的边集合。  
4. 试证若图有奇数长的圈，它必有奇数长的基本圈  
5. 试证圈 $\pmb{\gamma}$ 的每个结点与 $\pmb{\gamma}$ 的偶数个边相交，而基本圈的每个结点与该圈的两条边相交。  
8. 设 $\gamma$ 是连接不同结点 $x$ 与 $y$ 的链，证明 $x$ 和 $y$ 与 $\gamma$ 的奇数条边相交，而 $\gamma$ 的其余每个结点与 $\gamma$ 的偶数条边相交。  
7. 设 $G = (N, E)$ 是图。对于结点 $x$ 和 $y$ ，定义 $x \sim y$ 意指或 $x = y$ 或当 $x \neq y$ 时， $x$ 和 $y$ 由链连接。证明 $\sim$ 有下列性质：（i） $x \sim x$ 。（ii） $x \sim y$ 蕴含 $y \sim x$ 。（iii） $x \sim y$ 和 $y \sim z$ 蕴含 $x \sim z$ 。  
8.（续练习T）对于结点 $x$ ，设 $C(x)$ 是所有的 $x \sim 2$ 的结点 $z$ 的集合，证明对于结点 $x$ 和 $y$ 或者 $C(x) = C(y)$ ，或者 $C(x) \cap C(y) = \varnothing$ 。还要证

明，如果 $C(x) \cap C(y) = \emptyset$ ，则没有 $C(x)$ 的结点与 $C(y)$ 的结点由一边连接。

9. (续练习8) 对于结点 $x$ ，设 $G_{\chi}$ 是图，其结点集合为 $C(x)$ ，其边为 $G$ 中的连接 $C(x)$ 的两结点的所有边集合。证明 $G_{\chi}$ 是连通图。图 $G_{\chi}$ 称为 $G$ 的连通分图，由练习8推出，没有链连接不同连通分图中的结点。  
10. 设 $G$ 是其结点的度至少为 2 的图, 证明 $G$ 至少有一个基本圈.  
11. 证明有 $n$ 个结点且边数多于 $\frac{1}{2} (n - 1)(n - 2)$ 的简单图是连通的，试举出有 $n$ 个结点和边数为 $-\frac{1}{2} (n - 1)(n - 2)$ 的图不是连通的。  
12. 下面哪个图有 Euler 图？如可能试构造出来。

![](images/2eda505729f3c6d86d7fed31e3989fd17223fa0d286e4a0992ba6597722e1316.jpg)  
图10.49

13. 完全图 $K_{n}$ 是有 $n$ 个结点且每对不同结点由边连接的简单图，试确定对哪些 $n$ 的值，完全图 $K_{n}$ 有Euler圈。  
14. 令 $G$ 是恰有两个奇数度的结点 $x$ 与 $y$ 的图。令 $G^{*}$ 由 $G$ 插入连接 $x$ 与 $y$ 的一新边而得到的图，证明当且仅当 $G^{*}$ 是连通时， $G$ 是连通的。  
15. 令 $G$ 是连通图，在 $G$ 的结点中恰有 $2k (k \geqslant 1)$ 个奇数度的结点，证明 $G$ 的边可以划分成 $E_1, E_2, \dots, E_k$ ，使得存在链 $\gamma_i$ ，其边为 $E_i$ 中的边， $i = 1, 2, \dots, k$ .  
16. 把练习15应用于图10.49中的最左图及图10.17中的图.  
17. 试笔不离纸和任何曲线只画一次地画出图10.15和图10.16中

的图.

18. 在任何线段不多画一次地描出图10.19的图中，有多少次必须使笔离开纸，并且重新开始？

19. 试找出图10.22中图的Hamilton圈，  
20. 试证下面的图没有Hamilton圈，找出Hamilton链。

![](images/fbd6443e8837d4aad4375fb22d5f21c57cac552f975787a0b20070d97cf62853.jpg)

21. 完全图 $K_{n}$ 有多少Hamilton圈？（参看练习13中的完全图的定义）  
22. 令 $G$ 是图，其结点为 $8 \times 8$ 棋盘的84个方块，其中两个方块由边连接，当且仅当它们有公共边。确定 $G$ 是否有连接对角线上两顶角上的方块的Hamilton路。  
23. 试用图论的语言叙述下面求解问题：一个马在 $8 \times 8$ 棋盘上应怎样才能走遍每个方块恰好一次？（马的走步由垂直方向走一个方块（或水平方向）跟着水平方向走两个方块（或垂直方向）组成。）

24*（Pósa，1962）设 $G$ 是有 $n \geqslant 3$ 个结点的简单图，使得：

（i）对于 $1 \leqslant k \leqslant (n - 1) / 2$ 的每个 $k$ ，至多度为 $k$ 的结点个数小于 $k$ 。  
(ii) 如果 $n$ 是奇数, 至多度为 $(n - 1)/2$ 的结点个数至多为 $(n - 1)/2$ . 证明 $G$ 有 Hamilton 圈.

25* 由练习24的结果推导定理10.3.1.

26* (Ore, 1961) 令 $G$ 是有 $n$ 个结点和 $m$ 条边的简单图，其中 $m \geqslant \frac{1}{2} (n - 1)(n - 2) + 2$ 。试证 $G$ 有 Hamilton 圈，举例说明有 $n$ 个结点和

1 $(n - 1)(n - 2) + 1$ 条边的简单因没有Hamilton题

27. 令 $n \geqslant 8$ 是整数。设 $G_{n}$ 是图，其结点是 $\{1, 2, \dots, n\}$ 的 $n!$ 个排列，其中两个排列是由边连接，当且仅当利用交换二数一个排列可由另一个排列得。从3.6节的结果导出 $G_{n}$ 有Hamilton圈。  
28. 证明有 $n \geqslant 2$ 个结点的树至少有 2 个悬挂结点.  
29. 确定有7个结点的不同树，（恰有11个树。）  
30. 设 $G$ 是有 $n$ 个结点的简单图，证明当且仅当 $G$ 是连通的，且有 $n - 1$ 条边时， $G$ 是树。  
31. 令 $G$ 是简单图，证明当且仅当 $G$ 是连通的，并且删掉任何边得到非连通图时， $G$ 是树。  
32. 令 $G$ 是简单图，证明当且仅当 $G$ 没有基本圈，但加入了连接 $G$ 的两结点的新边后，恰好产生一个基本圈时， $G$ 是树。  
33. “森林”是没有蜀的简单图，证明森林的连通分图是树。  
34. 证明从树删掉一边就得到恰有两个连通分图的森林.  
35. 给定有 $k$ 个连通分图的森林，必须向 $G$ 加入多少条新边才能得到树？  
38. 确定从图10.15到10.18的每个图的生成树。  
37. 考察下图和其边分别为 $\{1, 2, 3, 6, 7\}$ 和 $\{7, 8, 9, 3, 10\}$ 的两个生成树 $T_{1}$ 与 $T_{2}$ . 试确定 $T_{2}$ 的一条边代替 $T_{1}$ 中的边 1, 以便得到新树. 对于 $T_{1}$ 的边 7 重作.

![](images/144b683cc67d2b8e0af526524d1e41d54d8a5d39e32a59f43c61508255f11b25.jpg)

38. 利用定理10.4.1和10.4.2，证明有 $n$ 个结点的每个树可以用下面方法“长”出来，并且下面构造总长成树：以结点 $x_{1}$ 开始。选取与 $x_{1}$ 不同的结点 $x_{2}$ ，并用一边连接 $x_{1}$ 与 $x_{2}$ 。假设 $k < n$ 并且已选取了 $x_{1}, x_{2}, \ldots, x_{k}$ ，选取 $x_{k+1}$ ，并用一边将 $x_{k+1}$ 仅与 $x_{1}, x_{2}, \ldots, x_{k}$ 中之一连接，当 $x_{1}, x_{2}, \ldots, x_{n}$ 已选出，停止。  
39. 令 $G$ 是连通图，在练习38所描述的构造中作必要调整，以便结果总是 $G$ 的生成树。  
40. 令 $G$ 是连通简单图。 $G$ 的割集是 $G$ 的这种边的集合 $F$ ，删掉其边剩下图为不连通的。基本割集是一割集 $F$ ，使得没有与 $F$ 自身不同的 $F$ 的子集是割集。验证 $\{1, 2, 9\}, \{3, 4, 5, 6\}$ 和 $\{1, 3, 6, 8\}$ 是练习37中因的割集。  
41. 令 $G$ 是连通的简单图和 $T$ 是 $G$ 的生成树。证明 $G$ 的每个割集至少包含 $T$ 的一条边。  
42. 令 $G = (N, E)$ 是连通的简单图和 $F$ 是 $G$ 的基本割集。证明存在 $N$ 的子集 $X$ ，使 $F$ 中的每条边连接 $X$ 中的结点与 $N - X$ 中的结点。  
43. 令 $G = (X, E)$ 是连通图和 $F$ 是不包括 $G$ 的任何圈的 $G$ 的边集合，证明 $G$ 有生成树 $T$ ，使 $F$ 的每条边是 $T$ 的边。  
44. 令 $G$ 是连通图，使每条边有正的成本 $c(e)$ 。如果 $G$ 中的各边成本均不相同，证明 $G$ 恰有成本尽可能小的生成树。  
45. 如果在练习37中图 $G$ 边上的数字表示边的成本，试确定 $G$ 有最小成本的生成树。  
46. 令在练习37中图 $G$ 的偶数字的边有成本1，并且奇数字的边有成本2。试确定有最小成本的生成树。这样的树多于一个吗？  
47. 考察下图，其中边上的数字表示成本，试确定有最小成本的

生成树.

![](images/723cd5fbe4d6389a141643442e949e4c3c5be9256df495f323645d1113e98440.jpg)

48. 把定理10.5.2的证明应用于练习37中的图和结点 $x_0$ ，其中边上的数字表示长。  
49. 把定理10.5.2的证明应用于练习47的图和结点 $x_0$ ，其中边上的数字表示长。  
50. 在Shannon开关对策中，当正对手 $P$ 后行动有 $P$ 取胜的策略，那么当 $P$ 先行动也有 $P$ 取胜的策略。试给出证明。  
51. 确定下面各图所进行的Shannon开关对策是否为正、负或中对策。（如若可能，试与朋友进行这些对策。）

![](images/319731a2fdaf0f77a62f318560f383a10ede9c1e59b25d65ba12e543297dfcff.jpg)

![](images/9026898956a64d2d380d141365fad0d00d7e9c31fa931ed5b790b88c77a49a51.jpg)

![](images/50bc76792627e8db0d68acaaf56b2a0077940ea59485c9a6aa0be210e160f086.jpg)

52. 令 $D$ 是有向图，证明如果存在从结点 $x$ 到结点 $y$ 的路，则存在从结点 $x$ 到结点 $y$ 的基本路。  
53. 证明有向图的回路上的弧可划分为集合 $A_{1}, A_{2}, \ldots, A_{h}$ ，使每个 $A_{i}$ 是一基本回路的弧集合。  
54. 令 $D$ 是每个结点的出度至少为 1 的有向图。证明 $D$ 有基本回路。  
55. 叙述并证明练习4与5对有向图的类似结论  
56. 证明有Euler回路的有向图是强连通的。  
57. 试给出没有Euler回路但其基础图有Euler圈的有向图的例子来.  
58. 证明定理10.7.1.   
59. 证明定理10.7.2.   
80. 考察完全图 $K_{n}$ （见练习13），并且对每边任意给出方向，于是得到有向图 $D$ ，则给定 $D$ 的任意两个不同结点 $\pmb{x}$ 和 $\pmb{y}$ ，存在从 $\pmb{x}$ 到 $\pmb{y}$ 或

从 $y$ 到 $x$ 的弧，但不是两者都存在。这样的有向图称为比赛图。证明 $D$ 有Hamilton路。

61. 令 $D$ 是强连通的比赛图（见练习60），证明 $D$ 有Hamilton回路。

62. 有向图 $D$ 的根是这样的结点 $x_0$ ，对所有的结点 $x \neq x_0$ ，存在从 $x$ 到 $x_0$ 的基本路。如果对每个结点 $x \neq x_0$ ，恰好存在从 $x$ 到 $x_0$ 的基本路，那么称 $D$ 为有根 $x_0$ 的树形图。证明有根 $x_0$ 的树形图的基础图是树。

63. 令 $D$ 是有根 $x_0$ 的有向图，证明与定理10.5.2类似的定理：存在有根 $x_0$ 的 $D$ 的生成树形图（即树形图 $D_0$ ，其结点是 $D$ 的结点，而弧为 $D$ 的某些弧），使得 $D$ 中从 $x$ 到 $x_0$ 的唯一的基本路是 $D$ 中从 $x$ 到 $x_0$ 的最短路

64. 令 $G$ 是具有嵌边 $e$ 的连通图，证明从 $G$ 中移去 $e$ 剩下的是恰好有两个连通分图的图。

65. 证明定理10.7.5的下面推广：令 $G$ 是连通图，则可能给 $G$ 的不是峡边的那些边赋予方向，并用两个方向相反的弧代替每个峡边，使所得有向图是强连通的。

86. 把定理10.7.5的证明应用于图10.16与图10.18中的图以及练习47.

67. 把练习65应用于图10.32中的图.

68. 证明有 $n \geqslant 2$ 个结点的连通图当且仅当每条边是峡边时是树

69. 令 $D$ 是其结点为 $N$ 的有向图。证明 $D$ 是强连通的，当且仅当对所有 $X \neq \emptyset$ 和 $X \neq N$ 的 $N$ 的子集合 $X$ ，至少存在其始点在 $X$ 中，终点在 $N - X$ 中的一弧以及其始点在 $N - X$ 中，终点在 $X$ 中的一弧。

70. 证明当且仅当有向图的结点可以排序为 $x_{1}, x_{2}, \cdots, x_{n}$ ，使得 $D$ 的每条弧形如 $(x_{i}, x_{j})$ ，（其中 $i < j$ ）时，它是没有基本回路的。

# 第十一章 色数、连通度及图的其他参数

数学中最有名的未解决的问题之一是四色问题。这个问题在1.4节已讨论过，现叙述如下。考虑一张平面地图，其中各个国家是连通区域，要求给各国家着色，使得有公共边界的国家具有不同的颜色。四种颜色是不是一定够用呢？我们将在11.3节证明五种颜色一定够用。1976年，K.Appel和W.Haken宣告1)他们解决了这个一百多年来的问题，证明了确实任何一张平面地图可以是四色的。考虑一张地图 $M$ ，并且在每一个区域内选择一点表示地图上一个国家，如果两个国家有公共边界，我们用曲线连接对应的两点。用这种方法我们可以构造一个平面图 $G$ ，其结点对应于国家，而且当且仅当对应的国家有公共边界时，两个结点是连接的，于是使得有公共边界的两个国家具有不同颜色的 $M$ 的一种着色，就相当于指定 $G$ 的结点的一个颜色，使得两个连接的结点指定不同的颜色。这便引出图的色数的概念。本章将研究图的色数，并证明平面图的色数至多为5，从而证明平面地图的五色定理。我们也要讨论与图有关的某些其它的常用的数值参数以及图的连通性。

# 11.1 色数

设 $G = (N, E)$ 是一个图。 $G$ 的着色是指定 $G$ 的各结点的一个颜色，使得由边连接的两点指定不同的颜色。如果这些颜

色取自 $k$ 种颜色的集合，那么不论这 $k$ 种颜色是否都用到了，这种着色都称为 $k$ 着色。图 $G$ 的色数 $\gamma(G)$ 是使 $G$ 具有 $k$ 着色的最小数 $k$ 。在研究图的色数时，我们将只限于简单图。这是合理的，因为倘若在图 $G$ 中存在把结点与其自身连接的边时，则 $G$ 无法着色，其次对于已经有边连接的两个结点，再添进一条连接这两个结点的新边，图的着色仍然不变。

例 考察图11.1中所示的图 $G$ 。它的色数至少是3，因为结点 $x, y, z$ 的每一对结点之间都有边连接。另一方面，如果我们指定 $x$ 和 $y$ 为红色， $u$ 和 $y$ 为兰色， $z$ 为绿色，就得到 $G$ 的3着色，因而 $\gamma(G) = 3$ 。

![](images/058e253ab1a2dd79185ef591348f19a5d307b40712826e6646b4baec08e089c8.jpg)  
图11.1

应用 在学期结束时，要求学生参加各门课程的考试。每门课程都在规定的课时内举行一次考试。为了不发生冲突（当学生在同一课时内要进行两门课程的考试时便出现冲突），所需的最少考试课时是多少？设学生的集合由 $S$ 表示，课程的集合由 $N$ 表示。对每门课程 $x$ ，令 $N_x$ 表示需要参加课程 $x$ 的考试的学生的集合。如果 $y$ 是另一门课程，则当且仅当 $N_x \cap N_y \neq \emptyset$ 时 $x$ 和 $y$ 必须安排在不同的课时内考试。构造图 $G = (N, E)$ ，当且仅当 $N_x \cap N_y \neq \emptyset$ 时 $[x, y]$ 为边。 $G$ 具有 $k$ 着色就相当于有 $k$ 个课时不发生冲突的考试安排——指定同一种颜色的课程都在同一课时内举行考试。因此，不发生冲突的最少考试课时数等于图 $G$ 的色数。

设 $G = (N, E)$ 是简单图，考虑用1，2，…， $k$ 表示的 $k$ 种颜色给 $G$ 着色的问题。对于 $i = 1, 2, \dots, k$ ，令 $N_i$ 表示已指定为颜色 $i$ 的 $G$ 的结点的集合。那么 $N_1, N_2, \dots, N_k$ 是 $N$ 的划

分，而且对于每个 $i$ ， $N_{i}$ 的任何两个结点间都没有边连接，设 $\pmb{A}$ 为图 $\pmb{G}$ 的一个结点集合，且 $\pmb{A}$ 的任何两个结点间都没有边连接，则称 $\pmb{A}$ 为一个稳定集合。于是 $\pmb{G}$ 的k着色给出把 $\pmb{N}$ 划分为 $\pmb{k}$ 个稳定集合的划分。反之，如果 $M_{1}$ ， $M_2$ ，…， $M_{k}$ 是把 $\pmb{N}$ 划分为 $\pmb{k}$ 个稳定集合的划分，则把 $M_{i}$ 中的结点指定为颜色 $i(i = 1$ ， $2,\dots ,k)$ ，就得到 $\pmb{G}$ 的一个k着色。于是 $\pmb{G}$ 的色数等于能够把结点集合N划分为稳定集合的最少个数。在图11.1所示的图 $\pmb{G}$ 中，色数等于3，而且 $\{x,v\} ,\{u,y\}$ 和 $\{z\}$ 是把结点集合划分为3个稳定集合的划分。

由于我们可以对图的各个结点指定为不同的颜色，图的色数不超过结点的个数。一个比较好而且也容易得到的色数界限由下面的定理给出。

定理11.1.1 设 $G = (N, E)$ 为简单图， $\Delta(G)$ 是 $G$ 的结点最大的度，则

$$
\gamma (G) \leqslant \Delta (G) + 1. \tag {11.1.1}
$$

证明 我们对 $G$ 的结点个数 $n$ 进行归纳证明。设 $n = 1$ ，则 $G$ 恰有一个结点没有边。因而 $\gamma(G) = 1$ ， $\Delta(G) = 0$ ，(11.1.1) 中的等式成立。现在设 $n > 1$ ，并且假定对有 $n - 1$ 个结点的图定理成立。设 $x$ 是 $G$ 的任何结点，又设 $G' = G_{N - \{x\}}$ 是由除 $x$ 以外的结点诱导子图。那么， $G'$ 有 $n - 1$ 个结点，并且根据归纳假设， $\gamma(G') \leqslant \Delta(G') + 1$ ，因为 $\Delta(G') \leqslant \Delta(G)$ ，我们断定 $\gamma(G') \leqslant \Delta(G) + 1$ 是正确的。现在来研究有 $\Delta(G) + 1$ 种颜色的 $G'$ 的着色。设 $y_1, y_2, \dots, y_k$ 是 $G$ 中和 $x$ 连接的结点，则 $k \leqslant \Delta(G)$ 。于是，在 $G'$ 的着色中， $\Delta(G) + 1$ 种颜色之一，比

如是红色，不指定给结点 $y_{1}, y_{2}, \cdots, y_{k}$ 中的任何结点。如果我们利用红色指定给 $x$ 从而把 $G'$ 的着色扩大到 $G$ ，那么我们便得到了有 $\Lambda(G) + 1$ 种颜色的 $G$ 的着色。因而， $\gamma(G) \leqslant \Delta G + 1$ 。于是由归纳可知定理为真。

在(11.1.1)中等号的出现是可能的。令 $n > 2$ ，并设 $C_{n}$ 表示有 $n$ 个结点和 $n$ 条边排成一个圈的简单图。那么 $\Delta(G_{n}) = 2$ ，可直接验证当 $n$ 为偶数时 $\gamma(C_{n}) = 2$ 和当 $n$ 为奇数时 $\gamma(C_{n}) = 3$ 。这样，对于 $n$ 为奇数， $\gamma(C_{n}) = \Delta(C_{n}) + 1$ 。现在令 $n \geqslant 1$ ，又令 $K_{n}$ 表示有 $n$ 个结点且每对不同结点都有一边连接的简单图。因而， $K_{n}$ 有

$$
\binom {n} {2} = \frac {n (n - 1)}{2}
$$

条边， $K_{n}$ 称为 $n$ 个结点的完全图。在 $K_{n}$ 的任何着色中，没有两个结点可以指定为相同颜色的。因而， $\gamma(k_{n}) = n$ 。另一方面， $\Delta(K_{n}) = n - 1$ 。于是， $\gamma(K_{n}) = \Delta(K_{n}) + 1$ 。Brooks 提出的定理断定，只有对连通图 $C_{n}(n$ 为奇数）和 $K_{n}(11,1,1)$ 中等式才成立。所以，如果 $G$ 不是 $C_{n}(n$ 为奇数）或 $k_{n}$ 的简单连通图，则 $\gamma(G) \leqslant \Delta(G)$ 。

当且仅当图没有边时，它的色数等于1。现在我们研究色数等于2的图。如果图 $G$ 有奇数长的基本圈，则由上一段可知， $\gamma(G)$ 不可能等于2。设 $G = (N, E)$ 是 $\gamma(G) = 2$ 的圈，这时结点集合 $N$ 能划分为两个稳定集合 $X$ 和 $Y$ 。于是， $G$ 的每条边都连接 $X$ 的结点和 $Y$ 的结点，即是形如 $[x, y]$ 的边，其中 $x \in X$ 和 $y \in Y$ 。所以，色数等于2的图是第九章所定义的偶图。因此，色数等于2的图的特征产生了偶图的特征。

定理11.1.2 设 $G = (N, E)$ 是至少有一条边的简单图。则当且仅当 $G$ 是没有奇数长的基本圈时 $\rho(G) = 2$ 。

证明 在前一段中，指出了有奇数长的基本圈的图不可能有等于2的色数。于是，假定 $G$ 没有奇数长的基本圈。我们可以设 $G$ 是连通的，否则，可在每个连通分图上论证（见练习5）。设 $x$ 是 $G$ 的任何结点， $Y$ 是全部连接 $x$ 的有奇数长链的结点 $y$ 的集合。设集 $X$ 是由 $x$ 以及全部连接 $x$ 的有偶数长链的结点 $z$ 所组成。假设在 $X \cap Y$ 中存在结点 $y$ ，则应存在连接 $x$ 和 $y$ 奇数长的链 $\gamma_1$ 与连接 $x$ 和 $y$ 偶数长的链 $\gamma_2$ 。可见 $G$ 会有奇数长的基本圈，其边是 $\gamma_1$ 和 $\gamma_2$ 的边的子集合（见练习9）。这是矛盾的，因此 $X \cap Y = \emptyset$ 。由于假设 $G$ 是连通的， $N = X \cup Y$ ，于是 $X, Y$ 是 $N$ 的划分。假若存在连接 $Y$ 的两结点 $y_1$ 与 $y_2$ 的边，那么由于存在从 $x$ 到 $y_1$ 的奇数边的链，故存在从 $x$ 到 $y_2$ 的偶数长的链。这与 $y_2$ 在 $Y$ 中是矛盾的。因而 $Y$ 是稳定集合。用同样方法，我们可以证明， $X$ 是稳定集合。所以， $N$ 划分为两个稳定集合 $X$ 和 $Y$ ，故 $\gamma(G) = 2$ 。

设 $G = (N, E)$ 是图，并令 $\pmb{x}$ 与 $\pmb{y}$ 是 $G$ 中没有边连接的两个不同结点。图 $G / xy$ 是给 $G$ 加入连接 $\pmb{x}$ 与 $\pmb{y}$ 的边 $[x, y]$ 而得到的图，称它为 $G$ 的联合。注意，图 $G / xy$ 和 $G$ 有同样的结点集合，但多了一条边。图 $G:xy$ 是用单个结点 $z_{xy}$ 代替 $G$ 中 $\pmb{x}$ 与 $\pmb{y}$ 而得到的，并且 $z_{xy}$ 和 $G$ 中原先与 $\pmb{x}$ 或 $\pmb{y}$ 连接的那些点相连接。图 $G:xy$ 称为 $G$ 的收缩（不管 $\pmb{x}$ 与 $\pmb{y}$ 是否连接，图 $G:xy$ 都有定义）。可以把 $G:xy$ 理解为将 $G$ 中 $\pmb{x}$ 与 $\pmb{y}$ 合并成单个结点并消去可能产生的任何双重边而得到的图。请注意， $G:xy$ 是比 $G$ 少一个结点并且它的边不比 $G$ 多。

例设 $G$ 是图11.2中所示的图，则 $G / xy$ 如图11.4所示，而 $G:xy$ 如图11.3所示.

定理11.1.3 设 $G$ 是图，又令 $x$ 与 $y$ 是 $G$ 的没有边连接的两个不同结点，则 $\gamma (G)$ 是数 $\gamma (G / xy)$ 与 $\gamma (G:xy)$ 的较小者：

![](images/271af0b3abc4a2b5679ee783c23c295fd5f191189e495b557f1e47a4bc3ac2dd.jpg)  
图11.2

![](images/4b10215c6c49a094f790ff7c45eb851c1ec641fb0ec79cc3cfa220e5708b588b.jpg)  
图11.4

![](images/2b5ec3c59a2cf896b3143c0b09d3f465474ec564f1fd1592d33a57c887b04f55.jpg)  
图11.3

$$
\gamma (G) = \min  \left\{\gamma (G / x y), \gamma (G: x y) \right\}.
$$

证明 设 $\gamma(G) = k$ ，并考虑 $G$ 的 $k$ 着色。假设把结点 $x$ 和 $y$ 指定为不同颜色，则 $G$ 的 $k$ 着色也是 $G / xy$ 的 $k$ 着色，因而，此时 $\gamma(G / xy) \leqslant k = \gamma(G)$ 。现在假设结点 $x$ 和 $y$ 的着色相同，那么 $G$ 的 $k$ 着色得到 $G : xy$ 的 $k$ 着色，只要把结点 $z_{xy}$ 指定与结点 $x$ 和 $y$ 为同样颜色。于是，这时 $\gamma(G : xy) \leqslant k = \gamma(G)$ 。由于在 $G$ 的着色中，或者指定 $x$ 与 $y$ 为不同颜色或者为相同颜色，所以

$$
\min  \left\{\gamma (G / x y), \gamma (G: x y) \right\} \leqslant \gamma (G). \tag {11.1.2}
$$

设 $y(G / xy) = k_{1}$ ，那么从 $G / xy$ 的 $k_{1}$ 着色得到 $G$ 的 $x$ 与 $y$ 指定为不同颜色的 $k_{1}$ 着色，因而

$$
\gamma (G) \leqslant k _ {1} = \gamma (G / x y) \tag {11.1.3}
$$

令 $y(G:xy) = k_2$ ，如果把 $z_{x,y}$ 的颜色指定给 $x$ 与 $y$ ，则从 $G:xy$ 的 $k_2$ 着色得到 $G$ 的 $k_2$ 着色。于是

$$
\gamma (G) \leqslant k _ {2} = \gamma (G: x y). \tag {11.1.4}
$$

合并(11.1.3)和(11.1.4)，我们得到

$$
\gamma (G) \leqslant \min  \left\{\gamma (G / x y), \gamma (G: x y) \right\}. \tag {11.1.5}
$$

综合(11.1.2)与(11.1.5)，可知定理的结论成立，证毕。

例 考察图11.2中所示的图 $G$ ，不难验证， $\gamma (G) = 3$ ， $\gamma (G / xy) = 4$ 和 $\gamma (G:xy) = 3$ 。于是

$$
\gamma (G) = 3 = \min  \{3, 4 \}
$$

定理11.13 提供了我们马上要叙述的图的色数的算法的基础。设 $G$ 是有 $n$ 个结点的简单图。

关于 $\gamma (G)$ 的算法：

(i) 如果 $G$ 是 $k_{n}$ , 则 $\gamma(G) = n$ . 如果 $G$ 不是 $k_{n}$ , 令 $H = G$ 并按(ii)进行.  
(ii) 选取 $H$ 的没有边连接的任意一对不同的结点 $x, y$ , 作图 $H / xy$ 与 $H:xy$ , 并按(iii)进行。  
(iii) 今 $H$ 是由 (ii) 得到的任何图。如果 $H$ 是完全图，比如说有 $k$ 个结点，则 $\gamma(H) = k$ 。如果 $H$ 不是完全图，依 (ii) 进行。

当所有得到的图都是完全图，算法结束。由于图的联合增加边数而不改变结点个数，图的收缩减少结点个数，所以经过有限步骤后，算法必定结束。从定理11.1.3可知， $G$ 的色数是所得到完全图的色数中最小者，因为完全图的色数等于它的结点个数， $\gamma(G)$ 是由算法生成的完全图的最少的结点数。

例 我们用图11.5中最上面图 $G$ 来解释上面算法。把 $x$ 和 $y$ 作为表示这个图中的在(ii)步所选用的一对结点的统用符号。我们把图的联合画在左边而把图的收缩画在右边。于是在得到一个 $K_{3}$ ，4个 $K_{4}$ 和3个 $K_{3}$ 后算法结束，因而 $\gamma(G) = 3$ 。我们可用下面方法确定 $G$ 的3着色。选取在算法中所得到的任何一个 $k_{3}$ ，那么存在唯一的图序列 $G_{1}, G_{2}, \cdots, G_{p}$ ，使得 $(a)G_{1}$ 选取为 $K_{3}, (b)G_{p}$ 是 $G$ ， $(c)$ 对于每个 $i = 1, 2, \cdots, p - 1$ ， $G_{i}$ 是 $G_{i + 1}$ 的联合或收缩。正如在定理11.1.3证明中所描述的那样， $G_{i}$ 的3着色确定了 $G_{i - 1}$ 的3着色。于是，从 $K_{3}$ 的3着色开始

得到 $G$ 的3着色。我们用图说明以图11.6中这种方法所得的 $G$ 的一种特殊3着色。我们使用颜色：红色 $R$ ，白色 $W$ ，兰色 $B$ 。

![](images/b479488d9f2fc4da170b04b7beed5f50cd9bd5f01153aaf937e60047b0f05291.jpg)  
图11·5

![](images/39f22729620536ee5d3481db1660e1e5caf5507db3088cbd89fba5271e7dc7f3.jpg)  
G

![](images/e01b563f5e2c107b4e2918439f20e3518af26b73aef5f78f87c4c54a9d0a53b3.jpg)  
G

![](images/a3876e625f66f039a1ab33721c1c6c9ac970672d5d009131a56c4641d3f854f7.jpg)  
G;

![](images/eb0d99f2773ea9c2cfc66ba3c7f534793512c8acac5aefca59e93524b079d936.jpg)  
G   
图11.8

下面我们讨论一种常用于确定图的色数的简便方法来结束这一节。设 $A$ 为图 $G = (N, E)$ 的结点的一个集合，如果从图 $G$ 移去集合 $A$ 中的结点（以及与 $A$ 相交的所有边）而得到的

图是不连通的，则称 $\pmb{A}$ 为关节集。于是，只要 $N - A$ 的诱导子图 $G_{N - A}$ 不是连通图，则 $\pmb{A}$ 是 $\pmb{G}$ 的关节集①.

例 对于图11.7中所示的图 $G$ ，集合 $A_{1} = \{y, v\}$ 和 $A_{2} = \{v, z\}$ 都是关节集。从 $G$ 移去 $A_{1} = \{y, v\}$ 的结点得到两个连通分图示于图11.8中。

![](images/765251521253ffe69706957c699eb48f4595fc4f0c8f60599e38b8c28520ee11.jpg)

![](images/4e88a15a69b4e5e6675f330771daed32c66f45c2e54363055471fdc79cef2f46.jpg)  
图11.7  
图11.8

定理11.1.4 设 $G = (N, E)$ 是简单图， $A$ 是 $G$ 的一个关节集，并且 $A$ 诱导子图 $G_{A}$ 是完全图。令 $N - A$ 的诱导子图 $G_{N - A}$ 的连通分图是 $G_{1} = (N_{1}, E_{1}), \cdots, G_{f} = (N_{f}, E_{f})$ 。那么， $G$ 的色数是图 $G_{A} \cup N_{1}, G_{A} \cup N_{2}, \cdots, G_{A} \cup N_{s}$ 的色数中的最大者：

$$
\gamma (G) = \max  \left\{\gamma \left(G _ {A} \cup N _ {1}\right), \dots , \gamma \left(G _ {A} \cup N _ {1}\right) \right\}.
$$

证明 设 $k = \max \left\{\gamma(G_{A} \cup N_{1}), \dots, \gamma(G_{A} \cup N_{i})\right\}, C$ 是 $k$ 种颜色的集合。那么，对于每个 $i = 1, 2, \dots, t$ ，可以给 $G_{A} \cup N_{i}$ 的结点指定集合 $C$ 的颜色得到 $G_{A} \cup N_{i}$ 的一种着色。因为 $G_{A}$ 是完全图， $A$ 的每对不同结点由边连接。于是，在 $G_{A} \cup N_{i}$ 的着色中， $A$ 的任何两个结点都指定不同的颜色。我们可以指定颜

色，使得 $A$ 的每个结点 $y$ ，在 $G_{A} \cup N_{1}$ ，…， $G_{A} \cup N_{t}$ 的着色中都用同一种颜色，我们可以使用 $C$ 中的颜色得到 $G$ 的着色如下： $A$ 的每个结点在每个 $G_{A} \cup N_{j}$ 的着色中指定同一种颜色；令 $x$ 是 $G$ 的结点但不是 $A$ 的结点，则 $x$ 恰好是在集合 $N_{1}$ ， $N_{2}$ ，… $N_{t}$ 之一，比如说 $x$ 在 $N_{j}$ 中，并且我们指定 $x$ 的颜色为它在 $G_{A} \cup N_{j}$ 的着色中指定的颜色。这便定义了 $G$ 的着色，因而

$$
\gamma (G) \leqslant | C | = k _ {\bullet}
$$

因为显然 $\gamma (G)\geqslant k$ ，这就完成了定理的证明。

这个定理的用途在于：确定 $G$ 的色数化为确定几个较小图的色数。

例设 $G$ 是图11.7中所示的图， $A = \{y, v\}$ ，则 $G_{A}$ 是完全图，根据这个定理， $G$ 的色数是图11.9中所示的两个图的色数的最大者。由于这两个图的色数都等于3，故 $\gamma(G) = 3$ 。

![](images/700cd2c57f5c93be1a422b9ffb9d24842248b41250e7c859399047aaffe03ea6.jpg)  
图11.9

# 11.2 平面图的Euler公式

为了证明平面图的五色定理，我们还需要使用Euler给出的重要公式。设 $G$ 是连通的平面图。现在允许存在几条边连接同一对结点和用一条边连接结点到其自身。平面图 $G$ 把平面分成一些区域，它们被 $G$ 的边组成的一些曲线所界定。这些区域中只有一个是无限的。

例图11·10中的图把平面分成6个区域，这个图有10个结点和14条边，无限区域是这个图的“外部”。注意， $10 - 14 + 6 = 2$ 。

我们把区域 $R_{4}$ 想象为由10条边所界定，这些边 $[y, u]$ ， $[u, v]$ ， $[u, w]$ 中每一条计算两次。正如我们通过 $R_{4}$ 的边界一样，这些边的每一条都通过两次，沿每个方向一次（见图

11.10中的虚线）.

R,

![](images/b98dcb4fdce9b5ae323203e3a6ebb1f69a41ab8986dc46b5d8eb5aaea7908a6f.jpg)  
图11·40

定理11.2.1（Euler公式）设 $G$ 是有 $\pmb{n}$ 个结点， $\pmb{e}$ 条边把平面分成r个区域的连通平面图，则利用关系

$$
n - e + r = 2
$$

$n, e, r$ 中任何一个数可由其它两个数来确定。

证明 首先假定 $G$ 是树，那么 $e = n - 1, r = 1$ ，且只存在一个无限区域。这时，

$$
n - e + r = n - (n - 1) + 1 = 2.
$$

现在假定 $G$ 不是树。由于 $G$ 是连通的， $G$ 至少有一个基本圈。因而，存在有限区域 $R$ ，并且 $R$ 的边界由基本圈 $\gamma$ 的边，也许还有其它的计算过两次的那种边组成。令 $G'$ 是移去 $G$ 的基本圈 $\gamma$ 中的一条边而得到的连通平面图。那么， $G'$ 有 $n'$ 个结点和 $e'$ 条边，并把平面分成 $r'$ 个区域，其中 $n' = n$ ， $e' = e - 1$ 和 $r' = r - 1$ （见图11.11，它是由图11.10移去边 $[y, z]$ 而得到的，此处 $R_0$ 和 $R_0$ 合并为一个区域）。因此，

$$
n ^ {\prime} - e ^ {\prime} + r ^ {\prime} = n - (e - 1) + (r - 1) = n - e + r,
$$

重复这一过程，最终我们得到一个树，它有 $n_0 = n$ 个结点和 $\pmb{e}_0 = n - 1$ 条边，其边把平面分成 $r_0 = 1$ 个区域，这里

$$
n - e + r = n _ {0} - e _ {0} + r _ {0} = n - (n - 1) + 1 = 2.
$$

这便完成了定理的证明。

![](images/33aa02e73633970d4cae57da9bb395769c1623fe0cc44c5afb22a522bf000415.jpg)  
图11·11

作为Euler公式的推论，我们得到下面关于简单图的定理。

定理11.2.2 设 $G$ 是连通简单平面图，则 $G$ 的结点的度至多为5。

证明 设 $G$ 有 $n$ 个结点和 $e$ 条边。我们可假设 $n > 2$ ，因为 $G$ 是简单图，由 $G$ 把平面划分的每个区域至少有三条边为界。另一方面，每条边恰好是两个区域的边界或者是一个区域的计数两次的边界。令区域是 $R_{1}, R_{2}, \ldots, R_{r}$ ，并令 $R_{i}$ 是由 $f_{i}$ 条边界定，则对于 $i = 1, 2, \ldots, r, f_{i} \geqslant 3$ ，使

$$
f _ {1} + f _ {2} + \dots + f _ {r} \geqslant 3 r. \tag {11.2.1}
$$

由于在和 $f_{1} + f_{2} + \dots + f_{n}$ 中每边计算两次，

$$
f _ {1} + f _ {2} + \dots + f _ {r} = 2 e. \tag {11.2.2}
$$

综合（11.2.1）与（11.2.2），我们得到

$$
3 r \leqslant 2 e. \tag {11.2.3}
$$

利用Euler公式， $r = 2 - n + e$ ，因而

$$
3 r = 6 - 3 n + 3 e, \tag {11.2.4}
$$

综合（11.2.3）和（11.2.4），可知

$$
6 - 3 n + 3 e \leqslant 2 e,
$$

或

$$
e \leqslant 3 n - 5. \tag {11.2.5}
$$

令 $G$ 的结点的度是 $d_{1}, d_{2}, \cdots, d_{n}$ 。假设与定理的结论相反，对于 $i = 1, 2, \cdots, n, d_{i} \geqslant 6$ 。那么利用（11.2.5）和方程 $d_{1} + d_{2} + \cdots + d_{n} = 2e$ ，我们得到

$$
6 n \leqslant d _ {1} + d _ {2} + \dots \dots + d _ {n} = 2 e \leqslant 6 n - 1 2,
$$

或 $0 \leqslant -12$ ，这是矛盾的，因而存在 $G$ 的结点，它的度至多为5.

利用Euler公式，可以证明某些图不是平面的。

定理11.2.3 当且仅当 $n \leqslant 4$ 时，完全图 $K_{n}$ 是平面的。

证明 对于 $n = 1,2,3,4$ ，显然 $K_{n}$ 是平面的，于是，我们只须证明当 $\pmb{n} \geqslant 5$ 时， $K_{n}$ 不是平面的，为此证明 $K_{5}$ 不是平面图就足够了。假若 $K_{5}$ 是平面的。考察平面图 $K_{5}$ ，因为 $K_{5}$ 有 $n = 5$ 个结点和 $e = 10$ 条边，从Euler公式可知，它把平面分成7个区域。令这些区域的边界的边数分别是 $f_{1}, f_{2}, \cdots, f_{7}$ 。则 $f_{i} \geqslant 3 (i = 1,2,\cdots,7)$ 和

$$
2 1 = 3 \times 7 \leqslant f _ {1} + f _ {2} + \dots + f _ {7} = 2 e = 2 \times 1 0 = 2 0.
$$

这是个矛盾，这就证明 $K_{3}$ 不是平面的。

设 $K_{m,n}$ 是有 $m + n$ 个结点的简单偶图，它划分成两个分别有 $m$ 与 $n$ 个结点的集合 $X$ 和 $Y$ ，使得对于 $X$ 中的每个 $x$ 和 $Y$ 中的每个 $y$ ， $[x,y]$ 是一条边。这种图称为完全偶图。

定理11.2.4 完全偶图 $K_{m,n}$ 当且仅当 $m \leqslant 2$ 或 $n \leqslant 2$ 时是平面图。

证明 容易看出，若 $m \leqslant 2$ 或 $n \leqslant 2$ ， $K_{m,n}$ 是平面的。现在设 $m \geqslant 3$ 和 $n \geqslant 3$ ，为了证明 $K_{m,n}$ 不是平面的，只需证明 $K_{3,3}$ 不

是平面的就足够了。假设 $K_{3,3}$ 是平面的，考察平面图 $K_{3,3}$ 。因为 $K_{3,3}$ 有6个结点和9条边，由Euler公式可知，它把平面划分成5个区域。另一方面，因为 $K_{3,3}$ 没有奇数长的圈，每个区域都是由4条或更多条的边界定。令5个区域的边界的边数分别是 $f_{1}, f_{2}, \cdots, f_{5}$ ，于是，对于 $i = 1, 2, \cdots, 5$ ， $f_{i} \geqslant 4$ 。从而

$$
2 0 = 4 \times 5 \leqslant f _ {1} + f _ {2} + \dots + f _ {8} = 2 e = 2 \times 9 = 1 8.
$$

这是一个矛盾，这就证明了 $K_{3,3}$ 不是平面的。

由定理11.2.3可知， $K_{6}$ 不是平面图。可见由 $K_{8}$ 在其边上逐渐插入新结点而得到的任何图都不是平面的。例如，图11.12中所示的图不是平面的。这样的图称为 $K_{6}$ 的同胚。同样，由定理11.2.4知， $K_{3,3}$ 不是平面的，并且由 $K_{3,3}$ 逐次在边上插入新结点而得到的任何图也不是平面的。（它称为 $K_{3,3}$ 的同胚）。当且仅当一个图既不包含 $K_{6}$ 的同胚也不包含 $K_{3,3}$ 的同胚时，它是平面的。这就是重要而著名的 Kuratowski 定理。

![](images/388865415f75c416dcaf78471072fff09bc8e42822ea53a258beddf6bf6825eb.jpg)  
图11.12

# 11.3 五色定理

本节我们证明五色定理：每个平面图的色数等于或少于

5. P.J.Heawood发现了Kempe在1879年证明每个平面的图色数等于或少于4的缺陷后，在1890年他证明了五色定理。由定理11.2.2，对顶点个数实行归纳，容易证明每个平面图的色数等于或少于6（见练习18）。

我们首先证明应用于非平面图以及平面图的予备定理。

定理11.3.1 设 $G$ 是简单图，并考虑 $G$ 的 $k$ 着色。令 $\alpha$ 和 $\beta$ 是其中的两种颜色，又设 $G_{\alpha \beta}$ 是 $G$ 中指定颜色为 $\alpha$ 或 $\beta$ 的那些结点的诱导子图。如果我们交换 $G_{\alpha \beta}$ 的连通分图 $C$ 中颜色 $\alpha$ 与 $\beta$ ，便得到 $G$ 的新的 $k$ 着色。

证明 假设交换 $G_{\alpha \beta}$ 的连通分图 $C$ 中颜色 $\alpha$ 与 $\beta$ 之后，存在两个结点 $x$ 和 $y$ ，它们被指定了同样颜色并且由一边连接着。这个颜色一定是 $\alpha$ 或 $\beta$ 。为确定起见，假设它是颜色 $\alpha$ 。如果 $x$ 与 $y$ 都是 $C$ 的结点，则它们应指定不同颜色。同样，如果 $x$ 与 $y$ 都不是 $C$ 的结点，那么它们应指定为不同颜色。因而，比方说 $x$ 是 $C$ 的结点而 $y$ 不是 $C$ 的结点，这时在给定 $G$ 的着色中， $x$ 指定为颜色 $\beta$ 和 $y$ 指定为颜色 $\alpha$ 。但由于 $x$ 和 $y$ 由边连接， $y$ 也是 $G_{\alpha \beta}$ 的连通分图 $C$ 的结点。这是一个矛盾，定理证毕。

定理11.3.2 令 $G$ 是简单平面图，则 $\gamma(G) \leqslant 5$

证明 对 $G$ 的结点数目n实行归纳证明，如果 $\pi \leqslant 5$ ，则显然 $\gamma (G)\leqslant 5$ ，现在令 $n > 5$ ，归纳假设是具有 $\pi -1$ 个结点的所有平面图，其色数等于或少于5．设 $G$ 是平面图.由定理11.2.2， $G$ 的结点的度至多为5．令 $H$ 是由 $G$ 移去结点x以及所有与x相交的边而得到的图（于是， $H$ 是 $G$ 的与 $\pmb{x}$ 不同的所有结点诱导的图）．由于 $H$ 是平面图，从归纳假设可知， $\gamma (H)\leqslant 5$

考虑 $H$ 的5着色。假设 $G$ 中 $\pmb{x}$ 的度小于5，则5种颜色的一种没有指定给 $G$ 中连接 $\pmb{x}$ 的任何结点，因此我们可以把这种颜色指定给以确定 $\gamma (G)\leqslant 5$ 。现在假定 $\pmb{x}$ 的度等于5，如果在

$G$ 中连接 $x$ 的两个结点在 $H$ 的着色中指定同样的颜色，则5种颜色中之一种颜色没有指定给连接 $x$ 的任何结点，因而我们又得到 $\gamma (G)\leqslant 5$ ，于是，我们可以假定在 $H$ 的着色中 $G$ 的连接 $x$ 的诸结点中任何两个结点都指定为不同颜色。设连接 $x$ 的结点是 $x_{1},x_{2},\dots ,x_{5}$ ，并且设它们在 $G$ 的平面图中的情形如图11.13中所示。对于 $i = 1,2,\dots ,5$ ，在 $H$ 的着色中令 $x_{i}$ 指定为 $\alpha_{i}$ 。

![](images/bc99975c1df7c9b7e48224da0b41d0523065837ca24c497d91b6aff2d1452160.jpg)  
图11.13

考虑结点 $x_{1}$ 和 $x_{3}$ 以及 $H$ 的着色为 $\alpha_{1}$ 或 $\alpha_{3}$ 的一些结点的诱导子图 $H\alpha_{1}\alpha_{3}$ 。假设 $x_{1}$ 与 $x_{3}$ 不在 $H\alpha_{1}\alpha_{3}$ 的同一个分图中，则由定理11.3.1可知，存在 $H$ 的5着色，其中 $x_{1}$ 与 $x_{3}$ 指定为同样颜色。如前可知， $\gamma(G) \leqslant 5$ 。这样，我们可以假定， $x_{1}$ 与 $x_{3}$ 是在 $H\alpha_{1}\alpha_{3}$ 的同一个分图中，因而 $H\alpha_{1}\alpha_{3}$ 中存在连接 $x_{1}$ 与 $x_{3}$ 的基本链。沿这条链的结点指定或为颜色 $\alpha_{1}$ 或为颜色 $\alpha_{3}$ 。这条链，连同 $G$ 中连接 $x$ 与 $x_{1}$ 的边和连接 $x$ 与 $x_{3}$ 的边一起定义一简单封闭曲线，使 $x_{2}$ 或者在它的内部（图11.14）或者在它的外部（图11.15）。

在这两种情况下，我们都知道， $x_{2}$ 与 $x_{4}$ 位于图 $H\alpha_{1}\alpha_{4}$ 的不同的连通分图中。由定理11.3.1可知，存在 $H$ 的5着色，使得在这种着色中， $x_{2}$ 与 $x_{4}$ 指定为同样颜色。我们再次得到结

![](images/d652879288b8ac386011c55cdfa21cfa8ceb965a67b3519eb31822821aa4901f.jpg)  
图11.14

![](images/1755ce4bf1cb6fe0be73d8be2fa74ef563545b29642b1ba19098a48dab44519b.jpg)  
图11.15

果 $\gamma (G)\leqslant 5$ ，由于考虑所有可能性，由归纳法定理得证.

H. Hadwiger提出了比四色问题更一般的问题。对于简单图 $G = (x, E)$ ，定义 $G$ 的部分图为简单图 $H = (X, F)$ ，其中 $F \subseteq E$ 。于是， $G$ 的部分图的结点是 $G$ 的结点，并且边是 $G$ 的某些边。 $G$ 的部分子图是 $G$ 的部分图的诱导子图。这样， $G$ 的部分子图的结点是 $G$ 的某些结点，边是 $G$ 的连接这些结点的一些边。

例设 $G$ 是图11.16所示的图，则图11.17中的图是 $G$ 的部分子图。

![](images/835063bbdfe64315263b4ea26da0db20b63528811ef21870ba06590ac97749d3.jpg)  
图11.16

![](images/9c69308ab5cb2f8318dbb2f0d1c36b4e6943c473dd6fa8e2a675d9eb9d2f354c.jpg)  
图11.17

Hadam 猜想如下：设 $G$ 是简单图，如果 $\gamma(G) = m$ ，则 $G$ 包含有与 $K_{m}$ 同胚的部分子图。等价地，如果对正整数 $m$ ， $G$ 不包含有与 $K_{m}$ 同胚的部分子图，则 $\gamma(G) < m$ 。

Hadamard猜想之逆不成立。于是，对于简单图可能包含 $K_{m}$ 的同胚并且色数小于 $m$ 。例如，如果图 $G$ 仅仅是长为 4 的基本圈，则 $G$ 自身是与 $K_{3}$ 同胚的，但 $\gamma(G) = 2$ 。

定理11.3.3 如果 Hadwiger 猜想对于 $m = 5$ 为真，则每个简单平面图有 4 着色。

证明 在 Hadwiger 猜想对 $m = 5$ 为真的前提下。假设存在简单平面图 $G$ ，且 $\gamma(G) > 4$ 。那么由定理11.3.2知 $\gamma(G) = 5$ 。从 Hadwiger 猜想对 $m = 5$ 为真得出， $G$ 包含与 $K_{5}$ 同胚的部分子图 $H$ ，因为 $G$ 是平面的，所以 $H$ 也是平面的。另一方面，由定理11.2.3可知， $H$ 不是平面的，这种矛盾确定了 $\gamma(G) \leqslant 4$ 。

尽管现在知道的每个简单平面图有4着色，对于 $m = 5$ 的Hadamard猜想仍未解决。不过，下面的定理成立。

定理11.3.4 Hadwiger猜想对于 $m = 2$ 和 $m = 3$ 为真。

证明 设 $G$ 是简单图。假设 $\gamma(G) = 2$ ，则 $G$ 至少包含有一条边。因而， $G$ 有部分子图 $K_{2}$ ，现在假定 $\gamma(G) = 3$ 。由定理

1.1.2可知， $G$ 有奇数长的基本圈。于是， $G$ 有长至少为3的圈，因为这种圈是 $K_{3}$ 的同胚，这便确立了对于 $m = 3$ 的Hadamard猜想。

对于 $m = 4$ 的 Hadwiger 猜想已经由 G. Dirac 证明（见练习 29）。这样，Hadam 猜想第一未解决情形是 $m = 5$ 的情形。

# 11.4 连通度

本节我们讨论关于图连通程度的一种度量。在10.4节我们曾评述过，连通图的独立圈数可作为图的连通性的度量，并且指出它的局限性。可以用下述方法定义更有用的度量。令 $G = (X, E)$ 是有 $n$ 个结点的图。 $G$ 的连通度，记为 $\kappa(G)$ ，它是从 $G$ 移去 $A$ 中的全部结点以及与 $A$ 相交的全部边后得到非连通图或单个结点图的集合 $A$ 中的最小结点数，符号表为

$\kappa (G) = \min \left\{\mid A\mid :G_{X - A}\right.$ 是非连通的或有单个端点}若 $G$ 不是连通的，取 $A = \emptyset$ 可以得到 $\kappa (G) = 0$

例 考察图11.18中所示的三个图 $G_{1}$ , $G_{2}$ 和 $G_{3}$ . 我们有 $\kappa(G_{1}) = 1$ , $\kappa(G_{2}) = 2$ 和 $\kappa(G_{3}) = 3$ .

![](images/f54fc3552e71b30568e702bf5cfd918774e0b3c72e95f5237c4329b9f620e465.jpg)  
G

![](images/2dece399824b40ff14f717c063db5a52d9436307953624eeb66dbb9475978d73.jpg)  
G

![](images/cc5caea79aabbf004e8a308274ac8ae41d0479139a9b65137d7685d3092dd154.jpg)  
G   
图11.18

由于加入环或连接已经由边连接的一对结点的新边不改变图的连通性，在本节我们限于研究简单图。

定理11.4.1 令 $G = (X, E)$ 是有 $\pmb{n}$ 个结点的简单图，则 $\kappa(G) \leqslant n - 1$ ，当且仅当 $G$ 是 $K_{n}$ 时等式成立。

证明 首先假设 $G$ 是 $K_{n}$ 。如果 $A \subseteq x$ 且 $|A| = t$ ，则 $G_{X - A}$ 是 $K_{n - t}$ 。于是，若 $t \leqslant n - 2, G_{X - A}$ 是连通的，而若 $t = n - 1, G_{X - A}$ 有单个结点，因而， $\kappa(G) = n - 1$ 。现在假设 $G$ 不是 $K_{n}$ ，于是 $G$ 有两个结点 $x$ 与 $y$ ，没有边连接。令 $A = X - \{x, y\}$ ，则 $G_{X - A} = G\{x, y\}$ 是有两个结点而没有边的图，所以它不是连通的。由于 $|A| = n - 2$ ， $\kappa(G) \leqslant n - 2$ 。

回忆图 $G$ 的关节集是结点集合 $A$ ，使得图 $G_{X-A}$ 不连通。 $K_{n}$ 没有关节集，正如上面定理证明所指的那样，不是 $K_{n}$ 的简单图有 $n-2$ 个结点的关节集。由此得出结论：不是 $k_{n}$ 的简单图的连通度是关节集中的最少结点数目。

对于简单图的连通度的一个简单的估计如下：

定理11.4.2 令 $G$ 是简单图，又令 $d$ 是 $G$ 的结点最小度，则

$$
\kappa (G) \leqslant d.
$$

证明 如果 $G$ 是 $K_{n}$ , 则 $\kappa(G) = n - 1 = d$ , 且定理成立。假设 $G$ 不是 $K_{n}$ , 则 $d \leqslant n - 2$ 。令 $x$ 是 $G$ 的度为 $d$ 的结点, $A$ 是由边连接到 $x$ 的那些结点组成的集合, 则 $|A| = d$ 和 $A$ 是关节集, 因而

$$
\kappa (G) \leqslant | A | = d _ {\bullet}
$$

很难利用结点的度得到连通度的下界。连通图的全部结点可以有“大”的度，图的连通度却仍是小的。为弄清这一点，取两个无公共结点的 $K_{n}(n \geqslant 2)$ 图，并连接一个图的结点 $x$ 与另一个图的结点 $y$ ，得到的图有 $2n$ 个结点，结点的最小度等于

$n - 1$ ，因为 $\{x\}$ 是关节集，图的连通度只是1．于是，对于图的连通度利用结点的度的下界必将是度的更复杂函数。在得到这样的界限之前（J.A.Bondy得出），我们需要一个预备知识。

定理11.4.3 设 $G$ 是有结点 $x_{1}, x_{2}, \cdots, x_{n}$ 的简单图，它们的度分别是 $d_{1}, d_{2}, \cdots, d_{n}$ ，且 $d_{1} \leqslant d_{2} \leqslant \cdots \leqslant d_{n}$ 。令 $m = d_{n}$ ，是结点的最大的度。假定对于所有 $k < n - m - 1$ ， $d_{k} \geqslant k$ ，则 $G$ 是连通图。

证明 假设相反， $G$ 是不连通的，则 $G$ 的结点可以划分为两个非空集合 $X_{1}$ 和 $X_{2}$ ，使得 $G$ 的每条边连接 $X_{1}$ 中两结点或 $X_{2}$ 中两结点。令 $|X_{1}| = p$ 和 $|X_{2}| = n - p$ ，其中 $p \leqslant n - p$ 。那么 $X_{1}$ 中结点的度至多为 $p - 1$ ，而 $x_{2}$ 中结点的度至多是 $n - p - 1$ ，因而 $m \leqslant n - p - 1$ ，或 $p \leqslant n - m - 1$ 。现在根据假设可得， $d_{p} \geqslant p$ 。于是， $G$ 的结点中第 $p$ 个最小的度至少为 $p$ 。但是，这是一个矛盾，因为 $X_{1}$ 包含 $p$ 个结点，每个结点的度至多为 $p - 1$ 。这个矛盾是由于假设 $G$ 不是连通的而产生的。所以， $G$ 是连通的。

定理11.4.4 设 $G = (X, E)$ 是具有结点 $x_1, x_2, \dots, x_n$ 的简单图，结点的度分别为 $d_1, d_2, \dots, d_n$ ，且 $d_1 \leqslant d_2 \leqslant \dots \leqslant d_n$ 。假设对某个整数 $h$ 且 $1 \leqslant h \leqslant n - 1$ ，对于所有 $k \leqslant n - d, n - h + 1 - 1$ ，有 $d_k \geqslant k + h - 1$ ，那么， $\kappa(G) \geqslant h$ 。

证明 我们通过指明 $G$ 没有由 $h - 1$ （或更少）个结点组成的关节集来证明 $\kappa(G) \geqslant h$ 。设 $A$ 是 $G$ 的 $h - 1$ 个结点的集合。图 $G' = G \times_{-A}$ 有 $n' = n - h + 1$ 个结点，我们利用定理11.4.3证明 $G'$ 是连通的。设 $X \cdots A$ 中的结点是 $x_1', x_2', \cdots, x_{n-h+1}'$ ，又设它们在图 $G'$ 中的度分别是 $d_1', d_2', \cdots, d_{n-h+1}'$ ，这里 $d_1' \leqslant d_2' \leqslant \cdots \leqslant d_{n-h+1}'$ 。令 $m' = d_{n-h+1}'$ ，是 $G'$ 中结点的最大度，因为 $|A| = h - 1$ ， $G$ 中 $x_1', x_2', \cdots, x_{n-h+1}'$

的度分别至多是 $d_{1}^{\prime} + (h - 1)$ ， $d_2^{\prime} + (h - 1),\dots ,d^{\prime}n - h + 1$ $+(h - 1)$ 。由此可得

$$
d _ {i} \leqslant d _ {i} ^ {\prime} + (h - 1) \quad (i = 1, 2, \dots , n - h + 1 = n ^ {\prime}) \tag {11.4.1}
$$

特别有

$$
d _ {n - h + 1} \leqslant d ^ {\prime} _ {n - h + 1} + (h - 1) = m ^ {\prime} + h - 1,
$$

因此

$$
m ^ {\prime} \geqslant d _ {n - h + 1} - h + 1. \tag {11.4.2}
$$

设 $k \leqslant n' - m' - 1$ ，则由(11.4.2)得到

$$
\begin{array}{l} k \leqslant (n - h + 1) - (d _ {n - h + 1} - h + 1) - 1 = n - d _ {n - h + 1} \\ - 1 _ {\bullet} \\ \end{array}
$$

根据假设

$$
d _ {k} \geqslant k + h - 1,
$$

所以，利用(11.4.1)，

$$
d ^ {\prime} k \geqslant d _ {k} - (h - 1) \geqslant (k + h - 1) - (h - 1) = k.
$$

由定理11.4.3得知， $G^{\prime}$ 是连通的，于是， $A$ 不是 $G$ 的关节集。我们断定 $G$ 的每个关节集至少包含 $k$ 个结点，故 $\kappa (G)\geqslant k$

设 $G$ 是至少有 2 个结点的简单图。则当且仅当 $\kappa(G) \geqslant 1$ 时 $G$ 是连通的。现在研究 $\kappa(G) \geqslant 2$ 的意义所在，连通度至少为 2 的简单图称为 2 连通的。一个 2 连通图必至少有 3 个结点。图 $G$ 的关节点定义为这样的结点 $x$ ，使得 $\{x\}$ 是关节集。

定理11.4.5 设 $G = (X, E)$ 是有 $n \geqslant 3$ 个结点的简单图。则下面各命题是等价的：

(i) $\kappa (G)\geqslant 2$   
(ii) $G$ 是连通的且没有有关节点.  
(iii) 给定任意 3 个不同的结点，存在连接任何 2 个结点但不包含第三个结点的基本链。

证明 因为 $G$ 至少有 3 个结点，当且仅当 $\kappa(G) = 0$ （即 $G$ 是不连通的）或 $\kappa(G) = 1$ （即 $G$ 有关节点）时， $\kappa(G) < 2$ 。这便证明了 (i) 与 (ii) 是等价的。

现在假设(ii)成立。令 $x$ ， $y$ ， $z$ 是三个不同结点，并考察 $x$ 和 $y$ 。因为 $G$ 没有关节点，所以， $z$ 不是关节点。因此，图 $G_{X - \{z\}}$ 是连通的。于是，在 $G_{X - \{z\}}$ 中存在连接 $x$ 与 $y$ 的链。这链是 $G$ 中不包含 $z$ 的链。于是(iii)成立。反之，假设(iii)成立。那么， $G$ 一定是连通的。假设 $z$ 是关节点，于是，图 $G_{X - \{z\}}$ 应至少有2个连通分图。令 $x$ 与 $y$ 是在 $G_{X - \{z\}}$ 的两个不同分图中的结点。那么，连接 $G$ 中的 $x$ 与 $y$ 的每个链包含 $z$ 。这是矛盾的，因此证明了(ii)成立，于是(ii)与(iii)等价，定理得证。

唯一的没有关系节点，且满足 $\kappa(G) < 2$ 的简单连通图是 $K_{2}$ .

设 $G = (X, E)$ 是简单图。即使 $G$ 不是 2 连通的，可能存在 $A \subseteq X$ ，使得诱导图 $G_A$ 是 2 连通的。

例设 $G = (X,E)$ 是图11.19所示的图，因为有象 $\{c\}$ 这样的关节点，故 $G$ 不是2连通的。如果 $A = \{b,c,d,e\}$ ，则 $G_{A}$ 是2连通的。

![](images/f65704016128ec29cdc025061002f765be05c8ca21400e7138802b1359af12a6.jpg)  
图11.19

连通简单图 $G = (X, E)$ 的块是连通的而又设有关节点的诱导子图 $G_{A}$ ，并且如果 $B$ 是具有 $A \subseteq B, A \neq B$ 性质的任何结点

集合，则 $G_{B}$ 或不是连通的或有关节点。于是，块对应着最大结点集合，它诱导的连通子图无关节点。综上所述，块或是 $K_{2}$ 或是 2 连通图。

例图11.19中所示的图的块是由 $\{a, b\}$ ， $\{b, c, d, e\}$ ， $\{c, f, g, h\}$ ， $\{h, i\}$ ， $\{i, j\}$ ， $\{i, k\}$ 诱导的那些子图。这些块中有四个是 $K_{2}$ 。注意每对不同块至多有一公共结点。

定理11.4.6 设 $G$ 是连通的简单图，则每对不同块至多有一公共结点。特别， $G$ 的每条边恰是 $G$ 中一个块的一条边（这些块划分了 $G$ 的边集合）。

证明 设 $G_A$ 与 $G_B$ 是 $G$ 的块且 $A \neq B$ 。如果 $A$ 或 $B$ 恰有 2 个结点，由块的定义可得， $A \cap B$ 至多包含一个结点。于是，我们可以假定 $A$ 与 $B$ 的每一个至少包含 3 个结点。因而，块 $G_A$ 和块 $G_B$ 都是 2 连通的。假设 $A \cap B$ 至少包含 2 个结点，由于 $G_A$ 和 $G_B$ 都是连通的以及 $A \cap B \neq \emptyset$ ， $G_A \cup B$ 是连通的。设 $x$ 是 $A \cup B$ 中的一结点。因为 $G_A$ 和 $G_B$ 都是 2 连通的， $GA - \{x\}$ 和 $GB - \{x\}$ 都是连通的。而且，由于 $A \cap B$ 至少包含两个结点，那么 $(A - \{x\}) \cap (B - \{x\}) \neq \emptyset$ ，因而 $G(A \cup B) - \{x\}$ 是连通的。我们由此断定， $G_A \cup B$ 是 2 连通的。因为 $A \neq B$ ， $A \cup B$ 是结点集合，它满足 $A \subseteq A \cup B$ ， $A \neq A \cup B$ ，使得 $G_A \cup B$ 是 2 连通的。这与 $G_A$ 是 $G$ 的块相矛盾。因此，两个块至多有一个公共结点。可见，没有任何边是在两个不同块中。所以，块划分 $G$ 的边。

现在我们确定2连通图的另一个特征。

定理11.4.7 设 $G = (X, E)$ 是具有 $n \geqslant 3$ 个结点的简单图，则当且仅当下面条件成立时， $G$ 是2连通的。

(iv)给定任意一对不同结点 $x$ 与 $y$ ，存在着包含 $x$ 和 $y$ 的基

本图。

证明 首先假定 $G$ 不是 2 连通的, 则由定理 11.4.5, $G$ 或者不是连通的或者有关节点. 假若 $G$ 不是连通的, 并且 $x$ 与 $y$ 是 $G$ 的不同连通分图中的结点, 则不存在连接 $x$ 与 $y$ 的任何链, 因而没有包含 $x$ 与 $y$ 的基本圈. 假设 $z$ 是 $G$ 的关节点. 令 $x$ 与 $y$ 是 $G_{X \{-z\}}$ 的不同连通分图中的结点, 那么 $G$ 中的连接 $x$ 与 $y$ 的每条链都包含 $z$ . 因此, 在 $G$ 中不存在包含 $x$ 和 $y$ 的基本圈. 这就证明了, 如果 $G$ 不是 2 连通的, (iv) 不成立.

其次假设 $G$ 是 2 连通的。令 $x$ 与 $y$ 是 $G$ 的不同结点，设 $A$ 是与 $x$ 不同的所有结点 $u$ 组成的集，使得存在包含 $x$ 和 $u$ 的基本圈。我们希望证明 $y$ 在 $A$ 中。我们首先证明 $A \neq \emptyset$ 。实际上，令 $u$ 是由边 $[x, u]$ 连接 $x$ 的任意结点。假设删除边 $[x, u]$ 得到非连通图 $G'①$ 。那么 $x$ 与 $u$ 是在 $G'$ 的不同连通分图中，又因 $G$ 至少有 3 个结点，这些分图的一个，比方说包含 $x$ 的那个分图至少有 2 个结点，那么 $\{x\}$ 是关节点，这是一个矛盾。所以 $G'$ 是连通的，并在 $G'$ 中存在连接 $x$ 与 $u$ 的基本链 $x, v, \cdots, u$ 。因此， $x, v, \cdots, u, x$ 是 $G$ 中包含 $x$ 与 $v$ 的基本圈。于是 $u$ 在 $A$ 中，故 $A \neq \emptyset$ 。假设 $y$ 不在 $A$ 中。令 $w$ 是 $A$ 中一结点，使得从 $w$ 到 $y$ 的基本链的长尽可能小。令 $\gamma_0$ 是从 $w$ 到 $y$ 的这种最短的基本链。由于 $w$ 在 $A$ 中，存在包含 $x$ 和 $w$ 的基本圈 $\gamma$ ，基本圈 $\gamma$ 包含连接 $x$ 与 $w$ 的两个基本链 $\gamma_1$ 与 $\gamma_2$ （见图11.20）。因为 $G$ 没有关节点， $w$ 不是关节点，因而存在不包含结点 $w$ 的连接 $x$ 与 $y$ 的基本链 $\gamma'$ 。令 $\omega'$ 是与 $w$ 不同的 $\gamma_0$ 的第一个结点，也是 $\gamma'$ 的一个结点（ $w'$ 可以是 $y$ ），又令 $\chi'$ 是 $\gamma'$ 的最末一个结点，也是 $\gamma$ 的结点（ $x'$ 可以是 $x$ ）。由于 $\chi'$ 或是 $\gamma_1$ 的结点或是 $\gamma_2$ 的结点，不妨令 $\chi'$ 是

$\gamma_{1}$ 的结点（见图11.21）。沿 $\gamma_{1}$ 由 $x$ 到 $x^{\prime}$ ，沿 $\gamma^{\prime}$ 由 $x^{\prime}$ 到 $x$ ，沿 $\gamma_{0}$ 由 $w^{\prime}$ 到 $w$ ，沿 $\gamma_{2}$ 由 $w$ 到 $x$ ，我们构造了包含 $x$ 与 $w^{\prime}$ 的基本圈。因此 $w^{\prime}$ 是在 $A$ 中。由于 $w^{\prime} \neq w$ ，存在比 $\gamma_{0}$ 还短的连接 $w^{\prime}$ 与 $y$ 的基本链。因为 $w^{\prime}$ 是在 $A$ 中，这与我们对 $w$ 的选取相矛盾。这种矛盾是由于假设 $y$ 不在 $A$ 中而产生的。因此， $y$ 是在 $A$ 中，并且存在包含 $x$ 与 $y$ 的基本链。故 (iv) 满足，定理证毕。

这个定理的另一种形式如下：

推论 设 $G$ 是具有 $n \geqslant 3$ 个结点的简单图。则当且仅当对每对不同结点 $x$ 与 $y$ ，存在连接结点 $x$ 与 $y$ 的二条基本链，除了 $x$ 与 $y$ 外它们没有公共结点时， $G$ 是 2 连通的。

这个推论是由K.Menger提出更一般定理的特殊情形。令 $G$ 是简单图， $h$ 是正整数，则当且仅当对于每对不同结点 $\pmb{x}$ 与 $\pmb{y}$ 存在连接 $\pmb{x}$ 与 $\pmb{y}$ 的 $h$ 个基本链，其中每对基本链只有公共结点 $\pmb{x}$ 与 $\pmb{y}$ 时， $\kappa(G) \geqslant h$ 。注意，对于 $h = 1$ ，上面叙述等价于：当且仅当每对不同结点由基本链连接时 $G$ 是连通的。

![](images/c07724fa9773a1030d852bd5bec279cb005edf7c9040212ffe6b648c9ed3602f.jpg)  
图11.20

![](images/da8f6c62468576e58725d85598056b7f706285804fd756aee1b8345bcdb409b0.jpg)  
图11.21

# 11.5 图的其它参数

图的色数是把图的结点集合划分为稳定集合的最小个数。回忆一下，图的结点集称为稳定的只要它没有两个结点是由边连接的。图 $G$ 的稳定性数 $\alpha (G)$ 是稳定集合中的最多结点个数。

例 令 $G$ 是图11.22中的图，则 $\{x, z\}$ 是在下述意义下的最大稳定集合，即不能利用加入结点使其扩大又保持是稳定的性质。 $\{u, v, y\}$ 是在下述意义下的最大稳定集合，即没有别的稳定集合包含的结点个数比它多。

![](images/e849baf38feb648eda6382acb733a30032ae5f3608cdac389806e1823598978d.jpg)  
图11.22

于是，在这种情形下， $a(G) = 3$

例八个皇后问题。Gauss提出并解决了下述问题，能否把8个皇后放置在 $8 \times 8$ 棋盘上，使得没有一个皇后被另一个皇后“吃掉”（即没有两个在同行、同列或对角线上）？令图 $G$ 的结点为棋盘上64个方块，当且仅当它们处于同行或同列或同条对角线上时两个方块由边连接。于是，当且仅当位于一个方块上的皇后可以被位于另一个方块上的皇后吃掉时两个方块由边连接。上面问题等价于在图 $G$ 中寻找8个结点的稳定集合。显然， $\alpha(G) \leqslant 8$ ，因为给定比8个多的正方形，它们之中的某两个位于同行（鸽笼原理），因而由一边连接。这样，问题化为是否 $\alpha(G) = 8$ 。这个问题的一个解如图11.23所示。已经知道这个问题，恰好存在92个解（有8个结点的稳定集合）。

设 $G = (X, E)$ 是简单图， $G$ 的补图 $\overline{G} = (X, \overline{E})$ 是有

![](images/ef54cb2291e55118153224b3850b74915f4999e209586eff1725c9ebefd15a99.jpg)  
图11.23

与 $G$ 同样的结点集合 $X$ 的简单图，在 $\overline{G}$ 中两结点当且仅当在 $G$ 中它们没有边连接时，有边连接。于是， $G$ 中的边数加上 $\overline{G}$ 中的边数等于 $\binom{n}{2}$ ，这里 $n = |X|$ 。

例画在图11.22中的简单图的补图如图11.24所示

![](images/57b1c94194c1a11aa0861f2ed7e77356a4af608126e6c71c7183098b5d54632d.jpg)  
图11.24

由稳定集合的定义得出，如果 $A$ 是 $G$ 的稳定集合，则 $A$ 在 $\overline{G}$ 中诱导一个完全子图（ $\overline{G}_A$ 是完全图）。在图 $G$ 中，集团集是使 $G_B$ 为完全集的最大的结点集合 $B$ （于是，对于 $B \subseteq C$ 且 $B \neq C$ 的每个结点集合 $C$ ， $G_C$ 不是完全图）。这样， $G$ 的最大稳定结点集是 $\overline{G}$ 的集团集，并且反之亦真。 $G$ 的集团数 $\omega(G)$ 是在 $G$ 的集团集中最大结点个数。由前面讨论可知，

$$
\alpha (G) = \omega (\overline {{G}}), \quad \omega (G) = \alpha (\overline {{G}}). \tag {11.5.1}
$$

例 在图11.22所示的简单图 $G$ 中， $\{u, v, y\}$ 与 $\{x, z\}$ 是最大稳定集合。在图11.24所示图 $G$ 的补图中， $\{u, v, y\}$ 和 $\{x, z\}$ 都是集团集。此外， $\{x, y\}$ 是 $G$ 的集团集，而 $\{x, y\}$ 是 $G$ 的最大稳定集。由此得到， $\alpha(G) = 3 = \omega(\overline{G})$ 和 $\omega(G) = 2 = \alpha(\overline{G})$ 。

在任何简单图 $G = (X, E)$ 的着色中，集团集的结点必须指定不同的颜色。由此得到

$$
\gamma (G) \geqslant \omega (G). \tag {11.5.2}
$$

对于图11.22所示简单图 $G$ ，(11.5.2)中的等式成立， $\gamma (G)$ 与 $\omega (G)$ 都等于2。另一方面，对于只有奇数长的基本圈所组成的简单图 $G$ ， $\gamma (G) = 3 > 2 = \omega (G)$

关于补图的色数概念是什么？设 $G = (X, E)$ 是简单图。 $G$ 的着色对应于把 $X$ 划分为稳定集合 $X_1, X_2, \dots, X_t$ 。由于 $X_i$ 是 $G$ 的稳定集合， $X_i$ 在 $G$ 的补图中诱导一个完全子图。对于简单图， $G$ 定义划分数 $\theta(G)$ 是使结点集合 $X$ 划分成 $t$ 个集合 $X_1, X_2, \dots, X_t$ ，且 $G_{X_1}, G_{X_2}, \dots, G_{X_t}$ 都是完全图的最小正整数。由此可见

$$
\gamma (G) = \theta (\overline {{G}}), \theta (G) = \gamma (\overline {{G}}). \tag {11.5.3}
$$

设 $G = (X, E)$ 是简单图，又设 $X_{1}, X_{2}, \cdots, X_{t}$ 是使 $G_{X_{1}}, G_{X_{2}}, \cdots, G_{X_{t}}$ 都为完全图的 $X$ 的划分。再令 $A$ 是稳定集合，则在任何 $X_{p}$ 中至多存在 $A$ 的一个结点，因此

$$
\theta (G) \geqslant a (G). \tag {11.5.4}
$$

例设 $G = (X,E)$ 是图11.25中的简单图，那么， $\{x,y,$ $z,u\}$ ， $\{v,w\}$ 和 $\{s\}$ 是把 $X$ 分成3个集合的划分，其中每个集合诱导出完全子图．而且，易见 $\theta (G) = 3$ ，由（11.5.4）可得， $\alpha (G)\leqslant 3$ ，由于 $\{y,w,s\}$ 是稳定集合，我们断定 $\alpha (G)$ $= 3$ ，因而对于该图 $G$ ，(11.5.4)中的等式成立，另一方面，如果 $G$ 是由奇数长 $2k + 1$ 的基本圈所组成的简单图，则 $\theta (G) = k$ $+1 > k = a(G)$

简单图称为三角形化的，如果对每个长比3大的基本圈存在一条边连接该圈上两个非相邻结点。例如，在图11.25中所示的图便是一个三角形化图。又如，任何树或林是三角形化的

图，因为这些树没有基本圈。显然，三角形化图的任何诱导子图是三角形化的（见练习61）。用下面方式可以得到一类三角形化图。令 $I_{1} = \{a_{1}, b_{1}\}, I_{2} = \{a_{2}, b_{2}\}, \dots, I_{n} = \{a_{n}, b_{n}\}$ 是直线上一组非空的闭区间。设 $G$ 是结点集合为 $\{I_{1}, I_{2}, \dots, I_{n}\}$ 的简单图，当且仅当 $I_{i} \cap I_{j} \neq \emptyset$ 时，才使得 $I_{i}$ 与 $I_{j}$ 是由边连接的，这样得到的图称为区间图。

![](images/828c9187d7d4cdb9442b89083ad9f41f613dd4d73cbcdd1905ec136113674231.jpg)  
图11.25

定理11.5.1 区间图是三角形化图。

证明设G是如上定义的区间图，令Ii1,Ii2,,Ii,，Ii是长为p>3的基本圈。假设对于k=1,2,…,p,有I∩I+2=0，（这里我们约定，如果γ>p，则I=r，其中r-j=p）则取k=1，我们可知，a<α<α或a<α<α.为确

定起见，令 $a_{i_1} < a_{i_2} < a_{i_3}$ 。现取 $k = 2$ ，我们得到 $a_{i_1} < a_{i_2} < a_{i_3} < a_{i_4}$ ，如果仿此继续下去，我们得到

$$
a _ {i _ {1}} <   a _ {i _ {2}} <   \dots <   a _ {p} <   a _ {i _ {1}}.
$$

于是 $a_{i_1} < a_{i_2}$ ，这是一个矛盾。用同样方法，可找到不等式 $a_{i_3} < a_{i_2} < a_{i_1}$ 导致

$$
a _ {i _ {1}} <   a _ {i _ {9}} <   a _ {i _ {p - 1}} <   \dots <   a _ {i _ {3}} <   a _ {i _ {2}} <   a _ {i _ {1}},
$$

这又给出一个矛盾，因此，对某个 $k = 1,2,\dots ,p$

$$
I _ {i _ {k}} \cap I _ {i _ {k + 2}} \neq \varnothing .
$$

于是，存在一边连接 $I_{i_k}$ 与 $I_{i_{k + 2}}$ ，所以， $G$ 是三角形化图.

现在我们集中精力证明C. Berge定理：对于三角形化图 $G$ ， $\gamma(G) \approx \omega(G)$ ，我们需要两个预备结果。

定理11.5.2 设 $G = (X, E)$ 是连通的简单图且存在关节集 $A$ ， $G_A$ 又是完全图。设子图 $G_{X-A}$ 的连通分图是 $G_1 = (X_1, E_1)$ ， $G_2 = (X_2, E_2), \ldots, G_t = (X_t, E_t)$ 。假设对于 $i = 1, 2, \ldots, t$ 有 $\gamma(G_A \cup X_i) = \omega(G_A \cup X_i)$ ，则 $\gamma(G) = \omega(G)$ 。

证明 设 $\omega(G) = k$ 。由于 $A$ 是关节集，对于 $i \neq j$ ，故 $X_{i}$ 的结点与 $X_{j}$ 的结点无边连接。于是， $G$ 的每个集团集是某个 $G_{A \cup X_{i}} (i = 1,2,\dots,t)$ 的集团集。因而在 1 到 $t$ 之间存在 $p$ ，使得 $\omega(G_{A \cup X_{p}}) = k$ 。由假设，我们也有 $k = \gamma (G_{A \cup X_{p}})$ ，再由假设我们得到，对于 $i = 1,2,\dots,t$

$$
\gamma (G _ {A \cup X _ {i}}) = \omega (G _ {A \cup X _ {i}}) \leqslant \omega (G) = k.
$$

因为 $G_{A}$ 是完全图，由定理11.1.4可知， $\gamma (G)\leqslant k$ ，由于 $\gamma (G)$ $\geqslant \omega (G) - k$ ，我们断定 $\gamma (G) = \omega (G)$

定理11.5.3 设 $G = (X, E)$ 是连通的三角形化简单图，又设 $A$ 是在下述意义下的最小关节集，即对于所有 $B \neq A$ 的 $B \subseteq A$ ， $B$ 不是关节集，则 $G_{\Delta}$ 是完全图。

证明 假设 $G_{A}$ 不是完全图，则存在 $A$ 中不同结点 $a$ 与 $b$ ，使 $a$ 与 $b$ 没有 $G$ 中的边连接。由于 $A$ 是关节集，图 $G_{X-A}$ 是不连通的。令 $G_{1}$ 与 $G_{2}$ 是 $G_{X-A}$ 的不同的连通分图。假定 $a$ 不与 $G_{1}$ 的任何结点相连接，若 $B = A - \{a\}$ ，图 $G_{X-B}$ 也是不连通的，这与 $A$ 是最小关节集的假定矛盾。于是， $a$ 至少与 $G_{1}$ 的一个结点相连接，同样 $b$ 也至少与 $G_{1}$ 的一个结点相连接。因为 $G_{1}$ 是连通的，存在连接 $a$ 与 $b$ 的基本链 $\mu_{1}$ ，其结点除了 $a$ 与 $b$ 外都是 $G_{1}$ 的结点。选取 $\mu_{1}$ ，使其长度最小。同样，我们可以选取连接 $a$ 与 $b$ 的基本链 $\mu_{2}$ ，其结点除了 $a$ 与 $b$ 外都是 $G_{2}$ 的结点并且有长度最小。那么， $\mu_{1}$ 接 $\mu_{2}$ 定义一条长至少为 4 的基本圈 $\mu$ （见图 11.26）。而且，由于 $\mu_{1}$ 与 $\mu_{2}$ 长度最小以及 $a$ 与 $b$ 不相连接。因此没有 $\mu$ 的两个非相邻结点由边连接，这与 $G$ 是三角形化图的假设相矛盾。这个矛盾是由于假设 $G_{A}$ 不是完全图产生的，故 $G_{A}$ 是完全图。

![](images/6765abaf529650067d245480e77e29b0e6ce11e1ee89083468ef83296f275cd4.jpg)  
图11.26

定理11.5.4 令 $G$ 是三角形化图，则 $\gamma(G) = \omega(G)$ 。

证明 我们对 $G$ 的结点数 $n$ 进行归纳证明. 易见, 如 $n \leqslant 3$ 定理为真. 现在令 $n > 3$ 并且假定对于少于 $n$ 个结点的三角形化简单图等式①成立, 我们可以取 $G$ 是连通图, 根据定理11.5.3, $G$ 有

关节集 $A$ ，使得 $G_{A}$ 为完全图，因为 $A$ 是关节集，图 $GX_{-A}$ 是不连通的并且有连通分图 $GX_{1},GX_{2},\dots ,GX_{t}$ ，其中 $\pmb {i}\geqslant 2$ ，对于 $i = 1,2,\dots ,t,$ 这些图 $GX_{i\cup A}$ 每一个都是结点数少于n的三角形化图．因而，由于归纳假设，对于 $i = 1,2,\dots ,t,\gamma (G_{x_i\cup A})$ $= \omega (GX_{i\cup A})$ .根据定理11.5.2， $\gamma (G) = \omega (G)$ .于是，由归纳法知定理为真.

推论 如果 $G$ 是区间图，则 $\gamma (G) = \omega (G)$

这个推论是定理11.5.1和11.5.4的直接结果。

对于三角形化简单图， $\alpha(G) = \theta(G)$ 亦真。这尽管难度大一点，仍可以用同样方法证明。另一方面，对于三角形化图 $G$ ，由 $\gamma(G) = \omega(G)$ 和下面的 L. Lovász 定理可推得 $\alpha(G) = \theta(G)$ 。若对于所有 $A \subseteq X, \gamma(G_A) = \omega(G_A)$ ，则称简单图 $G = (X, E)$ 为 $\gamma$ 完全的；又若对于所有 $A \subseteq X, \alpha(G_A) = \theta(G_A)$ ，则称简单图 $G = (X, E)$ 为 $\alpha$ 完全的。那么当且仅当 $G$ 是 $\alpha$ 完全的，它才是 $\gamma$ 完全的。现在假定 $G$ 是三角形化图，则对每一个 $A \subseteq X, G_A$ 亦是三角形化图。由此可得，对于三角形化图 $G, \alpha(G) = \theta(G)$ 。下面 C. Berge 猜想尚未解决，即当且仅当 $G$ 或 $G$ 没有任何诱导子图是仅由大于 3 的奇数长的基本圈所组成时，图 $G$ 是 $\gamma$ 完全的（等价于 $\alpha$ 完全的）。

# 练习

1. 举例说明如果没有假设各国家是连通区域，4种颜色对于地图着色是不够用的。  
2. 设 $G$ 是有 $n$ 个结点的简单图，何时 $\gamma(G) = n?$   
3. 求下面各图的色数。（见324页）  
4. 求立方体，八面体，十二面体的边图的色数

![](images/8c40398a2c1ffff1b853d066bdef837f71524aae759a25d123f39d27e5fa3806.jpg)

![](images/a3aa68e105d2affdeca8b220a0ad4f82b338278f9a22eef7874fea67cccf101b.jpg)

![](images/2e630798fb1a3743275ac2a34eb85b1fe3e778ddc199bc2118d8a171c49e5331.jpg)  
3题图

$$
\gamma (G) = \max  \left\{\gamma (G _ {1}), \gamma (G _ {2}), \dots , \gamma (G _ {k}) \right\}.
$$

5. 设 $G$ 是具有连通分图 $G_{1}, G_{2}, \cdots, G_{k}$ 的图. 证明  
6. 设 $G = (X, E)$ 和 $H = (X, F)$ 是简单图，其中 $F \subseteq E$ 证明 $\gamma(H) \leqslant \gamma(G)$ . 试举出 $F \neq E$ 但 $\gamma(H) = \gamma(G)$ 的一例.  
7. 试证明树是偶图.   
8. 试证明练习3中最左边的图是偶图  
8. 设 $x$ 与 $y$ 是图 $G$ 的两个结点。假设存在连接 $x$ 与 $y$ 奇数长的链 $\gamma_1$ 和连接 $x$ 与 $y$ 偶数长的链 $\gamma_2$ 。证明 $G$ 有奇数长的基本圈，其边是 $\gamma_1$ 与 $\gamma_2$ 的边的子集合。  
10. 证明下面的命题：恰有一个奇数长的基本圈的图色数是3。  
11. 利用11节所描述的算法，求练习3中的第二和第三图的色数  
12. 对于 $k = 3, 4, 5$ 和 6，试求图11.7中的图不同的 $k$ 着色的个数。  
13. 利用定理11.1.4求练习3中图的色数  
14. 利用定理11.1.4求下图的色数

![](images/72c49b0ace4a26e2edaf747341d85344e589e59ff7e737ad16487dd4311129ae.jpg)

16. 试求 $K^n$ 移去一边而得到图的色数.

16. 试求 $Kn$ 移去两个不相交的边而得到图的色数

17. 试证明如果图不是简单图，定理11.2.2不成立。

18. 如果 $G$ 是连通简单平面图, 利用定理 11.2.2 试给出 $\gamma(G) \leqslant 6$ 的简单的证明.

19. 设 $G$ 是由完全偶图加入一条新边而得到的简单图，证明 $G$ 不是偶图.

20. 证明对于 $n = 1, 2, 3, 4, K$ 是平面图.

21. 证明如果 $m \leqslant 2$ 或 $n \leqslant 2$ , $Km, n$ 是平面图.

22. 试举出简单平面图 $G$ 的例子, 使得 $\gamma(G) = 4$ 但 $G$ 不包含 $K_{4}$ 作为诱导子图.

23. 试找出关于简单平面图 $G, \gamma(G) \leqslant 4$ 的下面证明的错误：因为 $K_{\mathfrak{B}}$ 不是平面图， $G$ 不包含 $K_{\mathfrak{B}}$ 作为诱导子图，于是，不存在 $G$ 的5个结点，其中每对由边连接。因此， $\gamma(G) \leqslant 4$ 。

24 假设图10.14, 10.15, 10.16和10.17表示地图，试求各个地图的4着色.

25 一平面被有限条直线划分为许多区域。证明可以用2种颜色把这些区域着色，使得有公共边界的区域有不同的颜色。

28. 用圆代替直线重作练习25.

27. 设 $G = (X, E)$ 是简单图. 若对于每个结点 $\pmb{x}$ , $\gamma(G_{X - \{x\}})$ < $\gamma(G)$ , 则称 $G$ 为临界的（关于色数）. 对于临界图证明下列命题:

(a) 对于每个结点 $x, \gamma (G_{X - \{x\}}) = \gamma (G) - 1.$

(b) $G$ 是连通图.

$(c)G$ 的每个结点的度至少为 $\gamma (G) - 1$

(d) $G$ 没有是完全图的关节集.

(e) 具有 $\gamma(G) = m$ 的每个简单图 $G$ 存在诱导子图 $G'$ ，使 $\gamma(G') = m$ 并且 $G'$ 是临界的。

28. 设 $m \geqslant 3$ . 证明如果 $G$ 是每个结点的度至少为 $m - 1$ 的简单图, 那么 $G$ 有长至少为 $m$ 的基本圈. 利用练习27, 推导 $\gamma(G) = m$ 的简单图 $G$ 有长至少为 $m$ 的基本圈.

29*. 证明如果 $G$ 是没有关键点且每个结点的度至少为 3 的简单图, 则 $G$ 包含与 $K_{4}$ 同胚的部分图. (提示: 从最大长度为 $m$ 的基本圈开始, 利用练习 28, $m \geqslant 4$ , ) 利用练习 27 验证当 $m = 4$ 时的 Hadwiger 猜想.

$30^{*}$ . 举例说明使用练习28和29中所描述对于 $m = 4$ 的方法不可能证明 $m = 5$ 的 Hadwiger 猜想.

31. 验证图11.18中的图 $G_{1}, G_{2}, G_{3}$ 的连通度分别等于1、2和3.  
32. 试求图11.7中图的连通度  
33. 试求完全偶图 $Km.n$ 的连通度  
34. 如果 $G(X, E)$ 是连通简单图，证明

$$
\kappa (G) = 1 + \min  \kappa (G X - \{x \}),
$$

其中最小值是跑遍 $G$ 的所有结点 $\pmb{x}$ 取得

35. 试求树的连通度.   
36. 求立方体，八面体和十二面体的边图的连通度  
37. 图 $G$ 的边连通度 $\lambda(G)$ 等于最小边数 $p$ , 使得存在 $G$ 的 $p$ 条边, 移去它们剩下的图是不连通的或是只有单个结点的图. 求不连通的图的边连通度: 图11.7的图的边连通度和树的边连通度.  
38 设 $G$ 是简单图，又令 $d(G)$ 表示 $G$ 的结点最小的度。证明

$$
\kappa (G) \leqslant \lambda (G) \leqslant d (G).
$$

39. 试举出 $\kappa(G) = \lambda(G)$ 的简单图的例子  
40. 试举出 $\kappa(G) < \lambda(G)$ 的简单图的例子  
41. 试举出 $\kappa(G) < \lambda(G) < d(G)$ 的简单图的例子  
42. 试用练习38求 $\lambda(K_{\pi})$ 和 $\lambda(K_m, n)$   
43. 设 $G$ 是有 $n$ 个结点的简单图，其每个结点的度是 $k$ （此时， $G$ 叫做 $k$ 度的正则图）。为保证 $G$ 是连通的， $k$ 必须是多大？为保证 $G$ 是 2 连通的， $k$ 必须是多大？  
44. 对于有 8 个结点，度分别为 3, 3, 4, 5, 5, 6, 6, 6 的简单图，利用定理 11.4.4 得出关于连通度可能达到的最好结论。  
45. 试确定下图的块.

![](images/e076bcc9491242d67c2227809e39dcd81be4dab40076a4de18df8b5953d28c5b.jpg)

46. 设 $G$ 是图。证明 $G$ 的边是块边（见10.7节）当且仅当它是 $G$ 的块为 $K_{2}$ 的唯一的边。  
47. 设 $G$ 是简单图, 证明当且仅当对于每个结点 $x$ 和每条边 $e$ 存在基本圈, 包含结点 $x$ 和边 $e$ 时, $G$ 是 2 连通的.  
48. 设 $G$ 是结点度都不为 0 的简单图, 证明当且仅当对于每对边 $e_1, e_2$ , 存在包含边 $e_1$ 和 $e_2$ 的基本圈时, $G$ 是 2 连通的.  
49. 设 $G$ 是至少有 2 个结点的连通图. 证明 $G$ 至少有 2 个不是关节点的结点. （提示：选取由 $G$ 中最长的基本链所连接的两个不同的结点）.  
50. 求立方体，八面体，十二面体的边图的稳定性数，  
51. 求练习46中图的稳定性数  
52. 试求与11.5节给出八个皇后问题的解不同的解  
53. 设 $G$ 是简单图, 证明或 $G$ 或它的补图 $\overline{G}$ (或两者) 一定是连通的.  
54. 说明对于简单偶图 $G$ ， $\gamma(G) = \omega(G)$ .  
55. 求立方体，八面体，十二面体的边图的集团数。  
56. $\theta (G)$ 等于使 $G$ 的结点集合可划分成t个集团集的最小整数t吗？  
57. 设 $G$ 是简单偶图，证明当且仅当 $G$ 没有圈（ $G$ 是林）时， $G$ 是三角形化图。  
58. 设 $G$ 是有 $\pmb{n}$ 个结点的简单图. 证明

$$
\gamma (G) a (G) \geqslant n
$$

和

$$
\gamma (G) + a (G) \leqslant n + 1.
$$

59. 证明如果 $G$ 是树，则 $\theta(G) = \alpha(G)$ .  
$80^{*}$ 证明如果 $G$ 是简单偶图，则 $\theta (G) = \alpha (G)$   
61. 证明三角形化图的诱导子图是三角形化图  
62. 举例说明定理11.5.3的逆不成立  
83*. 证明关于简单图 $G$ 的稳定性数 $\alpha(G)$ 和划分数 $\theta(G)$ 与定理 11.5.2 类似的定理.  
84. 利用练习63，证明对于三角形化简单图 $G, \theta(G) = \alpha(G)$ （A，Hajnal和J.Suranyi定理）.  
85. 验证具有长是大于3的奇数的基本圈的图既不是γ完全图也不是α-完全图.

# 第十二章 优化问题

数学优化涉及到数学模型的建立、模型优化解（在某种事先给定的意义下）的研究以及当它们存在时优化解的计算和算法的分析。本章我们讨论几个优化问题的实例，它们不需要很多实际的背景。所讨论的问题不具有代表性，不过这些优化问题具有某些组合意义。我们并不去研究数学优化的理论。

我们在前些章节已经考察过几种优化问题。在第八章的配偶问题就是确定男女的最优匹配的问题（或申请者和职位的最优匹配问题）——即是建立一个匹配使配偶的个数最大的问题。10.4节的极小连接问题就是优化问题。这里“优化”意指“花费最小成本”。同样，在10.5节中讨论过求一个图两结点间的最短链问题也是优化问题；确定图的色数以及在第十一章中讨论过某些其它的图参数也都是优化问题。在色数问题中，我们设法确定最小的 $k$ ，使得图可以 $k$ 着色。

虽然线性规划是数学优化的重要部分，但这里我们不去讨论它。因为要给出它应有的完整的论述，需要的篇幅太大。不过，我们在12.3节中研究重要的Hitchcock运输问题，这个问题就是一个可使用特别的方法的线性规划问题。

本章我们假定已经具备第十章讨论过的图论的某些知识。在12.4和12.5节里，我们将研究两个问题，8.1节讨论过的配偶问题是其中一个问题的特殊情况。

# 12.1 稳定分配

我们考察下面问题。一个工厂为 $n$ 种工作要雇用几个人，每个人根据各人对所分配的工作的爱好来评价各科工作。我们假设不存在任何其它约束。于是，如果一个人对两种或多种工作都是不感兴趣，我们仍然认为他对这些工作爱好有所不同，他说出每个人对各种工作的爱好程度就有高低之分，因此，每个人按次序1，2，…， $n$ 评价各种工作。同样，管理人员根据各人对执行各种工作所表现的爱好评价每个人。为了叙述简便，我们便说工作评价各个人，各种工作按1，2，…， $n$ 次序评价每个人。这里，我们再强调一下，不存在任何约束。当然，分配 $n$ 个人去完成 $n$ 种工作有 $n!$ 种方法。如果有两个人 $a$ 和 $b$ 以及两种工作 $A$ 和 $B$ ，分配 $a$ 去完成工作 $A$ ， $b$ 去完成工作 $B$ ，但是 $b$ 评价 $A$ 比 $B$ 高， $A$ 评价 $b$ 比 $a$ 高，这时我们则称这种分配是不稳定的。在不稳定分配中， $b$ 和 $A$ 会各自独立地去改变它们的分配以使 $A$ 分配给 $b$ ，因为两者都认为变化会带来改进。这样，原来的分配在下述意义下来说是“不稳定的”，即人员与工作由于某种意义上的共同利益而一致行动推翻这种分配。稳定分配就是非不稳定分配。于是，产生的第一个问题是：稳定分配是否一定存在？

我们可以利用完全偶图 $K_{n,n}$ 建立起该问题的数学模型。我们取 $n$ 个结点的集合 $X$ 代表人，取 $n$ 个结点的集合 $Y$ 代表工作，并

且用边连接每个人到每种工作就得到一个完全偶图。对应于每条边 $[x, y]$ ，存在一个有序数对 $i$ ， $j$ ，其中 $i$ 表示 $x$ 对 $y$ 的评价， $j$ 表示 $y$ 对 $x$ 的评价。人对于工作的 $n!$ 种可能的分配对应着偶图中 $n$ 条边的 $n!$ 种匹配。

利用优先级矩阵可以得到更方便的模型。这是一个 $n$ 行 $n$ 列的 $n \times n$ 矩阵，行对应人和列对应工作。在第 $r$ 行和第 $s$ 列所对应的位置上，我们放置一个数对 $i$ ， $j$ ，其中 $i$ 表示人 $r$ 对工作 $s$ 的评价， $j$ 表示工作 $s$ 对人 $r$ 的评价。安排 $n$ 个人去做 $n$ 种工作的分配对应矩阵的 $n$ 个位置的集合 $S$ ，这些位置位于矩阵中不同的行和不同的列上。

例. 设有两个人，比方说 $a$ 和 $b$ ，有两种工作，比如说 $A$ 和 $B$ 。又设优先级矩阵是

$$
\left. \begin{array}{l l} a & A \\ b & 1, 2 \\ c & 2, 1 \end{array} \right\}, \tag {12.1.1}
$$

存在两种可能人对工作的分配： $a$ 去做 $A$ ， $b$ 去做 $B$ 与 $a$ 去做 $B$ $b$ 去做 $A$ 。容易看出，第一种是稳定的，第二种是不稳定的，因为 $b$ 喜欢 $B$ 胜过于 $A$ ，而 $B$ 喜欢 $b$ 胜过 $a$ 。

例 设有三个人，比方说 $a, b$ 和 $c$ ，有三种工作，比方说 $A, B$ 和 $C$ 。优先级矩阵是

$$
\begin{array}{c c c} A & B & C \\ a & 1, 3 & 2, 2 \\ b & 3, 1 & 1, 3 \\ c & 2, 2 & 3, 1 \end{array} \tag {12.1.2}
$$

有 $3! = 6$ 种安排人员去做各种工作的分配。一个可能的分配是分配 $a$ 去作 $A$ ， $b$ 去作 $B$ 和 $c$ 去作 $C$ 。由于每个人在这种分配中得到他的第一种选择，这种分配是稳定的，即使每种工作得到它的最后选择。另一种稳定分配是把每种工作给出第一

种选择.

现在我们证明，稳定分配必定存在。

定理12.1.1 安排人员去做各项工作的稳定分配是存在的。

证明 这个证明定义了求解人对于工作的稳定分配的迭代过程。这个算法称为延缓接受算法，其理由是显然的。在这个算法中，各人提出工作申请，或者被拒绝或者等候录取（延缓接受）。

每个人选择他喜爱的工作。

I 每项有人选择的工作，从这些选择它的人中挑选出最合意的，要他等候录取，同时拒绝其他的申请人。

每个被拒绝的人再选取他评价为较次的工作。

每项有人选择的工作再从第Ⅰ步中选择它的那些人以及在前一步中等候录取它的人中挑选出它最合意的人，要他等候录取，同时拒绝其他的申请人。

我们重复Ⅰ和Ⅳ直到没有被拒绝的人和每种工作都有人选择为止。当达到这一步时，每项工作恰好有一人等待（因为人员正好和工作一样多，每个人正等待一种工作），并且这时我们给每个人分配要他等待的工作。现在我们证明这种分配是稳定的。假设 $a$ 分配工作 $A$ ， $b$ 分配工作 $B$ ，但是 $b$ 喜爱 $A$ 胜过于 $B$ 。这时由算法的某步可得， $b$ 选择工作 $A$ 但是被 $A$ 拒绝，因为 $A$ 喜爱别人胜于 $b$ 。又因，工作 $A$ 喜爱被分配的人 $a$ 胜于 $b$ 。因而，这种分配是稳定的。定理证毕。

例 我们把延缓接受算法应用于优先级矩阵（12.1.2）的情形。在算法的第一步中， $a$ 选取 $A$ ， $b$ 选取 $B$ 和 $c$ 选取 $C$ 。不存在拒绝，因为每项工作恰好接受一次申请，所以，该算法在第一步之后结束，并且所得到的分配（ $a$ 做 $A$ ， $b$ 做 $B$ ， $c$ 做 $C$ ）是稳定的。

例 我们把延缓算法应用于优先级矩阵是

$$
\left( \begin{array}{l l l l} A & B & C & D \\ 1. 2 & 2, 1 & 3, 2 & 4. 1 \\ 2. 4 & 1, 2 & 3, 1 & 4. 2 \\ 2. 1 & 3, 3 & 4, 3 & 1. 4 \\ 1. 3 & 4, 4 & 3. 4 & 2. 3 \end{array} \right) \tag {12.1.3}
$$

的情形.

该算法的结果如下：

（i） $a$ 选取 $A, b$ 选取 $B, c$ 选取 $D, d$ 选取 $A, A$ 拒绝 $d$ .  
(ii) $d$ 选取 $D_{1}$ $D$ 拒绝 $\pmb{c}_{\bullet}$   
(iii) $c$ 选取 $A_{3}$ $A$ 拒绝 $\pmb{a}_{\bullet}$   
(iv) $a$ 选取 $B_{1}$ $B$ 拒绝 $\pmb{b}_{\bullet}$   
(v) $b$ 选取 $A; A$ 拒绝 $b$ .   
(iv) $b$ 选取 $C_{\bullet}$

现在每项工作恰好有一个人在等候录取，因而把 $a$ 分配给 $B$ ， $b$ 分配给 $C$ ， $c$ 分配给 $A$ ， $d$ 分配给 $D$ 是稳定分配.

如果在延缓接受算法中，我们交换人员与工作这两个项目并且由这些工作根据它们的喜爱选择人，我们可能得到与由上述的算法不同的稳定分配。以级矩阵（12.1.2）为例，前一种情形我们得到的分配是 $a$ 做 $A, b$ 做 $B, c$ 做 $C$ ，后一种情形得到的分配 $a$ 做 $c$ ， $b$ 做 $A$ ， $c$ 做 $B$ 。

如果在一个稳定分配中，每个人对他所安排的工作的评价不低于其他稳定分配中他对他安排的工作的评价，则称这个稳定分配是对人的最优分配。同样地可以定义对工作的最优分配。可见，如果存在一种分配是对人最优的，它必定是唯一的。这是因为在评价中没有任何约束。如果存在两种不同的对人最优分配，则至少有一人在一种分配比在另一种分配下会境

况更好。因此，其中有一种分配不是对人最优的。同样论述可应用于对工作的最优分配。

定理12.1.2 由延缓接受算法得到的稳定分配是对人最优的。

证明 如果存在某种稳定分配把某项工作分配给某人，则称这项工作对某人是可行的。我们将利用归纳法证明，在由延缓接受算法得到的分配中，没有任何人被对他是可行的工作所拒绝。这就意味着，由延缓接受算法得到的这种分配对每个人所安排的工作比他在任何稳定分配中可能得到的工作要好。于是，这样就会证明了这个定理。

作为归纳的基础，首先证明在算法第一步结束时没有任何人被对他是可行的工作所拒绝。假设 $a$ ， $b$ 都选取 $A$ ，（于是， $A$ 被 $a$ 与 $b$ 评价为第一），并且 $A$ 拒绝 $a$ 而有利于 $b$ ，则 $A$ 对于 $a$ 是不可行的。否则，就有稳定分配，其中 $A$ 分配给 $a$ ，把其他工作分配给 $b$ ，比方说 $B$ 。但是，因为 $A$ 是 $b$ 的第一个选择，我们知道 $b$ 喜爱 $A$ 胜于 $B$ 和 $A$ 喜爱 $b$ 胜于 $a$ 。因此，这个分配不是稳定的。

令 $k \geqslant 1$ ，假设在算法的第 $k$ 步结束时没有人被对他是可行的工作所拒绝。假定在第 $k + 1$ 步 $A$ 拒绝 $a$ 而有利于 $b$ 。我们需要证明 $A$ 对于 $a$ 不是可行的。由算法可得， $b$ 喜爱 $A$ 胜于除了在前面 $k$ 步拒绝他的那些工作以外所有其余工作。由归纳假设，那些工作对 $b$ 是不可行的，因而不存在稳定分配，其中 $b$ 被分配给这些工作的一项。假定存在稳定分配，其中 $a$ 分配 $A$ 。在这样的分配中， $b$ 分配某项工作，比方说 $B$ ，它是比 $A$ 对他还不利。但是另一方面， $b$ 喜爱 $A$ ，又由于在 $k + 1$ 步 $A$ 拒绝 $a$ 而有利于 $b$ ，则 $A$ 喜爱 $b$ 。于是，这个分配是不稳定的，产生矛盾。因此， $A$ 对于 $a$ 是不可行的。所以由归纳可得，在延缓接受算法中，没有人被对他是可行的工作所拒绝。定理证毕。

由此可知，如果在延缓接受算法中我们交换人员与工作这两项，我们便得到对工作最优的稳定分配。

我们以讨论一个类似问题来结束本节，对这个问题不再保证稳定分配的存在性。假定有偶数个女孩，希望配对成为同房的伙伴。每个女孩对别人喜欢程度都有个次序，并且假定没有其他约束。配对集合称为是稳定的，只要不在同房间的任何两个女孩，她们中的每一个喜爱另一个不胜于她实际同房间的人。稳定配对集是否常常存在呢？考察4个女孩 $a, b, c$ 和 $d$ 的情形，其中 $a$ 评价 $b$ 为第一， $b$ 评价 $c$ 为第一， $c$ 评价 $a$ 为第一，并且 $a, b$ 和 $c$ 中每个都把 $d$ 评价为最末一个。那么不论其他如何评价，不可能存在任何稳定配对集合。例如，假定 $a$ 和 $d$ 是同房间的人，则 $b$ 和 $c$ 也是同房间的人。但是， $c$ 喜爱 $a$ 胜于 $b$ ，又因为 $a$ 对 $d$ 评价最差， $a$ 喜爱 $c$ 胜于 $d$ 。于是，这种配对集合是不稳定的。如果 $b$ 和 $d$ 或 $c$ 和 $d$ 是同房间的人，也可以得到同样结论。因此，稳定的配对集合不存在。

# 12.2 核心，分配

我们考虑下面经商问题。有 $n$ 个商人，每个人进入市场，出售一种不可分的货物（例如，轿车或房子）。为简单起见，我们假定商人经营货物绝不多于一种，除非假定这种货物可免费从一个商人转让给另一个商人。每个商人都对带入市场的（包括他自己的） $n$ 种货物的喜爱的程度有一个顺序。市场活动的结果是根据商人喜爱使这些货物的所有权重新分配。这种 $n$ 种货物的重新分配将称为一个分配。一个分配将称为核心分配，如果不存在少于 $n$ 个商人的集合 $S$ ，使得对于 $S$ 中的商人，还有可能按照比给定的分配对每个人有更好的方式在他们之中

进行交易。

例 设有五个商人 $T_{1}, T_{2}, T_{3}, T_{4}, T_{5}$ 。每个商人对各人的货物的喜爱程度由下面的矩阵决定：

$$
\begin{array}{l} T _ {1} \quad 4 \quad 3 \quad 1 \quad 2 \quad 5 \\ T _ {2} \quad 4 \quad 3 \quad 1 \quad 2 \quad 5 \\ T _ {3} \quad 4 \quad 3 \quad 5 \quad 1 \quad 2 \\ T _ {4} \quad 1 \quad 4 \quad 3 \quad 5 \quad 2 \\ T _ {5} \quad 4 \quad 5 \quad 2 \quad 1 \quad 3 \end{array} , \tag {12.2.1}
$$

这个矩阵的意义解释如下：第一行表明商人 $T_{1}$ 的喜爱顺序。这样， $T_{1}$ 估价 $T_{3}$ 的货物为最高，然后是按 $T_{4}, T_{2}, T_{1}$ 和 $T_{5}$ 的这个次序估价各货物。矩阵其余各行的意义是类似的。

![](images/70754474ff101014599d92d9e89f339e7b5f25e3e3cd91533d3b86f8ff3763f4.jpg)

![](images/55f6410c42263410f81c5f6cb22aca1f7dbfd01209c9aad6dc51aa7f621099fd.jpg)  
图12.1

图12.1中表示一种分配。这个图的意义解释如下： $T_{1}$ 得到 $T_{2}$ 的货物， $T_{2}$ 得到 $T_{3}$ 的货物， $T_{3}$ 得到 $T_{1}$ 的货物。 $T_{4}$ 与 $T_{5}$ 的货物互相交换。这个分配不是核心分配，因为 $T_{1}$ 和 $T_{4}$ 可以交换，结果 $T_{1}$ 得到 $T_{4}$ 的货物，他对这种货物的估价高于 $T_{2}$ 的货物，同时 $T_{4}$ 得到 $T_{1}$ 的货物，他对这种货物的估价高于 $T_{5}$ 的货物。反之，可以证明由图12.2所确定的分配是核心分配。

定理12.2.1 核心分配必定存在。

证明 设商人的集合是 $\{T_{1}, T_{2}, \dots, T_{n}\}$ 。考察结点

![](images/c0da5ba193aa13195f52924e26711b7c09724ac6a41d90823449f892554ce4d2.jpg)

![](images/fa0ffbd4ce24b4f16cd4e6fb7c6f0577072832dbf85ebc46aeea35d53bdf594e.jpg)

![](images/dd29f87771a2362ad7858a4827a521960a73f9d57e54fa3045c2ac91859fc079.jpg)  
图12.2

集合是 $N = \{T_{1}, T_{2}, \dots, T_{n}\}$ 的有向图 $D_{1}$ ，并且这里从 $T_{i}$ 到 $T_{j}$ 有弧当且仅当 $T_{i}$ 喜爱 $T_{j}$ 的货物胜于所有其余货物。由于商人可能喜爱他自己的货物胜于其它的货物，故存在其初始结点和终止结点相重合的一些弧。有向图 $D_{1}$ 的每个结点出度等于1。因此不难看到（见第十章练习54），有向图 $D_{1}$ 至少有一个基本回路。这个回路可以退化为只有一个结点。设 $N_{1}$ 是 $D_{1}$ 的一个基本回路的结点集合。设 $D_{2}$ 是结点集合为 $N - N_{1}$ 的有向图，并且其中存在从 $T_{i}$ 到 $T_{j}$ 的弧当且仅当 $T_{i}$ 喜爱 $T_{j}$ 的货物胜于所有在 $N - N_{1}$ 中其他商人的货物。 $D_{2}$ 的每个结点的出度等于1，因此 $D_{2}$ 有基本回路。设 $N_{2}$ 是 $D_{2}$ 的一个基本回路的结点集合。由于 $N_{2}$ 是 $N - N_{1}$ 的子集， $N_{1}$ 与 $N_{2}$ 不相交。现在我们构造有向图 $D_{3}$ ，其结点集合是 $N - N_{1} - N_{2}$ ，并且存在从 $T_{i}$ 到 $T_{j}$ 的弧当且仅当 $T_{i}$ 喜爱 $T_{j}$ 的货物胜于所有在 $N - N_{1} - N_{2}$ 中其他商人的货物。设 $N_{3}$ 是 $D_{3}$ 的一个基本回路的结点集合，则集合 $N_{1}, N_{2}$ 和 $N_{3}$ 两两不相交。我们仿此继续构造有向图 $D_{1}, D_{2}, \dots, D_{p}$ 和相应的结点集合 $N_{1}, N_{2}, \dots, N_{p}$ ，这里 $N_{1}, N_{2}, \dots, N_{p}$ 是商人集合 $N$ 的划分。对于 $i = 1, 2, \dots, p$ ， $N_{i}$ 是 $D_{i}$ 的一个基本回路

$$
T _ {1} ^ {i}, T _ {2} ^ {i}, \dots T _ {k _ {i}} ^ {i}, T _ {1} ^ {i}
$$

的结点集合。考察由这 $p$ 个基本回路所确定的分配。于是，对

于每个 $i = 1, 2, \dots, p, T_1^i$ 得到 $T_2^i$ 的货物， $T_2^i$ 得到 $T_3^i$ 的货物，…，和 $T_{k_i}^i$ 得到 $T_1^i$ 的货物。我们断言这个分配是核心分配。

设S是少于n个商人的集合.令j是从1到p使得S∩Nj≠0的第一个整数，则

$$
S \subseteq N _ {j} \cup N _ {j + 1} \cup \dots \cup N _ {p} = N - N _ {1} - N _ {2} - \dots - N _ {j - 1}.
$$

于是， $S$ 是由有向图 $D_{j}$ 的某些结点组成。设 $T_{s}$ 是在 $S \cap N_{j}$ 中的任一商人。那么在上面定义的分配中， $T_{s}$ 得到的货物，按他的估价其价值高于在 $N - N_{1} - N_{2} - \dots - N_{j-1}$ 的商人的各种货物，因而也高于 $S$ 的商人各种货物。因此，由于在 $S$ 的成员中交换， $T_{s}$ 不可能改善他的结果。现在由定义可知，上面定义的分配是核心分配。定理证毕。

如果有向图 $D_{1}$ 是由两两不相交基本回路组成，则由前面证明中的算法所定义的分配已由 $D_{1}$ 确定，并且每个商人得到他估价最高的货物。

例 考察由矩阵（12.2.1）所确定的经商问题。有向图 $D_{1}$ 如图12.3所示，有向图 $D_{1}$ 恰有一个基本回路，这个回路的结点集合是 $\{T_{1}, T_{5}, T_{4}\}$ 。有向图 $D_{2}$ 正如图12.4所示。由于 $D_{2}$ 是由不相交基本回路组成，于是由定理的算法导出所求的核心分配并且如图12.2所示。

![](images/97f5c04ce249e0a4f7d6aa4e7fe7f48aa611b7f97a3ffa2f5016a7a57e29753f.jpg)  
图12.3

![](images/8ae3dd32dbd66c8eaf502f728059d3b65606b04d7dd25b2a2efdcece60ecb52e.jpg)

![](images/d8397f332e624f7d8c27361fad8a4644625f298bf6cd88eca0e553c99fbe55d7.jpg)  
图12.4

# 12.3 Hitchcock运输问题

1941年，F.L.Hitchcock提出如下的运输问题。在 $m$ 个补给仓库处，分别有供给物品 $a_{1}, a_{2}, \cdots, a_{n}$ 单位。这种物品是要分发给 $n$ 个消费仓库，它们需要量分别为 $b_{1}, b_{2}, \cdots, b_{n}$ 单位。从第 $i$ 个补给仓库到第 $j$ 个消费仓库运输一个单位的物品，其成本是 $C_{ij}$ 元。假定总补给量等于总需求量，即是

$$
a _ {1} + a _ {2} + \dots + a _ {m} = b _ {1} + b _ {2} + \dots + b _ {n}. \tag {12.3.1}
$$

根据这种问题的性质，数 $a_{1}, a_{2}, \cdots, a_{m}, b_{1}, b_{2}, \cdots, b_{n}$ 可以取作非负整数。我们只假定它们是非负实数，并且在(12.3.1)中的和都是正的。对于 $i = 1, 2, \cdots, m$ 和 $j = 1, 2, \cdots, n$ ，令 $x_{ij}$ 表示从第 $i$ 个补给仓库运送到第 $j$ 个消费仓库的物品的数量。为了满足供销的需要， $x_{ij}$ 必须满足下面条件：

$$
x _ {i j} \geqslant 0 (i = 1, 2, \dots , m, j = 1, 2, \dots , n), \tag {12.3.2}
$$

$$
x _ {i 1} + x _ {i 2} + \dots + x _ {i n} = a _ {i} (i = 1, 2, \dots , m), \tag {12.3.3}
$$

$$
x _ {1} j + x _ {2} j + \dots + x _ {m} j = b _ {j} (j = 1, 2, \dots , n), \tag {12.3.4}
$$

方程（12.3.3）等价于从第 $i$ 个补给仓库运出的物品的总数应等于 $a_{i}$ 。方程（12.3.4）等价于运到第 $j$ 个消费仓库的物品的总数应等于 $bj$ 。不等式（12.3.2）是问题固有的。我们可以把运送量 $x_{i}j$ 排列成形如 $m \times n$ 阵列或矩阵。

$$
X = \left( \begin{array}{c c c c} x _ {1 1} & x _ {1 2} & \dots & x _ {1 n} \\ x _ {2 1} & x _ {2 2} & \dots & x _ {2 n} \\ \vdots & & & \\ x _ {m 1} & x _ {m 2} & \dots & x _ {m n} \end{array} \right). \tag {12.3.5}
$$

如果（12.3.2）～（12.3.4）被满足，则这样的矩阵称为运输计划。用矩阵术语来说，（12.3.3）是说矩阵第 $i$ 行各数之和等于 $a_{i}$ ，而（12.3.4）是说矩阵第 $j$ 列的各数之和等于 $b_{j}$ 。矩阵（12.3.5）简记为 $X = [x_{ij}]$ ，我们可以把运输成本排列成形如下面的 $m \times n$ 矩阵

$$
C = \left( \begin{array}{c c c c} c _ {1 1} & c _ {1 2} & \dots & c _ {1 n} \\ c _ {2 1} & c _ {2 2} & \dots & c _ {2 n} \\ \vdots & & & \\ c _ {m 1} & c _ {m 2} & \dots & c _ {m n} \end{array} \right), \tag {12.3.6}
$$

并简记为 $C = [c_{ij}]$ 。从第 $i$ 个补给仓库到第 $j$ 个消费仓库运送 $x_{ij}$ 单位物品，其成本为 $c_{ij} x_{ij}$ ，整个运输计划（12.3.5）的成本是

$$
C * X = \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {n} c _ {i j} x _ {i j}. \tag {12.3.7}
$$

所谓运输问题便是求运输计划 $X$ ，使得由（12.3.7）所确定的成本尽可能小。

定理12.3.1 运输计划必存在。

证明 令 $d = a_{1} + a_{2} + \dots +a_{m}$ ，由（12.3.1），也有 $d = b_{1} + b_{2} + \dots +b_{n}$ 定义 $X = [x_{i},j]$ 如下：

$$
x _ {i j} = \frac {a _ {i} b _ {j}}{d ^ {i}} \quad (i = 1, 2, \dots , m, j = 1, 2, \dots ,
$$

n）

因为 $d > 0$ 和 $a_1, a_2, \dots, a_m, b_1, b_2, \dots, b_n$ 是非负数，(12.3.2) 满足。此外，对于 $i = 1, 2, \dots, m,$

$$
x _ {i 1} + x _ {i 2} + \dots + x _ {i n} = \frac {a _ {i} b _ {1}}{d ^ {1}} + \frac {a _ {i} b _ {2}}{d ^ {2}} + \dots + \frac {a _ {i} b _ {n}}{d ^ {n}}
$$

$$
\begin{array}{l} = \frac {a _ {1}}{d} (b _ {1} + b _ {2} + \dots + b _ {n}) \\ = \frac {a _ {i}}{d} * d = a _ {i}; \\ \end{array}
$$

对于 $j = 1,2,\dots n,$

$$
\begin{array}{l} x _ {1} j + x _ {2} j + \dots + x _ {m} j = \frac {a _ {1} b j}{d ^ {2}} + \frac {a _ {2} b j}{d ^ {2}} + \dots + \frac {a _ {m} b j}{d ^ {2}} \\ = \left(a _ {1} + a _ {2} + \dots + a _ {m}\right) \frac {b _ {j}}{d} \\ = d ^ {l} \cdot \frac {b _ {j}}{d ^ {l}} = b _ {j} \\ \end{array}
$$

因而，（12.3.3）和（12.3.4）都满足，故 $X$ 是运输计划。

设 $X$ 是由（12.3.5）给出的运输计划。我们可以把 $X$ 与由 $BG(X)$ 表示的偶图连系起来如下，并且称 $BG(X)$ 为运输计划 $X$ 的偶图。我们取结点 $s_1, s_2, \cdots, s_m$ 分别对应于各个补给仓库，取结点 $t_1, t_2, \cdots, t_n$ 分别对应于各个消费仓库。当 $x_{ij} = 0$ 时，我们用边连接 $s_i$ 和 $t_j$ 。换句话说，对于第 $i$ 个补给与第 $j$ 个消费仓库的结点由边连接，只要在运输计划中从第 $i$ 个补给仓库运送到第 $j$ 个消费仓库的物品的数量不为零。运输计划 $X$ 的图可以从完全偶图 $K_{m,n}$ 中删除那些运送量为零的对应边而得到。我们用运输计划的偶图作为主要理论方法。

给定运输计划 $X = \{x_{ij}\}$ ， $BG(X)$ 的一个基本圈对应于 $X$ 中为按象棋车的走步连接起来的非零运送量的集合 $S$ 。为了使这个圈是基本的， $S$ 由每行和每列的或零个或两个运送量所组成。图12.5中给出一例，基本圈的边对应于画圆圈的运送量。

![](images/a677732561400d2467927652bc4803d525a153bd8fe09ac1f46c59a3a7d2a1b1.jpg)

![](images/ca1174e8c4cd42a35a40a04d21a78d3b5591bc1a94342ff496616686833a57e6.jpg)  
图12.5

定理12.3.2 设 $X = [x_{ij}]$ 是使 $BG(X)$ 有基本圈的运输计划，则存在一个运输计划 $Y$ ，使得 $BG(Y)$ 没有圈并且其成本不大于运输计划 $X$ 的成本：

$$
C \cdot Y \leqslant C \cdot X.
$$

证明 假设 $BG(X)$ 包含基本圈 $\gamma$ 。圈 $\gamma$ 必有偶数长（见定理（11.1.2）。令对应于这圈的边的运送量是

$$
\begin{array}{l} x _ {i _ {1} i _ {2}}, \quad x _ {i _ {1}} j _ {2}, \quad x _ {i _ {2}} j _ {2}, \quad x _ {i _ {2}} j _ {3}, \dots , \quad x _ {i _ {k}} j _ {k}, \\ x _ {i _ {k}} j _ {1} \cdot \\ \end{array}
$$

这里 $i_{1}, i_{2}, \cdots, i_{k}$ 和 $j_{1}, j_{2}, \cdots, j_{k}$ 都是不同的。由 $X$ 的偶图的定义，这些运送量都是正的。对应于这些运送量的单位运输成本是

$$
\begin{array}{l} c _ {i _ {1} j _ {1}}, c _ {i _ {1} j _ {2}}, c _ {i _ {2} j _ {2}}, \dots , c _ {i _ {k} j _ {k}}, \\ c _ {i _ {k}} j _ {1} \cdot \\ \end{array}
$$

令 $p = c_{i_1}j_1 + c_{i_2}j_2 + \dots +c_{i_k}j_k$ 和 $q = c_{i_1}j_2 + c_{i_2}j_3 + \dots$ $+c_{i_k}j_1$ 。为确定起见，假定 $q\geqslant p$ 。令 $\alpha$ 是 $x_{i_1}j_2,x_{i_2}j_3,\dots ,$ $x_{i_k}j_1$ 中的最小数。令 $Z$ 是由 $X$ 向数 $x_{i_1}j_1$ ， $x_{i_2}j_2$ ，…， $x_{i_k}j_k$ 加上 $\alpha$ 和从数 $x_{i_1}j_2$ ， $x_{i_2}j_3$ ，…， $x_{i_k}j_1$ 减去 $\alpha$ 而得到 $m\times n$ 矩阵。对于 $k = 3$ 我们用（12.3.8）来说明。

$$
Z = \left( \begin{array}{c c c} x _ {i 1} j _ {1} + \alpha & x _ {i 1} j _ {2} - \alpha & \\ & x _ {i 2} j _ {2} + \alpha & x _ {i 2} j _ {3} - \alpha \\ x _ {i 3} j _ {1} - \alpha & & x _ {i 3} j _ {3} + \alpha \end{array} \right). \tag {12.3.8}
$$

因为 $\alpha$ 是数 $x_{i1j_2}$ ， $x_{i2j_3}$ ， $\cdots$ ， $x_{ik_1}$ 的最小者，由此可得数 $x_{i1j_2} - \alpha$ ， $x_{i2j_3} - \alpha$ ， $\cdots$ ， $x_{ikj_1} - \alpha$ 是非负的并且至少有一个等于0。另外，由于Z的每行或与X的对应行相同或由X对应行上的一个运送量加上 $\alpha$ 和另一个运送量减去 $\alpha$ 而得到，由此可见，对于每个 $i = 1, 2, \cdots, m, Z$ 的第 $i$ 行中数的和等于 $a_i$ 。同样，我们知道，对于 $j = 1, 2, \cdots, n$ ， $Z$ 的第 $j$ 列中数的和等于 $bj$ 。因此， $Z$ 是运输计划。而且， $BG(Z)$ 至少比 $BG(X)$ 少一条边，因 $\gamma$ 至少有一边不是 $Z$ 的偶图的边。计算 $Z$ 的运输计划的成本，有

$$
\begin{array}{l} C \bullet Z = C \bullet X + \alpha \left(c _ {i 1 j _ {1}} + c _ {i 2 j _ {2}} + \dots + c _ {i k j _ {k}}\right) - \\ \alpha \left(\alpha_ {i j} j _ {2} + c _ {i j} j _ {3} + \dots + c _ {i k} j _ {k}\right) \\ = C * X + \alpha (p - q) \\ \end{array}
$$

由于 $q \geqslant p$ ，故

$$
C \cdot Z \leqslant C \cdot X _ {\bullet}
$$

如果 $BG(Z)$ 没有圈，我们可取 $Y$ 为 $Z$ 。如果 $BG(Z)$ 有圈，用 $Z$ 代替 $X$ 并重复上面论述。经有限次迭代后，便得到运输计划 $Y$ ，其图无圈且 $C \cdot Y \leqslant C \cdot X$ 。

由定理12.3.2可知，为了确定最优运输计划，即最小成本的运输计划，只须考虑其图无圈的那些运输计划。因此，我们把注意力限于其偶图是树或林（即连通分图是树的那些图）的运输计划。现在证明，对于给定的补给和消费的需要量，这就把要考察的运输计划归结为有限集合的情形。

定理12.3.3 设给定补给和消费的需要量 $a_{1}, a_{2}, \cdots, a_{m}, b_{1}, b_{2}, \cdots, b_{n}$ 。设 $X = [x_{ij}]$ 是使得 $BG(X)$ 无圈的运输计划，则没有其他运输计划具有与 $X$ 一样的偶图。

证明 我们说明 $X$ 的运送量由 $X$ 的偶图和补给与消费的需要量 $a_{1}, a_{2}, \cdots, a_{m}, b_{1}, b_{2}, \cdots, b_{n}$ 唯一确定来证明这个定理。对和 $m + n$ 使用归纳法，如果 $m + n = 2$ ，则 $X$ 恰好包含一个运送量，即 $x_{11} = a_{1} = b_{1}$ 。今假定 $m + n > 2$ ，因为 $BG(x)$ 无圈，它有悬挂结点和悬挂边（见定理10.4.2）。如果必要时重新加以编号，我们总可以假定，对应于第一个补给仓库的结点 $s_{1}$ 是悬挂结点和连接 $s_{1}$ 与第一个消费仓库的边是悬挂的。于是， $x_{12} = \cdots = x_{1n} = 0$ ，并且运输计划有如下的形状：

$$
X = \left( \begin{array}{c c c c} x _ {1 1} & 0 & \dots & 0 \\ x _ {2 1} & x _ {2 2} & \dots & x _ {2 n} \\ \vdots & & & \\ x _ {m _ {1}} & x _ {m _ {2}} & \dots & x _ {m n} \end{array} \right).
$$

由于在 $X$ 中的第一行中运送量之和是 $a_{1}$ ，我们有

$$
x _ {1 1} = a _ {2 1}.
$$

注意，这就意味着 $b_{1} \geqslant a_{1}$ 。于是确定了 $X$ 的第一行。令

$$
X ^ {2} = \left( \begin{array}{c c c c} x _ {2 1} & x _ {2 2} & \dots & x _ {2 n} \\ \vdots & & & \\ x _ {m _ {1}} & x _ {m _ {2}} & \dots & x _ {m n} \end{array} \right).
$$

那么， $X'$ 是补给仓库 $s_2, s_3, \dots, s_m$ ① 分别补给 $a_2, a_3, \dots, a_m$ ② 以及消费仓库 $t_1, t_2, \dots, t_n$ 分别消费 $b_1 - a_1, b_2, \dots, b_n$ 的 $(m-1) \times n$ 运输计划。此外， $BG(X')$ 是由 $BG(X)$ 删除悬挂结点 $s_1$ 与它

相交的悬挂边而得到的（它是与 $s_1$ 不同的那些结点的诱导子图）。于是 $BG(X)$ 无圈。 $BG(X')$ 也无圈，而且， $(m - 1) + n = m + n - 1 < m + n$ 。因此，由归纳假设可知， $X'$ 的运输计划是唯一确定的。所以， $X$ 的运送量是唯一确定的。这就完成归纳法并证明了定理。

由于结点集合为 $s_1, s_2, \dots, s_m, t_1, t_2, \dots, t_n$ 的无圈的偶图只有有限多个（的确存在有圈或无圈的 $2^{mn}$ 个偶图），我们便得到定理12.3.3的下面的推论。

推论 假设已知补给和消费的需要量 $a_{1}, a_{2}, \dots, a_{m}, b_{1}, b_{2}, \dots, b_{n}$ ，则存在偶图是无圈的有限多个运输计划。

这个推论与定理12.3.2一起把寻找最优运输计划从无限集合化为有限集合。

定理12.3.3 的证明提供了构造运输计划 $X = \{x_{ij}\}$ 并使 $BG(X)$ 无圈的方法。选某个 $i$ 和 $j$ 并确定 $x_{ij}$ 如下：(i) 如果 $a_i < b_j$ ，我们令 $x_{ij} = a_i$ 且对于 $k \neq j$ ，令 $x_{ik} = 0$ ；(ii) 如果 $a_i > b_j$ ，令 $x_{ij} = b_j$ 且对于 $k \neq i$ ，令 $x_{kj} = 0$ ；(iii) 如果 $a_i = b_j$ ，令 $x_{ij} = a_i = b_j$ 且对于 $k \neq i$ 令 $x_{kj} = 0$ ， $k \neq j$ 令 $x_{ik} = 0$ 。 $X$ 的一行或一列的运送量现在已被确定。在 (i) 情形中，第 $i$ 补给仓库的补给需要量现在减少为 0，而第 $j$ 个消费仓库的消费需要量减少为 $b_j - a_i$ 。对于 (ii) 和 (iii) 情形进行同样的考虑。现在继续重复化简运输计划，以所有可能方式运用这个过程便产生所有运输计划 $X$ ，而且 $BG(X)$ 无圈（见练习32）。在实行这个过程中，我们可以采用西北角准则，由此始终如一地选取确定在运输计划西北角中的运送量。把西北角准则应用于具有补给需要量 3，4，9，7 和消费需要量 2，3，5，5，8 的运输问题：（见346页）

这正是定理12.3.3证明的结果，即上面阐述的过程用于各种可能方式，将得到所有偶图无圈的运输计划（见练习32）。

$$
\begin{array}{l} \left[ \begin{array}{l l l l} 2 & 1 & 0 & 0 \\ 0 & 2 & 2 & 0 \\ 0 & 0 & 3 & 5 \\ 0 & 0 & 0 & 0 \end{array} \right] \begin{array}{l l l l} 3 & 1 & 0 \\ 4 & 2 & 0 \\ 9 & 6 & 1 & 0 \\ 7 & 0 \end{array} \\ 2 3 5 5 8 \\ 0 2 3 0 7 \\ 0 0. 0 \\ \end{array}
$$

而且，这样所确定的运送量，对于某些 $i_1, i_2, \dots, i_p$ 和 $j_1, j_2, \dots, j_q$ 且 $1 \leqslant i_1 < i_2 < \dots < i_p \leqslant m$ 和 $1 \leqslant j_1 < j_2 < \dots < j_q \leqslant n$ 如果不是零，都是

$$
\left(a _ {i 1} + a _ {i 2} + \dots + a _ {i t}\right) - \left(b _ {j 1} + b _ {j 2} + \dots + b _ {j t}\right) \tag {12.3.9}
$$

或 $(b_{j_1} + b_{j_2} + \dots + b_{j_q}) - (a_{i_1} + a_{i_2} + \dots + a_{i_p})$ 。

$$
(1 2, 3, 1 0) \tag {12.3.10}
$$

（见练习33）。特别地，如果补给和消费的需要量是整数，则运送量也是整数。此外，假若在（12.3.9）和（12.3.10）中的表达式除了 $p = m$ 和 $q = n$ 外，永不为零，那么构造的运输计划的偶图是连通的，而且是树。

例 考察补给需要量为2，4，6，7和消费需要量为3，3，4，4，5的运输计划，使用西北角准则，得到运输计划。

$$
\begin{array}{l} \left[ \begin{array}{l l l l} 2 & 0 & 0 & 0 \\ 1 & 3 & 0 & 0 \\ 0 & 0 & 4 & 2 \\ 0 & 0 & 0 & 2 \\ 5 \end{array} \right] \begin{array}{l} 2 \\ 4 \\ 8 \\ 8 \\ 7 \\ 8 \end{array} \\ \text {S S S} 5 \\ \textbf {1 0 0 2 0} \\ \begin{array}{c c} \mathbf {0} & \mathbf {0} \\ \hline \end{array} \\ \end{array}
$$

这个运输计划的偶图由图12.6给出，并且是不连通的。注意， $2 + 4 = 3 + 3$ 。

现在，我们描述运输问题的对偶问题，它使我们能够确定何时运输计划是最优的。令 $u_{1}, u_{2}, \dots, u_{m}$ 是对应每个补给仓库的实数，又令 $v_{1}, v_{2}, \dots, v_{n}$ 是对应每个消费仓库的实数，使得

$$
\begin{array}{r l} u _ {i} + v _ {j} \leqslant c _ {i j} & (i = 1, 2, \dots , m, \\ f = 1, 2, \dots , n). \end{array} \tag {12.3.11}
$$

![](images/a23aef799496f9ac18fb4a505c519dccc48ee4c882f65a6b33cb89ca3c886242.jpg)  
图12.6

考虑和数

$$
\begin{array}{l} \sum_ {i = 1} ^ {m} a _ {i} u _ {i} + \sum_ {j = 1} ^ {n} b _ {j} v _ {j} = \left(a _ {1} u _ {1} + a _ {2} u _ {2} + \dots + a _ {m} u _ {m}\right) + \left(b _ {1} v _ {1} \right. \\ + b _ {2} v _ {1} + \dots + b _ {n} v _ {n}). \\ \end{array}
$$

定理12.3.4 设 $X = [x_{ij}]$ 是运输计划，则对任何满足（12.3.11）的实数 $u_1, u_2, \dots, u_m, v_1, v_2, \dots, v_n, X$ 的成本满足

$$
C \cdot X \geqslant \sum_ {i = 1} ^ {m} a _ {i} u _ {i} + \sum_ {j = 1} ^ {n} b _ {j} \oplus j. \tag {12.3.12}
$$

此外，若当 $x_{i} > 0$ 时，

$$
u _ {j} + u _ {j} = c _ {i j}, \tag {12.3.13}
$$

则 $X$ 是最优运输计划且其成本为

$$
C \cdot X = \sum_ {i = 1} ^ {m} a _ {i} u _ {i} + \sum_ {j = 1} ^ {n} b _ {j} u _ {j}. \tag {12.3.14}
$$

证明 设 $u_{1}, u_{2}, \cdots, u_{m}, v_{1}, v_{2}, \cdots, v_{n}$ 是满足(12.3.11)的实数，则 $\sum_{i=1}^{m} \sum_{j=1}^{n} c_{ij} x_{ij} \geqslant \sum_{i=1}^{m} \sum_{j=1}^{n} (u_{i} + v_{j}) x_{ij}$

$$
\begin{array}{l} = \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {n} u _ {i} x _ {i j} + \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {n} v _ {j} x _ {i j} \\ = \sum_ {i = 1} ^ {m} u _ {i} \sum_ {j = 1} ^ {n} x _ {i j} + \sum_ {j = 1} ^ {n} v _ {j} \sum_ {i = 1} ^ {m} x _ {i j} \\ = \sum_ {i = 1} ^ {m} a _ {i} u _ {i} + \sum_ {j = 1} ^ {n} b _ {j} v _ {j}. \\ \end{array}
$$

于是，（12.3.12）成立。如果还有（12.3.13）成立，那么上面运算的不等式就成了等式，因为只要 $c_{ij} > u_i + v_j$ ，便有 $x_{ij} = 0$ 。因此，这时 $\sum_{i=1}^{m} \sum_{j=1}^{n} c_{ij} x_{ij} = \sum_{i=1}^{m} a_i u_i + \sum_{j=1}^{n} b_j v_j$ ，这即是（12.3.14），定理证毕。

这个定理的意义如下。假设 $X$ 是我们要检验最优性的运输计划。根据定理，如果我们找到满足（12.3.11）和（12.3.13）的数 $u_{1}, u_{2}, \cdots, u_{m}, v_{1}, v_{2}, \cdots, v_{n}$ ，则可以断言 $X$ 是最优运输计划。

其逆亦真：如果 $X$ 是最优运输计划，则存在满足(12.3.11)和（12.3.13）的数 $u_{1}, u_{2}, \cdots, u_{m}, v_{1}, v_{2}, \cdots, v_{n}$ ，因而也满足（12.3.14）。在 $X$ 的偶图无圈的情况下，这可从下面要叙述的求最优运输计划的算法得出。再由定理12.3.2和12.3.4可知，下面论断成立。

运输计划的最小成本等于和数 $\sum_{i=1}^{m} a_i u_i + \sum_{j=1}^{n} b_j v_j$ 的最大值，其中数 $u_1, u_2, \cdots, u_{mp}, v_1, v_2, \cdots, v_n$ 满足（12.3.11）

确定最优运输计划的算法

首先假定，补给需要量是 $a_1, a_2, \dots, a_m$ 和消费需要量是

$b_{1}, b_{2}, \cdots, b_{n}$ 使得不存在任何运输计划，其偶图是不连通的。形如（12.3.9）和（12.3.10）的表达式除了 $p = m$ 和 $q = n$ 外均不为零。

I 构造一个运输计划 $X = \{x_{ij}\}$ , 其偶图无圈（可以使用西北角准则）。

I 确定数 $u_{1}, u_{2}, \cdots, u_{m}$ 和 $v_{1}, v_{2}, \cdots, v_{n}$ ，使得当 $x_{ij} \neq 0$ 时

$$
u _ {i} + v _ {j} = c _ {i j}. \tag {12.3.15}
$$

由于 $BG(X)$ 是有 $m + n$ 个结点和无圈的连通偶图（由定理10.4.1知其是树），它恰有 $m + n - 1$ 条边。于是，刚好存在 $m + n - 1$ 个数对 $i, j$ ，使得 $x_{ij} \neq 0$ ，因而（12.3.5）恰好有 $m + n - 1$ 个方程。由于含有 $m + n$ 个未知数的 $m + n - 1$ 个方程，其解是不唯一的。我们可以得到一个解如下。令 $u_1 = 0$ ，其次确定所有的 $vj$ ，使得结点 $i_j$ 连接结点 $s_1$ 。再确定所有的 $u_i = s_1$ 使得 $s_i$ 连接到 $vj$ 已经确定的一个结点 $t_j$ ，等等。

对于所有 $x_{ij} = 0$ 的 $i, j$ , 计算 $c_{ij} - u_i - v_j$ . 如果对于所有 $x_{ij} = 0$ 的 $i, j, c_{ij} - u_i - v_j \geqslant 0$ , 则由定理12.3.4可知, $X$ 是最优运输计划. 现在假定对于 $x_{i_0j_0} = 0$ 的某个 $i_0, j_0$ , 有 $c_{i_0j_0} - u_{i_0} - v_{j_0} < 0$ . 假若向 $BG(X)$ 添加连接 $s_{i_0}$ 和 $t_{j_0}$ 的新边 $e_0$ , 我们就得到恰有一个圈 $\gamma_0 = (t_{j_0}, s_{i_0}, t_{j_1}, s_{i_1}, \dots, t_{jk}, s_{ik}, t_{j_0})$ 的新的偶图, 其中一条边为 $e_0$ (见定理10.4.6). 圈 $\gamma_0$ 是偶图的一个长度为偶数的圈. $\gamma_0$ 的每条不同于 $e_0$ 的边对应于运输数 $x_{ij} > 0$ . 令 $X' = (x'_i)_j$ 是由 $X$ 交替地把 $\theta$ 和 $-\theta$ ( $\theta$ 由下面给出) 加到对应于 $\gamma_0$ 的边的运送量而得到的运输计划.

$$
x ^ {\prime} _ {i 0 j _ {0}} = \theta , x ^ {\prime} _ {i 0 j _ {1}} = x _ {i 0 j _ {1}} - \theta , x ^ {\prime} _ {i 1 j _ {1}} = x _ {i 1 j _ {1}} + \theta , \dots ,
$$

$x_{ik}i_k = x_{ik}j_k + \theta$ ， $x^{\prime}_{ik}j_{0} = x_{ik}i_{0} - \theta$ 和在其他情况下，

$$
x _ {i j} ^ {\prime} = x _ {i j}.
$$

对于 $k = 3$ 可以用图式说明如下：

$$
\left. \begin{array}{c c c c} j _ {0} & j _ {1} & i _ {2} & i _ {3} \\ i _ {0} & \theta & x _ {i 0} j _ {1} - \theta & \\ i _ {1} & & x _ {i 1} j _ {1} + \theta & x _ {i 1} j _ {2} - \theta \\ i _ {2} & & & x _ {i 2} j _ {2} + \theta & x _ {i 2} j _ {3} - \theta \\ i _ {3} & x _ {i 3} j _ {0} - \theta & & x _ {i 3} j _ {3} + \theta \end{array} \right\} \tag {12.3.16}
$$

现在选 $\theta$ 是运送量

$$
x _ {i _ {0} j _ {1}}, x _ {i _ {1} j _ {2}}, \dots , x _ {i _ {k} j _ {k}} \tag {12.3.17}
$$

中的最小者。则 $x'$ 是运输计划。 $X'$ 的偶图是由 $X$ 的偶图加入连接 $s_{i\theta}$ 与 $t_{j\theta}$ 的边且删除所有对应于（12.3.17）中运送量等于 $\theta$ 的那些边而得到。由于 $\theta$ 是（12.3.17）中那些数的最小者，故至少删除掉一条边。因而 $BG(X')$ 无圈。由于所作的初始假设可知， $BG(X')$ 是连通的，因而是树（这恰恰意味着(12.3.17)中那些数有一个等于 $\theta$ ）。现在来计算 $X'$ 的成本：

$$
C \cdot X ^ {\prime} = C \cdot X + \theta d _ {i o j 0}, \tag {12.3.18}
$$

其中

$$
\begin{array}{l} d _ {i 0 j _ {0}} = \left(c _ {i 0 j _ {0}} + c _ {i 1 j _ {1}} + \dots + c _ {i k j _ {k}}\right) - \left(c _ {i _ {0} j _ {1}} + c _ {i _ {1} j _ {2}} + \right. \\ \dots + c _ {i k j _ {0}}) _ {\bullet} \tag {12.3.19} \\ \end{array}
$$

利用（12.3.15），我们得到

$$
\begin{array}{l} u _ {i 1} + v _ {j 1} = c _ {i 1} j _ {1}, \quad - u _ {i 0} - v _ {j 1} = - c _ {i 0} j _ {1}, \\ u _ {i 2} + v j _ {2} = c _ {i 2} j _ {2}, \quad - u _ {j 1} - v j _ {2} = - c _ {i 1} j _ {2}, \\ \end{array}
$$

$$
u _ {i k} + v j _ {j k} = c _ {i k j k}, \quad - u _ {i k} - v j _ {0} = - c _ {i j} j _ {0}.
$$

把这些方程加起来相消之后，我们得到

$$
- u _ {i 0} - v _ {j 0} = d _ {i 0 j 0} - c _ {i 0 j 0},
$$

或

$$
d _ {i 0} j _ {0} = c _ {i 0} j _ {0} - u _ {i 0} - v _ {j 0}. \tag {12.3.20}
$$

因此 $d_{ij} < 0$ ，并且

$$
C \cdot X ^ {\prime} <   C \cdot X, \tag {12.3.21}
$$

故 $X^{\prime}$ 是成本小于 $X$ 的运输计划。

现在用 $X^{\prime}$ 代替 $X$ 重复I和I．每次完成迭代便产生一个运输计划，其偶图无圈并且其成本小于上次运输计划．由于只存在有限个运输计划（见定理12.3.3的推论），最后我们得到一个最优运输计划.

在说明这个算法之前，我们对“退化”情形将作某些解释，这里运输计划不需要有连通的偶图。如上所述退化在运输问题中仅当对于某个 $p$ 和 $q$ 并且 $1 \leqslant p < m$ 和 $1 \leqslant q < n$ 以及某些 $i_1, i_2, \cdots, i_p$ 且 $1 \leqslant i_1 < i_2 < \cdots < i_p \leqslant m$ 与某些 $j_1, j_2, \cdots, j_q$ 且 $1 \leqslant j_1 < j_2 < \cdots < j_q \leqslant n$ 时，才出现

$$
a _ {i _ {1}} + a _ {i _ {2}} + \dots + a _ {i _ {p}} = b _ {j _ {1}} + b _ {j _ {2}} + \dots + b _ {j _ {p}}.
$$

如果出现退化，则我们改为考虑 $\varepsilon$ 运输问题，其中补给需要量是 $a_{1} + n\varepsilon, a_{2}, \cdots, a_{n}$ 和消费需要量是 $b_{1} + \varepsilon, b_{2} + \varepsilon, \cdots, b_{n} + \varepsilon$ ，并且与原来运输问题有同样成本。其中 $\varepsilon$ 选择为充分小的正数（不必规定），以便不出现退化。在 $\varepsilon$ 运输问题中仅当

$$
a _ {i _ {1}} + a _ {i _ {2}} + \dots + a _ {i _ {p}} = b _ {j _ {1}} + b _ {j _ {2}} + \dots + b _ {j _ {q}} + q \varepsilon
$$

或

$$
a _ {i 1} + a _ {i 2} + \dots + a _ {i p} - n \varepsilon = b _ {j 1} + b _ {j 2} + \dots + b _ {j q} + q e
$$

时退化才会出现，因 $q < n$ ，就消除了原来的运输问题的退化。如果 $\varepsilon$ 充分小，不会发生新的退化。不难证明，如果 $X\varepsilon$ 是 $\varepsilon$ 运

输问题的运输计划且其偶图无，则

$$
X _ {6} = X + \varepsilon A,
$$

其中 $X$ 是已知运输问题的运输计划且其偶图无圈，而 $A$ 是整数矩阵（见练习36）。这里所谓 $\varepsilon A$ ，我们的意思是指当 $A$ 中的每个数乘以 $\varepsilon$ 而得到矩阵， $X + \varepsilon A$ 是 $X$ 与 $\varepsilon A$ 的对应元素相加得到的矩阵。所以，由于假定 $\varepsilon$ 非常小，若 $X_{\varepsilon}$ 有 $\varepsilon$ 运输问题的最小成本，则 $X$ 具有给定的运输问题的最小成本（见练习37）。注意， $X$ 是令 $X_{\varepsilon}$ 中的 $\varepsilon$ 等于0而得到。

例 考察补给需要量为7，9，11和消费需要量为3，5，5，14的运输问题以及运输成本矩阵

$$
C = \left( \begin{array}{l l l l} 2 & 1 & 2 & 1 \\ 3 & 2 & 3 & 2 \\ 1 & 2 & 1 & 3 \end{array} \right).
$$

I 用西北角准则得到初始运输计划 $X = \{x_{i,j}\}$

$$
\begin{array}{l} X = \left[ \begin{array}{l l l l} 3 & 4 & 0 & 0 \\ 0 & 1 & 5 & 3 \\ 0 & 0 & 0 & 1 1 \end{array} \right] \begin{array}{l l l l} 7 & 4 & 0 \\ 9 & 8 & 8 & 0 \\ 1 1 & 0 \end{array} \\ 8 5 6 1 4 \\ 0 1 0 1 1 \\ \begin{array}{c c} \mathbf {0} & \mathbf {\cdot} \\ \hline \end{array} \\ \end{array}
$$

1 根据 $u_{i} + v_{j} = c_{ij}$ （当 $x_{ij} \neq 0$ ）来计算数 $u_{1}, u_{2}, u_{3}, v_{1}, v_{2}, v_{3}, v_{4}$ ，先令 $u_{1} = 0$ ，然后对 $X$ 的第一行进行搜索寻找不为零的 $x_{ij}$ 并计算 $v_{j}$ ，再搜索对应于刚计算过的 $v_{j}$ 的那些列寻找不为零的 $x_{ij}$ 并计算 $u$ ，等等。结果是

$$
\begin{array}{l} v _ {1} = 2 v _ {2} = 4 v _ {3} = 2 v _ {4} = 1 \\ \begin{array}{r} u _ {1} = 0 \\ u _ {2} = 1 \\ u _ {3} = 2 \end{array} \left( \begin{array}{c c c c c} 3 & 4 & 0 & 0 \\ 0 & 1 & 5 & 3 \\ 0 & 0 & 0 & 1 1 \end{array} \right). \\ \end{array}
$$

对于所有 $x_{ij} = 0$ 的i，j，计算 $c_{ij} - u_i - v_j$

$$
\begin{array}{l} c _ {1 3} - u _ {1} - v _ {3} = 2 - 0 - 2 = 0, c _ {1 4} - u _ {1} - v _ {4} = 1 - 0 - 1 = 0, \\ c _ {2: 1} - u _ {3} - v _ {1} = 3 - 1 - 2 = 0, c _ {3: 1} - u _ {3} - v _ {1} = 1 - 2 - 2 = - 3, \\ c _ {3 2} - u _ {3} - v _ {2} = 2 - 2 - 1 = - 1, c _ {3 3} - u _ {3} - v _ {3} = 1 - 2 - 2 = - 3. \\ \end{array}
$$

由于这些数不全是非负的，初始运输计划 $X$ 不是最优的。我们选取一个绝对值最大的负数，其中有一个数就是 $c_{31} - u_3 - v_1 = -3$ 。在我们向 $BG(X)$ 添加连接 $s_3$ 与 $t_1$ 的边之后，便得到对应于下面 $X$ 的带方框的运送量的一个圈，对带方框的元素交替

$$
X = \left( \begin{array}{c c c c} 3 & 4 & 0 & 0 \\ 0 & 1 & 5 & 3 \\ 0 & 0 & 0 & 2 \end{array} \right).
$$

加1和-1得到新的运输计划 $Y = [y_{i};]$

$$
Y = \left( \begin{array}{c c c c} 2 & 5, 0 & 0 \\ 0 & 0 & 5 & 4 \\ 1 & 9 & 0 & 1 0 \end{array} \right).
$$

现在根据当 $y_{ij} \neq 0$ 有 $u_i + v_j = c_{ij}$ 来计算 $u_1, u_2, u_3, v_1, v_2, v_3, v_4$ .

$$
\begin{array}{l} v _ {1} = 2 v _ {2} = 1 v _ {3} = 5 v _ {4} = 4 \\ \begin{array}{r} u _ {1} = 0 \\ u _ {2} = - 2 \\ u _ {3} = - 1 \end{array} \left( \begin{array}{c c c c} 2 & 5 & 0 & 0 \\ 0 & 0 & 5 & 4 \\ 1 & 0 & 0 & 1 0 \end{array} \right). \\ \end{array}
$$

对于所有 $y_{i}j = 0$ 的 $\pmb{i}$ ， $j$ 计算 $c_{ij} - u_i - v_j$

$c_{13} - u_1 - v_3 = 2 - 0 - 5 = -3, c_{14} - u_1 - v_4 = 1 - 0 - 4 = -3, c_{21} - u_2 - v_1 = 3 + 2 - 2 = 3, c_{22} - u_2 - v_2 = 2 + 2 - 1 = 3, c_{32} - u_3 - v_2 = 2 + 1 - 1 = 2, c_{33} - u_3 - v_3 = 1 + 1 - 5 = -3.$ 由于这些数不全为非负的，运输计划 $Y$ 不是最优的。我们选取绝对值最大的负数，例如选取数为 $c_{13} - u_1 - v_3 = -3$ 。当向 $BG(Y)$ 添加连接 $s_1$ 与 $t_3$ 的边时，我们得到对应于下面 $Y$ 的带方

框的运送量的一个圈，

$$
Y = \left( \begin{array}{c c c c} 2 & 5 & 0 & 0 \\ 0 & 0 & 5 & 0 \\ 1 & 0 & 0 & 0 \end{array} \right)
$$

交替地对带方框的数加2和-2便得到新的运输计划 $Z = [z_{i}j]$

$$
Z = \left( \begin{array}{c c c c} 0 & 5 & 2 & 0 \\ 0 & 0 & 3 & 6 \\ 3 & 0 & 0 & 8 \end{array} \right).
$$

I 依据当 $z_{ij} \div 0$ 有 $u_{i} + v_{j} = c_{ij}$ 来计算数 $u_{1}, u_{2}, u_{3}, v_{1}, v_{2}, v_{3}, v_{4}$ .

$$
\begin{array}{r} v _ {1} = - 1 \quad v _ {2} = 1 \quad v _ {3} = 2 \quad v _ {4} = 1 \\ u _ {1} = 0 \quad 0 \quad 5 \quad 2 \quad 0 \\ u _ {2} = 1 \quad 0 \quad 0 \quad 3 \quad 6 \\ u _ {3} = 2 \quad 3 \quad 0 \quad 0 \quad 8 \end{array} .
$$

对于所有使 $z_{i j} = 0$ 的 $\pmb{i}$ ， $j$ 计算 $c_{ij} - u_i - v_j$

$c_{11} - u_1 - v_1 = 2 - 0 + 1 = 3, c_{14} - w_1 - v_4 = 1 - 0 - 1 = 0,$ $c_{21} - u_2 - v_1 = 3 - 1 + 1 = 3, c_{22} - u_2 - v_2 = 2 - 1 - 1 = 0,$ $c_{32} - u_3 - v_2 = 2 - 2 - 1 = -1, c_{33} - u_3 - v_3 = 1 - 2 - 2 = -3.$ 因为这些数不全是非负的，运输计划Z不是最优的。我们选取负数 $c_{33} - u_3 - v_3 = -3$ 当向BG(Z)添加连接 $s_3$ 与 $t_3$ 的边时，得到对应于下面Z的带方框的运送量的一个圈：

$$
Z = \left( \begin{array}{c c c c} 0 & 5 & 2 & 0 \\ 0 & 0 & \text {圆} & \text {圆} \\ 3 & 0 & \text {圆} & \text {圆} \end{array} \right).
$$

交替地向带方框的元素加3和-3得到新的运输计划 $W = [w_{i}j]$

$$
W = \left( \begin{array}{c c c c} 0 & 5 & 2 & 0 \\ 0 & 0 & 0 & 9 \\ 3 & 0 & 3 & 5 \end{array} \right).
$$

I 根据当 $w_{ij} \neq 0$ 有 $u_{i} + v_{j} = c_{ij}$ 来计算数 $u_{1}, u_{2}, u_{3}, v_{1}, v_{2}, v_{3}, v_{4}$ ,

$$
v _ {1} = 2 v _ {2} = 1 v _ {3} = 2 v _ {4} = 4
$$

$$
\begin{array}{r} u _ {1} = 0 \\ u _ {2} = - 2 \\ u _ {3} = - 1 \end{array} \left( \begin{array}{c c c c} 0 & 5 & 2 & 0 \\ 0 & 0 & 0 & 9 \\ 3 & 0 & 3 & 5 \end{array} \right).
$$

对所有 $w_{ij} = 0$ 的 $i,j$ 计算 $c_{ij} - u_i - u_j$

$$
\begin{array}{l} c _ {1 1} - u _ {1} - v _ {1} = 2 - 0 - 2 = 0, c _ {1 4} - u _ {1} - v _ {4} = 1 - 0 - 4 = - 3, \\ c _ {2 1} - u _ {2} - v _ {1} = 3 + 2 - 2 = 3, c _ {2 2} - u _ {2} - v _ {2} = 2 + 2 - 1 = 3, \\ c _ {2 3} - u _ {2} - v _ {3} = 3 + 2 - 2 = 3, c _ {3 2} - u _ {3} - v _ {2} = 2 + 1 - 1 = 2. \\ \end{array}
$$

运输计划 $W$ 不是最优的，我们选取负数 $c_{14} - u_1 - v_4 = -3$ 。当向 $BG(W)$ 添加连接 $s_1$ 与 $t_4$ 的边时，我们得到对应于下面 $W$ 的带方框的运输数的一个圈：

$$
W = \left( \begin{array}{c c c c} 0 & 5 & 2 & 0 \\ 0 & 0 & 0 & 9 \\ 3 & 0 & 8 & 5 \end{array} \right).
$$

交替地向带方框的数加上2和-2得到新的运输计划 $P = \{p_i; i\}$

$$
P = \left( \begin{array}{c c c c} 0 & 5 & 0 & 2 \\ 0 & 0 & 0 & 9 \\ 3 & 0 & 5 & 3 \end{array} \right),
$$

根据当 $p_{ij} \neq 0$ 有 $u_i + v_j = c_{ij}$ 来计算数 $u_1, u_2, u_3, v_1, v_2, v_3, v_4$ :

$$
v _ {1} = - 1 v _ {2} = 1 v _ {3} = - 1 v _ {4} = 1
$$

$$
\begin{array}{r} u _ {1} = 0 \\ u _ {2} = 1 \\ u _ {3} = 2 \end{array} \left( \begin{array}{c c c c c} 0 & 5 & 0 & 2 \\ 0 & 0 & 0 & 9 \\ 3 & 0 & 5 & 3 \end{array} \right).
$$

对于所有 $p_{ij} = 0$ 的i， $j$ ，计算 $c_{ij} - u_{i} - v_{j}$

$$
\boldsymbol {c} _ {1 1} - \boldsymbol {u} _ {1} - \boldsymbol {v} _ {1} = 2 - 0 + 1 = 3, \quad c _ {1 3} - \boldsymbol {u} _ {1} - \boldsymbol {v} _ {3} = 2 - 0 + 1 = 3,
$$

$$
\begin{array}{l} c _ {2 1} - u _ {2} - v _ {1} = 3 - 1 + 1 = 3 ①, c _ {2 2} - u _ {2} - v _ {2} = 2 - 1 - 1 = 0, \\ c _ {2 3} - u _ {2} - v _ {3} = 3 - 1 + 1 = 3 ^ {(2)}, c _ {3 2} - u _ {3} - v _ {2} = 2 - 2 - 1 = - 1. \\ \end{array}
$$

运输计划 $P$ 不是最优的，于是再选取仅有的负数 $c_{32} - u_3 - v_2 = -1$ 。当向 $BG(P)$ 添加连接 $s_3$ 与 $t_2$ 的边时，我们得到对应于下面 $P$ 的带方框的运送量的一个圈：

$$
P = \left( \begin{array}{c c c c} 0 & \text {国} & 0 & \text {国} \\ 0 & 0 & 0 & 9 \\ 3 & \text {国} & 5 & \text {国} \end{array} \right).
$$

交替地向带方框的数加3和-3得到新的运输计划 $Q = [q_{i};]$

$$
Q = \left[ \begin{array}{c c c c} 0 & 2 & 0 & 5 \\ 0 & 0 & 0 & 9 \\ 3 & 3 & 5 & 0 \end{array} \right].
$$

1 根据当 $q_{ij} \neq 0, u_i + v_j = c_{ij}$ 来计算数 $u_1, u_2, u_3, v_1, v_2, v_3, v_4$ :

$$
\begin{array}{r} v _ {1} = 0 \quad v _ {2} = 1 \quad v _ {3} = 0 \quad v _ {4} = 1 \\ u _ {1} = 0 \quad 0 \quad 2 \quad 0 \quad 5 \\ u _ {2} = 1 \quad 0 \quad 0 \quad 0 \quad 9 \\ u _ {3} = 1 \quad 3 \quad 3 \quad 5 \quad 0 \end{array} .
$$

对于所有 $q_{i}j = 0$ 的 $i,j,$ 计算 $c_{ij} - u_{i} - v_{j}$

$$
c _ {1 1} - u _ {1} - v _ {1} = 2 - 0 - 0 = 2, c _ {1 3} - u _ {1} - v _ {3} = 2 - 0 - 0 = 2,
$$

$$
\begin{array}{l} c _ {2 1} - u _ {2} - v _ {1} = 3 - 1 - 0 = 2, c _ {2 2} - u _ {2} - v _ {2} = 2 - 1 - 1 = 0, \\ c _ {2 3} - u _ {2} - v _ {3} = 3 - 1 - 0 = 2, c _ {3 4} - u _ {3} - v _ {4} = 3 - 1 - 1 = 1. \\ \end{array}
$$

由于所有这些数是非负的， $Q$ 是最优运输计划，算法完成。

# 12.4 最优分配问题

设 $n$ 是大于1的整数。假设安排 $n$ 个人去做 $n$ 种工作。每人对于每种工作都有一个速率或效率。设第 $i$ 个人对第 $j$ 种工作的速率是非负数 $c_{ij}, i = 1, 2, \dots, n$ 和 $j = 1, 2, \dots, n$ 。有 $n!$ 种可能的分配方案安排这些人员去做各项工作，而这些分配方案对应于 $\{1, 2, \dots, n\}$ 的排列。 $\{1, 2, \dots, n\}$ 的排列 $i_1 i_2 \dots i_n$ 对应于分配方案

人 $1 \rightarrow$ 工作 $i_{1}$ , 人 $2 \rightarrow$ 工作 $i_{2}, \cdots$ 人 $n \rightarrow$ 工作 $i_{n}$ .

为简单起见，我们把排列 $i_1 i_2 \cdots i_n$ 认为是一种分配方案。对于每种分配方案 $i_1 i_2 \cdots i_n$ ，总速率被定义为

$$
c _ {1 i _ {1}} + c _ {2 i _ {2}} + \dots + c _ {n i _ {n}} \tag {12.4.1}
$$

最优分配问题就是求一种分配方案 $i_1 i_2 \cdots i_n$ ，使其总速率最大。现在我们说明最优分配问题可以作为 12.3 节的 Hitchcock 运输问题的特殊情形加以解决。

考虑 $n$ 个补给需要量1，1，…，1和 $n$ 个消费需要量1，1，…，1以及运输成本矩阵 $C = [c_{ij}]$ 的Hitchcock运输问题。设 $i_1i_2\dots i_n$ 是 $\{1,2,\dots ,n\}$ 的一个排列，又设 $X = [x_{ij}]$ 是 $n\times n$ 矩阵，其中 $x_{1i_1} = 1$ ， $x_{2i_2} = 1$ ，…， $x_{ni_n} = 1$ 而其余 $x_{ij} = 0$ 。那么 $X$ 的每行和每列都有一个1和 $n - 1$ 个0。因此， $X$ 是成本由（12.4.1）给定的运输计划。请注意， $X$ 的偶图是不连通的；的确， $BG(X)$ 有 $n$ 个连通分图，其中每个分图都是由单边组成。图12.7中所给出的例子是对应于分配2，4，1，3。反之，假设 $Y = [y_{ij}]$ 是使 $BG(Y)$ 无圈的运输计划。从定理12.3.3以及其后的讨论可知， $Y$ 的运送量全是非负整数。因为补给和消费的需要量全都等于1，这意味着 $Y$ 的所有运送量等于0或1以

![](images/c117a45cffcdc79905141dc767eb92a33c78b3b26347cd6897b6db76bab74453.jpg)  
图12.7

及在 $Y$ 的每行和每列中恰好有一个 1。令在 $Y$ 的第 $k$ 行中的 1 是在第 $j_{k}$ 列中，那么 $j_{1}j_{2}\dots j_{n}$ 是 1 到 $n$ 之间的不同整数，因而 $j_{1}j_{2}\dots j_{n}$ 是 1，2，…， $n$ 的排列。于是 $Y$ 对应于分配方案 $j_{1}j_{2}\dots j_{n}$ 。所以安排各个人员去做各项工作的分配方案与一个偶图无圈的运输计划之间一一对应。在这个对应下，分配方案的总速率等于运输计划的成本。

在这个所要解决的问题和12.3节的运输问题之间有一个差别。这里，我们要求一种分配方案使总评价（12.4.1）尽可能大。用运输模型的术语来说，就是要确定一个运输计划，使其成本尽可能大（而不象在12.3中那样尽可能小）。然而，完全类似于12.3中讨论过的那样，有一种确定运输计划其成本尽可能大的理论，例如，若在定理12.3.4中，我们颠倒（12.3.11）和（12.3.12）的不等式，那么我们便得到同样成立的定理，只要把最优运输计划解释为其成本是尽可能大的运输计划。因此，我们有下面的定理。

定理12.4.1 考虑 $n$ 个补给需要量 $1, 1, 1, \dots, 1$ 和 $n$ 个消费需要量 $1, 1, \dots, 1$ 的运输问题。设 $X = [x_{ij}]$ 是运输计划，则对满足

$$
u _ {i} + u _ {j} \geqslant c _ {i j} (i, j = 1, 2, \dots , n), \tag {12.4.2}
$$

的任何实数 $u_{1}, u_{2}, \cdots, u_{n}, v_{1}, v_{2}, \cdots, v_{n}, X$ 的成本满足

$$
C ^ {*} X \leqslant \sum_ {i = 1} ^ {n} u _ {i} + \sum_ {j = 1} ^ {n} v _ {j}. \tag {12.4.3}
$$

此外，又若当 $x_{ij} > 0$ 时，

$$
u _ {i} + v _ {j} = c _ {i j}, \tag {12.4.4}
$$

则 $X$ 是运输计划，其成本

$$
C ^ {*} X = \sum_ {i = 1} ^ {n} u _ {i} + \sum_ {j = 1} ^ {n} v _ {j} \tag {12.4.5}
$$

是尽可能大。

注意，对应最优分配问题的运输问题是退化的；于是，在利用解相应运输问题的方法来求解最优分配问题中，我们需要考虑 $\varepsilon$ 运输问题。现在我们举例说明这个方法。

例 在本例中沿用12.3节的符号，考察有 $n = 3$ 和由

$$
\left( \begin{array}{c c c} 1 & 2 & 1 \\ 4 & 3 & 2 \\ 2 & 1 & 3 \end{array} \right)
$$

给出速率矩阵 $C = \{c_{ij}\}$ 的最优分配问题。我们求解有补给需要量 $1 + 3\varepsilon$ ，1，1和消费需要量 $1 + \varepsilon$ ， $1 + \varepsilon$ ， $1 + \varepsilon$ 的 $\varepsilon$ 运输问题，这里 $\varepsilon$ 是充分小的正数。

I 利用西北角准则得到一个初始运输计划 $X = [x_{i};j]$ ：

$$
\begin{array}{l} X = \left( \begin{array}{c c c} 1 + e & 2 e & 0 \\ 0 & 1 - e & e \\ 0 & 0 & 1 \end{array} \right) \begin{array}{c c c} 1 + 3 e & 2 e & 0 \\ 1 & 2 & 0 \\ 1 & 0 \end{array} \\ \begin{array}{c c c} \text {1 + 2} & \text {1 + 3} \\ 0 & \text {1 + 4} & \text {1} \\ & 0 & 0 \end{array} \\ \end{array}
$$

I 根据当 $x_{ij} \neq 0$ ， $u_i + v_j = c_{ij}$ 来计算 $u_1, u_2, u_3, v_1, v_2, v_3$

$$
\begin{array}{r l} v _ {1} = 1 & v _ {2} = 2 \quad v _ {3} = 1 \\ u _ {1} = 0 & \left( \begin{array}{c c c} 1 + \varepsilon & 2 \varepsilon & 0 \\ 0 & 1 - \varepsilon & \varepsilon \\ 0 & 0 & 1 \end{array} \right), \end{array}
$$

对于所有 $x_{ij} = 0$ 的i， $j$ 计算 $u_i + v_j - c_{ij}$

$$
\begin{array}{l} \boldsymbol {u} _ {1} + \boldsymbol {v} _ {3} - c _ {1 3} = + 1 - 1 = 0, \boldsymbol {u} _ {2} + \boldsymbol {v} _ {1} - c _ {2 1} = 1 + 1 - 4 = - 2, \\ u _ {3} + v _ {1} - c _ {3 1} = 2 + 1 - 2 = 1, u _ {3} + v _ {2} - c _ {3 2} = 2 + 2 - 1 = 3. \\ \end{array}
$$

由于这些数不全是非负的，初始运输计划不是最优的（在最大成本的意义下）。我们选取负数 $u_{2} + v_{1} - c_{21} = -2$ 。当向 $BG(X)$ 添加连接结点 $s_{2}$ 与结点 $t_{1}$ 的边时，得到对应于下面 $X$ 的带方框运输数的一个圈。

$$
X = \left( \begin{array}{c c c} \frac {\boxed {1 + \varepsilon}}{\boxed {0}} & \frac {\boxed {2 \varepsilon}}{\boxed {1 - \varepsilon}} & 0 \\ 0 & 0 & 1 \end{array} \right).
$$

利用交替地向带方框的数加上 $1 - \varepsilon$ 和 $-(1 - \varepsilon)$ 得到新的运输计划 $Y = [y_{i}; i]$ ：

$$
Y = \left( \begin{array}{c c c} 2 \varepsilon & 1 + \varepsilon & 0 \\ 1 - \varepsilon & 0 & \varepsilon \\ 0 & 0 & 1 \end{array} \right).
$$

I 根据当 $y_{ij} \neq 0$ ， $u_i + v_j = c_{ij}$ 来计算数 $u_1, u_2, u_3, v_1, v_2, v_3, v_4$

$$
v _ {1} = 1 v _ {2} = 2 v _ {3} = - 1
$$

$$
\left. \begin{array}{l l l} u _ {1} = 0 & 2 \varepsilon & 1 + \varepsilon \\ u _ {2} = 3 & 1 - \varepsilon & 0 \\ u _ {3} = 4 & 0 & 0 \end{array} \right\}.
$$

对于所有 $y_{ij} = 0$ 的 $i,j$ 计算 $u_{j} - v_{j} - c_{ij}$

$$
\begin{array}{l} u _ {1} + v _ {3} - c _ {1 3} = 0 - 1 - 1 = - 2, u _ {2} + v _ {2} - c _ {2 2} = 3 + 2 - 3 = 2, \\ u _ {3} + v _ {1} - c _ {3 1} = 4 + 1 - 2 = 3, \quad u _ {3} + v _ {2} - c _ {3 2} = \frac {4}{4} + 2 - 1 = 5. \\ \end{array}
$$

因为这些数不全是非负的，运输计划 $Y$ 不是最优的。我们选负数 $\pmb{u}_{1} + \pmb{v}_{3} - c_{13} = -2$ 。当向 $BG(Y)$ 添加连接结点 $s_{1}$ 与 $t_{3}$ 的

边时，便得到对应于下面 $Y$ 的带方框运送量的一个圈：

$$
Y = \left( \begin{array}{c c c} \frac {\vert 2   \varepsilon \vert}{\vert 1 - \varepsilon \vert} & 1 + \varepsilon & \overline {{\vert 0 \vert}} \\ \overline {{\vert 1 - \varepsilon \vert}} & 0 & \overline {{\vert \varepsilon \vert}} \\ 0 & 0 & 1 \end{array} \right).
$$

交替地向带方框的数加上 $e$ 和 $-e$ 得到新的运输计划 $Z = [z_{ij}]$ ：

$$
Z = \left[ \begin{array}{c c c} \varepsilon & 1 + \varepsilon & \varepsilon \\ 1 & \mathbf {0} & \mathbf {0} \\ \mathbf {0} & \mathbf {0} & 1 \end{array} \right].
$$

根据当 $z_{ij} \neq 0, u_i + u_j = c_{ij}$ 来计算数 $u_1, u_2, u_3, v_1, v_2, v_3$ :

$$
v _ {1} = 1 v _ {2} = 2 v _ {3} = 1
$$

$$
\begin{array}{l} u _ {1} = 0 \\ u _ {2} = 3 \\ u _ {3} = 2 \end{array} \left\{ \begin{array}{l l l} \varepsilon & & 1 + \varepsilon \\ 1 & & 0 \\ 0 & & 0 \\ 0 & & 0 \end{array} \right\}.
$$

对于所有 $z_{ij} = 0$ 的 $\pmb{i}$ ， $j$ 计算 $u_{i} + u_{j} - c_{ij}$

$$
\begin{array}{l} u _ {2} + u _ {2} - c _ {2 2} = 3 + 2 - 3 = 2, u _ {2} + u _ {2} - c _ {2 3} = 3 \div 1 - 2 = 2, \\ u _ {3} + u _ {1} - c _ {3 1} = 2 + 1 - 2 = 1, \quad u _ {3} + u _ {2} - c _ {3 2} - 2 + 2 - 1 = 3. \\ \end{array}
$$

因为所有这些数是非负的，故 $Z$ 是有成本

$$
u _ {1} + u _ {2} + u _ {3} + u _ {1} + u _ {2} + u _ {3} = 0 + 3 + 2 + 1 + 2 + 1 = 9
$$

的关于 $\varepsilon$ 运输问题的最优运输计划。

在 $Z$ 中令 $\varepsilon = 0$ ，得到

$$
\left( \begin{array}{c c c} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{array} \right),
$$

它是初始运输问题的最优运输计划。上面计划对应于分配方案2，1，3或人 $1 \rightarrow$ 工作2，人 $2 \rightarrow$ 工作1，人 $3 \rightarrow$ 工作3.

它便是有总速率等于9的最优分配。当然，由于 $n$ 仅仅是3，我

们这时才能够较容易地计算6种可能的分配方案的总速率以及选取其中最大者。但是，若 $n$ 是很大时，计算所有 $n!$ 种分配方案的总速率就不是有效的了。

在8.3节末简要讨论过的分配问题，那是这里所讨论最优分配问题的特例。在8.3节问题中，有 $m$ 个人申请 $n$ 种工作，每人胜任某几种工作。问题是确定可以担任工作的最多人数，每种工作都要由胜任工作的申请者担当。不失一般性，假设 $m = n$ ，否则我们可以引进不胜任任何工作的假想的人（ $m < n$ ），或没有人胜任的假想的工作（ $m > n$ ）。于是，假设有 $n$ 个人和 $n$ 种工作之后，我们确定第 $i$ 个人对第 $j$ 种工作效率为 $c_{ij}$ ，其中若第 $i$ 个人胜任第 $j$ 种工作， $c_{ij} = 1$ ；否则， $c_{ij} = 0$ 。具有最大总效率 $c_{1i_1} + c_{2i_2} + \dots + c_{ni_n} = p$ 的分配方案确定 $p$ 个人对他们胜任工作的分配，只须若 $c_{ki_k} = 1$ ，把工作 $i_k$ 分配给人 $k$ 。易见，不可能存在一种分配方案，使多于 $p$ 个人能胜任分配给他们的工作。

# 12.5 瓶颈问题

我们将研究一个一般问题，下面两个问题是其特殊情形。

极大交通流问题。设 $G$ 是连通简单图， $u$ 和 $v$ 是 $G$ 的两个不同的结点。我们把 $G$ 看成为连接 $u$ 与 $v$ 的道路系统。对应于 $G$ 的每条边 $e$ ，设有给定的非负数 $c(e)$ ，称之为边 $e$ 的容量。我们把

$c(e)$ 看作表示边 $e$ 上的每小时可通过的交通流量。设 $\gamma$ 是连接 $u$ 和 $v$ 的基本链。定义 $\gamma$ 的容量 $c(\gamma)$ 是 $\gamma$ 的边的极小容量。因为通过 $\gamma$ 的每小时交通流量受到 $\gamma$ 的各边通过的每小时交通流量所限制， $c(\gamma)$ 表示在 $\gamma$ 上每小时可通过的交通流量。对于 $\gamma$ 的边 $e'$ ， $c(e') = c(\gamma)$ 是连接 $u$ 和 $v$ 的链的瓶颈。于是，通过 $\gamma$ 的每小时交通流量等于在 $\gamma$ 的瓶颈上通过的每小时交通流量。问题就是要决定一条连接 $u$ 和 $v$ 且有极大容量的基本链，即，在连接 $u$ 和 $v$ 的所有基本链中容许有最大的每小时交通流量通过的基本链。于是，我们要确定 $\gamma_0$ ，使得

$$
c (\gamma_ {0}) = \max  _ {\gamma} c (\gamma) = \max  _ {\gamma} \min  _ {\gamma} c (e),
$$

这里极大是取遍连接 $u$ 和 $\pmb{\upsilon}$ 的所有基本链 $\gamma$ 取得的，极小是取遍 $\gamma$ 的所有边来确定。分离 $u$ 和 $\pmb{\upsilon}$ 的割集是 $G$ 的边集 $K$ ，它含有连接 $u$ 和 $\pmb{\upsilon}$ 的每条基本链（因而每条链）的一条边。等价地， $K$ 是分离 $u$ 和 $\pmb{\upsilon}$ 的割集，只要 $u$ 和 $\pmb{\upsilon}$ 是位于由 $G$ 移去 $K$ 中所有边而得到的图的不同连通分图中。分离 $u$ 和 $\pmb{\upsilon}$ 的基本割集是分离 $u$ 和 $\pmb{\upsilon}$ 的极小割集。于是，割集 $K$ 是基本的，只要去掉 $K$ 中任何边而剩下集合不再是分离 $u$ 和 $\pmb{\upsilon}$ 的割集。设 $K$ 是分离 $u$ 和 $\pmb{\upsilon}$ 的基本割集，又 $\gamma$ 是连接 $u$ 和 $\pmb{\upsilon}$ 的基本链，则 $\gamma$ 的某条边是 $K$ 的一条边，因而

$$
c (\gamma) \leqslant \max  _ {e \in K} c (e). \tag {12.5.1}
$$

因为这对于分离 $u$ 和 $v$ 的所有基本割集 $K$ 都是正确的，由此可得

$$
c (\gamma) \leqslant \min  _ {K} \max  _ {e \in K} c (e), \tag {12.5.2}
$$

这里，极小是取遍分离 $u$ 和 $v$ 的所有基本割集 $K$ 取得的。因为（12.5.2）对于连接 $u$ 和 $v$ 的所有基本链都是正确的，所以，

$$
\max  _ {\gamma} c (\gamma) \leqslant \min  _ {K} \max  _ {e \in K} c (e), \tag {12.5.3}
$$

其中，左边的极大是取遍连接 $\pmb{u}$ 和 $\pmb{v}$ 的全部基本链达到的。因此，

$$
\begin{array}{l} \max  \min  c (e) \leqslant \min  \max  c (e). \tag {12.5.4} \\ \gamma \quad \gamma \text {的 一 边} e \quad K \quad e \in K \\ \end{array}
$$

我们会看到，在（12.5.4）中等式实际上是成立的，并且知道怎样确定链 $\gamma_0$ 和割集 $K_0$ ，使得

$$
\begin{array}{l} \min  c (e) = \max  c (e), \\ \gamma_ {0} \text {的 边} e \quad e \in K \\ \end{array}
$$

极小高度问题。继续采用前一个问题中的符号，现在我们把 $c(e)$ 理解为表示边 $e$ 的高度。如果 $\gamma$ 是连接 $u$ 和 $v$ 的基本链，则沿着 $\gamma$ 穿过所碰到的最大高度是

$$
\begin{array}{l} c ^ {\prime} (\gamma) = \max  c (e). \\ y \text {的 边} e \\ \end{array}
$$

问题是确定连接 $u$ 和 $v$ 的一条基本链，它具有的高度尽可能小。根据十分类似于上例中所用到的推理，我们得到

$$
\begin{array}{l} \min  \max  c (e) \geqslant \max  \min  c (e), \tag {12.5.5} \\ \gamma \quad \gamma \text {的 边} e \quad K \quad e \in K \\ \end{array}
$$

这里，左边取极小是取遍连接 $\pmb{u}$ 和 $\pmb{v}$ 的所有基本链 $\gamma$ 得到的，右边取极大是取遍分离 $\pmb{u}$ 和 $\pmb{v}$ 的所有基本割集得到的。我们将会看到，在（12.5.5）中等式实际上是成立的，并且知道怎样确定链 $\gamma^{\prime}$ 和割集 $K^{\prime}$ ，使得

$$
\begin{array}{l} \max  c (e) = \min  c (e). \\ y \text {的 边} e \quad e \in K ^ {\prime} \\ \end{array}
$$

现在我们考虑这两个问题的不同特点。设 $E$ 是图 $G$ 的边集合， $\mathcal{P}$ 表示连接 $u$ 和 $v$ 的基本链的边集合的集族，则 $\mathcal{P}$ 是 $E$ 的子集的集族。若 $P_{1}$ 和 $P_{2}$ 都是 $\mathcal{P}$ 的成员并且 $P_{1} \subseteq P_{2}$ ，由于链是基本的，所以 $P_{1} \vdash P_{2}$ 。当一个 $E$ 的子集的集族中没有任何成员包含不等于它的另一个成员时，我们称这集族为 $E$ 上的杂乱集。

于是， $\mathcal{S}$ 是 $E$ 上的杂乱集。令 $\mathcal{K}$ 表示分离 $u$ 和 $v$ 的基本割集的集族，因为割集是基本的，由此可得 $\mathcal{K}$ 也是 $E$ 上的杂乱集。由于分离 $u$ 和 $v$ 的割集会有连接 $u$ 和 $v$ 的每条链的边，对于 $\mathcal{S}$ 的每个成员 $P$ 和 $\mathcal{K}$ 的每个成员 $K$ ，我们有 $P \cap K \neq \emptyset$ ，不等式（12.5.4）和（12.5.5）正是依赖于这后一个性质。 $\mathcal{K}$ 不仅仅具有其每个成员与 $\mathcal{S}$ 的每个成员有非空交这种性质，而且也有这样的性质，对于与 $\mathcal{S}$ 的每个成员有非空交的边集合 $F$ ，存在 $\mathcal{K}$ 的成员 $K$ 使 $K \subseteq F$ 。这是分离 $u$ 和 $v$ 的每个割集包含分离 $u$ 和 $v$ 的基本割集的一个推论。为了使 $\mathcal{S}$ 和 $\mathcal{K}$ 能起相同的作用，我们引出如下定义。在定理12.5.1将显示出与前面讨论的联系。

设 $E$ 是一个任意的有限集合， $\mathcal{P}$ 和 $\mathcal{X}$ 是 $E$ 上的两个杂乱集，那么有序对 $(\mathcal{P}, \mathcal{X})$ 称为 $E$ 的分块组只要下面的性质成立：

(*) 对于每个把 $E$ 分成两个集合 $E_{1}$ 和 $E_{2}$ 的划分，或存在 $\mathcal{P}$ 的成员 $P$ 且 $P \subseteq E_{1}$ 或存在 $\mathcal{K}$ 的成员 $K$ 且 $K \subseteq E_{2}$ ，但两者不能同时成立。

交换 $(\bullet)$ 中的 $E_{1}$ 和 $E_{2}$ ，我们得到，如果 $(\mathcal{P}, \mathcal{X})$ 是 $E$ 的分块组，则 $(\mathcal{X}, \mathcal{P})$ 也是 $E$ 的分块组。

第一个定理具体地表明怎样构造分块组。

定理12.5.1 设 $E$ 是有限集， $\mathcal{P}$ 是 $E$ 上的一个杂乱集。令 $\mathcal{K}$ 是由 $E$ 的，与 $\mathcal{P}$ 的每个成员有非空的交并且关于这个性质是极小的所有子集组成，则 $\mathcal{K}$ 是 $E$ 上唯一的杂乱集，使得 $(\mathcal{P}, \mathcal{K})$ 是 $E$ 的分块组。

证明 因为 $\mathcal{K}$ 的定义中的极小条件， $\mathcal{K}$ 是 $E$ 上的一个杂乱集。我们首先证明 $(\mathcal{P}, \mathcal{K})$ 是 $E$ 上的一个分块组。设 $E$ 被划分成两个集合 $E_1$ 和 $E_2$ 。假设没有 $\mathcal{P}$ 的成员是 $E_1$ 的子集，则 $\mathcal{P}$ 的每个成员与 $E_2$ 有非空交集，因此由 $\mathcal{K}$ 的定义， $E_2$ 包含 $\mathcal{K}$ 的成员。现在假定既存在 $\mathcal{P}$ 的成员 $P$ 且 $P \subseteq E_1$ ，又存在 $\mathcal{K}$ 的成员 $K$

且 $K \subseteq E_{2}$ 。由于 $E_{1}$ 和 $E_{2}$ 是 $E$ 的划分， $E_{1} \cap E_{2} = \phi$ ，因而 $P \cap K = \phi$ 。后者与 $\mathcal{X}$ 的定义矛盾。我们断言 $(\bullet)$ 是满足的，从而 $(\mathcal{P}, \mathcal{X})$ 是 $E$ 的分块组。

现在证明至多存在一个 $E$ 上的杂乱集 $\mathcal{K}^{\prime}$ , 使 $(\mathcal{P}, \mathcal{K}^{\prime})$ 是分块组。从已经作的证明就推出 $\mathcal{K}$ 是 $E$ 上的唯一的杂乱集, 使 $(\mathcal{P}, \mathcal{K})$ 是 $E$ 的分块组。假设 $(\mathcal{P}, \mathcal{K}_1)$ 和 $(\mathcal{P}, \mathcal{K}_2)$ 是 $E$ 的分块组且 $\mathcal{K}_1$ 与 $\mathcal{K}_2$ 是不同的。必要的话可交换 $\mathcal{K}_1$ 和 $\mathcal{K}_2$ , 因此我们可以假设存在集合 $K_1$ 是 $\mathcal{K}_1$ 的成员但不是 $\mathcal{K}_2$ 的成员。我们考虑 $E - K_1$ 和 $K_1$ 形成 $E$ 的划分, 并把 (*) 应用到分块组 $(\mathcal{P}, \mathcal{K}_1)$ , 没有 $\mathcal{P}$ 的成员是 $E - K_1$ 的子集。现在把 (*) 应用到分块组 $(\mathcal{P}, \mathcal{K}_2)$ 上, 我们可知, 存在 $\mathcal{K}_2$ 的成员 $K_2$ 且 $K_2 \subseteq K_1$ 。由于 $K_1$ 不是 $\mathcal{K}_2$ 的成员, $K_2 \neq K_1$ 。再来考察 $E$ 的划分 $E - K_2$ 和 $K_2$ 。假若有 $\mathcal{K}_1$ 的成员 $K_1'$ 且 $K_1' \subseteq K_2$ ，那么由于 $K_2 \subseteq K_1$ 但 $K_2 \neq K_1$ ，这就意味着 $K_1' \subseteq K_1$ 和 $K_1' \neq K_1$ ，这与 $\mathcal{K}_1$ 是 $E$ 的杂乱集这个事实矛盾。因此，没有 $\mathcal{K}_1$ 的成员是 $K_2$ 的子集。把 (*) 应用到 $(\mathcal{P}, \mathcal{K}_1)$ 上, 我们可得, 有 $\mathcal{P}$ 的成员 $P \not\subseteq P \subseteq E - K_2$ 。但是，对于 $E$ 的划分 $E - K_2$ 和 $K_2$ ，我们有 $\mathcal{P}$ 的成员 $P$ 且 $P \subseteq E - K_2$ ，又由于 $K_2$ 也是 $\mathcal{K}_2$ 的成员且 $K_2 \subseteq K_2$ ，这与 $(\mathcal{P}, \mathcal{K}_2)$ 是 $E$ 上的分块组这个事实相矛盾。因此，定理证毕。

例设 $G$ 是连通图， $\pmb{u}$ 与 $\pmb{\nu}$ 是 $G$ 的两个结点。令 $\mathcal{P}$ 表示连接 $\pmb{u}$ 和 $\pmb{\nu}$ 的基本链的边集的集族，又令 $\mathcal{K}$ 表示分离 $\pmb{u}$ 和 $\pmb{\nu}$ 的基本割集的族集。从定理12.5.1可得， $(\mathcal{P},\mathcal{K})$ 是 $G$ 的边集E的一个分块组。

例设 $G$ 是一个连通图，其边集为 $E$ 。令 $\mathcal{P}$ 表示 $G$ 的生成树的边集的集族（见10.4节）。显然， $\mathcal{P}$ 是 $E$ 的一个杂乱集。根据定理12.5.1， $E$ 上存在唯一的一个杂乱集 $\mathcal{K}$ ，使 $(\mathcal{P}, \mathcal{K})$

是 $E$ 的分块组。又 $\mathcal{X}$ 是由所有这样的边集组成的，这种边集与 $G$ 的每个生成树有一公共边并且关于这个性质是极小的。由定理10.4.5知，每个连通图有一个生成树。所以，一个边集 $K$ 是 $\mathcal{X}$ 的成员当且仅当(i)由 $G$ 移去 $K$ 中所有边得到的图不是连通的和(ii) $K$ 关于这个性质是极小的。 $\mathcal{X}$ 的成员通常称为连通图 $G$ 的初等余圈。

现在我们讲本节的主要定理.

定理12.5.2 设 $(\mathcal{P}, \mathcal{K})$ 是有限集合 $E$ 的分块组，又 $f$ 是定义在 $E$ 上的一个实值函数，则

$$
\max  _ {F \neq F} \min  f (e) = \min  _ {R \neq R} \max  f (e). \tag {12.5.6}
$$

证明 设 $P$ 是 $\mathcal{P}$ 的一个成员和 $K$ 是 $\mathcal{K}$ 的一个成员。由定理 12.5.1 可知，在 $P \cap K$ 中存在元素 $e_0$ ，因而

$$
\min  _ {e \neq p} f (e) \leqslant f \left(e _ {0}\right) \leqslant \max  _ {e \neq K} f (e),
$$

或

$$
\min  _ {e \in E} f (e) \leqslant \max  _ {e \in E} f (e). \tag {12.5.7}
$$

由于上式对 $\mathcal{P}$ 中的所有 $P$ 和 $\mathcal{X}$ 中的所有 $K$ 成立，我们得到，

$$
\max  _ {P \neq R} \min  f (e) \leqslant \min  _ {K \neq K} \max  f (e). \tag {12.5.8}
$$

现在证明在(12.5.8)中等式成立。设 $a_1, a_2, \cdots, a_m$ 是由函数 $f$ 所确定的不同值，其中 $a_1 > a_2 > \cdots > a_m$ 。令 $F_0 = \emptyset$ ，并对 $i = 1, 2, \cdots, m$ 令 $F_i$ 是 $E$ 中所有使 $f(e) = a_i$ 的 $e$ 组成的子集，设 $k$ 是使 $F_0 \cup F_1 \cup \cdots \cup F_k$ 含有 $\mathcal{P}$ 的成员 $P'$ 的第一个整数。由于 $F_0 \cup F_1 \cup \cdots \cup F_{k-1}$ 不包含有 $\mathcal{P}$ 的任何成员，由此可得 $P' \cap F_k \neq \emptyset$ ，从而

$$
\underset {e \rightarrow F} {\min } f (e) = a _ {k}. \tag {12.5.9}
$$

所以

$$
\max  _ {P \in \mathcal {P}} \min  _ {e ^ {\prime} \in \mathcal {P}} f (e) \geqslant a _ {k.} \tag {12.5.10}
$$

现在 $F_{0} \cup F_{1} \cup \cdots \cup F_{k-1}$ 和 $F_{k} \cup F_{k+1} \cup \cdots \cup F_{m}$ 构成 $E$ 的一个划分。由于 $F_{0} \cup F_{1} \cup \cdots \cup F_{k-1}$ 不含有 $\mathcal{P}$ 的成员，所以存在 $\mathcal{X}$ 的成员 $K'$ 且 $K' \subseteq F_{k} \cup F_{k+1} \cup \cdots \cup F_{m}$ 。因为 $P' \subseteq F_{0} \cup F_{1} \cup \cdots \cup F_{k}$ ，我们必有 $K' \mp F_{k+1} \cup \cdots \cup F_{m}$ ，否则，就和 (*) 矛盾。于是 $K' \cap F_{k} \neq \emptyset$ ，由此得到

$$
\max  _ {i \in I} f (e) = a _ {k _ {*}} \tag {12.5.11}
$$

所以

$$
\min  _ {K \in \mathbb {Z}} \max  _ {e \in K} f (e) \leqslant a _ {k}. \tag {12.5.12}
$$

把它与(12.5.10)合并起来，我们得到

$$
\min  _ {K \text {发}} \max  _ {c \text {发}} f (e) \leqslant \max  _ {i} \min  _ {i} f (e). \tag {12.5.13}
$$

由此式和(12.5.8)我们得到方程(12.5.6)，左边和右边的公共值是 $a$ 。

上面的证明包含了在 $\mathcal{P}$ 中确定 $P^{\prime}$ ，使得

$$
\min  _ {e \in P} f (e) = \max  _ {P \neq e} \min  _ {e \in P} f (e)
$$

的一种算法。为了应用它，需要能辨认何时 $E$ 的子集含有 $\mathcal{P}$ 的成员。当然， $\mathcal{P}$ 的成员不是总能用列表形式给出，但可以象在已经讨论过的例子中那样，给出它们一个描述性的定义。

现在我们来完成极大交通流量问题和极小高度问题的讨论。令 $G$ 是一个作为连接两结点 $u$ 和 $v$ 的道路系统的连通图，设 $\mathcal{P}$ 是连接 $u$ 和 $v$ 的基本链的边集的集族， $\mathcal{K}$ 是分离 $u$ 和 $v$ 的基本割集的集族。那么 $(\mathcal{P}, \mathcal{K})$ 是 $E$ 的分块组，因而 $(\mathcal{K}, \mathcal{P})$ 也是 $E$ 的分块组。由定理12.5.2可得，在(12.5.4)中等式成立。定理的证明提供了一种确定联接 $u$ 和 $v$ 并且允许有最大交通流量的基本

链的算法。由于 $(\mathcal{K}, \mathcal{P})$ 也是 $E$ 的分块组，由定理12.5.2同样可得，在(12.5.5)中的等式成立。其证明同样提供了一种确定连接 $u$ 和 $v$ 且达到最小高度的基本链的算法。

我们以定理15.5.1和15.5.2的另一种应用来结束本节。

瓶颈分配问题 一条连续生产线上的 $n$ 项工作分配给 $n$ 个人。对于 $i = 1, 2, \dots, n$ 和 $j = 1, 2, \dots, n$ ，令 $c_{ij}$ 表示第 $i$ 个人如果被分配第 $j$ 项工作时可以加工项目的速率。显然生产线上只能按最慢的人（瓶颈）的速率进行。问题就是要确定人对工作的一种分配使之有最快的最慢速率。 $E$ 如在12.4节的最优分配问题那样，我们可以把这种分配与排列等同起来。令 $i_1 i_2 \dots i_n$ 是一种分配，那么完成生产线上各项目的速率（理论上）是数 $c_{1i_1} c_{2i_2} \dots c_{ni_n}$ 的极小值。于是，问题便是确定

$$
\begin{array}{l} \max  \quad \min  c _ {i j j}, \tag {12.5.14} \\ i _ {1} i _ {2} ^ {* * *} i _ {n} \quad 1 \leq j \leq n \\ \end{array}
$$

这里，确定极大值是取遍所有分配 $i_1 i_2 \cdots i_n$ ，并且确定一种分配 $k_1 k_2 \cdots k_n$ 使得

$$
\begin{array}{l} \min  c _ {j k j} = \max  \quad \min  c _ {j i j.} \tag {12.5.15} \\ 1 \leq j \leq n \quad i _ {1} i _ {2} \dots i _ {n} 1 \leq j \leq n \\ \end{array}
$$

考虑结点为 $s_1, s_2, \dots, s_n$ （ $n$ 个人）和 $t_1, t_2, \dots, t_n$ （ $n$ 种工作）并且每个 $s_i$ 到每个 $t_j$ 都存在一条连接它的边的偶图 $G$ 。于是， $G$ 是完全偶图 $K_{n,n}$ 。人对工作的分配对应着 $G$ 中 $n$ 条边的匹配，现在就把这些匹配看作分配。设 $E$ 是 $G$ 的所有边的集合，又令 $\mathcal{P}$ 是所有分配的集合。这样， $\mathcal{P}$ 的每个成员是 $n$ 条边的集合，其中没有任何两条边有一个公共结点。由此易知， $\mathcal{P}$ 是 $E$ 上的杂乱集（见练习55）。根据定理12.5.1，存在 $E$ 上唯一的杂乱集 $\mathcal{H}$ ，使得 $(\mathcal{P}, \mathcal{H})$ 是 $E$ 的分块组。而且， $\mathcal{H}$ 是由所有这样的边集组成，这种边集与每种分配有非空交并且关于这

个性质是极小的，设 $K$ 是一个边集合， $G(K)$ 是由 $G$ 移去 $K$ 的所有边而得到的图。由定理8.2.2可知， $G$ 没有 $n$ 条边的匹配（等价地， $K$ 与每种分配有非空交）当且仅当存在 $G$ 的 $n - 1$ 个结点的集合 $S = S_{1} \cup S_{2}$ 且 $S_{1} \subseteq \{s_{1}, s_{2}, \dots, s_{n}\}$ 和 $S_{2} \subseteq \{t_{1}, t_{2}, \dots, t_{n}\}$ ，使 $G(K)$ 的每条边的结点有一个在 $S$ 中——等价地，使得 $G(K)$ 不存在连接 $A_{1}$ 中结点到 $A_{2}$ 中结点的边，这里

$$
A _ {1} = \left\{s _ {1}, s _ {2}, \dots , s _ {n} \right\} - S _ {1},
$$

$$
A _ {2} = \left\{t _ {1}, t _ {2}, \dots , t _ {n} \right\} - S _ {2},
$$

即， $K$ 包含有连接 $A_{1}$ 的结点到 $A_{2}$ 的结点的边。由于 $S$ 包含有 $n - 1$ 个结点， $A_{1} \cup A_{2}$ 包含有 $n + 1$ 个结点。根据定理12.5.1可得，边集 $K$ 是 $\mathcal{X}$ 的一个成员当且仅当存在集合 $A_{1} \subseteq \{s_{1}, s_{2}, \dots, s_{n}\}$ 和 $A_{2} \subseteq \{t_{1}, t_{2}, \dots, t_{r}\}$ 且 $|A_{1}| + |A_{2}| = n + 1$ ，使 $K$ 是 $G$ 的一切连接 $A_{1}$ 中结点和 $A_{2}$ 中结点的边的集合。利用 $\mathcal{X}$ 的这种描述，应用定理12.5.2，我们可以写成

$$
\begin{array}{r c l r} \max  & \min  & c _ {j i i} = & \min  \\ i _ {1} i _ {2} \dots i _ {n} & 1 \leq j \leq n & A _ {1}, A _ {2} \subseteq \{1, 2, \dots , n \} & i \in A _ {1} \\ & & | A _ {2} | + | A _ {2} | = n + 1 & j \in A _ {2} \end{array}
$$

其中右端的极小值是取遍 $\{1, 2, \dots, n\}$ 的所有使 $|A_1| + |A_2| = n + 1$ 的子集 $A_1, A_2$ 后确定的。定理12.5.2的证明提供了一个确定最优瓶颈分配的算法。

# 练习

1. 给出 $2 \times 2$ 优先级矩阵，对于该矩阵人对工作的两种分配都是稳定的，  
2. 假设在人对工作的择优分配问题中，存在一个人 $a$ 和一项工作 $A$ 使 $a$ 评价 $A$ 为第一和 $A$ 评价 $a$ 为第一。证明在任何稳定分配中， $a$ 被分配于 $A$ .  
3.对于(12.1.2)中给出的优先级矩阵，确定所有稳定分配  
4. 在3个人和3种工作的择优分配问题中，有几种稳定分配的可能性吗？  
5. 优先级矩阵有怎样的特性才能有稳定分配，使其中每个人都得到他或她的第一种选择？  
6. 可以把优先级矩阵看成人的和工作的优先级矩阵的并置。假设这些矩阵分别是

$$
\left( \begin{array}{c c c c} 1 & 2 & 3 & \dots & n \\ n & 1 & 2 & \dots & n - 1 \\ \dots & \dots & \dots & \dots & \dots \\ 3 & 4 & 5 & \dots & 2 \\ 2 & 3 & 4 & \dots & 1 \end{array} \right) \left( \begin{array}{c c c c} n & n - 1 & n - 2 & \dots & 1 \\ 1 & n & n - 1 & \dots & 2 \\ \dots & \dots & \dots & \dots & \dots \\ n - 2 & n - 3 & n - 4 & \dots & n - 1 \\ n - 1 & n - 2 & n - 3 & \dots & n \end{array} \right).
$$

证明至少有 $n$ 种稳定分配.

7.对下面优先级矩阵

$$
\begin{array}{c c c c c} & A & B & C & D \\ a & 1, 3 & 2, 3 & 3, 2 & 4, 3 \\ b & 1, 4 & 4, 1 & 3, 3 & 2, 2 \\ c & 2, 2 & 1, 4 & 3, 4 & 4, 1 \\ d & 4, 1 & 2, 2 & 3, 1 & 1, 4 \end{array}
$$

应用延缓接受算法求稳定分配

8. 证明在练习7中的优先级矩阵中，恰好存在一种稳定分配。  
9. 说明存在稳定分配中，使任何人或任何工作都得不到它的第一个

选择.（提示：用练习7和8.）

10.对下面优先级矩阵

$$
\begin{array}{c c c c} & A & B & C \\ \hline a & 1, 3 & 2, 2 & 3, 1 & 4, 3 \\ b & 1, 4 & 2, 3 & 3, 2 & 4, 4 \\ c & 3, 1 & 1, 4 & 2, 3 & 4, 2 \\ d & 2, 2 & 3, 1 & 1, 4 & 4, 1 \end{array} ,
$$

应用延缓接受算法求稳定分配

11. * 证明在 $n$ 个人和 $n$ 种工作的择优分配问题中应用延缓接受算法至多需要 $n^3 - 2n + 2$ 步，验证练习10中当 $n = 4$ 时达到了这个界限。  
12. 试把定理12.1.1和12.1节的延缓算法推广到 $m$ 个人和 $n$ 种工作的情形，这里 $m \leqslant n$ .  
13. * 试把 12.1 节的理论推广到学院招生问题: 有 $n$ 个学生申请 $m$ 个学院的某几个或全部, 这里第 $i$ 个学院有 $q_{i}$ 个学生的定额. 每个学生确定了对他所申请的学院的优先次序; 每个学院确定了对它提出申请的学生的优先次序, 首先要去掉在任何情况下即使在定额不满时也不接纳的申请. 我们假定在每种情况下都没有任何约束. 试定义稳定分配并推广定理 12.1.1 与 12.1.2 以及 12.1 节的延缓接受算法.  
14. 证明如果一个商人喜爱他自己的货物胜于所有其它的货物，则在任何核心分配中，他得到他自己的货物。  
15. 试构造经商问题中的例子，其中在每种核心分配中恰有一商人得到他的第一种选择。  
16. 设商人优先级阵列为

$$
\begin{array}{c} T _ {1} T _ {2} T _ {3} \\ T _ {1} \left[ \begin{array}{l l l} 2 & 1 & 3 \\ 3 & 2 & 1 \\ 3 & 1 & 3 \end{array} \right], \end{array}
$$

在这个经商问题中恰有2个核心分配.其中哪一个结果是由定理12.2.1证明中得到的算法得到的？

17.假设有一商人把他自己货物评价为第 $k$ 等，证明在任何核心分配

中，该商人得到他自己货物或更好的货物，

18. 验证用定理12.2.1的证明中的算法得到核心分配中，至少一商人得到他的第一个选择，证明可能有一个核心分配，其中没有任何商人得到他的第一选择。  
19. 证明在经商问题中存在一个核心分配，其中任何人得到他的第一选择当且仅当由定理12.2.1的证明所构造的有向图 $D_{\mathfrak{A}}$ 是由两两不相交的基本圈组成的。  
20. 试构造由下面阵列

$$
\left. \begin{array}{c c c c c c c c} T _ {1} & T _ {2} & T _ {3} & T _ {4} & T _ {5} & T _ {6} & T _ {7} \\ T _ {1} & 2 & 3 & 1 & 4 & 7 & 5 & 6 \\ T _ {2} & 1 & 6 & 4 & 3 & 2 & 7 & 5 \\ T _ {3} & 2 & 7 & 3 & 5 & 1 & 4 & 6 \\ T _ {4} & 3 & 4 & 2 & 7 & 1 & 6 & 5 \\ T _ {5} & 1 & 3 & 4 & 2 & 5 & 7 & 6 \\ T _ {6} & 2 & 4 & 1 & 5 & 3 & 7 & 6 \\ T _ {7} & 7 & 3 & 4 & 2 & 1 & 6 & 5 \end{array} \right\}
$$

所确定的商人优先级的核心分配.

21. 用定理12.2.1的证明中所述的算法，证明一旦有一个有向图 $D_{k}$ 是由两两不相交基本圈所组成，则核心分配就确定了。  
22. 考虑补给需要量为 $a_{1}, a_{2}, \cdots, a_{n}$ 和消费需要量为 $b_{1}, b_{2}, \cdots, b_{n}$ 的运输问题，这里 $a_{1} + a_{2} + \cdots + a_{n} < b_{1} + b_{2} + \cdots + b_{n}$ . 证明利用引进补给需要量为 $(b_{1} + b_{2} + \cdots + b_{n}) - (a_{1} + a_{2} + \cdots + a_{n})$ 的虚构补给仓库使得从虚构补给仓库到消费仓库的单位运输成本全为零，我们就得到一个运输问题，其解给出了把原来问题所有补给需要量以最小成本运送到消费仓库并且没有一个消费仓库超量的运输计划.  
23.类似于上题叙述并求解当 $a_1 + a_2 + \dots + a_n > b_1 + b_2 + \dots + b_r$ 的问题.  
24. 考虑补给量为 2, 4, 5 和消费量为 3, 4, 4 以及运输成本矩阵

$$
C = \begin{array}{c c c} 2 & 3 & 1 \\ 3 & 2 & 4 \\ 2 & 1 & 3 \end{array}
$$

的运输问题，考虑运输计划 $X = \begin{bmatrix} 1 & 2 & 1 \\ 1 & 1 & 3 \end{bmatrix}$ 。用定理12.3.2证明的方法求一个运输计划Y且其偶图无圈，使得 $C * Y \leqslant C * X$ 。

25. 考虑补给量为3，8，8和消费量为2，4，5，6的运输问题，试求一个运输计划，其偶图给出如下，或确定任何计划都不存在，

![](images/11e97ce0695deb336a091475133c9736728abbe9d5bd2d8767490c076e04e2c9.jpg)

![](images/b8d2fe7147d5b8829ed060834bff0a00c0d33bcceccd0e5c786654bcd072e0eb.jpg)

23. 用上面偶图求解练习25.

27. 考虑一运输问题和运输计划 $X$ 其偶图至少有一个圈，证明对于该问题存在无限多个运输计划，其偶图与 $X$ 的偶图相同。（对照定理12.3.3）  
28. 设 $X$ 是补给量为 $a_{1}, a_{2}, \cdots, a_{n}$ 和消费量为 $b_{1}, b_{2}, \cdots, b_{n}$ 以及运输成本矩阵为 $C = (c_{ij})$ 的运输问题的运输计划。令 $N$ 是 $X$ 的非零运送量的个数，证明使用定理12.3.2的证明中所给出的方法，至多需要 $N - m - n + 1 \leqslant mn - m - n + 1$ 次重复就可以求出运输计划 $Y$ ，其偶图无圈，并满足

$$
C * Y \leqslant C * X.
$$

29. 试用西北角准则，求下列补给量和消费量的运输计划：

(a)3，6，7和4，4，8.   
(6)10, 20和5，5，5，5，5，5.   
(c)3，3，3和3，3，3.   
(d)1，1，1，1，5和2，2，2，2.

30. 试用东北角准则，求练习29给出的补给量和消费量的运输计划。

31. 证明用西北角准则总能得到一运输计划其偶图无圈。

32. 证明按照证明定理12.3.3所描述的一般过程给出的所有运输计划其偶图无圈。

33. 证明偶图无圈的运输计划的非零运输数的形式由（12.3.9）和（12.3.10）给出。

34. 考虑补给量为 2, 4, 6, 7 和消费量为 3, 3, 4, 4, 5 的退化运输问题。试构造一个运输计划其偶图连通又无圈的例子来。

35. 考虑补给量为 $a_{1}, a_{2}, \cdots, a_{n}$ 和消费量为 $b_{1}, b_{2}, \cdots, b_{n}$ 及成本矩阵为 $C = \{c_{ij}\}$ 的运输问题，令 $X = \{x_{ij}\}$ 是使得存在数 $u_{1}, u_{2}, \cdots, u_{m}$ 和 $v_{1}, v_{2}, \cdots, v_{n}$ 以及对于 $i = 1, 2, \cdots, m$ 和 $j = 1, 2, \cdots, n$ 有 $u_{i} + v_{j} \leqslant c_{ij}$ 的运输计划，而且 $C \cdot X = \sum_{i=1}^{m} a_{i} u_{i}$

+∑bovj. 证明X是最优运输计划，

36. 考虑退化运输问题。对于 $e$ 运输问题，设 $X_{\varepsilon}$ 是用西北角准则得到的运输计划。设 $X$ 是原问题用西北角准则得到的运输计划。证明 $X$ 是 $X_{\varepsilon}$ 中用 0 代替 $e$ 而得到的。试举出具有下面性质的退化运输问题的例子。这个性质是：对于 $e$ 运输问题存在两个运输计划 $X_{\varepsilon}$ 和 $Y_{\varepsilon}$ ，使得用 0 代替 $X_{\varepsilon}$ 和 $Y_{\varepsilon}$ 中的 $e$ 得到同一计划 $X$ 。

37. 考虑退化运输问题，对于 $\varepsilon$ 运输问题，设 $X_{\varepsilon}$ 是一运输计划其偶图无圈。证明 $X_{\varepsilon} = X + \varepsilon A$ ，其中 $X$ 是已知问题的运输计划其偶图无圈，而 $A$ 是整数矩阵。

38.（续练习37）证明若 $X_{\varepsilon}$ 具有 $\varepsilon$ 运输问题的最小成本，则对所给定的运输问题 $X$ 有最小成本，

39. 试解补给量为 3, 8 和消费量为 5, 6 以及运输成本矩阵为

$$
C = \left[ \begin{array}{l l} 2 & 3 \\ 2 & 1 \end{array} \right] \text {的 运 输 问 题}.
$$

40.试求解补给量为10，10和消费量为4，7，9以及运输成本矩

阵为 $C = \begin{bmatrix} 4 & 3 & 2 \\ 2 & 3 & 5 \end{bmatrix}$ 的运输问题。

41.求解补给量为6，7，8和消费量为5，5，11以及运输成本矩阵为 $C = \left[ \begin{array}{lll}4 & 3 & 2\\ 1 & 2 & 3\\ 3 & 4 & 1 \end{array} \right]$ 的运输问题.

42. 试解补给量为 6, 7, 8, 9 和消费量为 5, 5, 5, 15 以及运输成本矩阵为 $C = \begin{pmatrix} 3 & 2 & 1 & 4 \\ 2 & 3 & 5 & 1 \\ 1 & 5 & 2 & 3 \\ 4 & 3 & 2 & 4 \end{pmatrix}$ 的运输问题.

43. 试解补给量为5，6，7和消费量为5，6，8以及运输成本矩阵为 $C = \begin{bmatrix} 3 & 1 & 2 \\ 2 & 3 & 2 \\ 1 & 1 & 2 \end{bmatrix}$ 的运输问题。

44. 试解补给量为 4, 4, 4 和消费量为 4, 4, 4 以及运输成本矩阵为 $C = \begin{bmatrix} 2 & 3 & 1 \\ 2 & 2 & 1 \\ 1 & 2 & 1 \end{bmatrix}$ 的运输问题.

45. 试解补给量为3，4，6，9和消费量为3，6，6，10以及运输成本矩阵为 $C = \begin{bmatrix} 2 & 1 & 3 & 2 \\ 4 & 2 & 1 & 3 \\ 2 & 2 & 2 & 4 \\ 1 & 1 & 3 & 3 \end{bmatrix}$ 的运输问题。

46. 考虑 $n$ 个补给量为 1, 1, ..., 1 和 $n$ 个消费量为 1, 1, ..., 1 的退化运输问题。试求用西北角准则于 $e$ 运输问题所得到的运输计

划。

47. 用南东角准则于练习46并比较之.

48. 用北京角准则于练习46并比较之.

49. 考虑补给量为 1, 1, 1 和消费量为 1, 1, 1 的退化运输问题。证明对于 $\varepsilon$ 运输问题，存在 18 个运输计划，且其偶图皆无圈。证明在这些运输计划中用 0 代替 $\varepsilon$ ，我们重复 3 次得到原问题的每一个运输计划。

2 3 150.试求评价矩阵为 $C = \begin{bmatrix} 2 & 4 & 2\\ 2 & 3 & 2\\ 1 & 3 & 2 \end{bmatrix}$ 的最优分配问题

51.试解评价矩阵为 $C = \left[ \begin{array}{llll}2 & 3 & 1 & 4\\ 3 & 2 & 1 & 3\\ 0 & 2 & 2 & 3\\ 4 & 3 & 4 & 2 \end{array} \right]$ 的最优分配问题，

52. 试把下面问题表述为最优分配问题并解之：有 5 个人和 6 种工作。第一个人胜任工作 2，3，4 和 5，第二个人胜任工作 1，3，5 和 6，第三个人胜任工作 5 和 6，第四个人胜任工作 6 和第五个人胜任工作 5 和 6。试求有多少个分配方案，使每人都胜任分配给他的工作。当然，不同的两个人不能被分配同一种工作。

53. 令 $\mathfrak{S}$ 是有限集 $E$ 上的杂乱集且空集 $\varnothing$ 是其成员，证明 $\mathfrak{S}$ 是 $\mathfrak{S}$ 的唯一成员。

54. 设 $\mathfrak{S}$ 是有限集 $E$ 上杂乱集且空集 $\mathfrak{X}$ 是其唯一成员，证明使 $(\mathfrak{S}, x)$ 为 $E$ 上的分块组的杂乱集 $\mathfrak{X}$ 是空集（即， $\mathfrak{X}$ 没有任何成员，甚至连 $\mathfrak{X}$ 也没有）。这时定理12.6.2应如何解释？

55. 设 $\mathfrak{S}$ 是有限集 $E$ 的子集的集族，且 $\mathfrak{S}$ 的所有成员含有同样数目的元素，证明 $\mathfrak{S}$ 是 $E$ 上的杂乱集。

56. 设 $n$ 是正整数且 $E = \{1, 2, \dots, 2n - 1\}$ . 令 $\pi$ 是由 $E$ 上所有有 $n$ 个元素的子集组成的杂乱集. 确定 $E$ 上的唯一杂乱集 $\pi$ , 使得

（，）是E的分块组，对于这种特殊分块组解释定理12.6.2.

57. 设 $n$ 是正整数且 $E = \{1, 2, \dots, 2n\}$ . 令 $\pmb{\pi}$ 是由 $E$ 上所有 $n$ 个元素的子集组成的杂乱集，确定 $E$ 上唯一的杂乱集 $\pmb{x}$ ，使（9，）是 $E$ 的分块组.  
58. 设 $G$ 是简单图， $E$ 是 $G$ 的结点集合。令 $\mathcal{P}$ 是 $E$ 上由 $G$ 的所有边组成的杂乱集。试描述 $E$ 上唯一的杂乱集，使 $(\mathcal{P}, \mathcal{P})$ 是 $E$ 的分块组。  
59. 设 $G$ 是下面所画出的图，边上的数字表示它们的容量

(a) 把容量解释为每小时交通流量，利用在定理12.5.2证明中提供的算法求出连接 $u$ 和 $v$ 且容许最大每小时交通流量的基本链。  
(b) 把容量解释为高度, 使用在定理12.5.2证明中提供的算法求出连接 $u$ 和 $v$ 且可达到最小的极大高度的基本链,

![](images/ebf16a8350ac3a17a4cc87791836c900708fb4211711d3495087b59dd0923e19.jpg)

60. 设 $(\pi, \pi)$ 是练习56或57的分块组，对这个分块组描述定理12.5.2证明中的算法。

3 2 2 61.试确定速率由阵列 $\left[ \begin{array}{lll}3 & 2 & 2\\ 3 & 4 & 3\\ 2 & 2 & 1 \end{array} \right]$ 给出的最优瓶颈分配  
62. 使用定理12.5.2证明中的算法，确定速率由阵列

给出的最优瓶颈分配.

63. 设 $G$ 是一个完全偶图 $Kn, n, E$ 是 $G$ 的边集。又令 $\mathcal{P}$ 是 $E$ 上由所有 $t$ 条边的匹配组成的杂乱集，利用定理 8.2.2 去确定 $E$ 上的唯一杂乱集 $\pi$ ，使 $(\mathcal{P}, \pi)$ 是 $E$ 的分块组。  
64. 试把8.3节所讨论的分配问题与练习63中的瓶颈分配问题联系起来（如果一个人胜任某工作，则该人对这个工作的速率是1，否则为0。）  
65. * 设 $\mathcal{P}$ 和 $\mathcal{E}$ 是 $E$ 上的杂乱集并且对于所有 $E$ 上的实值函数 $f$ ，方程（12.5.8）成立，证明（ $\mathcal{P}, \mathcal{E}$ ）是 $E$ 上的分块组

# 文献目录

Claude Berge, Graphs and Hypergraphs. New York: Elsevier, 1973.   
R.G. Busacker and T. L. Saaty. Finite Graphs and Networks: An Introduction with Applications. New York: McGrawHill, 1965.   
G.B.Dantzig and B.C.Eaves.Studies in Optimization,MAA Studies in Mathematics,Vol.10.Washington:Mathematics Association of America,1974.   
Shimon Even, Algorithmic Combinatorics New York: Macmillan, 1973.   
L.R.Ford, JR.and D.R.Fulkerson, Flows in Networks, Princeton, New Jersey: Princeton University Press, 1982. Frank.Harary, Graph Theory.New York: Addison-Wesley, 1969.   
C.L. Liu, Topics in Combinatorial Mathematics, Washington, Mathematics Association of America, 1972.   
L. Mirsky, Transversal Theory, New York: Academic, 1971.

O.Ore, The Four Color Problem. New York: Academic, 1867.

H.J.Ryser, Combinatorial Mathematics, Carus Mathematical Monograph No. 14. Washington: Mathematics Association of America, 1963.

N.Vilekin, Combinatorics, New York: Academic, 1971.

# 选题解答

# 第一章

3 不能. $4f(n) = f(n - 1) + f(n - 2);f(12) = 233$

5 11. 13 不能. 15 国家 1, 2 和 10 之中每两个国家都有公共边界，因此至少要有 3 种颜色，用红、白和蓝色有 6 种不同着色.

4 3 2 1 4 2 1 3 3 1 2 4 2 3 1 1 3 4 2

段上的和必须同为 $(1 + 2 + \dots + 7) / 3$ ，但这不是一个整数.

# 第二章

2.3.5参看D.O.Shklarsky，N.N.Chentzou和I.M.Yaglom, The USSR Olympiad Problem Book, San Francisco:Freeman,1962,PP.169-171,12 (a)应用一个边长为1的等边三角

形的如下分割：

![](images/e49d03eb163be6fe1cde1ebeb3a994e8329fc7e800066657da2a76ef1e0c397c.jpg)

16 所选的两个整数必须是相邻的，因而有一个最大的公因子是 1.

# 第三章

1 $\emptyset: 5^4, \{(a)\}: 5 \times 4 \times 3 \times 2; \{(b)\}: 2 \times 5^3, \{(a)\}$

(b)： $2 \times 4 \times 3 \times 2$ 2 52！.3 $52 \times 51 \times 50 \times 49 \times 48$ ;

C（52，5）.4 （a） $5\times 3\times 4\times 2$ ，（b） $3\times 2\times 2$ ，512.

6 $(6\times 5 + 3\times 6\times 5 + 3\times 7\times 6\times 5) + 7\times 7\times 6\times 5\times 4$

+7×7×6×5×4×3+7×7×6×5×4×3×2+7×7！,

7 6！5！.8 10×9×9！.9 C(12.2)C(10，2)+

$C(12,3)C(10,1) + C(12,4)$ 从前面答案减去 $C(11,1)$

$C(9,1) + C(11,2)$ 10 $C(20,3) - 2\times 17 - 17\times 16$

-18.12 C(8,5)5!C(8,4)4!C(7,5)5!.13 C(25,

3）；C（25，4），158！：（8！）².17（.8！8！）/（3！

51）：C（12.8）\*（8！8！）/（3！5！），18 C（16，9），

$C(10,9) - C(7,4)C(8,4),20.C(15,3)\times 3!$

C（12.3）.22 $(n_{1} + 1)(n_{2} + 1)\dots (n_{4} + 1).$ 29 35124;

1254.31 35168274的逆序序列是2，4，0，4，0，0，1，

0.32 逆序序列为2，5，5，0，2，1，1，0的排列是48165723.

333 (a) 1, (b) 5, (c) 14.38 2, 3, 4, 7, 8, 9; 2,

3. 4. 6, 8, 10.

# 第四章

6 $\binom{18}{5}3^{5}(-2)^{18};0.\quad 7\quad\sum_{k=0}^{n}\binom{n}{k}r^{k}=(1+r)^{n}.$

8 在二项式定理中取 $x = -1$ 和 $y = 3$ . 10 微分二项式定理所给出的公式 $(1 + x)^{n}$ , 然后用 -1 代替 $x$ . 11 从 0 到 1 积分二项式定理所给出的公式 $(1 + x)^{n}$ . 12 $1 / (n + 1)$ . 14 $a = 6$ , $b = 6$ , $c = 1$ , $6 \binom{m+1}{4} + 6 \binom{m+1}{3} + \binom{m+1}{2}$ . 20 在公式中代入 $x_{1} = x_{2} = \cdots = x_{l} = 1$ .

10 10！ 3 4 0 2

# 第五章

1 5,334. 3 10.000=（190+21）+4=9.883.4 455-

$(120 + 185 + 120 + 84) + (20 + 10 + 4 + 20 + 10 + 4) = 34$ 691

$-(21 + 15 + 45) + (3 + 1) = 14.7120 - (21 + 21 + 21) = 57.$

9 首先作变量代换 $y_{1} = x_{1} - 1$ ， $y_{2} = x_{2}$ ， $y_{3} = x_{3} - 4$ ， $y_{4} = x_{4} -$

2.10 8！-（4）7！+（4）6！-（3）5！+（4）4！.12

$(\frac{8}{4})D_{\bullet}$ 14 $(a)D_{\tau}.$ （20 $(b)7! - D_{\tau},(c)7!$ $(D_{\text{平}} + 7D_{\text{平}})$

9! 7！ 6！ 8！ 16 34121

$$
+ \frac {6 !}{4 !} + \frac {5 !}{3 !}) - 3!
$$

# 第六章

（a） $f(2n) = 1$ ，（c） $(-1)^{n}f(n - 1) + 1$

（e） $f(n - 1)f(n + 3) + 3$ ，2 $G(n) = G(n - 1) +$

$G(n - 2)$ ， $h(n) = 0$ 当 $n$ 是偶数： $h(n) = 4^{(n - 1) / 2}$ 当 $n$ 是奇数，7通解是 $c_{1} + c_{2}3^{n} + c_{3}(-3)^{n};\quad c_{i} = -\frac{1}{4},$

$c_{2} = \frac{1}{3}, c_{3} = -\frac{1}{12}$ . 通解是 $c_{1} + c_{2}n + c_{3}(-2)^{n}$

$c_{1} = \frac{8}{9}, c_{2} = -\frac{6}{9}, c_{3} = \frac{1}{9}$ 10 通解是 $c_{1}(-1)^{n}+$

$c_{2}2^{n} + c_{3}n2^{n} + c_{4}n^{2}2^{n}$ 11 (a) $h(n) = 3^n$ （c） $h(n)$

$= ((-1)^{n + 2} + 1) / 2$ 3 $\begin{array}{r}3\left( \begin{array}{c}m + 1\\ 1 \end{array} \right) + 3\left( \begin{array}{c}m + 1\\ 2 \end{array} \right) \end{array}$

+4（m+1）；15 (a)p(x)=x³-3x+1.

# 第七章

1 $V((1 / - cx))$ 2 $\frac{1}{1 + x},(b)(1 - x)^2,$   
（d） $\frac{1}{x^4}\left(\frac{1}{(1 - x)^2} -\dots (1 + 2x + 3x^2 +4x^3)\right)_{1}(e)e^{x},$

3 $(a)\left(\frac{x}{1 - x^2}\right)^4;(b)\left(\frac{1}{1 - x^3}\right)^4;(c)\frac{1 + x}{(1 - x)^2}.$

4 $[x]_n = x(x - 1)\dots (x - n + 1).$ （20 $6(a)a_{n} = 0$ 当 $n$ 是偶数； $a_{n} = 4(n - 1) / 2$ 当 $n$ 是奇数； $(c)a_{n} = \frac{1}{12} (-3 + 4(3)^{n}$

$$
\begin{array}{l} - (- 3) ^ {n}), n = 0, 1, 2, \dots , (e) a _ {n} = \frac {8}{9} - \frac {6}{9} n + \frac {1}{9} (- 2) ^ {n}. \\ n = 0, \quad 1, 2, \dots , \quad 8 x (x ^ {2} + 4 x + 1) / (1 - x) ^ {4}. \\ 1 1 x ^ {2} / (1 - x) ^ {8}. \quad 1 4 1 / (1 - x). \quad 1 5 (1 + x) ^ {a}. \\ 1 6 \quad (a) \left(x + x ^ {8} / 3! + x ^ {6} / 5! + \dots\right) k; \quad (b) \left(e ^ {x} - 1 - x - \right. \\ x ^ {2} / 2! - x ^ {3} / 3! \quad) k: (d) (1 + x) (1 + x + x ^ {2} / 2!) \dots (1 + x + \\ x ^ {2} / 2! + \dots + x k / k!). \\ \end{array}
$$

# 第八章

1 除 $(a)$ 以外，个个都有代表组，仅仅 $(b)$ 有相异代表组。2

2.3 只有 $\left|A_{1} \cup A_{2} \cup \cdots \cup A_{k}\right| \geqslant k$ 不满足。4（a）4；（b）5；（c）5。13 $A_{1} = \{a, b, e\}$ ， $A_{2} = \{b, c, d\}$ ， $A_{3} = \{e\}$ ， $A_{4} = \{e, f\}$ ， $A_{5} = \{e, f\}$ 。 $A_{1}, A_{2}, A_{3}, A_{4}; A_{1}, A_{2}, A_{3}, A_{4}$ 和 $A_{1}, A_{2}, A_{3}, A_{5}$ 的每一组都有相异代表组。14 关于 $M$ 的交错链是 $x_{3}, y_{1}, x_{1}, y_{3}$ ，并且它产生匹配 $\{(x_{2}, y_{2}), [x_{3}, y_{1}], [x_{1}, y_{3}]\}$ 。22 $\left|A_{1} \cup A_{2} \cup A_{4} \cup A_{5} \cup A_{7} \cup A_{9} \cup A_{10}\right| + (10 - 7) = 5 + 3 = 8$ ； $A_{1}, A_{2}, A_{3}, A_{4}, A_{6}, A_{7}, A_{8}$ 有相异代表组1，5，2，10，8，7，13，6。于是由定理8.1.3，8是有相异代表组的集合的最大个数。23 对于（a），5：5条边的匹配是 $\{[x_{2}, y_{3}], [x_{2}, y_{6}], [x_{6}, y_{8}], [x_{7}, y_{8}]\}$ 。覆盖边的5

个结点集合 $S$ 是 $\{x_{1}, x_{2}, x_{3}, x_{4}, x_{7}, y_{2}\}$ 。

# 第九章

3 $-0 = 0, -1 = 4, -2 = 3, -3 = 2, -4 = 1; 1^{-1} = 1,$ $2^{-1} = 3, 3^{-1} = 2, 4^{-1} = 4, 6$ $Z_{6}$ 中有乘法逆元的元素：1，5. $Z_{8}$ 中有乘法逆元的元素：1，3，5，7.7 $Z_{n}$ 的元素a有乘法逆元当且仅当a与n的最大公因子是1.8（b）8；（d）10；（f）11；（h）4；（j）17；（l）12.15 （a）-z-7；（c）

$$
- \frac {2}{1 1} z + \frac {1}{1 1}; (d) \frac {1}{5}. 1 6 (a) z; (b) z ^ {2} + 1; (d) z ^ {2} + z.
$$

17 $z^2 + z + 2$ 和 $z^2 + 2z + 2$ 是 $Z_3[z]$ 中不可约多项式，因为在 $Z_3$ 中两者都没有根。18 $(a)9z + 4; (b)8z + 10; (c)9z + 1$ 。25 $y = 3x + 6$ 。27 $y = zx + (z^2 + z + 1)$ 。

30

$$
\begin{array}{l} \left( \begin{array}{c c c c c c c} 7 & 6 & 5 & 4 & 3 & 2 & 1 \\ 6 & 5 & 4 & 3 & 2 & 1 & 7 \\ 5 & 4 & 3 & 2 & 1 & 7 & 6 \\ 4 & 3 & 2 & 1 & 7 & 6 & 5 \\ 3 & 2 & 1 & 7 & 6 & 5 & 4 \\ 2 & 1 & 7 & 6 & 5 & 4 & 3 \\ 1 & 7 & 6 & 5 & 4 & 3 & 2 \end{array} \right), \\ \left( \begin{array}{c c c c c c c} 7 & 1 & 2 & 3 & 4 & 5 & 6 \\ 6 & 7 & 1 & 2 & 3 & 4 & 5 \\ 5 & 6 & 7 & 1 & 2 & 3 & 4 \\ 4 & 5 & 6 & 7 & 1 & 2 & 3 \\ 3 & 4 & 5 & 6 & 7 & 1 & 2 \\ 2 & 3 & 4 & 5 & 6 & 7 & 1 \\ 1 & 2 & 3 & 4 & 5 & 6 & 7 \end{array} \right). \\ \end{array}
$$

39

$$
\left( \begin{array}{c c c c c c} 1 & 3 & 4 & 6 & 5 & 2 \\ 3 & 4 & 1 & 5 & 2 & 6 \\ 5 & 6 & 3 & 2 & 4 & 1 \\ 2 & 5 & 6 & 4 & 1 & 3 \\ 4 & 1 & 2 & 3 & 6 & 5 \\ 6 & 2 & 5 & 1 & 3 & 4 \end{array} \right)
$$

41 考虑阵列 $\left\{ \begin{array}{l} C_{11}C_{12}C_{13}C_{14}C_{15}C_{16}C_{17} \\ C_{21}C_{22}C_{23}C_{24}C_{25}C_{26}C_{27} \\ C_{31}C_{32}C_{33}C_{34}C_{35}C_{36}C_{37} \end{array} \right\}$ 中的21个元素.

下面70个三元组形成一个21阶Steiner三元组：（i）由每列中三个元素形成7个三元组，（ii）由一行中3个元素形成21个三元组，只要含有这3个元素的那些列对应着7阶Steiner三元组的7个三元组的一个，（iii）由出现在不同行和不同列的3个元素而得到的42个三元组，只要含有这3个元素的那些列对应着7阶Steiner三元组中的7个三元组之一。

# 第十章

2 不. 12 仅仅第二个图有一个Euler 圈. 13 $K_{n}$ 有 Euler 圈当且仅当 $n$ 是奇数. 21 对于 $n = 2$ ，一个没有，对于 $n > 2$ ，有 $n!$ ，（ $n - 1$ ）或（ $n - 1$ ）/2 个，这取决于人们的观点. 22 没有这种 Hamilton 路存在. 45 生成树的最小成本是 18.51 正，正，中. 57 一个简单例子是有 2 个结点 $x, y$ 和 2 个孤 $(x, y)$ （ $x, y$ ）的有向图.

# 第十一章

1 见386页上图. 2 仅当 $G$ 是完全图 $Kn$ . 3 2; 3; 4, 4 2; 3; 3.8 考察练习3中所找到的那种着色. 10从一个偶图利用一边连接两结点得到这种图12 18 $(k = 3)$ ; 336 $(k = 4)$ ; 6! $(k = 6)$ . 14 4. 15 $n - 1$ . 16 $n - 2$ . 25 对直线个数施行归纳法. 26 对圆的个数

![](images/1d70b1b960b825d2e16738ba45501fd3200f30e321f17b6ab88c2e15f46aeb65.jpg)

施行归纳法. 32 2.33 $\kappa (Km,n) = \min \{m,n\}$ . 35 1.

36 3:4;3.37 0,2,1.4 $\lambda (Kn) = n - 1$   
$\lambda (K_{m,n}) = \min \{m,n\} .44\kappa (G)\geqslant 3.504,2,8.$   
55 2；3；2.62 $A = \{a,b,c\} .B = \{a,b\}$

![](images/c13015031a973f74a9ae87125c703458a82f0e28b98d2778a0af0141dda969ce.jpg)

# 第十二章

1 $\left[ \begin{array}{ll}1,2 & 2,1\\ 2,1 & 1,2 \end{array} \right].$ 存在3种稳定分配：a对A，b对B，c对C；a对C，b对A，c对B；a对B，b对C，c对A.5没有两个人有同样的第一种选择．6对于 $k = 1$ ，2，…，n，每个人得到他的第k种选择的分配是稳定的〔因为每种工作得到它的第（n-k+1）种选择）.7 a对C,b对D,c对A.d对B.10 a对C，b对D,c对A,d对B.15令优先级阵列是

$$
\left. \begin{array}{c c c} \tau & \tau_ {1} & \tau_ {2} \\ \tau_ {1} & 1 & 2 \\ \tau_ {2} & 1 & 2 \\ \tau_ {3} & 1 & 2 \end{array} \right\}
$$

20

![](images/d165815e570223441735c43f462e96ae205f9ef59201f1a36f0b27b7aac22ad0.jpg)

![](images/530fa57fc7673c98e857bf2b06661b8e50201853e593daaa98d3e4fe4e0f8844.jpg)

![](images/b4752158c5e642def4e3e81c6bf8bbdaa65cac6c315289e3d59237e4e5d66cb6.jpg)

![](images/604d251bd82cd25f21fd3ef5a74b8febebc9a825b84fd603b4ba60b2a85e054f.jpg)

24

$$
\left( \begin{array}{c c c} 2 & 0 & 0 \\ 0 & 4 & 0 \\ 1 & 0 & 4 \end{array} \right)
$$

25

$$
\left[ \begin{array}{c c c c} 0 & 0 & 3 & 0 \\ 0 & 4 & 0 & 2 \\ 2 & 0 & 2 & 4 \end{array} \right]
$$

26 运输计划不存在，

29

$$
\begin{array}{c c c c c} (a) & 1 & 4 & 1 \\ & 0 & 0 & 7 \end{array} ,
$$

$$
(b) \left\{ \begin{array}{c c c c c c c} 0 & 0 & 5 & 5 & 5 & 5 \end{array} \right.
$$

30

$$
(a) \left\{ \begin{array}{l l l} 0 & 0 & 3 \\ 0 & 1 & 5 \\ 4 & 3 & 0 \end{array} \right.,
$$

$$
\left( \begin{array}{l} b \\ 5 \end{array} \right) \left( \begin{array}{l l l l l l} & & & & & \\ 5 & 5 & 5 & 5 & 0 & 0 \end{array} \right).
$$

39

$X = \left[ \begin{array}{ll}3 & 0\\ 2 & 6 \end{array} \right]$ 是最优运输计划，41 $X = \begin{array}{ccc}5 & 2 & 0\\ 0 & 0 & 8 \end{array}$ 是最优运

输计划.

$$
4 4 \quad X = \left( \begin{array}{c c c} 0 & 0 & 4 \\ 0 & 4 & 0 \\ 4 & 0 & 0 \end{array} \right) \text {是 最 优 运 输 计 划}.
$$

$$
\begin{array}{l} 4 6 \left( \begin{array}{c c c c c c c} 1 + e & (n - 1) e & 0 & 0 & \dots & 0 & 0 \\ 0 & 1 - (n - 2) e & (n - 2) e & 0 & \dots & 0 & 0 \\ 0 & 0 & 1 - (n - 3) e & (n - 3) e & \dots & 0 & 0 \\ \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \dots \end{array} \right) \\ 0 & 0 & 0 & 0 & \dots & 1 - e & e \\ 0 & 0 & 0 & 0 & \dots & 0 & 1 \end{array}
$$

50 1, 2, 3是最优分配. 51 4, 1, 2, 3是最优分配. 56 x = x, 57 x 是E的所有(n+1)个元素的子集的杂乱集. 58 若一个结点集合A和所有边相交并且关于这个性质是极小的, 则A是x的成员(用第八章术语,A是边的最小覆盖). 59 (a) 最大的每小时交通流量等于2. (b)在一条连接u和v的链上的最小高度是2. 61 2, 3, 1是最优瓶颈分配. 62 5, 4, 2, 1, 3是最优瓶颈分配.

# 译者的话

组合数学是一门既古老又年轻的数学分支。随着计算机的出现和广泛应用，组合数学取得迅速的发展。现今在计算机科学、空间技术、通讯网络、人工智能、物理、化学、生物学、遗传学、物质结构、概率统计、实验设计、信息编码、运筹学、电机、土木工程、建筑学、心理学、社会学、经济学、人类学、语言学及工艺美术等方面，都有一系列的应用。

R.A.Brualdi著的《组合学导引》是学习组合数学的一本良好的入门教材。作者在威斯康星大学多次讲授这门课程，本书是作者根据讲稿整理成书的。这本书内容丰富，叙述详尽，论证严谨，并附有近500道不同难度的练习题，适合初学者阅读。考虑到目前国内这方面的书籍不多，因此我们将这本书翻译出来，献给读者。

全书共分十二章，内容包括鸽笼原理，排列与组合，二项式系数，容斥原理，递归关系，生成函数，相异代表组，组合设计，图论入门，色数以及最优化等问题。王玉丰、林化夷、吴乐光同志参加本书译稿的审校，本书的出版得到华中工学院出版社的大力支持，我们表示感谢。由于我们水平有限，错误之处请读者指正。

本书的翻译得到徐利治教授的关心和支持，我们深表谢意。

译者

1982年8月

# Community Detection in Multiplex Networks

MATTEO MAGNANI, Infolab, Uppsala University, Sweden  
OBAIDA HANTEER, IT University of Copenhagen, Denmark  
ROBERTO INTERDONATO, CIRAD, UMR TETIS, France  
LUCA ROSSI, IT University of Copenhagen, Denmark  
ANDREA TAGARELLI, University of Calabria, Italy

A multiplex network models different modes of interaction among same-type entities. In this article, we provide a taxonomy of community detection algorithms in multiplex networks. We characterize the different algorithms based on various properties and we discuss the type of communities detected by each method. We then provide an extensive experimental evaluation of the reviewed methods to answer three main questions: to what extent the evaluated methods are able to detect ground-truth communities, to what extent different methods produce similar community structures, and to what extent the evaluated methods are scalable. One goal of this survey is to help scholars and practitioners to choose the right methods for the data and the task at hand, while also emphasizing when such choice is problematic.

CCS Concepts: • General and reference → Surveys and overviews; • Theory of computation → Social networks;

Additional Key Words and Phrases: Community detection, multiplex networks, multiplex community detection

# ACM Reference format:

Matteo Magnani, Obaida Hanteer, Roberto Interdonato, Luca Rossi, and Andrea Tagarelli. 2021. Community Detection in Multiplex Networks. ACM Comput. Surv. 54, 3, Article 48 (May 2021), 35 pages. https://doi.org/10.1145/3444688

# 1 INTRODUCTION

Multiplex network analysis has emerged as a promising approach to investigate complex systems. A multiplex network is a model used to represent multiple modes of interaction or different types of relationships among entities of the same type (e.g., people). This model has been used to study a large variety of systems across disciplines, ranging from living organisms and human societies to transportation systems and critical infrastructures. For example, a description of the full protein-protein interactome<sup>1</sup> involves, for some organisms, up to seven distinct modes of

<sup>1</sup>An interactome is the totality of protein–protein interactions happening in a cell.

Authors' addresses: M. Magnani, Infolab, Uppsala University, Uppsala, Sweden; email: matteo.magnani@it.uu.se; O. Hanteer and Luca Rossi, IT University of Copenhagen, Copenhagen, Denmark; emails: {obha, lucr}@itu.dk; R. Interdonato, Cirad, TETIS, Montpellier, France and TETIS, Univ. of Montpellier, APT, Cirad, CNRS, INRAE, Montpellier, France; email: roberto.interdonato@cirad.fr; A. Tagarelli, University of Calabria, Rende, Italy; email: andrea.tagarelli@unical.it.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

$\odot$ 2021 Association for Computing Machinery.

0360-0300/2021/05-ART48 $15.00

https://doi.org/10.1145/3444688

![](images/1b3443783e67106aecd2561582a4ad5c41c619d6000de4039785118058f6327f.jpg)  
Fig. 1. An example of a multiplex network with two types of interaction among five actors. This is represented as five nodes replicated in two layers. The two nodes representing the same actor (e.g., the same person) are linked by a dotted line.

interaction among thousands of protein molecules [17]. Another example is in air transportation systems when modeling the connections between airports through direct flights; here, the different commercial airlines can be seen as different modes of connection among airports [13].

Figure 1 shows a typical layered representation of a multiplex network, where each layer corresponds to a type of interaction and nodes (also called vertices) in different layers can be associated to the same actor, e.g., the same person or the same airport. Here, we adopt the term actor from the field of social network analysis, where multiplex networks have been first applied, and the term layer from recent generalizations of the original multiplex model [12, 18, 32, 39].

A core task in network analysis is to identify and understand communities, also known as clusters or cohesive groups; that is, to explain why groups of entities (actors) belong together based on the explicit ties among them and/or the implicit ties induced by some similarity measures given some attributes of these entities. Since members of a community tend to share common properties, revealing the community structure in a network can provide a better understanding of the overall functioning of the network.

Unfortunately, community detection methods for simple graphs are not sufficient to deal with the complexity of the multiplex model, for three main reasons. First, without allowing the analysis of subsets of the layers some communities may become hidden by edges in irrelevant layers. This is a common problem also in traditional multivariate data analysis, where several preprocessing methods have been developed to remove irrelevant information and algorithms have been extended to explore subsets of the data dimensions, as done by subspace clustering methods. Second, algorithms not explicitly representing the different layers cannot differentiate between different types of multiplex communities, e.g., those present on a single layer and those made of specific combinations of layers. Third, without a concept of layer it is not possible to include the same actor in different communities depending on the layer where the actor is active. In other words, community detection methods for simple graphs cannot conceptually represent (and thus discover) some types of communities that can only be defined on multilayer networks, although this does not imply that non-multilayer methods will always be outperformed by multilayer algorithms.

To address the above limitations, several community detection algorithms for multiplex networks have been recently proposed, based on different definitions of community and different computational approaches. Recent works have provided a partial overview of existing algorithms. [30] proposed some criteria to compare multi-layered community detection algorithms, but without any experimental evaluation. Similarly, Reference [7] highlighted the conceptual differences

among different clustering methods over attributed graphs, including edge-labeled graphs that can be used to represent multiplex networks, but only provided a taxonomy of the different algorithms without any experimental analysis. Reference [37] instead performed a pairwise comparison of the different clusterings produced by some existing algorithms.

This article provides a systematic review and experimental comparison of existing methods, with the aim of simplifying the choice and the setup of the most appropriate algorithm for the task at hand. We test the accuracy of the different methods with respect to some given ground truth on both synthetic and real networks, and we study their scalability in terms of the size of the network, both vertically (number of layers) and horizontally (number of actors). At the same time, we highlight weaknesses and strengths of specific methods and of the current state-of-the-art as a whole, showing how even the most sophisticated methods fail to identify some types of communities.

The focus of this survey is on algorithms explicitly designed to discover communities in multiplex networks through the analysis of the network structure. Several community detection algorithms have been proposed to deal with models related to but not compatible with the multiplex model, such as Heterogeneous Information Networks [53, 54, 55, 62] and bipartite networks [2, 22], and are not included in our article. Since we focus on network structure, graph clustering on attributed networks [6, 35, 48, 50, 51, 60, 63] is also not included in our analysis. For a survey on attributed graph clustering, we refer the reader to Reference [7].

The rest of this work is organized as follows. Section 2 provides some basic definitions used throughout the article. In Section 3, we introduce a taxonomy of existing multiplex community detection methods. Section 4 provides a theoretical comparison of the reviewed algorithms, while Section 5 presents the experimental settings and the evaluation datasets used in our experiments. The results of the experimental analysis are given in Section 6. We summarize our main findings and indicate usage guidelines emerged from our experiments in Section 7.

# 2 MULTIPLEX NETWORKS AND COMMUNITIES

A multiplex network is a special case of a multilayer network. A multilayer network is defined as a tuple $(A, L, V, E)$ , where $A$ is a set of actors, $L$ is a set of layers, and $(V, E)$ is a graph on $V \subseteq A \times L$ . Notice that this definition does not require all the actors to be present in all the layers, and allows actors to be present in some layers without having any neighbor on those layers.

In multiplex networks, $E$ is restricted to intra-layer edges, that is, an edge $((v_{1}, l_{1}), (v_{2}, l_{2}))$ is allowed only if $l_{1} = l_{2}$ . In the following, we use $a, l, v$ , and $e$ to refer to the cardinality of, respectively, $A, L, V$ , and $E$ . We use the terms vertex or node to indicate the elements of $V$ , that is, actors inside a layer.

The most common output of a community detection algorithm for multiplex networks is a set of communities $C = \{C_1, C_2, \dots, C_k\}$ such that each community contains a non-empty subset of $V$ . $C$ is a representation of the community structure of the network. Sometimes the term cluster is also used as a synonym of community, although the term community can be interpreted more broadly to also refer to the subgraph induced by its nodes, or even more broadly to indicate the real-world concept it represents, e.g., a group of people with shared norms, values or objectives in a social network. A few community detection methods discover clusters of edges instead of clusters of nodes or actors. Keeping the above considerations in mind, the term clustering is also used to refer to the set of all communities. Figure 2 illustrates different possible types of clusterings on a multiplex network.

A clustering $C$ is total if every node in $V$ belongs to at least one community, and it is partial otherwise. We also call a clustering node-overlapping if there is at least a node that belongs to more than one cluster, otherwise the clustering is called node-disjoint. Analogously, if there is at least an

![](images/433f910408c1e1402dab27d17bd47148537063d63c1189ad38d68e0a230158c7.jpg)  
(a) Total

![](images/c4674d5128bc333dc87d82cde6fb8be15930bdc0dd2063fa14cde7fae95ce884.jpg)  
(b) Partial

![](images/ac3e3cc2b644f13e96951e00c041f7b642e799282ac585433fd1e6968400d923.jpg)  
(c) Node-overlapping

![](images/714cfbf9ead715dad49d33c57ac63e2276999def63db5a12ebd9d25c84c2ee82.jpg)  
(d) Node-disjoint

![](images/c4a03fac120938a52a8b29599a5f8806d1a6a91bb68fc556152cfdef411b2ef1.jpg)  
(e) Actor-overlapping

![](images/7def58cc713bfed48500d212d244d81ce0d400fa8d40bbf78586a46682ea050f.jpg)  
(f) Actor-disjoint

![](images/7b7fe59a720901e92de4883cefbed82d762dfaaaa66db8df8aac20d827cda79e.jpg)  
Fig. 2. Different types of clustering on a multiplex network. In panel (c) the two overlapping nodes are A4_L1 and A3_L2. In panel (e) A2 is the overlapping actor.   
(a) Pillar communities

![](images/9a4029a7b528c71249548dd7271a85e49a6857624b20217de7073122be5c1154.jpg)  
(b) Semi-pillar communities   
Fig. 3. Pillar and semi-pillar multiplex community structures.

actor belonging to more than one cluster, then we call the clustering actor-overlapping, otherwise it is called actor-disjoint. Notice that a node-overlapping clustering is also actor-overlapping, while an actor-overlapping clustering may or may not be node-overlapping.

Finally, a multiplex community is called semi-pillar on layers $L' \subset L$ if for each actor $a \in A$ in the community all nodes in $\{(a,l) \in V : l \in L'\}$ are included in the community. When $L' = L$ , we talk of a pillar community (Figure 3). Please notice that two pillar communities are either disjoint or both actor- and node-overlapping.

# 3 A TAXONOMY OF THE REVIEWED ALGORITHMS

In this section, we provide a taxonomy of multiplex community detection methods with three levels of classification. The top-level distinction is between global or local methods, respectively, discovering all communities in the input network or generating a single community around one

![](images/f0b521f59824b628dd4096be6bfe5539fe62768b804edf483a6b361c7922294d.jpg)  
Fig. 4. A taxonomy of multiplex community detection algorithms.

or more seed nodes. The results of these two types of algorithms are not directly comparable without arbitrary choices in the selection of seed nodes, so we treat them in separate sections in our experimental evaluation. The second level regards the way in which the algorithms handle the presence of multiple layers: reducing them to a single layer (flattening), processing each layer independently (e.g., performing single-layer community detection) to then merge the results of the processing, or considering all the layers at the same time. The last level of the taxonomy groups the algorithms based on more specific approaches, such as optimizing an objective function, considering the behavior of a random walker or identifying dense subgraphs. Figure 4 and Table 1 show an overview of the related methods. Please notice that Section 4, describing some theoretical properties of the algorithms such as whether they are deterministic or not, can also be used to differentiate between different types of algorithms.

# 3.1 Global Methods

Global methods are designed to discover all possible communities in a network, thus requiring knowledge of the whole network structure. As it happens for many multiplex data analysis methods [18], global community detection algorithms can also be grouped into three typical main classes, described in the following.

3.1.1 Flattening. The first approach consists in simplifying the multiplex network into a graph by merging its layers, using a so-called flattening algorithm, then applying a traditional community detection algorithm. This process is illustrated in Figure 5.

The algorithms belonging to this class are defined by the flattening method and by the single-layer community detection algorithm applied to the flattened network. The simplest flattening method consists in creating an unweighted graph where two nodes are adjacent if their corresponding actors are adjacent on any of the input layers [4]. The advantage of this approach is that the resulting graph is easier to handle, because there are more clustering algorithms for simple graphs than for weighted graphs and weights often imply an additional level of complexity, e.g.,

Table 1. Multiplex Community Detection Algorithms Covered in This Survey   

<table><tr><td>Algorithm</td><td>Notation</td><td>Reference</td></tr><tr><td>Non-Weighted Flattening</td><td>NWF</td><td>[4]</td></tr><tr><td>Weighted Flattening (Edge Count)</td><td>WF_EC</td><td>[4]</td></tr><tr><td>Weighted Flattening (Neighbourhood)</td><td>WF_N</td><td>[4]</td></tr><tr><td>Weighted Flattening (Differential)</td><td>WF_Diff</td><td>[31]</td></tr><tr><td>Frequent pattern mining-based community discovery</td><td>ABACUS</td><td>[5]</td></tr><tr><td>Ensemble-based Multi-layer Community Detection</td><td>EMCD</td><td>[56]</td></tr><tr><td>Principal Modularity Maximization</td><td>PMM</td><td>[58, 59]</td></tr><tr><td>Subspace Analysis on Grassmann Manifolds</td><td>SCML</td><td>[19]</td></tr><tr><td>Cross-Layer Edge Clustering Coefficient (based on)</td><td>CLECC</td><td>[11]</td></tr><tr><td>Multi Layer Clique Percolation Method</td><td>ML-CPM</td><td>[1]</td></tr><tr><td>Locally Adaptive Random Transitions</td><td>LART</td><td>[33]</td></tr><tr><td>Modular Flows on Multilayer Networks</td><td>Infomap</td><td>[16, 20]</td></tr><tr><td>Generalized Louvain</td><td>GLouvain</td><td>[29, 43]</td></tr><tr><td>Fast algorithm for comm. detection based on multiplex net. modularity</td><td>FCDMNN</td><td>[61]</td></tr><tr><td>Multilink community detection</td><td>MLink</td><td>[41]</td></tr><tr><td>Multi-Layer Many-objective Optimization algorithm</td><td>MLMaOP</td><td>[47]</td></tr><tr><td>Multilevel memetic algorithm for composite community detection</td><td>MNCD</td><td>[38]</td></tr><tr><td>Multi Dimensional Label Propagation</td><td>MDLPA</td><td>[8]</td></tr><tr><td>Andersen-Chung-Lang cut</td><td>ACLcut</td><td>[28]</td></tr><tr><td>Multilayer local community detection</td><td>ML-LCD</td><td>[27]</td></tr></table>

![](images/417dbf5e77cb301d858cab228f05c503c9e79c1eb19d38742efcfefe5580bf1c.jpg)  
Fig. 5. The general process used by flattening methods: a single-layer network is first constructed merging edges from the different layers, then a traditional community detection algorithm is applied to the flattened network, and its result can be used to induce communities on the original network.

deciding a threshold above which weighted edges should be considered. A potential disadvantage is that an unweighted flattening is more susceptible to noise.

Weighted flattenings reflect some structural properties of the original multiplex network in the form of weights assigned to the output edges [4, 31]. In theory these methods are less susceptible to noise, but the resulting communities may be biased towards edges appearing on several layers, and the results can be more difficult to interpret because of the weights.

In general, the algorithms in this class are only able to identify pillar communities, and some communities may emerge because of edges spread on different layers that would not constitute a community on any individual layer, because of the flattening process.

3.1.2 Layer by Layer. While the methods in the previous class merge the layers and then apply traditional community detection algorithms, layer-by-layer methods first process each layer (e.g.,

![](images/2f64579ab4fc0ad6667f91d92f4d74cad1e559a9268cd2d25fba7e1ec6369f45.jpg)  
Fig. 6. The general process used by layer-by-layer methods: communities are identified in each layer, the information obtained from each layer is used to cluster the actors, and this clustering can be used to induce communities on the original network.

applying traditional community detection algorithms), then merge the results of the processing. This is illustrated in Figure 6.

As a consequence of the layer-by-layer community detection step, these methods include actors in the same community only when they are part of the same community in at least one layer. Also, due to the merging of layer-specific communities, these methods can in principle only identify pillar communities.

We have identified three types of layer-by-layer approaches in the literature. The pattern mining approach exploits association rule mining methods, which are among the main data mining tasks used to find objects that frequently co-occur together in different transactions. (A typical example of transaction is the basket of products bought together by a customer at a supermarket.) ABACUS considers each single-layer community as a transaction, so that the final communities contain actors that are part of the same community in at least a minimum number of layers [5].

The second way to merge the result of single-layer community detection methods is based on a notion of consensus: given a set (or ensemble) of community structure solutions from the individual layers, the goal is to find a single, meaningful solution that is representative of the input ensemble, by optimizing an objective function that is designed to aggregate information from the individual solutions in the ensemble. While early approaches such as the one in Reference [34] are limited to use a clustering ensemble method as a black-box tool for combining multiple clustering solutions from a single-layer network, the first well-principled formulation of the ensemble-based multilayer community detection (EMCD) problem, provided in Reference [56], does not limit aggregation at node membership level, but rather it accounts for intra-community and intercommunity connectivity. The consensus solution discovered by EMCD is the one with maximum multilayer modularity from a search space of candidates delimited by topological upper-bound and lower-bound solutions, respectively, of the input multilayer network.

Finally, some methods in the literature process the layer-specific adjacency matrices, or derived matrices, and extend spectral-clustering for simple graphs by exploiting the relationship between the eigenvectors and eigenvalues in the constructed matrices and the presence of clusters in the corresponding graphs. As an example, the principal modularity maximization (PMM) method [58] extracts structural features from the various layers, then concatenates the features and performs PCA to select the top eigenvectors. Using these eigenvectors, a low-dimensional embedding is computed to capture the principal patterns across the layers, finally a simple $k$ -means is applied to assign nodes to communities. Further details on this class of approaches can be found in Reference [57].

3.1.3 Multilayer. The third class of algorithms operates directly on the multiplex network model, as shown in Figure 7. As an example, a method belonging to this class based on a random walker would allow the walker to switch from one layer to the other.

![](images/04ba81baebee86c3c0b3036b8b5f78e0b0e3d987b8e0704880952834d936d9bf.jpg)  
Fig. 7. Multilayer methods discover communities directly on the multiplex data.

Various approaches originally developed for simple graphs have been extended to the multilayer case. Density-based methods first identify dense regions of the network, then include adjacent regions in the same community. A popular method for simple graphs is clique percolation, where dense regions correspond to cliques and adjacency consists in having common nodes. The multi-layer clique percolation method (ML-CPM) extends this process by looking for cliques spanning multiple layers, and redefining adjacency so that both common nodes and common layers are required [1]. CLECC uses a different but related approach, identifying sparse locations of the network having a low cross-layer clustering coefficient [11]. The higher the proportion of common neighbors across all layers (or any number of layers provided as input), the higher the cross-layer clustering coefficient.

Methods based on random walks consider that an entity randomly following the edges in a network would tend to get trapped inside communities, because of the higher edge density between nodes inside the same community, less frequently moving from one community to the other. LART [33] and Infomap [16] are both based on this consideration, with Infomap using a shortest information coding approach to identify the corresponding communities.

Several of the reviewed algorithms in the multilayer class use an objective function that, given an assignment of the nodes to communities, returns a higher value when there are more edges inside communities and less edges across communities. Once the objective function has been defined, then different optimization methods can be used to identify a community assignment corresponding to a high value of the function. Generalized Louvain (GLouvain) [29, 43], the best-known method in this class, uses an extended version of modularity, and has been analyzed in more detail in Reference [23]. While GLouvain has become the most popular modularity-based method for multiplex networks, it is worth mentioning the alternative approach used by Reference [47], aimed at obtaining a high modularity in each individual layer instead of a global extended definition of modularity. This class also includes a method returning a different type of communities with respect to the ones generated by the other algorithms, where edges are grouped instead of actors and nodes [41].

Finally, the multilayer class includes an algorithm based on label propagation [8]. A traditional label propagation method would start assigning a different label to each node, then having each node replace its label with one that is frequent among its neighbors, until some stopping condition is satisfied. The multilayer version of this approach follows the same idea, weighting the contribution of each neighbor based on their similarity with the node on the different layers. For example, two nodes being adjacent on all layers and having the same neighbors on all layers would have a higher probability of getting the same label.

# 3.2 Local Methods

Local methods (also known as node-centric) are query-dependent, i.e., they are designed to discover the community around a set of input query nodes. Please notice that the term local has also been

used with other meanings in the literature, for methods finding global community structures using only neighborhood information when processing vertices in the graph. At the time of writing, we recognize the availability of two methods able to discover multiplex local communities: ML-LCD [27] and ACLcut [28]. ML-LCD searches for the local community associated to a seed actor without having a complete knowledge of the network graph, through an incremental exploration of the neighborhood of the query actor, according to the optimization of a criterion function based on the internal and external connectivity of the local community. ACLcut exploits the solution of a personalized PageRank approximated for an input seed-set (i.e., a set of query actors) to find the local communities, using a sweep cut method to sample local communities based on the lowest conductance values. Both methods operate directly on the multiplex network model, so that the Local branch of our hierarchy only includes the Multilayer class. Nevertheless (even if, to the best of our knowledge, there are no such examples in literature) it is in theory possible to easily design multiplex local community detection methods that operate through flattening or layer-by-layer schemes, by exploiting existing single-layer local community detection methods such as LCD [14] and Lemon [36].

# 3.3 Selection of Algorithms

In the following sections, we will provide a detailed comparative analysis of a large subset of the algorithms in our taxonomy. We include at least one representative method for each leaf in the taxonomy. In those cases where different well-known methods inside the same leaf show significant differences, either theoretically or experimentally, we have also included them, as detailed in the following.

We only focus on a selection of the flattening methods, with one representative for each class (unweighted and weighted), because of the small variation between the different approaches and because the features and performance of these algorithms are determined more by the single-layer approach used to implement them than by the way in which weights are assigned. While the main interest of this article is on multilayer-specific methods, we still considered it important to test some flattening methods in detail, because as we will see in our comparative analysis these simpler approaches can still produce good and sometimes better results than more sophisticated methods.

We include all the methods from the layer-by-layer class (ABACUS, EMCD, PMM, and SCML), because they are representative of different ways to merge the results of the single-layer algorithms. PMM has been first published in conference proceedings [58] and then abstracted and extended in a journal article [59]. We use the conference version, because the code for the journal version is not available.

From the multilayer class, we include at least one representative method for each sub-class. Among the modularity-optimization methods we have selected GLouvain, because it is the best-known optimization algorithm as witnessed by its large number of citations. MLink has only been included in the scalability analysis, because it produces link communities that are not directly comparable with the ones produced by other methods.

We also include all the local methods (ACLcut and ML-LCD), because they use significantly different approaches.

# 4 THEORETICAL ANALYSIS

In this section, we present some theoretical properties of the reviewed algorithms. We describe the types of community structures that can be returned by each algorithm, we indicate some features of the algorithms themselves such as whether they are deterministic, and we discuss parameter setting and computational complexity.

Table 2. Types of Clustering Produced by the Reviewed Methods and Algorithmic Properties   

<table><tr><td>Algorithm</td><td>Category</td><td>NPC</td><td>AO</td><td>NO</td><td>Pa</td><td>LR</td><td>Det</td><td>AK</td><td>SS</td><td>Compl</td></tr><tr><td>NWF</td><td>G-Flat</td><td>×</td><td>*</td><td>*</td><td>*</td><td>×</td><td>*</td><td>*</td><td>*</td><td>O(e + δ)</td></tr><tr><td>WFEC</td><td>G-Flat</td><td>×</td><td>*</td><td>*</td><td>*</td><td>×</td><td>*</td><td>*</td><td>*</td><td>O(e + δ)</td></tr><tr><td>ABACUS</td><td>G-LBL</td><td>×</td><td>✓</td><td>✓</td><td>✓</td><td>×</td><td>*</td><td>✓</td><td>✓</td><td>O(lδ) + ARM</td></tr><tr><td>EMCD</td><td>G-LBL</td><td>×</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>*</td><td>✓</td><td>✓</td><td>O(lδ) + O(i(e + lc))</td></tr><tr><td>PMM</td><td>G-LBL</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>-</td></tr><tr><td>SCML</td><td>G-LBL</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>-</td></tr><tr><td>ML-CPM</td><td>G-ML</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>✓</td><td>≥ O(l3a/3)</td></tr><tr><td>Infomap</td><td>G-ML</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>-</td></tr><tr><td>LART</td><td>G-ML</td><td>✓</td><td>✓</td><td>×</td><td>×</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>-</td></tr><tr><td>GLouvain</td><td>G-ML</td><td>✓</td><td>✓</td><td>×</td><td>×</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>-</td></tr><tr><td>MDLPA</td><td>G-ML</td><td>✓</td><td>✓</td><td>×</td><td>×</td><td>✓</td><td>×</td><td>✓</td><td>✓</td><td>O(ea + e2l + ei)</td></tr><tr><td>ML-LCD</td><td>L-ML</td><td>×</td><td>-</td><td>-</td><td>-</td><td>✓</td><td>✓</td><td>-</td><td>✓</td><td>O(|C|2dΦ)</td></tr><tr><td>ACLcut</td><td>L-ML</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>×</td><td>×</td><td>-</td><td>✓</td><td>-</td></tr></table>

The second column recalls the class of the algorithm (G-Flat: global flattening, G-LBL: global layer by layer, G-ML: global multilayer, L-ML: local multilayer). Columns NPC (Non-Pillar), AO (Actor-Overlapping), NO (Node-Overlapping), and Pa (Partial) indicate if the algorithm can $(\checkmark)$ or cannot $(\times)$ produce that type of community structure. Columns LR (Layer Relevance), Det (Deterministic), AK (Automated selection of the number of communities), and SS (Subgraph Structure) refer to the functioning of the algorithm. $(^{*})$ indicates that the answer depends on the single-layer clustering algorithm used by the method. $(-)$ indicates that the property is not relevant for the algorithm. The last column indicates the time complexity of the method if studied in the original paper or easily derivable from the algorithm: $a$ : number of actors, $e$ : number of edges, $l$ : number of layers, $c$ : number of communities, ARM: cost to compute closed association rules, $|C|$ : size of local community, $d$ : maximum node degree, $\delta$ : complexity of the single-layer community detection algorithm used as a sub-procedure, $\Phi$ : parameter depending on the used subprocedure (see Section 4.4).

These properties should be considered in combination with the results of our experimental evaluation. For example, the fact that in theory an algorithm is able to produce some types of multiplex communities does not imply that these types of communities will be found in practice. Nonetheless, knowing that some algorithms are not able to return some types of communities or that their execution time grows exponentially with respect to the number of layers can be useful to choose which algorithms to use in specific situations.

# 4.1 Types of Community Structures

In Section 2, we have described different properties of multiplex community structures. Table 2 indicates which ones are associated to each reviewed algorithm. In particular,

(NPC) if the algorithm can generate Non-Pillar Communities;

(AO) if it can generate Actor-Overlapping community structures;   
(NO) if it can generate Node-Overlapping community structures;   
(Pa) if it can generate Partial community structures.

An algorithm not satisfying these properties (i.e., those with an “ $\times$ ” in the table) would, respectively, only be able to produce pillars, only partition the actors and nodes, and force all nodes to belong to at least one community. Notice that this can be perfectly fine in some cases, so satisfying or not the properties above does not mean that the algorithm is worse or better. These properties should only be used as an indication about the appropriateness of the algorithm for specific scenarios.

The following are some considerations summarized in Table 2:

- For all flattening methods, the type of the resulting community structure (Overlapping/Disjoint and Total/Partial) depends on the single-layer algorithm used after flattening. The choice of the single-layer algorithm can then be made depending on the wanted result.   
- All flattening methods produce pillar communities, because the actors on different layers are reduced to a single node in the flattened graph.   
- All multilayer methods can produce non-pillar communities in theory, although our experimental evaluation shows that pillar communities are often returned by some of these methods.   
- Pillar actor-overlapping communities are always node-overlapping, by definition.   
- Non-pillar actor-overlapping communities may be or not node-overlapping.

# 4.2 Algorithmic Properties

In their survey work, the authors of Reference [30] discussed a classification framework based on a set of desired properties for multilayer community detection methods. These properties are: multiple layer applicability, consideration of each layer's importance, flexible layer participation (i.e., every community can have a different coverage of the layers' structure), no-layer-locality assumption (e.g., independence from initialization steps biased by a particular layer), independence from the order of layers, algorithm insensitivity, and overlapping layers (e.g., two or more communities can share substructures over different layers).

We observe that the first of the properties listed above (multiple layer applicability) is satisfied by all methods we reviewed, therefore we do not elaborate on this further. By contrast, the second property (consideration of each layer's importance) is also included in our list and further elaborated, as detailed below (Layer Relevance). We collapse the properties about independence from the order in which nodes and layers are examined into a single property, also including stochastic behaviors such as in the case of random walkers (Determinism). As we focus on multiplex networks, we do not treat the case where layers are ordered. The insensitivity property (i.e., independence or robustness against main tunable input parameters) is instead replaced by a more specific property on whether the number of communities is automatically derived (Auto-detection), and a more general discussion about how to set additional parameters. The last property we consider (Subgraph Structure) was not discussed in previous surveys.

In light of the above considerations, we define the following properties, indicated in Table 2.

(LR) Layer relevance. Some methods take into consideration each layer's importance, also called relevance in some of the reviewed works, to control their contribution to the computation of the multiplex community structure. Layer relevance is either learned based on the layer characteristics, or it can be an input of the algorithm based on a-priori knowledge (e.g., user preferences).   
(Det) Determinism. This refers to whether a method has a deterministic behavior, e.g., its output is independent from the order of examination of the nodes and/or layers.   
(AK) Auto-detection of the number of communities. Some methods expect the number of communities to be decided ahead of time while other methods can automatically define the number of communities.   
(SS) Subgraph structure. The primary product of all the reviewed methods are the cluster memberships of nodes. However, some methods also tell us something about the multilayer subgraph structures underlying each community, that is, we can get more information about which edges contributed to the discovery of each community.

Different algorithms tune layer relevance (LR) in different ways. The only algorithm allowing to specify weights as input parameters is GLouvain, through the parameter omega $(\omega)$ that gives more or less importance to the fact that the same actor is included in the same community in different layers. However, these weights are assigned to pairs of actors in different layers, not to individual layers, and in practice $\omega$ is set to a single value for the whole network. In EMCD, the importance of the various layers may be considered by differently setting the resolution parameter in the multilayer modularity. Both LART and MDLPA use a concept of layer relevance (that is, how important a layer is for a node or a pair of nodes) to weight the probability of the random walker to switch layer or of a label to be propagated. ML-LCD is designed to explicitly incorporate layer relevance weighting schemes in the local community functions.

Non-determinism is the result of different features in different algorithms: using heuristics to optimize an objective function (such as GLouvain), using non-deterministic clustering algorithms as sub-procedures (as PMM and SCML), using stochastic choices (as LART) or the iterative computations performed by MDLPA and ACLcut, depending on the order in which nodes are processed.

The automated selection of the number of communities is a practically important property especially for networks. Traditional clustering algorithms requiring the number of clusters as input, such as k-means, can be run multiple times to optimize $k$ using some measures of clustering quality, but this procedure has not been explored for the algorithms studied in this survey.

With regard to the last property, all the methods returning non-pillar communities provide information about which layers define each community. For example, in ML-CPM communities are combinations of adjacent cliques, so all the edges in these cliques can be considered part of the community. As another example, MDLPA computes a score for each pair of nodes indicating how likely a label should be propagated from one to the other, leading to a common community. However, also methods not returning information about layers as their primary output could be used to indicate which layers and edges determine each community. EMCD only accounts for those edges from different layers that contribute to maximize the multilayer modularity of the consensus community structure solution. In ABACUS, even if the output of the algorithm is about actors, for each pair of actors included in the same community we could look at which layers determined that assignment.

# 4.3 Parameter Setting

Apart from the number of communities to discover, which is required by some algorithms as input, the reviewed methods have a variety of additional input parameters to set. While explaining the meaning of each parameter goes beyond the aims of this survey, it is useful to characterize the methods with respect to how difficult and/or important it is to properly set their parameters.

Some methods can be executed parameter-free. This is the case for all flattening methods, except if their single-layer clustering algorithm needs some, and for MDLPA and Infomap, although Infomap provides additional options that the interested reader can check on the information-rich website provided by the authors.2

ABACUS and ML-CPM require to specify minimum values for the number of layers and actors to be included in a community, which makes them able to identify partial community structures. These parameters affect the result by making it more and more difficult to accept some groups of nodes as a community, and while setting the correct values may require multiple trials, in our opinion the meaning of these parameters is easy to grasp.

EMCD requires to specify the co-association threshold, $\theta$ , that may have a strong impact on the resulting consensus communities. The original paper presenting this algorithm indicates optimal

ranges of values on some networks and suggests that similar values can be used for similar networks.

PMM requires to specify the number of structural features, which can be any number between 1 and $\# a - 2$ . Also in this case different settings can lead to quite different results, and this parameter has a less intuitive meaning if compared with those required by other methods. Similarly, SCML requires a regularization parameter lambda. In addition, both methods require to specify the number of expected communities, as mentioned in the previous section, and the number of times the k-means algorithm used as a sub-procedure should be repeated. In general, different executions of k-means can lead to different results.

GLouvain requires only two parameters: $\omega$ , weighting inter-layer contributions, and $\gamma$ , the so-called resolution parameter. Regarding $\gamma$ , we refer the reader to the literature about its usage and shortcomings in the single-layer version of modularity. $\omega$ , which in theory can be set individually for each actor and pair of layers but is more practically set to a single value, has an apparently intuitive meaning: a low value would give priority to intra-layer communities, a higher value would tend to discover communities spanning multiple layers. We refer the reader to Reference [23] for a deeper discussion about what can and cannot be identified with different settings of $\omega$ .

LART requires four parameters: $t$ , $\epsilon$ , $\gamma$ , and linkage. While the interpretation of some of these parameters is intuitive, in particular the type of hierarchical clustering to be performed inside the algorithm (linkage) and the number of steps to be taken by the random walker ( $t$ ), it is in general difficult to predict what impact each setting would have on the final result, which makes these parameters more difficult to be set if compared with other methods.

Regarding the local methods, they naturally take the set of query nodes as an input parameter. ML-LCD has no additional parameters, except for the ones controlling layer weights in the ML $\mathrm{LCD}_{(lwsim)}$ formulation. However, in absence of exogenous information about the importance of each layer, uniform weights can be used without loss of generality. Concerning ACLcut, the main parameters are the ones controlling the random walk generating the input transition tensor. Two alternative models can be used, which differ in how they navigate the multiplex network: a classic random walk, controlled by an uniform interlayer edge weight $\omega$ , and a relaxed random walk, controlled by a layer-jumping probability $r$ . These parameters are shown to have a major impact in the characteristics of resulting local communities, thus it is not clear how to set them in general cases. ACLcut also includes an underlying Approximated Personalized PageRank (APPR) procedure, whose resolution is controlled by two additional parameters: the teleportation parameter $\gamma$ and the truncation parameter $\epsilon$ . A default value of 0.95 can be used for $\gamma$ , while arbitrary small values can be used for $\epsilon$ (e.g., inversely proportional to the number of nodes in the network).

# 4.4 Some Notes on Computational Complexity

In most cases, a detailed study of the computational complexity of community detection algorithms is not provided in the original references. This can be explained by the fact that many well-known algorithms have not been developed by computer scientists nor published in computer science venues. However, we also notice that worst-case complexity would often be not particularly informative: execution time typically strongly depends on data and parameter setting, making an experimental analysis more useful in characterizing the methods. At the same time, some considerations can be useful to either predict or understand the behaviour of some algorithms in specific situations.

For flattening methods, time complexity depends on the flattening step and on the subsequent single-layer community detection step. Basic types of flattening are in $O(e)$ , in which case the complexity of the algorithm corresponds to the one of the community detection step. It is

interesting to notice that higher layer similarity for example in terms of edge Jaccard [10] would lead to a lower number of edges, possibly resulting in a lower execution time of the single-layer community detection algorithm.

As for layer-by-layer methods, the complexity also depends on the community detection algorithm applied to each layer, but the step where the communities from the different layers are merged can be significantly more expensive than a flattening. ABACUS uses association rule mining, which can in theory generate an exponential number of rules. The actual execution time is however dependent on the input thresholds: the minimum number of layers where actors must be assigned to the same community to be included in the final result (corresponding to the support count measure in association rule mining) and the minimum number of actors in a community to be counted (limiting the transaction size in the association rule mining algorithm). EMCD linearly scales with the number of multilayer edges and with the number of consensus communities. While the paper introducing PMM does not provide a complexity analysis, the algorithm requires two expensive steps: the extraction of $f$ eigenvalues from each layer and a singular value decomposition on data of size $a \times fl$ ; therefore, its complexity depends on the number of actors, the number of layers (that is, the data), and on the number of features (which is an input parameter).

ML-CPM requires the computation of maximal cliques, that is NP-Hard even on a single layer. This implies that dense regions of the input networks across $m$ or more layers consisting of a few tens of nodes may lead to impractically slow computations. Maximal clique detection can however be very fast in practice for sparser networks with small communities. GLouvain uses a heuristic to optimize an extended modularity objective function, as modularity optimization is already NP-Hard on single networks. In general, label propagation algorithms have a complexity of $O(ei)$ , where $i$ is the number of iterations, which is often small. However, MDLPA also contains a subroutine iterating over all subsets of the layers, to compute pairwise weights to be used when labels are propagated. This makes its complexity exponential in the number of layers $l$ .

Computational complexity of ML-LCD is proportional to the size of the generated community, thus the overall upper bound is $O(|C|^2 \times d \times \Phi)$ , where $|C|$ is the size of the local community, $d$ is the maximum degree of a node in the network and $\Phi$ is the cost of optimizing the $LC$ function. Possible values of $\Phi$ depend on the three alternative formulations and are $O(ld)$ for ML-LCD(lwsim), $O(ld^2 \log d)$ for ML-LCD(wlsim), and $O(|C|d^2 \log dl^2)$ for ML-LCD(clsim). The complexity of ACLcut has not been studied in the original paper.

# 5 EXPERIMENTAL EVALUATION

We devised an experimental evaluation to pursue two main goals in comparing the various methods: one relating to the quality of the produced communities, the other to efficiency aspects. More specifically, our experiments were carried out to answer the following research questions:

Q1 To what extent are the evaluated methods able to detect ground truth communities?   
Q2 To what extent do the evaluated methods produce similar community structures?   
Q3 To what extent are the evaluated methods scalable?

Two main stages of evaluation were devised: one for global methods (Section 6.1), whose output is a set of communities, and one for local methods (Section 6.2), whose output is a single community centered around a node (or set of nodes). Due to their structural differences, these two tracks had to be evaluated separately and by means of different criteria. The reason why we have not tested the algorithms on single-layer networks is that multilayer methods are generalizations of single-layer algorithms, so their results would be exactly the same as those already reported in single-layer studies.

![](images/9dddfc074a0f8ff7aa1b34594f727d89ca88e9f0617d3696112a755c5d8c9235.jpg)

![](images/be3296d628b573d2a68714feaa101d4098735d72ab683a68d65db52c32fc78d6.jpg)

![](images/46e41f497cafc12ca2ee12af49d477a98ea8c3d49a2a97a56c42dac5e1687860.jpg)

![](images/e9b303cdb62037d2dbbc59afd039d05c4f2004c48309f67f81ced6d0c7ac2e1e.jpg)

![](images/48eca407dc6b04bb6c221e3e729fcc809c4f44425703e497166397491d6224f2.jpg)  
(e) Semi-pillar Equal Parti-(f) Semi-pillar Equal Over-(g) Semi-pillar Non-equal (h) Semi-pillar Non-equal tioning (SEP) lapping (SEO) Partitioning (SNP) Overlapping (SNO)

![](images/ff403b0a91de207135bf0194319ccf29849755930578b359d68049d1d34de0dd.jpg)  
(i) Hierarchical (HIE)

![](images/75a3324ce158f175cb431dd8b07a711ee4a51515a60fb1546ca9d71bbd5997e1.jpg)  
(j) Mixed (MIX)   
Fig. 8. An illustration of the types of synthetic multiplex networks generated for different possible multiplex community structures. Equal/Non-Equal refers to the number of nodes (size) in the communities.

# 5.1 Data

To evaluate the communities discovered by the tested methods, we use a selection of real datasets widely used in the literature, representing different application areas and with different characteristics: AUCS (short for Aarhus University Computer Science) [49], a hybrid online/offline network with different types of relationships between employees of a university department; DKPol (short for Dansk Politik) [25], a network with three types of online relations between Danish Members of the Parliament on Twitter, Airports (short for Air Transportation Multiplex) [13], with flight connections between European airports, and Rattus [17], about genetic interactions. AUCS and DKPol also come with some possible community structures, referred to as ground truth in the following: respectively, the research groups at the department, and affiliation to political parties.

We have also generated synthetic datasets forcing specific types of community structures, illustrated in Figure 8. This has two motivations: first, ground truth should be used carefully in cluster analysis, with no single accepted definition of what the correct result should be. So-called ground truths should only be used as part of a broader evaluation, as well known in the field of clustering and also pointed out about community detection [46]. In addition, the ground truth in the real datasets has a quite simple structure, mostly containing pillar non-overlapping communities. Therefore the synthetic networks are used to check whether the tested algorithms are able to identify specific types of structures. We used small datasets to be able to compare all methods including those not scaling well. One should however consider that smaller probabilistically generated

Table 3. Summary of Structural Characteristics of the Evaluation Networks: Number of Layers (l), Number of Actors (a), Number of Edges (e), and Mean/Std Over the Layers of Density (den), Average Degree (a_deg), Average Path Length (a_p_len), and Clustering Coefficient (cccoef)   

<table><tr><td>Network</td><td>l</td><td>a</td><td>e</td><td>den</td><td>a_deg</td><td>a_p_len</td><td>ccoeff</td></tr><tr><td>AUCS</td><td>5</td><td>61</td><td>620</td><td>0.12 ± 0.07</td><td>5.21 ± 2.46</td><td>2.43 ± 0.73</td><td>0.43 ± 0.1</td></tr><tr><td>DKPol</td><td>3</td><td>490</td><td>20,226</td><td>0.07 ± 0.08</td><td>28.85 ± 44.24</td><td>3.43 ± 1.32</td><td>0.24 ± 0.26</td></tr><tr><td>Airports</td><td>37</td><td>417</td><td>3,588</td><td>0.06 ± 0.02</td><td>3.13 ± 1.45</td><td>2.25 ± 0.34</td><td>0.07 ± 0.08</td></tr><tr><td>Rattus</td><td>6</td><td>2,640</td><td>3,956</td><td>0.05 ± 0.07</td><td>1.62 ± 0.62</td><td>2.75 ± 2.22</td><td>0.03 ± 0.08</td></tr></table>

(a) Real datasets   
(b) Synthetic datasets with a controlled community structure   

<table><tr><td>Network</td><td>l</td><td>a</td><td>e</td><td>den</td><td>a_deg</td><td>a_p_len</td><td>cccoef</td></tr><tr><td>PEP</td><td>3</td><td>100</td><td>943</td><td>0.05 ± 0.00</td><td>5.32 ± 0.32</td><td>3.39 ± 0.09</td><td>0.31 ± 0.05</td></tr><tr><td>PNP</td><td>3</td><td>100</td><td>1,584</td><td>0.1 ± 0.01</td><td>9.51 ± 0.52</td><td>2.77 ± 0.04</td><td>0.41 ± 0.02</td></tr><tr><td>PEO</td><td>3</td><td>100</td><td>1,487</td><td>0.09 ± 0.00</td><td>8.78 ± 0.33</td><td>2.51 ± 0.02</td><td>0.28 ± 0.03</td></tr><tr><td>PNO</td><td>3</td><td>100</td><td>2,079</td><td>0.13 ± 0.00</td><td>12.71 ± 0.44</td><td>2.29 ± 0.03</td><td>0.37 ± 0.01</td></tr><tr><td>SEP</td><td>3</td><td>100</td><td>966</td><td>0.06 ± 0.00</td><td>5.45 ± 0.14</td><td>3.36 ± 0.06</td><td>0.34 ± 0.02</td></tr><tr><td>SNP</td><td>3</td><td>100</td><td>1,360</td><td>0.08 ± 0.02</td><td>7.96 ± 2.32</td><td>3.01 ± 0.39</td><td>0.38 ± 0.03</td></tr><tr><td>SEO</td><td>3</td><td>100</td><td>1,314</td><td>0.08 ± 0.02</td><td>7.63 ± 2.03</td><td>2.8 ± 0.49</td><td>0.28 ± 0.01</td></tr><tr><td>SNO</td><td>3</td><td>100</td><td>1,762</td><td>0.11 ± 0.05</td><td>10.65 ± 4.54</td><td>2.63 ± 0.63</td><td>0.37 ± 0.02</td></tr><tr><td>HIE</td><td>3</td><td>100</td><td>1,820</td><td>0.11 ± 0.06</td><td>11.05 ± 5.64</td><td>2.76 ± 0.58</td><td>0.41 ± 0.05</td></tr><tr><td>MIX</td><td>3</td><td>100</td><td>388</td><td>0.02 ± 0.01</td><td>2.21 ± 0.78</td><td>2.78 ± 0.29</td><td>0.42 ± 0.05</td></tr></table>

networks have a larger structural variability, and when testing scalable methods larger networks can be used to reduce variance in the results. Here, we focus on the comparison between methods, which are all tested on the same data. The code used to generate these networks is available at https://bitbucket.org/uuinfolab/20csur.

General information about these networks including the mean and standard deviation over the layers for density, degree, average path length and clustering coefficients are reported in Table 3. More information about the datasets used in the experiments is provided in the supplementary online material.

Finally, we generated networks with varying numbers of actors (100 to 10,000) and layers (1 to 20) to perform scalability tests. These networks have the same structure indicated as PEP (Pillar Equal Partitioning) in Figure 8, because this is the only type of community structure that most of the methods can correctly recover, as we shall see in the results of our experiments. While the number of layers and actors varies, the probabilities of node adjacency inside and across communities are set in the same way as for the PEP network in Table 3.

# 5.2 Detailed setting for each method

For all methods based on a single-layer algorithm, we use Louvain. Using the same algorithm makes the comparison fairer; however, we must point out how this deviates from some original publications. We also tested the methods using the single-layer algorithm mentioned in the original references (e.g., label propagation). We think that the relevance of these methods for this article lies in the way they deal with the multilayer structure rather than the specific algorithm that is used on the single-layer network. Within this perspective, using Louvain provides more stable, more accurate, and more comparable results in general.

With respect to parameter setting, in general, we used the default values proposed by the original works. In some specific cases, where different parameter settings are expected to be used to identify different types of community structures (i.e., GLouvain, ML-CPM, ABACUS, ACLcut, ML-LCD, and Infomap), we tested multiple settings as detailed in the following.

- For ABACUS, two main parameters have effect on filtering out possible multiplex communities when single-layer communities are merged into the final result, namely, the minimum number of actors in a community $(k)$ and the minimum number of single-layer communities in which the actors must have been grouped together $(m)$ . We use this algorithm with two settings, $\mathrm{ABACUS}_{31}$ with $(k = 3, m = 1)$ and $\mathrm{ABACUS}_{42}$ with $(k = 4, m = 2)$ , which filters out the communities that are not expanded over multiple layers.   
- PMM takes three parameters: the number of communities to return, the number of structural features, and the number of times k-means should be executed as a subroutine, that we set to 5. The number of communities has been set to the number of known communities in the data where that is known, and to an arbitrary number (10) for Airports and Rattus. The fact that we used knowledge about the expected result to setup the algorithm should be considered when the different methods are compared. We did not find heuristics to set the number of structural features (Ell), so we used two settings: low and constant $(\mathrm{Ell} = 10)$ , and high and dependent on the number of actors $(\mathrm{Ell} = a/2)$ ; these are among the settings returning good results for AUCS and PEP, for which a ground truth compatible with the results that PMM can return exists. However, please notice that the results may vary very significantly by varying this parameter, and we set it based on knowledge of the expected result. This should also be considered when looking at the experimental results.   
- SCML takes two parameters: the number of communities, for which the same settings and reflections for PMM apply, and lambda, set to the default value 0.5.   
- EMCD takes one parameter, theta, for which different settings can lead to significantly different results. The original reference contains an evaluation of appropriate ranges of theta for datasets with different statistics. We based our settings on these considerations: 0.03 for Airports and Rattus, 0.01 for DKPol, 0.2 for AUCS, 0.1 for the synthetic networks.   
- ML-CPM: two main parameters can influence the results and the execution time of the algorithm, namely, the minimum number of actors that form a multilayer clique $(k)$ , and the minimum number of layers to be considered when counting the multilayer cliques $(m)$ . To be more inclusive, we defined two settings for these parameters, ML-CPM $_{31}$ with $(k=3, m=1)$ , which allows single-layer communities but could be computationally very expensive with large networks, and ML-CPM $_{42}$ with $(k=4, m=2)$ , which is less expensive computationally, but forces the communities to be expanded over at least two layers.   
- LART has been executed with default parameter settings: $t = 9$ (number of steps for random walker to take), $\mathrm{eps} = 1$ (for binary matrices this will mean adding a self-loop to each node on each layer), gamma $= 1$ (recommended by the authors), and linkage $=$ average (determining the type of hierarchical clustering performed in the algorithm).   
- Infomap can be used to find both overlapping and non-overlapping communities. Consequently, we included it twice in our experiments, i.e., forcing a non-overlapping community discovery $(\text{Infomap}_{no})$ , and accepting overlapping communities $(\text{Infomap}_o)$ .   
- For GLouvain, we defined two settings, $\mathrm{GLouvain}_h$ to denote high weight assigned to the inter-layer edges $(\omega = 1)$ , and $\mathrm{GLouvain}_l$ to refer to a low value for the inter-layer edge weight $(\omega = 0.1)$ . The motivation is that high values for $\omega$ favor the identification of pillar communities and may prevent the identification of actor-overlapping communities that the algorithm can retrieve with a low $\omega$ .

- MLink takes two input parameters leading to different types of results. As we have not analyzed the resulting communities, for which we refer to the original reference, we use the default values used in the original implementation for scalability analysis.   
- MDLPA has no input parameters.   
- For ACLcut, two settings were used. One with a classical random walker $\mathrm{ACLcut}_c$ , and another with a relaxed random walker $\mathrm{ACLcut}_r$ .   
- For ML-LCD we used three settings corresponding to different ways to optimize the $LC$ function during the selection of nodes to join a local community, namely, $\mathsf{ML - LCD}_{(lwsim)}$ , for the layer-weighted similarity-based $LC$ , $\mathsf{ML - LCD}_{(wlsim)}$ for the within-layer similarity-based $LC$ , and $\mathsf{ML - LCD}_{(clsim)}$ for the cross-layer similarity-based $LC$ .

# 5.3 Software

The following experiments have been performed using a combination of original code (LART in Python2.7, EMCD in Java, PMM, SCML, and MLink in MATLAB, Infomap in $\mathrm{C + + }$ ) and the implementations of the other algorithms available in the multinet library (NWF, $\mathrm{WF}_{EC}$ , ABACUS, ML-CPM, GLouvain, MDLPA, all written in $\mathrm{C + + }$ and also available for R and Python). We also use the multinet library for basic functions to read networks, communities, to compute the Omega index, and so on. Infomap was also run from inside multinet, but the code is the one from the authors with minor adaptations to make it compatible with the requirements of the CRAN repository. The implementation of ABACUS uses code from https://borgelt.net/eclat.html for the association rule mining subroutine. All the algorithms are available at https://bitbucket.org/uuinfolab/20csur, except ACLCut, which has not been ported to the latest version of the multinet library. The MATLAB code in this repository is run using Octave. All the MATLAB code could be executed in Octave, except the internal edge clustering subroutine used by MLink. As we did not compare the results of Mlink with other algorithms, we skipped that part of the execution, which does not affect our conclusions about its scalability.

# 5.4 Assessment Criteria

To measure pairwise similarity between two global community structures, we use the Omega index, which is a well known measure [15] that can be applied to situations where both, one, or neither of the clusterings being compared is overlapping [44]. It does so by averaging the number of agreements on both clusterings and then adjusting that by the expected number of agreements between the two clusterings in case they were generated at random. An agreement is when two nodes are clustered together in the same number of clusters $(j)$ in both clusterings. The values of $j$ start from 0, meaning that if two nodes are never clustered together in both clusterings, this still counts as an agreement.

Given two clusterings $C_1, C_2$ , the similarity between them using Omega index is given by

$$
\text {O m e g a} \left(C _ {1}, C _ {2}\right) = \frac {\text {O b s e r v e d} \left(C _ {1} , C _ {2}\right) - \text {E x p e c t e d} \left(C _ {1} , C _ {2}\right)}{1 - \text {E x p e c t e d} \left(C _ {1} , C _ {2}\right)}, \tag {1}
$$

$$
\text {O b s e r v e d} \left(C _ {1}, C _ {2}\right) = \frac {1}{N} \sum_ {j = 0} ^ {l} A _ {j}, \tag {2}
$$

$$
\text {E x p e c t e d} \left(C _ {1}, C _ {2}\right) = \frac {1}{N ^ {2}} \sum_ {j = 0} ^ {l} N _ {(j, 1)} N _ {(j, 2)}, \tag {3}
$$

where $\mathrm{Observed}(C_1, C_2)$ refers to the observed agreement represented by the average number of agreements between $C_1$ and $C_2$ , $l$ is the maximum number of times a pair appears together in both

$C_1$ and $C_2$ at the same time, $N$ is the total number of possible pairs, $A_{j}$ is the number of pairs that are grouped together $j$ times in both clusterings, and $N_{(j,1)}, N_{(j,2)}$ indicate the numbers of pairs that have been grouped together $j$ times in $C_1, C_2$ , respectively. Theoretically, values of the Omega index are in the range $[-1, 1]$ . However, in practice, Omega index returns 1 for two identical clusterings, and values close to 0 when one of the two input clusterings is a totally random reordering of the other one.

To clarify the formulas above, we provide two examples. First, to understand the meaning of each part of the formulas, consider two equal overlapping clusterings of four elements 1, 2, 3, and 4: \( C_1 = \{\{1,2,3\}, \{2,3,4\}\} \) and \( C_2 = \{\{1,2,3\}, \{2,3,4\}\} \). In this case the number of possible pairs \( N \) is 6 (\{1,2\}, \{1,3\}, \{1,4\} \ldots). A_0 = 1 \), because only the pair \(\{1,4\}\) does not appear inside a same cluster in both clusterings. \( A_1 = 4 \), corresponding to pairs \(\{1,2\}, \{1,3\}, \{2,4\} \), and \(\{3,4\}\), all appearing together once in each clustering. Only the pair \(\{2,3\}\) is assigned to two different clusters in each clustering, therefore \( A_2 = 1 \). The other values to compute the omega index are \( N_{(0,1)} = 1 \), \( N_{(0,2)} = 1 \), \( N_{(1,1)} = 4 \), \( N_{(1,2)} = 4 \), \( N_{(2,1)} = 1 \), \( N_{(2,2)} = 1 \). As a result, we have: Observed \( (C_1,C_2) = \frac{1}{6} (1 + 4 + 1) \) and Expected \( (C_1,C_2) = \frac{1}{36} (1 \cdot 1 + 4 \cdot 4 + 1 \cdot 1) \). The corresponding Omega index is 1, as expected, because the two clusterings are identical. Now consider the two clusterings \( C_1 = \{\{1,2\}, \{3,4\}\} \) and \( C_2 = \{\{1,2\}, \{3\}, \{4\}\} \). We now have Observed \( (C_1,C_2) = \frac{1}{6} (4 + 1) \) and Expected \( (C_1,C_2) = \frac{1}{36} (4 \cdot 5 + 2 \cdot 1) \) with Omega index 0.57.

The reason why we choose the Omega index is that it is, by definition, a valid measure when one, both or none of the two clusterings is overlapping as we discuss in Reference [24]. In addition, Omega index is an adjusted similarity measure that accounts for the by-chance agreements that might still exist between any two random clusterings over the same node-set.

For measuring similarity between two local communities $s_1, s_2$ , we use the Jaccard coefficient:

$$
J C = \frac {N \left(s _ {1} , s _ {2}\right)}{N \left(s _ {1}\right) + N \left(s _ {2}\right) - N \left(s _ {1} , s _ {2}\right)}, \tag {4}
$$

where $N(s_{1})$ refers to the number of actors in solution $s_1$ and $N(s_{1},s_{2})$ refers to the number of common actors between two solutions $s_1, s_2$ . The values of the Jaccard coefficient lie in the range [0,1] where 1 means perfect similarity and 0 means perfect dissimilarity.

To measure the accuracy of the solutions obtained by global methods with respect to a ground truth (Section 6.1.2), we resort again to the Omega index. The accuracy of local community detection methods (Section 6.2.1) has been evaluated by comparing pairwise similarities (using the Jaccard index) between a given actor (i.e., seed node) and the ground truth community it belongs to. The average Jaccard index over all actors is then used as the final accuracy score.

# 6 RESULTS

In this section, we present the experimental results of our comparative evaluation. Results of the comparative evaluation of global methods are reported in Section 6.1, while results related to the evaluation of local methods are reported in Section 6.2.

# 6.1 Global Methods

In this section, we report the experimental results of the comparative evaluation of global multiplex community detection methods. The section is structured as follows: Section 6.1.1 reports on the main properties of the community structures detected by the evaluated methods in different datasets. Section 6.1.2 presents the results of the accuracy analysis. Section 6.1.3 discusses the results of the pairwise comparison between different methods. Section 6.1.4 focuses on scalability.

Table 4. Statistics About the Community Structures Obtained on the AUCS Network (Results Averaged Over 10 Runs)   

<table><tr><td>method</td><td>#c</td><td>sc1</td><td>sc2/sc1</td><td>%n</td><td>%p</td><td>%ao</td><td>%no</td><td>%s</td></tr><tr><td>NWF</td><td>5.00</td><td>75.00</td><td>0.92 ± 0.01</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>WFEC</td><td>5.00</td><td>75.00</td><td>0.92 ± 0.01</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>ABACUS31</td><td>46.50 ± 3.44</td><td>29.90 ± 2.54</td><td>0.96 ± 0.02</td><td>0.70</td><td>0.00</td><td>0.96</td><td>0.61</td><td>0.00</td></tr><tr><td>ABACUS42</td><td>25.50 ± 3.58</td><td>29.20 ± 2.52</td><td>0.96 ± 0.03</td><td>0.59 ± 0.01</td><td>0.00</td><td>0.67 ± 0.01</td><td>0.39 ± 0.01</td><td>0.00</td></tr><tr><td>EMCD</td><td>11.00</td><td>70.00</td><td>0.92</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.45</td></tr><tr><td>PMMl</td><td>8.00</td><td>103.00 ± 21.00</td><td>0.49 ± 0.14</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.05 ± 0.08</td></tr><tr><td>PMMh</td><td>8.00</td><td>79.50 ± 16.94</td><td>0.75 ± 0.17</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.02 ± 0.05</td></tr><tr><td>SCML</td><td>8.00</td><td>66.00 ± 3.00</td><td>0.94 ± 0.11</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>ML-CPM31</td><td>40.00</td><td>59.00</td><td>0.61</td><td>0.61</td><td>0.00</td><td>0.93</td><td>0.46</td><td>0.00</td></tr><tr><td>ML-CPM42</td><td>11.00</td><td>18.00</td><td>0.88</td><td>0.27</td><td>0.00</td><td>0.34</td><td>0.11</td><td>0.00</td></tr><tr><td>LART</td><td>48.60 ± 0.66</td><td>51.00 ± 2.00</td><td>0.58 ± 0.02</td><td>1.00</td><td>0.98</td><td>0.01</td><td>0.00</td><td>0.91</td></tr><tr><td>Infomapno</td><td>5.09 ± 0.30</td><td>86.00 ± 8.30</td><td>0.80 ± 0.06</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>Infomapo</td><td>20.60 ± 0.80</td><td>157.00 ± 44.00</td><td>0.49 ± 0.12</td><td>1.00</td><td>0.30 ± 0.02</td><td>0.69 ± 0.02</td><td>0.69 ± 0.02</td><td>0.00</td></tr><tr><td>GLouvainl</td><td>7.50 ± 0.67</td><td>80.20 ± 7.34</td><td>0.85 ± 0.08</td><td>1.00</td><td>0.44 ± 0.08</td><td>0.55 ± 0.08</td><td>0.00</td><td>0.00</td></tr><tr><td>GLouvainh</td><td>5.00</td><td>76.50 ± 4.50</td><td>0.85 ± 0.04</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>MDLPA</td><td>6.70 ± 0.78</td><td>87.50 ± 18.74</td><td>0.65 ± 0.17</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr></table>

AUCS. $1 = 5$ , $a = 61$ , $e = 620$ .   
We denote with #c the number of communities, with sc1 the size of the largest community (number of nodes), with sc2/sc1 the ratio between the size of the second largest community and the largest, with $\% n$ the percentage of nodes assigned to at least one community, with $\% p$ the percentage of pillars, with $\% a_0$ the percentage of actors in more than one community, with $\% n_0$ the percentage of nodes in more than one community and with $\% s$ the percentage of singleton communities.

6.1.1 Basic Descriptive Statistics. As the first step of our comparative analysis, we analyzed the structural properties of the different community structures identified by the evaluated methods. Tables 4 and 5 present the statistics concerning the community structures obtained on the smallest (AUCS) and largest (Airports) of the real-world multiplex networks taken into account.

It can be observed how LART generates a number of communities that is higher than that of most other methods on all real networks. However a large percentage of these communities appear to be singletons, indicating that this algorithm mostly fails in aggregating nodes into communities. Other algorithms that appear to generate a relatively high number of communities regardless of the network structure are Infomap $_o$ and ABACUS, both variants. Interestingly, both retrieve a large number of communities without retrieving any singleton, showing a different behavior from LART. The discovery of many communities by Infomap $_o$ and ABACUS is associated with a high percentage of node overlapping. As regards to the size of the largest community, higher values correspond to $\mathsf{PMM}_l$ and Infomap $_o$ . On the other end, ABACUS (both variants) and ML-CPM $_{42}$ assign a small number of nodes to the largest communities, in both the AUCS and the Airports networks. This can be explained by the strong requirements that ABACUS and (even more) ML-CPM have to cluster nodes together. Concerning $sc2/sc1$ , we can observe how the values tend to be all relatively high for the smallest (AUCS) and largest (Airports) networks, indicating that in these cases the largest communities for each identified community structure have comparable sizes. An algorithm grouping most of the nodes together, and thus not able to structure them into separate communities, would have a very low value for $sc2/sc1$ .

Table 5. Statistics About the Community Structures Obtained on the Airports Network (Results Averaged Over 10 Runs)   

<table><tr><td>method</td><td>#c</td><td>sc1</td><td>sc2/sc1</td><td>%n</td><td>%p</td><td>%ao</td><td>%no</td><td>%s</td></tr><tr><td>NWF</td><td>6.80 ± 0.60</td><td>4,229.10 ± 653.56</td><td>0.77 ± 0.16</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>WFEC</td><td>6.70 ± 0.45</td><td>4,417.80 ± 532.90</td><td>0.73 ± 0.11</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>ABACUS31</td><td>5,320.30 ± 89.42</td><td>84.00</td><td>0.95</td><td>0.12</td><td>0.00</td><td>0.63</td><td>0.10</td><td>0.00</td></tr><tr><td>ABACUS42</td><td>4,086.40 ± 69.71</td><td>84.00</td><td>0.95</td><td>0.09</td><td>0.00</td><td>0.32 ± 0.01</td><td>0.07</td><td>0.00</td></tr><tr><td>EMCD</td><td>314.00</td><td>2,035.00</td><td>0.30</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.97</td></tr><tr><td>PMMl</td><td>10.00</td><td>14,289.40 ± 246.98</td><td>0.03 ± 0.01</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.48 ± 0.15</td></tr><tr><td>PMMh</td><td>10.00</td><td>2,171.90 ± 153.49</td><td>0.86 ± 0.07</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>SCML</td><td>10.00</td><td>4,336.39 ± 1128.68</td><td>0.46 ± 0.21</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>ML-CPM31</td><td>62.00</td><td>93.00</td><td>0.72</td><td>0.04</td><td>0.00</td><td>0.35</td><td>0.00</td><td>0.00</td></tr><tr><td>ML-CPM42</td><td>3.00</td><td>8.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>Infomapno</td><td>7.70 ± 1.55</td><td>10,330.40 ± 4920.32</td><td>0.17 ± 0.22</td><td>0.80 ± 0.35</td><td>0.79 ± 0.35</td><td>0.00</td><td>0.00</td><td>0.08 ± 0.10</td></tr><tr><td>Infomapo</td><td>34.10 ± 2.02</td><td>6,034.70 ± 993.70</td><td>0.83 ± 0.11</td><td>1.00</td><td>0.36</td><td>0.63</td><td>0.63</td><td>0.00</td></tr><tr><td>GLouvainl</td><td>11.20 ± 0.60</td><td>6,161.80 ± 382.63</td><td>0.35 ± 0.06</td><td>1.00</td><td>0.50</td><td>0.49</td><td>0.00</td><td>0.00</td></tr><tr><td>GLouvainh</td><td>9.50 ± 1.11</td><td>5,372.40 ± 333.32</td><td>0.57 ± 0.13</td><td>1.00</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr></table>

Airports. $\mathrm{l} = {37},\mathrm{a} = {417},\mathrm{e} = 3,{588}$ .   
We denote with $\#c$ the number of communities, with sc1 the size of the largest community (number of nodes), with sc2/sc1 the ratio between the size of the second largest community and the largest, with $\%n$ the percentage of nodes assigned to at least one community, with $\%p$ the percentage of pillars, with $\%ao$ the percentage of actors in more than one community, with $\%no$ the percentage of nodes in more than one community and with $\%s$ the percentage of singleton communities.

The values found in columns $\% n, \% p, \% ao$ and $\% no$ can be explained as follows:

- With regards to the percentage $\% n$ of nodes assigned to at least one community, as we discussed in Section 2, certain methods<sup>3</sup> are forced to provide a community assignment for each node: in these cases the value of $\% n$ will always be 1.   
- Regarding the percentage $\% p$ of pillars,both flattening methods always return pillar communities (since the information about layers is lost during the flattening process). Infomap and GLouvain can detect non-pillar clusters in theory. Data show how Infomap can return non-pillars both in the overlapping and in the non-overlapping version, while only GLouvain $l$ returns non-pillar communities.   
- The percentage of overlapping actors (\%ao) and nodes (\%no) mainly depends on the properties of the specific methods whether they allow overlapping (on the node level or the actor level) or not.   
- The percentage of singleton communities $\% s$ appears to be extremely high in the case of LART and EMCD and high in the case of $\mathsf{PMM}_l$ . It should be noted that, with the exception of Infomap, that returns a small fraction of singletons in the Airports network, the methods that return singletons in the AUCS network return a larger percentage of singletons in the Airports network suggesting that the behaviour is not induced by the network but amplified by its complexity.

6.1.2 Accuracy Analysis. With the aim of answering Q1 (i.e., "To what extent are the evaluated methods able to detect ground truth communities?", cf. Section 5), we perform here an extensive quantitative analysis about the accuracy obtained by each method with respect to ground truth communities. For real-world networks, only two of them have an available ground truth: AUCS

![](images/b601604f116fc2e0b1db1864f55994bd41b4bd3c2e4d9770bca29df51ba43ac6.jpg)

![](images/9bdde08b624f6c526ed421e4ea229c3f92818366d4a0925dc5e27a376976a3c4.jpg)

![](images/552b265b1746ab5860eedd64fce180e0b03807dfa5526a0816f7e2b699b2f716.jpg)  
(a) Pillar Equal Partitioning (PEP)   
(c) Semi-pillar Equal Partitioning (SEP)

![](images/57413d1b34633606d9cf818cf41d9618a660bc1aadf6adf339c5c9a5e26030f6.jpg)  
(b) Pillar Non-equal Partitioning (PNP)   
(d) Pillar Equal Overlapping (PEO)   
Fig. 9. Accuracy with respect to a ground truth, Omega index, selected synthetic networks.

(i.e., affiliations to research groups) and DKPol (i.e., affiliation to political parties). All synthetic networks come with controlled ground truth.

A selection of our results is reported in Figures 9, 10, and 11. From these figures, we can see how the main element playing a role in methods' accuracy is the pillar nature of the community structure.

In the case of Pillar Equal Partitioning (PEP) structures almost all the methods perform very well, with $\mathsf{WF}_{EC}$ , NWF, Infomap, and GLouvain (both versions) reaching perfect accuracy. Overall, only ML-CPM (both versions) and LART score below 0.5. In the first case, the strict rules imposed by its parameters explain the performance, for the latter, as we saw in Table 5 LART does not seem to be able to group a considerable number of nodes into communities. Similar patterns, even if

![](images/67e43ec278242403717ce141cf90717a8cfc0c67378d27cb628d94ee5501343c.jpg)  
(a) Mixed (MIX)

![](images/a978e8010455c03e0b6d078b682d0613664b8d1cd170eb897a32d5f1a7214feb.jpg)  
(b) Hierarchical (HIE)

![](images/b50458dd5b5ea479691b03055b29ab45465143d93d55c520bdf8ab6bcb93a579.jpg)  
Fig. 10. Accuracy with respect to a ground truth, Omega index, mixed and hierarchical communities.   
(a) AUCS

![](images/d00c331946920df4556f005e39ecc061cb6fada782d537a546e17c7998ef9492.jpg)  
(b) DKPol   
Fig. 11. Accuracy with respect to a ground truth for real-world networks, measured using Omega index.

with worse levels of accuracy, are visible for all the Pillar structures (PNP, PEO, PNO). Minor notable differences are present in the Pillar Non-equal Partitioning structure where Infomap (both variations) performs better than all the other methods (that also score above 0.8). Despite the positive results for many methods, one could easily ask if in the general context of pillar community structures proper multilayer methods are necessary, since the same (good) results can be achieved with flattening-based methods.

The more the network moves away from a pillar structure (with semi-pillar, mixed and hierarchical structures) the worse the results are among most of the methods. A notable exception is

ABACUS that, regardless of the variation, keeps performing above the average with Semi-Pillar and Mixed Communities, with ML-CPM<sub>31</sub> also performing better than most other methods on Semi-Pillar structures. Hierarchical structures are extremely challenging for all the methods with the notable exceptions of ML-CPM<sub>31</sub> and GLouvain<sub>l</sub>, although GLouvain is finding communities on individual layers and thus it is not clearly identifying any hierarchy spanning multiple layers.

The reason why some methods have an Omega index around 0 is that in these cases these methods only find one or two large communities. This is not surprising if we consider the structures of some synthetic datasets. In the overlapping community structures, all the communities are kept together by their overlapping parts, and in the semi-pillar structures the well-separated semi-pillar communities spanning a subset of the layers result connected by the different communities on the remaining layers.

These results may indicate that, even though for simple Pillar Equal Partitioning structures multilayer methods do not seem to provide any real advantage over flattening-based methods, more complex structures show how proper multilayer methods can perform better than flattening-based methods.

Figure 11 reports on the accuracy obtained by the evaluated methods on real-world networks. It can be observed how accuracy values are relatively low on both networks for all methods, i.e., with Omega index always below 0.8 and often below 0.5. More interestingly, the best performing methods do not entirely overlap with the methods that perform the best with the synthetic data. On AUCS, the best performing method is SCML (0.70), followed by EMCD.

The results are even more variable on DKPol, where many methods show low results. An exception to this is the two variants of GLouvain, reaching accuracies of 0.68 (GLouvain $_h$ ) and 0.43 (GLouvain $_l$ ), respectively. SCML, NWF, WF $_{EC}$ , and EMCD also perform relatively well with scores around 0.6.

As a final remark, the difference in performance between real-world and synthetic networks confirms how the "ideal" concept of community, i.e., the one based on topological density that is used to build the synthetic ones and to drive the detection process of the methods, is often far from the ground truth communities observed in real cases (which are, in turn, often questionable and subjective). This is a well known problem in the community detection field, and poses challenges in both ways, i.e., concerning the need to design both more powerful methods and more reliable ground truths.

6.1.3 Pairwise Comparison Analysis. To answer Q2 (i.e., "To what extent do the evaluated methods produce similar community structures?", cf. Section 5), we performed pairwise comparisons between the selected methods, to determine the similarity between the community structures produced by each pair of methods on each network.

Figure 12 reports on the results of pairwise analysis among Pillar Equal Partitioning and Semi-Pillar Non-equal Partitioning, with Omega index values for the pairwise similarities. We show Omega index values for a matter of homogeneity, since NMI cannot be applied to overlapping solutions.

These results confirm and expand the understanding of the methods we have described so far. In the case of Pillar Equal Partitioning networks, almost all the methods produce very similar structures, with the notable exception of ML-CPM and LART. In the case of Semi-Pillar Partitioning communities the similarities are much smaller with few notable exceptions: $\mathrm{Infomap}_{no}$ returns communities extremely similar to those returned by GLouvain $_h$ and both also show a strong

![](images/ddf4cd0b5257b7586dba33858ef45a365f4bf6fd54f563f4c6cd2658fe586d33.jpg)

![](images/34c2960280eb94436ef2793949a209511aa9ab59ef6b481d8b723ac2ef832de1.jpg)  
(a) PEP   
(b) SNP   
Fig. 12. Pairwise comparison, Omega index: pillar and semi-pillar partitioning communities.

similarity (0.7) with the communities returned from the flattening-based methods. Results for other data are not reported here for space reasons, but confirm the same trends highlighted by the analysis of accuracy. Node-partitioning methods may produce similar community structures on specific cases (i.e., depending on the methods and the target network), suggesting that, when

![](images/57592168259cd09c60fc1cfabe8811611afa9d82741d142f28a46447b16edc35.jpg)  
(a)

![](images/f0d46fb1f6a68bec55b6d9e8bfcb96ee2c5760e23764d037163d16d5bc6ede31.jpg)  
(b)

![](images/1346721192b9b70bdba08f63a14c2073db5e5732e9c81eb7ef868b5d313bf4f8.jpg)  
Fig. 13. Scalability of different community detection methods with respect to the number of actors.   
(a)

![](images/a5fb641dcc8d9973842631e37b293b90192e556eab1bbc6578bd9cca1f4d505a.jpg)  
(b)   
Fig. 14. Scalability of different community detection methods with respect to the number of layers.

multiple community memberships are not allowed, some communities will often be unambiguously recognized in the network topology. Conversely, multiple community memberships allowed by overlapping methods end up in extremely variate solutions, i.e., relatively low similarities are observed regardless of the selected network and pair of methods.

6.1.4 Scalability Analysis. To answer Q3 ("To what extent are the evaluated methods scalable?", cf. Section 5), we tested the scalability of the selected methods with respect to number of actors and number of layers. The reported results were obtained on a MacOS Catalina system version 10.15.5 with a 2,4 GHz Dual-Core Intel Core i7 processor and 16 GB of RAM.

Figures 13-14 report the scalability of each method with respect to an increment in the number of actors and the number of layers, respectively. Note that in both cases the scalability of the flattening algorithms largely depends on the one of the community detection method used at the final step, since the computational cost of the flattening process is irrelevant. Some methods proved to be extremely scalable, more specifically, EMCD and Infomap—all of which could run in less than a minute on networks containing up to 8,000 actors. However, EMCD takes single-layer community structures as input, therefore the time to find these communities is not counted in the plot. Considering the whole process, we would find EMCD close to the flattening methods.

![](images/89e2545fb16a427ecb3b89a7a285fc29dcbb8c27f9f9723bb74cc02ebbcb5c14.jpg)  
(a) AUCS

![](images/a4d17ed50f0ca92181153d146731d3437235fb8e5caf843a83436b94c0783868.jpg)  
(b) DKPol   
Fig. 15. Average accuracy of the local methods with respect to a ground truth, on real-world networks.

ML-CPM (both variations), MLink and LART proved to be much less scalable, with a running time quickly increasing with the number of actors.

As regards to the scalability in the number of layers (Figure 14), we see that, generally speaking, it affects the results less than the number of actors. Only four methods show some significant increase in execution time: ML-CPM with $m = 1$ , MLink, LART, and MDLPA. The behavior of MDLPA is in accordance with its theoretical time complexity.

# 6.2 Local Methods

In this section, we report the experimental results of the comparative evaluation of local multiplex community detection methods. The section is structured as follows: Section 6.2.1 presents the results of the accuracy analysis, Section 6.2.2 reports on the results of the pairwise comparison between different methods, while Section 6.2.3 discusses scalability issues.

6.2.1 Accuracy Analysis. We performed an accuracy analysis on the local community detection methods, by comparing the local community of each actor to the one that same actor belongs to in the ground truth. Similarity is computed using the Jaccard index, while the final accuracy value is the average over all actors.

Figure 15 shows results on real-world networks. On AUCS, accuracy is in the range of 0.5-0.7 for four of five methods, with ML-LCD $(wlsim)$ being the best performer (0.7). Much lower accuracy values were obtained on DKPol, where the best performing method was ML-LCD $(lwsim)$ (0.27).

Concerning synthetic networks, we limited our analysis to networks with a pillar partitioning community structure (PEP and PNP), for compatibility with the methods' output (both return actor communities). In these cases, we observed that accuracies are much higher than the ones observed for real-world networks, with all values in the range [0.8,1.0]. ML-LCD $(clsim)$ is the best performing method, since it is able to perfectly identify the ground truth community structure on both networks.

Summarizing, while all methods proved to be able to identify synthetic pillar community structures, their performance was much worse on real-world networks. These results confirm the behavior observed for global methods (cf. Section 6.1.2). Moreover, it should be pointed out that comparing a global community structure (i.e., the ground truth) to a set of local ones (i.e., the results obtained by local methods on all actors) may not be completely fair. The ground truth in this

![](images/ce8220e4d64a308f33ae00dc1cc8040495c6be7ddc123b1ee08c6734b36c9841.jpg)  
(a) AUCS

![](images/8dffe990c550e4a976f4fa98512673200bed9e8c6505884fecdb9ad211dc6478.jpg)  
(b) DKPol   
Fig. 16. Average pairwise similarity among the different local methods on real-world networks.

case represents a global partitioning of the network, while local communities are actor-centered, query dependent and, in general, they overlap with each other. Moreover, they may be discovered without having a complete knowledge of the network graph, which is the case for ML-LCD. Although based on the comparison of conceptually different objects (i.e., global and local communities), our accuracy analysis is still significant as it quantifies to what extent the local community formed around a certain actor falls inside the community found in the global structure that contains the actor. Unfortunately, no networks with an associated ground truth of multiplex local communities are available at the time of writing.

6.2.2 Pairwise Comparison. As seen in Section 6.1.3 for global methods, we set up an equivalent evaluation stage based on pairwise comparison between the local methods. In this case, we resorted to the Jaccard index to measure the similarity of the community solutions produced by two local methods. Since these methods are query-dependent (i.e., they return the local community of a given query/seed node), we computed the Jaccard similarity between each pair of communities obtained using the same actor as seed, and then averaged the results over all actors.

Figure 16 reports on the results obtained on real-world networks. On most of these networks (DKPol, Airports, and Rattus), we can note that communities identified by different variants of ML-LCD and ACLcut tend to be very different. Looking at AUCS, the communities identified by all variants of both ML-LCD and ACLcut tend to be less different and a higher similarity can be observed among the three variants of ML-LCD.

For synthetic networks (Figure 17), it can be noted how similarities are higher for networks based on pillar community structures. In some cases (i.e., PEP and PNP) all methods are practically interchangeable, with all similarities equal or near to 1.0. In other networks with pillar (i.e., PEO and PNO), semi-pillar (i.e., SEP and SEO), or both (MIX and HIE) community structures, similarities are stronger between the different variants of each method. Summing up, we observed some similarities in the behavior of all local methods on some real-world and synthetic networks, with an expected tendency of the variants of a same method to identify similar local communities. Nevertheless, this cannot be taken as a general rule, since we also observed specific cases where all methods behaved differently from each other, both on real-world and synthetic networks.

6.2.3 Scalability Analysis. We tested the scalability of local community detection methods in terms of number of actors and number of layers. To carry out the experiment, we used the synthetic networks already used for the global case (Section 6.1.4). For each network, we present median

![](images/abad6a5646fd8bd1aa29bc88f0d1d0c22882bb97d3181571657d2871d67d7e3f.jpg)  
(a) PEP

![](images/65f0b81f030c0fa417692b2adc9e42335c35e18fd383796bb2eb84b41915da9b.jpg)  
(b) PEO

![](images/23170951a56f74c95dfd3e0ba372e912368a9c8936ed3f82c8af62f1bd6d2dd5.jpg)  
(c) HIE

![](images/e427f6f6052aca4002b8e72f09b0d31482274356ff2741b7761e0be760ba4263.jpg)  
(d) MIX

![](images/e4009cc2023ff1d27ecde147b7f1174f8be45fda15572c36e9e0d3b25b643c32.jpg)  
Fig. 17. Average pairwise similarity among the different local methods when the same seed is used as an input, on selected synthetic networks.   
(a) $\mathbf{ACLcut}_c$

![](images/b5e21cc2474ccd5fade6dbf5b9f654ee4ffebfe88e7ac228e3d20822a27588c3.jpg)  
(b) ML-LCD(clsim)   
Fig. 18. Median scalability of local methods with respect to the number of actors in the multiplex network.

execution times obtained on 100 random seeds. For each method, we choose the least scalable variant as a representative of that method's scalability.

Figures 18 and 19 show results related to scalability in terms of number of actors and of layers, respectively. Both methods showed a similar good scalability, with ML-LCD showing a higher dispersion depending on the chosen seeds.

![](images/c87e90f9b9b7fa2b5f32165941aaa27b052c3d3a3755f978ac9c238d54823a0c.jpg)  
(a) $\mathbf{ACLcut}_c$

![](images/247d42cb88a4ae3c194e67ac48c330a9f41ab23de13b6db8437061d9c751b8b6.jpg)  
(b) ML-LCD(clsim)   
Fig. 19. Median scalability of local methods with respect to the number of layers in the multiplex network.

# 7 DISCUSSION

Our experimental study had two main outcomes. First, it allowed us to identify guidelines about which methods can be the most appropriate for the data and the task at hand. Second, observing in which cases the reviewed methods consistently failed in identifying the expected communities allowed us to identify the multiplex community structures that are challenging with the currently available community detection algorithms. While the comparative evaluation of community detection methods is a complex process, and additional types of analysis should also be considered in the future, such as the approach used by Reference [21] for single-layer methods, our work highlights a set of open problems for community detection methods in multiplex networks.

Accuracy analysis on synthetic networks has revealed that most of the methods perform very well when the community structure is made of disjoint pillars. Among the many well-performing methods, Infomap and SCML are consistently discovering community structures that are close or equal to the ground truth, GLouvain and the methods using Louvain are also performing well but have some issues with communities of varying size. whereas ML-LCD $(clsim)$ appears to be the best choice among the local methods. It is worth noticing that simpler flattening methods are also among the best methods.

With regard to non-pillar community structures, we have observed a considerable reduction in the achieved accuracy scores for almost all methods. This observation raises the following question: what kind of assumptions are considered by different methods when multiplex communities are identified? It is clear that there is a tendency, even if not always explicitly declared, to assume that multiplex communities are pillar communities expanding over all the layers of the multiplex network. For instance, multi-slice modularity [42] rewards pillar communities when calculating the modularity score, and spectral methods assume the existence of a latent community structure at actor level. While pillar community structures are perfectly reasonable and can be assumed to exist in many scenarios, they are also the simplest possible cases we tested in this article. As multiplex approaches have been developed to overcome the oversimplification of monoplex networks, relying on a single type of ideal community structure seems, at least, a missed opportunity. Thus, more work has to be done on improving the accuracy of community detection methods for non-pillar community structures.

A second set of considerations can be drawn by looking at the results obtained by the evaluated methods when applied to real-world datasets. Our experiments have shown that, on real-world datasets, the detected community structures largely differ from the ground truth. This raises two

interesting questions. First, to which extent is the assumed ground truth itself a valid assumption? In other words, does the ground truth given for a real dataset always describe the community structures identified by a community detection method, or does it capture only one part of the whole picture? The answer to this question is never trivial even in monoplex networks. Nevertheless it is easy to see how adding more layers makes it further complicated. For example, both DKPol and AUCS ground truths group together individuals belonging to the same organization (political parties in one case and research groups in the other). The question then becomes whether it is reasonable to assume that the selected relations, observed in the multiplex networks, will produce a community structure corresponding to this formal grouping, and to some extent, how different relations (thus different layers) can be more or less aligned with the hypothesis described above. Will members of the same research group work together, or publish together? Have lunch and fun together? Will members of the same political party retweet each other on Twitter, and reply to each other? Indeed, looking at the accuracy of the community structures identified for the real world datasets, especially in the case of DKPol, one might ask whether we are observing a generalized failure of the community detection methods, or conversely, whether the community detection methods were actually able to observe relevant structures that were just different from the community structures assumed in the ground truth.

The second question, which is strongly related to the first one, is whether all the layers included in these datasets positively contribute to an accurate identification of the community structure in these datasets, or whether some of them add more noise that heavily affect the identification process. Indeed, the fact that most of the community detection methods always give an output, no matter what layers are included in the input multiplex network, makes the inclusion of more input layers potentially problematic. Layers, besides being defined by a specific internal topology, are also defined by internal logics that might or might not be coherent with those of the other layers. The DKPol dataset represents a good example of this problem. Some detailed analysis of the three layers composing the multiplex network has shown that retweets and following/follower interactions follow relatively assortative dynamics for political parties. The replies, however, are more frequent between members of different political parties. Here, we think that more efforts have to be made in the modelling phase of the multiplex network and some layer-specific measures should be developed to lead the choice of the layers that contribute to the identification of the communities. Several such multilayer network simplification methods exist, and more can be developed, as reviewed in Reference [26].

A separate consideration should be made about the similarities of the obtained results. Focusing, for the above-mentioned reason, mainly on the results obtained from the synthetic networks, it is possible to observe some general patterns. Global partitioning methods show a remarkable level of similarity in detecting community structures based on a pillar-like model. Semi-pillar and hierarchical community structures show a lower degree of similarity between the retrieved community structures. We should also consider that differences in the results of different algorithms may be partially due to the fact that some algorithms use heuristics to optimize an objective function (e.g., generalized Louvain), therefore they might not achieve the optimal value.

Local methods show a behavior that is, to some extent, similar to the global partitioning methods. When tested on pillar communities they show a remarkable similarity between the produced communities, which can easily lead to calling them interchangeable. Nevertheless, the less pillar-like the community structure in the data is, the higher the differences seem to be at first between ACLcut and ML-LCD and then also between different settings of the same algorithm.

Scalability analysis has also provided useful information about specific methods with scalability issues, which can be used to select feasible approaches depending on the data.

We would also like to draw additional remarks that might be considered mainly by practitioners. Community detection remains a challenging task, and further complicated in multilayer networks, which is testified by the plethora of available approaches and methods, most of which have been studied in our extensive survey, while new others are currently under development at the time of this writing. From a practical viewpoint, the core problems are, on the one hand, (i) to select the most suited algorithm and parameterization for a target application domain and, on the other hand, (ii) to have it clear in mind what kind of community we are interested in or we expect to detect. Problem (i) should be addressed by taking into account that community detection methods, especially if belonging to different methodological approaches, will easily discover different patterns in a multilayer network, mainly because every method has its own bias resulting from the optimization of different criteria. We believe this variety of choice should not be seen as a negative point, but rather as an opportunity to find out communities with different structures and related meanings. Also, if the need for having a unified solution from different available ones still remains as a priority, the ensemble-based consensus approach could be considered as the way to go. Understanding problem (ii) will nonetheless be crucial in most cases, as it may pose a requirement for the structure of the communities to be discovered, thus possibly impacting on the choice of the method to be used. In any case, this will also depend on the actual presence of communities of a desired form in the input network; for instance, any method based on the identification of cliques of a given size will likely fail if such cliques are rare or missing at all in the input network. Therefore, one suggestion in this regard would be to deepen as much as possible the study of structural micro/mesoscopic characteristics of the input network, both in its entirety as a complex system and at the level of its constituent layers, to better prepare the subsequent analysis for the community detection task.

Despite the complexity of the multiplex community detection task emerging from our study, we would like to conclude our discussion on a positive note. There are many cases where we have a good expectation of what type of community structures could be found in the data. One example is the simple case of actor communities that expand over multiple layers, as in the AUCS network where people inside the same research group work together, publish papers together and go to lunch together—although the multilayer data allows us to appreciate how administrative people are part of the community only on some layers, and not for example on the co-authorship one. Another example are hierarchical communities where the layers represent different organizational levels, e.g., University-level interactions, Department-level interactions, research-group-level interactions, and so on. Overlapping can also be expected inside data describing flexible organizations with people having multiple roles. These examples share the same features of some of our synthetic networks (Pillar, Hierarchical, Overlapping). Therefore, domain knowledge about what type of communities to expect can be used together with our accuracy (and scalability, in case of larger networks) plots to determine which algorithms to prioritize.

# 8 CONCLUDING REMARKS

This work has highlighted some facts. When the community structure is simple (pillar, non overlapping communities of similar size), we can expect most of the reviewed methods to work well. Therefore, scalability considerations may be used to choose the best algorithm. When we depart from this simple type of communities it becomes more difficult to identify them, and the concept of type of community structure itself requires more research. We can however see how some more sophisticated approaches not relying on flattening can be more successful in specific cases. Given the different types of communities identified by different methods, it can be valuable to try multiple approaches while exploring multiplex network data. The difficulty to handle some of the data suggests that more research in network preprocessing would be valuable

[26], in addition to the development of new community detection methods. Community detection in multiplex networks is an active area, and it will be interesting to see how new algorithms address the challenges highlighted in this work. The code used for the experiments is available at https://bitbucket.org/uuinfolab/20csur and can be extended to include additional methods and data.

# REFERENCES

[1] Nazanin Afsarmanesh and Matteo Magnani. 2018. Finding overlapping communities in multiplex networks. In Proceedings of the International Conference on Social Informatics (SocInfo'18).   
[2] Michael J. Barber. 2007. Modularity and community detection in bipartite networks. Phys. Rev. E 76 (2007), 066102.   
[3] Marya Bazzi, Lucas G. S. Jeub, Alex Arenas, Sam D. Howison, and Mason A. Porter. 2020. A framework for the construction of generative models for mesoscale structure in multilayer networks. Phys. Rev. Res. 2, 4 (2020), 023100. Retrieved from https://link.aps.org/doi/10.1103/PhysRevResearch.2.023100.   
[4] Michele Berlingerio, Michele Coscia, and Fosca Giannotti. 2011. Finding and characterizing communities in multidimensional networks. In Proceedings of the International Conference on Advances in Social Networks Analysis and Mining (ASONAM'11). IEEE Computer Society Washington, DC, 490-494.   
[5] Michele Berlingerio, Fabio Pinelli, and Francesco Calabrese. 2013. ABACUS: Frequent pAttern mining-BAsed Community discovery in mUltidimensional networkS. Data Min. Knowl. Discov. 27, 3 (2013), 294-320. arxiv:arXiv:1303.2025v2   
[6] Brigitte Boden, Stephan Gunnemann, Holger Hoffmann, and Thomas Seidl. 2012. Mining coherent subgraphs in multi-layer graphs with edge labels. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'12). ACM Press, 1258. DOI: https://doi.org/10.1145/2339530.2339726   
[7] Cecile Bothorel, Juan David Cruz, Matteo Magnani, and Barbora Micenkova. 2015. Clustering attributed graphs: Models, measures and methods. Netw. Sci. 3, 3 (2015), 408-444.   
[8] Oualid Boutemine and Mohamed Bouguessa. 2017. Mining community structures in multidimensional networks. ACM Trans. Knowl. Discov. Data 11, 4 (June 2017), 1-36. DOI: https://doi.org/10.1145/3080574   
[9] Piotr Bródka. 2016. A method for group extraction and analysis in multilayer social networks. CoRR, abs/1612.02377. https://arxiv.org/abs/1612.023775.   
[10] Piotr Bródka, Anna Chmiel, Matteo Magnani, and Giancarlo Ragozini. 2018. Quantifying layer similarity in multiplex networks: A systematic study. Roy. Soc. Open Sci. (2018). DOI: https://doi.org/10.1098/rsos.171747 Retrieved from https://arxiv:1711.11335.   
[11] Piotr Bródka, Tomasz Filipowski, and Przemysław Kazienko. 2013. An introduction to community detection in multilayered social network. In Information Systems, E-learning, and Knowledge Management Research. Springer, Berlin, 185-190. DOI: https://doi.org/10.1007/978-3-642-35879-1_23   
[12] Piotr Bródka, Krzysztof Skibicki, Przemyslaw Kazienko, and Katarzyna Musial. 2011. A degree centrality in multilayered social network. In Proceedings of the International Conference on Computational Aspects of Social Networks (CASoN'11). IEEE, 237-242. DOI: https://doi.org/10.1109/CASON.2011.6085951 Retrieved from https://arxiv:1210.5184.   
[13] Alessio Cardillo, Jesús Gómez-Gardeñas, Massimiliano Zanin, Miguel Romance, David Papa, Francisco del Pozo, and Stefano Boccaletti. 2013. Emergence of network features from multiplexity. Sci. Rep. 3, 1344 (2013). https://doi.org/10.1038/srep01344   
[14] Jiyang Chen, Osmar R. Zaïane, and Randy Goebel. 2009. Local community identification in social networks. In Proceedings of the International Conference on Advances in Social Network Analysis and Mining (ASONAM'09). 237-242.   
[15] Linda M. Collins and Clyde W. Dent. 1988. Omega: A general formulation of the rand index of cluster recovery suitable for non-disjoint solutions. Multivar. Behav. Res. 23, 2 (Apr. 1988), 231-242.   
[16] Manlio De Domenico, Andrea Lancichinetti, Alex Arenas, and Martin Rosvall. 2015. Identifying modular flows on multilayer networks reveals highly overlapping organization in interconnected systems. Phys. Rev. X 5 (Mar. 2015), 011027. Issue 1. DOI: https://doi.org/10.1103/PhysRevX.5.011027   
[17] Manlio De Domenico, Vincenzo Nicosia, Alexandre Arenas, and Vito Latora. 2015. ARTICLE structural reducibility of multilayer networks. Nature Commun. 6, 6864 (2015). https://doi.org/10.1038/ncomms7864   
[18] Mark E. Dickison, Matteo Magnani, and Luca Rossi. 2016. Multilayer Social Networks. Cambridge University Press.   
[19] Xiaowen Dong, Pascal Frossard, Pierre Vandergheynst, and Nikolai Nefedov. 2014. Clustering on multi-layer graphs via subspace analysis on grassmann manifolds. IEEE Trans. Signal Process. 62, 4 (Feb. 2014), 905-918. DOI: https://doi.org/10.1109/TSP.2013.2295553   
[20] Daniel Edler, Ludvig Bohlin, and Martin Rosvall. 2017. Mapping higher-order network flows in memory and multilayer networks with Infomap. Retrieved from https://arxiv:1706.04792.

[21] Amir Ghasemian, Homa Hosseinmardi, and Aaron Clauset. 2019. Evaluating overfit and underfit in models of network community structure. IEEE Trans. Knowl. Data Eng. 32, 9 (2019), 1722-1735. DOI: https://doi.org/10.1109/TKDE.2019.2911585   
[22] Roger Guimera, Marta Sales-Pardo, and Luis A. Nunes Amaral. 2007. Module identification in bipartite and directed networks. Phys. Rev. E 76 (2007), 036102.   
[23] Obaida Hanteer and Matteo Magnani. 2020. Unspoken assumptions in multi-layer modularity maximization. Sc. Rep. 10, 1 (2020), 11053. DOI: https://doi.org/10.1038/s41598-020-66956-0   
[24] Obaida Hanteer and Luca Rossi. 2019. The meaning of dissimilar: An evaluation of various similarity quantification approaches used to evaluate community detection solutions. In Proceedings of the IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. 513-518.   
[25] Obaida Hanteer, Luca Rossi, Davide Vega D'Aurelio, and Matteo Magnani. 2018. From interaction to participation: The role of the imagined audience in social media community detection and an application to political communication on Twitter. In Proceedings of the IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,ASONAM, Ulrik Brandes, Chandan Reddy, and Andrea Tagarelli (Eds.). IEEE Computer Society, 531-534. DOI: https://doi.org/10.1109/ASONAM.2018.8508575   
[26] Roberto Interdonato, Matteo Magnani, Diego Perna, Andrea Tagarelli, and Davide Vega. 2020. Multilayer network simplification: Approaches, models and methods. Comput. Sci. Rev. 36 (2020), 100246. DOI: https://doi.org/10.1016/j.cosrev.2020.100246   
[27] Roberto Interdonato, Andrea Tagarelli, Dino Iencó, Arnaud Sallaberry, and Pascal Poncelet. 2017. Node-centric community detection in multilayer networks with layer-coverage diversification bias. In Proceedings of the 8th Conference on Complex Networks (CompleNet'17). Springer International Publishing, 57–66.   
[28] Lucas G. S. Jeub, Michael W. Mahoney, Peter J. Mucha, and Mason A. Porter. 2017. A local perspective on community structure in multilayer networks. *Netw. Sci.* 5, 2 (2017), 144–163. DOI: https://doi.org/10.1017/nws.2016.22   
[29] Inderjit S. Jutla, Lucas G. S. Jeub, and Peter J. Mucha. 2011-2017. A Generalized Louvain Method for Community Detection Implemented in Matlab. Technical Report. Retrieved from http://github.com/GenLouvain.   
[30]ungeun Kim and Jae-Gil Lee. 2015. Community detection in multi-layer graphs. ACM SIGMOD Rec. 44, 3 (2015), 37-48.   
[31]ungeun Kim, Jae-gil Lee, and Sungsu Lim. 2016. Differential flattening: A novel framework for community detection in multi-layer graphs. ACM Trans. Intell. Syst. Technol. 8, 2 (2016), 27.   
[32] Mikko Kivelä, Alexandre Arenas, Marc Barthelemy, James P. Gleeson, Yamir Moreno, and Mason A. Porter. 2014. Multilayer networks. J. Complex Netw. 2, 3 (Sep. 2014), 203-271. DOI: https://doi.org/doi:10.1093/comnet/cnu016   
[33] Zhana Kuncheva and Giovanni Montana. 2015. Community detection in multiplex networks using locally adaptive random walks. In Proceedings of the IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM. ACM Press, 1308-1315. DOI: https://doi.org/10.1145/2808797.2808852   
[34] Andrea Lancichinetti and Santo Fortunato. 2012. Consensus clustering in complex networks. Sci. Rep. 2, 336 (2012).   
[35] Huajing Li, Zaiqing Nie, Wang-Chien Lee, Lee Giles, and Ji-Rong Wen. 2008. Scalable community discovery on textual data with relations. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM'08). ACM, New York, NY, 1203-1212. DOI: https://doi.org/10.1145/1458082.1458241   
[36] Yixuan Li, Kun He, David Bindel, and John E. Hopcroft. 2015. Uncovering the small community structure in large networks: A local spectral approach. In Proceedings of the 24th International Conference on World Wide Web (WWW'15). 658-668.   
[37] Chuan Wen Loe and Henrik Jeldtoft Jensen. 2015. Comparison of communities detection algorithms for multiplex. Physica A 431 (2015), 29-45. DOI: https://doi.org/10.1016/j.physa.2015.02.089 Retrieved from https://arxiv:arXiv:1406.2205v1.   
[38] Lijia Ma, Maoguo Gong, Jianan Yan, Wenfeng Liu, and Shanfeng Wang. 2018. Detecting composite communities in multiplex networks: A multilevel memetic algorithm. Swarm Evolution. Comput. 39 (Apr. 2018), 177-191. DOI: https://doi.org/10.1016/J.SWEVO.2017.09.012   
[39] Matteo Magnani and Luca Rossi. 2011. The ML-model for multi-layer social networks. In Proceedings of the International Conference on Advances in Social Networks Analysis and Mining (ASONAM'11). DOI: https://doi.org/10.1109/ASONAM.2011.114   
[40] Matteo Magnani and Luca Rossi. 2013. Formation of multiple networks. In Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics).   
[41] Raul J. Mondragon, Jacopo Iacovacci, and Ginestra Bianconi. 2018. Multilink communities of multiplex networks. PLOS One 13, 3 (Mar. 2018), e0193821. DOI: https://doi.org/10.1371/journal.pone.0193821   
[42] Peter J. Mucha and Mason A. Porter. 2010. Communities in multislice voting networks. *Chaos* 20, 4 (2010). DOI: https://doi.org/10.1063/1.3518696

[43] Peter J. Mucha, Thomas Richardson, Kevin Macon, Mason A. Porter, and Jukka-Pekka Onnela. 2010. Community structure in time-dependent, multiscale, and multiplex networks. Science 328, 5980 (May 2010), 876-878. DOI: https://doi.org/10.1126/science.1184819   
[44] Gabriel Murray, Giuseppe Carenini, and Raymond Ng. 2012. Using the omega index for evaluating abstractive community detection. In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization. Association for Computational Linguistics, 10-18.   
[45] Vincenzo Nicosia, Ginestra Bianconi, Vito Latora, and Marc Barthelemy. 2013. Growing multiplex networks. Phys. Rev. Lett. 111 (2013), 058701. Retrieved from http://prl.aps.org/abstract/PRL/v111/i5/e058701.   
[46] Leto Peel, Daniel B. Larremore, and Aaron Clauset. 2017. The ground truth about metadata and community detection in networks. Sci. Adv. 3, 5 (May 2017), e1602548. DOI: https://doi.org/10.1126/sciadv.1602548   
[47] Clara Pizzuti and Annalisa Socievole. 2017. Many-objective optimization for community detection in multi-layer networks. In Proceedings of the IEEE Congress on Evolutionary Computation (CEC'17). IEEE, 411-418. DOI: https://doi.org/10.1109/CEC.2017.7969341   
[48] Guo-Jun Qi, Charu C. Aggarwal, and Thomas Huang. 2012. Community detection with edge content in social media networks. In Proceedings of the IEEE 28th International Conference on Data Engineering (ICDE'12), Vol. 00. 534-545. DOI: https://doi.org/10.1109/ICDE.2012.77   
[49] Luca Rossi and Matteo Magnani. 2015. Towards effective visual analytics on multiplex and multilayer networks. Chaos, Solitons Fractals 72 (2015). DOI: https://doi.org/10.1016/j.chaos.2014.12.022   
[50] Yiye Ruan, David Fuhy, and Srinivasan Parthasarathy. 2012. Efficient community detection in large networks using content and links. https://arxiv.org/abs/1212.0146.   
[51] Arlei Silva, Wagner Meira, Jr., and Mohammed J. Zaki. 2012. Mining attribute-structure correlated patterns in large attributed graphs. Proc. VLDB Endow. 5, 5 (Jan. 2012), 466-477. DOI: https://doi.org/10.14778/2140436.2140443   
[52] Chris Stark, Bobby-Joe Breitkreutz, Teresa Reguly, Lorrie Boucher, Ashton Breitkreutz, and Mike Tyers. 2006. BioGRID: A general repository for interaction datasets. *Nucleic Acids Res.* 34 (2006), D535–D539.   
[53] Yizhou Sun and Jiawei Han. 2013. Mining heterogeneous information networks: A structural analysis approach. ACM SIGKDD Explor. Newslett. 14, 2 (2013), 20-28.   
[54] Yizhou Sun, Jiawei Han, Peixiang Zhao, Zhijun Yin, Hong Cheng, and Tianyi Wu. 2009. RankClus: Integrating clustering with ranking for heterogeneous information network analysis. In Proceedings of the International Conference on Extending Database Technology (EDBT'09). ACM Press, 565-576. DOI: https://doi.org/10.1145/1516360.1516426   
[55] Yizhou Sun, Yintao Yu, and Jiawei Han. 2009. Ranking-based clustering of heterogeneous information networks with star network schema. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'09). ACM Press, 797. DOI: https://doi.org/10.1145/1557019.1557107   
[56] Andrea Tagarelli, Alessia Amelio, and Francesco Gullo. 2017. Ensemble-based community detection in multilayer networks. Data Min. Knowl. Discov. 31, 5 (Sep. 2017), 1506-1543. DOI: https://doi.org/10.1007/s10618-017-0528-8   
[57] Lei Tang and Huan Liu. 2010. Community Detection and Mining in Social Media. Morgan & Claypool Publishers.   
[58] Lei Tang, Xufei Wang, and Huan Liu. 2009. Uncovering groups via heterogeneous interaction analysis. In Proceedings of the 9th IEEE International Conference on Data Mining. 503-512.   
[59] Lei Tang, Xufei Wang, and Huan Liu. 2012. Community detection via heterogeneous interaction analysis. Data Min. Knowl. Discov. 25, 1 (July 2012), 1-33. DOI: https://doi.org/10.1007/s10618-011-0231-0   
[60] Zhiqiang Xu, Yiping Ke, Yi Wang, Hong Cheng, and James Cheng. 2012. A model-based approach to attributed graph clustering. In Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD'12). ACM, New York, NY, 505-516. DOI: https://doi.org/10.1145/2213836.2213894   
[61] Xuemeng Zhai, Wanlei Zhou, Gaolei Fei, Weiyi Liu, Zhoujun Xu, Chengbo Jiao, Cai Lu, and Guangmin Hu. 2018. Null model and community structure in multiplex networks. Sci. Rep. 8, 1 (Dec. 2018), 3245. DOI: https://doi.org/10.1038/s41598-018-21286-0   
[62] Yang Zhou and Ling Liu. 2013. Social influence based clustering of heterogeneous information networks. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 338. DOI: https://doi.org/10.1145/2487575.2487640   
[63] Yang Zhou, Hong Cheng, and Jeffrey Xu Yu. 2009. Graph clustering based on structural/attribute similarities. Proc. VLDB Endow. 2, 1 (2009), 718-729. DOI: https://doi.org/10.14778/1687627.1687709

Received March 2019; revised December 2020; accepted December 2020

TRUONG DAI HQC MÖ - DIA CHÁT

# BAO CAO TONG KET DE TAI NCKH SINH VIEN

# XÂY Dùng Mô Hìn H U BAO CHUÖI THÔI GIAN BÁNG PYTHON

TRUONG DAI HQC MÖ - DIA CHÁT

# BAO CAO TONG KET DÉ TAI NCKH SINH VIÊN

# XÂY Dùng Mô Hìn H U BAO

# CHUOI THOI GIAN BÁNG PYTHON

Truong nhomon nghiên cuu: Vū Dinh Hoàng - Cộng ngế thùng tin (CLC) A1 K66

Thanh viên tham gia thuc hiên:

Nguyen Tuan Nghia - Cóng néphé phàn mèm K65C

Nguyen Ngoc Minh - Cộng nghe thong tin (CLC) A1 K66

Chu Tien Son - Cóng néghé thòng tin (CLC) A1 K66

Nguòi huóng dān: Ths.法律顾问 Thu Hǎng

# MUC LUC

MUC LUC III

DANH MUC CAC HINH VE.

DANH MUC CAC BANG BIEU VII

MO DAU 1

CHUONG 1 Mô Hìn HQC SÂU TRONG DU BÁO PHÂN TÍCH CHUÖI THOI

GIAN 11

1.1 Tim hieu vè Time Series 11   
1.2 Tim hieu vè Deep Learning 12

1.2.1Dinhghia 18   
1.2.2 Cac than phan cua Deep Learning 18   
1.2.3 Cach thuc hoat dong cua Deep Learning 20   
1.2.4 Môt vài mô先进技术 trong Deep Learning 21   
1.2.5 U du diem, nhuc diem cua Deep Learning 22   
1.2.6 Ung dung cua Deep Learning 24

1.3 Tim hiéu vè cóng nghê sù dung trong du an. 25

1.3.1 Ngôn ngūr Python 25   
1.3.2 Jupyter Notebook 26   
1.3.3 Môshima RNN 27   
1.3.4 Môshima LSTM 29

CHUONG 2 DU LIÉU VA MA TRÁN DANH GIÁ 32

2.1 Dulieu 32

2.1.1 Giopi thieu 32   
2.1.2 Phan tich 33   
2.1.3 Chia têp du lieuu 37

2.2 Ma trān dānh giá 39

2.2.1 RMSE 39   
2.2.2 MAPE 40

# CHUONG 3 DÉ XUÁT VÀ DANH GIÁ KÉT QUÁ Mô HìnH. 43

3.1 De xuát 43

3.1.1 RNN 44   
3.1.2 LSTM 47

3.2 Két qua 54

3.2.1 RNN 54   
3.2.2 LSTM 55

3.3 Giai thich y nghia cua biieu do... 57   
3.4 Danh giama hinh 59

# KETLUANVAHUONGPHATRIEN 61

# Tai lieu tham khao 63

# DANH MUC CÁC H_INH V_E

Hinh 0-1 Mo hinh RNN. 3   
Hinh 0-2 Môhh LSTM. 4   
Hinh 0-3 Môshima GRU 5   
Hinh 0-4 Môshima MLP 5   
Hinh 0-5 Tông tièu thú NLSC (EJ) cùa cac nuóc dài diên nám 2020-2021................................8   
Hinh 0-6 Tieu thu ng soan quoc nam 2019-2025. 10   
Hinh 1-1 Cac thanh phan cua Time Series. 12   
Hinh 1-2 Nhūng dαu míc quántrightng cúa Deep Learning. 12   
Hinh 1-3 Cau truc cua DeepLearning. 20   
Hinh 1-4 Cau truc mo hinh RNN. 28   
Hinh 1-5 Cau truc mo hinh LSTM. 30   
Hinh 2-1 Thong tin dtilieu 34   
Hinh 2-2 Cac thuoc tinh du lieuu 34   
Hinh 2-3 Môt sô dür liêu dâu tiên. 35   
Hinh 2-4 Dtilieu duoc mo ta. 35   
Hinh 2-5 Giá tri thieu trong d'riieu 35   
Hinh 2-6 Biéu do t urong quan. 36   
Hinh 2-7 Lap bièu do nanh mòi ngay. 36   
Hinh 2-8 Bièu do nhanh mi ngay. 37   
Hinh 2-9 Tâp din lieuu vu nhom ngay. 38   
Hinh 2-10 Chia tep dtr lieuu. 38   
Hinh 2-11 Gi tri thuc va gi tri du doan 39   
Hinh 2-12 Tinh RMSE 40   
Hinh 2-13 Ham tinh MAPE 41   
Hinh 2-14 Tinh MAPE. 42   
Hinh 3-1 Cau truc LSTM 44   
Hinh 3-2 Cau truc RNN 44   
Hinh 3-3 biéu do du doan muc tiéu thu näng luong so vovi thuc te. 54

Hinh 3-4 Bieu do qua trinh huan luyen mo sinh LSTM de du doan muc tieu thu dien nang. 55   
Hinh 3-5 Bieu do két qua du rdoan va giá tri thuc té tren tap dtilieu kiém tra. 55   
Hinh 3-6 Bièu do qua trinh huàn luyén mo hinh LSTM de du doán mIRC tiêu thu dien nang. 56   
Hinh 3-7 Bieu do két qua du doan va giá tri thuc tê tren tap du lieuu kiém tra......56   
Hinh 3-8 Bièu do qua trinh huàn luyén mo hinh LSTM de du doán mIRC tièu thu dién nang. 57   
Hinh 3-9 Bieu do két qua du rdoan va giá tri thuc té tren tap dtilieu kiém tra.57

# DANH MUC CÁC BÁNG BIEU

Bang 1 Tóng tiéu thu NLSC cua thegioi va cac khu vucnam 2020 - 2021. Don Vietinh: EJ.

Bang 2 So sanh chi so danh giá. 59

# MÖDAU

# 1. Tông quan vè mô hinh duý báo chuǒi thòi gian “Time Series Forecasting”

# 1.1. Dý báo lá gì?

Dú báo lā sú tiên doán nhùng ván de se xay rastrong tuong lái dura trén môt co sô não bó. Day lâ mot ván de luón nhân duoc su r quant tám cuà nhieu nuha khoa hoc, nuha quán ly boi ví não có môt vai trò rat quanstrong trong thuc tí. Tuy nhiên cho dén nay, du báo vān lā bàilando chua có lòi giäi cuó i cung (Abbasov & Mamedova, 2003; Tai, 2019). Trong thóng kê, dura tren dū liEU qua khú, mù hinh de du bao cho tuong lái duoc thiet lap. Doi voi dū liEU dang chuöi, hòi quy va chuöi thoi bian lá hai mo先进技术 duoc ap dung pho bien ngay nay.

Time Series Forecasting:

Dú bó chuǒi thòi gian (Time Series Forecasting) làMZt kǐy thuát du doán cac su kîên thùng quaMZt chuǒi thòi gian. Nó duǎoán cac su kîên trong tuong lái bang cach phân tich cac xu huóng trong qua khú, vói già dinh rang cac xu huóng trong tuong lái sī tuǒng tu né nuh可视.

Dú báo chuǒi thòi bian cūng là mòt linh vuc quan trong cuà hoc may va có the duoc coi là mòt ván de hoc tâp có giám sát. Các phuong pháp hoc may nhur:

Regression,

Neural Networks,

Support Vector Machines,

Random Forests,

XGBoost

co the duoc ap dung truong hop nay.

Dru bao chuoi thoi giant thoung duoc sur dung cung voi viec phan tich. Phan tich chuoi thoi giant lien quan den viec phat trien cac mo sinh tim kiem tri thuc trong ddu lieuu. Dru bao chuoi thoi giant thuc hienc buoc tiep theo voi kien thuc vua tm duoc. No

doi hoi phai phat trién cac mo sinh dura trén du lieu truc do va ap dung chung de thuc hiên cac quan sát huóng dan cac quyét dinh chién luoc trong tuong lai.

# 1.2. Nhùng phuong pháp dμ bao chuõi thòi giant

MôhhMachine Learning:

Mô先进技术 ARIMA (AutoRegressive Integrate Moving Average): Dua tiên,gia thuyét chuõi dûng và phuong sai sai só khóng dòi. Mô先进技术 dûng dαu vào chính lá nhūng tí ní hiếu qua khú cúa chuõi duýc dú báo dê du báo né. Các tí ní hiếu do bao gǒm: chuõi tú hói qui AR (auto regression) và chuõi trung binh truǒt MA (moving average). Hân hé tát céc chuõi thói giant sē có xu huǒng tāng hoǎc giàm theo thói giant, do do yéu tó chuõi dûng thùng khóng dát duýc. Trong trùròng hop chuõi khòng dûng thi ta sē can bién dōi sang chuõi dûng bang sai phàn. Khi do tham só dăc trung cuà mô先进技术 sē cothem thanh phàn bèc cuà sai phàn d và mô先进技术 dúc tá bói 3 tham só ARIMA (p, d, q).

Mô先进技术 SARIMA: Vè bànCHATdaylàmôhinhARIMA nhung duoc dieu chinh dac biét déap dung cho nhūng chuōi thoi gian có yéu tó mua vu. Nhu chúng ta da biét vèbànchat ARIMA chínhàmô hìn hòi qui tuyén tính nhung miquan hé tuyén tính thuong khòng giāi thich tot chuōistrong truong hop chuōixuát hiên yéu tó mua vu. Chinch vi the,bang cach tim ra chu kicua qui luat mua vu va loài bó no khói chuōi tase dé dang hòi qui mô hìn theo phuong pháp ARIMA.

Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术: Mô先进技术:

Mô先进技术: Cát có l� sà chuế, lú sà chung. Mô先进技术: Cát có l� sà chung. Mô先进技术: Cát có l� sà chung. Mô先进技术: Cát có l� sà chung. Mô先进技术: Cát có l� sà chung. Mô先进技术: Cát có l� sà chung. Mô先进技术: Cát có l� sà chung. Mô先进技术: Cát có l� sà chung. Mô先进技术: Cát có l� sα chung. Mô先进技术: Cát có l� sα chung. Mô先进技术: Cát có l� sα chung. Mô先进技术: Cát có l� sα chung. Mô先进技术: Cát có l� sα chung. Mô先进技术: Cát có l� sα chung. Mô先进技术: Cát có l� sα chung. Mô先进技术: Cát có l� sα ch Chung. Mô先进技术: Cát có l� sα ch Chung. Mô先进技术: Cát có l� sα ch Chung. Mô先进技术: Cát có l� sα ch Chung. Mô先进技术: Cát có l� sα ch Chung. Mô先进技术: Cát có l� sα ch Chung. Mô先进技术: Cát có l� sα ch Chung. Mô先进技术: Cát có l� sα ch Chung.

MôhhinehDeep learning:

Mô先进技术 RNN (Recurrent Neural Network): Máng no-ron hòi quy (RNN) là loài máng có cαu truc dṛçc thiet ké dé xú ly chuōi duī lièu tuàn tμ. RNN khòng chí su r dung duī lièu dαu vào ma cò sù dung cac dαu ra trucó do dé duì doán dαu ra hiên tαi. Sú lién két giūa cac nut máng trong mô先进技术 RNN tαo nèn mòt so do co huóng và bān than bó nhó先进技术 dṛçc su dung dé xú lý cac chuōi dαu vào linh hoṭ. Trang thai cúa miǒi nút thaty doi theo thòi gian bòi ham kich hoṭ có giá tri thuc. Mô先进技术 huan luyén先进技术 RNN dṛçc xác dinh bang cach chuyén doi giūa cac trang thai nèn no luón có cung kich trucó dαu vào. Mát khác, cùng mòt ham chuyén doi sé có cung hé só o miǒi buoc dā先进技术 su dung trong hé thóng.

![](images/bb841dd9c0e6383b3cb80a4e75d97d1cf6ed6ea38cbd6fad49ad0381fd5aecdb.jpg)  
Hinh 0-1 Môhh RNN

Máng RNN có cái truc tuàn hoàn, có theg hǐ náo các dàu vào truc do, ghi náo tùng thùng tin theo thói giant, do do rát hūu izh trong du doán cho du lieu chuoi thói giant. Thân chí, máng RNN có cái su dung vovi cac lóp máng tích chap dé mo róng

các giá tri trong ma trân pixelMZt cach hiêu qua. Ngoài ra, moi tzung quan giūa cac māu cūng duoc già dinh thóc qua cac mo hinh hoa chuõi dür liêu trong máng RNN.

Mô Hinh LSTM (Long Short - Term Memory): Máng LSTM làMZt dang dac biêct cua RNN, voci cáu truc gom cac cóng có kha nang ghi nhó va hoc duoc cac thong tin va rang buoc trong khoang thoi giant dai ma khong can bát ký su can thiêp não. Cac máng hòi quy déu có dang laMZt chuovi cac mo-dun lap di lap lái cua mang no-ron. Doi voci mang RNN chuàn, cac mo-dun có cau truc rat don giàn, thuong laMZt tang tanh. Mac du LSTM cūng có kien truc dang chuovi nuh mang RNN chuàn, nhung cac mo-dun bèntrong sé có cáu truc phúc tap hon. Thay vi chi cóMZt tang mang no-ron, chún có tí 4 tāng tuong tac voci nhauMZt cach rat dac biêt.

![](images/d62cc6ad322748fade8a3b176383c24d07b2028e12f7ad84465dc2596520c1b0.jpg)  
Hinh 0-2 Mô先进技术 LSTM

Mô先进技术 GRU (Gated Recurrent Unit): GRU là phiên bàn cai thieu cóu máng RNN truyen thóc. De giai quyét ván de mat mat gradient cóu máng RNN truyen thóc, GRU dā duoc sù dung và goi lá cóng cap nhát và cóng cai dát lái (update gate và reset gate). Vé co bàn, do chinh lá hai vector quyét dinh thóc tin não se duoc truyen cho dαu ra. Dieu dac biêt lá né có thec dao tao de giūr thóc tin tú láu trucó do, khòng hé xía thóc tin khòng liên quan dén duoán dαu ra.

![](images/773de581552241be451dee4a1b2f2a59c25fca350b76d43241ebe014566beac5.jpg)  
Hinh 0-3 Mô先进技术 GRU

Mô先进技术 MLP (Multilayer Perceptron): làMZtMZhinh mang no-ron duoc két nonzero day du tieu chuàn. Nó bao gôm cac layer cua node trong do mi node duoc két nonzero voci tát ca cac dàu ra tù layer trunc và dàu ra cua mi node duoc két nonzero vói tát ca cac dàu vào cho cac node o layer tiép theo.

![](images/e8947a06e40950bfaeeafd495cb1d1107bd4f7bf29b2f75bf500a23623cfc6d2.jpg)  
Hinh 0-4 Môshima MLP

1.3. Ung dung mo hinh mang hoc sau du bao san luong tiu thu nang luong

Trong cuoc cach mang cóng nghe 4.0, tri tué nhân tao (AI) dang tro thanh xu huóng toàn cau và duoc Ung dung pho bién trong mi nganh nghe, linh vuç, to chuc, doanh nghiêp. Tái Viet Nam trong nhūng nám gànday, nu cau tiêu thu näng luong ngay môt tang, dédam bao an ninh näng luong va phat trièn kinh né ben vūng, cac giài phap sù dung triu né nhân tao trong phan tich dμ bao san luong tiêu thu näng luong thu hú su quan tam cua cap chinh quyen giup co quant chien luoc có the xac dinh huong phat trièn dung dan trong tuong lai. Do do ngay cang có nhieu nghiên cru vè cac giài phap phan tich du doán hôtro boi AI duoc trièn khai va ap dung trong dμ báo san luong tiêu thu näng luong.

# 2. Tinh cap thiet cua ván dê du bao nhu cau tiêu thu nǎng luǒng

# 2.1. An ninth näng luǒng là gì?

An ninh nang luong la dam bao cung cap nang luong day du de duy tri san xuat, phat trien kinh te quoc gia va dap ung nu h cuu tiêu dung cua nguoi dan o muc on dinh sinh thuong.

# 2.2. An ninh nang luong quoc gia la tièn de quan trong thuc hiên cóng nghiêp hóa, hiên dai hóa

Dùng truc yèu cùu cóng nghiêp hóa, hiên dài hóa dát nuoc, nhu cau sù dung näng luong cúa Viet Nam không ngùng gia tāng,strong khi nguôn cung nǎng luǒng ngay cang can kiét. Do vay, chúng ta can co lô trinh cu the trong xày dung mo hinh näng luong sách trong tuong lái.

Ngay 11-2-2020, Tóng Bi thu, Châu nucoc Nguyen Phú Trếgh thay mαt Bô Chinh triy ban hàngh Nghi quyét so 55-NQ/TW, “Vè dinh huóng Chien luoc phà trién nang luong quoc gia cua Viet Nam dén nam 2030, tām nhìn dén nam 2045”. Nghi quyét khäng dinh, phat trién nang luong gán vói thuc thi chinh sánchez bao vê miǒi trùng, nhám dát muc tiếu giám phat thai khi nhà kinh, thuc day kinh né tuàn hoàn và phat trién bèn vūng.

Viếct Nam can phai phat triên nganh nang luong gän lien voi phat triên kinh té, dê cac nganh kinh t'énhân thuc duoc giói han cua nganh nang luong, tù do xày dung

chién luoc, quy hoach phat trien phu hop. Khi toan bó nèn kinh tí khóng cón súc ép lén nganh näng luong, moi có khà näng bao dām duoc an ninh näng luong quó gia.

# 2.3. Nhu cêu tiêu thú nang luong toàn cêu

<table><tr><td rowspan="2">Quócgia/Ch慮 l走势图</td><td rowspan="2">2020</td><td rowspan="2">2021</td><td colspan="2">Tóc do tânghu truong</td><td rowspan="2">Ti网络传播 nám 2021</td></tr><tr><td>2021</td><td>2011-2021</td></tr><tr><td>Bế c Mếy</td><td>108,79</td><td>113,70</td><td>4,8%</td><td>-0,1%</td><td>19,1%</td></tr><tr><td>Nam &amp; Trung Mếy</td><td>26,66</td><td>28,46</td><td>7,0%</td><td>0,3%</td><td>4,8%</td></tr><tr><td>Ch慮 A U</td><td>78,93</td><td>82,38</td><td>4,7%</td><td>-0,6%</td><td>13,8%</td></tr><tr><td>CIS</td><td>37,46</td><td>40,32</td><td>7,9%</td><td>0,9%</td><td>6,8%</td></tr><tr><td>Các khác Trungdog</td><td>36,60</td><td>37,84</td><td>3,7%</td><td>2,2%</td><td>6,4%</td></tr><tr><td>Ch慮 Phi</td><td>18,89</td><td>19,99</td><td>6,2%</td><td>2,1%</td><td>3,4%</td></tr><tr><td>Ch慮 Á-TBD</td><td>256,69</td><td>272,45</td><td>6,4%</td><td>2,8%</td><td>45,8%</td></tr><tr><td>Toàn Thé giá</td><td>564,01</td><td>595,15</td><td>5,8%</td><td>1,3%</td><td>100,0%</td></tr><tr><td>- OECD</td><td>220,20</td><td>229,89</td><td>4,7%</td><td>-0,2%</td><td>38,6%</td></tr><tr><td>- Ngoài OECD</td><td>343,82</td><td>365,26</td><td>6,5%</td><td>2,4%</td><td>61,4%</td></tr><tr><td>- EU</td><td>57,07</td><td>60,11</td><td>5,6%</td><td>-0,6%</td><td>10,1%</td></tr></table>

Nguon: BP Statistical Review of World Energy 2022 | 71st edition.   
Báng 1 Tóng tiéu thú NLSC cuà thegior va cac khu vuc nám 2020 - 2021. Don ví tính: EJ.

Ghi chu: Tóc do tângh duoc diu chinh cho cac nam nhuan. EJ = Exajoules =1 x 1018 joules.

Trong bó cáo nay, nāng luǒng so cap (bao gòm céc loài nhiên lièu duoc giao dìch thuong mai và nǎng luǒng tái táo hiên dai).

Nang luong tát cá cac nguôn sān xuát dién phi hóa thach duoc tinh trèn co sô dàu vào tuong duong.

![](images/b652979c0f7bf49ff72bf444349caa04cbe01783e05e70d0721746714a6a6ad1.jpg)

Ghi chu: 30 nuoc dai dien có muc NLSC nam 2021 bang hoac lon hon cua Singapo (3,44 EJ).

Hinh 0-5 Tông tiếu thú NLSC (EJ))cua cac nuóc dài diên nám 2020-2021

Tùng NLSC tiếu thú doàn càu nám 2021 tân 5,8% so vói nám 2020 và trong giai doàn 2011 - 2021 tân binh quán 1,3%/nám.

Tuy tiên, tiếu thú NLSC)cua tung chau luc, khu vuc có khac biét vói buc tranh chung,cua theg ioi. Cu the la:

Bóc My: Nám 2021 táng so voci nám 2020 là 4,8%, nhung gimiasmhe 0,56% so voci nám 2011, cã giai doan 2011 - 2021 gimiasm binh quân 0,1%/nám. Nám 2021 chiem týtright ng 19,1% tiêu thú NLSC toân cau, gimiasm so voci nám 2011 (chiem 21,95%).

Nam và Trung My: Nám 2021 tân so vóc nám 2020 là 7,0% và so vóc nám 2011 tân 2,67%. Cà giai doàn 2011 - 2021 tân binh quán 0,3%/nám. Nám 2021 chíém týtright ng 4,8% tiếu thú NLSC toân cău, giám so vóc nám 2011 (chím 5,33%).

Chân Aú: Nám 2021 tân so vói não 2020 là 4,7% và giam 5,87% so vói não 2011, cã giai doanh 2011 - 2021 gIAM binh quán 0,6%/nám. Nám 2021 chíém týtright ng 13,8% tiếu thú NLSC toân cău, gIAM so vói não 2011 (chíém 16,8%).

Khoi CIS: Nam 2021 tāng so vói nám 2020 là 7,9% và so vói nám 2011 tāng tuong doi cao, tí 9,12%. Cà giai doàn 2011 - 2021 tāng binh quán 0,9%/nám. Nám

2021 chiem týtright 6,8% tiêu thú NLSC toan cau, giam so vói nám 2011 (chíém 7,1%), nguyên nhân chu yéu là do týtright cúaMZt só khu vuč tāng cao hon.

Cáncuoc Trung Dong: Nam 2021 tāng so vói não 2020 là 3,7% và so vói não 2011 tāng cao, tí 23,95%. Cà giai doan 2011 - 2021 tāng binh quán 2,2%/nám. Nam 2021 chiem týtrightg 6,4% tiêu thú NLSC toàn cau, tāng so vói não 2011 (chíém 5,87%).

Chái Phi: Nám 2021 tân so vói nám 2020 là 6,2% và so vói nám 2011 tân cao, tí = 23,32%. Cá giai doan 2011 - 2021 tân binh quán 2,1%/nám. Nám 2021 chíém t ytright 3,4% tiếu thú NLSC toàn cau, tân so vói nám 2011 (chíém 3,12%).

Chân Á - Thế Binh Duong: Nám 2021 tân so vói nám 2020 là 6,4% và so vói nám 2011 tân cao, tí = 31,20%. Cà giai doàn 2011 - 2021 tân binh quán 2,8%/nám, lá chân luc có múc tân cao nghtrong nám 2021 và cá giai doàn 2011 - 2021. Nám 2021 chiem týtrightng 45,8% tiêu thú NLSC toàn cái, tân cao so vói nám 2011 (chíém 39,87%). Trung Quóc có tóng NLSC lón nhat thegói vói 157,65 EJ.

Khoi OECD: Nam 2021 tāng so vói nám 2020 là 4,7% và so vói nám 2011 giám 1,74%. Cà giai doàn 2011 - 2021 giám nhe sinh quán 0,2%/nám. Nám 2021 chiem tv trong 38,6% tiếu thú NLSC trèn toàn thegói, giám mânh so vói nám 2011 (chiem 44,93%).

Khoi ngoài OECD: Nám 2021 tân so vóc nám 2020 là 6,5% và so vóc nám 2011 tân cao, tí 27,30%. Cá giai doan 2011 - 2021 tân binh quán 2,4%/nám. Nám 2021 chiem týtright 61,4% tiếu thú NLSC toân cău, tân vóc nám 2011 (chím 55,07%).

Khoi EU: Nam 2021 tāng so vóinam 2020 là 5,6% và so vói nam 2011 giám 5,88%. Cà giai doàn 2011 - 2021 giám binh quán 0,6%/nám. Nam 2021 chiem týtright ng 10,1% tiêu thú NLSC toân cau, giám so vói nam 2011 (chiem 12,27%).

# 2.4. Nhu tiên tiếu thú nang luong tαι Viet Nam

Nam 2021, Viet Nam tiêu thú NLSC 4,32 EJ, tāng 2,6% so vóinam 2020 và chiem 0,7% tóng tiêu thú NLSC toàn cau. Trong giai doan tür 2011 - 2021 có tiên dô tiêu thú NLSC tāng binh quán 7,2%/nám, vào loài cao trân thegioi.

Trong co cái tiên thú NLSC nám 2021 cóa Viet Nam than chiem $49,77\%$ ; dαu $21,76\%$ ; thuy diên $16,44\%$ ; NLTT $6,25\%$ ; khi dot $6,02\%$ . Nhu vay, than chiem týstead ng cao nhát, cón NLTT néu cóng caú thuy diên tí có týstead ng chiem $22,69\%$ , vao loài tuếng doi cao trèn thegói.

![](images/60bcda4380eccf9442dd0864d5374705bcaef3651fbf3922ffc9844f6863a096.jpg)  
tóng tièu thu náng luǒng doàn quoc trong giai doàn tú nám 2019 dén nám 2025   
Hinh 0-6 Tieu thu nang luong toan quoc nam 2019-2025

Do vay, ván de là di doi vovi tang cuong khai thac cac nguôn tāi nguyén nang luong trong nuoc, nhát lā näng luong tāi tao, can phai day mânh nhap khau va dαu tur khai thac tāi nguyén näng luong ò nuoc ngoài, nhát lā than, dαu khi. Dong thoi, thai phat huy cao do viêc thuc hiên sur dung näng luong tiét kiêt va hiêu qua, dac biêt la su dung näng luong hiêu qua theo dinh huóng nhur dā néu tren nham dam bao dong thoi dαt 3 muc tiêu: Dām bao an ninh näng luong, tang truong kinh tè bèn vūng, bao vê mi o truong xanh - sacha - dep.

# CHUONG 1 MÔ HINH HQC SÂU TRONG DU BÁO PHÂN TÍCH CHUÖI
THÔI GIAN

# 1.1 Tim hieu vê Time Series

Time series làMZt loai du lieu thong kê thu thap theo thai giant. Nó bao gôm cacgia tri duoc ghi lái trong cac khoang thai giant có dinh hoac khong déu. Time series thuong duoc su dung de nghiên cuu va phân tich cac xu huóng, mo先进技术 data, co hai loai chinh:

- Chuài thói giant thóc thóc (regular time series), loài thóc thóc duoc goi là sô lieu.   
- Chuōi thòi bian bát throung (events) là nhùng su kien.

Môt du lieu chuoithoigian thuong duoc phan ra thanh 4 thanh phan con sau:

Xu huóng (Trend): thanh phàn nay chi ra xu huóng tóng quan cua dür lièu theo thói bian: lên hoac xuóng, tāng hoac giám.   
- Mua vú (seasonality): thanh phàn chí rα cac xu huóng theo mùa, theo tháng, theo quy.   
- Chu kì (Cycle): thânphàn chu ký, né khác yéu tó mua vu ò chô thânphàn.
- nay có su vân dòng trong khoàng thói giant dai hon (nhieu nam).   
- Yếu cóntbat thuong (Irregular remainder): hay cón goi la nhieu trang (white noise) thanh phàn nhieu cón lái sau khi trich xuát hét cac thanh phàn òtron, não chí ra su bát vuong cua cac diém du lieu.

![](images/9b42e66df54eff823078de745d879674f6585ed9e796cce415ea3d92b0945517.jpg)  
Hinh 1-1 Cac thanh phan cua Time Series

# 1.2 Tîm hiêu vê Deep Learning

Các dαu móc quando trong一则 DeepLearning:

![](images/559aebf8dfb123459a030768f0f4120f659d283e692c23f1792327f85b9beb8b.jpg)  
Hinh 1-2 Nhùng dαu mác quan trong cua Deep Learning

# Perceptron (60s)

Môttron ngn nong dou tièn cua neural network va deep learning la perceptron learning algorithm (hoac ggon la perceptron). Perceptron laMZt thuat toan supervised learning giup gai quyét bai toan phan lop nhiphan, duoc khoi nguôn boi

Frank Rosenblatt nám 1957 trong môt nghiên,cuú duoc tài tro bói Vǎn phòng nghiên,cuú hái quàn Hoa Ký (U.S Office of Naval Research - tú môt co qua nien quan dén quàn sú). Thuát toán perceptron duoc chung minh là hói tu néu hai lôp dū lieu là linearly separable. Vói thanh cóng nay, nám 1958, trong môt hói tháo, Rosenblatt dā có môt phát biếu gay tranh cai. Tú phát biếu nay, tó New York Times dā có môt bái báo cho ràng perceptron duoc Hái quàn Hoa Ký mong doyi “có the di, não chuyén, nhìn, viét, tú sinh sàn, và tú nhân thuc duoc su tôn tαι cuà minh”. (Chúng ta biét ràng cho tói giò cac hé thóng náng cao hon perceptron nhieiù lan vǎn chua the).

Mác dù thai toolán nay mang lái nhieu ký vong, bó nanh chóng duoc chung minh khòng theg iāi quyét nhūng bai toán don gian. Nám 1969, Marvin Minsky và Seymour Papert trong cuón szech nou tiếng Perceptrons dā chung minh ràng khòng the 'hoc' duoc hàn só XOR khi su r dung perceptron. Phát hiên nay lám choáng váng giói khoa hoc thói bian do (báy gio chung ta tháy viếc nay khá hiên). Perceptron duoc chung minh ràng chi hoat dong néu du lieu la linearly separable.

Phát hiên nay khién cho cac nghiên círu vè perceptron bi gián doàn gàn 20 nám. Thói ký nay con duoc goi là Mùa dòng AI thúnhát (The First AI winter).

- MLP và Backpropagation ra dòi (80s)

Geoffrey Hinton tôt nghiêp PhD nganh neural networks nám 1978. Nám 1986, ong cung vói hai tácgia khac xuát bàn môt bái báo khoa先进技术 Nature vói tiên d'é "Learning representations by back-propagating errors". Trong bái báo này, nhóc一则 chung minh rângh neural nets vói nhièu hidden layer (duoc goi là multi-layer perceptron hoác MLP) có the duoc huan luyên môt cach hiêu qua dura trân môt quy trinh don gian duoc goi lá backpropagation (backpropagation lá tên goi my miêu一则 quy tâc chuôi – chain rule – long tângh dão ham. Viêc tinh duoc dào ham一则 phúc tap mô tâ quan he gùra dàu vao va dàu ra一则 môt neural net lá rát quamtron vì hêt céc thuat toán tói uru dèu duoc thuc hiên thùng qua viêc tinh dào ham, gradient descent lá môt ví dú). Viêc nám giúp neural nets thoaṭ duoc nhūng hàn ché一则 perceptron vê viêc chi biêu diēn duoc cac quan he tuyen tính. De biêu diēn cac quan he phi tuyen, phía sau môi layer lá môt ham kích hoàt phi tuyen, ví duham sigmoid hoác

tanh. (ReLU ra doi nám 2012). Vói hidden layers, neural nets duoc chung minh rang co kha näng xap xi hau het bát ký ham so não quaMZt dinh ly duoc goi là universal approximation theorem. Neurel nets quay tro lái cuoc choi.

Thuếntoán nay mang lái môt vài thân cóng ban dαu, bói troi là convolutional neural nets (convnets hay CNN) (còn duoc goi là LeNet) cho bái toán nhân dang chūr só viét tay duoc khoi nguòn bói Yann LeCun tαi AT&T Bell Labs (Yann LeCun là sinh viên sao cao hoc)cua Hinton tαi dαi hoc Toronto nám 1987-1988). Duovi dαy là bàn demo duoclay tu tranq web cua LeNet, network lα mòt CNN vòi 5 layer, còn duoc goi là LeNet-5 (1998).

Mô先进技术 nay duoc sùr dung róng rāi trong cac hé thong doc só viét tay tiên céc check (sec ngânHang) và mā vùng buu diên)cua nuóc My.

LeNet là thuat tiên có chàntroidi gian do cho bai toán nhân dang anh chu sô Viet tay. Nó tot hon MLP thong thuong (voci fully connected layer) vì não có kha nang trich xuát duoc dac trung trong khong gian hai chieu cuà anh thong qua cac filters (bô loc) hai chieu. Hon nuà, cac filter nay nhô nên viếc luru tru và tinh toán cūng tot hon so voci MLP thong thuong. (Yan LeCun có xuát phat tu Electrical Engineering nên rát quen thuc coci cac bó loc).

- Mua dong AI thú hai (90s - dàu 2000s)

Cac mo hinh tuong tuy duoc ky vong sê giai quyét nhieu bai toan image classification khac. Tuy nhiên, khong nuh cac chū sô, cac loài anh khac lui rát han ché vi may anh so chu a pho bien tai thoi diem do. Anh duoc gan nhān lui cang hiém. Trong khi dé có the huan luyen duoc mo hinh convnets, ta can rat nhieu dū lieu huán luyen. Ngay ca khi dū lieu có du, mot ván dé nan giai khac la kha näng tinh toán)cua cac may tinh thoi do con rát han ché.

Mệnh ché khacCEEa cac kien truc MLP nao chung la ham mat mat khong phai la mott hamloi. ViEc nay khién cho viEc tim nghiem toi uu toan cuc cho bai toan toi uru ham maT mat tro nen rát khó khan. MdT van de khac lien quan den goi hantinh toan cua may tinh cung khién cho viEc huan luyen MLP khong hiieu qua khi so luong hidden layers lón len. Van de nay có ten la vanishing gradient.

Nhac lui rang ham kich hoat duoc su dung thoi bian do la sigmoid hoac tanh - la cac ham bi chan trong khoang (0, 1) hoac (-1, 1) (Nhac lui dao ham cua ham sigmoid la tich cua hai so nhô hon 1). Khi su dung backpropagation dé tinh dao ham cho cac ma tran he so o cac lop dau tiên, ta can phai nhân rat nhieu cac giá tri nhô hon 1 vói nhau. Viêc nay khién cho nhieu dao ham thanh phan bang 0 do xap xi tinh toán. Khi dao ham cuaMZt thanh phan bang 0, no sé khong duoc cap nhát thong qua gradient descent!

Nhùng hàn ché nay khién cho neural nets môt lan nua roi vào thói ký bang giá. Vào thói diém nhùng nám 1990 vu dàu nhùng nám 2000, neural nets dàn duoc thay the bói support vector machines -SVM. SVMs có uru diém là bai toán tói uru dé tim)cac tham só一则沒 lát môt bai toán lòi – có nhieu cac thuát toán tói uru hiêu qua giúp tim,nghiem一则 no. Cac ký thuát vè kernel cūng phat trièn giúp SVMs giai quyét duoc ca cac ván dé vè vièc dür lièu khòng phân biêt tuyén tinh.

Nhièu nhà khoa hoc lam machine learning chuyén sang nghiên cuu SVM trong thói bian do, trùMZt vài nhà khoa hoc cung dαu...

- Cái tiên duoc lamar – Deep Learning (2006)

Nam 2006, Hinton môt lan nua cho rang ong biét bó não hoat dong nhu the não, và gói thieu y tuong cua tièn huan luyén khòng giám sát (unsupervised pretraining) thóc qua deep belief nets (DBN). DBN có the duoc xem nhu su xép chong cac unsupervised networks don gn nhu restricted Boltzman machine hay autoencoders.

Lày vú du vói autoencoder. Môi autoencoder làMZt neural net vóiMZt hidden layer. Só hidden unit it hon so input unit, va so output unit bang vói so input unit. Network nay don gian duoc huan luyén dé két qua o output layer giong vói két qua o input layer (và vì vêt duoc goi lá autoencoder). Quá trinh dū liêu di tù input layer tí hidden layer có the coi là mã hoá, qua trinh dū liêu di tù hidden layer ra output layer có the duoc coi là giai mã. Khi output giong vói input, ta có the thá rǎng hidden layer vói it unit hon có dé mã hoá input khá thanh cóng, va có the duoc coi mang nhùng tinh chatça input. Néu ta bó output layer, co dinh (freeze) két não giūa input va hidden layer, coi dàu raça hidden layer làMZt inputMZt, sau do huan luyenMZt

autoencoder khac, ta duoc them mot hidden layer nua. Qua trinh nay tiep tuc keo dai ta se duoc mot network du sau ma output cua network lon nay (chinh la hidden layer cua autoencoder cuoi cung) mang thong tin cua input ban dau. Sau do ta co thethem cac layer khac tuy thuoc vao bai toan (chang hanz them softmax layer o cuoi cho bai toan classification). C network duoc huan luyen them mot vai epoch nua. Qua trinh nay duoc goi la tink chinh (fine tuning).

Tài sao qua trinh huàn luyén nhu trèn mang lái nhieu loci ich?

Mệnh ng ng h an ché dā dé cap cua MLP là ván dé vanishing gradient. Nh ng ma trântron so ng voi cac layer dau cua network rát khó duoc huan luyen vi dao ham cua ham mat mat theo cac ma trân nay nhô. Vói y tuong cua DBN, cac ma trântron so ònh ng hidden layer dau tiên duoc tiên huan luyen (pretrained). Cáctrion so duoc tièn huan luyen nay có the coi la già tri khói tāo tot cho cac hidden layer phia dau. Viêc nay giup phàn nào tranh duoc su rphièn hà cua vanishing gradient.

Ké tûday, neural networks vói nhièu hidden layer duoc dōi tén thanh deep learning.

Vân bè vanishing gradient duoc gai quyêt phàn não (vān chura thuc su triét bè), nhung vān còn nhūng ván bè khác cuà deep learning: dür lièu huàn luyên qua it, và khǎ näng tinh toán cuà CPU còn rát hàn ché trong viêc huàn luyên cac deep networks.

Nam 2010, giao su Fei-Fei Li, tiên giao sur nganh computer vision dαu nganh tαi Stanford, cùng vói nhóc一则 a bα tαo raMZt co sδ duī liêu có ten ImageNet vovi hang triêu buc anh chuoc 1000 lóp duī liêu khánc nhau dā duǒc gán nhān. Dú an nay duoc thuc hiên nhò vào su bùng não一则 internet nhūng nám 2000 va luóng anh khóng lò duoc upload lén internet thói bian do. Céc buc anh nay duoc gán nhān bói rát nhieu nguòi (duoc tra cóng).

Bô co sô dür liEU nay duoc cap nhátHangnam, vα ké tú nám 2010, não duoc dung trongMZtuoc thi thuong niên có ten ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Trong cuoc thi nay, dür liEU huan luyen duoc giao cho cac doi thamgia. Mõi doi can sür dung dür liEU nay de huan luyen cac mo生涯phân lóp, cac mo hinh nay sé duoc ap dung de doan nhān cua dür liEU moi (duoc giū boi ban to

chúc). Trong hai nám 2010 và 2011, có rát nhièu doì tham gia. Các mô先进技术 hàn hàn nám nay chu yèu là sù két hop cuà SVM vói*c feature duoc xêt dùng bòi先进技术 hàn hand-crafted descriptors (SIFT, HoG, v.v.). Mô先进技术 chíén thân top-5 error rate lâ 28% (càng nhô càng tôt). Mô先进技术 chíén thân top-5 error rate lâ 26%. Cái先进技术 khòng nhièu!

Ngoài le: top-5 error rate duoc tinh nhu sau. Môi mô先进技术 doan 5 nhân cuaMZt buc anh. Néu nhan that cua buc anh nam trong 5 nhân do, ta cóMZt diem duoc phân lôp chinh xac. Ngoài ra, buc anh do duoc coi laMZt error. Top-5 error rate là ti Lê só buc anh error trong soan bo so anh kiém thú vói error duoc tinh theo cach nay. Top-1 error cóng vói classification accuracy (phân tra) chinh bang 100 phân tra.

- Dótphá (2012)

Nam 2012, cung tai ILSVRC, Alex Krizhevsky, Ilya Sutskever, và Geoffrey Hinton (lài lâ ong) tham gia và dát két qua top-5 error rate 16%. Két qua nay lam sūng sò giói nghiên cuuthosei bian do. Mô先进技术 Deep Convolutional Neural Network, sau nay duoc goi là AlexNet.

Trong bai bao nay, rát nhieu cac ký thai tiên miếoc giói thieu. Trong do hai dong gop não bát khác làHAM ReLU vα dropout.Ham ReLU vovi cach tinh va dāo ham don gian (bang 1 khi dαu vαo khóng am, bang 0 khi nguc lái) giúp tóc do huan luyén tāng len dang ke. Ngoài ra, viêc ReLU khóng bi chǎn trên bói 1 (nhur softmax hay tanh) khién cho ván de vanishing gradient cūng duoc giäi quyét phàn não. Dropout cūng la môt ký thai don gian va cúc ký hiêu qua. Trong qua trinh training, nhieu hidden unit bi tát ngau nhiên va mô先进技术 huan luyén trên cac bó tham so cón lái. Trong qua trinh test, toan bó các unit sē duoc su dung. Cach lam nay khá lá có ly khi doi chieu vovi con nguòi. Néu chi dung mô phàn näng luc dā dem lái hiêu qua thì dung toàn bó näng luc sē mang lái hiêu qua cao hon. Viêc nay cūng giúp cho mô先进技术 tranh duoc overfitting va cūng duoc coi giong vovi ký thai ensemblestrong cac hé thóng machine learning khác. Vovi mioci cach tát céc unit, ta có môt mô先进技术 hau. Vói nhieu to hop unit bi tát khánc nhau, ta thu duoc hiêu mô先进技术. Viêc két hop o cuói cung duoc coi nhur sγket hop cua nhieu mô先进技术 (và vì vay, né giong vovi ensemble learning).

Mệnh ng ng yéu t o quan ng nhat giup AlexNet thanh cóng là viêc sùdùng GPU (card do hoa) dé huan luyén mô hinh. GPU duoc tao ra cho game thu, vói khá näng chay song song nhieu lòi, dã tro thanh mô cóng cu cuc ký phù hop vói cac thuát toán deep learning, giup tāng tóc thuat toán lên nhieu lan so vói CPU.

Sau AlexNet, tát cá cac mô hinh giài cao trong cac nám tiếp theo déu là cac deep networks (ZFNet 2013, GoogLeNet 2014, VGG 2014, ResNet 2015). Tói sê giànhMZt bai cua blog dé viét vè cac kien trunc quamtronng nay. Xu the chung có the thay la cac mô hinh cang ngay cang deep. Xem hinh duóiday.

Nhùng cóng tv cóng nghe lón cung dé y tiên viếphát triên cac phong nghiên cuu deep learning trong thói gian nay. Rát nhieu cac的能量 dung cóng nghe dôt phá dā duoc ap dung vào cuoc songHang ngay. Cung ké túnam 2012, sô luong cac bai bao khoa先进技术 deep learning tanglen theo ham so mu. Cac blog vè deep learning cung tāng lin tung ngay.

# 1.2.1 Dinh nghoa

Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 Deep Learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô先进技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning
Mô工程技术 deep learning

Deep Learning (hoc sào) có the duoc xem làMZt linh vuc con cua Machine Learning (hoc may) - o do cac may tinh sé hoc va cai thieu chinh no thong qua cac thuat toan. Deep Learning duoc xay dung dura tren cac khai niem phuc tap hon rat nhieu, chu yéu hoat dong voi cac mang no-ron nhân tao de bat chuoc kha nang tu duy vu suy nghi一则 bô não con nguòi.

# 1.2.2 Céc thàn phân cúa Deep Learning

Lóp dαu vào (Input layer): Day lā tang nhân duī lièu dαu vào va truyen chung qua máng no-ron. Só luǒng nút (neurons) trong tāng dαu vào phú tuǒc vào kích thuoc va dac diém cuà duī lièu dαu vào.

Lóp ān (Hidden layers): Day la nhūng tāng nám giūa tāng dαu vào va tāng dαu ra. Mōi tāng ān thuòng chúa nhieu nút no-ron va cák két néi trong só (weight connections) giūa cháng. Mōi nút trong tāng ān tính toán dαu ra dua trèn dαu vào túc nút ò tàng trunc do và truyen cho tàng ké tiếp.

Hàm kích hoàt (Activation function): Hàm kích hoàt duoc bóp dung cho dαu ra cua miǒi nút trong máng no-ron dé gioi hàn và hiêu chinh giá tri dαu ra. Céc ham kích hoàt thóc dung bao gòm Sigmoid, Tanh, ReLU (Rectified Linear Unit), và Softmax (th corrobor duoc su dung trong cac tāng dαu ra cua mô hinh phân loài).

Lóp dàu ra (Output layer): Day lā táng cuói cungstrong máng no-ron, dura ra dàu ra du doán cho bái tiên,cú the. So luong nútstrong tang dàu ra phú tuoc vào loài bái toán, ví du: môt nút cho bái tiên du doán nhí phân (binary classification), nhieu nút cho bái tiên phân loài da lóp (multiclass classification), hoac môt nút cho bái tiên du doán giá tri thuc (regression).

Hàm mát mát (Loss function): Hàm mát mát do luong sù sai khác giūa dāu rα du doán và gói tri thuc né trong qua trinh huàn luyén. Nhiém vú)cua mo先进技术 Deep Learning là toí thiéu hóa gói tri hàm mát mát nay dé dura ra du doán chín xác.

Thuế tân亲友: Deep Learning sverige dung cac thai tiên refere uru hóa nhur Gradient Descent, Stochastic Gradient Descent (SGD),先进技术 biế nhe nhur Adam, RMSprop dé dièu chinh trong so va cap責 mô先进技术 dô lòi (error) tinh toán tù huàmát mét.

![](images/63d1bbbb169ff521f5fcc0a14dd014d050702917d6783cef37124568944bdb50.jpg)  
Hinh 1-3 Cau truc cua DeepLearning

# 1.2.3 Cach thuc hoat dong cua Deep Learning

Deep Learning hoat dong bang cach xay dung va huan luyen mang no-ron sau de hoc tu dtilieu. Qua trinh hoat dong cua Deep Learning co the duoc mo ta nhur Sau:

- Chuan bi dū lièu: Dàu tièn, dū lièu duǒc chuàn bi và tièn xu lý dé dam bao chat luǒng dū lièu dàu vào. Dièu nay có theb bao gòm các buóc nhur chuàn hóa, mā hóa, chia thanh tāp huán luyén và tāp kěm tra, và xu lý các giá tri thièu.   
Xay dung mo hinh Deep Learning: Mo hinh Deep Learning duoc xac dinh bang cach chon kien truc mang no-ron sau, bao gom so luong tang an vao so luong nut trong moi tang. Cac lura chon khac bao gom ham kich hoat, ham mat mat va thuat toan toi uru hoa.   
- Khoi tao trong so: Trong so cua mang no-ron sau duoc khoi tao ngau nhien hoac su dung cac phuong phap khoi tao trong so thong minh nhur Xavier hoac He.   
- Lan truyen thuan (Forward propagation): Du lieu duoc dura vao mo hinh va lan truyen qua cac tang no-ron tut tang dau vao den tang dau ra. Moi nut tinh toan dai ra dua tren dai vao tu cac nut o tang truc do va trong so két此時.   
- Tinh ioan dàu ra: Sau khi di qua cac tāng no-ron, mô hinh tính ioan dàu ra duà trên dū lièu dàu vào và trong só.

Danh giá dαu ra: Dαu ra cua mo hinh duoc so sanh vovi giá tri thuc té (trong truong hop huan luyén) hoac giá tri mong doi (trong truong hop du doán) bang cach su dung ham mat mat. Hàm mat mat do luong muc do sai khac giūa dαu ra du doán và giá tri thuc té.   
- Lan truyen nguoc (Backpropagation): Qua trinh lan truyen nguoc tinh soan dao hamCEE ha maT mat theo trong so. Dao hAM nay se chi ra cach thay doi trong so se anh huong den giT tri maT.   
- Capest難 trong so: Thuat toan toi uru hoa duoc sur dung de cap nhat trong so mang no-ron sau dua tren dao ham tinh duoc trong qua trinh lan truyen nguc. ViEc cap nhat trong so nham giam thieu giacute mat ma t va cai thieu hiieu suat cua mo hinh.   
- Lái qua trinh huân luyên: Qua trinh lan truyen thuan, tinh toán dαu ra, dinh giá, lan truyen nguc và cap nhát責s so duoc lap lui cho dén khi dat duoc dieu kiên dung, chang hàn nhú dαt du so luong epoch (vòng lap huan luyên) bóac khi gia tri mat mét không còn thay doi dang ké.   
- Dângh Giá và du dōan: Sau khi mô hinh duoc huan luyén, não duoc sùr dung dé dānh giá hiêu suat trèn tâp dū liēu kiém tra và du dōan trèn dū liēu mi.

Qua trinh nay duoc thuc hiên thong qua viec t ur dong hoc cac dac trung va moi.  
quan hé phuc tap t ur dinu thong qua viec dieu chinh trong so mang no-ron sau.

# 1.2.4 Môt vài mô先进技术 sòng Deep Learning

- Multilayer Perceptron (MLP): MLP làMZt dang co bān cua mang no-ron sau, gòm it nhát hai táng ān giūa táng dαu vào va táng dαu ra. Mōi nútstrong táng ān tinh toán dαu ra bang cach su dung hàn kich hoat phi tuyén, nhr ReLU (Rectified Linear Unit) hoac sigmoid. MLP duyec su dung pho bien trong cac bai toán phan loài va du doán.

- Convolutional Neural Networks (CNNs): CNNs duoc phat trien dac biét cho xú ly và phân tich dür liêu khóng gian nhú hinh anh và video. CNNs sür dung cac lôp tich chap (convolutional layers) dé trich xuát cac dac trung tür dür liêu. Sau do, cac lôp Pooling duoc sür dung dé giám kích thuoc cúa dür liêu. Cuói cùng, cac lôp fully connected layers duoc sür dung dé phân loài dür liêu hoac thuc hiên cac tac vu khác.

- Recurrent Neural Networks (RNNs): RNNs duoc thiet ké dé xu ly dür lieu theothosei gian và mô hinh cac phu chuoc dür lieu dura trên thoi gian. Mõi nútstrong RNN tínhtoán dura trên dür lieu hiên tai và trang thai an truc do. RNNs có kha näng lru trthong tinstrong qua khur va sur dung né annihil huong dén dai ra hiên tai. Tuy nhièn, RNNs thuong gap ván dé mat mat thong tin dai han. Do do, cac bien theh LSTM(Long Short-Term Memory) và GRU (Gated Recurrent Unit) duoc phat trién dé khacphuc ván dé nay.

- Generative Adversarial Networks (GANs): GANs làMZtkién truc mang no-ron sau gom hai phan:MZt mäng sinh (generator) vamzmt mäng phan biét (discriminator). Mng sinh duoc huan luyen de tao ra cac mau gionghur dulieu thc,trong khi mang phan biét duoc huan luyen de phan biét giura dulieu that va dur lieu tao ra boi mang sinh. Hai mang nay canh tranh va cung cai thien nhau tong qua trinh huan luyen,dan den viec tao ra cac mau chat luong cao va chan thuc.

- Transformer: Transformer latch忡忡 Deep Learning duoc dé xuát cho cactac vú xú ly ngôn ngū tú nhiên (NLP). Transformer sṛ dung kien truc khòng giant thói gian tuxem (self-attention) dé tim hiếu mí qui quan hé giūra cac tṛtrongh cái. Mô先进技术 nay cho phép viếc xú lý dòng thói cac tṛtrongh cái và giáp cai thién hiếu suattrongh viếc dìch may, táo racám chô hìn hop thanh vǎn bàn và nhiếu tác vú NLP khái.

Các mí hinh Deep Learning nay chì lá môt só ví du và khòng bao gòm tát cà cac kien truc có sǎn. Mōi mí hinh duçc thiet ké dé giai quyét môt tác vu cu thevà phù hop vói loài dū lièu và yêu cùa bàilando.

# 1.2.5 U du diem, nhuc diem cua Deep Learning

A. Uü diēm:

- Khā näng bó tù dū lièu phúc bóp: Deep Learning có khā näng bóc bó các bóc bó trung phúc bóp và mò hìn hóa miǒi quan hé phi tuyén giūa dū lièu. Dièu nay cho phép bó xú lý céc bài toán phúc bóp mà cac phuong pháp triyèn thóng gāp kho khǎn.   
- Tú dòng trich xuát dac trung: Deep Learning có kha nang tú dòng hoc và trich xuát dac trung tú dū liEU, giám thieu su phu thuoc vao vièc chon và xu lý dac trung

thu cóng. Dieu nay giup tiét kiem thoi bian va cóng suc trong qua trinh chuàn bi du liéu.

- Hiếu suát cao trên dū liếu lón: Deep Learning throung có hiếu suát cao trên dū liếu lón. Vói sú gia tāng cua cac nguôn dū liếu lón nhr先进技术, video, âm thanh, Deep Learning tro thanhMZtóng cu mângh mē dé xu ly va phân tích dū liếu do.

Tinh tóng quán hóa tí: Môt mù hinh Deep Learning huán luyén tí có khà nang tóng quán hóa tú dū liEU huan luyén sang dū liEU mí. Dieu nay cho phép bó du doán và xu ly cac mǎu dū liEU chua duoc nhìn tháy trucó bó.

B. Nhuoc diem:

- Doi hoi luong du lieu lón: Deep Learning yèu cαu luong du lieu lón de dαt duoc hiêu suat tot. Néu du lieu huan luyén han ché, có the xay ra hiên tuǒng overfitting, khi mo hinh chi hoc nhó du lieu huan luyén ma khòng the tóng quán háo cho du lieu mi.

- Doi hoi khoi luong tinh toan lón: Deep Learning yèu cau sú túnh toán mánh mē và tāi nguyén phàn cung de huan luyen và trièn khai mô先进技术. Cac mang no-ron sào có so luong tham so lón, dièu nay doi hoi phàn cung mánh de dam bao hiêu suat và thói gian chay.

- Khô khǎn trong diēn giai: Mô sinh Deep Learning thuong rát phúc tap và có só luǒng tham sô lón, dièu nay lam cho diēn giai két qua và hieu cach mô sinh dura ra dur doán tro nên khô khǎn. Môt mô sinh Deep Learning thuong duoc coi làMZt "hôp den" vi khó xác dinh cach cac quyét dinh duoc dura ra.

- Yếu cău kŷ thuát cao và kien thúc chuyén sào: Xañy dung và huan luyén mô,
hinh Deep Learning doi hói kien thúc chuyén sào vè linh vúc nay, cūng nhu kŷ näng,
lap trinh và xu ly dū liEU. Diēu nay lam cho viéc ap dung Deep Learning tro nên khô,
khǎn doi vói nhùng nguòi khòng có kien thúc chuyén môn.

Tóm lái, mác dù Deep Learning cónhūng uru diēm nãoi troítrong viêc xu lý duī liēu phúc tap, né cūng có nhúoc diēm và doìi hòi cac yèu tí nhú luǒng dū liēu lón, tinh toán mângh mē và kien thúc chuyén sào dé ap dung hiēu qua.

# 1.2.6 Ung dung cua Deep Learning

Deep Learning dã có nhùng (£ng £dàng và róng kháptron cuoc sòng häng ngay. Duói (£ay £lmight so vi du vè (£ng £ung £cua Deep Learning:

- Nhan dang的人生啊: Deep Learning dādāt duoc thanh tuu lón trong viêc nhân dang và phân loai的人生啊. No duoc sùr dung trong cac ting dung nhân dang khuôn mát, nhan dang doi tuong, phân loai anh y khoa và tu dong gan nhân anh.   
- Xú ly ngôn ngū tú nhiên (NLP): Deep Learning duoc sù dung trong NLP dé xú ly và hieu ngôn ngū tú nhiên. Céc的能量 dung bao gôm dìch may, phân loài vǎn bàn, phân tích cám xuç, tra lòi cái hòi và táo vǎn bàn tú dòng.   
• Tú lái xe: Deep Learning bóng vai tro quan trong trong cóng跟他 t ur lái xe. Nó duoc su dung dé nhân dang bién bao giao thong, phat hiên va phân loai doi tu ng trê duong, du doán hàngh vi cua cac phuong tiên giao thong va quan ly hé thóng lái xe t ur dong.   
- Nhan dang giong não: Deep Learning duoc sù dung trong cac úng dung nhān dang giong não nhu tro ly ao, dièu khién giong não và giao tiép ngôn ngū tú nhiên.   
- Du doan chuoi thoi giant: Deep Learning duoc ap dung trong viêc du doan chuoi thoi giant nhu du bao thoi tiét, du doan giac o phieu, du doan lru luong truy cap tranq web và du doan nhu cauHang hoa.   
- Táo khác dung sang táo: Deep Learning có the duoc su dung de tao ra khác dung sang táo nhu先进技术, am nhac, video và vân bàn.   
- Y tá chám sós khoe: Deep Learning duoc sùdung trong linh vuc y tí dé hôtro chan doan bênh, dú doán két qua dieu tri, phân tich hinh anh y khoa va tién hxanh phân loai bênh.   
- Tú dòng bóa cóng viêc: Deep Learning có the duoc sùr dung de tú dòng bóa cac cóng viêc và quy trinh trong cac linh vu c nhu sān xuát, logicties và dìch vu khách hàng.

Nhùng的能量每天都在不断增长，而它却在不断膨胀。它就像一个巨大的能量球，它能从地下喷涌出来，使天空的色彩更加鲜艳。

# 1.3 Tîm hiêu vê cóng nghê sù dung trong du án

# 1.3.1 Ngôn ngür Python

Python làMZt ngôn ngūr láp trinh thong dich, duoc tao ra vào nhūng nám 1990 bói Guido van Rossum. Nó láMZt ngôn ngūr láp trinh rát pho bién va duoc sù dung róng rāi trong nhieu linh vuc khac nhau, bao gòm phat trién web, tri tué nhân táo, khoa先进技术 duu lièu, tú dōng hóa, và nhieu Úng dung khac.

- Cu phap don gian: Python có cu phap de dong và de hièu. Cac khoi mă duoc dinh dang bang cac thut le (indentation) thay vi sù dung dαu ngoac nhon nhur nhieu ngôn ngū khac.

- Da muc dích: Python làMZt ngôn ngū da muc dích, có the duoc sù dung cho cac muc dích khac nhau nuu phat trién ñng dung desktop, web, phan tich du liéu, may hoc, và nhieu hon nña.

- Thu tiên và Framework phong/phú: Python có môt cóng dong lón và phong phú, cung cap nhieu thai tiên và framework mánh mê. Ví du: NumPy, Pandas, TensorFlow, Django, Flask, và nhieu thai tiên khác.

- Dynamic typing: Python lα ngôn ngū kiéu dòng, dièu nay có nhiglia là ban khòng can khai báo kiéu dū lièu truc cki sür dung bién. Bién có the duoc gán bát ký giá tri não và có the thay doi kieu dū lièutrong qua trinh thuc thi.

- Hô tro doi tuong: Python hô tro lap trinh huóng doi tuong, cho phép bàn tao ra cac lóp, doi tuong và ké thùa dé to chúc và tái sù dung mă.

Thu vièn mà Python thuòng hǎo troy trong mo hìn du bǎo:

- scikit-learn: Scikit-learn làMZt thu Viet monoc may pho bien trong Python. Mác du khóng phài láMZt thu Viet monoc chuyén vè du bao chuǒi thoi giant, nhung scikit-learn cung capMZt soMZm inh hoc may nhur hòi quy tuyén tinh, hòi quy Ridge va hòi quy Lasso có the duoc ap dung cho du bao chuǒi thoi giant.

- TensorFlow và Keras: TensorFlow làMZt thu viên hoc sau (deep learning) phô bien và Keras làMZt giao diên trên nèn TensorFlow. Vôi TensorFlow và Keras, bàn có thexay dung và huan luyên mô sinh mang no-ron (neural network) dé du bao chuõi thòi bian.

# 1.3.2 Jupyter Notebook

Jupyter Notebook làMZtungdung webmngon mocho phep ban tao va chia sé tai lieuu tuong tac, chua mPython va cac phan mo ta,hinh anh,cong thuc toan hoc va visualizations.No cho phep ban tao ra cac tep notebook co chu ma ngon Python duoc thuc thi tung phan, giup ban kham pha durdieu, thuc hi en cac thi nghiem, viet va chay mngon PythonMZt cach tuong tac.

Môt sô tinh nǎng quan trong cuà Jupyter Notebook bao gôm:

- Mā tuong tác: Jupyter Notebook cho phép bàn viét và chay mā Pythonùng ò (cell)MZ. Bàn có the thuc hiên tin h toán, xu ly du lieu và hièn thi két qua ngay tai chǎ.   
- Hien thi két qua va visualizations: Khi bàn chay môt ò chuá mã Python trong Jupyter Notebook, não sê hien thi két qua truc tiếp du ori ò do. Ban cūng có the tao ra cac bi éu do va truc quando háo dür liêntron cu ng nép notebook.   
- Rich text formatting: Bàn có the sù dung Markdown dé dinh dang vǎn bàn, tao tiếu dé, dans sachu, bóng thuc toán先进技术 và nhieuiform dung khac trong céc o tuong tác.   
- Tích hop vói*cac cóng cu va thu viên: Jupyter Notebook tích hop vói,nhieu cóng cu va thu vien pho bién trong cóng dong Python nhur NumPy, Pandas, Matplotlib và SciPy, giúp bàn lam viêc vói du lieu va thuc hiên cac phân tích phúc tap.   
- Chia se và xuát bàn: Bàn có the chia sè típ notebookCEEa minh duovi dang file .ipynb hoac chuyén doi chung thanh cac dinh dang khac nhur HTML, PDF hoac slide dé chia sé vovi nguòi khac.

Jupyter Notebook rát pho bien trong cóng dong khoa hoc du liieu, hoc may vànhieu linh vu c kac cua Python, vi nó cung cap moi truong tuong tac va linh hoat dé thuc hiên va chia sé cac din an va phan tich.

Jupyter Notebook không phài là môt thu viên hay cóng cu dãc thù cho du r bó chuǒi thòi giant, nhùng né là môt mù truong láp trinh linh hoát và mánh mē, cho hép

bàn sù dung céc thu viên dür bóo chuǒi thòi bian phô bien nhur Statsmodels, Prophet, scikit-learn, TensorFlow, và PyCaret.

Voi Jupyter Notebook, bàn có thec hienc cac buoc du bao chuoi thoi bian, bao gom:

- Khám phá và xú ly dū liêu: Sū dung cac thu viên nhr Pandas và NumPy, bàn có the doc và xu ly dū liêu chuǒi thoi gian, tao cac dac trung (features), xu ly cac giá tri thiéu, và thuc hiên cac bién doi dū liêu khac de chuǎn bi cho qua trinh du r bó.   
- Xày dung mô sinh du báo: Sù dung cac thu viên nuh Statsmodels, Prophet, scikit-learn, TensorFlow, và PyCaret, bàn có the xáy dung và dào tao cac mô sinh du báo chuǒi thòi bian nuh ARIMA, SARIMA, máng no-ron, hòi quy tuyén tính, hay mô sinh tóng hop khác.   
Danh Giá và tinh chinh mù先进技术: Sú dung cac phuong pháp dánghiá và do luong hiếu suát nhú do chinh xác, do lòi (RMSE, MAE), hé so xac dinh (R-squared), hay cac phuong pháp cross-validation, bàn có the dé nánghiá và tinh chinh mù先进技术. bó cúa minh dé dát hiếu suát tot khác.   
- Truc quán háo và bao cao: Sû dung thu viên nhr Matplotlib, Seaborn và Plotly, bàn có the truc quán háo két qua dý bao, so sânh giūa dý bao và du liEU thuc té, và táo bao cao có chúc cac bièu do và visualizations tuong tác.

Jupyter Notebook cung cap môt mi ori truong tuc và truc quan cho viêc thuc hiên dý bao chuõi thoi bian, giúp bàn khám phá, phân tích và chia sé két qua dé dang.

# 1.3.3 Môhh RNN

# A. Dinh nghia

- Mô先进技术 RNN (Recurrent Neural Network) làMZ kieu mô先进技术 no-ron.
- nhan tao duoc sur dung róng rāi trong xu ly du lieu chuovi và du lieu có lien quan thói,
- gian. RNN duoc theiet ké dé có khǎ nang nhó thùng tin tú cac buoc truc do vá ap dung
- não vào cac buoc tinh toán hiên tαι.   
- Mô先进技术 RNN cóMZI cau truc két não dac biêt giūa cac no-ron, cho phép truyèn nguoc thong tin tù cac buóc truóc do sang cac buóc sau do. Cac no-rontron

mǒt RNN khòng chi nhân dàu vào tú buóc hiên tαι mà còn nhân thong tin tútrightáng thai ān (hidden state))cua buóc truc. Trang thai ān làmight dang bónhó cua mo hinh, giúnp nám bǎt thòng tin lién quan tú qua khú.

- Céc buóc tinh tiên trong mô先进技术 RNN duoc thuc hiên tuân tú qua cac thói diém (time step) cúa dū liếu chuǒi. Tái mōi先进技术, RNN先进技术 vào hiên tát vā先进技术.
- Trang thai an tú buóc truc, sau do tinh先进技术, vā先进技术.
- Láp di lap lái chôden khi dat duoc dαu rα mong doi先进技术.
- Trinh nay先进技术.   
- Mô先进技术 RNN có the duoc sù dung trong nhieu tac vu khac nhau nu du doan chuài, phân loai chuài, dich may, tao vǎn bàn, nhân dang giong néi va nhieu ñng dung khac lien quan dén dür lièu chuài.   
- Môn trong nhùng ván de cua mô hinh RNN là ván de vanishing gradient, khi gradient (dào ham) truyen nguoc tù lóp cuói cung dén cac lóp dàu tiên bì giám dàn và gay khó khàntrongh viêc capnhat trong so. De khac phuc ván de nay, cac phiên bàn cai tién cúa RNN nhur LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Unit) dà duoc phát trièn.

Tóm lài, mù先进技术 RNN là khác kieu先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术先进技术关键技术科学技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术的技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术技术

![](images/86298c486ddf173a0ca6069c7dfb0cc2a9f13e6be5f9d7957278cfbe9e64a1c8.jpg)  
Hinh 1-4 Cáu truc mô sinh RNN

# B. Phan loai bai toán RNN

One to one: mău bai是多么 cho Neural Network (NN) vă Convolutional Neural Network (CNN), 1 input vă 1 output, vindu voi CNN input là anh va output là anh dinoc segment.

One to many: báilando có 1 input nhung nhieu output, ví du: bái toán caption cho anh, input là 1 anh nhung output là nhieu chur mô tá cho anhday, duói dangMZt cau.

Many to one: bái tiên input nhung chí có 1 output, vú du bái ].loài khác dòng video, input là ].nhieu anh (frame) tach ra tú video, ouptut là ].hàn dòng video

Many to many: bai toán có nhieu input và nhieu output, ví du bai toán dìch tú tiếng anh sang tiếng Viet, input là 1 cái góc nhieu chūr: “I love Vietnam” và output cūng là 1 cái góc nhieu chūr “Tói yếu Viet Nam”.

# 1.3.4 Môshima LSTM

Mô先进技术 LSTM (Long Short-Term Memory) làMZdng dac biêt cua mang no-ron hoi quy (RNN) duoc thiet ké dé xu ly du lieu chuoi va giai quyêt ván de vanishing gradient trong RNN.

Môt LSTM có cau truc tuong tu nhr RNN, nhung não cóthem cac coché dac biét nhu cóng (gates) dé dièu chinh thong tin duoc lru tru va truyen qua láitron qua trinh tinh toán.

Các cóng trong LSTM bao gòm:

- Cóng quên (Forget gate): Xác dinh thóc tin não trong trang thai trucoc do can bi lang quên. No quyét dinh giūr lai thóc tin não va bó qua thóc tin não tù qua khúr.   
- Cộng dαu vào (Input gate): Xác dinh thong tinmericano sé duoc lru tru trong trang thai tiép theo. No quyét dinh thong tinmericano sé duoc capnish tu dαu vao hiend tai.   
- Cóng dαu ra (Output gate): Xác dinh phàn não cóu trang thai hiên tαi sè la dαu ra cóu LSTM. No quyét dinh thong tin não sè duoc dura ra tù trang thai hiên tαi.

- Trang thai an (Cell state): LàMZbHOdai hAN, lRU tru thong tin tu qua khur va truyen thong tin tói cac buoc tinh toan tiép theo trong chuoi.   
- Vói*cóng nay, LSTM có khà nang luru trū thòng tin quan trong tú qua khú và loài bó thòng tin khòng quan trong. Dièu nay giúp LSTM giāi quyét ván de vanishing gradient và có khà näng xu lý duoc cac chuǒi dai và tzung quan phúc tap hon so vói RNN thòng thùng.   
- Mô先进技术 LSTM duoc su dung róng rai trong cac linh vu cnc nhur xu ly ng on ngu tu nhiên, dich may, nhân dang giong néi, din bao chu o i thoi giant và nhieu u ng dung khac lien quan dén dür li eu chu o i.

Tóm lái, mô hinh LSTM làMZdng dac biét cua mang no-ron hoi quy, duoc thiet ké dé xu ly dū liêu chuovi và giai quyét ván dé vanishing gradient. Nósurdung cac cóng dé dieu chinh thong tin va trang thai trinh tinh toán, giúp não luru tru thong tin qua trong va loai bó thong tin khong quan trong. LSTM dā duoc chung minh láMZc ng cu mânh mētrong viéc xu ly va du doán cac du liêu chuovi phuc tap.

![](images/c48688cc59264450c444bc3405e962f690fec4ac4745bef0ecf5ae4e1e8bf577.jpg)  
Hinh 1-5 Cău truc mo先进技术 LSTM

Output:ct,ht,ta goi c la cell state,t la hidden state.

Input: ct-1,ht-1,xt. Trong do xt la input o state thu t cua model.

Doc bièu do ò trên ta có the thé thay ki hiêu σ, tanh có nhgia la ò buoc day dung ham sigma, tanh.

Phép nhān òday la phép nhān tung phàn tú, phép cóng la cóng ma tran.

$\mathrm{F}_{\mathrm{t}}, \mathrm{i}_{\mathrm{t}}, \mathrm{o}_{\mathrm{t}}$ truong bói forget gate, input gate, output gate:

Forget gate: $f_{t} = \sigma (U_{f} * x_{t} + W_{f} * h_{t-1} + b_{f})$

Input gate: $\mathrm{i}_{\mathrm{t}} = \sigma (U_{i} * x_{t} + W_{i} * h_{t-1} + bi)$

Output gate: $o_t = \sigma(Uo * xt + W_o * ht - 1 + bo)$

Nhān xét 0<ft,i,t,ot<1;bf,bi,bo la cac hê so bias,hê so W,U gionghu bai RNN

$c = \tanh \left( Uc * xt + Wc * ht - 1 + bc \right)$

$c_{t} = ft * ct - 1 + it * ct$ , forget gate quyêt dinh xem lay bao nhieu tù cell state truóc và input gate sé quyêt dinh lay bao nhieu tù input state và hidden layer一则 layer truóc.

$h_t = ot * \tanh(ct)$ , output gate quyêt dinh xem can lay bao nhieu tu cell state détro thanh output cua hidden state. Ngoài ra ht cūng dc dung dé tinh ra output yt cho state t.

Nhân xét: ht,ct khá giong vói RNN,nên model có short tem memory. Trong khi do ct giong nuh môt bang chuyen o trên mô sinh RNN, thùng tin não can qua nang trong và dung o sau sê duoc gui vao dung khi can=>co the mang thong tin tù di xa.Do do mot hinh LSTM có ca short term memory và long term memory.

# CHUONG 2 DÜ LIEU VÀ MA TRÁN DANH GIÁ

# 2.1 Dū lielight

# 2.1.1 Giói thieu

Dür liEU duoc lay tu bó dür liEU trén Kaggle có tên “Tetuan City Power Consumption” do nguòi dung GMKeshav tao ra(1). Bô dür liEU nay duoc thu tháp tai thânphó Tetuan, Marocstrong khoàng thói gian nám 2017. Nó bao gòm thùng tin vè tiEU thú diên cuà cac hô gia dinh vu céc toà nhà khác nhau trong thângh phó Tetuan. Tác già dā thu tháp dür liEU tù cóng ty diēn luc cuà thângh phó Tetuan, Maroc và chia sè bó dür liEU nay trèn Kaggle dé cho cac nhà駐hiên cuu vu dür liEU hoc có the sù dung và phân tích. Nó duoc cóng bó duǒi giáy phép CCO, có nghīa lá bó dür liEU nay lá miěn phi và có the duoc sù dung cho muc dích thuring mai hoac phi thuring mai mà khòng can yèu cùu su r chopép cuà tác già.

Thong tin nap duiieu(2):

Bànthúghiem dé uroc tinh suc chuá dā duoc trién khai trong phong 6m × 4,6m. Thiét lapt bao gom 7 nut cām bien và môt nuti bien trong cau hinh sao vovi cac nuti cām bien truyen dū liEU dén bien sau miōi 30 giay bang cach sū dung bó tu hu phat khóngday. Khòng có hé thóng HVAC nao duoc su'dung trong khi bó du lieu dang duoc thu thap.

Nam loai cam bien khong xam nhap khac nhau dà duoc sù dung trong thí nghiem nay: nhiét do, anh sang, am thanh, CO2 và hóng ngoai thu dong kū thai so (PIR). Céc cam bien CO2, am thanh và PIR can hiếu chuàn tu cóng. Doi vói cam bien CO2, hiếu chinh diém 0 duoc thuc hiên tu cóng truc lán sù dung dαu tiến bang cach giū nãotron mi triuong sach trong hon 20 phút, sau do kéo chan hiếu chuàn (chân HD) xuóng múc tháp trong hon 7 giày. Cam bien am thanh vè co bàn làmight micro có bó khuéch dai truong tú có múc tāng thay doi duoc gán vào né. Do do, dαu ra一则 cam bién nay là tin hiếu truong tú duoc doc boi ADC一则 vi diéu khién tinh bang vôn. Chiét ap gán vói múc tāng一则 bô khuéch dai dà duoc diú chinh dé dam bǎo do nhay cao

nhát. Cam bien PIR có hai nut dièu chinh: môt dé dièu chinh do nhay và môt dé dièu chinh thói giant dé dàu ra duy tri ò muc cao sau khi phát hiên chuyén dong. Cá hai dièu nay dà duoc dièu chinh dén cacgia tri cao nhát. Cac nút cam bien S1-S4 bao gôm cam bien nhiét do, anh sang va am thanh, S5 có cam bien CO2 và S6 và S7 co môt cam bien PIR, miōi nút duoc trièn khai trèn cac go tran ò môt góc giúp tói da hóa trùng nhìn)cua cam bien cho chuyén dong phát hiên.

Dulieu duoc thu thap trong khoang thoi gian 4 ngay môt cach có kiêm soat vovi suc chuia trong phong thay doi turo 3 nguoi. Sut that co bàn vé so luong nguoi trong phong duoc ghi lui bang tay.

Thong tin thuoc tinh:

Ngay: YYYYY/MM/DD

Thói gian: HH:MM:SS

Nhiêt dô: Bǎng dô C

Ánh sáng: Bǎng Lux

Am thanh: Bäng Vôn (dàu ra bó khuéch dai duoc doc boi ADC)

CO2: Bǎng PPM

Dô alc CO2: Dô alc cua cac giá tri CO2 duoc lay trong môt cua so truôt

PIR: Giá tri nhí phàn truyèn phát hiên chuyén dòng

Room_Occupancy_Count: Ground Truth

# 2.1.2 Phan tich

Srdung thu vien Pandas de doc duiieu:

```python
In [15]: path = "Tetuan City power consumption.csv" data = pd.read_csv(path) data 
```

Out[15]:   

<table><tr><td></td><td>DateTime</td><td>Temperature</td><td>Humidity</td><td>Wind Speed</td><td>general diffuse flows</td><td>diffuse flows</td><td>Zone 1 Power Consumption</td><td>Zone 2 Power Consumption</td><td>Zone 3 Power Consumption</td></tr><tr><td>0</td><td>1/1/2017 0:00</td><td>6.559</td><td>73.8</td><td>0.083</td><td>0.051</td><td>0.119</td><td>34055.69620</td><td>16128.87538</td><td>20240.96386</td></tr><tr><td>1</td><td>1/1/2017 0:10</td><td>6.414</td><td>74.5</td><td>0.083</td><td>0.070</td><td>0.085</td><td>29814.68354</td><td>19375.07599</td><td>20131.08434</td></tr><tr><td>2</td><td>1/1/2017 0:20</td><td>6.313</td><td>74.5</td><td>0.080</td><td>0.062</td><td>0.100</td><td>29128.10127</td><td>19006.68693</td><td>19668.43373</td></tr><tr><td>3</td><td>1/1/2017 0:30</td><td>6.121</td><td>75.0</td><td>0.083</td><td>0.091</td><td>0.096</td><td>28228.86076</td><td>18361.09422</td><td>18899.27711</td></tr><tr><td>4</td><td>1/1/2017 0:40</td><td>5.921</td><td>75.7</td><td>0.081</td><td>0.048</td><td>0.085</td><td>27335.69620</td><td>17872.34043</td><td>18442.40964</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>52411</td><td>12/30/2017 23:10</td><td>7.010</td><td>72.4</td><td>0.080</td><td>0.040</td><td>0.096</td><td>31160.45627</td><td>26857.31820</td><td>14780.31212</td></tr><tr><td>52412</td><td>12/30/2017 23:20</td><td>6.947</td><td>72.6</td><td>0.082</td><td>0.051</td><td>0.093</td><td>30430.41825</td><td>26124.57809</td><td>14428.81152</td></tr><tr><td>52413</td><td>12/30/2017 23:30</td><td>6.900</td><td>72.8</td><td>0.086</td><td>0.084</td><td>0.074</td><td>29590.87452</td><td>25277.69254</td><td>13806.48259</td></tr><tr><td>52414</td><td>12/30/2017 23:40</td><td>6.758</td><td>73.0</td><td>0.080</td><td>0.066</td><td>0.089</td><td>28958.17490</td><td>24692.23688</td><td>13512.60504</td></tr><tr><td>52415</td><td>12/30/2017 23:50</td><td>6.580</td><td>74.1</td><td>0.081</td><td>0.062</td><td>0.111</td><td>28349.80989</td><td>24055.23167</td><td>13345.49820</td></tr></table>

52416 rows $\times 9$ columns

# Hinh 2-1 Thong tin du lieuu

In [14]: data.info()   
```txt
<class 'pandas.core.frame.DataFrame'>  
RangeIndex: 52416 entries, 0 to 52415  
Data columns (total 9 columns):  
# Column Non-Null Count Dtype  
--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— —— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——— ——————— 
0DateTime 52416 non-null object  
1 Temperature 52416 non-null float64  
2 Humidity 52416 non-null float64  
3 Wind Speed 52416 non-null float64  
4 general diffuse flows 52416 non-null float64  
5 diffuse flows 52416 non-null float64  
6 Zone 1 Power Consumption 52416 non-null float64  
7 Zone 2 Power Consumption 52416 non-null float64  
8 Zone 3 Power Consumption 52416 non-null float64  
dtypes: float64(8), object(1)  
memory usage: 3.6+ MB 
```

# Hinh 2-2 Cac thuoc tinh d'lieu

Tóng sô dong: 52416 dong   
- Co 9 thuoc tinh:

1. DateTime: ngaygio   
2. Temperature: nhiêt do   
3. Humidity: dô âm   
4. Wind Speed: tóc dôgio   
5. General diffuse flows: dong khuech tan chung   
6. Diffuse flows: dong khuech tan   
7. Zone 1 Power Consumption: tiêu thú diên vùng 1

8. Zone 2 Power Consumption: tiêu thú diên vùng 2   
9. Zone 3 Power Consumption: tièu thú diên vùng 3

Kich thuoc: 3.6 MB

Mô tâ dū liEU:

<table><tr><td>In [17]:</td><td colspan="9">data = pd.read_csv(path, index_col = &#x27;DateTime&#x27;)</td></tr><tr><td>In [18]:</td><td colspan="9">data.head()</td></tr><tr><td>Out[18]:</td><td>Temperature</td><td>Humidity</td><td>Wind Speed</td><td>general diffuse flows</td><td>diffuse flows</td><td>Zone 1 Power Consumption</td><td>Zone 2 Power Consumption</td><td>Zone 3 Power Consumption</td><td></td></tr><tr><td rowspan="5">DateTime</td><td>1/1/2017 0:00</td><td>6.559</td><td>73.8</td><td>0.083</td><td>0.051</td><td>0.119</td><td>34055.69620</td><td>16128.87538</td><td>20240.96386</td></tr><tr><td>1/1/2017 0:10</td><td>6.414</td><td>74.5</td><td>0.083</td><td>0.070</td><td>0.085</td><td>29814.68354</td><td>19375.07599</td><td>20131.08434</td></tr><tr><td>1/1/2017 0:20</td><td>6.313</td><td>74.5</td><td>0.080</td><td>0.062</td><td>0.100</td><td>29128.10127</td><td>19006.68693</td><td>19668.43373</td></tr><tr><td>1/1/2017 0:30</td><td>6.121</td><td>75.0</td><td>0.083</td><td>0.091</td><td>0.096</td><td>28228.86076</td><td>18361.09422</td><td>18899.27711</td></tr><tr><td>1/1/2017 0:40</td><td>5.921</td><td>75.7</td><td>0.081</td><td>0.048</td><td>0.085</td><td>27335.69620</td><td>17872.34043</td><td>18442.40964</td></tr></table>

# Hinh 2-3 Môt sô dür lièu dàu tién

In [19]: data.describe() #mó tí dú liEU

<table><tr><td colspan="9">Out[19]:</td></tr><tr><td></td><td>Temperature</td><td>Humidity</td><td>Wind Speed</td><td>general diffuse flows</td><td>diffuse flows</td><td>Zone 1 Power Consumption</td><td>Zone 2 Power Consumption</td><td>Zone 3 Power Consumption</td></tr><tr><td>count</td><td>52416.000000</td><td>52416.000000</td><td>52416.000000</td><td>52416.000000</td><td>52416.000000</td><td>52416.000000</td><td>52416.000000</td><td>52416.000000</td></tr><tr><td>mean</td><td>18.810024</td><td>68.259518</td><td>1.959489</td><td>182.696614</td><td>75.028022</td><td>32344.970564</td><td>21042.509082</td><td>17835.406218</td></tr><tr><td>std</td><td>5.815476</td><td>15.551177</td><td>2.348862</td><td>264.400960</td><td>124.210949</td><td>7130.562564</td><td>5201.465892</td><td>6622.165099</td></tr><tr><td>min</td><td>3.247000</td><td>11.340000</td><td>0.050000</td><td>0.004000</td><td>0.011000</td><td>13895.696200</td><td>8560.081466</td><td>5935.174070</td></tr><tr><td>25%</td><td>14.410000</td><td>58.310000</td><td>0.078000</td><td>0.062000</td><td>0.122000</td><td>26310.668692</td><td>16980.766032</td><td>13129.326630</td></tr><tr><td>50%</td><td>18.780000</td><td>69.860000</td><td>0.086000</td><td>5.035500</td><td>4.456000</td><td>32265.920340</td><td>20823.168405</td><td>16415.117470</td></tr><tr><td>75%</td><td>22.890000</td><td>81.400000</td><td>4.915000</td><td>319.600000</td><td>101.000000</td><td>37309.018185</td><td>24713.717520</td><td>21624.100420</td></tr><tr><td>max</td><td>40.010000</td><td>94.800000</td><td>6.483000</td><td>1163.000000</td><td>936.000000</td><td>52204.395120</td><td>37408.860760</td><td>47598.326360</td></tr></table>

# Hinh 2-4 Dür lieu duoc mô tá

Xu ly du lieu:

Kiém tra có GPIa tri nao bi thieu khong.

In [5]: # finding all columns that have nan: data.columns[data.isnull().any()

Out[5]: Index([], dtype='object')

Hinh 2-5 Giá tri thieu trong din lieu

Kiém tra tiên tuyén tinh và bien thieu cua dür liEU. Phân tich nó dé xem xu huóng và tinh thoi vu. Cung cap quy trinh lam vièc chung cho du báo.

Duǒi dāy, cháng ta có mòt bieu dò vè cach tú turong quan cúa cac tín hiēu khac nhau thay dōi theo dô trē dè mo hìn hóa tot hon.

In [6]: #To fill in missing values, first check for linearity in each column using autocorrelation plots def get_acf(data,lags): frame $=$ [] for i in range(lags+1): frame.append(data.apply( lambda col: col.autocorr(i), axis=0)) return pd.DataFrame(frame).plot.line() get_acf(data,10)

![](images/22634b2db51a06b278a2d7637e8cdbb120172f5b5a8ae7fb6504a69c57b24b2a.jpg)  
Hinh 2-6 Bièu do tvr twork quan

Chung ta có thec hiên cac so do loi cua cac tup hop khac nhau de xem du lieuu thay doi nhur the nao. Lap biéu do nanh moi ngay.

Hinh 2-7 Lap bièu dô nhanh mi ngay   
```python
In [8]: #Its imperative from autocorealtion plots that we dont need each minute data, perhaps we can decide roll up the aggregate i = 1 # plot each column plt.figure(figsize=(20, 15)) for counter in range(1, len clean_data.columns): plt.subplot(len clean_data.columns), 1, i) plt.plot Clean_data.resample('D').mean().values[;, counter], color = 'blue') plt.title clean_data.columns[count], y=0.8, loc='right') i = i+1 plt.show() 
```

![](images/3de31d6635296bc7a7648a4151bba91d4f6f438d377b65a5211e75ac6ab3bc84.jpg)  
Hinh 2-8 Bièu dô nhanh miì ngày

# 2.1.3 Chia têp dãr lièu

Chúng ta có the chia bó dū lièu thàngh 2 tí le 80:20, tuc la 80% du liêu duoc su dung de huan luyén mo sinh và 20% du lièu duoc su dung de tanh giá hiêu suát cua mo sinh.

- Têp huan luyen (training set) gôm 80% dãr liêu.   
Têp dângh ia (test set) gôm 20% du lieu.

```python
# Import the dataset and encode the date  
df = pd.read_csv('Tetuan City power consumption.csv')  
group = df.groupby('DateTime')  
Real_Price = group['Zone 1 Power Consumption'].mean()  
print(Real_Price)  
DateTime  
1/1/2017 0:00 34055.69620  
1/1/2017 0:10 29814.68354  
1/1/2017 0:20 29128.10127  
1/1/2017 0:30 28228.86076  
1/1/2017 0:40 27335.69620  
...  
9/9/2017 9:10 30628.67257  
9/9/2017 9:20 31291.32743  
9/9/2017 9:30 31750.08850  
9/9/2017 9:40 32419.11504  
9/9/2017 9:50 32724.95575  
Name: Zone 1 Power Consumption, Length: 52416, dtype: float64 
```

*** Split the dataset.**

Hinh 2-9 Tâp dür lièu và nhóm ngày

```txt
In [3]: # split data df_train = Real_Price.sample( frac=0.8, random_state=123) df_test = Real_Price.drop(df_train.index) print(df_train)  
DateTime  
4/19/2017 1:40 24738.85899  
8/3/2017 7:20 28058.42397  
11/25/2017 20:20 37390.76923  
4/8/2017 19:20 42824.88698  
12/13/2017 6:10 22904.94297  
...  
8/1/2017 21:10 48215.22752  
7/27/2017 6:10 26305.91362  
9/11/2017 0:10 31437.87611  
6/17/2017 14:20 35640.79470  
5/31/2017 5:40 21516.59016  
Name: Zone 1 Power Consumption, Length: 41933, dtype: float64 
```

Hinh 2-10 Chia tep du liieu

# 2.2 Ma trân dánh tiên

# 2.2.1 RMSE

Trung sinh, sai sô sinh phuong trung sinh góc (RMSE) làMZt so liieu cho chúng ta biét cac giá tri du doán cua chúng ta khac nhau bao xa so vói cac giá tri quan sát duoctrongMZt mo hinh(3).No duoc tinh nhur sau:

$$
R M S E = \sqrt {\sum_ {i = 1} ^ {n} \frac {(\widehat {y} _ {i} - y _ {i}) ^ {2}}{n}}
$$

- $\widehat{y}_l$ là GPIa tri uoc luong   
yla biéndoc lap   
$\mathbf{n} = (\mathbf{N} - \mathbf{k} - 1)$   
N: sô tóng luǒng quan sát   
k: tong luong bian

# Tinh RMSE bang Python

Gia sur chung ta có cac mang giá tri du doan va thuc té sau:

$$
\text {t h y c} \quad t e = [ 3 4, 3 7, 4 4, 4 7, 4 8, 4 8, 4 6, 4 3, 3 2, 2 7, 2 6, 2 4 ]
$$

$$
d u \text {d o a n} = [ 3 7, 4 0, 4 6, 4 4, 4 6, 5 0, 4 5, 4 4, 3 4, 3 0, 2 2, 2 3 ]
$$

# Hinh 2-11 Giá tri thuc và giá tri du doán

Dé tinh tiên RMSE giūa giá tri thuc và giá tri du doán, chung ta chi can lay can先进技术 hàn mean_squared_error() tu thu viên sklearnern.metrics:

Hinh 2-12 Tinh RMSE   
```txt
nhap thu vien can thiiet   
tu sklearn.negs metrics nhap mean_squared_error   
tu toan nhap sqrt   
tinh toan RMSE sqrt(mean_squared_error(thuc, truoc))   
2.4324199198 
```

RMSE l'mot cach huu ich de xem muc do phu hop cua moot mo sinh voi tap du lieu. RMSE cang lon thi chenh lech giua ga tri du doan va gia tri quan sat cang lon, nghia la mô sinh phu hop voi d'liêu cang tê. Nguoc lái, RMSE cang nh'ti mo sinh cang phu hop voi du lieu.

Có the dac biét hru ich khi so sanh RMSE cua hai mo sinh khac nhau voi nhau dé xem mo hinh nao phu hop vovi din(3).

# 2.2.2 MAPE

Lói tý lê phân trám tuyêt doi trung binh (MAPE) thuong duoc su dung de do luong do chinh xac du doan cua cac mo sinh(4). Nó duoc tinh nhur sau:

$$
M A P E = (1 / n) * \sum \left(\left| \frac {\widehat {y} _ {i} - y _ {i}}{y _ {i}} \right|\right) * 1 0 0
$$

n la so luong cac mau trong tad du lieu   
- $\widehat{y}_{\iota}$ làgia tri du doan thu i   
yla gia tri thuc te thu i

MAPE duçc sù dung phô bien vì nó de hièu và de giài thích. Ví du: giá tri MAPE à 11,5% có nghía là chênh lech trung binh giūa giá tri du Rodr doán và giú tri thuc t'é la 11,5%.

Gia tri MAPE cang thap thi mo先进技术 co kha nang du doan cac gia tri tot hon. Ví du:MZt mo先进技术 co MAPE la 5% sé chinh xac honMZt mo先进技术 co MAPE la 10%.

# Cach违法违规 MAPE trong Python

Không có hàm Python tích hop dé túnh MAPE, nhùng chung ta có the tao môt hàm don giàn dé lām nhú vay:

nhap numpy duoi dang np   
ban do def (thc t,du doan): thc t,pred $\equiv$ np.array(actual),np.array(pred) tra la np.mean(np.abs((thc t -du doan)/thc t)) \*100

# Hinh 2-13 Hàm tiên MAPE

Sau do, chung ta có the sù dung hàn nay de tinh toán MAPE cho hai máng: môt máng chuá cac giá tri dū liEU thuc né và môt máng chuá cac giú tri dū liEU du doán.

thuc = [12, 13, 14, 15, 15, 22, 27]

truoc = [11, 13, 14, 14, 15, 16, 18]

mape (thuc te, du doan)

10.8009

# Hinh 2-14 Tinh MAPE

Tú két qua, chung ta có the thay rang sai so phan tram tuyêt doi trung binh cho mô先进技术 nay la 10,8009%. Nói cach khac, chênh lech trung binh giu ga tri du doanh và tiên thuc né la 10,8009%.

# Ynghia cua MAPE

1. Phu chuoc vao giá tri thuc né: MAPE chia tý lé phàn trám sai lêch theo giá tri thuc né.新股 giá tri thuc né hoac bang 0, phép chia nay có the dan dén sai sot hoac khong xac dinh. Ví du, khi yi = 0, tý lé phàn trám sai lêch sé khòng có y nggia hoac khong the tinh duoc.   
2. Nhay cam voi giá tri thay doi gàn 0: MAPE khong xu ly tôt khi giá tri thuc té gàn 0 hoac có giá tri rát nho. Khi giá tri thuc té xap xi 0, cac sai so nhô sê co annihil huong lón dént y le phàn trám sai lêch, gay ra bieu do lêch lón và khong chinh xac.

# CHUONG 3 DÉ XUÁT VÀ DANH GIÁ KÉT QUÀ MÔ H/Internal

Trong linh vuc du bao chuoi thoi gian, viec nghién cuu va ap dung cac mo hinh hoc sau da dong vai tro quan trong trong viec du bao xu huong va du doan trong tuong lai

Trong bói tiên do, mô先进技术 LSTM (Long Short-Term Memory) và RNN (Recurrent Neural Network) dã thú hút sù quan tân và tro先进技术 cóng cù quan先进技术 choc先进技术.

Trong bái khác ncu nay, chung toci dé xuát sù dung mô hinh LSTM và RNN dé dür bó muc tiêu thú näng luóng cúa môt THANh phó. Muc tiêu chinh cúa cháng toci là xay dung mò mô hinh dür bó chinh xác và tin cay, giú dura ra dür doán vè xu huóng tiêu thú näng luóng trong tuong lái.

Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术
Mô先进技术

# 3.1 Dé xuát

Cái truc cúa mô先进技术 LSTM bao gócdon ví LSTM (Long Short-Term Memory) duoc xép chong lén nhau. Mói don ví LSTM bao góccong dac biét dé dièu chinh thong tin va qua trinh truyen thong tin qua thoi giant. Céc cóng chinh bao gócquên (forget gate) dé quyét dinh xem thong tin nàn nén bi loai bó, cóng dαu vào (input gate) dé quyét dinh thong tin nào nén duoc cap nhát va cóng dαu ra (output gate) dé quyét dinh thong tin nào nén duoc truyen tiép. Cái truc nay guip LSTM luu tru vú ly thong tintronghi giant dai, giup no phu hop cho du báo chuovi thoi giant.

Hinh 3-1 Câu truc LSTM   
```r
model = Sequential()  
model.add(LSTM(67, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))  
# model.add(Dropout(0.3))  
model.add(LSTM(45, return_sequences=True))  
model.add(LSTM(38))  
model.add(Dropout(0.3))  
model.add(Dense(1))  
model.compile(loss='mse', optimizer='adam')  
# fit network  
history = model.fit(train_X, train_y, epochs=100, batch_size=30, validation_split=0.2, verbose=2, shuffle=False) 
```

Mô先进技术 RNN bao gôm cécdon vì RNN (Recurrent Neural Network) duoc két não vói nhau theo thu tú thoi giant. Môi don vì RNN sēnhân dàu vào tú thói diém trucó do và táo ra dàu rα tαi thói diém hiên tαi. Cáu truc nay cho phép RNN xu ly thong tin chuδi và truyen thóc tin qua thói giant.

Hinh 3-2 Câu truc RNN   
#Initialising the RNN   
regressor $=$ Sequential()   
#Adding the input layer and the LSTM Layer   
regressor.add(LSTM(units $= 100$ ,activation $\equiv$ 'sigmoid',input_shape $\equiv$ None,1))   
#Adding the output layer   
regressor.add(Dense(units $= 1$ ))   
#Compiling the RNN   
regressor.compile(optimizer $\equiv$ 'adam',loss $\equiv$ 'mean_squared_error')   
#Fitting the RNN to the Training set   
regressor.fit(X_train,y_train,batch_size $= 5$ ,epochs $= 100$ ）

Cac tham so quan trong trong mô先进技术 LSTM và RNN bao gôm so luong don vi (units) trong môi máng,Ham kich hoat (activation function) duoc su dung, tóc do先进技术 (learning rate) và so luong lôp an (hidden layers). So luong don vi anh huong dén khá nang mô先进技术 và xú ly thóc tin, trong khi ham kich hoat giup mô先进技术 céc môi quan hé phi tuyén. Tóc do先进技术 hóng dén toc do hói tu cua qua trinh huán luyén và so luong lôp an cho phép mô先进技术 céc muc trùr技术创新 khac nhau cua dür liêu(5).

# 3.1.1 RNN

Máng RNN (Recurrent Neural Network) làMZtkiéntruc mäng no-ron nhân tao dac biét duoc thiet ké dé xu ly dμ liêu tuàn tμ, có tinh chat thoi giant. Nó có khā näng

luu tru thong tin tuc cac thoi diem truc do va sur dung thong tin do duan cac thoi diem tiep theo tong dai dui lieu.

Càu truc mäng RNN bao gòm cac thanh phàn chín sau:

- Dàu vào (Input): Dū lièu dàu vào làmight chuǒi dū lièu tuàn tú duòc biéu diēn duǒi dang ma tràn có kích thuoc (sequence_length, input_dimention). Mōi thòi diémtron ghuói dū lièu sē duòc dura vàomight layer RNN.   
- Layer RNN (RNN Layer): Layer nay thuc hiên xu ly duilieu tuan t. Trong môt RNN, cac dαu vao tuc cac thoi diém truc do duoc két hop vovi dαu vao hiên tai dé tinh toán dαu ra. Môttron ng ngk ieu RNN pho bién la LSTM (Long Short-Term Memory), nhur troq doan code ban da cung cap.   
- Dàu ra (Output): Dàu ra一则 mäng RNN có the lê môt giá tri du doan cho mi o thoi diém trong chuovi dür liêu hoac chi la môt giá tri du doan o thoi diém cuo i cung. Doi voci bai toán du doan du liêu dang so, môt layer fully connected (Dense) duoc su dung dé tinh toán dàu ra.   
Tongquan, mäng RNN có cac layer RNN duoc xép chong len nhau dé xu lý du lieu tuàn tµ theo cac thoi diém. Cac két não giūa cac layer cho phép truyen du liEU qua thoi giant, tµ cac thoi diém trucoc do dén cac thoi diém sau do.

Qua trinh huan luyen (tiép tuc): Qua trinh huan luyen cua mang RNN thuong bao gom cac buoc sau:

- Chuan bi du liu huan luyen: Du lieu huan luyen duoc chan bi duoi dang chuoi cac mau du liu tuan t, voi dau vao va dau ra tuong ting.   
- Xêt yung mô hinh: Mô先进技术 RNN duoc xêt yung bang cach khoi tao mot mang RNN và them cac layer RNN, layer dâu vào và layer dâu ra.   
- Bien dich mô先进技术: Mô先进技术 biên先进技术; chê nang cach chê dinh tuân tói uu (optimizer) và hêt mét mét (loss function) dé dānh giú suý khác biét giūa dαu rα doán và gia tri thuc né.   
Huan luyen mo hinh: Mô先进技术 huan luyen bang cach dura du lieuu huan luyen vao mang va cap nhat cac trong so cua cac no-ron thong qua qua trinh lan truyen

nguc. Qua trinh nay duoc thuc hiên qua nhieu epoch (vong lap) d' tang do chinh xac cua mo hinh.

Danh giáo hinh: Sau khi huan luyen, mô hinh duoc dānh giáo trèn du lieu kiém tra dé dānh giáo hiêu suat và do chinh xacCEE mô hinh.   
- Du doán: Mô先进技术 huan luyén có the duoc sùr dung de du doan cac giá tri dαu ra cho du lièu mi.

Máng RNN có the duoc ap dung cho nhieu loai bai toan du doan du lieu tuan tu, nuh du doan chuoi thoi gian, dich may, nhân dang gionnéi va nhieu ting dung khac. Dac diem quan trong cua mang RNN la kha nang xu ly thong tin tuan tu va kha nang ghi nho thong tin tu cac thoi diem truc do, lam cho no phu hop vovi nhieu bai toan có tinh chat thoi giant.

Tham so cua mang RNN: Mang RNN có môt so tham so quan trong can duoc xac dinh và dièu chinh dê dat duoc hiêu suát tot. Môt so tham so quan trong bao gòm:

- So luong units: Day la so luong don vi (no-ron) trong layer RNN. So luong units anh huong dén kha nang mo hinh hoc va kha nang nam bát thong tin tu du liu tuan tu.   
- Hàm kích hoat (activation function): Hàm kích hoat duoc ap dung cho dαu rα cua mõi no-ron trong layer RNN. Trong doàn code, hàm sigmoid duoc su dung lam hàm kích hoat.   
- Kích thúoc dαu vào (input shape): Day la kích thúoc cua dür liêu dαu vào, thυòng duoc bièu diēn duroi dang (None, input_dimention), trong do input_dimension là so chieu cua dür liêu dαu vào.   
- Hàm mat mat (loss function): Day la hàm duoc sùr dung dé dānh giá sūkhac biét giura dàu ra dyr doán và giá tri thuc te. Trong doàn code, hàm mat mat mean squared error (MSE) duoc sùr dung.   
- Thuậnlando (optimizer): Thuậnlando uru duoc sutured g de dieu chinh caciastrong so cua mang. Trong doan code, thuan Adam duoc suturedung.

Máng RNN có kha näng mo sinh hóa cac quan hé phúc tap giūr cac thài diémtron du lieu tuàn tμ. Dieu nay cho phép no phù hop cho cac bai toán nuh du doánchuoi thoi giant, ngông uī tμ nhient, và xu ly dū liEU tuàn tμ khac.

Môt uru diém khac cua mang RNN là khà nang xu ly du lieu dau vao có do dai khac nhau. Trong doan code, input_shape dinc dāt là (None, 1), cho phép mang RNN xu ly du lieu dau vao có so luyng thoi diem khong xac dinh va chieu du lieu dau vao lá 1.

Batch size và só epoch: Trong qua trinh huán luyên, batch size (kíchthurc lô) và só epoch (só lan lap lái dür liêu huán luyên) duoc chi dinh. Batch size xac dinh só luong mău dür liêu duoc dura vao mang cungMZt luc, và só epoch xac dinh só lan lap lái viêc huan luyên trén toan bó dür liêu huan luyên.

Dú doán: Sau qua trinh huan luyén, mang RNN dā hoc duoc cac mau va quy luàttron dū liEU tuàn tμ. De duoán giá tri cho dū liEU moi, chung ta có the sù dung mohinh dā duoc huan luyén và dura dū liEU moi qua mang de nhân duoc dαu ra du doán.

Mang RNN làMZtrong nhng kien trucquan trong tong linh vuc hoc sau (deep learning) va xu ly dur lieu tuan t. No co the duoc ap dung cho nhieu bai toan nhur du doan chuoi thoi bian, xu ly ngon ngu tui nhien, dich may, nhan dang gionnéi va nhieu bai toan khac có tinh chat thoi bian.

# 3.1.2 LSTM

A. Mô tâ cau truc mang

Trong mang LSTM, cái truc cóa cac don ví LSTM cho phép chung ghi bó thóc tin tù而成 khú và dieu chinh thóc tin nay theo cach phù hop. Dièu nay guip máng LSTM xú ly duoc cóc chuǒi dür liêu dai và có mí qui quan hé phúc tap.

Dàu vào (Input): Dàu vào cái mäng LSTM làMZ chuoǐ duī lièu, thùng lá dür liêu chuǒi thòi bian. Môi phàn tuìrìng chuǒi duoc gài láMZtime step va có the chúaMZt hǎc nhièu dāc trung (features).

Layer LSTM (LSTM Layer): Layer LSTM là thanh phân chinhCEE mäng LSTM. Môi LSTM layer bao gômMZtap hop cac don vi LSTM (LSTM units) hoat dong doc

lasp. Moi don ví LSTM có khā näng luru trūvà truy cap vào thòng tin tú qua khú và dièu chinh thòng tin nay theo cach phù hop.

Dàu ra (Output): Dàu ra一则 mang LSTM thuong laMZ hoac nhieu don vi Dense (fully connected) dura ra cac du doan dua trén thong tin da hoc duoc tu du lieu chuoi dàu vao.

B. Giāi thích céc hê sô   
I. Lop Sequential()

Trong thu Vieto Keras la môt cach tiép can don giàn và phô bien dé xay dung cac mo hinh neural network tuân tú. Trong mô hinh tuân tú, cac lôp duoc xêp chong len nhau theo môt tuú ngc truyén qua tung lôp theo chieu tuân tú.

Uu diem chinh cua viêc sù dung lôp Sequential() là tinh don giàn và de sù dung. Chúng ta chi can them cac lôp vào mô hìn bàng cach sù dung phuong thuc add(), ma khòng can phài quan támden viêc két não cac lôp hay quán ly dαu vào/dàu ra cúa túng lôp.

Lóp Sequential() duoc sù dung dé xày dung mòt mang neural network tuàn tú vói cac lóp LSTM xép chòng lên nhau, lóp Dropout dé tranh overfitting, và lóp Dense dé táo ra dàu ra din doán.

Tuy nhiên, lôp Sequential() có mô t s o hàn ché. Môt trong s o do là khòng thec xay dung duoc cac mô先进技术 (branches)先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术;先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术；先进技术：

II. Phuong thuc add()

Phuong thuc add() duoc sù dung dé them)cac lóp vào mô先进技术 (Sequential().phuong thuc add() duoc sù dung dé them)cac lóp LSTM, Dropout va

Dense vào mô hinh. Céc tham sô)cua phuong thúc add() sé khac nhau tuy tuóc vào loài lôp ma chùng ta dang them vào.

add(LSTM) làMZ phuong thuc cua lop Sequential trong thu vien Keras déthemMZLSTM(Long Short-TermMemory)vao mo hinh.Doi so units xac dinhso luong don vi (no-ron)trong lop LSTM.Khi sur dung lop LSTM,chung ta co the tuychinh cac tham so khac nhur input_shape de xac dinh kich thuoc dau vao cua miaudulieu chuoi.Cungco the sur dung cac tham so khac nhur return_sequences de quyetdinh lieuu lop LSTM co tra vé chuoi két qua cho moi thoi diem hay chi két qua cuoi cung.Lop LSTM trong mo hinh duoc them vao thong qua phuong thuc add(),vacaclop LSTM co the duoc xep chong len nhau hoac két hop voci cac lop khac de tao thanhmot kién trunc mang phuc tap hon.

Lóp LSTM: Trong lóp LSTM, chung ta can chi dinh sô luong don vi (units) và cac tham so khac nhu input_shape, return_sequences (hoac return_state) và activation

Units: tham so units dé cap dén so luong don vi (no-ron) trong môt lôp cu the. chung ta can chi dinh so luong don vi trong lôp do bang tham so units. So luong don vi xac dinh kich thuoc cua dau ra tù lôp do và anh huong dén khà nang hoc và bieu dién cua mang neural network.

Trong lôp Dense, vú du nhu model.add(Dense(units=64)), units=64 xác dinh rǎng lôp nay sê có 64 don vi (no-ron). Mói don vi trong lôp Dense sê thuc hiênMZt phép tinh tuyén tinh tiên treadmill vào cua não, két hop vói he sotronso turing的能量, và ap dungMZt ham kich hoat dé tao ra dàu ra.

Só luǒng dōn vì (units) trong mòt lóng ành huǒng dén kha nǎng mô hinh hoc và bièu diēn cák mí quan hé phúc tāp giūa dür lièu dàu vào và dàu ra. Mòt só luǒng dōn vì lón hon có the ché cho phép mäng hoc và bièu diēn cák mǎu phúc tāp hon, nhung dòng thòi cūng lám tàng dophúc tāp và só luǒng tham só cúa mô hìn.

Viêc chôn sô luǒngdon vi phù hop lá mòt qua trinh thu và sai, có the dura trên kien thuc vè bài tiên cú the hǎoc thùng qua viêc thuc hiên cac phuong máp tói uu hóa

và dánh giá trên tâp huân luyên. Qua qua trinh huan luyên và dánh giá, chung ta có the dièu chinh sô luǒngdon vi dé tim ra giá tri tí tói uu cho mô先进技术 minh.

input_shape: tham so input_shape duoc su dung de chi dinh kich thuoc cua dau vao (input) cho môt lôp trong mô hinh.

Khi thêm môt lôp vào mô hìn sù dung phuong thúc add() trong lôp Sequential cúa thai viên Keras, chùng tathurong can xác dinh kích thúoc)cua dàu vào cho lôp do thóc qua tham so input_shape. Day la môt tuple hoac list chuá thùng tin vè kích thúc cúa dür liếu dàu vào.

Öday, input_shape tiên xác dinh bang cach sùr dung train_X.shape[1] và train_X.shape[2]. Day l'à th的增长 tin vè kíchthuroc cua dür lièu huan luyén train_X. Th的增长, train_X làMZt máng dα chieu cókichthuroc (só mau, sothose diem, so não truncg). Vi vay, train_X.shape[1] sé lásóthose diemstrong miia mau va train_X.shape[2] sé lásó dac trungstrong miiathose diem.

Thóc qua input_shape, mô hinh sê biét kích thúócCEE du dμi du vào vá có the xác dinh kích thúóc céc trçng so tzung cho cac lôp trong mang neural network. Dieu nay rát quan trçng de dam bao tinh nhát quán giūa kích thúóc cμa du dμi lieuu và cac phép tinh trçng mang.

Luu y rāng tham só input_shape chi can duoc xác dinh cho lóp dàu tiéntron mô hinh. Doi vói cac lóp tiêp theo, Keras sé tú dòng suy ra kích thuoc dàu vào dura trên két qua一则 lóp trucó do.

Tham so return_sequences: duoc sù dung trong cac lóp LSTM (Long Short-Term Memory) dé xác dinh lièu lóp do có tra vè chuǒi két qua cho cac thòi diém dàu ra hay khòng.

Môt mù先进技术 LSTM có the duoc sù dūng dé xu lý duì lièu theo chuǒi,tron do thóc tin tú céc thói diém trucoc do duoc truyen qua và luu trǔ先进技术 ],vú mòi don vi xu lý thóc tin tú mù先进技术 và truyen先进技术 tin sang cac don vi tiếp theo.

Trong mùng mùng先进技术 LSTM thóc thóc, chì lôp cuǒi cùng,cuà máng sě tra vè két qua cho mõi chuǒi dür liếu dàu ra, trong khi cac lôp LSTM trún c do thóc chi tra vè két qua cho thói diém cuǒi cùng. Dièu nay duoc thuc hiên bòi tham so return_sequences.

Khi return_sequences=True, lôp LSTM sê TRADE chuõi két qua cho miôi thói diémtronchuõi dαu vào. Dièu nay có yghia la lôp LSTM sê có dαu ra có kich thuoc (só mαu, sô thói diém, sô don vi). Thulong thi cac lôp LSTM trung gianttrongo hinh sê duoc dαt return_sequences=True, dé truyen tiếp thong tin cho cac lôp LSTM sau do.

Khi return_sequences=False (giá tri măc dinh), chi két qua cho thói diém cuói cùng strong chuõi dàu vào sê duoc tra vè. Dieu nay dān dén dàu ra có kích thuóc (sô mău, sô don vi). Thuòng thì lóp LSTM cuõi cungstrong máng sê khóng có tham sô return_sequences, déthu tháp th的增长 tin tùr toàn bó chuõi dàu vào và dura ra két qua turong Úng.

Quyét dinh sù dung return_sequences=True hay return_sequences=False phu chuóc vào bái tiên,cú thevà cach xuì lý duī liEU chuǒi. Trong môt só truong hop, cháng ta có themún laying thóng tin tù mōi thói diémtron chuǒi dàu vào,tron khi trong nhūng truòng hop khác, chi can két qua cuǒi cung là duì dé dura ra duì doán.

add(Dropout) làMZtphuongthuccualopSequentialtrongthurienKerasd'themMZtlopDropoutvao mo hinh.

Lóp Dropout làMZtphuong phap chinh quy hoa(regularization)trong mang neural network,duoc sur dung de ngau nhiên tát (bO qua)môt sô don vi (no-ron)trong qua trinh huan luyén. Dièu nay giúp hàn ché hiên tuǒng qua khóp (overfitting) và cai thieu khà näng tóng quát hoa cái mô,hinh.

Trong Keras, chung ta có the sür dung add(Dropout(rate)) déthemMZlop Dropout vao mo hinh. Dói so rate xac dinh ty lé (giūa 0 va 1) cac don vi sé bi tát. Ví

du: add(Dropout(rate=0.3)) sêthemMZtlop Dropout vovi tôlê 0.3,tuc la khoang $30\%$ don vi sê bi táttron qua trinh huan luyên.

Lóp Dropout duoc them vao sao môt lóp truc dostrong mo hinh, va cac don vi trong lóp truc do có khà nang bi tát strong qua trinh huàn luyén. Dieu nay giúp mang tro nên chóng chu vuovi nhieu vu khong cho phép cac don vi phu thuoc qua nhieu vao nhau.

Lop Dropout thuong duoc su dung sau cac lop có so luong don vi lón, nuh Dense hoac LSTM, dégium hiên tuong qua khop va cai thieu kha nang tóng quat hoa cua mo先进技术. Ngoài ra, lop Dropout cūng giup can bang giūa cac don vi va gim kha nang mô so don vi qua phu tuoc vào nhūng dǎc trung cu thetrong du liEU.

Lóp Dropout trong mäng neural network làMZt cóng cu hhu ich de tang tinh linh hoat va khà näng chóng chu cua mo hinh trong qua trinh huan luyen va du doán.

add(Dense) làMZtphuong thuccua lop Sequentialtrong thu vien Keras déthemMZt lOpDense vao mo hinh.

Lóp Dense lā môt lôpiday du két néi (fully connected layer) trong máng neural network. Nà lâ môt lôp mang neural co dién, trong do mõi no-ron trong lôp nay duoc két néi voci tát ca cac no-ron trong lôp trucó do (hoac dau vào néu não lâ lôp dau tiên).

Trong Keras, chung ta có the sür dung add(Dense(units)) déthem môt lôp Dense vào mô hinh. Doi so units xac dinh so luong don vi (no-ron) trong lôp Dense. Ví du: add(Dense(units=64)) séthem môt lôp Dense vói 64 don vi vào mô hinh.

Lóp Dense thuòng duoc sù dung lám lóp cuǒi cung trong mò hinh dé du doán két qua cuǒi cung. Dàu ra消费品 lóp Dense có thec kich thuoc khac nhau tuy thuoc vào bái toán cu the, ví du nhu mòs dón vi cho bái toán hòi quy (regression) hoac so don vi tuong ung vói so lóp phàn loài trong bái toán phàn loài (classification).

Lóp Dense trong mang neural network làMZt lôp quan trong dé hoc và Bièu dién cac quan hé phi tuyén tinh va phi tuyén tinh giūa dαu vào và dαu ra. Các trong só trong lôp Dense se duoc cap nhát trong qua trinh huàn luyén dé tim ra miǒi quan hé tot nhát giūa cac dac trung dαu vào và két qua du doán.

# III. Phuong thuc compile()

Trong mô先进技术 Sequential)cua thu viên Keras duoc sùr dung dé cau先进技术 quá trinh huân luyên,cua mô先进技术 trucoc khi nó duoc sùr dung dé fit vào dür liieu huan luyên.

Khi goi compile(), chung ta can chi dinh hai thong so chinh:

Hàm mát mát (loss function): Day la hàm duoc sùr dung de do luong muc do sai khac giūa gó tri du doán cúa mo hìn và gó tri thuc né trong qua trinh huàn luyên. Trong bái toán hòi quy (regression), mse (mean squared error - sai sô binh phuong trung bìn) làMZt luya chon pho bien de do luong mát mát. Trong bái toán phân loài (classification), chúng ta có the su dung categorical_crossentropy cho phân loài da lóp hoac binary_crossentropy cho phân loài nhi phân.

Thuếnt do不了 uru hóa (optimizer): Day la thai tao duoc sùdung dé tói uu hóa mô先进技术 trèn ham mét mat và cap nhát céc积累 cóu mang. Trong truong hop nay, adam (Adaptive Moment Estimation) làMZt thuyat doan tói uu hóa pho bien duoc su dung trong máng neural network. No két hop giūa hai phuong phap tói uu hóa khac nhau là Momentum và RMSProp dé dát duoc toc do hói tu nanh chóng và khā näng khám phá totrung khòng giant tham so.

# IV. Phuong thuc fit()

Trong mô先进技术 Sequential cua thu viên Keras duoc sùr dung de huan luyên mô先进技术 voci dū liEU huan luyên dā duoc chuǎn bi truc do. Nó thuc hiên qua trinh tói uru hóa mô先进技术 dū rân dū liEU huan luyên và cac tham só duoc chi dinh.

Duroiday la giaithich chi tiét vécac tham so cua phuong thuc fit():

- x: Dū liEU dau vào (features) dé huán luyén mô hinh. Day co the làmight numpy array, moins dans sachaç array hoacmight generator.   
- y: Nhan tuong ng voi du lieu huan luyen. Cung co the la môt numpy array,might dansh sách cac array hoac môt generator.   
- epochs: Só luǒng epoch (vòng láp) mà mù hìn sě duǒc huán luyén qua dū lièu huán luyén. Mòi epoch tróng úng vòi vièc sùr dung toàn bó dū lièu huán luyénmight bàn dé cap奋斗目标 trong só cua mù hìn.

- batch_size: Kích thúóc batch (nhém) mäu duóc sùr dung trong qua trinh huân luyên. Dū liEU huan luyên duóc chia thanh cac batch nhô và trong so)cua mo,hinh duóc cap nhát sau miOI batch.   
- validation_data: Dū liēu validation (kiěm tra) dé dānh giá hiēu suát)cua mô hinh trong qua trinh huán luyên. Có the làMZt tuple (x_val, y_val) hoacMZt generator.   
- verbose: Xác dinh múc do dαu ra thong tin trong qua trinh huán luyen. Có ba giá tri khā dung: 0 (khòng hiến thi thong tin), 1 (hiến thi thanh tiến trinh) và 2 (hiến thi môt dòng thong tin cho miǒi epoch).   
- shuffle: Cô chí dinh xem dür lièu huán luyén có duoc xuo trón (shuffle) trucó mõi epoch hay khòng.

Phuong thuc fit() se thuc hiên qua trinh tiên uru hoa mô先进技术 latch lap lái cac epoch và cac batch, cap責 trong sô dúa trèn ham mat mat và thuế tân tiên uru hoa dā do gc xác dinh trong compile(). Quá trinh huàn luyên sī tiếp tuc cho dén khi hoàn thân sô luong epoch duoc chi dinh hoac khi dièu kien dingh duoc dap ng.

# 3.2 Két qua

# 3.2.1 RNN

![](images/9c0a3c9eab660acc1cb30a5f2dc9cdb8637a0999ecdc275c8b1fa529343a654c.jpg)  
Hinh 3-3 bièu dô du doán mIRC tièu thu näng luong so vói thuc te

Ma trān dānh gia:

Test (Validation) RMSE = 519.6161644739401

MAPE: 1.3589821083403826 %

# 3.2.2 LSTM

# 1 lóp LSTM

![](images/cdc61dd01bdb2fc82f2e481191bf5b408f72d3e69cd809bffab68905accdad2c.jpg)

![](images/bbe9dc4f9981a5747b79d6bbdcb7fb6a00694d296846432273c5dac4f6d88277.jpg)  
Hinh 3-4 Bieu do qua trinh huan luyen mo sinh LSTM de du doan muc tieu thu dien nang   
Hinh 3-5 Bièu do két qua du doán và giá tri thuc té tren tup din lieuu kiém tra.

Ma trān dānh gia:

Test (Validation) RMSE = 70.40343

MAPE: 0.16513976734131575 %

![](images/53b50daa64e9279e4fdaa6b4082f63e8ccbdda5a5b43bc2ba58ff0dc20d1deb7.jpg)  
2 lóp LSTM

![](images/4fc530ed9d76f54031ebd45489b7308e2a5cefe31a83dfe17510012acdd71a2e.jpg)  
Hinh 3-6 Biêu do quando trinh huân luyên mô先进技术 LSTM de xu doan先进技术 duoc tiêu thy diên nǎng   
Hinh 3-7 Bièu do két qua du doán va giá tri thuc té tren tap din lieuu kiém tra

Ma trān dānh gia:

Test (Validation) RMSE = 88.007454

MAPE: 0.2064310247078538 %

![](images/3b20aed7bcca9bd397311a5322a6b325497c862fc129b22c4d357e324c8c5f7d.jpg)  
3 lóp LSTM

![](images/b580349bc59815cd8c2a80644cbf25fb918ef3bbcc41eaa89c3571902523aa00.jpg)  
Hinh 3-8 Bieu do qua trinh huan luyen mo sinh LSTM de du doan muc tieu thu dien nang   
Hinh 3-9 Bieu do két qua du doán và giá tri thuc té tren tup din lieuu kiém tra

Ma trān dānh gia:

Test (Validation) RMSE = 96.25525

MAPE: 0.20641274750232697 %

# 3.3 Giài thích y nghiaCEE bu biéu do

a) Bièu do qua trinh huàn luyén mô hinh LSTM dé du rdoan muc tiêu thu diên näng.

Trên biếu do, truc x biếu diēn sô luong epoch (vòng láp)tron qua trinh huán luyén,strong khi truc y biếu diēn giá tri cua ham loss (mean squared error) trèn tâp huan luyén và tâp kiém tra (validation).

Đύng mù xanh biếu diēn giá tri ham loss trên tân tân huán luyên (train), trong khi duong mù do biếu diēn giá tri ham loss trên tân tân kiếm tra (test). Múc tiếu cúa qua trinh huàn luyên là giam thiéu ham loss dé dát dinoc mo hìn du r doán tot trân ca tân luyên và tân kiếm tra.

Ban dαu, giα tri ham loss trén ca hai tup du rieu la cao. Khi qua trinh huan luyén dien ra, mo hinh duoc cap nhát dan dan de toi uru hoa du doan. Ket qua la giα tri ham loss giam dan theo so luong epoch.

Tuy nhiên, ta can quan tân dén ca giá tri ham loss trênp kiem tra. Nou giá triham loss trênp kiem tra tāng lentrong qua trinh huan luyen sau môt so epoch nhát dinh, có the cho thay mo hinh dang bi overfitting, tuc la qua tâp trung vao dür lièu huan luyen ma khong tóng quat hoá tot cho du lièu mi。

Dé dānh giá chat luǒng cúa mô hìn, ta có the so sánchez giūa giá tri ham loss trên tâp huán luyén và tâp kiém tra. Nou giá tri ham loss trên tâp kiém tra tháp va tiép tuc giam theo thòi bian, mô hìn duoc cho là có kha näng du doán tot trên din lièu moi.

Biéu do nay cung cap thong tin quan trong vè hiêu suat cua mô hinh LSTM trong qua trinh huàn luyén va giúp dānh già xem mô hinh có dát duoc muc tiêu du r doán tot hay không.

b) Bièu dô két qua du rdoán và giá tri thuc tê trênp du rliéu kiém tra.

Đύng mù xanh lá cay trèn bèu do bèu thi tiên tri thuc né cua múc tiêu tu diên nang trèn tân tiếm tra. Mõi diēm trèn duong mù xanh lá cay bèu diēn giá tri thuc né t aimight thoci diēm cu the.

Đυόng mùu xanh duong trèn biếu do biếu thi giú tri dür doán cuà múc tiếu tu diên náng trèn tânnap kiếm tra. Day la két qua duếc mù hinh LSTM dür doán. Duong mùu xanh duong cho tháy múc tiếu tu dür doán theo/thói giant.

Báng cach so sanh giūa duòng mùxanh lá cay và duòng mùxanh duong, ta có the dé nán gia hiēu suat cuà mo hìn trong vièc dür doán muc tiêu thú diên näng. Néu duòng mùxanh duong tiēm can hoac trùng vói duòng mùxanh lá cay, mù hìn duǒc cho lá có khā näng dür doán chín xánc muc tiêu thú diên näng trèn dür liêu kiém tra.

# 3.4 Dângh Giú mô先进技术

Báng 2 So sánchez chi só dánh giá   

<table><tr><td>Mà khác
Mô先进技术</td><td>MAPE</td><td>RMSE</td></tr><tr><td>RNN</td><td>1.3589%</td><td>519.6161</td></tr><tr><td>1 lôp LSTM</td><td>0.1651%</td><td>70.4034</td></tr><tr><td>2 lôp LSTM</td><td>0.2064%</td><td>88.0074</td></tr><tr><td>3 lôp LSTM</td><td>0.2064%</td><td>96.2552</td></tr></table>

Nhin vao cac chi sô RMSE và MAPE, mù hinh LSTM vói 1 lôp cho thay hiêu suát tot khác trong sô cac mô hinh dã liêt kê. RMSE cua mù hinh nay lá tháp ], chì lá 70.40343, cho tháy muc do chinh xac cao trong viêc dý báo. Tuong tú, MAPE cua mù hinh LSTM vói 1 lôp cūng lá tháp ], chì lá 0.16513976734131575%, dong ].ghia vói viêc sai sô dý báo tháp.

Trong khi do, mo sinh RNN co két qua voi RMSE la 519.6161644739401 va MAPE la 1.3589821083403826%. Dieu nay cho thay voi tap dinghiên

cúu nay, mí hinh RNN duý bó khòng hiếu qua bǎng mí hinh LSTM dā duǒc dé xuát òt rén.

Mô先进技术 LSTM vovie 2 lôp và 3 lôp cūng cho két qua tôt, nhung RMSE và MAPE cuá cháng cao hon so vovie LSTM vovie 1 lôp. Dièu nay có the chí rǎng viếc them cac lôp khòng mang lái cái dang kétronghiéu suát du bó.

Do do, dura trillion cac két qua cung cap, mô hinh tot nhát dé du bao trong truong hop nay là LSTM vovi 1 lôp.

# KÉT LUÁN VÀ HUÔNG PHÁT TRIÊN

Két qua dát duoc cúa dé tāi:

Xày dung mô sinh du báo: Dè tài dā thanh cóng trong viêc xày dung mò mô sinh du báo múc tiêu thu nang luong cho thanh phó Tetouan. Mô先进技术ung céc thóc so nhú nhiet do, do am, toc do gió và cac dong chay phân tan de du doán tiêu thu nang luong.

Nhomidān cóng trong viêc xay dung hai mo hinh du r bao s u dung mang no-ron tai phan tang (RNN) và Long Short-Term Memory (LSTM) dé du r bao muc tiêu thu nang luong cho thanh pho Tetouan. Mô先进技术 chung toci dduoc xay dung bang cach su dung cac thong so nhur nhiet do, do am, toc do gió va cac dong chay phan tan nhur dau vao dé du doán muc tiêu thu nang luong.

Phân tích yèu tí anh huǒng: Báo cáo dā phân tích su ānh huǒng cuà nhiet do, do àm và tóc do gio dén tiếu thú nǎng luǒng. Két qua phân tích nay giúp hieu rō hon vè tác dòng cuà yèu tí thói tiét dén nhú cùu nǎng luǒng cuà thânphó.

Dú doán tiếu thú khu vuc: Mô先进技术 duoc ap dung dé duoan muc tiếu thú nang luong cua tùng khu vuc trong Tetouan. Dieu nay cung cap thong tin quan trong cho Vietc lasp ké hoach va quán ly nang luong trong cac khu vuc cu the.

Han ché cua dé tai:

Dulieu dau vao: De tai gap kho khan trong viec thu thap va xu ly duu lieuu dai vao. Can dam bao rang dulieu ve tieu thu nang lurong cua tung khu vu cduoc thu thap day du va chinh xac de tang do chinh xac cua mo hinh du bao.

Yéu tó khác: Báo cao can xem xét cac yéu tó khác có the annihil hóng dén múc tiêu thú näng luǒng, nuh tinh sinh kinh tí, mat do dān só, cóng nghiép và thuong mai. Dieu nay giúp tāng do chính xac va toàn dièn cua mo hinh du báo.

Huóng phát trièn cuà dé tài:

Náng cao dó chính xác: Dé cai thieu dó chính xuacuá du bao, có thecánghién cuu va su dung cac phuong phap phan tich du lieu tién vamô hinh hóa phuc tap

hon. Dong thoi, can tiep tuc cai thien qua trinh thu thap va xu ly du lieu du vao de dam bao tinh day du va chinh xac cua du lieu.

Dura vao yéu to ben vūng: De tai có the mo róng de xem xét cac yéu to ben vūng khac nhur sū dung näng luǒng tái táo, hiêu qua näng luǒng va quan ly thai ra mi o truong. Dieu nay giúp táo ra mô mô hinh du bǎo näng luǒng toàn diên hon, khòng chi tâp trung vao tiêu thú näng luǒng ma con vao viêc xem xét ca cac khía canh ben vūng va mi o truong.

Phân tich tac dong xã hoi va kinh né: De tai có theghiên cuu tac dong cua tiêu thú nang luong dén cac khia canh xã hoi va kinh né cua thanh pho. Viếc dānh giá chi phi, tac dong miо truong va tac dong xã hoi cua viếc tiêu thú nang luong sī giú p dura ra cac giai phap va chinh sacb hiêu qua dé qu'an ly va giám thiéu tac dong tiêu cync.

Áp dung cóng nghe mi: Nghiên cuu có thet lap trung vao Vietc ap dung cóng nghe mi nhut rui nhanh tao, hoc may va mang luoi dien thong minh de cai thien mo hinh du bao va quan ly nang luong. Vietc sur dung cac cóng nghe tien tien nay co the giup tang cuong kha nang du bao, toi uru hoa Vietc sur dung nang luong va tao ra cac giai phap thong minh hon trong quan ly nang luong.

Tóng két:

Tóng két, dé tāi du r bó muc ti éu thu näng luóng cua mòt thanh phó dā dat duoc két qua khá quan trong viec xay dung mo hinh du r bó và phân tích xu huóng ti éu thu. Tuy nhiên, can tiép tuc nghiên cúu và phat trién dé cai thiên do chín xác và tinh úng dung cúa dé tāi, bao gòm náng cao do chính xác, tích hop yéu tó bèn vūng, phân tích tác dòng xǎ hói và kinh né, và ap dung cóng nghé mí.

# Tài lieuu tham khào

1. tetuan-city-power-consumption [Internet]. [cited 16 Tháng Nám 2023]. Available at: https://www.kaggle.com/datasets/gmkeshav/tetuan-city-power-consumption   
2. Kidder GW, Montgomery CW. Oxygenation of frog gastric mucosa in vitro. Am J Physiol. Thang Chap 1975;229(6):1510-3.   
3. rmse-python [Internet]. [cited 19 Tháng Nám 2023]. Available at: https://www.statology.org/rmse-python/   
4. mapepython [Internet]. [cited 19 Tháng Nám 2023]. Available at: https://www.statology.org/mapepython/   
5. 13853244_Long_Short-term_Memory [Internet]. [cited 24 Tháng Nám 2023]. Available at: https://www.researchgate.net/publication/13853244_Long_Short-term_Memory

# Bridging the Gap: A Decade Review of Time-Series Clustering Methods

JOHN PAPARRIZOS, The Ohio State University, USA

FAN YANG, The Ohio State University, USA

HAOJUN LI, The Ohio State University, USA

Time series, as one of the most fundamental representations of sequential data, has been extensively studied across diverse disciplines, including computer science, biology, geology, astronomy, and environmental sciences. The advent of advanced sensing, storage, and networking technologies has resulted in high-dimensional time-series data, however, posing significant challenges for analyzing latent structures over extended temporal scales. Time-series clustering, an established unsupervised learning strategy that groups similar time series together, helps unveil hidden patterns in these complex datasets. In this survey, we trace the evolution of time-series clustering methods from classical approaches to recent advances in neural networks. While previous surveys have focused on specific methodological categories, we bridge the gap between traditional clustering methods and emerging deep learning-based algorithms, presenting a comprehensive, unified taxonomy for this research area. This survey highlights key developments and provides insights to guide future research in time-series clustering.

Additional Key Words and Phrases: Time-series data; Clustering; Data mining; Representation

# ACM Reference Format:

John Paparrizos, Fan Yang, and Haojun Li. 2018. Bridging the Gap: A Decade Review of Time-Series Clustering Methods. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym 'XX). ACM, New York, NY, USA, 52 pages. https://doi.org/XXXXXXXXX.XXXXXXX

# 1 Introduction

Time series, an ordered sequence of real-valued data, has been widely acknowledged as one of the most basic data formats. With the development of technologies in sensing, storage, networking, and data mining, massive raw data could be obtained, stored, and processed on the fly [174, 181]. Due to the advantage of chronological representation, we could see the application of time series in almost every scientific field or industry [74, 102, 126, 140, 151, 166, 185, 186, 249], including but not limited to: Environment [99, 123, 216], Biology [220, 239, 268], Finance [4, 64, 79, 217], Psychology [129], Artificial Intelligence [227]. However, in the information era, increasing data sizes that contain thousands or even millions of dimensions have become increasingly common. This has introduced a new layer of complexity, requiring efficient adaptive solutions [53, 104, 105, 147, 148] and presenting challenges in analyzing the underlying relationships between time series in various tasks, including indexing [54, 176, 182, 183, 187, 212], anomaly detection [31-36, 149, 150, 175, 180, 222], clustering [18, 178, 179, 184], classification [177, 211], and forecasting [188].

Clustering has been one of the earliest concepts developed in the field of unsupervised machine learning, which is shown to be one of the most efficient tools to help unveil the latent structure from the raw data. The main goal of

Authors' Contact Information: John Paparrizos, The Ohio State University, Columbus, USA, paparrizos.1@osu.edu; Fan Yang, The Ohio State University, Columbus, USA, yang.7007@osu.edu; Haojun Li, The Ohio State University, Columbus, USA, li.14118@osu.edu.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.

Manuscript submitted to ACM

i. Examples of time-series clustering across various domains (left: ECG, right: StarLightCurves).

![](images/b1700989c0c22ef438e8dd9f433ceafa77c7c044ba379018f978f7a163118429.jpg)

![](images/0fde2fa5105bfe3428e7eb5c38942e0ed2a35676273454b8140efbd1fa96fe23.jpg)

ii. Examples of time-series clustering for various applications (left: anomaly detection, right: indexing).

![](images/1b6ede200ab707a669ea528444aaadfdbbbdd977c00e1b6b147630f6f3dcc96b.jpg)

![](images/14bdda4645c8f0181f0fb464cc57e6be3503f2ccd51b93e6b6ad38f55efcd8a9.jpg)  
Fig. 1. Examples of time-series clustering across different domains and applications.

clustering is to find a partition of various given objects, in which the similarity within each group is maximized, and minimized between groups. In other words, a group of "similar" data samples is viewed as a cluster. Consequently, the characteristics within each cluster of objects consist of the pattern in the dataset, i.e., common features shared within the cluster. For example, in computer vision tasks, the pattern can be a certain type of edge or color for similar objects [153], while in the time-series domain, the position of rapid growth or decline across time steps may have practical meaning. On the one hand, data samples sharing the same pattern would have a small distance and thus be partitioned into the same cluster. On the other hand, a good clustering strategy would in turn facilitate the search for these representative features. Figure 1 depicts the examples of time-series clustering across different domains and applications.

As one of the most well-known clustering methods, the k-Means algorithm [161] provides an expectation-maximization (EM) based strategy to search the medoids and clustering assignments based on Euclidean Distance for each iteration. Its good performance has enabled the application in different fields such as electrical engineering, computer science, biology, and finance. However, traditional methods such as k-Means have suffered from significant performance degradation in the time-series domain. Traditional measurements in Euclidean space have proven inefficient for addressing the variety of distortions in time-series data, including shifting, scaling, and occlusion [178], which are common in real-world scenarios. To solve this problem, many methods have been proposed, which can offer invariances to the inherent distortions and robustness to noise or outliers. For example, Dynamic Time Warping (DTW) [46] proposes an elastic

Manuscript submitted to ACM

measure to deal with the many-to-many alignment issue and finds the optimal warping path that minimizes the total distance between 2 sequences. In order to further reduce the computation cost, k-Shape [178] introduces (i) shape-based distance (SBD) with time complexity of $O(n \log(n))$ and (ii) a novel centroid computation derived from an optimization problem, which achieves a significant improvement in clustering tasks [178, 184]. In recent years, numerous clustering methods have emerged from the deep learning era, capturing significant attention and interest. Various unsupervised learning strategies, like contrastive learning, enable neural networks to generate representative features in a reduced dimension space, which significantly alleviates the pressure of the downstream tasks such as dissimilarity measure and centroid computation. Leveraging parallel computing strategies and advanced GPU resources, a clustering model can be trained and deployed in significantly less time.

Past reviews [3, 143] have explored time-series clustering algorithms proposed in decades and provided insights from various perspectives of views, i.e., data representation, dissimilarity measure, clustering methods, and evaluation metrics. However, many survey papers, as mentioned above, either only discuss the conventional time-series clustering before the deep learning era [3, 143] or mainly focus on end-to-end deep representation learning [130]. [7] reviews the conventional time-series clustering works and prior deep clustering methods. However, it mainly focuses on the case study in the context of biological time-series clustering without a comprehensive study on both sides as mentioned. To the best of our knowledge, this work is the first attempt to build a bridge between the conventional time-series clustering methods and the deep learning-based models, providing a novel and comprehensive taxonomy for various time-series clustering in each category. We anticipate that this work will provide valuable insights for next-generation clustering algorithm designs.

# 2 Time-series Clustering Overview

In this section, we first introduce the definition of time-series data, and the difference between univariate and multivariate time series. Then we present the problem formulation of time-series clustering and the general pipeline of the clustering process, which motivates the newly proposed taxonomy in the following section.

# 2.1 On the Definition of Time-series Data

Time-series data can be categorized based on their major characteristics. From the perspective of dimensionality, they can be classified into three main types: univariate, multivariate, and tensor fields. Additionally, depending on the sampling strategies employed during data acquisition, time-series data can be either regular or irregular. In the following sections, we will explore each of these categories in detail. The formal definition of time-series data is provided below.

Definition 2.1 (Time series). A time series $x_{i}$ is defined as a sequence of observations with $T > 1$ time steps $x_{i} = \{x_{i1}, x_{i2}, \ldots, x_{iT}\}$ , where $x_{it} \in \mathbb{R}^{d}$ . Based on the dimension $d$ , each observation $x_{it}$ at time $t$ can be a real-valued number $(d = 1)$ or a vector $(d > 1)$ .

Definition 2.2 (Subsequence). Given a time series $x_{i} = \{x_{i1},x_{i2},\ldots ,x_{iT}\}$ , where $x_{it}\in \mathbb{R}^d$ , a subsequence $C_i$ is defined as a sequence of consecutive time steps obtained from $x_{i}$ : $C_i = \{x_{im},x_{i(m + 1)},\dots ,x_{i(m + L - 1)}\}$ , where $L$ is the length of the subsequence, $1\leq m\leq T + 1 - L$ .

Definition 2.3 (Sliding Windows). Given a Time series $x_{i} = \{x_{i1},x_{i2},\dots,x_{iT}\}$ , where $x_{it}\in \mathbb{R}^d$ , sliding windows are defined as a set of subsequences extracted by sliding a "window" of length $L$ across the current time series $x_{i}$ , with a stride size $s$ ( $1\leq L\leq T - 1$ and $1\leq s\leq T - L$ ). The size of the sliding windows matrix is $\left(\lfloor \frac{T - L}{s}\rfloor +1,L\right)$ .

![](images/7baa7788c00490225cc6c157d4d68884aff66fa19a2b5a3bb31d100739ee7126.jpg)  
Fig. 2. Examples of univariate time series (left) and multivariate time series (right). Compared with univariate time-series clustering, multivariate time-series clustering needs to consider time steps from all channels.

2.1.1 Univariate versus Multivariate. From the discussion above, we could clearly categorize the univariate and multivariate time series based on the dimension of each observation across time steps. Univariate time series (UTS) represents an ordered sequence of real-valued data in one dimension ( $d = 1$ ), e.g., ECG200 in UCR dataset [40] consists of 200 ECG recordings of a single patient, each indicating the changes of electric activity during one heartbeat. Compared with univariate time series, the multivariate time series (MTS) contains observations of more than one dimension ( $d > 1$ ). For example, PenDigits in UEA dataset [14] records the movement of both $x$ and $y$ coordinates when writers draw digits between 0 and 9. Compared with UTS, Methods designed for MTS need to address the dependencies across multiple channels, which adds complexity and poses additional challenges to the clustering task in MTS scenarios.

Figure 2 presents examples of clustering for both univariate and multivariate time series, highlighting the importance of considering time steps from all channels in the development of algorithms for multivariate time-series clustering. The left side of this figure illustrates that we can directly categorize two univariate time series into two distinct clusters by observing the difference in the number of peaks between them. On top of that, the right side of the figure shows that clustering two multivariate time series by using the rule that only considers a single channel, a univariate time series, is inadequate. Rather, a MTS clustering algorithm should incorporate the impact of all channels to ensure accuracy.

# 2.2 On the Definition of Time-series Clustering

Clustering, as one of the earliest concepts in the machine learning field, has been widely applied in the time-series domain. The overall goal of clustering is to find a solution to group different data samples in a way that, the distances, or the dissimilarity measurements, within each group are minimized, and the distances between each group are maximized. This clustering procedure not only finds a special partition way for a whole dataset, but also provides valuable insights for understanding the latent structure of the data and strongly facilitates downstream tasks such as time-series classification, segmentation, anomaly detection, etc. The detailed definition is provided below.

Definition 2.4. [3] Given a dataset of $N$ time-series data $\mathcal{X} = \{x_1, x_2, \dots, x_N\}$ , where $x_i \in \mathbb{R}^{d \times T}$ , the process of time-series clustering is to partition $\mathcal{X}$ in to $K$ clusters $C = \{c_1, c_2, \dots, c_K\}$ , $K < N$ . In general, homogenous time-series data that share similar characteristics are grouped together based on a pre-defined dissimilarity measure.

Manuscript submitted to ACM

![](images/d53b08db4c217ef8d7b861c4be4f69295f0da1e7990e4d61f884d39169ae5707.jpg)  
Fig. 3. Categories of time-series clustering labeling. Left: whole time-series clustering; right: subsequence time-series clustering.

Based on the scope, time-series clustering methods can be categorized into 3 main types: whole time-series clustering, subsequence clustering, time point clustering [3].

- Whole Time-series Clustering: Given a set of time series, individual time series are clustered into distinct groups. In the process, all timesteps will participate in the dissimilarity measure to decide the intra and interrelationship across instances.   
- Subsequence Clustering: Given a time series, subsequence clustering is defined as the clustering procedure on a set of subsequences obtained by sliding windows.   
- Time Point Clustering: Given a time series, time point clustering partitions all time points into several groups based on temporal proximity and the similarity in values.

However, not all of the three types of time-series clustering are meaningful following the discussion in [3]. As illustrated in [3], time point clustering and time-series segmentation exhibit minimal differences. The main difference lies in the fact that some time points may be omitted as noise in the clustering process. According to the prior studies in [115], subsequence clustering may yield random results in the experiments. To create meaningful clusters, the process requires specific conditions that are hard to meet in the real world [115]. The difference between whole time-series clustering and subsequence time-series clustering is shown in Figure 3. As a result, we will focus on the whole time-series clustering in this survey.

We introduce two main types of the whole time-series clustering, identified in Figure 4 based on the contribution of each time step. Figure 4(a) presents the conventional clustering process that treats all time-series steps equally, while Figure 4(b) depicts subsequence-based whole time-series clustering which employs a set of subsequences as guidance. Importantly, the latter type differs from what's known as subsequence clustering in that it still produces the clustering labels for the whole time series in the end.

![](images/bce3fb06f13f665c9e974c9a01340567dd651aa3e00e2331e8d984cb101261f2.jpg)  
Fig. 4. Two main types of whole time-series clustering methods. Left: clustering methods based on entire time series; right: clustering methods based on subsequences.

# 2.3 Time-series Clustering Pipeline

Before delving into any specific clustering algorithms, it is important to know the general process of time-series clustering. Building upon the insights presented in prior studies [3, 7, 143], we summarize the common pipeline of time-series clustering into three parts (shown in Figure 5): representation, dissimilarity measure and clustering procedure. This decomposition will become beneficial not only for the comparative evaluation of diverse time-series clustering algorithms, but also help uncover the essence of time-series clustering and develop novel algorithms.

2.3.1 Representation. The representation procedure, also named as data representation or feature extraction, denotes the data format of time series for facilitating downstream tasks. Raw time-series data is the basic format with rich feature information from natural signals, which is widely applied in different clustering algorithms. However, the noise interference from the signal recording poses a challenge in the search of meaningful patterns, and also the large dimension from the original space also significantly increases the time and cost for data analysis. Therefore, an effective approach to extracting meaningful features while retaining the essential information becomes beneficial. The definition of the time-series representation can be found in Definition 2.5.

Definition 2.5. Given a time-series data $x_{i} = \{x_{i1},x_{i2},\dots,x_{iT}\}$ with $T$ time steps, we want to find a transformation $\phi :x_i\to x_i'$ , where $x_{i}^{\prime} = \{x_{i1}^{\prime},x_{i2}^{\prime},\dots,x_{iK}^{\prime}\}$ denotes a new representation, specifically $K < T$ in the dimension reduction scenario. The transformation space should retain the essential information from the original space in such a way that, if $x_{i}$ is similar to $y_{i}$ , then $x_{i}^{\prime}$ is similar to $y_{i}^{\prime}$ , and vice versa.

As can be seen from Figure 5, there are two main types of time-series representation: numeric and symbolic. The numeric time-series representation utilizes real-valued array (univariate) or matrix (multivariate) to denote the feature information in original signals, usually with reduced dimensions. Representative techniques include Discrete Fourier Transform (DFT) [6, 60, 113], Discrete Wavelet Transform (DWT) [6, 37, 113], Piecewise Linear Approximation (PLA) [117], Piecewise Aggregate Approximation (PAA) [118, 255], or neural networks after the emergence of deep learning [29, 71, 83, 162, 256, 258]. Symbolic time-series representation [144, 164, 208], on the other hand, offers the benefit from both dimensionality reduction and the wealth of text-based methodology, e.g. hashing, and sequence matching. Representative methods are Symbolic Aggregate approXimation (SAX) [116, 145], indexable Symbolic Aggregate Manuscript submitted to ACM

![](images/7db503ce223bee6bdeaa67c8e6625e0467f09e42dcfefb7786a7ffbbda4d9309.jpg)  
Fig. 5. The overview of the time-series clustering pipeline.

approximation (iSAX) [145], Symbolic Fourier Approximation (SFA) [208], and 1d-SAX [164]. As depicted in Figure 5, the input of symbolic representation could be either raw time-series data or a transformed representation.

2.3.2 Dissimilarity Measure. As shown in Definition 2.4, the primary objective of time-series clustering lies in the process of partitioning the data in a way that time series exhibiting the same pattern should be grouped together. To solve this problem in a mathematical way, the dissimilarity measure, also called distance measure, is proposed to quantify the proximity or separation relationship between two sequences (could be raw data or transformed representation as shown in Figure 5). Based on the definition, two identical time series should have a zero dissimilarity measure and a sufficiently large value when they belong to different clusters. With different designs, the dissimilarity measure can capture the similarity information in time, shape, and structure [3]. Generally, there are three major types of dissimilarity measures: lock-step, elastic, and sliding measure. An overview of each is depicted in Figure 5.

These three types of dissimilarity measures all have their own pros and cons. On the one hand, the one-to-one mapping assumption of the lock-step measure simplifies the comparison between different time series with much less time complexity, i.e., close to $O(n)$ , where $n$ is the sequence length. However, on the other hand, dissimilarity measures like Euclidean Distance impose limitations on varying lengths of time-series data, and the one-to-one mapping may suffer from the noise interference inherent in natural signals. On the contrary, the elastic measures provide a flexible alignment in different regions which is robust to various kinds of disturbances and achieves great success in diverse time-series analysis tasks. However, according to the discussion in prior works [182], a majority of elastic measures do not perform significantly better than sliding measures on the benchmark while there exists a nonnegligible gap between the time consumption. In other words, sliding measures may provide a good trade-off between runtime and accuracy in comparison to lock-step or elastic measures in different cases.

- Lock-step Measure focuses on one-to-one mapping for two whole time series. The final result is usually calculated by the summation or mean value of errors across each time point. The representative methods are Euclidean

Distance (ED) [60, 73], Minkowski (i.e., $L_{p}$ -norm) [19, 73, 182], Lorentzian (i.e., the natural logarithm of $L_{1}$ ) [73, 182], Manhattan [73, 182], Jaccard [73, 182].

- Elastic Measure has been frequently utilized in scenarios when the one-to-one mapping assumption does not hold firmly. Due to the noise interference and the nature of signals, two time-series data samples may be similar but exhibit different distortion in amplitude (scaling) and offset (translation), where the lock-step measures are likely to fail or suffer from performance degradation. To solve this problem, elastic measure methods are proposed to create one-to-many/many-to-one mapping in an "elastic" way, which can provide a flexible alignment across time points in various regions [182]. Representative elastic measures are dynamic time warping [25, 46, 198, 207], the Longest Common Subsequence (LCSS) [10, 233], Move-split-merge (MSM) [89, 215].   
- Sliding Measure is another type of dissimilarity measure, which follows a sliding mechanism to create a global alignment for different sequences. Representative sliding measures include variants of Normalized Cross-Correlation (NCC), such as Shape-based distance (SBD) [178]. Thanks to the advantage of Fast Fourier Transform (FFT), the cost can be reduced to $O(n \log n)$ , which is a significant speed improvement compared to the original version of DTW (time complexity of $O(n^{2})$ ). [182],

# 3 Time-series Clustering Taxonomy

In this section, we describe our proposed taxonomy of the time-series clustering algorithms including both traditional and deep learning-based strategies. All methods are divided into 4 categories: (i) Distance-based, (ii) Distribution-based, (iii) Subsequence-based, and (iv) Representation-learning-based. Figure 6 illustrates our proposed taxonomy with a sketch for each category. Next, we review the definition of each category in the following subsections.

![](images/11ac1256ca2baf90dc3133849a606d814be90ae384e53582479b85f4dd7c731c.jpg)  
Fig. 6. The taxonomy of time-series clustering algorithms.

# 3.1 Distance-based

As can be seen from the name itself, the crucial concept behind Distance-based methods is the way to measure the distance $Dist$ between two raw time series $X_{A}$ and $X_{B}$ , where $Dist(X_{A},X_{B}) = 0$ if the two time series are the same. Euclidean Distance (ED), as one of the most widely used distance measures in a variety of data formats, has achieved great success in many research fields. However, it is noticed that, for its lock-step design, this efficient distance measure usually suffers from the variance issue inherent in time-series data, e.g., scaling variance, shift variance, occlusion Manuscript submitted to ACM

variance, etc. To deal with this problem, many distance measures tailored for time-series data have been proposed, such as Dynamic time warping (DTW) and Shape-based Distance (SBD).

Given the distance between each pair of time series, one can tell that $X_{A}$ and $X_{B}$ should be put together if $\text{Dist}(X_{A}, X_{B})$ is small and vice versa. To make a proper decision for each time series, there come methods under two second-level categories: partitional and hierarchical models. More detail will be discussed in Section 4.

- The partitional clustering algorithm focuses on partitioning $N$ unlabeled time series into $K$ clusters with centroids. Each time series has the smallest distance from the centroid of the current cluster. The cluster centroids and assignments are usually optimized iteratively through the training process.   
- The hierarchical clustering is an approach of grouping objects into clusters through a hierarchy of clusters. Depending on the hierarchy structure, methods can be divided into two types: agglomerative and divisive. In the process, all time series will keep merging (agglomerative) or get separated apart (divisive).

# 3.2 Distribution-based

Distribution-based clustering focuses modeling the distribution of the time-series data, which offers guidance in building the boundary between each cluster. It is worth noting that, the distribution here should be considered a general version, including both (i) explicit distribution, e.g., the density of data points; and (ii) implicit distribution, e.g., encoding the time series using a pre-trained dictionary. Here we name four second-level categories: Model-based, Density-based, Feature-based, and Encoding-based models. More detail will be discussed in Section 5.

- The Model-based clustering methods focus on modeling the explicit distribution of time-series data with learnable parameters, such as the Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM). Each time series can be modeled and represented by a set of parameters, which could serve as guidance for further time-series clustering.   
- The Density-based clustering methods define clusters as regions of concentrated objects, with cluster boundaries typically occurring in sparse or empty areas. For example, these methods expand clusters in dense neighborhoods and establish boundaries where data points become sparse.   
- The Feature-based time-series clustering methods are proposed to find descriptive features to represent the characteristics of time series in a global way. Considering the inherent variance issue in time-series data modality, noise interference could pose a challenge in the distance measure between samples. To solve this problem, descriptive features could serve as a noise-robust representation for the clustering purpose.   
- The Encoding-based time-series clustering methods focus on building a mapping function $\mathcal{F}: \mathcal{X} \to \mathcal{Z}$ between the original space and the transformed space (also called the latent space), where the latent representation $\mathcal{Z}$ contains the essential feature information of the original data. When the dimension of the transformed space is smaller than the original space, this process is often viewed as a dimension reduction technique.

# 3.3 Subsequence-based

Subsequence-based clustering is a special case in the whole time-series clustering categories as defined in Section 2.2. In the process, representative subsequences will be extracted from the entire time series as an informative pattern for clustering purposes. As the noise perturbation sometimes leads to performance degradation when considering all time steps, by selecting descriptive subsequences models may circumvent this issue in different cases. There are two major second-level categories: Sliding-based and Shapelet-based models, which will be discussed in Section 6.

- The Sliding-based methods will obtain a set of subsequences using sliding windows. In some cases, these subsequences can be seen as segments of the raw time series to measure the distance from a lower level, while other models focus on multi-stage clustering using subsequence information as a prior.   
- The Shapelet-based methods, on the other hand, put attention to subsequences that could function as meaningful patterns to represent the raw time series, which are also called shapelets [253]. Depending on the objective, prior studies either iteratively search for special subsequences or directly learn shapelets from the dataset.

# 3.4 Representation-learning-based

Similar to the aforementioned categories like feature-based or encoding-based models, the Representation-learning-based methods also focus on the design of a new representation for the original time-series data. However, they both need explicit mathematical formulas to calculate the numeric value, while the representation-learning-based methods obtain the representation through a learning process. The new representation can serve as an input to some simple clustering models such as k-Means [161]. With the advent of deep neural networks and unsupervised learning strategies, representation-learning-based algorithms have achieved great success in numerous tasks. Depending on the characteristics of learning strategies, we divide three second-level categories: Comparative-based, and Generative-based models. More detail will be discussed in Section 7.

- The Comparative-based time-series clustering methods learn an encoding mapping function $\mathcal{E}:\mathcal{X}\to \mathcal{Z}$ in a comparative way, e.g., comparing similar/dissimilar time-series samples. The encoder mapping function can be learned by a neural network, which is going to be discussed in detail in the following sections.   
- The Generative-based time-series clustering methods, in contrast to comparative-based clustering methods, learn the robust representation by casting constraints on the generation output. One good example is the reconstruction task: by jointly learning the encoding mapping function $\mathcal{E}:X\to \mathcal{Z}$ and the decoding mapping function $\mathcal{D}:\mathcal{Z}\rightarrow X$ in a reconstruction way, deep neural networks are able to find a good latent space for data representation with possible fewer dimensions.

# 4 Distance-based Methods

In this section, we explore distance-based clustering methods that leverage and solely depend on measures of dissimilarity, either between data, between clusters, or a combination of both, to group similar elements together. The methods within this category typically do not require specific data representation or feature selection. They operate effectively by utilizing raw time series alone to generate meaningful outputs. Furthermore, this categorization can be further divided into two sub-categories: partitional methods and hierarchical methods. The partitional methods divide a set of $n$ unlabeled time-series data into $k$ clusters and ensure that each cluster comprises at least one time-series data while the hierarchical algorithms group data through establishing a hierarchical structure. In the subsequent sections, we will delve into a more detailed exploration of the concepts and representative clustering methods of these two sub-categories in Section 4.1 and 4.2.

# 4.1 Partitional Clustering

The partitional clustering algorithm partitions $n$ unlabeled time-series data into $k$ clusters and guarantees each cluster contains at least one time-series data. Moreover, partitional clustering methods can be classified into two main categories as illustrated in Figure 7: crisp partitional methods and fuzzy partitional methods. In crisp partitional methods, each data point is exclusively assigned to a single cluster. In contrast, fuzzy partitional methods associate each data with Manuscript submitted to ACM

membership likelihoods and permit each data to belong to multiple clusters simultaneously. In the following sections, we will discuss representative methods belonging to these two categories separately in more detail and related methods can be found in Table 1.

# Crisp partitional methods

One of the most widely-used partitional clustering algorithms is k-Means [161]. Initially, given the number of centroids $k$ , k-Means randomly selects $k$ time-series data as its initial centroids. Then, k-Means repeatedly uses Euclidean Distance (ED) to compute distances between each object and all centroids, assigns each object to one cluster whose centroid is the closest to that object, and updates each centroid to the mean of objects in that cluster until one of the pre-defined criteria is meet. However, since k-Means is sensitive to the initialization of centroids, [13] proposed k-Means++ to improve the performance of k-Means by defining centroids initialization rules which iteratively adding the new centroid based on the probability proportional to objects' distances from their corresponding the nearest previously selected centroids and aims at separating initial centroids from each other as far as possible.

A variation of k-Means clustering is named k-Medians [100]. Compared with the iteration described for k-Means, unlike k-Means, instead of using ED as a distance measure and com

Table 1. Summary of the Partitional clustering methods.   

<table><tr><td>Method Name</td><td>Distance Measure</td><td>Prototype</td><td>Dim</td></tr><tr><td>k-Means [161]</td><td>ED</td><td>k-Means</td><td>M</td></tr><tr><td>k-Medoids [199]</td><td>*</td><td>k-Medoids</td><td>I</td></tr><tr><td>k-Medians [100]</td><td>Manhattan</td><td>k-Medians</td><td>I</td></tr><tr><td>M-RC [123]</td><td>ED</td><td>k-Medoids</td><td>M</td></tr><tr><td>CB-FCM [75]</td><td>CC-based</td><td>FCM</td><td>I</td></tr><tr><td>NTSA-TC [192]</td><td>ED</td><td>FCM</td><td>I</td></tr><tr><td>M-k-Medoids [142]</td><td>DTW</td><td>k-Medoids</td><td>I</td></tr><tr><td>FSTS [170]</td><td>STS distance</td><td>FCM</td><td>I</td></tr><tr><td>k-DBA [190]</td><td>DTW</td><td>k-DBA</td><td>I</td></tr><tr><td>K-SC [250]</td><td>STI distance</td><td>K-SC</td><td>I</td></tr><tr><td>DFC [103]</td><td>ED</td><td>FCM</td><td>I</td></tr><tr><td>DKM-S [209]</td><td>Arbitrary</td><td>k-Median</td><td>M</td></tr><tr><td>k-Shape [178]</td><td>SBD</td><td>k-Shape</td><td>I</td></tr><tr><td>k-MS [179]</td><td>SBD</td><td>k-MS</td><td>I</td></tr><tr><td>m-kAVG [173]</td><td>m-ED</td><td>k-Means</td><td>M</td></tr><tr><td>m-kDBA [173]</td><td>m-DTW</td><td>k-DBA</td><td>M</td></tr><tr><td>m-kSC [173]</td><td>m-STI</td><td>K-SC</td><td>M</td></tr><tr><td>m-kShape [173]</td><td>m-SBD</td><td>k-Shape</td><td>M</td></tr></table>

I: Univariate; M: Multivariate; $*$ : Arbitrary;

puting the mean for each cluster as its centroid, k-Medians applies Manhattan distance as its distance measure and calculates the median.

Another important clustering algorithm that is worth mentioning is k-Medoids [112] whose objective is also to minimize the distance between the data point assigned to a cluster and the centroid of that cluster. However, different from k-Means, k-Medoids can use arbitrary distance measures and it uses actual and the most representative data points named medoids as centroids. In other words, the medoid is an actual data point within a cluster, which minimizes the average distance to all other points in that same cluster. Partitioning Around Medoids (PAM) [112] is one of the most classic and representative methods that belong to the family of the k-Medoids clustering algorithm. Different from k-Means, PAM applies an algorithm

![](images/ffacedbc6fed65aefef9881e44e6c9b1f669bb1433ca0d5e092c62544620800e.jpg)  
Fig. 7. An overview of partitional time-series clustering. Two tables in the figure represent the assignment probability under both crisp and fuzzy partitional methods.

named PAM Swap to perform centroid updates. For each medoid $m$ , PAM Swap swaps it with a non-medoid point $o$ , performs data points assignment, and calculates the total cost of the clustering. If the total cost decreases, the swap is

Manuscript submitted to ACM

performed and repeated. Otherwise, PAW Swap will undo the last swap and end. Compared with k-Means, incorporating medoids as centroids enhances the algorithm's robustness to the outliers and elevates the interpretability of centroids. Nevertheless, akin to k-Means, the k-Medoids clustering algorithm also requires the number of clusters as its input.

Furthermore, another extended variant of k-Means is called k-DBA [190] which incorporates Dynamic Time Warping (DTW) [207] as distance measure and adapts DTW barycenter averaging (DBA) as its centroid computation. For each refinement, DBA updates each coordinate of the average sequence with the barycenter of its associated coordinates obtained by calculating DTW between the average sequence and each other sequence individually. K-Spectral Centroid (K-SC) [250] clustering algorithm, modified from k-Means, incorporates a scaling and translation invariant (STI) distance measure as well as matrix decomposition technique to update its centroids.

Another algorithm in this category is k-Shape [178] and it is a current state-of-the-art time-series clustering algorithm. Different from k-Means, k-Shape utilizes SBD as its distance measure. Adopting normalized cross-correlation and speeding up the computation in the frequency domain, SBD becomes a cheaper and more efficient algorithm compared to some other good-performing algorithms such as DTW. To compute centroids, k-Shape aligns objects within the same cluster towards the cluster's centroid based on SBD, and, since cross-correlation captures similarity, the objective function becomes finding a new centroid to maximize the sum of squared similarities within a cluster. Then, the optimization problem can be modified into maximization of the Rayleigh Quotient [76] and the new centroid becomes the eigenvector associated with the largest eigenvalue.

# Fuzzy Partitional methods

Besides partitioning n objects into k clusters in a crisp manner which forces each object to become a part of exactly one cluster, there are fuzzy partitional algorithms that enable one object belonging to more than one cluster to a certain degree. One of the most representative algorithms in this category is named Fuzzy c-Means (FCM) [51] [26]. In FCM, each data is assigned a fuzzy membership value for each cluster. These membership values are real numbers, ranging from 0 to 1 and representing the degree of likelihood that the data belongs to a specific cluster. Moreover, for each data $x_{i}$ and cluster $C_j$ pair, FCM calculates the distance between $x_{i}$ and the cluster centroid $c_{j}$ and weights the distance by the membership that $x_{i}$ belongs to $C_j$ . With the goal of minimizing the sum of weighted distances across all data-cluster pairs, FCM iteratively updates membership values and cluster centroids until convergence. After convergence, the clustering configuration is finalized based on ultimate membership values.

[75] applied FCM to functional MRI and discussed its possible optimizations respecting to three different parameters: 1. data set pre-processing methods; 2. distance measures; 3. cluster numbers. Since they expected that clustering the pixel time courses should be performed based on similarity, they introduced two cross-correlation-based (CC-based) distance and performed comparisons among those two newly proposed measures plus ED to find the best measure.

Considering that many real-world situations involve the problem of short time series (STS) and unevenly sampled time series and motivated by observations in the field of molecular biology, [170] proposed a modified fuzzy clustering scheme by applying the proposed STS distance which was able to capture both the similarity of shapes formed by relative change of amplitude and the corresponding temporal information into a standard fuzzy clustering algorithm.

# 4.2 Hierarchical Clustering

Hierarchical clustering is an approach to grouping objects into clusters by constructing a hierarchy of clusters, which has great visualization power in time-series clustering. Hierarchical clustering methods can be divided into two types: Manuscript submitted to ACM

agglomerative and divisive. While agglomerative hierarchical (AH) clustering algorithms start by creating clusters individually for each time series and then iteratively merging small clusters into large clusters until meeting certain criteria, divisive hierarchical (DH) clustering algorithms tend to assign all time series into one cluster and perform division till satisfying certain criteria. Moreover, related methods can be found in Table 2.

Table 2. Summary of the Hierarchical clustering methods.   

<table><tr><td>Method Name</td><td>Linkage</td><td>Distance Measure</td><td>Dim</td></tr><tr><td>DC-MTS [109]</td><td>AH + /</td><td>KL + Chernoff</td><td>M</td></tr><tr><td>TSC-CBV [230]</td><td>AH + /</td><td>Root mean square</td><td>I</td></tr><tr><td>local-clustering [193]</td><td>AH + single</td><td>Ad hoc distance</td><td>I</td></tr><tr><td>hError [128]</td><td>AH + error-adjusted</td><td>Gaussian models of errors</td><td>I</td></tr><tr><td>TFDC [213]</td><td>AH + /</td><td>KL</td><td>M</td></tr><tr><td>ODAC [201]</td><td>DH + AH + criteria</td><td>RNOMC</td><td>M</td></tr><tr><td>TSC-CN [263]</td><td>AH + average</td><td>Triangle similarity + DTW</td><td>I</td></tr><tr><td>TSC-PDDTW [155]</td><td>AH + average</td><td>DTW + DDTW</td><td>I</td></tr><tr><td>HSM [59]</td><td>AH + spectral-based</td><td>Total variation distance</td><td>I</td></tr><tr><td>TSC-BD [221]</td><td>AH + complete</td><td>DTW</td><td>I</td></tr><tr><td>TSC-COVID [157]</td><td>AH + Ward&#x27;s</td><td>DTW</td><td>I</td></tr></table>

I: Univariate; M: Multivariate; /: Non-specify;

In AH clustering, since all merging operations are performed among the cluster level and each cluster contains at least one object, an extra similarity or dissimilarity measure should be introduced between two clusters and how to measure and represent the distance between two clusters in AH clustering methods becomes an important topic to explore. Thus, the idea of linkage is used and, in this survey, we list some widely used linkages to measure the distance between clusters [112]:

- Single linkage: In single linkage, given two clusters $C_i$ and $C_j$ of time series, we initially need to compute all pairwise distances $D = \{dist(x,y) \mid \forall x \in C_i \text{ and } \forall y \in C_j\}$ , where $dist(x,y)$ is a distance function used to measure the distance between two time series $x$ and $y$ . Then, the distance between $C_i$ and $C_j$ is defined as the shortest distance in $D$ .   
- Complete linkage: In complete linkage, given two clusters $C_i$ and $C_j$ of time series, we need to calculate all pairwise distances $D$ following the same expression in the single linkage. Then, the distance between $C_i$ and $C_j$ is defined as the longest distance in $D$ .   
- Average linkage: In average linkage, given two clusters $C_i$ and $C_j$ of time series, we should obtain all pairwise distance $D$ following the same expression mentioned above. After that, the distance between $C_i$ and $C_j$ is defined as the average value of $D$ .   
- Centroid linkage: In Centroid linkage, given two clusters $C_i$ and $C_j$ of time series, we initially need to compute the centroid (the mean time series) of each cluster and denote them as $\overline{C_i}$ for cluster $C_i$ and $\overline{C_j}$ for cluster $C_j$ . Then, the distance between those two clusters is represented as $dist(\overline{C_i}, \overline{C_j})$ .   
- Ward's linkage [237]: In Ward's linkage, considering two clusters $C_i$ and $C_j$ , the distance between two clusters, denoted as $\Delta(C_i, C_j)$ , is defined as the increase in total within-cluster variance that occurs after merging.

It is worth mentioning that although the majority of papers are using AH with different linkages as their clustering methods, [201] introduced a method named Online Divisive-Agglomerative Clustering (ODAC) system which applies

Manuscript submitted to ACM

DH and an agglomerative phase to cluster time-series data streams. ODAC uses DH with Rooted Normalized One-Minus-Correlation (RNOMC) as its dissimilarity measure and a splitting criterion that divides the node based on the most dissimilar pair of streams. Additionally, it incorporates an agglomerative phase to reassemble a previously divided node, responding to variations in the correlation structure between time series.

Compared with partitional clustering algorithms, hierarchical clustering algorithms do not require the pre-definition of the number of clusters, and mentioned by [143], given the suitable distance measure, hierarchical clustering may cluster time series of varying length. However, hierarchical clustering algorithms are difficult to adjust the clusters after they start and, thus, they are sometimes weak in quality and performance. Moreover, [3] stated that hierarchical clustering has quadratic computational complexity, and thus, due to its computational complexity and poor scalability, it is not optimal to run hierarchical clustering on large datasets.

# 5 Distribution-based Methods

In this section, we will discuss distribution-based clustering methods, where time-series data are grouped based on their explicit or implicit distribution. Emphasizing on extracting, selecting, learning, and utilizing the distribution of time-series data makes the distribution-based clustering methods be distinguished from the distance-based clustering methods. We further classified the distribution-based clustering methods into four second-level sub-categories: model-based (Section 5.1), density-based (Section 5.2), feature-based (Section 5.3), and encoding-based (Section 5.4) clustering methods. Subsequent sections will provide precise definitions for each sub-category, accompanied by an exposition of representative methods.

# 5.1 Model-based

Model-based clustering methods focus on modeling the latent distribution of time-series data using sets of parameters. In this way, the distance between two time series can be translated to the comparison between two parameter sets of each. Representative modeling techniques include the Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), Autoregressive Moving Average (ARMA), and Autoregressive Integrated Moving Average (ARIMA). Related methods can be found in Table 3.

# Gaussian Mixture Model (GMM)

GMM [28] is a probabilistic model that approximates the dataset with a mixture of Gaussian distributions. Suppose the number of cluster $K$ is given, the function for a mixture of $K$ multivariate Gaussian distribution is the following:

$$
\mathcal {N} \left(x _ {i} \mid \mu_ {j}, \Sigma_ {j}\right) = \frac {1}{(2 \pi) ^ {D / 2} \left| \Sigma_ {j} \right| ^ {1 / 2}} \exp \left(- \frac {1}{2} \left(x _ {i} - \mu_ {j}\right) ^ {T} \Sigma_ {j} ^ {- 1} \left(x _ {i} - \mu_ {j}\right)\right) \tag {1}
$$

$$
p \left(x _ {i}\right) = \sum_ {j = 1} ^ {K} \pi_ {j} \cdot \mathcal {N} \left(x _ {i} \mid \mu_ {j}, \Sigma_ {j}\right) \tag {2}
$$

the multivariate Gaussian density with unknown parameters $(\mu_j,\Sigma_j)$ was denoted as $\mathcal{N}(x_i|\mu_j,\Sigma_j)$ and $D$ represents the dimension of data. For cluster $j$ , $\mu_{j}$ is its mean, $\Sigma_{j}$ represents its covariance matrix, and $\pi_{j}$ is its mixture proportion. In order to estimate these parameters, maximizing the complete log-likelihood which has closed-form maxima can be applied. However, since the complete log-likelihood requires the observation of cluster assignments $z_{i}$ for each $x_{i}$ which are unknown and should be learned, in the learning stage of GMM, the Expectation-Maximization algorithm Manuscript submitted to ACM

(EM) is applied. Starting with a random initialization, we should iteratively execute the Expectation step (E-step) and Maximization step (M-step) until the convergence. In the E-step, the following fuzzy class membership is computed:

$$
\gamma_ {i j} = \frac {\pi_ {j} \cdot \mathcal {N} \left(x _ {i} \mid \mu_ {j} , \Sigma_ {j}\right)}{\sum_ {k = 1} ^ {K} \pi_ {k} \cdot \mathcal {N} \left(x _ {i} \mid \mu_ {k} , \Sigma_ {k}\right)} \tag {3}
$$

Meanwhile, in the M-step, the closed-form maxima solutions for parameters are given by the following equations:

$$
\pi_ {j} = \frac {1}{N} \sum_ {i = 1} ^ {N} \gamma_ {i j} \tag {4}
$$

$$
\mu_ {j} = \frac {\sum_ {i = 1} ^ {N} \gamma_ {i j} x _ {i}}{\sum_ {i = 1} ^ {N} \gamma_ {i j}} \tag {5}
$$

$$
\Sigma_ {j} = \frac {\sum_ {i = 1} ^ {N} \gamma_ {i j} \left(x _ {i} - \mu_ {j}\right) \left(x _ {i} - \mu_ {j}\right) ^ {T}}{\sum_ {i = 1} ^ {N} \gamma_ {i j}} \tag {6}
$$

where $N$ is the number of data. After the completion of model training, for a new data point, we are able to compute the probability of this new data belonging to each cluster based on learned parameters and assign it to a cluster that gives the highest probability.

# Hidden Markov Model (HMM)

HMM [28] is a probabilistic graphical model that aims to infer the underlying hidden states and transitions between states based on the observed data and Figure 8 exhibits an example of HMM. There are two primary assumptions behind the HMM. From state space $S = \{s_1, s_2, \dots, s_N\}$ and observation space $O = \{o_1, o_2, \dots, o_V\}$ , given a sequence of $n$ hidden states written as $X = \{x_1, x_2, \dots, x_n\}$ and a sequence of $n$ observations denoted as $Y = \{y_1, y_2, \dots, y_n\}$ , the next state $x_{i+1}$ and current observation $y_i$ only depend on current state $x_i$ . In HMM, apart from initial

![](images/bad0cee280126f3de11127e1c8b42f5c1051affd98e5c58436a51ef6143647e6.jpg)

![](images/30fd860920c18a44d1ef93a2fef4adf0d067eae37c49eaa5ce4eb0ef8ab59012.jpg)  
Fig. 8. An example of Hidden Markov Model (HMM).

probabilities, there are two types of probabilities: transition probabilities and emission probabilities. The transition probability articulates the likelihood of traveling from one hidden state to another while the emission probability is defined as the probability of generating a particular observation given the current hidden state. Additionally, the transition matrix, denoted as $\mathcal{T} \in \mathbb{R}^{N \times N}$ , encapsulates the transition probabilities between hidden states while the emission matrix $Q \in \mathbb{R}^{N \times V}$ captures the emission probabilities from hidden states to observations. To estimate the parameters of HMM, in general, we are trying to find:

$$
\mathcal {T} ^ {*}, Q ^ {*} = \operatorname {a r g m a x} _ {\mathcal {T}, Q} P (Y | \mathcal {T}, Q) \tag {7}
$$

HMM learning also applies the EM algorithm to estimate parameters and one of the most famous instances of the EM algorithm for HMM is called the Baum-Welch algorithm [20]. With trained HMM, for new data, we can infer the most likely sequence of hidden states that generates this observation and perform clustering based on the similarity between the inferred hidden states.

# Autoregressive Model (AR)

AR [95], a type of statistical model, believes that a future value in a time series is influenced by its own historical values. This model predicts a value in a time series applying a linear combination of a predefined number of previous values, known as the order of AR, and an AR with order $p$ can be calculated by the following equation:

$$
x _ {t} = \sum_ {j = 1} ^ {p} \phi_ {j} x _ {t - j} + \varepsilon_ {t} + c \tag {8}
$$

where, in a time series $x$ , $x_{t}$ denotes the current value at time $t$ , $\phi_{j}$ is the coefficient associated with $x_{t - j}$ which represents the previous value at time $t - j$ , $\varepsilon_{t}$ is the error term at time $t$ , and $c$ is a constant term.

# Moving Average Model (MA)

MA [95], another classic statistical model, relates the current value in the time series to historical errors. Given the order of MA $q$ , MA tends to calculate the value of the time series at time $t$ by incorporating the mean of the time series $\mu$ with a linear combination of previous errors and follows the exact equation as below:

$$
x _ {t} = \sum_ {j = 1} ^ {q} \theta_ {j} \varepsilon_ {t - j} + \varepsilon_ {t} + \mu \tag {9}
$$

where, in a time series $x, x_{t}$ represents the value at time $t$ , $\theta_{j}$ is the parameter associated with $\varepsilon_{t - j}$ which is the error at time $t - j$ , and $\varepsilon_{t}$ denotes the error at time $t$ .

Combining the AR with order $p$ and the MA with order $q$ , we can obtain a time series analysis model named Autoregressive Moving Average (ARMA) model [95] which characterizes the current value in a time series based on both previous values and error terms. Extending from ARMA, Autoregressive Integrated Moving Average (ARIMA) [95] integrates differencing with ARMA, empowering it with the capacity to handle non-stationarity in the time-series data.

Table 3. Summary of the Model-based clustering methods.   

<table><tr><td>Method Name</td><td>Clustering</td><td>Model</td><td>Distance Measure</td><td>Dim</td></tr><tr><td>TSC-ARIMA-ED [191]</td><td>AH + complete</td><td>ARIMA</td><td>ED</td><td>I</td></tr><tr><td>TSC-HISMOOTH [24]</td><td>AH + /</td><td>HISMOOTH</td><td>/</td><td>I</td></tr><tr><td>TSC-D-HMM [133]</td><td>4 search levels</td><td>HMM</td><td>Log-likelihood</td><td>M</td></tr><tr><td>TSC-HMM-DTW [171]</td><td>Hybrid</td><td>Discrete HMM</td><td>DTW</td><td>M</td></tr><tr><td>ICL [27]</td><td>TSC-ICL</td><td>GMM</td><td>Log-likelihood</td><td>M</td></tr><tr><td>TSC-AR-HT [163]</td><td>AH + test-based</td><td>AR</td><td>Hypothesis test</td><td>I</td></tr><tr><td>MBCD [194]</td><td>AH + prob-based</td><td>Markov Chain</td><td>KL distance</td><td>M</td></tr><tr><td>TSC-LPC-ARIMA [110]</td><td>PAM</td><td>ARIMA</td><td>ED</td><td>I</td></tr><tr><td>BHMMC [134]</td><td>4 search levels</td><td>HMM</td><td>BIC</td><td>M</td></tr><tr><td>FCM-SV [227]</td><td>Modified FCM</td><td>GMM</td><td>Log-likelihood</td><td>I</td></tr><tr><td>TSC-ARMAM [245]</td><td>Prob-based</td><td>ARMAs</td><td>Log-likelihood</td><td>I</td></tr><tr><td>R-TS-BHC [47]</td><td>Bayesian HC</td><td>Gaussian processes</td><td>Dirichlet process</td><td>I</td></tr><tr><td>TSC-HMM-S-KL [72]</td><td>PAM</td><td>HMM</td><td>S-KL divergence</td><td>M</td></tr></table>

I: Univariate; M: Multivariate; /: Non-specify;

# 5.2 Density-based

Manuscript submitted to ACM

In density-based clustering, clusters are subspaces of areas where objects are concentrated, and those dense areas are separated by empty or sparse areas. Diverging from model-based clustering methods, density-based clustering techniques utilize the explicit distribution of time-series data. Notably, many density-based clustering methods can be applied directly to raw time-series data without requiring specific data representations. In the remaining portion of this section, we will discuss some well-known and classic methods within this category and, additionally, more related methods can be found in Table 4.

Density Based Spatial Clustering of Applications with Noise (DBSCAN) [58] expands a cluster if its

Table 4. Summary of the Density-based clustering methods.   

<table><tr><td>Method Name</td><td>Representation</td><td>Distance Measure</td><td>Dim</td></tr><tr><td>DBSCAN [58]</td><td>Raw</td><td>ED</td><td>M</td></tr><tr><td>DENCLUE [87]</td><td>Map-oriented</td><td>Gaussian kernel</td><td>M</td></tr><tr><td>OPTICS [11]</td><td>Raw</td><td>ED</td><td>M</td></tr><tr><td>FDBSCAN [125]</td><td>Fuzzy object</td><td>Fuzzy distance</td><td>M</td></tr><tr><td>DENCLUE 2.0 [86]</td><td>Raw</td><td>Gaussian kernel</td><td>M</td></tr><tr><td>D-Stream [41]</td><td>Raw</td><td>Density function</td><td>M</td></tr><tr><td>DPC [202]</td><td>Raw</td><td>ED</td><td>M</td></tr><tr><td>TADPole [23]</td><td>Raw</td><td>cDTW</td><td>M</td></tr><tr><td>YADING [50]</td><td>Raw</td><td>L1</td><td>I</td></tr><tr><td>ADBSCAN [119]</td><td>Raw</td><td>ED</td><td>M</td></tr></table>

I: Univariate; M: Multivariate.

neighborhood is dense. Given the radius $(\epsilon)$ of a circular neighborhood whose center is an object and minimum density threshold (MinPts), DBSCAN first separates core points, defined as objects whose circular neighborhoods contain at least MinPts objects, from non-core points. Then, DBSCAN forms a cluster by randomly choosing a core point among ungrouped core points and expands that cluster by iteratively adding new core points that are within $\epsilon$ distance from any current core point in that cluster until no core points can be added into that cluster. Moreover, the new cluster includes all non-core points that are within $\epsilon$ distance from any core point in that cluster. Finally, repeat the previous steps besides step one to form the rest of the clusters until every core point belongs to a cluster. The power of DBSCAN is mainly manifested in the following aspects: Firstly, DBSCAN does not require users to determine parameters for the number and shape of clusters; Secondly, DBSCAN is able to handle large datasets; Thirdly, DBSCAN is a robust algorithm that is immune to outliers and noises.

Ordering Points To Identify the Clustering Structure (OPTICS) [11], instead of generating an explicit clustering, forms an augmented ordering of objects and produces a visual representation known as a reachability plot, reflecting the density-based clustering structure of the objects. Extending from DBSCAN, OPTICS introduces extra two terms. The first term is called core-distance and the core-distance of an object $p$ can be calculated through the following equation:

$$
\operatorname {c o r e} - \operatorname {d i s t} (p) = \left\{ \begin{array}{l l} \epsilon^ {\prime}, & \text {i f} p \text {i s a c o r e p o i n t} \\ \text {U n d e f i n e d ,} & \text {o t h e r w i s e} \end{array} \right. \tag {10}
$$

where $\epsilon'$ represents the minimum distance demanded to classify $p$ as a core point. Another term is named reachability-distance and the reachability-distance of $p$ with respect to $q$ is defined as:

$$
\operatorname {r e a c h a b i l i t y - d i s t} (p, q) = \left\{ \begin{array}{l l} \max  (\text {c o r e - d i s t} (q), \text {d i s t} (q, p)), & \text {i f} q \text {i s a c o r e p o i n t} \\ \text {U n d e f i n e d}, & \text {o t h e r w i s e} \end{array} \right. \tag {11}
$$

where $dist(q, p)$ represents the distance between object $q$ and object $p$ and, by default, ED can be used. Compared with DBSCAN, OPTICS provides several advantages such as enabling extracting clusters at different density levels and being less sensitive to the parameters.

DENsity-based CLUstEring (DENCLUE) [87], a more efficient algorithm compared to DBSCAN, models the influence of each object through a Gaussian influence function with ED, which describes an object's influence within its vicinity. By performing the map-oriented representation, the clustering step of DENCLUE is accelerated by only considering the highly populated cubes and cubes that are connected to them. In order to make the execution more efficient, instead of calculating the overall density of the data space, defined as the summation of all objects' influence functions, DENCLUE computes the local density function which approximates the overall density function by only considering the influence of the neighboring points. Then, the density-attractor, the local maxima of the density function, can be obtained through the hill-climbing procedure. Supported by its firm mathematical basis, besides being invariant to noise, DENCLUE is also efficient on large and high-dimensional data and suitable for finding clusters with various shapes.

Density Peak Clustering (DPC) algorithm [202] is another noteworthy representative in this category and is supported by the idea that the cluster center is distinguished by having a higher density in contrast to its neighbors while maintaining a relatively notable distance from points with higher densities. For each object $i$ , DPC calculates the local density $\rho_{i}$ and the relative distance $\delta_{i}$ by following the equations below and applying them to plot a decision graph.

$$
\rho_ {i} = \sum_ {j} \chi (d i s t (i, j) - d i s t _ {c}) \tag {12}
$$

$$
\chi (x) = \left\{ \begin{array}{l l} 1, & \text {i f} x <   0 \\ 0, & \text {o t h e r w i s e} \end{array} \right. \tag {13}
$$

$$
\delta_ {i} = \left\{ \begin{array}{l l} \max  _ {j} (d i s t (i, j)), & \text {i f} i \text {h a s t h e h i g h e s t d e n s i t y} \\ \min  _ {j: \rho_ {j} > \rho_ {i}} (d i s t (i, j)), & \text {o t h e r w i s e} \end{array} \right. \tag {14}
$$

where $dist(i,j)$ denotes the distance between object $i$ and object $j$ and $dist_{c}$ represents the cutoff distance. Upon computing $\rho_{i}$ and $\delta_{i}$ for each object $i$ in the dataset, DPC constructs a decision graph. This graph utilizes $\rho$ on the x-axis and $\delta$ on the y-axis to determine cluster centers defined as objects with higher $\rho$ and $\delta$ values. Once cluster centers are identified, DPC assigns each remaining point to the same cluster as its nearest neighbor with a higher density. In order to distinguish outliers, for every cluster, DPC identifies a border region, characterized as the collection of objects belonging to that cluster and also lying within a distance of $dist_{c}$ from objects assigned to other clusters. Within the border region of each cluster, DPC identifies an object $b$ that has the highest local density and records $\rho_{b}$ . By comparing the local density value of each object in the cluster with $\rho_{b}$ , any object with local density that is smaller than $\rho_{b}$ is considered as an outlier.

Time-series Anytime DP (TADPole) [23], a variant of DPC, applies cDTW as its distance measure. Compared to DPC, TADPole requires extra upper bound and lower bound matrices and, as stated by the author, the space complexity is not an issue since the bottleneck of DPC is CPU, and encoding the lower bound matrix into a sparse matrix can reduce the space overhead. During the local density calculation, TADPole prunes the distance computation based on four cases. Meanwhile, it applies a two-phase pruning strategy in calculating the distance to the nearest neighbor with a higher density for each object. As mentioned in the article, the univariate TADPole is able to be extended to the multivariate case with only a few changes.

# 5.3 Feature-based

Time series, as one of the basic data formats, represents the changes in signal values across time. However, the noise interference as mentioned above, could pose a challenge in the search for meaningful information especially when the dimension becomes extremely large. To solve this problem, feature-based time-series clustering methods are proposed to find descriptive features to represent the characteristics of time series in a global way. In this section, we will discuss representative methods in detail, and Table 5 contains more related methods.

Motivated by the observations that long time series and missing data might cause failures in the many existing clustering algorithms, Characteristic-Based Clustering (CBC) [234] was proposed. CBC utilizes global structural characteristics measures, combining classical statistical measures with advanced special measures, to cluster time series and they are: trend, seasonality, periodicity, serial correlation, skewness, kurtosis, chaos, non-linearity, and self-similarity. With the assistance of these global representations, CBC is able to significantly decrease the dimensionality of time-series data as well as demonstrate increased robustness against missing or noisy data. As for the clustering methods, for the purpose of obtaining good visualizations, the authors of CBC performed experiments by only applying hierarchical clustering and SOM. Moreover, as stated by the authors, since an appropriate set of features will not only make the computation more efficient but also generate better clustering results, they designed a new technique that is built upon a greedy Forward Search (FS) in order to select the optimized subset of features.

Recognizing the time-consuming aspect of time-series feature engineering, caused by the challenge of navigating through numerous algorithms of signal processing and time-series analysis to extract appropriate and meaningful features from time series, tsfresh [44], a widely used and well-known python package, was purposely to accelerate this process. It not only offers 63 time-series characterization methods, which compute a total of 794 time-series features, but also automates the feature extraction and selection based on the FeatuRe Extraction based on Scalable Hypothesis tests (FRESH) algorithm [45] which demonstrates the ability to scale linearly with the number of features, the quantity of devices/samples, and the number of different time series. However, since the variations in feature calculation costs are affected by their complexities, adjusting the computed features will significantly alter the runtime of tsfresh.

22 CAnonical Time-series CCharacteristics (catch22) [154] is a widely used feature set distilled from highly comparative time-series analysis (hctsa) [67] toolbox which contains over 7,700 time-series features in its comprehensive version and 4,791 features in the filtered version. Although hctsa has the capacity to select appropriate features for a given application, it is computationally expensive and performs redundant evaluations. Motivated by this observation, the authors of catch22 built a data-driven pipeline that incorporated statistical prefiltering, performance filtering, and redundancy minimization. It is worth mentioning that, in the redundancy minimization phase, they clustered the filtered high-performance features into 22 clusters by applying hierarchical clustering with complete linkage based on the Pearson Correlation (PC) between those features' performance vectors and manually selected features based on their simplicity and interpretability. Executing this pipeline resulted in a canonical set of 22 features, offering a huge improvement in computation efficiency and scalability while only sacrificing $7\%$ of classification accuracy on average. Moreover, catch22 captures the diverse and representative characteristics inherent in time-series data and its distilled features fall into 7 categories: distribution, simple temporal statistics, linear and nonlinear autocorrelation, successive difference, fluctuation analysis, and others.

Table 5. Summary of the Feature-based clustering methods.   

<table><tr><td>Method Name</td><td>Clustering</td><td>Feature</td><td>Distance Measure</td><td>Dim</td></tr><tr><td>TSC-GC-ED [235]</td><td>SOM</td><td>Global</td><td>ED</td><td>I</td></tr><tr><td>CBC [234]</td><td>*</td><td>Comprehensive</td><td>*</td><td>I</td></tr><tr><td>TSC-SSF [236]</td><td>*</td><td>Statistical</td><td>*</td><td>M</td></tr><tr><td>TSC-SF-EU [196]</td><td>k-Means</td><td>Statistical</td><td>ED</td><td>M</td></tr><tr><td>TSBF [21]</td><td>*</td><td>Statistical</td><td>ED</td><td>M</td></tr><tr><td>hctsa[66]</td><td>Linear</td><td>Comprehensive</td><td>*</td><td>I</td></tr><tr><td>FBC [1]</td><td>FBC</td><td>Fuzzy</td><td>ED</td><td>I</td></tr><tr><td>TSA-CF [65]</td><td>*</td><td>Comprehensive</td><td>*</td><td>M</td></tr><tr><td>tsfresh [44]</td><td>*</td><td>Comprehensive</td><td>*</td><td>M</td></tr><tr><td>catch22 [154]</td><td>*</td><td>Canonical</td><td>*</td><td>M</td></tr><tr><td>TSC-CN [30]</td><td>Community Detection</td><td>Visibility Graph</td><td>ED</td><td>M</td></tr><tr><td>TSC-SFLP-ED [43]</td><td>k-Means</td><td>Statistical + Load Profile</td><td>ED</td><td>I</td></tr><tr><td>FeatTS [224, 225]</td><td>k-Medoid</td><td>TSfresh</td><td>ED</td><td>I</td></tr><tr><td>TSC-FDDO [265]</td><td>Density Peak Search</td><td>Comprehensive</td><td>ED</td><td>I</td></tr><tr><td>TSC-GPF-ED [91]</td><td>k-Means</td><td>Global + Peak</td><td>ED</td><td>I</td></tr><tr><td>AngClust [132]</td><td>Affinity propagation</td><td>Angular</td><td>PC</td><td>M</td></tr><tr><td>FGHC-SOME [242]</td><td>SOM</td><td>Statistical</td><td>PC</td><td>M</td></tr><tr><td>FTSCP [57]</td><td>*</td><td>Comprehensive</td><td>ED</td><td>M</td></tr><tr><td>TSC-VF [240]</td><td>*</td><td>Visual</td><td>*</td><td>I</td></tr></table>

I: Univariate; M: Multivariate; \*: Arbitrary;

![](images/463de178f508af3034c6b96e8a899e35dcda98dc3916365b4ccb40ec850ae176.jpg)  
Fig. 9. An example of Symbolic Aggregate approXimation (SAX).

# 5.4 Encoding-based

The encoding-based time-series clustering methods focus on building a mapping function $\mathcal{F}:X\to X^{\prime}$ between the raw data space and the transformed space (also called the latent space). This process could be viewed as an encoding process. Compared with time-series clustering methods using raw data, the new representation $\mathcal{X}^{\prime}$ after encoding captures the crucial information from the original signals with possibly much fewer dimensions. With this advantage, the new representation $\mathcal{X}^{\prime}$ usually becomes a better option for the downstream procedure with better performance and less computation cost. We summarize all the encoding-based methods in Table 6.

It is noteworthy that although methods from both categories generate a new sequence of values for clustering purposes, there is one crucial difference between the definition of feature-based clustering and encoding-based clustering: feature-based clustering methods put attention on manually selecting descriptive features based on human knowledge of the current field, e.g., activation strength and delay in fMRI data, while encoding-based methods will use explicit Manuscript submitted to ACM

functions to automatically decompose the signal components such as Discrete Fourier Transform, and Piecewise Linear Approximation (PLA).

Generally, the mapping function $\mathcal{F}$ , which is the crucial part of the encoding-based time-series clustering methods, could be represented by a predefined mathematical formula, e.g., Fourier Transform. Representative methods will be discussed in the following paragraphs.

Table 6. Summary of the Encoding-based clustering methods.   

<table><tr><td>Method Name</td><td>Clustering</td><td>Encoding</td><td>Distance Measure</td><td>Dim</td></tr><tr><td>MKM [238]</td><td>Modified k-Means</td><td>LPC coefficients</td><td>Modified Itakura distance</td><td>I</td></tr><tr><td>CA-CTS [211]</td><td>AH</td><td>PCA</td><td>ED</td><td>M</td></tr><tr><td>CDM [116]</td><td>Hierarchical</td><td>SAX</td><td>CDM</td><td>I</td></tr><tr><td>I-kMeans [146]</td><td>k-Means / EM</td><td>Wavelets</td><td>ED</td><td>I</td></tr><tr><td>CTS-CD [15]</td><td>k-Means / k-Medoids</td><td>Clipped</td><td>ED</td><td>I</td></tr><tr><td>TSC-CR-LB [197]</td><td>k-Means</td><td>Clipped</td><td>LB_clipped</td><td>I</td></tr><tr><td>SAX [145]</td><td>Hierarchical / Partitional</td><td>SAX</td><td>MINDIST</td><td>I</td></tr><tr><td>TSC-ICA-SDA [82]</td><td>Modified k-Means</td><td>ICA</td><td>Unknown</td><td>I</td></tr><tr><td>ICTS-FC [2]</td><td>FCM</td><td>DWT</td><td>LCS</td><td>I</td></tr><tr><td>TSC-DSA-DTW [81]</td><td>k-Means</td><td>DSA</td><td>DTW</td><td>I</td></tr><tr><td>TTC [5]</td><td>Hybrid</td><td>PAA</td><td>ED + DTW</td><td>I</td></tr><tr><td>SAX Navigator [206]</td><td>AH</td><td>SAX</td><td>MINDIST variant</td><td>I</td></tr><tr><td>SPIRAL [131]</td><td>DTW-preserving</td><td>SPIRAL</td><td>DTW</td><td>M</td></tr></table>

# Discrete Fourier Transform (DFT)

Discrete Fourier Transform (DFT) is one of the most popular mathematical techniques in digital signal processing (DSP), which is also the main focus of our time-series field. Generally, Discrete Fourier Transform converts a sequence of N raw data $\mathbf{x} = \{x_0,x_1,\dots ,x_{N - 1}\}$ , into a new sequence of complex numbers $\mathbf{X} = \{X_0,X_1,\dots ,X_{N - 1}\}$ in the frequency domain as shown in Eq. 15. It is found that the new representation from the frequency domain has a very nice interpretability: low-frequency components usually capture the signals in the raw data which changes slowly over time, while high-frequency components would put more attention on the rapid changes. In real cases, low-frequency components have a high chance of revealing the dominant trends in the raw data and the background noise could be represented by the high frequency.

$$
X _ {k} = \sum_ {n = 0} ^ {N - 1} x _ {n} \cdot \mathrm {e} ^ {- \frac {i 2 \pi}{N} k n} \tag {15}
$$

# Principal Component Analysis (PCA)

PCA [189] is a classic and well-known technique for dimensionality reduction. The primary objective of PCA is to encode a high-dimensional dataset into a lower-dimensional representation while maintaining as much original information as possible. Prior to performing PCA, it is crucial to make the mean of each variable become 0 since this pre-processing step will eliminate bias in principal components. After obtaining these mean-adjusted data, a symmetric covariance matrix $A$ is calculated in order to explore any correlation between variables. On top of that, PCA requires the calculation of eigenvectors $\mathbf{v}$ and eigenvalues $\lambda$ (Equation (16)) for the covariance matrix $A$ to determine the principal components which are orthogonal and capture the information about directions in the original data where variation occurs. It is worth mentioning that the first principal component encapsulates the direction of the highest variation in

the data, the second principal component represents the direction of the second highest variation, and so forth. Since the definition of eigenvectors of the covariance matrix is the same as principal components, PCA sorts the eigenvectors in descending order based on their corresponding eigenvalues, forms a feature matrix $U$ with the eigenvectors in the columns by deciding which eigenvectors to keep, and projects the matrix $X$ with the mean-adjusted original data in rows onto the selected principal components to obtain the final data $X'$ based on Equation (16).

$$
A \mathbf {v} = \lambda \mathbf {v}, \quad X ^ {\prime} = X \cdot U \tag {16}
$$

# Piecewise Aggregate Approximation (PAA)

Given a time series with length $n$ , denoted as $x = \{x_{1},x_{2},\ldots ,x_{n}\}$ , PAA [114] encodes it into another vector $p = \{p_1,p_2,\dots ,p_w\}$ where $w$ is a user-specified input and $w\ll n$ . PAA separates the time series into $w$ equal-sized frames and calculates the mean for each frame. The vector consists of these computed mean values, serving as the PAA representation (see Eq. 17).

$$
p _ {i} = \frac {w}{n} \sum_ {j = \frac {n}{w} (i - 1) + 1} ^ {\frac {n}{w} i} x _ {j}, \tag {17}
$$

where each element $p_i \in p$ can be calculated by the above equation based on $x_j \in x$ .

# Symbolic Aggregate Approximation (SAX)

SAX [144] is another noteworthy representation method that encodes the time series in a symbolic manner and Figure 9 exhibits an example of SAX. Given a time series with length $n$ , SAX will reduce it into a string of length $w$ , where $w$ is defined by users and $w \ll n$ . After performing Z-normalization on the time series, SAX applies PAA representation which splits the time series into $w$ equal-sized frames, computes the mean for each frame, and concatenates these values into a vector $P = \{p_1, p_2, \ldots, p_w\}$ . Since the normalized time series exhibits the strongly Gaussian distribution, given the alphabet size $\alpha$ , SAX determines the breakpoints, denoted as a sorted list $B = \{b_1, b_2, \ldots, b_{\alpha - 1}\}$ ( $b_0$ and $b_\alpha$ are defined as $-\infty$ and $\infty$ correspondingly), which separate the Gaussian curve $\mathcal{N}(0, 1)$ into $\alpha$ equal-sized regions and each region under the curve is equal to $\frac{1}{\alpha}$ . Based on the obtained PAA representation and breakpoints, SAX contrasts all PAA coefficients to breakpoints in order to find the specific range that each PAA coefficient falls into and assigns the range-associated alphabet to represent that PAA frame. The final representation is denoted as $S = \{s_1, s_2, \ldots, s_w\}$ and each $s_i$ is calculated by the following function:

$$
s _ {i} = a l p h a _ {j}, i i f b _ {j - 1} \leq p _ {i} <   b _ {j} \tag {18}
$$

where $\text{alpha}_j$ represents the alphabet associated to range from $b_{j-1}$ to $b_j$ .

# 6 Subsequence-based Methods

Subsequence-based clustering is a special case in the whole time-series clustering categories as mentioned in Section 2.2. Unlike previous clustering methods such as partition-based or encoding-based, subsequence-based methods apply one or several subsequences to represent the entire time series, e.g., time series with similar shapelets can be clustered into the same group. It is important to note that, contrary to the subsequence clustering in Section 2.2, subsequence-based clustering methods still aim to assign a single label for an entire time series, which exactly aligns with whole time-series clustering in our definition. The subsequence-based methods have two second-level categories: sliding-based (Section 6.1) and shapelet-based (Section 6.2) algorithms. Related methods can be found in Table 7.

Manuscript submitted to ACM

Table 7. Summary of the Subsequence-based clustering methods.   

<table><tr><td>Method Name</td><td>Second Level</td><td>Distance Measure</td><td>Dim</td></tr><tr><td>U-Shapelets [257]</td><td>Shapelets</td><td>LN-ED</td><td>I</td></tr><tr><td>SUSH [229]</td><td>Shapelets</td><td>ED</td><td>I</td></tr><tr><td>USLM [261]</td><td>Shapelets</td><td>Soft minimum function</td><td>I</td></tr><tr><td>LDPS [152]</td><td>Shapelets</td><td>Convolutional score</td><td>I</td></tr><tr><td>USSL [262]</td><td>Shapelets</td><td>ED</td><td>I</td></tr><tr><td>FOTS [62]</td><td>Shapelets</td><td>FOTS</td><td>I</td></tr><tr><td>LSH-us [156]</td><td>Shapelets</td><td>LN-ED</td><td>I</td></tr><tr><td>Trendlets [107]</td><td>Shapelets</td><td>Ward Distance</td><td>I</td></tr><tr><td>ShapeNet [135]</td><td>Shapelets</td><td>ED</td><td>M</td></tr><tr><td>SPF [139]</td><td>Shapelets</td><td>Boolean</td><td>I</td></tr><tr><td>MUSLA [260]</td><td>Shapelets</td><td>ED</td><td>M</td></tr><tr><td>TSC-BLU [108]</td><td>Shapelets</td><td>LN-ED</td><td>M</td></tr><tr><td>CDPS [55]</td><td>Shapelets</td><td>DTW + ED</td><td>I</td></tr><tr><td>CSL [141]</td><td>Shapelets</td><td>Multiple Distance</td><td>M</td></tr><tr><td>MSCPF [200]</td><td>Sliding Window</td><td>ED</td><td>M</td></tr><tr><td>TS3C [80]</td><td>Sliding Window</td><td>ED</td><td>I</td></tr><tr><td>TCMS [137]</td><td>Sliding Window</td><td>ED</td><td>I</td></tr><tr><td>MPNCMI [136]</td><td>Sliding Window</td><td>ED</td><td>I</td></tr></table>

# 6.1 Sliding-based

Contrary to previous methods which attempt to compare time series across all time steps, sliding-based methods tackle this problem in a smaller scope each time: sliding windows. As illustrated in Section 2.1, sliding windows are a set of subsequences extracted by sliding a "window" in the same length. These subsequences could then be applied to provide feature information or directly function as one way of time-series similarity measure. There are two major directions under this category: (i) Matrix Profile and (ii) Subsequence Clustering. Related methods are summarized in Table 7.

# Matrix Profile

Matrix profile [254, 267] is a scalable algorithm for time series all-pairs-similarity-search in the subsequence level. Using a sliding window mechanism, the algorithm could extract possible subsequences and make matches efficiently, which is helpful for both motif discovery and solving discord problems. There are two primary components: matrix profile and matrix profile index. The definition is shown below.

Definition 6.1 (Matrix Profile [254]). A matrix profile $P_{AB}$ of time series $A$ and $B$ is a vector of the normalized Euclidean distances between each pair in the similarity join set $J_{AB}$ , i.e., a set contains all subsequences pairs $(A_i, B_j)$ obtained from sliding windows where each pair in the set is the nearest neighbor.

Definition 6.2 (Matrix Profile Index [254]). A matrix profile index $I_{AB}$ of time series $A$ and $B$ is a vector of indexes where $I_{AB}[i] = j$ if the pair $(A_i, B_j)$ in the set is the nearest neighbor.

Given the definition, the matrix profile $P_{AB}$ and the profile index $I_{AB}$ can be seen as metadata or a distance measure of special format between two time series. TCMS [137] defines the degree of correlation between two time series as the number of matched subsequences, i.e., the most similar subsequences found by Matrix Profile. Then the time-series dataset can be transformed to a graph network where each vertex is a time series and the edge is represented by the

Manuscript submitted to ACM

correlation. To solve the clustering problem, community detection is adopted for partitioning the network. MPNCMI [136] adopts the Matrix Profile in the similarity measure process to find similar subsequences. The normal cloud model is applied to the subsequence pairs to filter the pairwise information. Community discovery is conducted on the complex network to acquire the clustering results.

# Subsequence Clustering

Prior studies also explore the possibilities of clustering time series through multi-stages. Given the subsequences from sliding windows, subsequence clustering can be first conducted to obtain preliminary feature information before the whole time-series clustering in the final stage. MSCPF [200] proposes a sliding window-based multi-stage clustering algorithm using dynamic sliding time windows (DSTW). Subsequence clustering is performed in the first stage for segment information. In the second stage, the small segmented clusters are aggregated to obtain the final clustering results. This multi-stage clustering also provides a foundation for further time-series forecasting tasks. TS3C [80] adopts a similar idea. In stead of the previous subsequence clustering strategy after sliding windows, a least-squares polynomial segmentation strategy is adopted. Then each time series can be mapped through the feature information of all segments for further clustering.

# 6.2 Shapelet-based

The shapelet-based method is another category of time-series clustering in the subsequence level. It is found that some repeated subsequences could be exploited as meaningful patterns for time-series representation, which is called shapelet [253]. With the development of data mining techniques, more and more shapelet-based methods have emerged to search for robust shapelets in time-series clustering tasks [62, 152, 229, 257, 261, 262]. In this section, we follow the definition of shapelet from [253, 257]. Related methods can be found in Table 7. In Figure 10, we show an overview of the general shapelet transform pipeline. With a set of learned shapelets, each time series can be projected to a new representation space by measuring the distance between the entire sequence and the shapelets. Time series from the same group might end up being closer in this new space, serving as critical guidance for the clustering process.

Definition 6.3 (Shapelet). Given a time-series dataset that consists of multiple classes, a set of shapelets $\hat{S}$ are subsequence time series that are highly predictive of the time-series classes.

![](images/c616759f6308d68a256ffa63b1d01ca32670c28fbc9a3a000d3aa0fdfbc34415.jpg)  
Fig. 10. The overview of a shapelet transform pipeline [135].

U-Shapelets [257] proposes a novel method for shapelet searching from the unlabeled time-series dataset, which tries to maximize the separation gap between clusters using different subsequences. A distance map can be easily obtained by calculating the distance between the subsequence distance $sdist$ between the shapelet and the given time series. Such distance maps can be further applied to different clustering algorithms like K-Means. However, the overall time complexity makes U-Shapelets search intractable and not applicable on large datasets. To solve this problem, Scalable U-Shapelet (SUSH) [229] is proposed to allow scalable shapelet discovery without significant performance loss. SAX representation with random masking is applied to speed up the similarity measure between time series, where the collision number in comparison can be viewed as an important predictor of qualities. Experiments have demonstrated that time complexity can be reduced by two orders of magnitude.

Unsupervised Shapelet Learning Model (USLM) [261], on the contrary, designs an efficient shapelet learning strategy for unlabeled time-series data, instead of the time-costly searching as before. In the process, the candidate shapelet $\hat{S}$ , classification boundary $W$ and pseudo-class label $Y$ can be jointly optimized through a least square minimization problem with shapelet transform. Learning DTW-Preserving Shapelets (LDPS) [152] proposes a novel learning strategy for DTW-preserving shapelets without labels. The overall objective is to approximate the DTW in the original space using the shapelet transformation in Euclidean space. It is noteworthy that the concept of shapelet transform and shapelet match share a similar form compared with the Convolutional Neural Network, which naturally brings the convolutional variant of LDPS for application.

With the advent of deep neural networks, more and more studies focus on exploring the possibility of deep learning strategies. CSL [141] proposes a multi-grained contrastive strategy for shapelet learning on multivariate time-series data. In the framework, the shapelet is represented as learnable parameters and a shapelet transformer is adopted as the encoder to acquire the latent embedding. Considering the issue of variable length, multi-scale alignments are designed to preserve consistency across each level. As an unsupervised representation learning algorithm, CSL could produce a shapelet-based representation suitable for various time-series downstream tasks.

# 7 Representation-learning-based Methods

In recent decades, the rise of deep learning has introduced numerous time-series clustering methods, leveraging the powerful representation capabilities of deep neural networks. In this section, we will first outline the key components of representation-learning-based methods (Section 7.1), and then review individual representative algorithms, consisting of two sub-level categories: Comparative-based (Section 7.2) and Generative-based (Section 7.3). A comprehensive summary of the surveyed methods is provided in Table 8.

# 7.1 Time-series Representation-learning Overview

Representation learning has been widely applied in numerous research topics, e.g., computer vision, natural language processing, etc. In early studies, researchers have found that while deep neural networks are mostly black boxes to human understanding, certain layers' output feature maps are capable of extracting meaningful feature information, such as edges or repeated patterns. In many cases, this kind of representation learned by a well-designed model architecture pre-trained on large datasets tends to be highly robust and could be easily adapted to downstream tasks. As the dimension of the learned representation can be highly reduced compared to the original data, these techniques have been widely adopted as a dimension reduction strategy in the time-series domain, where the input length could be extremely large. Figure 11 shows an example of the learned representation (visualized by T-SNE).

![](images/d85a088e27bd62513842bbcf3a1f8d449b45fac88aac745f8b82e0778704215d.jpg)  
(a) CBF Dataset

![](images/9805a81a954748526b90f181b38b160f1ddd4c3f8c2ab4c20179043a1cb49e01.jpg)  
(b) FacesUCR Dataset   
Fig. 11. The T-SNE visualization of the encoded representation (learned by representation-learning-based strategy) on the UCR datasets. Left: CBF Dataset with 3 clusters. Right: FacesUCR Dataset with 14 clusters.

In general, given the learning strategy design, there are three different representation learning categories: supervised learning, semi-supervised learning, and unsupervised learning. In this survey, under the settings of the time-series clustering task, we mainly focus on unsupervised learning (some semi-supervised techniques are also included). The overall pipeline of the representation-learning-based methods in the clustering tasks can be summarized as follows:

- Pre-training stage. Given the designed model and training set, the objective of the pre-training stage is to learn a representative latent space suitable for time-series clustering.   
- Clustering stage. The representation obtained from the pre-trained model is utilized as a new input for conventional clustering methods, such as k-Means and k-Shape.

Following the discussion in [130], there are three major components in time-series representation-learning-based clustering methods: (i) model architecture (ii) pretext loss (iii) clustering loss. The model architecture will influence the overall performance and inference speed, while the pretext loss and clustering loss design determine the structure of the latent space. Each component will be discussed in the following sections.

7.1.1 Model Architecture. There are many different architecture designs in deep learning-based models, as well as in the time-series clustering domain. Among them, the most basic ones are Fully Connected Neural Network (FCN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Attention-based Neural Network, and Graph Neural Network (GNN).

# Fully Connected Neural Network

Fully Connected Neural Network (FCN), also known as Multilayer Perceptron (MLP), is one of the most basic architectures in deep learning-based models, originating from the idea in Neuroscience [203]. It proposes a hypothetical nervous system called a perceptron. Each perceptron calculates a mapping between multiple inputs and one single output using learnable weights and activation functions (shown in Figure 12). To extract different features, multiple perceptrons can be stacked together across layers and generate an output that usually possesses a lower dimension compared to the original input. Utilizing this idea, different FCN-based models have been proposed in various domains. Manuscript submitted to ACM

Table 8. Summary of the Representation-learning-based clustering methods.   

<table><tr><td></td><td>Second Level</td><td>Strategy</td><td>Backbone</td><td>Label</td><td>Dim</td></tr><tr><td>DEC [244]</td><td>Comparative-based</td><td>CLS</td><td>AE</td><td>U*</td><td>I*</td></tr><tr><td>ClusterGAN [70]</td><td>Comparative-based</td><td>ADV</td><td>GAN</td><td>U</td><td>M*</td></tr><tr><td>TCGAN [92]</td><td>Comparative-based</td><td>ADV</td><td>GAN</td><td>U</td><td>M</td></tr><tr><td>T-Loss [63]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>TS2Vec [256]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>SleepPriorCL [259]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>TF-C [264]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>BTSF [251]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>MHCCL [168]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>Ts2DEC [98]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>Se*</td><td>I</td></tr><tr><td>DCRLS [243]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>CROCS [122]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>S</td><td>I</td></tr><tr><td>LDVR [9]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>I</td></tr><tr><td>RDDC [228]</td><td>Comparative-based</td><td>CLS</td><td>RNN</td><td>U</td><td>M</td></tr><tr><td>TSTCC [56]</td><td>Comparative-based</td><td>CNRV</td><td>TRAN</td><td>U</td><td>M</td></tr><tr><td>PCL [138]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>CCL [210]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>CRLI [158]</td><td>Comparative-based</td><td>ADV</td><td>GAN</td><td>U</td><td>M</td></tr><tr><td>TS-CTS [38]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>Se</td><td>M</td></tr><tr><td>TNC [226]</td><td>Comparative-based</td><td>CNRV</td><td>CNN</td><td>U</td><td>M</td></tr><tr><td>IDEC [83]</td><td>Generative-based</td><td>REC</td><td>AE</td><td>U</td><td>I</td></tr><tr><td>DEPICT [71]</td><td>Generative-based</td><td>REC</td><td>AE</td><td>U</td><td>I</td></tr><tr><td>DCN [248]</td><td>Generative-based</td><td>REC</td><td>AE</td><td>U</td><td>M</td></tr><tr><td>CKM [68]</td><td>Generative-based</td><td>REC</td><td>AE</td><td>U</td><td>I</td></tr><tr><td>SOM-VAE [61]</td><td>Generative-based</td><td>REC</td><td>VAE</td><td>U</td><td>I</td></tr><tr><td>SDCN [29]</td><td>Generative-based</td><td>REC</td><td>GCN</td><td>U</td><td>I</td></tr><tr><td>DTCR [160]</td><td>Generative-based</td><td>REC</td><td>RNN</td><td>U</td><td>I</td></tr><tr><td>DTC [162]</td><td>Generative-based</td><td>REC</td><td>LSTM</td><td>U</td><td>I</td></tr><tr><td>VADE [106]</td><td>Generative-based</td><td>ELBO</td><td>VAE</td><td>U</td><td>I</td></tr><tr><td>TST [258]</td><td>Generative-based</td><td>REC</td><td>TRAN</td><td>U</td><td>M</td></tr><tr><td>KMRL [111]</td><td>Generative-based</td><td>REC</td><td>LSTM</td><td>U</td><td>I</td></tr><tr><td>T-DPSOM [165]</td><td>Generative-based</td><td>REC</td><td>LSTM</td><td>U</td><td>I</td></tr><tr><td>DTSS [93]</td><td>Generative-based</td><td>FCST</td><td>TCN</td><td>U</td><td>M</td></tr><tr><td>DeTSEC [96]</td><td>Generative-based</td><td>REC</td><td>GRU</td><td>U</td><td>M</td></tr><tr><td>IT-TSC [246]</td><td>Generative-based</td><td>FCST</td><td>TCN</td><td>U</td><td>M</td></tr><tr><td>DTCC [266]</td><td>Hybrid-based*</td><td>REC+CNRV</td><td>LSTM</td><td>U</td><td>I</td></tr><tr><td>TimeCLR [252]</td><td>Hybrid-based</td><td>REC+CNRV</td><td>CNN</td><td>U</td><td>I</td></tr><tr><td>conDetSEC [97]</td><td>Hybrid-based</td><td>REC+CNRV</td><td>CNN</td><td>Se</td><td>M</td></tr><tr><td>MCAE [247]</td><td>Hybrid-based</td><td>REC+CNRV</td><td>LSTM</td><td>U</td><td>M</td></tr><tr><td>GPT4TS [223]</td><td>Foundation-Model</td><td>/</td><td>LLM</td><td>U</td><td>M</td></tr><tr><td>Chronos [12]</td><td>Foundation-Model</td><td>FCST</td><td>LLM</td><td>U</td><td>I</td></tr><tr><td>MOMENT [78]</td><td>Foundation-Model</td><td>REC</td><td>TRAN</td><td>U</td><td>I</td></tr><tr><td>TimesFM [48]</td><td>Foundation-Model</td><td>FCST</td><td>LLM</td><td>U</td><td>M</td></tr><tr><td>UniTS [69]</td><td>Foundation-Model</td><td>REC</td><td>*</td><td>U</td><td>M</td></tr></table>

I: Univariate, M: Multivariate; Se: Semi-Supervised, U: Unsupervised; $*$ : Arbitrary model backbone. Hybrid-based: Methods using learning strategies from both comparative-based and generative-based categories.

[83, 244]. Each layer of the network is named a dense layer, or a fully connected layer, consisting of multiple perceptrons as mentioned above. In mathematical form, the function of the $i$ th layer can be expressed in the following equation:

$$
Z _ {i} = W _ {i} ^ {T} X _ {i} + b _ {i}, Y _ {i} = \sigma (Z _ {i}) \tag {19}
$$

where $X_{i}$ , $Z_{i}$ , and $Y_{i}$ denote the input, intermediate, and output value of this current $i$ th layer. $W$ and $b$ represent the learnable weights in the model. $\sigma$ here represents the activation function, which plays an important role in introducing the nonlinearity in neural networks. In practical use, the activation function $\sigma$ includes but is not limited to sigmoid, tanh, ReLU, and GELU. Different choices usually depend on factors such as learning tasks, model structures, datasets.

![](images/77300664854422c44f052130fd6dee8331fddfffd8f86e610b4c7e7c7be47896.jpg)

![](images/b7b9642fa6c9daad8f5536a0376c55145d7bbdb841d2fdaa90a608c7a8acc7b9.jpg)  
Fig. 12. The overview of the structure of perceptron and the multilayer perceptron (MLP).

# Convolutional Neural Network

Convolutional Neural Network (CNN) has been widely applied as the basic structure, especially in computer vision tasks. Representative CNN-based models are VGG [214], AlexNet [127], and ResNet [85], which have achieved great success in different tasks such as image classification, detection, segmentation. As depicted in Figure 13, each convolutional layer has $m$ kernels of size $k$ , which consist of learnable weights. Basically, each kernel can be viewed as a filter, scanning the data input in a sliding window way and searching for a certain pattern across dimensions. Compared with the FCN design, CNN layers take advantage of the kernel design which can be applied and reused across all dimensions, or channels, at the same time and thus need much fewer parameters than fully connected layers. Most widely used convolutional layers are in the format of 2-dimension (for image) or 1-dimension (for speech, text, and time-series data).

![](images/fc367e9d048d79fbc9b0b3ef43f3801361d02a952aa9fe0a64fb7a9bf2fb6059.jpg)  
(a) 1D Convolution

![](images/66964e426c252361ffb33c273e6385d7728345b4a6999b32e3bc471332ae6b53.jpg)  
(b) 2D Convolution   
Fig. 13. The overview of the convolution mechanism.

# Recurrent Neural Network

However, an issue persists for both the fully connected neural network and the convolutional neural network discussed above: they are not able to accommodate inputs with varying lengths, an essential characteristic for some data formats such as speech, text, or general time series. To solve this problem, the recurrent neural network (RNN) is proposed [42, 88, 90], which has become one of the most important model architectures in various research areas such as speech recognition, machine translation, and time-series clustering. Unlike traditional feedforward neural networks, the internal memory mechanism allows RNN-based models to take the input step by step and recursively Manuscript submitted to ACM

update the hidden states (shown in Figure 14). Representative models are Long Short Term Memory (LSTM) [88] and Gated Recurrent Units (GRU) [42], which have achieved great success in different fields. In mathematical form, the updating rule of the hidden state $h_t$ in traditional RNN can be expressed in the following equation (at time step $t$ ):

![](images/8df3df7eff90b097b85593d38b87fdab23d764432833ae6dccd764c2b0bdba10.jpg)

![](images/532552cf232d89d39efd5278d388df145534af4d18565b8ca2e1636bd75ca5d4.jpg)

![](images/115018ad4ba7ec3aec3c269000d59e688bdf80c554daa3240c5ea4b9f0238469.jpg)

![](images/a6feb42ac03450ada4d181db2b0759b8690e655e883a782cb4d6b8fc1bbc664a.jpg)

![](images/f65d0941684a2863862e35461b61db4e23421ec0595a5f5a0d18f629be8072c5.jpg)

![](images/5420dfa00b291c65fde7ea3079813c7f41424eac33f7b2bdfc1a6332a1aeb5f4.jpg)  
Fig. 14. The overview of the LSTM architecture, one of the classic RNN-based networks.

$$
h _ {t} = \tanh  \left(W _ {h} h _ {t - 1} + W _ {x} x _ {t} + b\right), \tag {20}
$$

where $x_{t}$ and $h_{t}$ denote the input and hidden state at time step $t$ . $W_{h}, W_{x}$ and $b$ represent the learnable weights in the RNN design. It is noted that RNN could also have multiple layers like FCN or CNN above.

# Attention-based Neural Network

The concept of the attention mechanism, a widely recognized deep learning architecture, is proposed in the paper [16]. It introduces the soft alignments between the encoder and decoder and achieves an apparent improvement in neural machine translation. In 2017, the Transformer architecture was introduced in the paper titled "Attention is all you need" by Vaswani et al. [231]. This innovation quickly became the dominant model architecture in diverse research domains, including natural language processing (NLP), computer vision (CV), etc. It describes a mapping function from query, key, and value pairs to an output, where each component can be modeled from the input of each layer. The attention mechanism stands out for its good explainability and great performance, while greatly reducing the computational time with parallel computing. Transformer (TRAN), one of the most important attention-based architectures, is shown in Figure 15. Give the query $Q$ , key $K$ and value $V$ , the attention mechanism can be expressed in the following way:

$$
\operatorname {A t t e n t i o n} (Q, K, V) = \operatorname {s o f t m a x} \left(\frac {Q K ^ {T}}{\sqrt {d _ {k}}}\right) V, \tag {21}
$$

where $\frac{1}{\sqrt{d_k}}$ is a scaling factor given the dimension of keys $d_{k}$

# Graph Neural Network

![](images/d54e28d37067ea73df1436ab6bd2d726a93a244b4b6b8205192b7d08ff567118.jpg)  
Scaled Dot-Product Attention

![](images/adc0a3ca4ffbf9c1aea8dcd2e4e8c908abbd7da465bd4f472fb7dbaf4b25c4a9.jpg)  
Multi-Head Attention

![](images/3e76cabe56069081aaaf6f3f187724f5ed34778449f31b29ceb7b02a8fc824b9.jpg)  
Fig. 15. The overview of the attention mechanism in Transformer [231].   
Fig. 16. The structure of a simple graph neural network.

Graph, as one of the most important data structures, has been widely explored in many research fields. It can be seen as a collection of nodes/vertices and edges. Following the development of deep neural networks, graph neural network (GNN) receives great attention, particularly in scenarios where the relation between data samples are crucial for solving the problem. There are various GNN architectures, such as Graph Convolutional Networks (GCN) [121], Graph Sample and Aggregated (GraphSAGE) [84], Graph Attention Networks (GATs) [232]. In time-series analysis, each data sample can be viewed as a node in a high-dimension space. The edges between nodes can be defined as the similarity or distance value. Considering the advantage of GNN in learning structural information, prior studies combine the GNN module with the traditional DNN module to obtain a better representation for time-series clustering [29]. In general form, the message and aggregation in each GNN layer can be expressed as Eq. 22:

$$
h _ {v} ^ {(k)} = f ^ {(k)} \left(W ^ {(k)} \cdot \frac {\sum_ {u \in \mathcal {N} (v)} h _ {u} ^ {(k - 1)}}{| \mathcal {N} (v) |} + B ^ {(k)} \cdot h _ {v} ^ {(k - 1)}\right), \tag {22}
$$

Manuscript submitted to ACM

where $h_v^{(k)}$ represents the node embedding of $v \in V$ after $k^{th}$ layer. $\mathcal{N}(v)$ denotes the neighborhood of the noise $v$ . $W^{(k)}$ and $B^{(k)}$ are the learnable weights and $f^{(k)}$ represents the activation function for $k^{th}$ layer. Figure 16 depicts the structure of a simple graph neural network.

7.1.2 Pretext Loss. Following the discussion in the prior study [130], the objective functions of the representation learning in the first stage can be categorized into two different losses: one is pretext loss, which helps the model learn meaningful features. However, the pretext loss designs are usually targeted at general time-series analysis, not specifically clustering tasks. To solve this issue, many methods integrate clustering loss in the pre-training stage which provides a certain constraint manifold of the latent space. In this section, we are going to discuss the pretext loss.

# Reconstruction Loss (REC)

Reconstruction loss is one of the most widely used loss functions for general representation learning. By minimizing the error between the original input and the reconstructed output, the autoencoder could learn meaningful features that could be extracted for representing the data using much fewer dimensions.

$$
\mathcal {L} _ {R E C} = \frac {1}{N} \sum_ {i = 1} ^ {N} \| X _ {i} - \mathcal {D} (\mathcal {E} (X _ {i})) \| ^ {2}, \tag {23}
$$

where $X_{i}$ is the $i$ th input in the dataset, with $N$ samples in total. $\mathcal{E}$ and $\mathcal{D}$ are the encoder and decoder module of the autoencoder respectively. Usually the reconstruction loss takes L2 norm as shown in the equation above.

# Multi-Reconstruction Loss (MREC)

This is an extended version of the reconstruction loss, where the loss is calculated for each level. This hierarchical design puts stronger constraints on the representation learning. It is noted that the symmetry structure is required for the encoder and decoder model design.

$$
\mathcal {L} _ {M R E C} = \frac {1}{N} \sum_ {i = 1} ^ {N} \sum_ {j = 1} ^ {L} \| o _ {\mathcal {D} _ {j}} ^ {i} - o _ {\mathcal {E} _ {j}} ^ {i} \| ^ {2}, \tag {24}
$$

where $o_{\mathcal{E}_j}^i$ and $o_{\mathcal{D}_j}^i$ represent the output from the $j^t h$ layer of the encoder and decoder.

# Variational Autoencoder Loss (VAE)

Variational autoencoder (VAE), as one of the most famous variants of the autoencoder (AE), is proposed to improve the capability of generalizing new data. A KL Divergence constraint is introduced to help regularize the probability distribution of the learned latent space compared to the pre-defined prior distribution (usually Gaussian distribution).

$$
\mathcal {L} _ {V A E} = \sum_ {i = 1} ^ {N} - \mathbb {E} _ {z _ {i} \sim q (z _ {i} | x _ {i})} [ \log p (x _ {i} | z _ {i}) ] + K L (q (z _ {i} | x _ {i}) | | p (z _ {i})), \tag {25}
$$

where $z_{i}$ is the learned latent representation of the input data $x_{i}$ , $p(z_{i})$ is the pre-defined prior distribution, $q(z_{i}|x_{i})$ and $p(x_{i}|z_{i})$ are two conditioned distribution which could be modeled by deep neural networks.

# Triplet Loss (TRPLT)

Triplet loss is one of the most important objective function designs widely used in contrastive learning. Given the anchor data sample, we could define the positive and negative pair by carefully selecting similar/dissimilar data samples. The overall goal is to pull closer the learned representation of similar data samples and push apart those of dissimilar

samples.

$$
\mathcal {L} _ {T R P L T} = - \log \left(\sigma \left(\mathcal {E} (X) ^ {\top} \mathcal {E} \left(X ^ {+}\right)\right)\right) - \sum_ {k = 1} ^ {K} \log \left(\sigma \left(- \mathcal {E} (X) ^ {\top} \mathcal {E} \left(X _ {k} ^ {-}\right)\right)\right), \tag {26}
$$

where the data sample $X$ is the anchor sample, $X^{+}$ and $X^{-}$ represent the positive and negative samples respectively. Denote by $\sigma$ the sigmoid function and $K$ the number of negative pairs.

# InfoNCE Loss

InfoNCE loss is another widely used contrastive learning loss design, widely used in unsupervised representation learning works such as SimCLR [39]. Similar to the triplet loss, the InfoNCE loss also enforces the model to capture the distance relationship between data pairs, e.g., positive pairs and negative pairs. Here we provide the equation in SimCLR [39] for illustration.

$$
\mathcal {L} _ {\text {I n f o N C E}} (i, j) = - \log \frac {\exp \left(\sin \left(z _ {i} , z _ {j}\right) / \tau\right)}{\sum_ {k = 1} ^ {2 N} \mathbb {1} _ {[ k \neq i ]} \exp \left(\sin \left(z _ {i} , z _ {k}\right) / \tau\right)} \tag {27}
$$

where $z_{i}$ and $z_{j}$ are learned representations of $i^{th}$ and $j^{th}$ data samples in the dataset. $\mathrm{sim}(\cdot)$ is the similarity function between two vectors. $\mathbb{1}$ denotes the indicator function. $N$ is the batch size during training and $\tau$ is the temperature parameter.

7.1.3 Clustering Loss. As discussed in the previous section, pretext losses can be applied for general deep representation learning, but may not specifically deal with the issue in clustering tasks. Here we list 7 widely used clustering losses in the literature to help solve this problem.

# DEC

This loss is first proposed in DEC [244] for unsupervised deep embedding learning. An auxiliary target distribution is introduced to help learn a representation suitable for clustering analysis. The loss is defined in Kullback-Leibler (KL) divergence format.

$$
\mathcal {L} _ {D E C} = \mathrm {K L} (\mathbf {P} \| \mathbf {Q}), \tag {28}
$$

where $\mathbf{P}$ represents the auxiliary target distribution and $\mathbf{Q}$ denotes the soft clustering assignment distribution. $N$ and $K$ are the batch size and number of clusters.

# IDEC

IDEC [83] is an extended version of DEC [244]. An under-complete autoencoder is applied along with the clustering loss (DEC) for better local structure preservation. $\gamma$ is a hyperparameter to balance between the two losses. When $\gamma$ becomes 0, the IDEC loss will be the same as DEC loss.

$$
\mathcal {L} _ {I D E C} = (1 - \gamma) \mathcal {L} _ {D E C} + \gamma \mathcal {L} _ {R E C}, \tag {29}
$$

# DEPICT

Similar to IDEC loss, DEPICT [71] also combines the clustering-oriented loss and the reconstruction loss. Different from the IDEC loss, the target distribution $\mathbf{P}$ also participates in the optimization process.

$$
\mathcal {L} _ {\text {D E P I C T}} = \mathrm {K L} (\mathbf {P} \| \mathbf {Q}) + \mathrm {K L} (\mathbf {f} \| \mathbf {u}) + \mathcal {L} _ {\text {M R E C}}, \tag {30}
$$

Manuscript submitted to ACM

where $\mathbf{f}$ and $\mathbf{u}$ are the empirical label distribution and the uniform prior respectively. In the paper, the target distribution $\mathbf{P}$ is computed using the clean pathway design.

# SDCN

SDCN [29] utilizes a graph neural network to capture the structure information of the data. The overall objective function includes the reconstruction loss, clustering loss and GCN loss.

$$
\mathcal {L} _ {S D C N} = \mathcal {L} _ {R E C} + \alpha \mathcal {L} _ {C L U} + \beta \mathcal {L} _ {G C N}, \tag {31}
$$

where $\mathcal{L}_{CLU} = \mathrm{KL}(\mathbf{P}\| \mathbf{Q})$ and $\mathcal{L}_{GCN} = \mathrm{KL}(\mathbf{P}\| \mathbf{Z})$ . $\mathbf{Q}$ and $\mathbf{Z}$ are the soft cluster assignment distribution by the DNN and GCN modules respectively.

# VaDE

VaDE [106] combines VAE and GMM to learn a deep representation that is well-suited for clustering tasks, and also capable of generating meaningful samples.

$$
\mathcal {L} _ {V a D E} = \mathbb {E} _ {q (z, c | x)} [ \log p (x | z) ] - D _ {K L} (q (z, c | x) \| p (z, c)), \tag {32}
$$

where $c \sim \mathrm{Cat}(\pi)$ denotes the cluster and $p(z, c)$ represents the Mixture-of-Gaussians (MoG) prior. The overall objective function is in the format of evidence lower bound (ELBO).

# DTCR

DTCR [160] incorporates an auxiliary classification module to discriminate between real and fake samples. Besides, a k-Means objective is also introduced to learn cluster-specific representations.

$$
\mathcal {L} _ {D T C R} = \mathcal {L} _ {R E C} + \mathcal {L} _ {A D V} + \gamma \mathcal {L} _ {K - M e a n s}, \tag {33}
$$

where $\mathcal{L}_{ADV}$ resembles the discriminator in Generative adversarial network (GAN), which takes the format of cross-entropy loss during training.

# ClusterGAN

ClusterGAN [70] proposed a GAN-based model to help learn meaningful representations in an unsupervised manner. A clusterer module $C$ is introduced to help learn the mapping from the data sample to the latent space.

$$
\mathcal {L} _ {\text {C l u s t e r G A N}} = \min  _ {\mathcal {G}, C} \max  _ {\mathcal {D}} \mathbb {E} _ {X \sim p (X)} [ \log \mathcal {D} (C (X), X) ] + \mathbb {E} _ {Z \sim p (Z)} [ \log (1 - \mathcal {D} (Z, \mathcal {G} (Z))) ], \tag {34}
$$

# 7.2 Comparative-based

Similar to previous clustering methods, such as Encoding-based methods, the Comparative-based time-series clustering methods also provide a mapping function $\mathcal{E}:X\to \mathcal{Z}$ to represent the time series in a latent vector (usually in a much fewer dimension) for downstream tasks. However, in this scenario, the encoder mapping function can be learned by a neural network in a comparative way, e.g., contrastive learning (CNRV) or generative adversarial networks (ADV). All methods are enumerated in Table 8.

7.2.1 Contrastive Learning. Contrastive learning (CNRV) is one of the most widely-used techniques now in unsupervised deep learning, which has achieved great success in numerous research areas such as computer vision, natural language

processing, speech recognition and so on. Given one data sample as the anchor, we could find similar and dissimilar examples to make: positive pairs and negative pairs. The goal of contrastive learning is to learn a robust representation such that the distances between positive pairs are shortened while the distance between negative pairs are enlarged (depicted in Figure 17). This strategy also naturally aligns with the core idea in some clustering techniques, e.g., partition-based methods, and thus has received great attention in recent years. In this section, we are going to introduce some widely-used contrastive learning loss and then go through the methods.

There are many different objective function designs for contrastive learning in general machine learning. Among them, two main objective functions or losses are widely used in time-series contrastive learning: Triplet Loss [63, 169] and InfoNCE Loss [39, 172] (see Eq. 26 and 27). Both of them have achieved great success in unsupervised representation learning and exhibit good performance in numerous downstream tasks.

Based on the number of negative samples $K$ , we could assume one or multiple negative pairs in the experiments, while $\tau$ is a temperature parameter that controls the distribution [241]. By carefully

![](images/c0858f484a5e161e67b50cc5fde7f52bc9a5ab4e795acd912f25c57461d9fc34.jpg)  
Fig. 17. Overview of the contrastive learning strategy.

selecting positive and negative pairs, the model could learn a meaningful representation in a contrastive manner. Based on the view of contrast, we discuss the contrastive strategy from three different categories: Temporal Contrast, Instance-wise Contrast, and Multi-view Contrast. It is noted that the concept of Temporal Contrast and Instance-wise Contrast also clearly align with our two types of (whole) time-series clustering: clustering from the level of subsequences or the entire time series respectively.

# Temporal Contast

Temporal Contrast is first proposed in the [63] to tackle the challenge of the positive/negative-pair selection under an unsupervised learning manner. With annotation, one could easily classify which should be positive or negative given the current anchor time-series sample. However, in unsupervised representation learning, such guidance is not available. Following word2vec's intuition, one solution is to use the idea of context, to automatically generate positive and negative pairs. Given a random subsequence $x^{ref}$ in a time-series sample $x$ , it is assumed that any subsequences $x^{pos}$ within the reference $x^{ref}$ should share a similar representation. On the other hand, any subsequence $x^{neg}$ obtained from other random time-series should be considered as negative samples with a large distance from $x^{ref}$ . Triplet loss (T-Loss) is applied for the objective function. Numerous experiments demonstrate the learned representation exhibits good performance in the time-series clustering task.

Temporal Neighborhood Coding (TNC) [226] adopts a similar concept as mentioned above. Instead of using T-Loss, a discriminator is applied to approximate the probability of one subsequence being the neighborhood of the other given the latent representation from the encoder. However, the aforementioned methods still do not take into account the selection of positive or negative pairs. In prior studies, it is found that anchors in minimal local variance could lead to poor performance [38]. The author proposes a Contrastive Triplet Selection strategy to select meaningful subsequence

samples $x^{ref}, x^{pos}, x^{neg}$ based on the local variance and the similarity relationship calculated by Euclidean Distance, which provides more guidance to the model learning.

# Instance-wise Contrast

Instead of temporal information in a subsequence level, Instance-wise Contrast attempts to learn the similarity relationship from the level of instance, i.e., the entire time series. Given the positive pairs $\{x^{ref}, x^{pos}\}$ and negative pairs $\{x^{ref}, x^{neg}\}$ , the model can learn meaningful representation through contrastive learning. Here, all inputs, $x^{ref}, x^{pos}, x^{neg}$ are all distinct times-series from the given dataset, instead of subsequences. In order to generate reliable triplet sets, different strategies are explored. Ts2DEC [98] proposes a semi-supervised deep embedding clustering frame that produces triplet constraints using ground truth, while methods like CCL [210] adopt the weak cluster labels from the learned representation.

Without supervision, the triplet set generation process faces the same situation as Temporal Contrast. As one of the earliest contrastive learning-based studies, SimCLR [39] explores a simple framework using image pairs generated by data augmentation. In the paper, two augmented views from the same data sample are considered as one positive pair, while different samples in one batch are noticed as negative pairs for each other. This strategy has achieved great success in different data modalities, including time series. LDVR [9] applies the same concept and proposes an unsupervised triplet section strategy for time-series data. Instead of using the original format, 2-D time-series images are generated to utilize the knowledge from the pre-trained model in the computer vision field, e.g., ResNet [85]. DCRLS [243] extends the SimCLR idea in time-series domain with multi-layer similarity contrasting and achieves good performance. Self-distillation is adopted as a regularization in the framework for knowledge transfer.

Similar to Temporal Contrast, some studies also focus on different sampling strategies for the triplet selection. SleepPriorCL [259] proposes a knowledge-based positive mining strategy to tackle the sampling bias problem in contrastive learning. Positive pairs are re-defined within a minibatch using the pre-calculated dissimilarity. Experiments indicate the performance improvement in the sleep staging task compared with baseline methods.

# Multi-view Contrast

Apart from the previous two types, some recent studies also explore the combination of different views for robust representation learning. TS2Vec [256] proposes a unified framework that utilizes both Instance-wise and Temporal contrast in a hierarchical way. TS-TCC [56] introduces the cross-view task for both temporal and contextual contrasting. In addition to the time domain, some researchers also explore the possibility of the frequency domain. TF-C [264] introduces a new frequency encoder to ensure both time and frequency consistency between instance-level pairs. Within the same year, BTSF [251] adopted the concept of frequency consistency as well. For better capturing the information from both domains, the bilinear feature is optimized in a fusion-and-squeeze manner iteratively.

Nevertheless, the aforementioned strategies mainly emphasize general representation learning through contrastive learning strategies, but may not specifically address the issue of clustering tasks. Cluster or prototype-based contrastive loss has been proposed to help improve the clustering quality [122, 138, 168]. Similar to Temporal Contrast or Instance-wise Contrast, the approaches enforce distinguishable representation between different clusters. As cluster-level contrast learning is hard to optimize on its own, it is usually treated as an auxiliary loss for a multi-view purpose in these papers.

7.2.2 Generative Adversarial Network. Generative Adversarial Network (GAN) [77] is initially proposed for high-quality image generation, consisting of two major components: (i) the generative model $\mathcal{G}$ attempts to capture the ground truth data distribution, and (ii) the discriminative model $\mathcal{D}$ estimates the probability of the given sample being synthesized

or real data. With the novel design of the minimax two-player game, the generator $\mathcal{G}$ is capable of generating close-to-realistic data and achieved great success in all kinds of data formats including image, video, text, time series and so on. The structure of the classic GANs is shown in Figure 18.

Specifically, given a noise input $z \sim p_z(z)$ , the generator will synthesize a fake data sample $\hat{x} = \mathcal{G}(z)$ , which shares the same shape as the real data $x \sim p_{data}$ . The responsibility of the discriminator is to predict the probability that the given data $x'$ is real, with a scalar of $\mathcal{D}(x')$ , and $1 - \mathcal{D}(x')$ for synthesized data. In the end, the discriminator $\mathcal{D}$ attempts to make a correct binary classification of the given data, and the generator $\mathcal{G}$ will try to fool the discriminator $\mathcal{D}$ with high-quality data samples.

![](images/0004b8adb8e35b0ba87bda0622d7c53d0678d5ae28d135e0e70f736746987465.jpg)  
Fig. 18. Overview of the generative adversarial networks (GAN).

Denote the value function $V(\mathcal{D},\mathcal{G})$ , the overall objective can be defined in a two-player minimax game way (Eq. 34). The loss will finally converge when the Discriminator is unable to distinguish the synthetic data from the real data and the Generator could not improve itself as well ( $\mathcal{D}(x) = \mathcal{D}(\mathcal{G}(z)) = \frac{1}{2}$ ).

In order to distill the knowledge from GAN-based model, prior studies have explored different techniques. ClusterGAN [70] proposes a deep generative adversarial network for the clustering task. In order to capture the feature information of the data, ClusterGAN introduces a clusterer module that can map the real data into a discriminative representation. To address the problem in time-series domain, TCGAN [92] is introduced. Apart from the normal design of the generator and discriminator, a representation encoder is created using the pre-trained discriminator during the unsupervised learning process and thus tailored for downstream tasks such as time-series classification or clustering. Some studies also focus on specific cases of the time-series clustering task. For incomplete time-series clustering, CRLI [159] provides an end-to-end method to optimize the imputation and clustering process using GAN-based network design. In this case, the generator can be viewed as an encoder that outputs the robust representation for future clustering processes.

# 7.3 Generative-based

Contrary to comparative-based clustering methods, generative-based time-series clustering methods utilize generative model architecture to learn the robust representation by casting constraints on the generation output. There are two major techniques in this field: (i) the reconstruction task: given input data, find a discriminative latent representation, which contains crucial feature information to reconstruct the original data. (ii) the forecasting task: given the subsequence of time-steps from 0 to $t - 1$ , predict the value of the next time step $t$ . In order to accomplish this, an encoding process is also required for feature extraction and future prediction. In these ways, it is possible for us to find a good latent space for data representation with possibly fewer dimensions and we could just apply a simple K-means method on it to obtain the final clustering results. Methods can be found in Table 8.

7.3.1 Reconstruction-based Learning. AutoEncoder (AE) is one of the most classic methods proposed for various tasks like dimension reduction, pre-training, and generation tasks using the concept of reconstruction [17, 22, 124, 205]. Numerous variants, including Variational AutoEncoder (VAE) [120], have achieved great success in computer vision, machine translation, speech recognition and so on. Specifically, AutoEncoder consists of two important parts: an

Manuscript submitted to ACM

Encoder $\mathcal{E}:\mathcal{X}\to \mathcal{Z}$ , which maps the input $\mathcal{X}$ to the latent space $\mathcal{Z}$ , and the Decoder $\mathcal{D}:\mathcal{Z}\rightarrow \widehat{\mathcal{X}}$ which produces reconstruction data $\widehat{\mathcal{X}}$ from the latent space $\mathcal{Z}$ (shown in Figure 19). The objective function can be defined as Eq. 23.

Given the remarkable performance, there has been extensive investigation into AE-based time-series clustering techniques. Based on the observation that the classical method DEC [244] could suffer from the distortion issue in the embedding space, IDEC [83, 162] proposes to add the reconstruction constraint along with the clustering loss and better capture the structure information. DE

![](images/3674f59a14a2e9ca89bfaf80823eb2a678c7f45f5c9a8b3745e5f0c0e7dec61a.jpg)  
Fig. 19. Overview of reconstruction-based learning.

PICT [71] extends the idea and applies reconstruction loss at each level of the denoising autoencoder. Different components like the Noisy (corrupted) Encoder, Decoder, and clean Encoder are jointly optimized with the KL-divergence clustering loss.

Considering the wide usage of k-Means methods in representation-learning-based methods, many studies also investigate techniques to discover a "k-Means friendly" space in reconstruction-based learning. For example, DCN [248] simultaneously optimizes the reconstruction and k-Means objective. However, the conventional k-Means loss design is non-differentiable and leads to the complexity of an alternating stochastic optimization strategy. To avoid that, CKM [68] introduces the deep k-Means strategies with concrete gradients and simultaneously optimizes the autoencoder parameters along with the cluster centroids. Following the concept of "k-Means friendly" representation learning, some researchers also delve into the potential of incorporating other constraints on the latent space for better clustering. VaDE [106] explores the combination of VAE and Gaussian Mixture Model (GMM). This integration helps learn good representations suitable for clustering tasks and enhances the capability to generate samples with Mixture-of-Gaussians (MoG) prior. Besides, other works also take the advantage of self-organizing map (SOM) design. SOM-VAE [61] and T-DPSOM [165] devise SOM-friendly representation learning frameworks by jointly optimizing the conventional VAE and the self-organizing map (SOM). This design strongly enhances the interpretability of time-series representation learning which benefits from the topological structure and smoothness in the learned latent space.

As various deep learning techniques continue to emerge, the influence of different model designs has become one of the focus in recent studies, e.g., GNN and attention-based neural networks. SDCN [29] proposes a dual self-supervised learning framework by incorporating the graph structure information from the GCN module. With this design, the knowledge can be transferred between the conventional autoencoder and the graph neural network for unified representation learning. As attention-based networks have become the state-of

![](images/93a0edd3a83f5c37d16c1ccbd4f6dbb3c3f9db55bcf3ff6020ea506aa1ad21e3.jpg)  
Fig. 20. Overview of forecasting-based learning.

the-art backbone design of many research fields, e.g., natural language processing, computer vision, etc., more and more researchers have been exploring its potential in time-series data representation learning. DeTSEC [96] first introduces the attention and gating mechanisms in the conventional recurrent autoencoder. To learn an embedding manifold

tailored for the clustering task, a clustering refinement process is applied in the second stage. TST [258] proposes the first transformer-based network for multivariate time-series analysis. For unsupervised pre-training, the model is enforced to predict the value of masked segments. Extensive experiments have demonstrated the effectiveness of this modeling approach on diverse downstream time-series tasks.

7.3.2 Forecasting-based Learning. Similar to reconstruction-based learning, forecasting-based (FCST) learning models take the time-series data as an input and generate synthetic data as an output. Contrary to reconstruction-based learning, which learns the representation from reconstructing the data, forecasting-based learning focuses on predicting the next steps by learning the relationship between the past and future (as shown in Figure 20). In one simple case of this kind of method, given a subsequence of time steps from 0 to $t - 1$ , the model predicts the value $\hat{x}_t$ at time step $t$ . The regression loss will be calculated based on the error of predictions compared to the real data $x_t$ (Eq. 35). It is noteworthy that depending on the forecasting scope, the regression objective function design could take different forms but the core idea remains consistent.

$$
\mathcal {L} _ {F C T} = \left\| \hat {x} _ {t} - x _ {t} \right\| \tag {35}
$$

Some recent studies are delving into this learning strategy for time-series analysis. IT-TSC [246] devises a multi-path neural network to capture the variable association graphs, where each path is associated with one cluster. Given the subsequence $X(*, t_1: t_{n-1})$ , the model would autoregressively predict $\hat{X}(*, t_2: t_n)$ . All the modules will be optimized together by minimizing the regression error. In the inference stage, the path with the least regression error will be assigned as the cluster label. DTSS [93] proposes a hybrid framework with temporal convolutional networks (TCN) and embedding sketching, which incorporates both local and global feature information. The embedding space is first trained by a forecasting-based learning strategy. For further dimension reduction, the sketch is extracted over the embedding space by sliding windows. Finally, the sequence of sketches is concatenated for clustering purpose.

# 8 Evaluation Methods

In this section, we discuss the evaluation algorithms for clustering, which serve as indices to assist individuals in evaluating the effectiveness of a clustering algorithm and deciding which one is suitable to use. Depending on whether they require external information or not, as discussed by [3], clustering evaluation indices are generally divided into two types: external index and internal index. More specifically, the external index relies on external resources to assess the results of clustering, whereas the internal index evaluates the clustering based on the intrinsic structures of the results produced by clustering algorithms. In the following sections, we will delve into a comprehensive discussion of the concepts and representative evaluation methods associated with these two categories in Section 8.1 and 8.2.

# 8.1 External Index

As described in [3], the external index is employed to assess the resemblance between clusters generated by clustering algorithms and externally provided standards such as class labels and ground truth, making it the most popular method for evaluating clustering performance.

1. Purity: Obtaining the ground truth clusters and the generated clusters, in order to compute the purity of generated clusters with respect to the ground truth, each generated cluster is assigned to a class based on the majority class label within that cluster and then, the purity is calculated by dividing the total number of correctly assigned data points by

Manuscript submitted to ACM

the total number of data points.

$$
P u r i t y = \frac {1}{N} \sum_ {i} \max  _ {j} \left| C _ {i} \cap T _ {j} \right| \tag {36}
$$

where $N$ represents the total number of time-series data points, $C_i$ represents the cluster $i$ , and $T_j$ represents for the ground truth class label $j$ . The purity ranges from 0 to 1 and a poor clustering yields a purity approaching 0 while a perfect clustering achieves 1 as its purity. However, when cluster numbers are large and there are only a few number of time-series data points in each cluster, it is easy to achieve high purity. Considering the extreme situation that each cluster only contains one data point, the purity value is equal to 1. Hence, clustering quality evaluation cannot be performed solely relying on the purity.

2. Rand Index (RI): The RI was introduced by [195]. Given a set of $N$ data points $\mathcal{X} = \{x_0, x_1, \dots, x_{N-1}\}$ and two clusterings generated on the same data $\mathcal{X}$ , the predicted clustering $C = \{C_0, C_1, \dots, C_{K-1}\}$ and the ground truth $T = \{T_0, T_1, \dots, T_{M-1}\}$ , the RI assesses the similarity between $C$ and $T$ and it can be calculated by normalizing the number of similar assignments of point-pairs by the total number of point-pairs.

$$
R I = \frac {T P + T N}{T P + T N + F P + F N} \tag {37}
$$

where True Positives (TP) represents the frequency with which data points in a point-pair are grouped within the same cluster in both $C$ and $T$ , True Negatives (TN) represents the frequency with which data points in a point-pair are assigned to different clusters in both $C$ and $T$ , False Positives (FP) represents the count of occurrences where data points in a point-pair are clustered to the same cluster in $C$ while $T$ separated them into different clusters, and False Negatives (FN) represents the count of occurrences where data points in a point-pair are separated into the different clusters in $C$ while $T$ grouped them into the same cluster. The range of the RI value is from 0 to 1, where $RI = 1$ can be explained as two clusterings are identical and $RI = 0$ can be interpreted as two clusterings are completely distinct.

3. Adjusted Rand Index (ARI): The ARI was proposed by [94], which is a corrected-for-chance extension of the RI. Given two random clusterings generated on the same set of data points, since the RI only considers the ratio of the number of similar assignments of point-pairs with respect to the total number of point-pairs, the RI may produce a relatively low or high value for this case. However, with corrections for chance, the ARI will generate value of 0 for random clustering results.

$$
A R I = \frac {R I - E x p e c t e d R I}{\text {M a x i m u m} R I - \text {E x p e c t e d} R I}. \tag {38}
$$

The ARI generates values from -1 to 1, where $ARI = 1$ indicates that two clusterings are identical, $ARI > 0$ represents that the degree of similarity between two clusterings is better than random chance, $ARI = 0$ suggests that the agreement of two clusterings is equal to random chance, and $ARI < 0$ conveys that the agreement of two clusterings is worse than random chance in this case.

4. Normalized Mutual Information (NMI): NMI [219] is a measure widely employed in information theory and data analysis to evaluate the correlation or mutual information between two clusterings. Given two clusterings $M$ and $N$ on the same set of data points $X$ , where $M$ and $N$ can have different numbers of clusters, NMI compares them by computing the mutual information between clusterings and normalizing it for the size of the clusters and the total

number of data point. Based on [219] and [218], NMI formula can be expressed as following equations:

$$
N M I (M, N) = \frac {I (M ; N)}{\sqrt {H (M) \cdot H (N)}}, \tag {39}
$$

$$
I (M; N) = H (N) - H (N | M), \tag {40}
$$

$$
H (M) = - \sum_ {m \in M} P (m) \log_ {2} P (m), \tag {41}
$$

$$
H (N) = - \sum_ {n \in N} P (n) \log_ {2} P (n), \tag {42}
$$

$$
H (N | M) = - \sum_ {m \in M, n \in N} P (m, n) \log_ {2} \frac {P (m , n)}{P (m)}. \tag {43}
$$

where $P(m, n)$ represents the joint probability of the value of $M$ is $m$ and the value of $N$ is $n$ while $P(m)$ and $P(n)$ represent the probability of $M$ taking on the value $m$ and the probability of $N$ taking on the value $n$ respectively. The score of NMI ranges from 0 to 1 where $NMI = 1$ implies the perfect correlation between two input clusterings and $NMI = 0$ suggests no mutual information between $M$ and $N$ .

In addition to the methods mentioned earlier, there are several other notable external indices that can be used for clustering results evaluation. F-measure [8] combines both precision and recall into a unified value and is computed as the harmonic mean of these two metrics. Moreover, the F-measure ranges from 0 to 1, with the higher score suggesting better performance. The Entropy of a cluster [8] is another classic external index and evaluates the level of impurity among the data within each cluster. On top of that, entropy also ranges from 0 to 1, with the higher score indicating the higher uncertainty in the data of that cluster. The Adjusted Mutual Information (AMI) [167], adjusted from the Mutual Information (MI), is designed to fix the fact that MI generates higher scores for two clusterings with more clusters, regardless of whether they share more information. Furthermore, AMI has a range from 0 to 1, where 1 represents an excellent agreement.

# 8.2 Internal Index

Also, as mentioned in [3], the internal index is different from the external index and it is utilized to evaluate the quality and performance of a clustering structure without requiring external resources.

1. Silhouette Coefficient: The silhouette coefficient [204] is one of the classic metrics belonging to the internal index and is applied to measure the quality of clustering results obtained by clustering algorithms without requiring any ground truth. Given a single data point $i$ in a dataset, its silhouette coefficient $s(i)$ can be calculated as follows:

$$
s (i) = \frac {b (i) - a (i)}{\max  \{a (i) , b (i) \}} \tag {44}
$$

where $b(i)$ is the minimum average distance from data point $i$ to data points in a different cluster and $a(i)$ is the average distance from data point $i$ to the other data points within the same cluster. For data point $i$ , its $s(i)$ ranges from -1 to 1 where a large value closing to 1 indicates that data point $i$ is well-clustered while a small value approaching -1 suggests an opposite meaning. Based on the individual silhouette coefficient, the overall silhouette score can be calculated by averaging silhouette coefficients across all data points. A high silhouette score suggests that data points are well-clustered while a really low score indicates that the clustering is inaccurate.

2. Davies-Bouldin index (DB index): DB index [49], which considers both dispersion within clusters and separation between clusters to measure the quality of clustering, is another representative and noteworthy clustering evaluation method that belongs to the internal index. The dispersion $S(i)$ of a cluster $C_i$ is calculated by the average distance Manuscript submitted to ACM

between each point $x \in C_i$ and its centroid $c_i$ , given by the following equation:

$$
S (i) = \frac {1}{\left| C _ {i} \right|} \sum_ {x \in C _ {i}} d \left(x, c _ {i}\right) \tag {45}
$$

Besides that, the separation between clusters $C_i$ and $C_j$ , as exhibited by the following equation, can be measured through calculating the distance between centroids $c_i$ and $c_j$ of cluster $C_i$ and $C_j$ :

$$
M (i, j) = d \left(c _ {i}, c _ {j}\right) \tag {46}
$$

After obtaining both dispersion within clusters and cluster separations, we are able to calculate the DB index for a clustering result that generates $K$ clusters through the function below:

$$
D B = \frac {1}{K} \sum_ {i = 1} ^ {K} \max  _ {j = 1 \dots K, j \neq i} \frac {S (i) + S (j)}{M (i , j)} \tag {47}
$$

For a clustering result assessed using the DB index, a lower score indicates that each cluster has a high internal similarity and is well-distinguished from other clusters.

3. Dunn index (DI): DI [52] is a classic internal index and its computation involves the consideration of both intra-cluster compactness and inter-cluster separation. To compute DI for $K$ clusters:

$$
D I = \frac {\operatorname* {m i n} _ {i = 1 , \cdots , K , j = 1 , \cdots , K , i \neq j} \left\{\operatorname {d i s t} \left(C _ {i} , C _ {j}\right) \right\}}{\operatorname* {m a x} _ {p = 1 , \cdots , K} \left\{\operatorname {d i a m} \left(C _ {p}\right) \right\}} \tag {48}
$$

$$
\operatorname {d i s t} \left(C _ {i}, C _ {j}\right) = \min  _ {x \in C _ {i}, y \in C _ {j}} | | x - y | | \tag {49}
$$

$$
\operatorname {d i a m} \left(C _ {p}\right) = \max  _ {x, y \in C _ {p}} | | x - y | | \tag {50}
$$

where the cluster $i$ is denoted as $C_i$ . If a clustering result has a high DI score, we can state that there exists compact and well-separated clusters. However, it is worth mentioning that DI is sensitive to outliers and computationally expensive.

4. Within-cluster sum of squares (WCSS): WCSS [161], a noteworthy internal index, evaluates the cluster cohesion which gauges the degree of similarity among objects within a cluster. Moreover, WCSS can be calculated by summing up the squared distances between each data $x$ and the centroid of its assigned cluster:

$$
W C S S = \sum_ {i = 1} ^ {K} \sum_ {x \in C _ {i}} d (x, c _ {i}) ^ {2} \tag {51}
$$

where $K$ is the number of clusters and $c_{i}$ represents the centroid of the $i^{th}$ cluster $C_i$ . Beyond that, WCSS is applied in the elbow method to determine the optimal number of clusters $K^*$ for clustering algorithms that require $K^*$ as an input. Evaluating a given clustering result by WCSS, a low score represents a desirable situation where each cluster's data are tightly grouped around the centroid. However, it is worth mentioning that an extremely low WCSS may be achieved by choosing a very large $K$ and, if we make $K$ equal to the number of data, WCSS will equal 0.

# 9 Conclusion

Time-series clustering is an unsupervised task that aggregates similar time-series data into groups, aiming to reduce the intra-class distance and maximize inter-class distance. In this survey, we collect more than 100 time-series clustering algorithms, dividing them into 4 first-level categories: Distance-based, Distribution-based, Subsequence-based, and Representation-learning-based. To further classify the gathered time-series clustering algorithms, we propose 10 secondary-level categories that stem from their respective parent categories. Based on the proposed taxonomy, we offer

an in-depth discussion of key algorithms within each category. In addition, building on the prior studies [3], we also investigate the external and internal indices utilized for evaluating the clustering results.

Despite decades of progress in this area, the challenge of time-series clustering still persists. Different clustering designs become crucial when the distortion of time series cannot be trivially diminished with pre-processing strategies in real-world scenarios [178, 250, 256, 257]. Partitional clustering methods, starting from the classic k-Means, have demonstrated a good balance in clustering accuracy and runtime [101, 178]. Hierarchical clustering methods, in comparison, provide more flexibility in clustering resolution [101, 109, 157], e.g., the dendrogram could be cut at different heights to obtain a finer or coarser clustering. As illustrated in previous sections, the choice of dissimilarity measure and representation plays an important role in determining the accuracy and runtime of one clustering method. Numerous approaches have been proposed to integrate new insights into these two crucial components from different perspectives, e.g., distribution-based and subsequence-based methods. The former focuses on modeling the distribution of the time-series data, such as the hidden Markov model (HMM) [134, 171] of the training data or the density across the raw time-series space [41, 58]. On the other hand, the latter exploits representative subsequences to represent each time series and exhibit robustness to noise perturbations, as it mainly focuses on salient patterns [200, 257]. With the development of deep learning techniques, many representation-learning-based methods have been introduced and demonstrated effectiveness in this domain [63, 130, 168, 256]. These unsupervised learning approaches demonstrate strong performance in both dimensionality reduction and representational capability [130, 256].

To reveal the landscape in this domain, a few evaluation studies have emerged and received increasing attention. [101] provides the first time-series clustering benchmark with 8 popular methods from partitional, hierarchical, and density-based categories. A steady increase has been observed for newly proposed clustering approaches, however, no single method could outperform others in all datasets. [130] presents a comprehensive study on the effectiveness across model architecture, learning strategies, and parameter setting in deep learning-based clustering methods, which sheds light on this direction. [184] provides a modular web engine named Odyssey that enables rigorous evaluation studies across 128 time-series datasets. Overall, there is no single method that proves superior across all scenarios, which highlights the need for in-depth investigation across various domains in future studies. A key issue is to find the right balance between clustering accuracy and runtime cost across various scenarios. We aim for this survey work to serve as a comprehensive exploration of this area, offering insights for future time-series clustering algorithm designs.

# References

[1] Tatiana Afanasieva, N Yarushkina, and Ivan Sibirev. 2017. Time series clustering using numerical and fuzzy representations. In 2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS). IEEE, 1-7.   
[2] Saeed Aghabozorgi, Mahmoud Reza Saybani, and Teh Ying Wah. 2012. Incremental clustering of time-series by fuzzy clustering. Journal of Information Science and Engineering 28, 4 (2012), 671-688.   
[3] Saeed Aghabozorgi, Ali Seyed Shirkhorshidi, and Teh Ying Wah. 2015. Time-series clustering-a decade review. Information Systems 53 (2015), 16-38.   
[4] Saeed Aghabozorgi and Ying Wah Teh. 2014. Stock market co-movement assessment using a three-phase clustering method. Expert Systems with Applications 41, 4 (2014), 1301-1314.   
[5] Saeed Aghabozorgi, Teh Ying Wah, Tutut Herawan, Hamid A Jalab, Mohammad Amin Shaygan, and Alireza Jalali. 2014. A hybrid algorithm for clustering of time series data based on affinity search technique. The Scientific World Journal 2014 (2014).   
[6] Rakesh Agrawal, Christos Faloutsos, and Arun Swami. 1993. Efficient similarity search in sequence databases. In International conference on foundations of data organization and algorithms. Springer, 69-84.   
[7] Ali Alqahtani, Mohammed Ali, Xianghua Xie, and Mark W Jones. 2021. Deep time-series clustering: A review. *Electronics* 10, 23 (2021), 3001.   
[8] Enrique Amigó, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information retrieval 12 (2009), 461-486.

[9] Gaurangi Anand and Richi Nayak. 2020. Unsupervised visual time-series representation learning and clustering. In Neural Information Processing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 18-22, 2020, Proceedings, Part V 27. Springer, 832-840.   
[10] Henrik Andre-Jönsson and Dushan Z Badal. 1997. Using signature files for querying time-series data. In *Principles of Data Mining and Knowledge Discovery: First European Symposium, PKDD'97 Trondheim, Norway*, June 24-27, 1997 Proceedings 1. Springer, 211-220.   
[11] Mihael Ankerst, Markus M Breunig, Hans-Peter Kriegel, and Jorg Sander. 1999. OPTICS: Ordering points to identify the clustering structure. ACM Sigmod record 28, 2 (1999), 49-60.   
[12] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Syndar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. 2024. Chronos: Learning the Language of Time Series. Transactions on Machine Learning Research (2024). https://openreview.net/forum?id=gerNCVqqtR   
[13] David Arthur and Sergei Vassilvitskii. 2007. K-means++ the advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. 1027-1035.   
[14] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. 2018. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075 (2018).   
[15] Anthony Bagnall and Gareth Janacek. 2005. Clustering time series with clipped data. Machine learning 58, 2 (2005), 151-178.   
[16] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).   
[17] Dana H Ballard. 1987. Modular learning in neural networks. In Proceedings of the sixth National Conference on artificial intelligence-volume 1. 279-284.   
[18] Mohini Bariya, Alexandra von Meier, John Paparrizos, and Michael J Franklin. 2021. k-shapestream: Probabilistic streaming clustering for electric grid events. In 2021 IEEE Madrid PowerTech. IEEE, 1-6.   
[19] Ildar Batyrshin. 2013. Constructing time series shape association measures: Minkowski distance and data standardization. In 2013 BRICS congress on computational intelligence and 11th Brazilian congress on computational intelligence. IEEE, 204-212.   
[20] Leonard E Baum et al. 1972. An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes. Inequalities 3, 1 (1972), 1-8.   
[21] Mustafa Gokce Baydogan, George Runger, and Eugene Tuv. 2013. A bag-of-features framework to classify time series. IEEE transactions on pattern analysis and machine intelligence 35, 11 (2013), 2796-2802.   
[22] Suzanne Becker. 1991. Unsupervised learning procedures for neural networks. International Journal of Neural Systems 2, 01n02 (1991), 17-33.   
[23] Nurjahan Begum, Liudmila Ulanova, Jun Wang, and Eamonn Keogh. 2015. Accelerating dynamic time warping clustering with a novel admissible pruning strategy. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 49-58.   
[24] Jan Beran and Guerino Mazzola. 1999. Visualizing the relationship between two time series by hierarchical smoothing models. Journal of Computational and Graphical Statistics 8, 2 (1999), 213-238.   
[25] Donald J Berndt and James Clifford. 1994. Using dynamic time warping to find patterns in time series. In Proceedings of the 3rd international conference on knowledge discovery and data mining. 359-370.   
[26] James C Bezdek. 2013. Pattern recognition with fuzzy objective function algorithms. Springer Science & Business Media.   
[27] Christophe Biernacki, Gilles Celeux, and Gérard Govaert. 2000. Assessing a mixture model for clustering with the integrated completed likelihood. IEEE transactions on pattern analysis and machine intelligence 22, 7 (2000), 719-725.   
[28] Christopher M Bishop and Nasser M Nasrabadi. 2006. Pattern recognition and machine learning. Vol. 4. Springer.   
[29] Deyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui. 2020. Structural deep clustering network. In Proceedings of the web conference 2020. 1400-1410.   
[30] Fabrizio Bonacina, Eric Stefan Miele, and Alessandro Corsini. 2020. Time series clustering: a complex network-based approach for feature selection in multi-sensor data. Modelling 1, 1 (2020), 1-21.   
[31] Paul Boniol, John Paparrizos, Yuhao Kang, Themis Palpanas, Ruey S Tsay, Aaron J Elmore, and Michael J Franklin. 2022. Theseus: navigating the labyrinth of time-series anomaly detection. Proceedings of the VLDB Endowment 15, 12 (2022), 3702-3705.   
[32] Paul Boniol, John Paparrizos, and Themis Palpanas. 2023. New Trends in Time Series Anomaly Detection.. In EDBT. 847-850.   
[33] Paul Boniol, John Paparrizos, and Themis Palpanas. 2024. An Interactive Dive into Time-Series Anomaly Detection. In 2024 IEEE 40th International Conference on Data Engineering (ICDE).   
[34] Paul Boniol, John Paparrizos, Themis Palpanas, and Michael J Franklin. 2021. Sand in action: subsequence anomaly detection for streams. Proceedings of the VLDB Endowment 14, 12 (2021), 2867-2870.   
[35] Paul Boniol, John Paparrizos, Themis Palpanas, and Michael J Franklin. 2021. SAND: streaming subsequence anomaly detection. Proceedings of the VLDB Endowment 14, 10 (2021), 1717-1729.   
[36] Paul Boniol, Emmanouil Sylligardos, John Paparrizos, Panos Trahanias, and Themis Palpanas. 2024. ADecimo: Model Selection for Time Series Anomaly Detection. In 2024 IEEE 40th International Conference on Data Engineering (ICDE).   
[37] Kin-Pong Chan and Ada Wai-Chee Fu. 1999. Efficient time series matching by wavelets. In Proceedings 15th International Conference on Data Engineering (Cat. No. 99CB36337). IEEE, 126-133.

[38] Yuan-Chi Chang, Dharmashankar Subramanian, Raju Pavuluri, and Timothy Dinger. 2022. Time series representation learning with contrastive triplet selection. In 5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD). 46-53.   
[39] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597-1607.   
[40] Yanping Chen, Eamonn Keogh, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, and Gustavo Batista. 2015. The UCR Time Series Classification Archive. www.cs.ucr.edu/~eamonn/time_series_data/.   
[41] Yixin Chen and Li Tu. 2007. Density-based clustering for real-time stream data. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining. 133-142.   
[42] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014).   
[43] Kushan Ajay Choksi, Sonal Jain, and Naran M Pindoriya. 2020. Feature based clustering technique for investigation of domestic load profiles and probabilistic variation assessment: Smart meter dataset. Sustainable Energy, Grids and Networks 22 (2020), 100346.   
[44] Maximilian Christ, Nils Braun, Julius Neuffer, and Andreas W Kempa-Liehr. 2018. Time series feature extraction on basis of scalable hypothesis tests (tsfresh-a python package). Neurocomputing 307 (2018), 72-77.   
[45] Maximilian Christ, Andreas W Kempa-Liehr, and Michael Feindt. 2016. Distributed and parallel time series feature extraction for industrial big data applications. arXiv preprint arXiv:1610.07717 (2016).   
[46] Selina Chu, Eamonn Keogh, David Hart, and Michael Pazzani. 2002. Iterative deepening dynamic time warping for time series. In Proceedings of the 2002 SIAM International Conference on Data Mining. SIAM, 195-212.   
[47] Robert Darkins, Emma J Cooke, Zoubin Ghahramani, Paul DW Kirk, David L Wild, and Richard S Savage. 2013. Accelerating Bayesian hierarchical clustering of time series data with a randomised algorithm. PloS one 8, 4 (2013), e59795.   
[48] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. 2024. A decoder-only foundation model for time-series forecasting. In *Forty-first International Conference on Machine Learning*. https://openreview.net/forum?id=jn2iTjas6h   
[49] David L Davies and Donald W Bouldin. 1979. A cluster separation measure. IEEE transactions on pattern analysis and machine intelligence 2 (1979), 224-227.   
[50] Rui Ding, Qiang Wang, Yingnong Dang, Qiang Fu, Haidong Zhang, and Dongmei Zhang. 2015. Yading: Fast clustering of large-scale time series data. Proceedings of the VLDB Endowment 8, 5 (2015), 473-484.   
[51] Joseph C Dunn. 1973. A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters. (1973).   
[52] Joseph C Dunn. 1974. Well-separated clusters and optimal fuzzy partitions. Journal of cybernetics 4, 1 (1974), 95-104.   
[53] Adam Dziedzic, John Paparrizos, Sanjay Krishnan, Aaron Elmore, and Michael Franklin. 2019. Band-limited training and inference for convolutional neural networks. In International Conference on Machine Learning. PMLR, 1745-1754.   
[54] Jens E d'Hondt, Odysseas Papapetrou, and John Paparrizos. 2024. Beyond the Dimensions: A Structured Evaluation of Multivariate Time Series Distance Measures. In 2024 IEEE 40th International Conference on Data Engineering Workshops (ICDEW). IEEE, 107-112.   
[55] Hussein El Amouri, Thomas Lampert, Pierre Gançarski, and Clément Mallet. 2023. Constrained DTW preserving shapelets for explainable time-Series clustering. Pattern Recognition 143 (2023), 109804.   
[56] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-Series Representation Learning via Temporal and Contextual Contrasting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IFCAI-21. 2352-2359.   
[57] Jonatan Enes, Roberto R Exposito, Jose Fuentes, Javier Lopez Cacheiro, and Juan Tourino. 2023. A pipeline architecture for feature-based unsupervised clustering using multivariate time series from HPC jobs. Information Fusion 93 (2023), 1-20.   
[58] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise.. In kdd, Vol. 96. 226-231.   
[59] Carolina Euan, Hernando Ombao, and Joaquin Ortega. 2018. The hierarchical spectral merger algorithm: a new time series clustering procedure. Journal of Classification 35 (2018), 71-99.   
[60] Christos Faloutsos, Mudumbai Ranganathan, and Yannis Manolopoulos. 1994. Fast subsequence matching in time-series databases. ACM Sigmoid Record 23, 2 (1994), 419-429.   
[61] Vincent Fortuin, Matthias Hüser, Francesco Locatello, Heiko Strathmann, and Gunnar Ratsch. 2019. Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series. In International Conference on Learning Representations. https://openreview.net/forum?id=rygjcsR9Y7   
[62] Vanel Steve Siyou Fotso, Engelbert Mephu Nguifo, and Philippe Vaslin. 2020. Frobenius correlation based u-shapelets discovery for time series clustering. Pattern Recognition 103 (2020), 107301.   
[63] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. 2019. Unsupervised scalable representation learning for multivariate time series. Advances in neural information processing systems 32 (2019).   
[64] Tak-chung Fu, Fu-lai Chung, Vincent Ng, and Robert Luk. 2001. Pattern discovery from stock time series using self-organizing maps. In Workshop Notes of KDD2001 Workshop on Temporal Data Mining, Vol. 1. Citeseer.   
[65] Ben D Fulcher. 2018. Feature-based time-series analysis. In Feature engineering for machine learning and data analytics. CRC press, 87-116.   
[66] Ben D Fulcher and Nick S Jones. 2014. Highly comparative feature-based time-series classification. IEEE Transactions on Knowledge and Data Engineering 26, 12 (2014), 3026-3037.

Manuscript submitted to ACM

[67] Ben D Fulcher and Nick S Jones. 2017. hctsa: A computational framework for automated time-series phenotyping using massive feature extraction. Cell systems 5, 5 (2017), 527-531.   
[68] Boyan Gao, Yongxin Yang, Henry Gouk, and Timothy M Hospedales. 2020. Deep clustering with concrete k-means. In ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 4252-4256.   
[69] Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros Tsiligkaridis, and Marinka Zitnik. 2024. UniTS: Building a Unified Time Series Model. arXiv (2024). https://arxiv.org/pdf/2403.00131.pdf   
[70] Kamran Ghasedi, Xiaoqian Wang, Cheng Deng, and Heng Huang. 2019. Balanced self-paced learning for generative adversarial clustering network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4391-4400.   
[71] Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng, Weidong Cai, and Heng Huang. 2017. Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization. In Proceedings of the IEEE international conference on computer vision. 5736-5745.   
[72] Shima Ghassempour, Federico Girosi, and Anthony Maeder. 2014. Clustering multivariate time series using hidden Markov models. International journal of environmental research and public health 11, 3 (2014), 2741-2763.   
[73] Rafael Giusti and Gustavo EAPA Batista. 2013. An empirical comparison of dissimilarity measures for time series classification. In 2013 Brazilian Conference on Intelligent Systems. IEEE, 82-88.   
[74] Rahul Goel, Sandeep Soni, Naman Goyal, John Paparrizos, Hanna Wallach, Fernando Diaz, and Jacob Eisenstein. 2016. The social dynamics of language change in online networks. In Social Informatics: 8th International Conference, SocInfo 2016, Bellevue, WA, USA, November 11-14, 2016, Proceedings, Part I 8. Springer, 41-57.   
[75] Xavier Golay, Spyros Kollias, Gautier Stoll, Dieter Meier, Anton Valavanis, and Peter Boesiger. 1998. A new correlation-based fuzzy logic clustering algorithm for FMRI. Magnetic resonance in medicine 40, 2 (1998), 249-260.   
[76] Gene H Golub and Charles F Van Loan. 2013. Matrix computations. JHU press.   
[77] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information Processing Systems, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.), Vol. 27. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f618f06494c97b1afcf3-Paper.pdf   
[78] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. 2024. MOMENT: A Family of Open Time-series Foundation Models. In International Conference on Machine Learning.   
[79] He-Shan Guam and Qing-Shan Jiang. 2007. Cluster financial time series for portfolio. In 2007 international conference on wavelet analysis and pattern recognition, Vol. 2. IEEE, 851-856.   
[80] David Guijo-Rubio, Antonio Manuel Durán-Rosal, Pedro Antonio Gutierrez, Alicia Troncoso, and César Hervás-Martínez. 2020. Time-series clustering based on the characterization of segment typologies. IEEE transactions on cybernetics 51, 11 (2020), 5409-5422.   
[81] Francesco Gullo, Giovanni Ponti, Andrea Tagarelli, Giuseppe Tradigo, and Pierangelo Veltri. 2012. A time series approach for clustering mass spectrometry data. Journal of Computational Science 3, 5 (2012), 344-355.   
[82] Chonghui Guo, Hongfeng Jia, and Na Zhang. 2008. Time series clustering based on ICA for stock data analysis. In 2008 4th international conference on wireless communications, networking and mobile computing. IEEE, 1-4.   
[83] Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. 2017. Improved Deep Embedded Clustering with Local Structure Preservation.. In IjCAI, 1753-1759.   
[84] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems 30 (2017).   
[85] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.   
[86] Alexander Hinneburg and Hans-Henning Gabriel. 2007. Denclave 2.0: Fast clustering based on kernel density estimation. In International symposium on intelligent data analysis. Springer, 70-80.   
[87] Alexander Hinneburg and Daniel A Keim. 1998. An efficient approach to clustering in large multimedia databases with noise. In Knowledge Discovery and Datamining (KDD'98). 58-65.   
[88] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780.   
[89] Christopher Holder, Matthew Middlehurst, and Anthony Bagnall. 2023. A review and evaluation of elastic distance functions for time series clustering. Knowledge and Information Systems (2023), 1-45.   
[90] John J Hopfield. 1982. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences 79, 8 (1982), 2554-2558.   
[91] Maomao Hu, Dongjiao Ge, Rory Telford, Bruce Stephen, and David CH Wallom. 2021. Classification and characterization of intra-day load curves of PV and non-PV households using interpretable feature extraction and feature-based clustering. *Sustainable Cities and Society* 75 (2021), 103380.   
[92] Fanling Huang and Yangdong Deng. 2023. TCGAN: Convolutional Generative Adversarial Network for time series classification and clustering. Neural Networks 165 (2023), 868-883.   
[93] Hao Huang, Tapan Shah, and Shinjae Yoo. 2022. Deep Time Series Sketching and Its Application on Industrial Time Series Clustering. In 2022 IEEE International Conference on Big Data (Big Data). IEEE, 1997-2006.   
[94] Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of classification 2 (1985), 193-218.   
[95] Rob J Hyndman and George Athanasopoulos. 2018. Forecasting: principles and practice. OTexts.

Manuscript submitted to ACM

[96] Dino Ienco and Roberto Interdonato. 2020. Deep multivariate time series embedding clustering via attentive-gated autoencoder. In Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I 24. Springer, 318–329.   
[97] Dino Ienco and Roberto Interdonato. 2023. Deep semi-supervised clustering for multi-variate time-series. Neurocomputing 516 (2023), 36-47.   
[98] Dino Ienc and Ruggero G Pensa. 2019. Deep triplet-driven semi-supervised embedding clustering. In Discovery Science: 22nd International Conference, DS 2019, Split, Croatia, October 28-30, 2019, Proceedings 22. Springer, 220-234.   
[99] Félix Iglesias and Wolfgang Kastner. 2013. Analysis of similarity measures in times series clustering for the discovery of building energy patterns. Energies 6, 2 (2013), 579-597.   
[100] Anil K Jain and Richard C Dubes. 1988. Algorithms for clustering data. Prentice-Hall, Inc.   
[101] Ali Javed, Byung Suk Lee, and Donna M Rizzo. 2020. A benchmark study on time series clustering. Machine Learning with Applications 1 (2020), 100001.   
[102] Hoyoung Jeung, Sofiane Sarni, Ioannis Paparrizos, Saket Sathe, Karl Aberer, Nicholas Dawes, Thanasis G Papaioannou, and Michael Lehning. 2010. Effective metadata management in federated sensor networks. In 2010 IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing. IEEE, 107-114.   
[103] Min Ji, Fuding Xie, and Yu Ping. 2013. A dynamic fuzzy cluster algorithm for time series. In Abstract and Applied Analysis, Vol. 2013. Hindawi.   
[104] Hao Jiang, Chunwei Liu, Qi Jin, John Paparrizos, and Aaron J Elmore. 2020. Pids: attribute decomposition for improved compression and query performance in columnar storage. Proceedings of the VLDB Endowment 13, 6 (2020), 925-938.   
[105] Hao Jiang, Chunwei Liu, John Paparrizos, Andrew A Chien, Jihong Ma, and Aaron J Elmore. 2021. Good to the last bit: Data-driven encoding with codecdb. In Proceedings of the 2021 International Conference on Management of Data. 843-856.   
[106] Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. 2017. Variational deep embedding: an unsupervised and generative approach to clustering. In Proceedings of the 26th International Joint Conference on Artificial Intelligence. 1965-1972.   
[107] CI Johnpaul, Munaga VNK Prasad, S Nickolas, and GR Gangadharan. 2020. Trendlets: A novel probabilistic representational structures for clustering the time series data. Expert Systems with Applications 145 (2020), 113119.   
[108] Eleanna Kafeza, Gerasimos Rompolas, Sokratis Kyriazidis, and Christos Makris. 2022. Time-Series Clustering for Determining Behavioral-Based Brand Loyalty of Users Across Social Media. IEEE Transactions on Computational Social Systems (2022).   
[109] Yoshihide Kakizawa, Robert H Shumway, and Masanobu Taniguchi. 1998. Discrimination and clustering for multivariate time series. J. Amer. Statist. Assoc. 93, 441 (1998), 328-340.   
[110] Konstantinos Kalpakis, Dhiral Gada, and Vasundhara Puttagunta. 2001. Distance measures for effective clustering of ARIMA time-series. In Proceedings 2001 IEEE international conference on data mining. IEEE, 273-280.   
[111] Antonina Kashtalian and Tomas Sochor. 2021. K-means Clustering of Honeynet Data with Unsupervised Representation Learning.. In IntelITISIS, 439-449.   
[112] Leonard Kaufman and Peter J Rousseeuw. 2009. Finding groups in data: an introduction to cluster analysis. John Wiley & Sons.   
[113] Kyoji Kawagoe and Tomohiro Ueda. 2002. A similarity search method of time series data with combination of Fourier and wavelet transforms. In Proceedings Ninth International Symposium on Temporal Representation and Reasoning. IEEE, 86-92.   
[114] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra. 2001. Dimensionality reduction for fast similarity search in large time series databases. Knowledge and information Systems 3 (2001), 263-286.   
[115] Eamonn Keogh and Jessica Lin. 2005. Clustering of time-series subsequences is meaningless: implications for previous and future research. Knowledge and information systems 8 (2005), 154-177.   
[116] Eamonn Keogh, Stefano Lonardi, and Chotirat Ann Ratanamahatana. 2004. Towards parameter-free data mining. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. 206-215.   
[117] Eamonn J Keogh and Michael J Pazzani. 1998. An Enhanced Representation of Time Series Which Allows Fast and Accurate Classification, Clustering and Relevance Feedback.. In Kdd, Vol. 98. 239-243.   
[118] Eamonn J Keogh and Michael J Pazzani. 2000. A simple dimensionality reduction technique for fast similarity search in large time series databases. In Pacific-Asia conference on knowledge discovery and data mining. Springer, 122–133.   
[119] Mohammad Mahmudur Rahman Khan, Md Abu Bakr Siddique, Rezoana Bente Arif, and Mahjabin Rahman Oishe. 2018. ADBSCAN: Adaptive density-based spatial clustering of applications with noise for identifying clusters with varying densities. In 2018 4th international conference on electrical engineering and information & communication technology (iCEEiCT). IEEE, 107-111.   
[120] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).   
[121] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations. https://openreview.net/forum?id=SJU4ayYgl   
[122] Dani Kiyasseh, Tingting Zhu, and David Clifton. 2021. CROCS: clustering and retrieval of cardiac signals based on patient disease class, sex, and age. Advances in Neural Information Processing Systems 34 (2021), 15557-15569.   
[123] Katarina Kosmelj and Vladimir Batagelj. 1990. Cross-sectional approach for clustering time varying data. Journal of Classification 7, 1 (1990), 99-109.   
[124] Mark A Kramer. 1991. Nonlinear principal component analysis using autoassociative neural networks. AIChE journal 37, 2 (1991), 233-243.

Manuscript submitted to ACM

[125] Hans-Peter Kriegel and Martin Pfeifle. 2005. Density-based clustering of uncertain data. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 672-677.   
[126] Sanjay Krishnan, Aaron J Elmore, Michael Franklin, John Paparrizos, Zechao Shang, Adam Dziedzic, and Rui Liu. 2019. Artificial intelligence in resource-constrained and shared environments. ACM SIGOPS Operating Systems Review 53, 1 (2019), 1-6.   
[127] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25 (2012).   
[128] Mahesh Kumar, Nitin R Patel, and Jonathan Woo. 2002. Clustering seasonality patterns in the presence of errors. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. 557-563.   
[129] Vladimir Kurbalija, Charlotte von Bernstorff, Hans-Dieter Burkhard, Jens Nachtwei, Mirjana Ivanovic, and Lidija Fodor. 2012. Time-Series Mining in a Psychological Domain. In Proceedings of the Fifth Balkan Conference in Informatics (Novi Sad, Serbia) (BCI '12). Association for Computing Machinery, New York, NY, USA, 58-63. https://doi.org/10.1145/2371316.2371328   
[130] Baptiste Lafabregue, Jonathan Weber, Pierre Gançarski, and Germain Forestier. 2021. End-to-end deep representation learning for time series clustering: a comparative study. Data Mining and Knowledge Discovery (2021), 1-53.   
[131] Qi Lei, Jinfeng Yi, Roman Vaculin, Lingfei Wu, and Inderjit S. Dhillon. 2019. Similarity Preserving Representation Learning for Time Series Clustering. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (Macao, China) (IFCAI'19). AAAI Press, 2845-2851.   
[132] Aimin Li, Siqi Xiong, Junhuai Li, Saurav Mallik, Yajun Liu, Rong Fei, Hongfang Zhou, and Guangming Liu. 2022. AngClust: angle feature-based clustering for short time series gene expression profiles. IEEE/ACM Transactions on Computational Biology and Bioinformatics 20, 2 (2022), 1574-1580.   
[133] Cen Li and Gautam Biswas. 1999. Temporal pattern generation using hidden Markov model based unsupervised classification. In International Symposium on Intelligent Data Analysis. Springer, 245-256.   
[134] Cen Li, Gautam Biswas, Mike Dale, and Pat Dale. 2001. Building models of ecological dynamics using HMM based temporal data clustering—a preliminary study. In International Symposium on Intelligent Data Analysis. Springer, 53-62.   
[135] Guozhong Li, Byron Choi, Jianliang Xu, Sourav S Bhowmick, Kwok-Pan Chun, and Grace Lai-Hung Wong. 2021. ShapeNet: A Shapelet-Neural Network Approach for Multivariate Time Series Classification. Proceedings of the AAAI Conference on Artificial Intelligence 35, 9 (May 2021), 8375-8383. https://doi.org/10.1609/aaai.v35i9.17018   
[136] Hailin Li and Manhua Chen. 2023. Time series clustering based on normal cloud model and complex network. Applied Soft Computing 148 (2023), 110876.   
[137] Hailin Li, Xianli Wu, Xiaoji Wan, and Weibin Lin. 2022. Time series clustering via matrix profile and community detection. Advanced Engineering Informatics 54 (2022), 101771.   
[138] Junnan Li, Pan Zhou, Caiming Xiong, and Steven C.H. Hoi. 2021. Prototypical Contrastive Learning of Unsupervised Representations. In ICLR.   
[139] Xiaosheng Li, Jessica Lin, and Liang Zhao. 2021. Time series clustering in linear time complexity. Data Mining and Knowledge Discovery 35 (2021), 2369-2388.   
[140] Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, and Bhiksha Raj. 2023. Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 2283–2296. https://doi.org/10.18653/v1/2023.emnlp-main.140   
[141] Zhiyu Liang, Jianfeng Zhang, Chen Liang, Hongzhi Wang, Zheng Liang, and Lujia Pan. 2023. Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning. arXiv preprint arXiv:2305.18888 (2023).   
[142] TW Liao, B Bolt, J Forester, E Hailman, C Hansen, RC Kaste, and J O'May. 2002. Understanding and projecting the battle state. In 23rd Army Science Conference, Orlando, FL, Vol. 25.   
[143] T Warren Liao. 2005. Clustering of time series data—a survey. Pattern recognition 38, 11 (2005), 1857-1874.   
[144] Jessica Lin, Eamonn Keogh, Stefano Lonardi, and Bill Chiu. 2003. A symbolic representation of time series, with implications for streaming algorithms. In Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery. 2-11.   
[145] Jessica Lin, Eamonn Keogh, Li Wei, and Stefano Lonardi. 2007. Experiencing SAX: a novel symbolic representation of time series. Data Mining and knowledge discovery 15, 2 (2007), 107-144.   
[146] Jessica Lin, Michail Vlachos, Eamonn Keogh, and Dimitrios Gunopoulos. 2004. Iterative incremental clustering of time series. In International Conference on Extending Database Technology. Springer, 106-122.   
[147] Chunwei Liu, Hao Jiang, John Paparrizos, and Aaron J Elmore. 2021. Decomposed bounded floats for fast compression and queries. Proceedings of the VLDB Endowment 14, 11 (2021), 2586-2598.   
[148] Chunwei Liu, John Paparrizos, and Aaron J Elmore. 2024. Adaedge: A dynamic compression selection framework for resource constrained devices. In 2024 IEEE 40th International Conference on Data Engineering (ICDE). IEEE, 1506-1519.   
[149] Qinghua Liu, Paul Boniol, Themis Palpanas, and John Paparrizos. 2024. Time-Series Anomaly Detection: Overview and New Trends. Proceedings of the VLDB Endowment (PVLDB) 17, 12 (2024), 4229-4232.   
[150] Qinghua Liu and John Paparrizos. 2024. The Elephant in the Room: Towards A Reliable Time-Series Anomaly Detection Benchmark. In NeurIPS 2024.

[151] Shinan Liu, Tarun Mangla, Ted Shaowang, Jinjin Zhao, John Paparrizos, Sanjay Krishnan, and Nick Feamster. 2023. Amir: Active multimodal interaction recognition from video and network traffic in connected environments. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 7, 1 (2023), 1-26.   
[152] Arnaud Lod, Simon Malinowski, Romain Tavenard, and Laurent Amsaleg. 2017. Learning DTW-preserving shapelets. In Advances in Intelligent Data Analysis XVI: 16th International Symposium, IDA 2017, London, UK, October 26-28, 2017, Proceedings 16. Springer, 198-209.   
[153] David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision 60 (2004), 91-110.   
[154] Carl H Lubba, Sarab S Sethi, Philip Knaute, Simon R Schultz, Ben D Fulcher, and Nick S Jones. 2019. catch22: CAnonical Time-series CCharacteristics: Selected through highly comparative time-series analysis. Data Mining and Knowledge Discovery 33, 6 (2019), 1821-1852.   
[155] Maciej Luczak. 2016. Hierarchical clustering of time series data with parametric derivative dynamic time warping. Expert Systems with Applications 62 (2016), 116-130.   
[156] Linghao Luo and Shu Lv. 2020. An accelerated u-shapelet time series clustering method with lsh. In Journal of Physics: Conference Series, Vol. 1631. IOP Publishing, 012077.   
[157] Zhixue Luo, Lin Zhang, Na Liu, and Ye Wu. 2023. Time series clustering of COVID-19 pandemic-related data. Data Science and Management 6, 2 (2023), 79-87.   
[158] Qianli Ma, Chuxin Chen, Sen Li, and Garrison W. Cottrell. 2021. Learning Representations for Incomplete Time Series Clustering. Proceedings of the AAAI Conference on Artificial Intelligence 35, 10 (May 2021), 8837-8846. https://doi.org/10.1609/aaaai.v35i10.17070   
[159] Qianli Ma, Chuxin Chen, Sen Li, and Garrison W. Cottrell. 2021. Learning Representations for Incomplete Time Series Clustering. Proceedings of the AAAI Conference on Artificial Intelligence 35, 10 (May 2021), 8837-8846. https://doi.org/10.1609/aaai.v35i10.17070   
[160] Qianli Ma, Jiawei Zheng, Sen Li, and Gary W Cottrell. 2019. Learning representations for time series clustering. Advances in neural information processing systems 32 (2019).   
[161] James MacQueen et al. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, Vol. 1. Oakland, CA, USA, 281-297.   
[162] Naveen Sai Madiraju. 2018. Deep temporal clustering: Fully unsupervised learning of time-domain features. Ph.D. Dissertation. Arizona State University.   
[163] Elizabeth Ann Maharaj. 2000. Cluster of Time Series. Journal of Classification 17, 2 (2000).   
[164] Simon Malinowski, Thomas Guyet, René Quiniou, and Romain Tavenard. 2013. 1d-sax: A novel symbolic representation for time series. In International Symposium on Intelligent Data Analysis. Springer, 273-284.   
[165] Laura Manduchi, Matthias Hüser, Julia Vogt, Gunnar Ratsch, and Vincent Fortuin. 2019. DPSOM: Deep probabilistic clustering with self-organizing maps. arXiv preprint arXiv:1910.01590 (2019).   
[166] Kathy McKeown, Hal Daume III, Snigdha Chaturvedi, John Paparrizos, Kapil Thadani, Pablo Barrio, Or Biran, Suvarna Bothe, Michael Collins, Kenneth R Fleischmann, et al. 2016. Predicting the impact of scientific concepts using full-text features. Journal of the Association for Information Science and Technology 67, 11 (2016), 2684-2696.   
[167] Marina Meilă. 2007. Comparing clusterings—an information based distance. Journal of multivariate analysis 98, 5 (2007), 873-895.   
[168] Qianwen Meng, Hangwei Qian, Yong Liu, Lizhen Cui, Yonghui Xu, and Zhiqi Shen. 2023. MHCCL: masked hierarchical cluster-wise contrastive learning for multivariate time series. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 9153-9161.   
[169] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems 26 (2013).   
[170] Carla S Möller-Levet, Frank Klawonn, Kwang-Hyun Cho, and Olaf Wolkenhauer. 2003. Fuzzy clustering of short time-series and unevenly distributed sampling points. In International symposium on intelligent data analysis. Springer, 330-340.   
[171] Tim Oates, Laura Firoiu, and Paul R Cohen. 1999. Clustering time series with hidden markov models and dynamic time warping. In Proceedings of the IFCAI-99 workshop on neural, symbolic and reinforcement learning methods for sequence learning. CiteSeer, 17-21.   
[172] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).   
[173] Mert Ozer, Anna Sapienza, Andres Abeliuk, Goran Muric, and Emilio Ferrara. 2020. Discovering patterns of online popularity from time series. Expert Systems with Applications 151 (2020), 113337.   
[174] Ioannis Paparrizos. 2018. Fast, scalable, and accurate algorithms for time-series analysis. Ph.D. Dissertation. Columbia University.   
[175] John Paparrizos, Paul Boniol, Themis Palpanas, Ruey S Tsay, Aaron Elmore, and Michael J Franklin. 2022. Volume under the surface: a new accuracy evaluation measure for time-series anomaly detection. Proceedings of the VLDB Endowment 15, 11 (2022), 2774-2787.   
[176] John Paparrizos, Ikraduya Edian, Chunwei Liu, Aaron J Elmore, and Michael J Franklin. 2022. Fast adaptive similarity search through variance-aware quantization. In 2022 IEEE 38th International Conference on Data Engineering (ICDE). IEEE, 2969-2983.   
[177] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series representation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 1762-1777.   
[178] John Paparrizos and Luis Gravano. 2015. k-shape: Efficient and accurate clustering of time series. In Proceedings of the 2015 ACM SIGMOD international conference on management of data. 1855-1870.   
[179] John Paparrizos and Luis Gravano. 2017. Fast and accurate time-series clustering. ACM Transactions on Database Systems (TODS) 42, 2 (2017), 1-49.

[180] John Paparrizos, Yuhao Kang, Paul Boniol, Ruey S Tsay, Themis Palpanas, and Michael J Franklin. 2022. TSB-UAD: an end-to-end benchmark suite for univariate time-series anomaly detection. Proceedings of the VLDB Endowment 15, 8 (2022), 1697-1711.   
[181] John Paparrizos, Chunwei Liu, Bruno Barbarioli, Johnny Hwang, Ikraduya Edian, Aaron J Elmore, Michael J Franklin, and Sanjay Krishnan. 2021. VergeDB: A Database for IoT Analytics on Edge Devices.. In CIDR.   
[182] John Paparrizos, Chunwei Liu, Aaron J Elmore, and Michael J Franklin. 2020. Debunking four long-standing misconceptions of time-series distance measures. In Proceedings of the 2020 ACM SIGMOD international conference on management of data. 1887-1905.   
[183] John Paparrizos, Chunwei Liu, Aaron J. Elmore, and Michael J. Franklin. 2023. Querying Time-Series Data: A Comprehensive Comparison of Distance Measures. IEEE Data Eng. Bull. 46, 3 (2023), 69-88. http://sites.computer.org/debull/A23sept/p69.pdf   
[184] John Paparrizos and Sai Prasanna Teja Reddy. 2023. Odyssey: An engine enabling the time-series clustering journey. Proceedings of the VLDB Endowment 16, 12 (2023), 4066-4069.   
[185] John Paparrizos, Ryen W White, and Eric Horvitz. 2016. Detecting devastating diseases in search logs. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 559-568.   
[186] John Paparrizos, Ryen W White, and Eric Horvitz. 2016. Screening for pancreatic adenocarcinoma using signals from web search logs: Feasibility study and results. Journal of oncology practice 12, 8 (2016), 737-744.   
[187] John Paparrizos, Kaize Wu, Aaron Elmore, Christos Faloutsos, and Michael J Franklin. 2023. Accelerating similarity search for elastic measures: A study and new generalization of lower bounding distances. Proceedings of the VLDB Endowment 16, 8 (2023), 2019-2032.   
[188] Nicos G Pavlidis, Vassilis P Plagianakos, Dimitris K Tasoulis, and Michael N Vrahatis. 2006. Financial forecasting through unsupervised clustering and neural networks. Operational Research 6, 2 (2006), 103-127.   
[189] Karl Pearson. 1901. LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science 2, 11 (1901), 559-572.   
[190] François Petitjean, Alain Ketterlin, and Pierre Gançarski. 2011. A global averaging method for dynamic time warping, with applications to clustering. Pattern recognition 44, 3 (2011), 678-693.   
[191] Domenico Piccolo. 1990. A distance measure for classifying ARIMA models. Journal of time series analysis 11, 2 (1990), 153-164.   
[192] Shai Policker and Amir B Geva. 2000. Nonstationary time series analysis by temporal clustering. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 30, 2 (2000), 339-343.   
[193] Jiang Qian, Marisa Doolled-Filhart, Jimmy Lin, Haiyuan Yu, and Mark Gerstein. 2001. Beyond synexpression relationships: local clustering of time-shifted and inverted gene expression profiles identifies new, biologically relevant interactions. Journal of molecular biology 314, 5 (2001), 1053-1066.   
[194] Marco Ramoni, Paola Sebastiani, and Paul Cohen. 2000. Multivariate clustering by dynamics. In AAAI/IAAI. 633-638.   
[195] William M Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association 66, 336 (1971), 846-850.   
[196] Teemu Räsänen and Mikko Kolehmainen. 2009. Feature-based clustering for electricity use time series data. In Adaptive and Natural Computing Algorithms: 9th International Conference, ICANNGA 2009, Kuopio, Finland, April 23-25, 2009, Revised Selected Papers 9. Springer, 401-412.   
[197] Chotirat Ratanamahatana, Eamonn Keogh, Anthony J Bagnall, and Stefano Lonardi. 2005. A novel bit level time series representation with implication of similarity search and clustering. In Pacific-Asia conference on knowledge discovery and data mining. Springer, 771-777.   
[198] Chotirat Ann Ratanamahatana and Eamonn Keogh. 2005. Three myths about dynamic time warping data mining. In Proceedings of the 2005 SIAM international conference on data mining. SIAM, 506-510.   
[199] LKPJ R dusseeun and P Kaufman. 1987. Clustering by means of medoids. In Proceedings of the statistical data analysis based on the L1 norm conference, neuchatel, switzerland, Vol. 31.   
[200] Lei Ren, Yongchang Wei, Jin Cui, and Yi Du. 2017. A sliding window-based multi-stage clustering and probabilistic forecasting approach for large multivariate time series data. Journal of Statistical Computation and Simulation 87, 13 (2017), 2494-2508.   
[201] Pedro Pereira Rodrigues, Joao Gama, and Joao Pedroso. 2008. Hierarchical clustering of time-series data streams. IEEE transactions on knowledge and data engineering 20, 5 (2008), 615-627.   
[202] Alex Rodriguez and Alessandro Laio. 2014. Clustering by fast search and find of density peaks. science 344, 6191 (2014), 1492-1496.   
[203] Frank Rosenblatt. 1958. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review 65, 6 (1958), 386.   
[204] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics 20 (1987), 53-65.   
[205] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating errors. nature 323, 6088 (1986), 533-536.   
[206] Nicholas Ruta, Naoko Sawada, Katy McKeough, Michael Behrisch, and Johanna Beyer. 2019. Sax navigator: Time series exploration through hierarchical clustering. In 2019 IEEE Visualization Conference (VIS). IEEE, 236-240.   
[207] Hiroaki Sakoe and Seibi Chiba. 1978. Dynamic programming algorithm optimization for spoken word recognition. IEEE transactions on acoustics, speech, and signal processing 26, 1 (1978), 43-49.   
[208] Patrick Schäfer and Mikael Hogqvist. 2012. SFA: a symbolic fourier approximation and index for similarity search in high dimensional datasets. In Proceedings of the 15th International Conference on Extending Database Technology (Berlin, Germany) (EDBT '12). Association for Computing

Manuscript submitted to ACM

Machinery, New York, NY, USA, 516-527. https://doi.org/10.1145/2247596.2247656   
[209] Onur Seref, Ya-Ju Fan, and Wanpracha Art Chaovalitwongse. 2014. Mathematical programming formulations and algorithms for discrete k-median clustering of time-series data. INFORMS Journal on Computing 26, 1 (2014), 160-172.   
[210] Vivek Sharma, Makarand Tapaswi, M Saqib Sarfraz, and Rainer Stiefelhagen. 2020. Clustering based contrastive learning for improving face representations. In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020). IEEE, 109-116.   
[211] CT Shaw and GP King. 1992. Using cluster analysis to classify time series. Physica D: Nonlinear Phenomena 58, 1-4 (1992), 288-298.   
[212] Jin Shieh and Eamonn Keogh. 2009. iSAX: disk-aware mining and indexing of massive time series datasets. Data Mining and Knowledge Discovery 19, 1 (2009), 24-57.   
[213] Robert H Shumway. 2003. Time-frequency clustering and discriminant analysis. Statistics & probability letters 63, 3 (2003), 307-314.   
[214] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).   
[215] Alexandra Stefan, Vassilis Athitsos, and Gautam Das. 2012. The move-split-merge metric for time series. IEEE transactions on Knowledge and Data Engineering 25, 6 (2012), 1425–1438.   
[216] Michael Steinbach, Pang-Ning Tan, Vipin Kumar, Steven Klooster, and Christopher Potter. 2003. Discovery of Climate Indices Using Clustering. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Washington, D.C.) (KDD '03). Association for Computing Machinery, New York, NY, USA, 446-455. https://doi.org/10.1145/956750.956801   
[217] Adrian Stetco, Xiao-jun Zeng, and John Keane. 2013. Fuzzy cluster analysis of financial time series and their volatility assessment. In 2013 IEEE International Conference on Systems, Man, and Cybernetics. IEEE, 91-96.   
[218] Alexander Strehl and Joydeep Ghosh. 2002. Cluster ensembles-a knowledge reuse framework for combining multiple partitions. Journal of machine learning research 3, Dec (2002), 583-617.   
[219] Colin Studholme, Derek LG Hill, and David J Hawkes. 1999. An overlap invariant entropy measure of 3D medical image alignment. Pattern recognition 32, 1 (1999), 71-86.   
[220] Numanul Subhani, Luis Rueda, Alioune Ngom, and Conrad J Burden. 2010. Multiple gene expression profile alignment for microarray time-series data clustering. Bioinformatics 26, 18 (2010), 2281-2288.   
[221] Mukund Subramaniyan, Anders Skoogh, Azam Sheikh Muhammad, Jon Bokrantz, Bjorn Johansson, and Christoph Roser. 2020. A generic hierarchical clustering approach for detecting bottlenecks in manufacturing. Journal of Manufacturing Systems 55 (2020), 143-158.   
[222] Emmanouil Sylligardos, Paul Boniol, John Paparrizos, Panos Trahanias, and Themis Palpanas. 2023. Choose wisely: An extensive evaluation of model selection for anomaly detection in time series. Proceedings of the VLDB Endowment 16, 11 (2023), 3418-3432.   
[223] Xue Wang Liang Sun Rong Jin Tian Zhou, Peisong Niu. 2023. One Fits All: Power General Time Series Analysis by Pretrained LM. In NeurIPS.   
[224] Donato Tiano, Angela Bonifati, and Raymond Ng. 2021. FeatTS: Feature-based time series clustering. In Proceedings of the 2021 International Conference on Management of Data. 2784-2788.   
[225] Donato Tiano, Angela Bonifati, and Raymond Ng. 2021. Feature-driven time series clustering. In 24th International Conference on Extending Database Technology, EDBT 2021.   
[226] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. 2021. Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding. In International Conference on Learning Representations. https://openreview.net/forum?id=8qDwejCuCN   
[227] Dat Tran and Michael Wagner. 2002. Fuzzy C-Means Clustering-Based Speaker Verification. In Advances in Soft Computing - AFSS 2002, Nikhil R. Pal and Michio Sugeno (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 318-324.   
[228] Daniel J Trosten, Andreas S Strauman, Michael Kampffmeyer, and Robert Jenssen. 2019. Recurrent deep divergence-based clustering for simultaneous feature learning and clustering of variable length time series. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 3257-3261.   
[229] Liudmila Ulanova, Nurjahan Begum, and Eamonn Keogh. 2015. Scalable clustering of time series with u-shapelets. In Proceedings of the 2015 SIAM international conference on data mining. SIAM, 900-908.   
[230] Jarke J Van Wijk and Edward R Van Selow. 1999. Cluster and calendar based visualization of time series data. In Proceedings 1999 IEEE Symposium on Information Visualization (InfoVis' 99). IEEE, 4-9.   
[231] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).   
[232] Petar Velicković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph Attention Networks. In International Conference on Learning Representations. https://openreview.net/forum?id=rfJXMpikCZ   
[233] Michail Vlachos, George Kollios, and Dimitrios Gunopoulos. 2002. Discovering similar multidimensional trajectories. In Proceedings 18th international conference on data engineering. IEEE, 673-684.   
[234] Xiaozhe Wang, Kate Smith, and Rob Hyndman. 2006. Characteristic-based clustering for time series data. Data mining and knowledge Discovery 13 (2006), 335-364.   
[235] Xiaozhe Wang, Kate A Smith, and Rob J Hyndman. 2005. Dimension reduction for clustering time series using global characteristics. In International Conference on Computational Science. Springer, 792-795.   
[236] Xiaozhe Wang, Anthony Wirth, and Liang Wang. 2007. Structure-based statistical features and multivariate time series clustering. In Seventh IEEE international conference on data mining (ICDM 2007). IEEE, 351-360.   
Manuscript submitted to ACM

[237] Joe H Ward Jr. 1963. Hierarchical grouping to optimize an objective function. Journal of the American statistical association 58, 301 (1963), 236-244.   
[238] J Wilpon and L Rabiner. 1985. A modified K-means clustering algorithm for use in isolated work recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing 33, 3 (1985), 587-594.   
[239] Axel Wismüller, Oliver Lange, Dominik R Dersch, Gerda L Leinsinger, Klaus Hahn, Benno Pütz, and Dorothee Auer. 2002. Cluster analysis of biomedical image time-series. International Journal of Computer Vision 46, 2 (2002), 103-128.   
[240] Jun Wu, Zelin Zhang, Rui Tong, Yuan Zhou, Zhengfa Hu, and Kaituo Liu. 2023. Imaging feature-based clustering of financial time series. Plos one 18, 7 (2023), e0288836.   
[241] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3733-3742.   
[242] Andreas Wunsch, Tanja Liesch, and Stefan Broda. 2022. Feature-based groundwater hydrograph clustering using unsupervised self-organizing map-ensembles. Water Resources Management 36, 1 (2022), 39–54.   
[243] Zhiwen Xiao, Huanlai Xing, Bowen Zhao, Rong Qu, Shouxi Luo, Penglin Dai, Ke Li, and Zonghai Zhu. 2023. Deep Contrastive Representation Learning with Self-Distillation. IEEE Transactions on Emerging Topics in Computational Intelligence (2023).   
[244] Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016. Unsupervised deep embedding for clustering analysis. In International conference on machine learning. PMLR, 478-487.   
[245] Yimin Xiong and Dit-Yan Yeung. 2002. Mixtures of ARMA models for model-based time series clustering. In 2002 IEEE International Conference on Data Mining, 2002. Proceedings. IEEE, 717-720.   
[246] Chenxiao Xu, Hao Huang, and Shinjae Yoo. 2021. A deep neural network for multivariate time series clustering with result interpretation. In 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 1-8.   
[247] Ziwei Xu, Xudong Shen, Yongkang Wong, and Mohan S Kankanhalli. 2021. Unsupervised motion representation learning with capsule autoencoders. Advances in Neural Information Processing Systems 34 (2021), 3205-3217.   
[248] Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. 2017. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. In international conference on machine learning. PMLR, 3861-3870.   
[249] Fan Yang, Muqiao Yang, Xiang Li, Yuxuan Wu, Zhiyuan Zhao, Bhiksha Raj, and Rita Singh. 2024. A closer look at reinforcement learning-based automatic speech recognition. Computer Speech & Language 87 (2024), 101641. https://doi.org/10.1016/j.csl.2024.101641   
[250] Jaewon Yang and Jure Leskovec. 2011. Patterns of temporal variation in online media. In Proceedings of the fourth ACM international conference on Web search and data mining. 177-186.   
[251] Ling Yang and Shenda Hong. 2022. Unsupervised time-series representation learning with iterative bilinear temporal-spectral fusion. In International Conference on Machine Learning. PMLR, 25038-25054.   
[252] Xinyu Yang, Zhenguo Zhang, and Rongyi Cui. 2022. Timeclr: A self-supervised contrastive learning framework for univariate time series representation. Knowledge-Based Systems 245 (2022), 108606.   
[253] Lexiang Ye and Eamonn Keogh. 2009. Time series shapelets: a new primitive for data mining. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, 947-956.   
[254] Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh. 2016. Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets. In 2016 IEEE 16th International Conference on Data Mining (ICDM). 1317-1322. https://doi.org/10.1109/ICDM.2016.0179   
[255] Byoung-Kee Yi and Christos Faloutsos. 2000. Fast time sequence indexing for arbitrary Lp norms. (2000).   
[256] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 8980-8987.   
[257] Jesin Zakaria, Abdullah Mueen, and Eamonn Keogh. 2012. Clustering time series using unsupervised-shapelets. In 2012 IEEE 12th International Conference on Data Mining. IEEE, 785-794.   
[258] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. 2021. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining. 2114-2124.   
[259] Hongjun Zhang, Jing Wang, Qinfeng Xiao, Jiaoxue Deng, and Youfang Lin. 2021. Sleeppriorcl: Contrastive representation learning with prior knowledge-based positive mining and adaptive temperature for sleep staging. arXiv preprint arXiv:2110.09966 (2021).   
[260] Nan Zhang and Shiliang Sun. 2022. Multiview unsupervised shapelet learning for multivariate time series clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 4 (2022), 4981-4996.   
[261] Qin Zhang, Jia Wu, Hong Yang, Yingjie Tian, and Chengqi Zhang. 2016. Unsupervised feature learning from time series.. In IJCAI. New York, USA, 2322-2328.   
[262] Qin Zhang, Jia Wu, Peng Zhang, Guodong Long, and Chengqi Zhang. 2018. Salient subsequence learning for time series clustering. IEEE transactions on pattern analysis and machine intelligence 41, 9 (2018), 2193-2207.   
[263] Xiaohang Zhang, Jiaqi Liu, Yu Du, and Tingjie Lv. 2011. A novel clustering method on time series data. Expert Systems with Applications 38, 9 (2011), 11891-11900.   
[264] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. 2022. Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in Neural Information Processing Systems 35 (2022), 3988-4003.

Manuscript submitted to ACM

[265] Zheng Zhang, Xuzhi Lai, Min Wu, Luefeng Chen, Chengda Lu, and Sheng Du. 2021. Fault diagnosis based on feature clustering of time series data for loss and kick of drilling process. Journal of Process Control 102 (2021), 24-33.   
[266] Ying Zhong, Dong Huang, and Chang-Dong Wang. 2023. Deep Temporal Contrastive Clustering. Neural Processing Letters (2023), 1-17.   
[267] Yan Zhu, Zachary Zimmerman, Nader Shakibay Senobari, Chin-Chia Michael Yeh, Gareth Funning, Abdullah Mueen, Philip Brisk, and Eamonn Keogh. 2016. Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins. In 2016 IEEE 16th International Conference on Data Mining (ICDM). 739-748. https://doi.org/10.1109/ICDM.2016.0085   
[268] Robert J Zupko, Tran Dang Nguyen, J Claude S Ngabonziza, Michee Kabera, Haojun Li, Thu Nguyen-Anh Tran, Kien Trung Tran, Aline Umimana, and Maciej F Boni. 2023. Modeling policy interventions for slowing the spread of artemisinin-resistant pfkelch R561H mutations in Rwanda. Nature Medicine 29, 11 (2023), 2775-2784.

Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009

# Methods in Nonlinear Analysis

Springer Monographs in Mathematics

# Methods in Nonlinear Analysis

Kung-Ching Chang

School of Mathematical Sciences

Peking University

100871 Beijing

People's Republic of China

E-mail: kcchang@math.pku.edu.cn

Library of Congress Control Number: 2005931137

Mathematics Subject Classification (2000): 47Hoo, 47J05, 47J07, 47J25, 47J30, 58-01, 58C15, 58E05, 49-01, 49J15, 49J35, 49J45, 49J53, 35-01

ISSN 1439-7382

ISBN-10 3-540-24133-7 Springer Berlin Heidelberg New York

ISBN-13 978-3-540-24133-1 Springer Berlin Heidelberg New York

This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilm or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer. Violations are liable for prosecution under the German Copyright Law.

Springer is a part of Springer Science+Business Media

springeronline.com

$\langle \widehat{\mathbb{C}}\rangle$ Springer-Verlag Berlin Heidelberg 2005

Printed in The Netherlands

The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

Typesetting: by the authors and TechBooks using a Springer IATEX macro package

Cover design: design & production GmbH, Heidelberg

Printed on acid-free paper

SPIN: 11369295

41/TechBooks

543210

# Preface

Nonlinear analysis is a new area that was born and has matured from abundant research developed in studying nonlinear problems. In the past thirty years, nonlinear analysis has undergone rapid growth; it has become part of the mainstream research fields in contemporary mathematical analysis.

Many nonlinear analysis problems have their roots in geometry, astronomy, fluid and elastic mechanics, physics, chemistry, biology, control theory, image processing and economics. The theories and methods in nonlinear analysis stem from many areas of mathematics: Ordinary differential equations, partial differential equations, the calculus of variations, dynamical systems, differential geometry, Lie groups, algebraic topology, linear and nonlinear functional analysis, measure theory, harmonic analysis, convex analysis, game theory, optimization theory, etc. Amidst solving these problems, many branches are intertwined, thereby advancing each other.

The author has been offering a course on nonlinear analysis to graduate students at Peking University and other universities every two or three years over the past two decades. Facing an enormous amount of material, vast numbers of references, diversities of disciplines, and tremendously different backgrounds of students in the audience, the author is always concerned with how much an individual can truly learn, internalize and benefit from a mere semester course in this subject.

The author's approach is to emphasize and to demonstrate the most fundamental principles and methods through important and interesting examples from various problems in different branches of mathematics. However, there are technical difficulties: Not only do most interesting problems require background knowledge in other branches of mathematics, but also, in order to solve these problems, many details in argument and in computation should be included. In this case, we have to get around the real problem, and deal with a simpler one, such that the application of the method is understandable. The author does not always pursue each theory in its broadest generality; instead, he stresses the motivation, the success in applications and its limitations.

The book is the result of many years of revision of the author’s lecture notes. Some of the more involved sections were originally used in seminars as introductory parts of some new subjects. However, due to their importance, the materials have been reorganized and supplemented, so that they may be more valuable to the readers.

In addition, there are notes, remarks, and comments at the end of this book, where important references, recent progress and further reading are presented.

The author is indebted to Prof. Wang Zhiqiang at Utah State University, Prof. Zhang Kewei at Sussex University and Prof. Zhou Shulin at Peking University for their careful reading and valuable comments on Chaps. 3, 4 and 5.

Peking University

Kung Ching Chang

September, 2003

# Contents

# 1 Linearization

1.1 Differential Calculus in Banach Spaces . .

1.1.1 Frechet Derivatives and Gateaux Derivatives . . . 2   
1.1.2 Nemytscki Operator . . 7   
1.1.3 High-Order Derivatives . . 9

1.2 Implicit Function Theorem and Continuity Method . . . . . . 12

1.2.1 Inverse Function Theorem . . 12   
1.2.2 Applications . . 17   
1.2.3 Continuity Method . . 23

1.3 Lyapunov–Schmidt Reduction and Bifurcation . . . 30

1.3.1 Bifurcation 30   
1.3.2 Lyapunov–Schmidt Reduction . . . 33   
1.3.3 A Perturbation Problem 43   
1.3.4 Gluing . . 47   
1.3.5 Transversality . . . 49

1.4 Hard Implicit Function Theorem 54

1.4.1 The Small Divisor Problem . . 55   
1.4.2 Nash–Moser Iteration . 62

# 2 Fixed-Point Theorems 71

2.1 Order Method 72

2.2 Convex Function and Its Subdifferentials . . 80

2.2.1 Convex Functions 80   
2.2.2 Subdifferentials 84

2.3 Convexity and Compactness . . 87   
2.4 Nonexpansive Maps . . . . . 104   
2.5 Monotone Mappings . . . . 109   
2.6 Maximal Monotone Mapping . . . . 120

# 3 Degree Theory and Applications . . . . 127

3.1 The Notion of Topological Degree . . . . 128   
3.2 Fundamental Properties and Calculations of Brouwer Degrees . 137   
3.3 Applications of Brouwer Degree . . . 148

3.3.1 Brouwer Fixed-Point Theorem . . . . 148   
3.3.2 The Borsuk-Ulam Theorem and Its Consequences . . . . . 148   
3.3.3 Degrees for $S ^ { 1 }$ Equivariant Mappings . . . . . 151   
3.3.4 Intersection . . 153

3.4 Leray–Schauder Degrees . . . . 155   
3.5 The Global Bifurcation . . 164   
3.6 Applications . . 175

3.6.1 Degree Theory on Closed Convex Sets . . . . . 175   
3.6.2 Positive Solutions and the Scaling Method . . . . . . . 180   
3.6.3 Krein–Rutman Theory for Positive Linear Operators . . . 185   
3.6.4 Multiple Solutions . . . 189   
3.6.5 A Free Boundary Problem . . . . 192   
3.6.6 Bridging . . . 193

3.7 Extensions . . 195

3.7.1 Set-Valued Mappings . . . . 195   
3.7.2 Strict Set Contraction Mappings and Condensing Mappings . . . 198   
3.7.3 Fredholm Mappings . . . 200

# 4 Minimization Methods . . . . 205

4.1 Variational Principles . . . 206

4.1.1 Constraint Problems . . . 206   
4.1.2 Euler–Lagrange Equation . . . . 209   
4.1.3 Dual Variational Principle . . . 212

4.2 Direct Method . 216

4.2.1 Fundamental Principle . . . 216   
4.2.2 Examples . . 217   
4.2.3 The Prescribing Gaussian Curvature Problem and the Schwarz Symmetric Rearrangement 223

4.3 Quasi-Convexity . . . . 231

4.3.1 Weak Continuity and Quasi-Convexity . . . . . . . 232   
4.3.2 Morrey Theorem . . 237   
4.3.3 Nonlinear Elasticity . . 242

4.4 Relaxation and Young Measure. . . . 244

4.4.1 Relaxations . . . 245   
4.4.2 Young Measure . . 251

4.5 Other Function Spaces . . . 260

4.5.1 BV Space . . . . 260   
4.5.2 Hardy Space and BMO Space . . . . 266   
4.5.3 Compensation Compactness . . . . 271   
4.5.4 Applications to the Calculus of Variations . . . . . . . 274

4.6 Free Discontinuous Problems . . . 279

4.6.1 Γ-convergence . . . 279   
4.6.2 A Phase Transition Problem . . . 280   
4.6.3 Segmentation and Mumford–Shah Problem . . . . . . . . . . 284

4.7 Concentration Compactness . 289

4.7.1 Concentration Function . 289   
4.7.2 The Critical Sobolev Exponent and the Best Constants 295

4.8 Minimax Methods . . . 301

4.8.1 Ekeland Variational Principle . . . 301   
4.8.2 Minimax Principle . . . 303   
4.8.3 Applications . . 306

5 Topological and Variational Methods . . . . 315

5.1 Morse Theory . . . . 317

5.1.1 Introduction . 317   
5.1.2 Deformation Theorem . . . 319   
5.1.3 Critical Groups . 327   
5.1.4 Global Theory . . . 334   
5.1.5 Applications . . . 343

5.2 Minimax Principles (Revisited) . . . . 347

5.2.1 A Minimax Principle . 347   
5.2.2 Category and Ljusternik–Schnirelmann Multiplicity Theorem . 349   
5.2.3 Cap Product . . . 354   
5.2.4 Index Theorem . . 358   
5.2.5 Applications . . . 363

5.3 Periodic Orbits for Hamiltonian System and Weinstein Conjecture 371

5.3.1 Hamiltonian Operator . 373   
5.3.2 Periodic Solutions . 374   
5.3.3 Weinstein Conjecture . . . 376

5.4 Prescribing Gaussian Curvature Problem on $S ^ { 2 }$ . . 380

5.4.1 The Conformal Group and the Best Constant . . . . . . . . . 380   
5.4.2 The Palais–Smale Sequence . . . . . 387   
5.4.3 Morse Theory for the Prescribing Gaussian Curvature Equation on $\mathbf { S } ^ { 2 }$ . 389

5.5 Conley Index Theory . 392

5.5.1 Isolated Invariant Set . . 393   
5.5.2 Index Pair and Conley Index . . . . 397   
5.5.3 Morse Decomposition on Compact Invariant Sets and Its Extension . 408

Notes . . . . 419

References . . . . 425

# Linearization

The first and the easiest step in studying a nonlinear problem is to linearize it. That is, to approximate the initial nonlinear problem by a linear one. Nonlinear differential equations and nonlinear integral equations can be seen as nonlinear equations on certain function spaces. In dealing with their linearizations, we turn to the differential calculus in infinite-dimensional spaces. The implicit function theorem for finite-dimensional space has been proved very useful in all differential theories: Ordinary differential equations, differential geometry, differential topology, Lie groups etc. In this chapter we shall see that its infinite-dimensional version will also be useful in partial differential equations and other fields; in particular, in the local existence, in the stability, in the bifurcation, in the perturbation problem, and in the gluing technique etc. This is the contents of Sects. 1.2 and 1.3. Based on Newton iterations and the smoothing operators, the Nash–Moser iteration, which is motivated by the isometric embeddings of Riemannian manifolds into Euclidean spaces and the KAM theory, is now a very important tool in analysis. Limited in space and time, we restrict ourselves to introducing only the spirit of the method in Sect. 1.4.

# 1.1 Differential Calculus in Banach Spaces

There are two kinds of derivatives in the differential calculus of several variables, the gradients and the directional derivatives. We shall extend these two to infinite-dimensional spaces.

Let $X$ , $Y$ and $Z$ be Banach spaces, with norms $\| \mathbf { \partial } \cdot \mathbf { \partial } \| _ { X } , \| \mathbf { \partial } \cdot \mathbf { \partial } \| _ { Y } , \| \mathbf { \partial } \cdot \mathbf { \partial } \| _ { Z }$ , respectively. If there is no ambiguity, we omit the subscripts. Let $U \subset X$ be an open set, and let $f : U \to Y$ be a map.

# 1.1.1 Frechet Derivatives and Gateaux Derivatives

Definition 1.1.1 (Fr´echet derivative) Let $x _ { 0 } \in U$ ; we say that $f$ is Fr´echet differentiable (or F-differentiable) at $x _ { 0 }$ , if $\exists A \subset L ( X , Y )$ such that

$$
\left\| f (x) - f \left(x _ {0}\right) - A \left(x - x _ {0}\right) \right\| _ {Y} = \circ (\left\| x - x _ {0} \right\| _ {X}).
$$

Let $f ^ { \prime } ( x _ { 0 } ) = A$ , and call it the Fr´echet (or $F$ -) derivative of $f$ at $x _ { 0 }$

If $f$ is F-differentiable at every point in $U$ , and if $x \mapsto f ^ { \prime } ( x )$ , as a mapping from $U$ to $L ( X , Y )$ , is continuous at $x _ { 0 }$ , then we say that $f$ is continuously differentiable at $x _ { 0 }$ . If $f$ is continuously differentiable at each point in $U$ , then we say that $f$ is continuously differentiable on $U$ , and denote it by $f \in$ $C ^ { 1 } ( U , Y )$ .

Parallel to the differential calculus of several variables, by definition, we may prove the following:

1. If $f$ is F-differentiable at $x _ { 0 }$ , then $f ^ { \prime } ( x _ { 0 } )$ is uniquely determined.

2. If $f$ is F-differentiable at $x _ { 0 }$ , then $f$ must be continuous at $x _ { 0 }$ .

3. (Chain rule) Assume that $U \subset X , V \subset Y$ are open sets, and that $f$ i s F-differentiable at $x _ { 0 }$ , and $g$ is F-differentiable at $f ( x _ { 0 } )$ , where

$$
U \xrightarrow {f} V \xrightarrow {g} Z
$$

Then

$$
(g \circ f) ^ {\prime} (x _ {0}) = g ^ {\prime} \circ f (x _ {0}) \cdot f ^ {\prime} (x _ {0}).
$$

Definition 1.1.2 (Gateaux derivative) Let $x _ { 0 } \in U$ ; we say that $f$ is Gateaux differentiable (or G-differentiable) at $x _ { 0 }$ , if $\because \forall h \in X , \exists d f ( x _ { 0 } , h ) \subset Y$ , such that

$$
\left\| f (x _ {0} + t h) - f (x _ {0}) - t d f (x _ {0}, h) \right\| _ {Y} = \circ (t) \quad a s \quad t \rightarrow 0
$$

for all $x _ { 0 } + t h \subset U$ . We call df $f ( x _ { 0 } , h )$ the Gateaux derivative (or G-derivative) of $f$ at $x _ { 0 }$ .

We have

$$
\frac {d}{d t} f (x _ {0} + t h) \mid_ {t = 0} = d f (x _ {0}, h),
$$

if $f$ is G-differentiable at $x _ { 0 }$

By definition, we have the following properties:

1. If $f$ is G-differentiable at $x _ { 0 }$ , then $d f ( x _ { 0 } , h )$ is uniquely determined.

2. $d f ( x _ { 0 } , t h ) = t d f ( x _ { 0 } , h ) \forall t \in \mathbb { R } ^ { 1 }$ .

3. If $f$ is G-differentiable at $x _ { 0 }$ , then $\forall h \in X$ , $\forall y ^ { * } \in Y ^ { * }$ , the function

$\varphi ( t ) = \langle y ^ { * } , f ( x _ { 0 } + t h ) \rangle$ is differentiable at $t = 0$ , and $\varphi ^ { \prime } ( t ) = \langle y ^ { \ast } , d f ( x _ { 0 } , h ) \rangle$

4. Assume that $f : U \to Y$ is G-differentiable at each point in $U$ , and that the segment $\{ x _ { 0 } + t h \mid t \in [ 0 , 1 ] \} \subset U$ , then

$$
\| f (x _ {0} + h) - f (x _ {0}) \| _ {Y} \leqslant \sup  _ {0 <   t <   1} \| d f (x _ {0} + t h, h) \| _ {Y}
$$

Proof. Let

$$
\begin{array}{l} \varphi_ {y ^ {*}} (t) = \langle y ^ {*}, f (x _ {0} + t h) \rangle \quad t \in [ 0, 1 ], \forall y ^ {*} \in Y ^ {*} \\ \left| \langle y ^ {*}, f (x _ {0} + h) - f (x _ {0}) \rangle \right| = \left| \varphi_ {y ^ {*}} (1) - \varphi_ {y ^ {*}} (0) \right| \\ = \left| \varphi_ {y ^ {*}} ^ {\prime} \left(t ^ {*}\right) \right| \\ = | \langle y ^ {*}, d f (x _ {0} + t ^ {*} h, h) \rangle | \\ \end{array}
$$

for some $t ^ { * } \in ( 0 , 1 )$ depending on $y ^ { * }$ . The conclusion follows from the Hahn–Banach theorem. □

5. If $f$ is F-differentiable at $x _ { 0 }$ , then $f$ is G-differentiable at $x _ { 0 }$ , with $d f ( x _ { 0 } , h ) = f ^ { \prime } ( x _ { 0 } ) h \quad \forall h \in X$ .

Conversely it is not true, but we have:

Theorem 1.1.3 Suppose that $f : U \to Y$ is $G$ -differentiable, and that $\forall x \in$ $U$ , $\exists A ( x ) \in L ( X , Y )$ satisfying

$$
d f (x, h) = A (x) h \quad \forall h \in X.
$$

If the mapping $x \mapsto A ( x )$ is continuous at $x _ { 0 }$ , then $f$ is $F$ -differentiable at $x _ { 0 }$ , with $f ^ { \prime } ( x _ { 0 } ) = A ( x _ { 0 } )$ .

Proof. With no loss of generality, we assume that the segment $\{ x _ { 0 } + t h \ \}$ $t \in [ 0 , 1 ] \}$ is in $U$ . According to the Hahn–Banach theorem, $\exists y ^ { * } \in Y ^ { * }$ , with $\parallel y ^ { * } \parallel = 1$ , such that

$$
\left\| f \left(x _ {0} + h\right) - f \left(x _ {0}\right) - A \left(x _ {0}\right) h \right\| _ {Y} = \left\langle y ^ {*}, f \left(x _ {0} + h\right) - f \left(x _ {0}\right) - A \left(x _ {0}\right) h \right\rangle .
$$

Let

$$
\varphi (t) = \left\langle y ^ {*}, f (x _ {0} + t h) \right\rangle .
$$

From the mean value theorem, $\exists \xi \in ( 0 , 1 )$ such that

$$
\begin{array}{l} \left| \varphi (1) - \varphi (0) - \langle y ^ {*}, A (x _ {0}) h \rangle \right| = \left| \varphi^ {\prime} (\xi) - \langle y ^ {*}, A (x _ {0}) h \rangle \right| \\ = \left| \langle y ^ {*}, d f (x _ {0} + \xi h, h) - A (x _ {0}) h \rangle \right| \\ = \left| \langle y ^ {*}, [ A (x _ {0} + \xi h) - A (x _ {0}) ] h \rangle \right| \\ = \circ (\left\| h \right\|), \\ \end{array}
$$

$$
\mathrm {i . e .}, f ^ {\prime} \left(x _ {0}\right) = A \left(x _ {0}\right).
$$

The importance of Theorem 1.1.3 lies in the fact that it is not easy to write down the F-derivative for a given map directly, but the computation of G-derivative is reduced to the differential calculus of single variables. The same situation occurs in the differential calculus of several variables: Gradients are reduced to partial derivatives, and partial derivatives are reduced to derivatives of single variables.

Example 1. Let $A \in L ( X , Y )$ , $f ( x ) = A x$ . Then $f ^ { \prime } ( x ) = A \quad \forall x$

Example 2. Let $X = \mathbb { R } ^ { n }$ , $Y = \mathbb { R } ^ { m }$ , and let $\varphi _ { 1 } , \varphi _ { 2 } \ldots , \varphi _ { m } \in C ^ { 1 } ( \mathbb { R } ^ { n } , R ^ { 1 } )$ . Set

$$
f (x) = \left( \begin{array}{c} \varphi_ {1} (x) \\ \vdots \\ \varphi_ {m} (x) \end{array} \right), \text {i . e . ,} f: X \to Y  .
$$

Then

$$
f ^ {\prime} (x _ {0}) = \left(\frac {\partial \varphi_ {i} (x _ {0})}{\partial x _ {j}}\right) _ {m \times n}.
$$

Example 3. Let $\Omega \subset \mathbb { R } ^ { n }$ be an open bounded domain. Denote by $C ( \Omega )$ the continuous function space on $\Omega$ . Let

$$
\varphi : \overline {{\Omega}} \times \mathbb {R} ^ {1} \longrightarrow \mathbb {R} ^ {1},
$$

be a $C ^ { 1 }$ function. Define a mapping $f : C ( \overline { { \Omega } } )  C ( \overline { { \Omega } } )$ by

$$
u (x) \mapsto \varphi (x, u (x)).
$$

Then $f$ is F-differentiable, and $\forall u _ { 0 } \in C ( \overline { { \Omega } } )$ ,

$$
\left(f ^ {\prime} \left(u _ {0}\right) \cdot v\right) (x) = \varphi_ {u} (x, u _ {0} (x)) \cdot v (x) \quad \forall v \in C (\overline {{\Omega}}).
$$

Proof. $\forall h \in C ( \Omega )$

$$
t ^ {- 1} \left[ f \left(u _ {0} + t h\right) - f \left(u _ {0}\right) \right] (x) = \varphi_ {u} (x, u _ {0} (x) + t \theta (x) h (x)) h (x),
$$

where $\theta ( x ) \in ( 0 , 1 )$ . $\forall \varepsilon > 0$ , $\forall M > 0$ , $\exists \delta = \delta ( M , \varepsilon ) > 0$ such that

$$
\left| \varphi_ {u} (x, \xi) - \varphi_ {u} (x, \xi^ {\prime}) \right| <   \varepsilon , \forall x \in \overline {{\Omega}},
$$

as $| \xi |$ $, \ \vert \xi ^ { \prime } \vert \ \leqslant M$ and $| \xi - \xi ^ { \prime } | \leqslant \delta$ . We choose $M = \parallel \boldsymbol { u } _ { 0 } \parallel + \parallel h \parallel$ , then for $| t | < \delta < 1$ ,

$$
\left| \varphi_ {u} (x, u _ {0} (x) + t \theta (x) h (x)) - \varphi_ {u} (x, u _ {0} (x)) \right| <   \varepsilon .
$$

It follows that $d f ( u _ { 0 } , h ) ( x ) = \varphi _ { u } ( x , u _ { 0 } ( x ) ) h ( x )$ .

Noticing that the multiplication operator $h \mapsto A ( u ) h = \varphi _ { u } ( x , u ( x ) ) \cdot h ( x )$ is linear and continuous, and the mapping $u \mapsto A ( u )$ from $C ( \Omega )$ into $L ( C ( { \overline { { \Omega } } } ) , C ( { \overline { { \Omega } } } ) )$ is continuous, from Theorem 1.1.3, $f$ is F-differentiable, and

$$
(f ^ {\prime} (u _ {0}) \cdot v) (x) = \varphi_ {u} (x, u _ {0} (x)) \cdot v (x) \forall v \in C (\overline {{\Omega}}).
$$

□

We investigate nonlinear differential operators on more general spaces. Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open set, and let $m$ be a nonnegative integer, $\gamma \in ( 0 , 1 )$ . $C ^ { m } ( \Omega )$ (and the H¨older space $C ^ { m , \gamma } ( \Omega )$ ) is defined to be the function space consisting of $C ^ { m }$ functions (with $\gamma$ -H¨older continuous $m$ -order partial derivatives).

The norms are defined as follows:

$$
\parallel u \parallel_ {C ^ {m}} = \max  _ {x \in \overline {{\Omega}}} \sum_ {| \alpha | \leq m} \left| \partial^ {\alpha} u (x) \right|,
$$

and

$$
\parallel u\parallel_{C^{m,\gamma}} = \parallel u\parallel_{C^{m}} + \max_{x,y\in \overline{\Omega}\quad |\alpha | = m}\frac{\left|\partial^{\alpha}u(x) - \partial^{\alpha}u(y)\right|}{|x - y|^{\gamma}},
$$

where $\alpha = ( \alpha _ { 1 } , \alpha _ { 2 } , \ldots , \alpha _ { n } )$ is a multi-index, $| \alpha | = \alpha _ { 1 } + \alpha _ { 2 } + \cdot \cdot \cdot + \alpha _ { n }$ , $\partial ^ { \alpha } =$ $\partial _ { x _ { 1 } } ^ { \alpha _ { 1 } } \partial _ { x _ { 2 } } ^ { \alpha _ { 2 } } \cdot \cdot \cdot \partial _ { x _ { n } } ^ { \alpha _ { n } }$ .

1 2 nWe always denote by $m ^ { * }$ the number of the index set $\left\{ \alpha = ( \alpha _ { 1 } , \alpha _ { 2 } , \ldots , \alpha _ { n } ) \ \right|$ $| \alpha | \leqslant m \}$ , and $\boldsymbol { D } ^ { m } \boldsymbol { u }$ the set $\{ \partial ^ { \alpha } u \mid | \alpha | \leqslant m \}$ .

Suppose that $r$ is a nonnegative integer, and that $\varphi \in C ^ { \infty } ( \overline { { \Omega } } \times \mathbb { R } ^ { r ^ { * } } )$ . Define a differentiable operator of order $r$ :

$$
f (u) (x) = \varphi (x, D ^ {r} u (x)).
$$

Suppose $\textit { m } \geqslant \textit { r }$ , then $f : C ^ { m } ( \Omega ) \to C ^ { m - r } ( \Omega )$ (and also $C ^ { m , \gamma } ( \Omega ) \to$ $C ^ { m - r , \gamma } ( { \overline { { \Omega } } } ) )$ is F-differentiable. Furthermore

$$
(f ^ {\prime} (u _ {0}) h) (x) = \sum_ {| \alpha | \leq r} \varphi_ {\alpha} (x, D ^ {r} u _ {0} (x)) \cdot \partial^ {\alpha} h (x), \forall h \in C ^ {m} (\overline {{\Omega}}),
$$

where $\varphi _ { \alpha }$ is the partial derivative of $\varphi$ with respect to the variable index $\alpha$

The proof is similar to Example 3.

Example 4. Suppose $\varphi \in C ^ { \infty } ( \overline { { \Omega } } \times \mathbb { R } ^ { r ^ { * } } )$ . Define

$$
f (u) = \int_ {\Omega} \varphi (x, D ^ {r} u (x)) d x \quad \forall u \in C ^ {r} (\overline {{\Omega}}).
$$

Then $f : C ^ { r } ( \overline { { \Omega } } ) \to \mathbb { R } ^ { 1 }$ is F-differentiable. Furthermore

$$
\langle f ^ {\prime} (u _ {0}), h \rangle = \int_ {\Omega} \sum_ {| \alpha | \leq r} \varphi_ {\alpha} (x, D ^ {r} u _ {0} (x)) \partial^ {\alpha} h (x) d x \quad \forall h \in C ^ {r} (\overline {{\Omega}}).
$$

Proof. Use the chain rule:

$$
C ^ {r} (\overline {{\Omega}}) \xrightarrow {\varphi (\cdot , D ^ {r} u (\cdot))} C (\overline {{\Omega}}) \xrightarrow {\int_ {\Omega}} \mathbb {R} ^ {1},
$$

and combine the results of Examples 1 and 3.

□

In particular, the following functional occurs frequently in the calculus of variations ( $r = 1 , r ^ { * } = n + 1$ ). Assume that $\varphi ( x , u , p )$ is a function of the form:

$$
\varphi (x, u, p) = \frac {1}{2} | p | ^ {2} + \sum_ {i = 1} ^ {n} a _ {i} (x) p _ {i} + a _ {0} (x) u ,
$$

where $p = ( p _ { 1 } , p _ { 2 } , . . . , p _ { n } )$ , and $a _ { i } ( x )$ , $i = 0 , 1 , \ldots , n$ , are in $C ( \overline { { \Omega } } )$ .

Set

$$
f (u) = \int_ {\Omega} \left[ \frac {1}{2} | \nabla u (x) | ^ {2} + \sum_ {i = 1} ^ {n} a _ {i} (x) \partial_ {x _ {i}} u + a _ {0} (x) u (x) \right] d x,
$$

we have

$$
\langle f ^ {\prime} (u), h \rangle = \int_ {\Omega} \left[ \nabla u (x) \cdot \nabla h (x) + \sum_ {i = 1} ^ {n} a _ {i} (x) \partial_ {x _ {i}} h (x) + a _ {0} (x) h (x) \right] d x
$$

$$
\forall h \in C ^ {1} (\overline {{\Omega}}).
$$

Example 5. Let $X$ be a Hilbert space, with inner product $( , )$ . Find the Fderivative of the norm $f ( x ) = \parallel x \parallel$ , as $x \neq \theta$ .

Let $F ( x ) = \parallel x \parallel ^ { 2 }$ . Since

$$
t ^ {- 1} \left(\| x + t h \| ^ {2} - \| x \| ^ {2}\right) = 2 (x, h) + t \| h \| ^ {2},
$$

we have $d F ( x , h ) \ = \ 2 ( x , h )$ . It is continuous for all $x$ , therefore $F$ is Fdifferentiable, and

$$
F ^ {\prime} (x) h = 2 (x, h).
$$

Since $f = F ^ { \frac { 1 } { 2 } }$ , by the chain rule

$$
F ^ {\prime} (x) = 2 \parallel x \parallel \cdot f ^ {\prime} (x).
$$

As $x \neq \theta$

$$
f ^ {\prime} (x) h = \left(\frac {x}{\| x \|}, h\right).
$$

In the applications to PDE as well as to the calculus of variations, Sobolev spaces are frequently used. We should extend the above studies to nonlinear operators defined on Sobolev spaces.

$\forall p \geqslant 1$ , $\forall$ nonnegative integer $m$ , let

$$
W ^ {m, p} (\Omega) = \left\{u \in L ^ {p} (\Omega) \mid \partial^ {\alpha} u \in L ^ {p} (\Omega) \mid | \alpha | \leqslant m \right\},
$$

where $\partial ^ { \alpha } u$ stands for the $\alpha$ -order generalized derivative of $u$ , i.e., the derivative in the distribution sense. Define the norm

$$
\| u \| _ {W ^ {m, p}} = \left(\sum_ {| \alpha \leq m} \| \partial^ {\alpha} u \| _ {L ^ {p} (\Omega)} ^ {p}\right) ^ {\frac {1}{p}}.
$$

The Banach space is called the Sobolev space of index $\{ m , p \}$ .

$W ^ { m , 2 } ( \Omega )$ is denoted by $H ^ { m } ( \Omega )$ , and the closure of $C _ { 0 } ^ { \infty } ( \Omega )$ under this norm is denoted by $H _ { 0 } ^ { m } ( \Omega )$ .

# 1.1.2 Nemytscki Operator

On Sobolev spaces, we extend the composition operator $u \mapsto \varphi ( x , u ( x ) )$ such that $\varphi$ may not be continuous in $x$ . The class of operators is sometimes called Nemytski operators.

Definition 1.1.4 Let $( \Omega , B , \mu )$ be a measure space. We say that $\varphi : \Omega \ \times$ $\mathbb { R } ^ { N } \to \mathbb { R } ^ { 1 }$ is a Caratheodory function, if

1. $\forall a . e . x \in \Omega$ , $\xi \mapsto \varphi ( x , \xi )$ is continuous.   
2. $\forall \xi \in \mathbb { R } ^ { N }$ , $x \mapsto \varphi ( x , \xi )$ is $\mu$ -measurable.

The motivation in introducing the Caratheodory function is to make the composition function measurable if $u ( x )$ is only measurable. Indeed, there exists a sequence of simple functions $\{ u _ { n } ( x ) \} ^ { \infty }$ , such that $u _ { n } ( x ) \  \ u ( x )$ a.e., $\varphi ( x , u _ { n } ( x ) )$ is measurable according to (2). And from (1), $\varphi ( x , u _ { n } ( x ) ) $ $\varphi ( x , u ( x ) )$ a.e., therefore $\varphi ( x , u ( x ) )$ is measurable.

Theorem 1.1.5 Assume $p _ { 1 } , p _ { 2 } \geqslant 1$ , $a > 0$ and $b \in L _ { d \mu } ^ { p _ { 2 } } ( \Omega )$ . Suppose that $\varphi$ is a Caratheodory function satisfying

$$
\left| \varphi (x, \xi) \right| \leqslant b (x) + a \left| \xi \right| ^ {\frac {p _ {1}}{p _ {2}}}
$$

Then $f : u ( x ) \mapsto \varphi ( x , u ( x ) )$ is a bounded and continuous mapping from $L _ { d \mu } ^ { p _ { 1 } } ( \Omega , \mathbb { R } ^ { N } )$ to $L _ { d \mu } ^ { p _ { 2 } } ( \Omega , \mathbb { R } ^ { N } )$ .

Proof. The boundedness follows from the Minkowski inequality:

$$
\parallel f (u) \parallel_ {p _ {2}} \leqslant \parallel b \parallel_ {p _ {2}} + a \parallel u \parallel_ {p _ {1}} ^ {\frac {p _ {1}}{p _ {2}}}  ,
$$

where $\| \cdot \| _ { p }$ is the $L _ { d \mu } ^ { p } ( \Omega , \mathbb { R } ^ { N } )$ norm. We turn to proving the continuity. It is sufficient to prove that $\forall \{ u _ { n } \} _ { 1 } ^ { \infty }$ i f $u _ { n } \longrightarrow u$ in $L ^ { p _ { 1 } }$ , then there is a subsequence $\{ u _ { n _ { i } } \}$ such that $f ( u _ { n _ { i } } ) ~ \to ~ f ( u )$ in $L ^ { p _ { 2 } }$ . Indeed one can find a subsequence $\{ u _ { n _ { i } } \}$ of $\{ u _ { n } \}$ which converges a.e. to $u$ , along which $\parallel u _ { n _ { i } } - u _ { n _ { i - 1 } } \parallel _ { p _ { 1 } } < \frac { 1 } { 2 ^ { i } }$ , $i = 2 , 3 , \dots$ ; therefore

$$
| u _ {n _ {i}} (x) | \leqslant \Phi (x) := | u _ {n _ {1}} (x) | + \sum_ {i = 2} ^ {\infty} | u _ {n _ {i}} (x) - u _ {n _ {i - 1}} (x) |.
$$

Since $\Phi$ is measurable, and

$$
\left(\int_ {\Omega} | \Phi (x) | ^ {p _ {1}} d \mu\right) ^ {\frac {1}{p _ {1}}} \leqslant \| u _ {n _ {1}} \| _ {p _ {1}} + \sum_ {i = 2} ^ {\infty} \| u _ {n _ {i}} - u _ {n _ {i - 1}} \| _ {p _ {1}} <   + \infty ,
$$

we conclude that $\Phi \in L _ { d \mu } ^ { p _ { 1 } } ( \Omega )$ . Noticing

$$
f \left(u _ {n _ {i}}\right) = \varphi (x, u _ {n _ {i}} (x)) \rightarrow \varphi (x, u (x)) \quad a. e.,
$$

and

$$
| f (u _ {n _ {i}}) (x) | \leqslant b (x) + a (\Phi (x)) ^ {\frac {p _ {1}}{p _ {2}}} \in L _ {d \mu} ^ {p _ {2}} (\Omega , R ^ {N}),
$$

we have $\parallel f ( u _ { n _ { i } } ) - f ( u ) \parallel _ { p _ { 2 } } \to 0$ , according to Lebesgue dominance theorem. This proves the continuity of $f$ . □

Corollary 1.1.6 Let $\Omega \subset \mathbb { R } ^ { n }$ be a smooth bounded domain, and let $1 \ \leq$ $p _ { 1 } , p _ { 2 } \ \leq \ \infty$ . Suppose that $\varphi : \Omega \times \mathbb { R } ^ { m ^ { * } }  \mathbb { R }$ is a Caratheodory function satisfying

$$
| \varphi (x, \xi_ {0}, \dots , \xi_ {m}) | \leqslant b (x) + a \sum_ {j = 0} ^ {m} | \xi_ {j} | ^ {\frac {\alpha_ {j}}{p _ {2}}} ,
$$

where $\xi _ { j }$ is $a \# \{ \alpha = ( \alpha _ { 1 } , . . . , \alpha _ { n } ) | | \alpha | = j \}$ -vector, $\begin{array} { r } { \alpha _ { j } \leqslant ( \frac { 1 } { p _ { 1 } } - \frac { m - j } { n } ) ^ { - 1 } } \end{array}$ , $a >$ 0, and $b ~ \in ~ L ^ { p _ { 2 } } ( \Omega )$ . Then $f ( u ) ( x ) \ : = \ : \varphi ( x , D ^ { m } u ( x ) )$ defines a bounded and continuous map from $W ^ { m , p _ { 1 } } ( \Omega )$ into $L ^ { p _ { 2 } } ( \Omega )$ .

Corollary 1.1.7 Suppose that $\Omega \subset \mathbb { R } ^ { n }$ and that $\varphi : \Omega \times \mathbb { R } ^ { 1 } \to \mathbb { R } ^ { 1 }$ and $\varphi _ { \xi } ( x , \xi )$ are Caratheodory functions. If $| \varphi _ { \xi } ( x , \xi ) | \leqslant b ( x ) + a | \xi | ^ { r }$ , where $b \in L ^ { \frac { 2 n } { n + 2 } } ( \Omega )$ , $a > 0$ , and $\textstyle r = { \frac { n + 2 } { n - 2 } }$ (if $n \leq 2$ , then the restriction is not necessary), then the functional

$$
f (u) = \int_ {\Omega} \varphi (x, u (x)) d x
$$

is $F$ -differentiable on $H ^ { 1 } ( \Omega )$ , with $F$ -derivative

$$
\langle f ^ {\prime} (u), v \rangle = \int_ {\Omega} \varphi_ {\xi} (x, u (x)) \cdot v (x) d x,
$$

where $\langle , \rangle$ is the inner product on $H ^ { 1 } ( \Omega )$

Proof. The Sobolev embedding theorem says that the injection $i : H ^ { 1 } ( \Omega ) \hookrightarrow$ $L ^ { \frac { \cdot 2 n } { n - 2 } } ( \Omega )$ is continuous, so is the dual map $i ^ { * } : L ^ { \frac { 2 n } { n + 2 } } ( \Omega ) \hookrightarrow ( H ^ { 1 } ( \Omega ) ) ^ { * }$ .

According to Theorem 1.1.5, $\varphi _ { \xi } ( \cdot , \cdot ) : L ^ { \frac { 2 n } { n - 2 } } \to L ^ { \frac { 2 n } { n + 2 } }$ is continuous. Therefore the Gateaux derivative

$$
d f (u, v) = \int_ {\Omega} \varphi_ {\xi} (x, u (x)) \cdot v (x) d x \quad \forall v \in H ^ {1} (\Omega)
$$

is continuous from $H ^ { 1 } ( \Omega )$ to $( H ^ { 1 } ( \Omega ) ) ^ { * }$ . Applying Theorem 1.1.3, we conclude that $f$ is F-differentiable on $H ^ { 1 } ( \Omega )$ . The proof is complete. □

Corollary 1.1.8 In Corollary 1.1.6, the differential operator

$$
f (u (x)) = \varphi (x, D ^ {m} u (x))
$$

from $C ^ { l , \gamma } ( \Omega )$ to $C ^ { l - m , \gamma } ( \Omega )$ , $l \geqslant m$ , $0 \leqslant \gamma < 1$ , is F-differentiable, with

$$
(f ^ {\prime} (u _ {0}) h) (x) = \sum_ {| \alpha | \leq m} \varphi_ {\alpha} (x, D ^ {m} u (x)) \partial^ {\alpha} h (x) \quad \forall h \in C ^ {l, \gamma} (\overline {{\Omega}}).
$$

# 1.1.3 High-Order Derivatives

The second-order derivative of $f$ at $x _ { 0 }$ is defined to be the derivative of $f ^ { \prime } ( x )$ at $x _ { 0 }$ . Since $f ^ { \prime } : U \to L ( X , Y ) , f ^ { \prime \prime } ( x _ { 0 } )$ should be in $L ( X , L ( X , Y ) )$ ). However, if we identify the space of bounded bilinear mappings with $L ( X , L ( X , Y ) )$ , and verify that $f ^ { \prime \prime } ( x _ { 0 } )$ as a bilinear mapping is symmetric, see Theorem 1.1.9 below, then we can define equivalently the second derivative $f ^ { \prime \prime } ( x _ { 0 } )$ as follows: For $f : U \to Y$ , $x _ { 0 } \in U \subset X$ , if there exists a bilinear mapping $f ^ { \prime \prime } ( x _ { 0 } ) ( \cdot , \cdot )$ of $X \times X \to Y$ satisfying

$$
\| f (x _ {0} + h) - f (x _ {0}) - f ^ {\prime} (x _ {0}) h - \frac {1}{2} f ^ {\prime \prime} (x _ {0}) (h, h) \| = \circ (\| h \| ^ {2}) \forall h \in X, \mathrm {a s} \| h \| \to 0,
$$

then $f ^ { \prime \prime } ( x _ { 0 } )$ is called the second-order derivative of $f$ at $x _ { 0 }$

By the same manner, one defines the $m$ th-order derivatives at $x _ { 0 }$ successively: $f ^ { ( m ) } ( x _ { 0 } ) : X \times \cdot \cdot \cdot \times X \to Y$ is an $m$ -linear mapping satisfying

$$
\left\| f (x _ {0} + h) - \sum_ {j = 0} ^ {m} \frac {f ^ {(j)} (x _ {0}) (h , \ldots , h)}{j !} \right\| = \circ (\| h \| ^ {m}),
$$

as $\parallel h \parallel \to 0$ . Then $f$ is called $m$ differentiable at $x _ { 0 }$ .

Similar to the finite-dimensional vector functions, we have:

Theorem 1.1.9 Assume that $f : U \to Y$ is m differentiable at $x _ { 0 } \in U$ . Then for any permutation $\pi$ of $( 1 , \ldots , m )$ , we have

$$
f ^ {(m)} (x _ {0}) (h _ {1}, \ldots , h _ {m}) = f ^ {(m)} (x _ {0}) (h _ {\pi (1)}, \ldots , h _ {\pi (m)}).
$$

Proof. We only prove this in the case where $m = 2$ , i.e.,

$$
f ^ {\prime \prime} (x _ {0}) (\xi , \eta) = f ^ {\prime \prime} (x _ {0}) (\eta , \xi) \quad \forall \xi , \eta \in X.
$$

Indeed $\forall y ^ { * } \in Y ^ { * }$ , we consider the function

$$
\varphi (t, s) = \left\langle y ^ {*}, f (x _ {0} + t \xi + s \eta) \right\rangle .
$$

It is twice differentiable at $t = s = 0$ ; so is

$$
\frac {\partial^ {2}}{\partial t \partial s} \varphi (0, 0) = \frac {\partial^ {2}}{\partial s \partial t} \varphi (0, 0).
$$

Since $f ^ { \prime } ( x _ { 0 } + t \xi + s \eta )$ is continuous as $| t |$ , $| s |$ small, one has

$$
\frac {\partial}{\partial s} \varphi (t, \cdot) | _ {s = 0} = \langle y ^ {*}, f ^ {\prime} (x _ {0} + t \xi) \eta \rangle ;
$$

and then,

$$
\frac {\partial^ {2}}{\partial t \partial s} \varphi (t, s) | _ {t = s = 0} = \left\langle y ^ {*}, f ^ {\prime \prime} (x _ {0}) (\xi , \eta) \right\rangle .
$$

Similarly

$$
\frac {\partial^ {2}}{\partial s \partial t} \varphi (t, s) | _ {t = s = 0} = \langle y ^ {*}, f ^ {\prime \prime} (x _ {0}) (\eta , \xi) \rangle .
$$

This proves the conclusion.

□

Theorem 1.1.10 (Taylor formula) Suppose that $f : U \to Y$ is continuously m-differentiable. Assume the segment $\{ x _ { 0 } + t h | t \in [ 0 , 1 ] \} \subset U$ . Then

$$
\begin{array}{l} f (x _ {0} + h) = \sum_ {j = 0} ^ {m} \frac {1}{j !} f ^ {(j)} (x _ {0}) (h, \dots , h) \\ + \frac {1}{m !} \int_ {0} ^ {1} (1 - t) ^ {m} f ^ {(m + 1)} (x _ {0} + t h) (h, \dots , h) d t. \\ \end{array}
$$

Proof. $\forall y ^ { * } \in Y ^ { * }$ , we consider the function:

$$
\varphi (t) = \left\langle y ^ {*}, f \left(x _ {0} + t h\right) \right\rangle .
$$

From the Hahn–Banach theorem and the Taylor formula for single-variable functions:

$$
\varphi (1) = \sum_ {j = 0} ^ {m} \frac {1}{j !} \varphi^ {(j)} (0) + \frac {1}{m !} \int_ {0} ^ {1} (1 - t) ^ {m} \varphi^ {(m + 1)} (t) d t,
$$

we obtain the desired Taylor formula for mappings between B-spaces. □

Example 1. $X = \mathbb { R } ^ { n }$ , $Y = \mathbb { R } ^ { 1 }$ . If $f : X \to Y$ is twice continuously differentiable, then

$$
f ^ {\prime \prime} (x) = H _ {f} (x) = \left(\frac {\partial^ {2} f (x)}{\partial x _ {i} \partial x _ {j}}\right) _ {i, j = 1, \dots , n}.
$$

Example 2. $X = C ^ { 1 } ( \Omega , \mathbb { R } ^ { N } )$ , $Y = \mathbb { R } ^ { 1 }$ . Suppose that $g \in C ^ { 2 } ( \Omega \times \mathbb { R } ^ { N } , \mathbb { R } ^ { 1 } )$ . Define

$$
f (u) = \frac {1}{2} \int_ {\Omega} | \nabla u | ^ {2} + \int_ {\Omega} g (x, u (x))
$$

as $u \in X$ . By definition, we have

$$
f ^ {\prime} (u) \cdot \varphi = \int_ {\Omega} [ \nabla u (x) \nabla \varphi (x) + g _ {u} ^ {\prime} (x, u (x)) \varphi (x) ] d x,
$$

and

$$
f ^ {\prime \prime} (u) (\varphi , \psi) = \int_ {\Omega} [ \nabla \psi (x) \nabla \varphi (x) + g _ {u u} ^ {\prime \prime} (x, u (x)) \varphi (x) \psi (x) ] d x.
$$

With some additional growth conditions on $g _ { u u } ^ { \prime \prime }$

$$
\left| g _ {u u} ^ {\prime \prime} (x, u) \right| \leq a \left(1 + \left| u \right| ^ {\frac {4}{n - 2}}\right), a > 0 \forall u \in R ^ {N},
$$

$f$ is twice differentiable in $H _ { 0 } ^ { 1 } ( \Omega , R ^ { N } )$ . As an operator from $H _ { 0 } ^ { 1 } ( \Omega , R ^ { N } )$ into itself,

$$
f ^ {\prime \prime} (u) = i d + (- \triangle) ^ {- 1} g _ {u} ^ {\prime \prime} (\cdot , u (\cdot)).
$$

is self-adjoint, or equivalently, the operator $- \triangle + g _ { u u } ^ { \prime \prime } ( x , u ( x ) )$ defined on $L ^ { 2 }$ is self-adjoint with domain $H ^ { 2 } \cap H _ { 0 } ^ { 1 } ( \Omega , R ^ { N } )$ .

Example 3. Let $X = H _ { 0 } ^ { 1 } ( \Omega , \mathbb { R } ^ { 3 } )$ , where $\Omega$ is a plane domain. Consider the volume functional

$$
Q (u) = \int_ {\Omega} u \cdot \left(u _ {x} \wedge u _ {y}\right).
$$

One has

$$
Q ^ {\prime} (u) \cdot \varphi = \int_ {\Omega} \varphi (u _ {x} \wedge u _ {y}) + u \cdot [ (\varphi_ {x} \wedge u _ {y}) + (u _ {x} \wedge \varphi_ {y}) ],
$$

and

$$
\begin{array}{l} Q ^ {\prime \prime} (u) (\varphi , \psi) = \int_ {\Omega} \varphi [ (\psi_ {x} \wedge u _ {y}) + (u _ {x} \wedge \psi_ {y}) ] + \psi [ (\varphi_ {x} \wedge u _ {y}) + (u _ {x} \wedge \varphi_ {y}) ] \\ + u \left[ \left(\varphi_ {x} \wedge \psi_ {y}\right) + \left(\psi_ {x} \wedge \varphi_ {y}\right) \right], \\ \end{array}
$$

$$
\forall \varphi , \psi \in H _ {0} ^ {1} (\Omega , \mathbb {R} ^ {3}).
$$

If further we assume $u \in C ^ { 2 } ( \overline { { \Omega } } , \mathbb { R } ^ { 3 } )$ , then from integration by parts and the antisymmetry of the exterior product, we have

$$
\begin{array}{l} \int_ {\Omega} u (\varphi_ {x} \wedge u _ {y}) = - \int_ {\Omega} (\varphi \wedge u _ {y}) \cdot u _ {x} + (\varphi \wedge u _ {x y}) \cdot u \\ = \int_ {\Omega} (u _ {x} \wedge u _ {y}) \varphi - \int_ {\Omega} (\varphi \wedge u _ {x y}) \cdot u, \\ \end{array}
$$

and

$$
\begin{array}{l} \int_ {\Omega} u (u _ {x} \wedge \varphi_ {y}) = - \int_ {\Omega} u _ {y} (u _ {x} \wedge \varphi) + u (u _ {x y} \wedge \varphi) \\ = \int_ {\Omega} \varphi (u _ {x} \wedge u _ {y}) + \int_ {\Omega} (\varphi \wedge u _ {x y}) u. \\ \end{array}
$$

Therefore,

$$
Q ^ {\prime} (u) = 3 \int_ {\Omega} \varphi \cdot (u _ {x} \wedge u _ {y}).
$$

By the same manner, we obtain

$$
Q ^ {\prime \prime} (u) (\varphi , \psi) = 3 \int_ {\Omega} u [ (\varphi_ {x} \wedge \psi_ {y}) + (\varphi_ {y} \wedge \psi_ {x}) ].
$$

Geometrically, let $u : \Omega \stackrel { C ^ { 2 } } { \to } \mathbb { R } ^ { 3 }$ be a parametrized surface in $\mathbb { R } ^ { 3 }$ ; $Q ( U )$ is the volume of the body enclosed by the surface.

As exercises, one computes the first- and second-order differentials of the following functionals:

1. $X = W _ { 0 } ^ { 1 , p } ( \Omega , \mathbb { R } ^ { 1 } )$ , Y = R1, 2 < p < ∞,

$$
f (u) = \int_ {\Omega} | \nabla u | ^ {p} d x.
$$

2. $X = C _ { 0 } ^ { 1 } ( \Omega , \mathbb { R } ^ { n } )$ , where $\Omega \subset \mathbb { R } ^ { n }$ is a domain,

$$
f (u) = \int_ {\Omega} d e t (\nabla u (x)) d x.
$$

3. $X = C _ { 0 } ^ { 1 } ( \overline { { \Omega } } , R ^ { 1 } )$ , where $\Omega \subset R ^ { n }$ is a domain,

$$
f (u) = \int_ {\Omega} \sqrt {1 + | \nabla u | ^ {2}} d x.
$$

# 1.2 Implicit Function Theorem and Continuity Method

# 1.2.1 Inverse Function Theorem

It is known that the implicit function theorem for functions of several variables plays important roles in many branches of mathematics (differential manifold, differential geometry, differential topology, etc.). Its extension to infinite-dimensional space is also extremely important in nonlinear analysis, as well as in the study of infinite-dimensional manifolds.

Theorem 1.2.1 (Implicit function theorem) Let $X$ , $Y$ , $Z$ be Banach spaces, $U \subset X \times Y$ be an open set. Suppose that $f \in C ( U , Z )$ has an $F$ -derivative w.r.t. $y$ , and that $f _ { y } \in C ( U , L ( Y , Z ) )$ . For a point $( x _ { 0 } , y _ { 0 } ) \in U$ , if we have

$$
f (x _ {0}, y _ {0}) = \theta ,
$$

$$
f _ {y} ^ {- 1} \left(x _ {0}, y _ {0}\right) \in L (Z, Y);
$$

then $\exists r , r _ { 1 } > 0$ , $\exists | u \in C ( B _ { r } ( x _ { 0 } ) , B _ { r _ { 1 } } ( y _ { 0 } ) )$ , such that

$$
\left\{ \begin{array}{l} B _ {r} (x _ {0}) \times B _ {r _ {1}} (y _ {0}) \subset U  , \\ u (x _ {0}) = y _ {0}  , \\ f (x, u (x)) = \theta \quad \forall x \in B _ {r} (x _ {0})  . \end{array} \right.
$$

Furthermore, if $f \in C ^ { 1 } ( U , Z )$ , then $u \in C ^ { 1 } ( B _ { r } ( x _ { 0 } ) , Y )$ , and

$$
u ^ {\prime} (x) = - f _ {y} ^ {- 1} \left(x _ {0}, u \left(x _ {0}\right)\right) \circ f _ {x} (x, u (x)) \quad \forall x \in B _ {r} \left(x _ {0}\right). \tag {1.1}
$$

Proof. (1) After replacing $f$ by

$$
g (x, y) = f _ {y} ^ {- 1} \left(x _ {0}, y _ {0}\right) \circ f \left(x + x _ {0}, y + y _ {0}\right),
$$

one may assume $x _ { 0 } = y _ { 0 } = \theta$ , $Z = Y$ and $f _ { y } ( \theta , \theta ) = i d _ { Y }$ .

(2) We shall find the solution $y = u ( x ) \in B _ { r _ { 1 } } ( \theta )$ of the equation

$$
f (x, y) = \theta \quad \forall x \in B _ {r} (\theta).
$$

Setting

$$
R (x, y) = y - f (x, y),
$$

it is reduced to finding the fixed point of $R ( x , \cdot ) \quad \forall x \in B _ { r } ( \theta )$ .

We shall apply the contraction mapping theorem to the mapping $R ( x , \cdot )$ . Firstly, we have a contraction mapping:

$$
\begin{array}{l} \| R (x, y _ {1}) - R (x, y _ {2}) \| = \| y _ {1} - y _ {2} - [ f (x, y _ {1}) - f (x, y _ {2}) ] \| \\ = \| y _ {1} - y _ {2} - \int_ {0} ^ {1} f _ {y} (x, t y _ {1} + (1 - t) y _ {2}) d t \cdot (y _ {1} - y _ {2}) \| \\ \leqslant \int_ {0} ^ {1} \| \mathrm {i d} _ {Y} - f _ {y} (x, t y _ {1} + (1 - t) y _ {2}) \| d t \cdot \| y _ {1} - y _ {2} \|. \\ \end{array}
$$

Since $f _ { y } : U \to L ( X , Y )$ is continuous, $\exists r , r _ { 1 } > 0$ such that

$$
\left\| R \left(x, y _ {1}\right) - R \left(x, y _ {2}\right) \right\| <   \frac {1}{2} \| y _ {1} - y _ {2} \| \tag {1.2}
$$

$\forall ( x , y _ { i } ) \in B _ { r } ( \theta ) \times B _ { r _ { 1 } } ( \theta )$ , $i = 1 , 2$ .

Secondly, we are going to verify $R ( x , \cdot ) : \overline { { B } } _ { r _ { 1 } } ( \theta )  \overline { { B } } _ { r _ { 1 } } ( \theta )$ . Indeed,

$$
\begin{array}{l} \| R (x, y) \| \leqslant \| R (x, \theta) \| + \| R (x, y) - R (x, \theta) \| \\ \leqslant \| f (x, \theta) \| + \frac {1}{2} \| y \| . \\ \end{array}
$$

For sufficiently small $r > 0$ , where

$$
\left\| f (x, \theta) \right\| <   \frac {1}{2} r _ {1}, \quad \forall x \in \bar {B} _ {r} (\theta), \tag {1.3}
$$

it follows that $\| R ( x , y ) \| < r _ { 1 }$ , $\forall ( x , y ) \in B _ { r } ( \theta ) \times B _ { r _ { 1 } } ( \theta )$ . Then, $\forall x \in { \overline { { B } } } _ { r } ( \theta )$ , $\exists | y \in { \overline { { B } } } _ { r _ { 1 } } ( \theta )$ satisfying $f ( x , y ) = \theta$ . Denote by $u ( x )$ the solution $y$ .

(3) We claim that $u \in C ( B _ { r } , Y )$ . Since

$$
\begin{array}{l} \parallel u (x) - u (x ^ {\prime}) \parallel = \parallel R (x, u (x)) - R (x ^ {\prime}, u (x ^ {\prime})) \parallel \\ \leqslant \frac {1}{2} \| u (x) - u (x ^ {\prime}) \| + \| R (x, u (x)) - R (x ^ {\prime}, u (x)) \|, \\ \end{array}
$$

we obtain

$$
\left\| u (x) - u \left(x ^ {\prime}\right) \right\| \leqslant 2 \| R (x, u (x)) - R \left(x ^ {\prime}, u (x)\right) \| . \tag {1.4}
$$

Noticing that $R \in C ( U , Y )$ , we have

$$
u \left(x ^ {\prime}\right)\rightarrow u (x) \quad \text {a s} \quad x ^ {\prime} \rightarrow x.
$$

(4) If $f \in C ^ { 1 } ( U , Y )$ , we want to prove $u \in C ^ { 1 }$ . First, by (1.2) and (1.4)

$$
\begin{array}{l} \| u (x) - u \left(x ^ {\prime}\right) \| \leqslant 2 \| f (x, u (x)) - f \left(x ^ {\prime}, u \left(x ^ {\prime}\right)\right) \| \\ \leqslant 2 \int_ {0} ^ {1} \| f _ {x} (t x + (1 - t) x ^ {\prime}, u (x)) \| d t \cdot \| x - x ^ {\prime} \|. \\ \end{array}
$$

$$
\left. \right.\left. \right.\left. \right.\left. \right.\left. \right.\left. \right.\left. \right.\left. \right.\left. \right.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left.\left. \mid h \| \right| \| \right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|\right|
$$

From

$$
f (x + h, u (x + h)) = f (x, u (x)) = \theta ,
$$

it follows that

$$
f (x + h, u (x + h)) - f (x, u (x + h)) + [ f (x, u (x + h)) - f (x, u (x)) ] = \theta ;
$$

also

$$
f _ {x} (x, u (x + h)) h + \circ (\| h \|) + f _ {y} (x, u (x)) (u (x + h) - u (x)) + \circ (\| h \|) = \theta .
$$

Therefore

$$
u (x + h) - u (x) + f _ {y} ^ {- 1} (x, u (x)) \circ f _ {x} (x, u (x + h)) h = \circ (\| h \|),
$$

i.e., $u \in C ^ { 1 }$ , and

$$
u ^ {\prime} (x) = - f _ {y} ^ {- 1} (x, u (x)) \circ f _ {x} (x, u (x)).
$$

![](images/f01e26072685eed9bc86b34bc568e90cd346daa17b754b860ba015b93d6fd6ba.jpg)

Remark 1.2.2 In the first part of Theorem 1.2.1, the space $X$ may be assumed to be a topological space. In fact, neither linear operations nor the properties of the norm were used.

Theorem 1.2.3 (Inverse function theorem) Let $V \subset Y$ be an open set, and $g \in C ^ { 1 } ( V , X )$ . Assume $y _ { 0 } \in V$ and $g ^ { \prime } ( y _ { 0 } ) \in L ( X , Y )$ . Then there exists $\delta > 0$ such that $B _ { \delta } ( y _ { 0 } ) \subset V$ and

$$
g: B _ {\delta} (y _ {0}) \to g (B _ {\delta} (y _ {0}))
$$

is a differmorphism. Furthermore

$$
(g ^ {- 1}) ^ {\prime} \left(x _ {0}\right) = g ^ {- 1} \left(y _ {0}\right), \text {w i t h} x _ {0} = g \left(y _ {0}\right). \tag {1.5}
$$

Proof. Set

$$
f (x, y) = x - g (y), \quad f \in C ^ {1} (X \times V, X).
$$

We use the implicit function theorem (IFT) to $f$ , there exist $r > 0$ and a unique $u \in C ^ { 1 } ( B _ { r } ( x _ { 0 } ) , B _ { r } ( y _ { 0 } ) )$ satisfying

$$
x = g \circ u (x).
$$

Since $g$ is continuous, $\exists \delta \in ( 0 , r )$ such that $g ( B _ { \delta } ( y _ { 0 } ) ) \subset B _ { r } ( x _ { 0 } )$ , therefore $g : B _ { \delta } ( y _ { 0 } )  g ( B _ { \delta } ( y _ { 0 } ) )$ is a diffeomorphism. And (1.5) follows from (1.1).

In the spirit of the IFT, we have a nonlinear version of the Banach open mapping theorem.

Theorem 1.2.4 (Open mapping) Let $X$ , $Y$ be Banach spaces, and let $\delta > 0$ and $y _ { 0 } \in Y$ . Suppose that $g \in C ^ { 1 } ( B _ { \delta } ( y _ { 0 } ) , X )$ and that $g ^ { \prime } ( y _ { 0 } ) : Y  X$ is an open map, then $g$ is an open map in a neighborhood of $y _ { 0 }$ .

Proof. We want to prove that $\exists \delta _ { 1 } \in ( 0 , \delta )$ and $r > 0$ , such that

$$
B _ {r} (g (y _ {0})) \subset g \left(B _ {\delta_ {1}} (y _ {0})\right).
$$

With no loss of generality, we may assume $y _ { 0 } = \theta$ and $g ( y _ { 0 } ) = \theta .$ . Let $A = g ^ { \prime } ( \theta )$

Since $A$ is surjective, $\exists C > 0$ such that

$$
\inf  _ {z \in \ker A} \| y - z \| _ {Y} \leqslant C \| A y \| _ {X}, \forall y \in Y, \tag {1.6}
$$

provided by the Banach inverse theorem. One chooses $\delta _ { 1 } \in ( 0 , \delta )$ and $r > 0$ satisfying

$$
\parallel g ^ {\prime} (y) - A \parallel \leqslant \frac {1}{2 (C + 1)} \quad \forall y \in B _ {\delta_ {1}} (\theta),
$$

and

$$
r <   \frac {\delta_ {1}}{2 (C + 1)} .
$$

Now, $\forall x \in B _ { r } ( \theta )$ , we are going to find $y \in B _ { \delta _ { 1 } } ( \theta )$ , satisfying $g ( y ) = x$ . Write

$$
R (y) = g (y) - A y.
$$

The problem is equivalent to solving the following equation:

$$
A y = x - R (y). \tag {1.7}
$$

We solve it by iteration.

Initially, we take $h _ { 0 } = \theta$ . Suppose that $h _ { n } \in B _ { \delta _ { 1 } } ( \theta )$ has been chosen; from (1.6), we can find $h _ { n + 1 }$ , satisfying

$$
A h _ {n + 1} = x - R \left(h _ {n}\right),
$$

and

$$
\left\| h _ {n + 1} - h _ {n} \right\| \leqslant (C + 1) \| A \left(h _ {n + 1} - h _ {n}\right) \| .
$$

Thus

$$
\begin{array}{l} \left\| h _ {n + 1} - h _ {n} \right\| \leqslant (C + 1) \| R (h _ {n}) - R (h _ {n - 1}) \| \\ = (C + 1) \| \int_ {0} ^ {1} g ^ {\prime} \left(t h _ {n} + (1 - t) h _ {n - 1}\right) d t - A \| \cdot \| h _ {n} - h _ {n - 1} \| \\ \leq \frac {1}{2} \| h _ {n} - h _ {n - 1} \| \quad \forall n \geqslant 1. \\ \end{array}
$$

Since

$$
\parallel h _ {1} \parallel \leqslant (1 + C) \parallel x \parallel \leqslant \frac {1}{2} \delta_ {1},
$$

and

$$
\begin{array}{l} \parallel h _ {n + 1} \parallel \leqslant \parallel h _ {1} \parallel + \sum_ {j = 1} ^ {n} \parallel h _ {j + 1} - h _ {j} \parallel \\ \leqslant \left(\frac {1}{2} + \dots + \frac {1}{2 ^ {n}} + \frac {1}{2 ^ {n + 1}}\right) \delta_ {1} <   \delta_ {1}, \\ \end{array}
$$

it follows that $h _ { n + 1 } \in B _ { \delta _ { 1 } } ( \theta )$ . Then we can proceed inductively.

The sequence $h _ { n }$ has a limit $y$ . Obviously $y$ is the solution of (1.7).

Essentially, the implicit function theorem is a consequence of the contraction mapping theorem. The continuity assumption of $f _ { y }$ in Theorem 1.2.2 seems too strong in some applications. We have a weakened version.

Theorem 1.2.5 Let $X , Y , Z$ be Banach spaces, and let $B _ { r } ( \theta ) \subset Y$ be a closed ball centered at $\theta$ with positive radius $r$ . Suppose that $T \in L ( Y , Z )$ has a bounded inverse, and that $\eta : X \times B _ { r } \to Z$ satisfies the following Lipschitz condition:

$$
\| \eta (x, y _ {1}) - \eta (x, y _ {2}) \| \leqslant K \| y _ {1} - y _ {2} \| \quad \forall y _ {1}, y _ {2} \in B _ {r} (\theta), \forall x \in X
$$

where $K < \parallel T ^ { - 1 } \parallel ^ { - 1 }$ . If $\eta ( \theta , \theta ) = \theta$ , and

$$
\| \eta (x, \theta) \| \leqslant (\| T ^ {- 1} \| ^ {- 1} - K) r;
$$

then $\forall x \in X$ , there exists a unique $u : X \to B _ { r } ( \theta )$ satisfying

$$
T u (x) + \eta (x, u (x)) = \theta \quad \forall x \in X.
$$

Furthermore, if $\eta$ is continuous, then so is u.

Proof. $\forall x \in X$ we find the fixed point of the map $- T ^ { - 1 } \eta ( x , y )$ . It is easily verified that $T ^ { - 1 } \cdot \eta ( x , \cdot ) : B _ { r } ( \theta ) \to B _ { r } ( \theta )$ is a contraction mapping. □

# 1.2.2 Applications

As we mentioned in the beginning of this section, the IFT plays an important role in solving nonlinear equations. However, the IFT is only a local statement. If a problem is local, then it is extremely powerful for local solvability. As to global solvability problems, we first solve them locally, and then extend the solutions by continuation. In this case, the IFT is applied in the first step. However, in this subsection we only present several examples to show how the method works for local problems, and in the next subsection for global problems.

Example 1. (Structural stability for hyperbolic systems)

A matrix $L \in G L ( n , R )$ is called hyperbolic if the set of eigenvalues of $L$ , $\sigma ( L ) \cap i R ^ { 1 } = \emptyset$ . The associate differential system reads as:

$$
\dot {x} = L x, x \in C ^ {1} (R ^ {1}, R ^ {n}).
$$

The flow $\phi _ { t } = e ^ { L t } \in G L ( n , R )$ is also linear. The flow line can be seen on the left of Fig. 1.1. Let $\xi \in C ^ { 0 , 1 } ( R ^ { n } , R ^ { n } )$ be a Lipschitzian (Lip.) map we investigate the hyperbolic system under the nonlinear perturbation:

$$
\dot {x} = L x + \xi (x), x \in C ^ {1} \left(R ^ {1}, R ^ {n}\right),
$$

and let $\psi _ { t }$ be the associate flow, the flow line of which is on the right of Fig. 1.1.

What is the relationship between $\phi _ { t }$ and $\psi _ { t }$ , if $\xi$ is small? One says that the hyperbolic system is structurally stable, which means that the flow lines $\phi _ { t }$ and $\psi _ { t }$ are topologically equivalent. More precisely, there is a homeomorphism $h : R ^ { n } \to R ^ { n }$ such that $h \circ \psi _ { t } = \phi _ { t } \circ h$ .

We shall show that the hyperbolic system is structurally stable. Let $A \ = \ e ^ { L }$ , then the set of eigenvalues $\sigma ( A )$ of $A$ satisfies $\sigma ( A ) \cap S ^ { 1 } = \emptyset$ . We decompose $\psi _ { 1 } = A + f$ , where $f \in C ^ { 0 , 1 } ( R ^ { n } , R ^ { n } )$ , it is known that the Lipschitzian constant of $f$ is small if that of $\xi$ is.

To the matrix $A \in G L ( n , R )$ , since $\sigma ( A ) \cap S ^ { 1 } = \emptyset$ , provided by the Jordan form, we have the decomposition $R ^ { n } = E _ { u } \oplus E _ { s }$ , where $E _ { u } , E _ { s }$ are invariant subspaces, on which the eigenvalues of $A _ { u } : = A | _ { E _ { u } }$ lie outside the unit circle, and those of $A _ { s } : = A | _ { E _ { s } }$ lie inside the unit circle. Due to these facts, one has

$$
\left\| A _ {s} \right\| <   1, \quad \left\| A _ {u} ^ {- 1} \right\| <   1.
$$

The following notations are used for any Banach spaces $X , Y$ . $C ^ { 0 } ( X , Y )$ stands for the space of all bounded and continuous mappings $h : X \to Y$ with norm:

$$
\| h \| = \sup  _ {x \in X} \| h (x) \| _ {Y}.
$$

$C ^ { 0 , 1 } ( X , Y )$ stands for the space of all bounded Lipschitzian maps from X to Y with norm:

![](images/636ada09a8daae9586ce1a6c5d1a748c1621270ac220c4c0c8c89e45756b2b93.jpg)

![](images/eecf692a89493c934224d6d287b99d02799230656127e223ba3347d7082635a0.jpg)

![](images/fdb3da8699e0810a9d187e74de1fa34fcdff2e16c3349487085d85ddf1ee2923.jpg)  
Fig. 1.1.

$$
\| h \| = \sup  _ {x \in X} \| h (x) \| _ {Y} + \sup  _ {x \in X, y \in X} \frac {\| h (x) - h (y) \| _ {Y}}{\| x - y \| _ {X}}.
$$

We shall prove:

Theorem 1.2.6 (Hartman–Grobman)  > 0, such that there exists a unique homeomorphism $h : R ^ { n } \to R ^ { n }$ satisfying

$$
\phi_ {t} \circ h = h \circ \psi_ {t} \forall t, \tag {1.8}
$$

if $\| f \| _ { C ^ { 0 , 1 } } < \epsilon$ .

Proof. (1) First we prove the case $t = 1$ , and would rather consider the problem more generally: assume $g \in C ^ { 0 , 1 } ( R ^ { n } , R ^ { n } )$ with $\| g \| _ { C ^ { 0 , 1 } } < \epsilon$ . We are going to solve continuous mappings $h , k : R ^ { n } \to R ^ { n }$ satisfying

$$
h \circ (A + f) = (A + g) \circ h \text {a n d} k \circ (A + g) = (A + f) \circ k, \tag {1.9}
$$

respectively.

Let $0 < \epsilon < \| A ^ { - 1 } \| ^ { - 1 }$ , then both $( A + f ) ^ { - 1 }$ and $( A + g ) ^ { - 1 }$ exist. We decompose $R ^ { n } = E _ { u } \oplus E _ { s }$ and let $P _ { u }$ and $P _ { s }$ be the projections onto $E _ { u }$ and $E _ { s }$ , respectively. Let $r = h - i d$ , then equation (1.9) for $r$ is equivalent to

$$
(A + g) \circ (\mathrm {i d} + r) = (\mathrm {i d} + r) \circ (A + f),
$$

i.e.,

$$
A \circ r = f - g \circ (\operatorname {i d} + r) + r \circ (A + f),
$$

or

$$
\left\{ \begin{array}{l} P _ {u} r = A _ {u} ^ {- 1} \circ P _ {u} r \circ (A + f) + A _ {u} ^ {- 1} \circ P _ {u} f - A _ {u} ^ {- 1} \circ P _ {u} g \circ (\mathrm {i d} + r) \\ P _ {s} r = A _ {s} \circ P _ {s} r \circ (A + f) ^ {- 1} - P _ {s} f \circ (A + f) ^ {- 1} \\ \quad + P _ {s} g \circ (\mathrm {i d} + r) \circ (A + f) ^ {- 1}. \end{array} \right. \tag {1.10}
$$

We do the same for the equation of $k$ .

(2) From the decomposition $C ^ { 0 } ( R ^ { n } , R ^ { n } ) = C ^ { 0 } ( R ^ { n } , E _ { u } ) \bigoplus C ^ { 0 } ( R ^ { n } , E _ { s } )$ , we reduce (1.10) to the form of Theorem 1.2.5. Set $S r = A _ { u } ^ { - 1 } \circ P _ { u } r \circ ( A + f ) \oplus A _ { s } \circ$ $P _ { s } r \circ ( A + f ) ^ { - 1 }$ . Then $S \in { \bf L } ( C ^ { \cup } ( R ^ { n } , R ^ { n } ) , C ^ { \cup } ( R ^ { n } , R ^ { n } ) )$ , and $\| S \| < 1$ . Thus $T : = { \mathrm { i d } } - S$ is invertible. By setting

$$
\eta (f, g; r) = A _ {u} ^ {- 1} \circ (P _ {u} f - P _ {u} g \circ (\mathrm {i d} + r)) \bigoplus (- P _ {s} f + P _ {s} g \circ (\mathrm {i d} + r)) \circ (A + f) ^ {- 1},
$$

Then (1.10) is equivalent to

$$
T r = \eta (f, g; r).
$$

Since $\eta ( f , g ; r )$ is Lip. with respect to $r$ , with Lip. constant $< \epsilon$ , and $\| \eta ( f , g ; \theta ) \| ~ < ~ 2 \epsilon$ . For sufficiently small $\epsilon > 0$ , all conditions of Theorem 1.2.5 are met. There exists a unique continuous map $r$ in a neighborhood of $\theta$ in $C ^ { 0 } ( R ^ { n } , R ^ { n } )$ , which is continuously dependent on $f$ and $g$ . Similarly, let $q = k - i d$ , one proves the existence of a unique continuous map $q = q ( f , g )$ in the same neighborhood.

(3) Setting $h = \mathrm { i d } + r , k = \mathrm { i d } + q$ , we prove that $h \circ k = k \circ h = \operatorname { i d }$ . In fact $h \circ k$ and $k \circ h$ satisfy the equations:

$$
h \circ k = (A + g) ^ {- 1} \circ (h \circ k) \circ (A + g),
$$

and

$$
k \circ h = (A + f) ^ {- 1} \circ (k \circ h) \circ (A + f),
$$

respectively. Both the equations have the unique solution id in a neighborhood of $\theta$ , and the conclusion is proved.

(4) We now prove the conclusion for arbitrary $t$ . We have

$$
\phi_ {1} \circ (\phi_ {t} \circ h \circ \psi_ {- t}) = (\phi_ {t} \circ \phi_ {1}) \circ (h \circ \psi_ {- t}) = (\phi_ {t} \circ h \circ \psi_ {- t}) \circ \psi_ {1}.
$$

Also as $| t - 1 | < \delta$ for small $\delta > 0$ , $\phi _ { t } \circ h \circ \psi _ { - t } - \mathrm { i d } \in C ^ { \cup } ( R ^ { n } , R ^ { n } )$ is in a small neighborhood of $\theta$ , and the above equation has a unique solution $h$ there; therefore

$$
\phi_ {t} \circ h \circ \psi_ {- t} = h.
$$

Then we can extend the procedure step by step to all $t$ , i.e. we have $\phi _ { t } \circ h = h \circ \psi _ { t } \forall t \in R ^ { 1 }$ . □

Example 2. (Local existence of isothermal coordinates)

Given a surface $M ^ { 2 }$ with a Riemannian metric $g$ , i.e., in local coordinates $\boldsymbol { x } = ( x _ { 1 } , x _ { 2 } )$ ,

$$
g = E d x _ {1} ^ {2} + 2 F d x _ {1} d x _ {2} + G d x _ {2} ^ {2},
$$

where $E , F$ and $G$ are functions of local coordinates $\boldsymbol { x } ~ = ~ \left( x _ { 1 } , x _ { 2 } \right)$ , and $\forall ( x _ { 1 } , x _ { 2 } )$ , $E \xi ^ { 2 } + 2 F \xi \eta + G \eta ^ { 2 }$ is a positive definite quadratic form. Our problem is to find a local coordinate $\boldsymbol { u } ( \boldsymbol { x } ) = ( u _ { 1 } ( x _ { 1 } , x _ { 2 } ) , u _ { 2 } ( x _ { 1 } , x _ { 2 } ) )$ ) in a neighborhood of $x ^ { 0 } = ( x _ { 1 } ^ { 0 } , x _ { 2 } ^ { 0 } )$ such that there exists a function $\lambda = \lambda ( x _ { 1 } , x _ { 2 } ) > 0$ satisfying

$$
g = \lambda \left(x _ {1}, x _ {2}\right) \left(d u _ {1} ^ {2} + d u _ {2} ^ {2}\right).
$$

The local coordinate $u = ( u _ { 1 } , u _ { 2 } )$ is called an isothermal coordinate.

We shall find $u = ( u _ { 1 } , u _ { 2 } )$ satisfying

$$
\left\{ \begin{array}{l} \lambda (x) \mid \partial_ {x _ {1}} u (x) \mid^ {2} = E (x), \\ \lambda (x) \partial_ {x _ {1}} u (x) \cdot \partial_ {x _ {2}} u (x) = F (x), \\ \lambda (x) \mid \partial_ {x _ {2}} u (x) \mid^ {2} = G (x). \end{array} \right. \tag {1.11}
$$

We may assume $( x _ { 1 } ^ { 0 } , x _ { 2 } ^ { 0 } ) = ( 0 , 0 )$ and $F ( 0 , 0 ) \neq 0$ (by translation and rotation of the local coordinates). After eliminating $\lambda$ , (1.11) is equivalent to:

$$
\left\{ \begin{array}{l} F \left| \partial_ {x _ {1}} u \right| ^ {2} = E \partial_ {x _ {1}} u \cdot \partial_ {x _ {2}} u \\ F \left| \partial_ {x _ {2}} u \right| ^ {2} = G \partial_ {x _ {1}} u \cdot \partial_ {x _ {2}} u. \end{array} \right. \tag {1.12}
$$

This is a first-order nonlinear differential system.

In a neighborhood $O$ of $\theta = ( 0 , 0 )$ , define

$$
\varphi (x, \nabla u (x)) = \left( \begin{array}{c} F (x) \cdot | \partial_ {x _ {1}} u (x) | ^ {2} - E (x) \cdot \partial_ {x _ {1}} u (x) \cdot \partial_ {x _ {2}} u (x) \\ F (x) \cdot | \partial_ {x _ {2}} u (x) | ^ {2} - G (x) \cdot \partial_ {x _ {1}} u (x) \cdot \partial_ {x _ {2}} u (x), \end{array} \right),
$$

$\forall x \in O$ , and let $f$ be the map satisfying

$$
f (u) (x) = \varphi (x, \nabla u (x)).
$$

We want to solve the equation:

$$
f (u) = \theta . \tag {1.13}
$$

Let $x = \varepsilon y$ , where $y \in D$ , the unit disk on the coordinate plane, and $\varepsilon > 0$ is a parameter, and let

$$
\varepsilon v (y) = u (x) - \bar {u} (x). \tag {1.14}
$$

where ${ \overline { { u } } } = \left( p _ { 1 } x _ { 1 } + p _ { 2 } x _ { 2 } , q _ { 1 } x _ { 1 } + q _ { 2 } x _ { 2 } \right)$ , and $( p _ { 1 } , p _ { 2 } , q _ { 1 } , q _ { 2 } )$ is chosen such that the following conditions are satisfied:

$$
\frac {\partial (u _ {1} , u _ {2})}{\partial (x _ {1} , x _ {2})} \neq 0,
$$

and

$$
\varphi (0, \nabla \bar {u}) = \theta .
$$

In fact, this is an algebraic system with four unknowns, and is trivially solvable.

Equation (1.13) is equivalent to

$$
\varphi (\varepsilon y, \nabla_ {y} v (y) + \nabla_ {x} \overline {{u}} (\varepsilon y)) = 0.
$$

Let $\dot { C } ^ { 1 , \alpha } ( \bar { D } )$ denote the space $C ^ { 1 , \alpha } ( D )$ modulo a constant. Set $F : \mathbb { R } ^ { 1 } \times$ $( C ^ { 1 , \alpha } \cap C _ { 0 } ^ { 0 } ( \overline { { { D } } } ) \oplus \dot { C } ^ { 1 , \alpha } ( \overline { { { D } } } ) ) \to C ^ { \alpha } ( \overline { { { D } } } ) ^ { 2 }$ , for some $\alpha \in ( 0 , 1 )$ , where

$$
F (\varepsilon , v) = \varphi (\varepsilon y, \nabla_ {y} v (y) + \nabla_ {x} \bar {u} (\varepsilon y)).
$$

Thus

$$
F (0, \theta) = \theta .
$$

It remains to find a pair $( \varepsilon , v )$ for small $\varepsilon > 0$ , satisfying

$$
F (\varepsilon , v) = \theta .
$$

Then by the transform(1.14), the solution $u$ in a small neighborhood of $\theta \ \varepsilon D$ is obtained. Note that $\left( p _ { 1 } , q _ { 1 } , p _ { 2 } , q _ { 2 } \right) = \left( \overline { { u } } _ { 1 , x _ { 1 } } , \overline { { u } } _ { 2 , x _ { 1 } } , \overline { { u } } _ { 1 , x _ { 2 } } , \overline { { u } } _ { 2 , x _ { 2 } } \right)$ . Let us introduce the notation $( E _ { 0 } , F _ { 0 } , G _ { 0 } ) = ( E , F , G ) | _ { ( 0 , 0 ) }$ ; we have

$$
F _ {v} (0, \theta) h = - \left(\mathbf {A} _ {1} \nabla h _ {1} + \mathbf {A} _ {2} \nabla h _ {2}\right),
$$

where $h = ( h _ { 1 } , h _ { 2 } ) \in C ^ { 1 , \alpha } \cap C _ { 0 } ^ { 0 } ( \overline { { { D } } } ) \oplus \dot { C } ^ { 1 , \alpha } ( \overline { { { D } } } ) ,$ , and

$$
\mathbf {A} _ {1} = \left( \begin{array}{c c} (E _ {0} p _ {2} - 2 F _ {0} p _ {1}) & E _ {0} p _ {1} \\ G _ {0} p _ {2} & (G _ {0} p _ {1} - 2 F _ {0} p _ {2}) \end{array} \right),
$$

$$
\mathbf {A} _ {2} = \left( \begin{array}{c c} (E _ {0} q _ {2} - 2 F _ {0} q _ {1}) & E _ {0} q _ {1} \\ G _ {0} q _ {2} & (G _ {0} q _ {1} - 2 F _ {0} q _ {2}) \end{array} \right)  .
$$

This is a constant coefficient first-order linear differential operator with the $2 \times 2$ matrix symbol:

$$
L = \left(\mathbf {A} _ {1} \omega , \mathbf {A} _ {2} \omega\right),
$$

where $\boldsymbol \omega = ( \xi , \eta )$

Since

$$
\det L = - 2 F _ {0} (E _ {0} \xi^ {2} - 2 F _ {0} \xi \eta + G _ {0} \eta^ {2}) \frac {\partial (\overline {{u}} , \overline {{v}})}{\partial (x , y)} \neq 0
$$

$\forall ( \xi , \eta ) \in \mathbb { R } ^ { 2 } \backslash \{ \theta \}$ , $L$ is elliptic.

According to the elliptic theory, $F _ { v } ( 0 , \cdot )$ has a bounded inverse. The IFT is applied to conclude the existence of $\varepsilon _ { 0 } > 0$ such that the equation

$$
F (\varepsilon , v) = \theta
$$

has a unique solution $v _ { \varepsilon }$ , $\forall \varepsilon \in ( - \varepsilon _ { 0 } , \varepsilon _ { 0 } )$ .

Remark 1.2.7 The boundary condition for the first-order system can be deduced from the second-order elliptic theory. In fact, $h = ( h _ { 1 } , h _ { 2 } )$ satisfies

$$
\mathbf {A} _ {1} \nabla h _ {1} + \mathbf {A} _ {2} \nabla h _ {2} = f, \tag {1.15}
$$

where $f$ is given.

Suppose that ${ \bf A } _ { 2 }$ is invertible (it is available); we have

$$
\nabla h _ {2} = C \nabla h _ {1} + \mathbf {A} _ {2} ^ {- 1} f, \tag {1.16}
$$

where

$$
C = - \mathbf {A} _ {2} ^ {- 1} \mathbf {A} _ {1},
$$

Let

$$
E = \left( \begin{array}{c c} 0 & 1 \\ - 1 & 0 \end{array} \right)  .
$$

Then

$$
E ^ {- 1} = \left( \begin{array}{c c} 0 & - 1 \\ 1 & 0 \end{array} \right)  ,
$$

and $E ^ { 2 } = - I$

Since

$$
h _ {2, x y} = h _ {2, y x}, \tag {1.17}
$$

we obtain a second-order equation for $h _ { 1 }$ , in which the principal symbol of the second-order equation reads as

$$
\begin{array}{l} - \omega^ {T} E A _ {2} ^ {- 1} A _ {1} \omega = - \frac {1}{\det  (A _ {2})} E ^ {2} (A _ {2} \omega) ^ {T} E ^ {- 1} A _ {1} \omega \\ = \frac {1}{\det  (A _ {2})} (A _ {2} \omega) ^ {T} E ^ {- 1} A _ {1} \omega \\ = \frac {\det  (L)}{\det  (A _ {2})} . \\ \end{array}
$$

The later is positive (or negative) definite because the first-order system is elliptic.

Thus the Dirichlet boundary condition for $h _ { 1 }$ is well posed, and then $h _ { 2 }$ follows from (1.16) modulo a constant.

Remark 1.2.8 There are many applications of the IFT similar to the above examples, e.g. a necessary and sufficient condition for an almost complex structure being a complex structure (Newlander–Nirenberg theorem, see L. Nirenberg [NN]), prescribing Ricci curvature problem (see DeTurck [DT] etc.). For applications to boundary value problems in ordinary and partial differential equations see S.N. Chow, J. Hale [CH] and J. Mawhin [Maw 3]

# 1.2.3 Continuity Method

We have shown the usefulness of the IFT in the existence of solutions for small perturbations of a given equation which has a known solution. As to large perturbations, the IFT is not enough, we have to add new ingredients. The continuity method is a general principle, which can be applied to prove the existence of solutions for a variety of nonlinear equations.

Let $X$ and $Y$ be Banach spaces, and $f : X \to Y$ be $C ^ { 1 }$ . Find the solution of the equation:

$$
f (x) = \theta .
$$

Let us introduce a parameter $t \in [ 0 , 1 ]$ and a map

$$
F: [ 0, 1 ] \times X \to Y
$$

such that both $F$ and $F _ { x }$ are continuous; in addition,

$$
F (1, x) = f (x).
$$

Assume that there exists $x _ { 0 } ~ \in ~ X$ satisfying $F ( 0 , x _ { 0 } ) = \theta$ ; we want to extend the solution $x _ { 0 }$ of the equation

$$
F (0, x) = \theta
$$

to a solution of

$$
F (1, x) = \theta . \tag {1.18}
$$

For this purpose, we define a set

$$
S = \{t \in [ 0, 1 ] \mid \text {s u c h t h a t} F (t, x) = \theta \text {i s s o l v a b l e} \}.
$$

What we want to do is to prove:

(1) $S$ is an open set (relative to [0, 1]). For this purpose, it is sufficient to prove that $\forall t _ { 0 } ~ \in ~ S$ , $\exists x _ { t _ { 0 } } ~ \in ~ X$ , which solves $F ( t _ { 0 } , x _ { t _ { 0 } } ) ~ = ~ \theta$ such that $F _ { x } ^ { - 1 } ( t _ { 0 } , x _ { t _ { 0 } } ) \in L ( Y , X )$ , provided by the IFT.

(2) $S$ is a closed set. Usually it depends on the a priori estimates for the solution set $\{ x \in X | \exists t \in S$ , such that $F ( t , x ) = \theta \}$ . For most PDE problems it requires special knowledge and features of the equations and techniques in hard analysis.

We present here two major ideas:

(a) If there exist a Banach space $X _ { 1 }$ , which is compactly embedded in $X$ , and a constant $C > 0$ such that

$$
\| x _ {t} \| _ {X _ {1}} \leqslant C \quad \forall t \in S,
$$

where $x _ { t }$ is a solution of $F ( t , x ) = \theta$ , then $S$ is closed.

In fact, we have $\{ t _ { n } \} ^ { \infty } \subset S$ , which implies $t _ { n } \to t ^ { * }$ , and

$$
\left\| x _ {t _ {n}} \right\| _ {X _ {1}} \leqslant C.
$$

Since the embedding $X _ { 1 } \ \hookrightarrow \ X$ is compact, $x _ { t _ { n } }$ subconverges to some point $x ^ { * } \in X$ in $X$ . From the continuity, it follows that $F ( t ^ { * } , x ^ { * } ) = \theta$ . This proves $t ^ { \ast } \in S$ , i.e., $S$ is closed.

(b) If $\forall t \in S$ , there exists a unique local solution $x _ { t }$ of the equation $F ( t , \cdot ) = \theta$ , and if there exists $C > 0$ such that

$$
\| \dot {x} _ {t} \| _ {X} \leqslant C,
$$

where $\dot { x } _ { t }$ is the derivative of $x _ { t }$ , then $S$ is a closed set.

Proof. Let $\{ t _ { n } \} _ { 1 } ^ { \infty }$ be a sequence included in an open interval contained in $S$ , with $t _ { n } \uparrow t ^ { * }$ . Then

$$
\parallel x _ {t _ {n}} - x _ {t _ {m}} \parallel \leqslant \int_ {t _ {m}} ^ {t _ {n}} \parallel \dot {x} _ {t} \parallel d t \leqslant C (t _ {n} - t _ {m}) \rightarrow 0,
$$

as $n \geqslant m  \infty$ . Let $x ^ { * }$ be the limit. If the IFT is applicable to $( t ^ { * } , x ^ { * } )$ , then we have $t ^ { \ast } \in S$ , i.e., $S$ is closed. □

Once (1) and (2) are proved, $S$ is a nonempty ( $0 \in S$ ) open and closed set. Therefore $S = [ 0 , 1 ]$ , and then the equation $F ( 1 , \cdot ) = \theta$ is solvable.

As an application of the continuity method, we have:

Theorem 1.2.9 (Global implicit function theorem) Let $X , Y$ be Banach spaces, and let $f \in C ^ { 1 } ( X , Y )$ with $f ^ { \prime } ( x ) ^ { - 1 } \in L ( Y , X ) \ \forall x \in X$ . If constants $A , B > 0$ such that

$$
\| f ^ {\prime} (x) ^ {- 1} \| \leqslant A \| x \| + B \quad \forall x \in X,
$$

then $f$ is a diffeomorphism.

Proof. (1) Surjective. We want to prove that $\forall y \in Y , \exists x \in X$ satisfying

$$
f (x) = y.
$$

$\forall x _ { 0 } \in X$ , define $F : \lfloor 0 , 1 \rfloor \times X \to Y$ as follows:

$$
F (t, x) = f (x) - \left[ (1 - t) f \left(x _ {0}\right) + t y \right].
$$

Set $S = \{ t \in [ 0 , 1 ] |$ $F ( t , \cdot ) = \theta$ is solvable . Obviously, $0 \in S$ , and since $F _ { x } ^ { - 1 } ( t , x ) = f ^ { \prime } ( x ) ^ { - 1 } \in L ( Y , X )$ , $S$ is open, from the IFT.

It remains to prove the closeness of $S$ . Indeed, in a component $( a , b )$ of $S$ , there is a branch of solutions $x _ { t }$ satisfying

$$
F (t, x _ {t}) = \theta \quad \forall t \in (a, b)  ,
$$

and then

$$
f ^ {\prime} \left(x _ {t}\right) \dot {x} _ {t} = y - f \left(x _ {0}\right).
$$

Thus,

$$
\left\| \dot {x} _ {t} \right\| \leqslant \| f ^ {\prime} (x _ {t}) ^ {- 1} \| \| y - f (x _ {0}) \| \leqslant (A \| x _ {t} \| + B) \| y - f (x _ {0}) \| . \tag {1.19}
$$

Set $\textstyle c = { \frac { a + b } { 2 } }$ , so

$$
\parallel x _ {t} \parallel \leqslant \parallel x _ {c} \parallel + \int_ {c} ^ {t} \parallel y - f (x _ {0}) \parallel (A \parallel x _ {s} \parallel + B) d s \quad \text {a s} t > c.
$$

Applying Gronwall’s inequality, $\exists$ a constant $C > 0$ such that

$$
\left\| x _ {t} \right\| \leqslant C. \tag {1.20}
$$

Substituting (1.20) into (1.19), we have another constant $C _ { 1 } > 0$ such that

$$
\| \dot {x} _ {t} \| \leqslant C _ {1}, \forall t \in (a, b).
$$

This proves the closeness of $S$ , therefore $f$ is surjective.

(2) Injective. We argue by contradiction. If $\exists y \in Y$ and $x _ { 0 } , x _ { 1 } \in X$ , satisfying $f ( x _ { i } ) = y , \ i = 0 , 1$ . Let $\gamma : [ 0 , 1 ] \to X$ be the segment connecting these two points:

$$
\gamma (s) = (1 - s) x _ {0} + s x _ {1} \quad s \in [ 0, 1 ].
$$

Thus $f \circ \gamma$ is a loop passing through $y$ . If we could find $x : [ 0 , 1 ] \to X$ satisfying

$$
x (i) = x _ {i} \quad i = 0, 1, \quad \text {a n d}
$$

$$
f \circ x (s) = y \forall s \in [ 0, 1 ],
$$

then this would contradict with the locally homeomorphism of $f$ .

Define $I = [ 0 , 1 ]$ and $T : I \times C _ { 0 } ( I , X ) \to C _ { 0 } ( I , Y )$ as follows:

$$
(t, u (s)) \mapsto f (\gamma (s) + u (s)) - t y - (1 - t) f (\gamma (s)),
$$

where

$$
C _ {0} (I, X) = \left\{u \in C (I, X) \mid u (0) = u (1) = \theta \right\}.
$$

We want to solve

$$
T (t, u) = \theta .
$$

Obviously, $T ( 0 , \theta ) = \theta$ ; and if we have $u \in C _ { 0 } ( I , X )$ satisfying $T ( 1 , u ( \cdot ) ) = \theta$ , then $x ( s ) = u ( s ) + \gamma ( s )$ is what we need. Now,

![](images/7ce72bdf9e20abe5d99505bf9249f60b5b4ee2b733c0058954c0b16de1c33867.jpg)  
Fig. 1.2.

1.

$$
T _ {u} (t, u) = f ^ {\prime} (\gamma (\cdot) + u (\cdot)) \in L (C _ {0} (I, X), C _ {0} (I, Y)),
$$

which has a bounded inverse. Therefore $S ~ = ~ \{ t ~ \in ~ [ 0 , 1 ] |$ $T ( t , u ) ~ = ~ \theta$ is solvable is open, from the IFT.

2. Let $\boldsymbol { u } _ { t } ( s )$ be a solution at $t \in S$ . Then

$$
f ^ {\prime} (\gamma (s) + u _ {t} (s)) \cdot \dot {u} _ {t} (s) = y - f \circ \gamma (s),
$$

where $\dot { u } _ { t }$ denotes the derivative with respect to $t$ . Again we obtain

$$
\left\| \dot {u} _ {t} \right\| _ {C _ {0} (I, X)} \leqslant \left(A \left\| u _ {t} \right\| _ {C _ {0} (I, X)} + B _ {1}\right) \| y - f \circ \gamma \| _ {C _ {0} (I, Y)},
$$

where ${ B _ { 1 } } > 0$ is another constant depending on B and $x _ { 0 } , x _ { 1 }$ only. As in paragraph (1), $\exists C > 0$ such that

$$
\| \dot {u} _ {t} \| _ {C _ {0} (I, X)} \leqslant C \quad \forall t \in S.
$$

Again, by the continuity method, $1 \in S$ . This is the contradiction. The injectivity of $f$ is proved. □

Next we shall give an example showing how a priori estimates give the closeness of $S$ .

Theorem 1.2.10 Suppose $f \in C ^ { 1 } ( \overline { { \Omega } } \times R ^ { 1 } \times R ^ { n } , R ^ { 1 } )$ , where $\Omega \subset R ^ { n }$ is a bounded domain of smooth boundary. Assume $\exists$ constants $C > 0$ , satisfying

(1) There exists an increasing function $c : R _ { + } ^ { 1 } \to R _ { + } ^ { 1 }$ such that

$$
| f (x, \eta , \xi) | \leq c (| \eta |) (1 + | \xi | ^ {2}), \forall (x, \eta , \xi) \in \overline {{\Omega}} \times R ^ {1} \times R ^ {n}.
$$

(2)

$$
\frac {\partial f}{\partial \eta} (x, \eta , \xi) \leqslant 0.
$$

(3) Assume $\exists M > 0$ such that

$$
f (x, \eta , \theta) = \left\{ \begin{array}{l l} <   0, & i f \eta > M \\ > 0, & i f \eta <   - M  . \end{array} \right.
$$

Assume $\phi \in C ^ { 2 , \gamma }$ , for some $\gamma \in ( 0 , 1 )$ . Then the equation

$$
\left\{ \begin{array}{l} - \triangle u = f (x, u (x), \nabla u (x)) \quad x \in \Omega \\ u | _ {\partial \Omega} = \phi \end{array} \right. \tag {1.21}
$$

possesses a unique solution in $C ^ { 2 , \gamma }$ .

Lemma 1.2.11 Under the assumption (3), if $u \in C ^ { 2 } ( \overline { { \Omega } } )$ is a solution of (1.21), then

$$
\parallel u \parallel_ {C (\overline {{\Omega}})} \leqslant \max  \left\{\max  _ {\partial \Omega} | \phi (x) |, M \right\}.
$$

Proof. Assume that $\vert u ( x ) \vert$ attains its maximum at $x _ { 0 } \in \Omega$ . We divide our discussion into two cases.

(1) $x _ { 0 } \in \partial \Omega$ ; the proof is done.

(2) $x _ { 0 } \in \breve { \Omega }$ , then $\nabla u ( x _ { 0 } ) = 0$ and $- \triangle u ( x _ { 0 } ) = f ( x _ { 0 } , u ( x _ { 0 } ) , \theta )$

If $u ( x _ { 0 } ) > M$ , then $\mathrm { L H S } \geqslant 0$ $\geqslant 0$ , but RHS $< 0$ . It is impossible.

Similarly, if $u ( x _ { 0 } ) \ < \ - M$ , then LHS $\leqslant ~ 0$ , but RHS $> ~ 0$ . Again, it is impossible. We have

$$
\left| u \left(x _ {0}\right) \right| \leqslant M.
$$

□

Lemma 1.2.12 Assume $a \in C ^ { 0 , \gamma } ( \overline { { \Omega } } )$ and $\phi \in C ^ { \cdot 2 , \gamma } ( \partial \Omega )$ . Then the equation

$$
\left\{ \begin{array}{l} - \triangle u + u = a (x) \left(1 + | \nabla u | ^ {2}\right) \\ u | _ {\partial \Omega} = \phi \end{array} \right. \tag {1.22}
$$

has a unique solution $u \in C ^ { 2 , \gamma } ( \Omega )$ , and

$$
\parallel u \parallel_ {C ^ {2, \gamma}} \leqslant C (\parallel a \parallel_ {C ^ {0, \gamma}}, \parallel \phi \parallel_ {C ^ {2, \gamma}}).
$$

Proof. We apply the continuity method to study equation (1.22). Define a map $F : X \times [ 0 , 1 ] \to Y$ as follows:

$$
(u, \tau) \mapsto (- \triangle u + u - a (\tau + | \nabla u | ^ {2}), u | _ {\partial \Omega} - \tau \phi),
$$

where $X = C ^ { 2 , \overline { { { \gamma } } } } ( \Omega ) , Y = C ^ { 0 , \overline { { { \gamma } } } } ( \Omega ) \times C ^ { 2 , \overline { { { \gamma } } } } ( \partial \Omega )$ and $\overline { \gamma } \in ( 0 , \gamma )$

Noticing that

$$
F _ {u} (u, \tau) v = \left(\left(- \triangle v + v - a \nabla u \cdot \nabla v\right), v | _ {\partial \Omega}\right),
$$

and from the Schauder estimates, $\forall ( u , \tau ) \in X \times [ 0 , 1 ] , F _ { u } ( u , \tau )$ has a bounded inverse. Define the set:

$$
S = \left\{\tau \in [ 0, 1 ] \mid \exists u _ {\tau} \text {s o l v i n g} F (u _ {\tau}, \tau) = \theta \right\}.
$$

Equation (1.22) is solvable if $1 \in S$ . Since $S$ is open, and $0 \in S$ , it remains to verify that the set $S$ is closed. To this end, it is sufficient to prove that there is a constant $C$ , depending on $a$ and $\phi$ such that $\forall u _ { \tau } \in S$ ,

$$
\parallel u _ {\tau} \parallel_ {C ^ {2}, \gamma} \leqslant C. \tag {1.23}
$$

Since $u _ { \tau }$ satisfies $F ( u _ { \tau } , \tau ) = \theta$ , $\begin{array} { r } { \dot { u } _ { \tau } = \frac { d u _ { \tau } } { d \tau } } \end{array}$ exists, from the IFT, and satisfies:

$$
\left\{ \begin{array}{l} - \triangle \dot {u} _ {\tau} + \dot {u} _ {\tau} = a + 2 a \nabla u _ {\tau} \cdot \nabla \dot {u} _ {\tau}, \\ \dot {u} _ {\tau} | _ {\partial \Omega} = \phi . \end{array} \right. \tag {1.24}
$$

Set $g = a + 2 a \nabla u _ { \tau } \cdot \nabla \dot { u } _ { \tau }$ ; we have

$$
\begin{array}{l} \| \dot {u} _ {\tau} \| _ {W ^ {2, p}} \leqslant C (p) (1 + \| g \| _ {L ^ {p}}) \\ \leqslant C (p, \| a \| _ {C}) \left(1 + \| \nabla u _ {\tau} \| _ {L ^ {2 p}} \| \nabla \dot {u} _ {\tau} \| _ {L ^ {2 p}}\right), \\ \end{array}
$$

provided by the $L ^ { p }$ estimates.

Since $\dot { u } _ { \tau }$ satisfies (1.24), from Lemma 1.2.11, $\Vert \dot { u } _ { \tau } \ \Vert _ { C ( \overline { { \Omega } } ) }$ is bounded by a constant depending on $\| a \| _ { C ( \overline { { \Omega } } ) }$ and $\| \phi \|$ , and from the Gagliardo–Nirenberg inequality

$$
\| \nabla \dot {u} _ {\tau} \| _ {L ^ {2 p}} \leqslant C _ {p} \| \nabla^ {2} \dot {u} _ {\tau} \| _ {L ^ {p}} ^ {\frac {1}{2}} \| \dot {u} _ {\tau} \| _ {L ^ {p}} ^ {\frac {1}{2}} + C \| \dot {u} _ {\tau} \| _ {L ^ {\infty}},
$$

we obtain:

$$
\left\| \dot {u} _ {\tau} \right\| _ {W ^ {2, p}} \leqslant C \left(1 + \left\| \nabla u _ {\tau} \right\| _ {L ^ {2 p}} ^ {2}\right).
$$

Again, by the Gagliardo–Nirenberg inequality, we have

$$
\parallel \nabla u _ {\tau} \parallel_ {L ^ {2 p}} \leqslant C _ {p} \parallel \nabla^ {2} u _ {\tau} \parallel_ {L ^ {p}} ^ {\frac {1}{2}} \parallel u _ {\tau} \parallel_ {L ^ {p}} ^ {\frac {1}{2}} + C \parallel u _ {\tau} \parallel_ {L ^ {\infty}}.
$$

Repeating the use of Lemma 1.2.11, $\Vert \textbf { \em u } _ { \tau } \ : \parallel _ { C }$ is bounded by a constant depending on $\| \phi \| _ { C }$ , and we obtain

$$
\left\| \dot {u} _ {\tau} \right\| _ {W ^ {2, p}} \leqslant C (\left\| \phi \right\| _ {C ^ {2, \gamma}}, \| a \| _ {C}, p) (1 + \left\| u _ {\tau} \right\| _ {W ^ {2, p}}).
$$

From

$$
\frac {d}{d \tau} \| u _ {\tau} \| _ {W ^ {2, p}} \leqslant \| \dot {u} _ {\tau} \| _ {W ^ {2, p}},
$$

and the Gronwall inequality, we obtain

$$
\parallel u _ {\tau} \parallel_ {W ^ {2, p}} \leqslant C e ^ {C \tau},
$$

where $C$ depends on $p , \| \phi \| _ { C ^ { 2 , \gamma } }$ and $\| a \| _ { C }$ .

As a consequence of the Sobolev embedding theorem, for $\textstyle p > { \frac { \pi } { 1 - \gamma } }$ , we have

$$
\parallel u _ {\tau} \parallel_ {C ^ {1}, \gamma} \leqslant C (\parallel \phi \parallel_ {C ^ {2}, \gamma}, \parallel a \parallel_ {C}, \gamma). \tag {1.25}
$$

Substituting into the equation $F ( u _ { \tau } , \tau ) = \theta$ , we apply the Schauder estimate:

$$
\| u _ {\tau} \| _ {C ^ {2, \gamma}} \leqslant C (\| \phi \| _ {C ^ {2, \gamma}}, \| a \| _ {C ^ {0, \gamma}}, \gamma).
$$

The continuity method is applicable; we have a solution $u$ of (1.22).

Indeed, the solution is unique. Let $u _ { 1 } , u _ { 2 }$ be two solutions and set $\omega =$ $u _ { 1 } - u _ { 2 }$ , then

$$
\left\{ \begin{array}{l} - \triangle \omega + \omega = a \nabla (u _ {1} + u _ {2}) \cdot \nabla \omega \quad \text {i n} \Omega  , \\ \omega | _ {\partial \Omega} = 0  . \end{array} \right.
$$

If $\operatorname* { m a x } \omega > 0$ , then $\exists x _ { 0 } ~ \in ~ { \mathring { \Omega } }$ such that $\operatorname* { m a x } _ { \Omega } \omega = \omega ( x _ { 0 } )$ . Thus, $\nabla \omega ( x _ { 0 } ) =$ 0, $- \triangle \omega ( x _ { 0 } ) \geqslant 0$ and $\omega ( x _ { 0 } ) > 0$ . This is impossible. Similarly, one proves that $\operatorname* { m i n } \omega < 0$ is impossible. Therefore $\omega \equiv 0$ . □

Now we come back to the proof of the theorem.

Proof. Applying the continuity method, we study the equation:

$$
\left\{ \begin{array}{l} - \triangle u = t f (x, u (x), \nabla u (x)) \quad t \in [ 0, 1 ] \\ u | _ {\partial \Omega} = t \phi \end{array} \right. \tag {1.26}
$$

and turn to considering the operator:

$$
\begin{array}{l} F: I \times C ^ {2, \sigma} (\bar {\Omega}) \rightarrow C ^ {\sigma} (\bar {\Omega}) \times C ^ {2, \sigma} (\partial \Omega), \quad I = [ 0, 1 ], \\ (t, u) \mapsto (- \triangle u - t f (x, u (x), \nabla u (x)), u | _ {\partial \Omega} - t \phi), \\ \end{array}
$$

where $\sigma \in ( 0 , \gamma )$

We want to solve $F ( 1 , u ) = \theta$ . However, $\forall \overline { { u } } \in C ^ { 2 , \sigma } ( \overline { { \Omega } } )$ ,

$$
F _ {u} (t, \bar {u}) v = (- \triangle v - t f _ {\eta} (x, \bar {u} (x), \nabla \bar {u} (x)) v - t f _ {\xi} (x, \bar {u} (x), \nabla \bar {u} (x)) \nabla v, v | _ {\partial \Omega}).
$$

By assumption (2) and the maximum principle for linear elliptic equations, $\forall g \in C ^ { \sigma } ( \Omega ) \times C ^ { 2 , \sigma } ( \partial \Omega )$

$$
F _ {u} (t, \bar {u}) v = g
$$

has a unique solution, i.e., $F _ { u } ( t , \overline { { u } } ) : C ^ { 2 , \sigma } \cap C _ { 0 } ( \overline { { \Omega } } ) \to C ^ { \sigma } ( \overline { { \Omega } } ) \times C ^ { 2 , \sigma } ( \partial \Omega )$ has a bounded inverse. Thus the set

$$
S = \{t \in I | F (t, u) = \theta \text {i s}
$$

is open, from the IFT.

We prove that $S$ is closed. Noting that if $u _ { t }$ satisfies

$$
F (t, u _ {t}) = \theta ,
$$

then we set

$$
a _ {t} (x) = \frac {t f (x , u _ {t} (x) , \nabla u _ {t} (x)) + u _ {t} (x)}{1 + | \nabla u _ {t} (x) | ^ {2}}.
$$

By assumptions (1) and (3) and Lemma 1.2.11,

$$
\left\| a _ {t} \right\| _ {C} \leq C \left(\left\| u _ {t} \right\| _ {C}\right) \leq C (M, \left\| \phi \right\| _ {C}).
$$

Since $u _ { t }$ satisfies (1.22), in which $a$ is replaced by $a _ { t }$ , and $\phi$ by $t \phi$ . According to equation (1.25),

$$
\left\| u _ {t} \right\| _ {C ^ {1, \gamma}} \leq C \left(\left\| \phi \right\| _ {C ^ {2, \gamma}}, M, \gamma\right),
$$

and then

$$
\left\| a _ {t} \right\| _ {C ^ {0, \gamma}} \leq C \left(\left\| \phi \right\| _ {C ^ {2, \gamma}}, M, \gamma , f\right).
$$

Again by Lemma 1.2.12, we have

$$
\left\| u \right\| _ {C ^ {2, \gamma}} \leq C \left(\| \phi \| _ {C ^ {2, \gamma}}, M, \gamma , f\right).
$$

This is the required estimate. Since the embedding $C ^ { 2 , \gamma } ( { \overline { { \Omega } } } ) \hookrightarrow C ^ { 2 , \sigma } ( { \overline { { \Omega } } } ) \times$ $C ^ { 2 , \sigma } ( \partial \Omega )$ is compact, we conclude that $S$ is closed, as we have seen previously.

The existence of the solution for $F ( 1 , u ) = \theta$ follows from the continuity method. The uniqueness is a consequence of the maximum principle.

Once we obtain a solution $u$ in $C ^ { 2 , \sigma }$ , it follows directly by Schauder estimates that u C2,γ $u \in C ^ { 2 , \gamma }$ □

Remark 1.2.13 We shall return to this example by dropping assumption (2) in Chap. 3.

# 1.3 Lyapunov–Schmidt Reduction and Bifurcation

# 1.3.1 Bifurcation

We often meet an equation with a parameter $\lambda$ :

$$
F (x, \lambda) = 0.
$$

The following phenomenon has been observed: a branch of solutions $x ( \lambda )$ depending on $\lambda$ , is either disappeared or split into several branches, as $\lambda$ attains some critical values. This kind of phenomenon is called bifurcation. For example, a simple algebraic equation:

$$
x ^ {3} - \lambda x = 0 \quad \lambda \in \mathbb {R} ^ {1},
$$

![](images/6685fe5b564cf5f4b506d9c1746156fd211058094af3bbafe26062e702c90a48.jpg)  
Fig. 1.3.

has a solution $x = 0 \forall \lambda \in \mathbb { R } ^ { 1 }$ . As $\lambda \leqslant 0$ , this is the unique solution; but as $\lambda > 0$ , we have two more branches of solutions

$$
x = \pm \sqrt {\lambda}.
$$

See Figure 1.3.

Bifurcation phenomena occur extensively in nature. Early in 1744, Euler observed the bending of a rod pressed along the direction of its axis. Let $\theta$ be the angle between the real axis and the tangent of the central line of the rod, and let $\lambda$ be the pressure. The length of the rod is normalized to be $\pi$ . We obtain the following differential equation with the two free end point conditions:

$$
\left\{ \begin{array}{l} \ddot {\theta} + \lambda \sin \theta = 0  , \\ \dot {\theta} (0) = \dot {\theta} (\pi) = 0  . \end{array} \right.
$$

Obviously, $\theta \equiv 0$ is always a solution of the ODE. Actually the solution is unique, if $\lambda$ is not large. As $\lambda$ increasingly passes through a certain value $\lambda _ { 0 }$ , it is shown by experiment that there exists a bending solution $\theta \neq 0$ .

The same phenomenon occurs in the bending of plates, shells etc. In addition, bifurcation occurs in the study of thermodynamics (B´ernard problem), rotation of fluids, solitary waves, superconductivity and lasers.

Mathematically, we describe the bifurcation by the following:

Definition 1.3.1 Let $X , Y$ be Banach spaces, and let $\wedge$ be a topological space. Suppose that $F : X \times \land  Y$ is a continuous map. $\forall \lambda \in \Lambda$ , let

$$
S _ {\lambda} = \{x \in X | F (x, \lambda) = \theta \}
$$

be the solution set of the equation $F ( x , \lambda ) \ = \ \theta$ , where $\lambda$ is a parameter (Fig. 1.4). Assume $\theta \in S _ { \lambda } , \forall \lambda \in \wedge$ . We call $( \theta , \lambda _ { 0 } )$ a bifurcation point, if for any neighborhood $U$ of $( \theta , \lambda _ { 0 } )$ , there exists $( x , \lambda ) \in U$ with $x \in S _ { \lambda } \backslash \{ \theta \}$ .

![](images/3a8677a5316833d6d3a7a83c4668922345e929507335454f595685d069fd096b.jpg)  
Fig. 1.4.

The following problems are of primary concern:

(1) What is the necessary and sufficient condition for a bifurcation point $( \theta , \lambda _ { 0 } )$ ?   
(2) What is the structure of $S _ { \lambda }$ near $\lambda = \lambda _ { 0 }$ ?   
(3) How do we compute the solutions near the bifurcation points?   
(4) How about the global structure of $\cup _ { \lambda \in \wedge } S _ { \lambda }$   
(5) Let $F ( x , \lambda ) = \theta$ ∈∧ be the steady equation of the evolution equation:

$$
\dot {x} = F (x, \lambda),
$$

we study the stability of solutions in $S _ { \lambda }$ as $\lambda$ approaches $\lambda _ { 0 }$

In this section, we focus our discussions on problems (1) and (2). (4) will be studied in Chap. 3.

For simplicity, we assume that $U \subset X$ is an open neighborhood of the origin $\theta$ of the Banach space $X$ , and that $F : U \times \wedge \longrightarrow Y$ is continuous, and satisfies

$$
F (\theta , \lambda) = \theta \quad \forall \lambda \in \wedge .
$$

What is the necessary condition for a bifurcation point $\lambda _ { 0 } \in \wedge$ ?

(1) Assume that $F _ { x } ( x , \lambda )$ is continuous. If $( \theta , \lambda _ { 0 } )$ is a bifurcation point, then $F _ { x } ( \theta , \lambda _ { 0 } )$ does not have a bounded inverse.

Proof. By the IFT directly.

(2) Assume

$$
F (x, \lambda) = L x - \lambda x + N (x, \lambda),
$$

where $L \in L ( X , Y ) , \lambda \in \mathbb { R } ^ { 1 }$ , and that $N : U \times \mathbb { R } ^ { 1 } \to X$ is continuous with

$$
\parallel N (x, \lambda) \parallel = \circ (\parallel x \parallel) \text {a s} \parallel x \parallel \rightarrow \theta
$$

uniformly for $\lambda$ in a neighborhood of $\lambda _ { 0 }$ . If $( \theta , \lambda _ { 0 } )$ is a bifurcation point, then $\lambda _ { 0 } \in \sigma ( L )$ , i.e., $\lambda _ { 0 }$ is a spectrum of $L$ .

Proof. If not, $\lambda _ { 0 } \in \rho ( L )$ , the resolvent set of $L$ . Since $\rho ( L )$ is open, $\exists \varepsilon > 0$ and $C _ { \varepsilon } > 0$ such that

$$
\left\| \left(L - \lambda I\right) ^ {- 1} \right\| \leqslant C _ {\varepsilon} \text {a s} | \lambda - \lambda_ {0} | <   \varepsilon .
$$

It follows that

$$
\| x \| \leqslant \| (L - \lambda I) ^ {- 1} N (x, \lambda) \| = \circ (\| x \|) \text {i f} | \lambda - \lambda_ {\varepsilon} | <   \varepsilon , \text {a n d} x \rightarrow \theta , \text {a s} x \in S _ {\lambda}.
$$

Thus $\exists \delta > 0$ , such that

$$
B _ {\delta} \times (\lambda_ {0} - \varepsilon , \lambda_ {0} + \varepsilon) \cap S _ {\lambda} = \left\{\left(\theta , \lambda\right) \mid \left| \lambda - \lambda_ {0} \right| <   \varepsilon \right\},
$$

i.e., $( \theta , \lambda _ { 0 } )$ is not a bifurcation point.

(3) The above condition is not sufficient. For example, let $X = \mathbb { R } ^ { 2 }$ , let

$$
x = \left( \begin{array}{c} u \\ v \end{array} \right)  ,
$$

and let

$$
F (x, \lambda) = \left( \begin{array}{c} u \\ v \end{array} \right) - \lambda \left( \begin{array}{c} u \\ v \end{array} \right) + \left( \begin{array}{c} - v ^ {3} \\ u ^ {3} \end{array} \right)  .
$$

Obviously, $F _ { x } ( \theta , \lambda ) = ( 1 - \lambda ) I d$ . $\lambda = 1$ is in the spectrum, but $( \theta , \lambda )$ is not a bifurcation point, because:

$$
F (x, \lambda) = \theta \Leftrightarrow u ^ {4} + v ^ {4} = 0, \text {i . e .}, x = \theta .
$$

In order to study the sufficient condition and the local behavior of the solution set near its bifurcation points, we introduce the following:

# 1.3.2 Lyapunov–Schmidt Reduction

Let $X$ , $Y$ be Banach spaces, and let $\wedge$ be a topological space. Assume that $F : U \times \wedge \to Y$ is continuous, where $U \subset X$ is a neighborhood of $\theta$ . We assume that $F _ { x } ( \theta , \lambda _ { 0 } )$ is a Fredholm operator, i.e.,

(1) Im $F _ { x } ( \theta , \lambda _ { 0 } )$ is closed in $Y$ ,   
(2) $d = \dim \ker F _ { x } ( \theta , \lambda _ { 0 } ) < \infty$   
(3) $d ^ { * } = \mathrm { c o d i m ~ I m } { \cal F } _ { x } ( \theta , \lambda _ { 0 } ) < \infty .$

Set

$$
X _ {1} = \ker F _ {x} (\theta , \lambda_ {0}), Y _ {1} = \operatorname {I m} F _ {x} (\theta , \lambda_ {0}).
$$

Since both dim $X _ { 1 }$ , and codim $Y _ { 1 }$ are finite, we have the direct sum decompositions:

$$
X = X _ {1} \oplus X _ {2}, Y = Y _ {1} \oplus Y _ {2},
$$

and the projection operator $P : Y  Y _ { 1 }$ . $\forall x \in X$ , there exists a unique decomposition:

$$
x = x _ {1} + x _ {2}, x _ {i} \in X _ {i}, i = 1, 2.
$$

Thus

$$
F (x, \lambda) = \theta \Leftrightarrow \left\{ \begin{array}{l} P F (x _ {1} + x _ {2}, \lambda) = \theta  , \\ (I - P) F (x _ {1} + x _ {2}, \lambda) = \theta  . \end{array} \right.
$$

Now, $P F _ { x } ( \theta , \lambda _ { 0 } ) : X _ { 2 }  Y _ { 1 }$ is a surjection as well as an injection. According to the Banach theorem, it has a bounded inverse. If we already have $F ( \theta , \lambda _ { 0 } ) = \theta$ , then from the IFT, we have a unique solution

$$
u: V _ {1} \times V \to V _ {2}
$$

satisfying

$$
P F (x _ {1} + u (x _ {1}, \lambda), \lambda) = \theta ,
$$

where $V _ { i }$ is a neighborhood of $\theta$ in $U \cap X _ { i } , i = 1 , 2$ , and $V$ is a neighborhood of $\lambda _ { 0 }$ .

It remains to solve the equation:

$$
(I - P) F (x _ {1} + u (x _ {1}, \lambda), \lambda) = \theta
$$

on $V _ { 1 } \times V$ . This is a nonlinear system of $d$ variables and $d ^ { * }$ equations.

The above procedure is called the Lyapunov–Schmidt reduction. It reduces an infinite-dimensional problem to a finite-dimensional system. Many applied mathematicians have been doing the same reduction in their concrete problems in their own language.

Before going further, let us study the simple properties of the solution $x _ { 2 } = u ( x _ { 1 } , \lambda )$ .

Lemma 1.3.2 Under the assumptions (1), (2), (3) of the Lyapunov–Schmidt reduction, if $F \in C ^ { p } ( U \times \land , Y ) , p \geqslant 1$ , satisfies $F ( \theta , \lambda ) = \theta$ , where  is again a Banach space, then we have

$$
\begin{array}{l} u (\theta , \lambda) = \theta , \\ u ^ {\prime} (\theta , \lambda_ {0}) = \theta . \\ \end{array}
$$

If $p = 1$ , then

$$
u \left(x _ {1}, \lambda\right) = \circ (\left\| x _ {1} \right\| + \left| \lambda - \lambda_ {0} \right|);
$$

and if $p = 2$ , then

$$
u \left(x _ {1}, \lambda\right) = O \left(\left\| x _ {1} \right\| ^ {2} + \left| \lambda - \lambda_ {0} \right| ^ {2}\right).
$$

Proof. According to the IFT, the solution in $V _ { 1 } \times V \to V _ { 2 }$ is unique. Since $F ( \theta , \lambda ) = \theta$ , we have $u ( \theta , \lambda ) = \theta$ . Again, by the IFT,

$$
u ^ {\prime} (\theta , \lambda_ {0}) (\bar {x} _ {1}, \bar {\lambda}) = - (P F _ {x} (\theta , \lambda_ {0})) ^ {- 1} (P F _ {x} (\theta , \lambda_ {0}) \bar {x} _ {1} + P F _ {\lambda} (\theta , \lambda_ {0}) \bar {\lambda})
$$

$\forall ( { \overline { { x } } } _ { 1 } , { \overline { { \lambda } } } ) \in X _ { 1 } \times \wedge$

From $F ( \theta , \lambda ) = \theta$ , it follows that $F _ { \lambda } ( \theta , \lambda ) = 0$ . Therefore

$$
u ^ {\prime} (\theta , \lambda_ {0}) = \theta ,
$$

provided

$$
\bar {x} _ {1} \in X = \ker F _ {x} (\theta , \lambda_ {0}).
$$

The last two conclusions follow from Taylor’s formula.

□

Next, we turn to the case $d = d ^ { * } = 1$ .

Theorem 1.3.3 (Crandall–Rabinowitz) Suppose that $U ~ \subset ~ X$ is an open neighborhood of $\theta$ , and that $F \in C ^ { 2 } ( U \times \mathbb { R } ^ { 1 } , Y )$ satisfies $F ( \theta , \lambda ) = \theta$ . If $F _ { x } ( \theta , \lambda _ { 0 } )$ is a Fredholm operator with $d = d ^ { * } = 1$ , and if

$$
F _ {x \lambda} (\theta , \lambda_ {0}) u _ {0} \notin \operatorname {I m} F _ {x} (\theta , \lambda_ {0}) \tag {1.27}
$$

for all $u _ { 0 } \in \ker F _ { x } ( \theta , \lambda _ { 0 } ) \backslash \{ \theta \}$ , then $( \theta , \lambda _ { 0 } )$ is a bifurcation point, and there exists a unique $C ^ { 1 }$ curve $( \lambda , \psi ) : ( - \delta , \delta ) \to \mathbb { R } ^ { 1 } \times Z$ satisfying

$$
\left\{ \begin{array}{l} F (s u _ {0} + \psi (s), \lambda (s)) = \theta \\ \lambda (0) = \lambda_ {0}, \psi (0) = \psi^ {\prime} (0) = \theta  , \end{array} \right.
$$

where $\delta > 0$ , and $Z$ is the complement space of span $\{ u _ { 0 } \}$ in $X$ . Furthermore, there is a neighborhood of $( \theta , \lambda _ { 0 } )$ , in which

$$
F ^ {- 1} (\theta) = \left\{\left(\theta , \lambda\right) \mid \lambda \in \mathbb {R} ^ {1} \right\} \cup \left\{\left(s u _ {0} + \psi (s), \lambda (s)\right) \mid | s | <   \delta \right\}.
$$

Proof. Decompose the spaces $X$ and $Y$ according to the Lyapunov–Schmidt reduction, and write down the reduction equation. By assumptions,

$$
\ker F _ {x} (\theta , \lambda_ {0}) = \operatorname {s p a n} \left\{u _ {0} \right\},
$$

and $\exists \phi ^ { * } \in Y ^ { * } \backslash \{ \theta \}$ , such that $\ker \phi ^ { * } = \mathrm { I m } F _ { x } ( \theta , \lambda _ { 0 } )$ , from the Hahn–Banach theorem. The reduction equation reads as:

$$
g (s, \lambda) = \left\langle \phi^ {*}, F \left(s u _ {0} + u \left(s u _ {0}, \lambda\right), \lambda\right) \right\rangle = 0. \tag {1.28}
$$

We have the trivial solution $s \ = \ 0$ , and we look for a nontrivial solution. Noticing

$$
g _ {s} ^ {\prime} (0, \lambda_ {0}) = \left\langle \phi^ {*}, F _ {x} (\theta , \lambda_ {0}) (u _ {0} + u _ {s} ^ {\prime} (\theta , \lambda_ {0})) \right\rangle ,
$$

and $u _ { 0 } \in \ker F _ { x } ( \theta , \lambda _ { 0 } )$ , by applying Lemma 1.3.2, we obtain $g _ { s } ^ { \prime } ( 0 , \lambda _ { 0 } ) = 0$ . Therefore, it is impossible to get a solution $s = s ( \lambda )$ directly from the IFT. However, we may consider $\lambda$ as a function of $s$ , in this case, the only difficulty is that $g ( 0 , \lambda ) = 0$ for all $\lambda$ . Let us introduce a new function:

$$
h (s, \lambda) = \left\{ \begin{array}{l l} \frac {1}{s} g (s, \lambda) & \quad \text {a s} s \neq 0 \\ g _ {s} ^ {\prime} (0, \lambda) & \quad \text {a s} s = 0  . \end{array} \right.
$$

When $s \neq 0$ , the solutions of $h ( s , \lambda ) = 0$ are the same as (1.28). Here we define $g _ { s } ^ { \prime } ( 0 , \lambda )$ to be the value of $h$ at $s = 0$ , in order to make $h \in C ^ { 1 }$ .

We assume this conclusion at this moment, and postpone the verification to the end. Since

$$
h (0, \lambda_ {0}) = g _ {s} ^ {\prime} (0, \lambda_ {0}) = 0,
$$

and

$$
\begin{array}{l} h _ {\lambda} ^ {\prime} (0, \lambda_ {0}) = g _ {s \lambda} ^ {\prime \prime} (0, \lambda_ {0}) \\ = \left\langle \phi^ {*}, F _ {x \lambda} (\theta , \lambda_ {0}) \left(u _ {0} + u _ {s} ^ {\prime} (\theta , \lambda_ {0})\right) + F _ {x} (\theta , \lambda_ {0}) u _ {s \lambda} ^ {\prime \prime} (\theta , \lambda_ {0}) \right\rangle \\ = \left\langle \phi^ {*}, F _ {x \lambda} (\theta , \lambda_ {0}) u _ {0} \right\rangle \\ \neq 0. \\ \end{array}
$$

Again, the IFT is applied, and we obtain a $C ^ { 1 }$ -curve $\lambda = \lambda ( s ) , | s | < \delta$ , satisfying

$$
\left\{ \begin{array}{l} h (s, \lambda (s)) = 0  , \\ \lambda (0) = \lambda_ {0}  . \end{array} \right.
$$

Set

$$
\psi (s) = u \left(s u _ {0}, \lambda (s)\right);
$$

we have

$$
\begin{array}{l} \psi (0) = u (0, \lambda_ {0}) = \theta , \\ \psi^ {\prime} (0) = \nabla u (\theta , \lambda_ {0}) (u _ {0}, \lambda^ {\prime} (0)) = \theta , \\ \end{array}
$$

and

$$
g (s, \lambda (s)) = \left\langle \phi^ {*}, F \left(s u _ {0} + u \left(s u _ {0}, \lambda (s)\right), \lambda (s)\right) \right\rangle = 0;
$$

i.e.,

$$
F \left(s u _ {0} + \psi (s), \lambda (s)\right) = \theta .
$$

This is what we need.

Now we turn to verifying $h \in C ^ { 1 } ( B _ { \eta } )$ for some $\eta > 0$ , where $B _ { \eta } = \{ ( s , \lambda ) \in$ $\mathbb { R } ^ { 2 } | \mathbf { \alpha } | s | ^ { 2 } + | \lambda - \lambda _ { 0 } | ^ { 2 } < \eta ^ { 2 } \}$ . Indeed, only want to verify that $h$ is $C ^ { 1 }$ at $s = 0$ . By definition,

$$
\lim _ {s \to 0} \frac {1}{s} g (s, \lambda) = g _ {s} ^ {\prime} (0, \lambda),
$$

which implies the continuity of $h$ . Moreover,

$$
\begin{array}{l} h _ {s} ^ {\prime} (0, \lambda) = \lim  _ {s \rightarrow 0} \frac {1}{s} [ h (s, \lambda) - h (0, \lambda) ] \\ = \lim  _ {s \rightarrow 0} \frac {1}{s ^ {2}} [ g (s, \lambda) - g (0, \lambda) - g _ {s} ^ {\prime} (0, \lambda) s ] \\ = \frac {1}{2} g _ {s s} ^ {\prime \prime} (0, \lambda), \\ \end{array}
$$

therefore

$$
\begin{array}{l} h _ {s} ^ {\prime} (s, \lambda) - h _ {s} ^ {\prime} (0, \lambda) = \frac {1}{s ^ {2}} \left[ g _ {s} ^ {\prime} (s, \lambda) s - g (s, \lambda) - \frac {1}{2} g _ {s s} ^ {\prime \prime} (0, \lambda) s ^ {2} \right] \\ = \circ (1) \quad \text {a s} | s | \rightarrow 0. \\ \end{array}
$$

Since

$$
\begin{array}{l} h _ {\lambda} ^ {\prime} (0, \lambda) = g _ {s \lambda} ^ {\prime \prime} (0, \lambda), \\ h _ {\lambda} ^ {\prime} (s, \lambda) - h _ {\lambda} ^ {\prime} (0, \lambda) = \frac {1}{s} g _ {\lambda} ^ {\prime} (s, \lambda) - g _ {s \lambda} ^ {\prime \prime} (0, \lambda) \rightarrow 0 \quad \mathrm {a s} s \rightarrow 0. \\ \end{array}
$$

The proof is complete.

□

![](images/5964189f00395ad96b34f3abd137471da33a333873ce6aafa0d8838d8c69b9e2.jpg)  
Fig. 1.5.

We present two simple applications.

Example 1. (Euler elastic rod) We study the bending problem raised at the beginning of this section:

$$
\left\{ \begin{array}{l} \ddot {\varphi} + \lambda \sin \varphi = 0 \quad \text {i n} (0, \pi)  , \\ \dot {\varphi} (0) = \dot {\varphi} (\pi) = 0  . \end{array} \right.
$$

Let

$$
X = \left\{u \in C ^ {2} [ 0, \pi ] | \dot {u} (0) = \dot {u} (\pi) = 0) \right\},
$$

$$
Y = C [ 0, \pi ];
$$

and let $F : X \times R ^ { 1 } \to Y$ be the map:

$$
(u, \lambda) \mapsto u ^ {\prime \prime} + \lambda \sin u.
$$

It is a continuous map satisfying $F ( \theta , \lambda ) = \theta$ . According to the necessary condition, if $( \theta , \lambda )$ is a bifurcation point, then $\lambda$ is in the spectrum of the linearized operator $\textstyle { \bigl ( } { \frac { d } { d t } } { \bigr ) } ^ { 2 } + \lambda \operatorname { I }$ , i.e.,

$$
\lambda = n ^ {2}, \mathrm {f o r s o m e} n = 1, 2, \ldots
$$

Since $\begin{array} { r }  \ker \big ( \big ( \frac { d } { d t } \big ) ^ { 2 } + n ^ { 2 } I \big ) \ = \ \{ s \cos n t | \ s \ \in \ \mathbb { R } ^ { 1 } \} \end{array}$ , and since the differential operator $\textstyle { \Bigl ( } { \frac { d } { d t } } { \Bigr ) } ^ { 2 }$ under the free end point condition is self-adjoint, we have

$$
\operatorname {c o k e r} F _ {u} (\theta , n ^ {2}) = \left\{s \cos n t \mid s \in \mathbb {R} ^ {1} \right\}.
$$

In addition

$$
F _ {u \lambda} ^ {\prime \prime} (\theta , n ^ {2}) = \cos n u | _ {u = \theta} = \mathrm {I},
$$

thus

$$
F _ {u \lambda} ^ {\prime \prime} (\theta , n ^ {2}) \cos n t = \cos n t \notin \operatorname {I m} F _ {u} (\theta , n ^ {2}).
$$

All the assumptions in the Crandall–Rabinowitz theorem are satisfied, and we obtain a family of $C ^ { 1 }$ curves $( \lambda _ { n } ( s ) , \psi _ { n } ( s ) ) : ( - \delta , \delta ) \to \mathbb { R } ^ { 1 } \times Z _ { n }$ , where $Z _ { n }$ is the complement of $\operatorname { s p a n } \{ \cos n t \}$ , satisfying

$$
\lambda_ {n} (0) = n ^ {2},
$$

$$
\frac {d}{d s} \psi_ {n} (0) = \psi_ {n} (0) = \theta ,
$$

for $n = 1 , 2 , 3 , \ldots$

If we set

$$
\varphi_ {n} (s, t) = s \cos n t + (\psi_ {n} (s)) (t) \quad t \in [ 0, \pi ],
$$

then

$$
\begin{array}{l} \left(\frac {\partial}{\partial t}\right) ^ {2} \varphi_ {n} (s, t) + \lambda_ {n} (s) \sin \varphi_ {n} (s, t) = 0 \quad 0 <   t <   \pi , | s | <   \delta , \\ \frac {\partial}{\partial t} \varphi_ {n} (s, 0) = \frac {\partial}{\partial t} \varphi_ {n} (s, \pi) = 0. \\ \end{array}
$$

We obtain the bifurcation diagram of Fig. 1.6.

![](images/a37787361c038a0500299dbefa0b7d37ab13d23ac1b6659b017e9f1c5306c2c2.jpg)  
Fig. 1.6.

Example 2. We consider the following elliptic BVP. Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open domain with smooth boundary $\partial \Omega$ , and $p \in ( 1 , \infty )$ :

$$
\left\{ \begin{array}{l} - \triangle u - \lambda u = | u | ^ {p - 1} u \quad \text {i n} \Omega  , \\ u | _ {\partial \Omega} = 0  . \end{array} \right.
$$

Set

$$
\begin{array}{l} X = C ^ {2, \gamma} \cap C _ {0} (\overline {{\Omega}}), \text {a n d} Y = C ^ {\gamma} (\overline {{\Omega}}) \text {f o r s o m e} \gamma \in (0, 1), \\ F: (u, \lambda) \mapsto - \triangle u - \lambda u - | u | ^ {p - 1} u, \\ \end{array}
$$

we have

$$
F _ {u} (\theta , \lambda) = - \triangle - \lambda I.
$$

Thus, for $( \theta , \lambda )$ being a bifurcation point, $- \lambda$ has to be in the spectrum of the Laplacian under the 0-Dirichlet data.

Let us first consider the first eigenvalue $\lambda _ { 1 }$ , which is simple, i.e., $\ker - \triangle - \lambda 1$ is one dimensional; let $\varphi _ { 1 }$ be the associate eigenfunction. By the same reasoning, coker $F _ { u } ( \theta , \lambda _ { 1 } )$ is also one dimensional, and

$$
F _ {u \lambda} (\theta , \lambda_ {1}) \varphi_ {1} = - \varphi_ {1} \notin \operatorname {I m} F _ {u} (\theta , \lambda_ {1}).
$$

Again, we apply the Crandall–Rabinowitz theorem, and conclude that $( \theta , \lambda _ { 1 } )$ is a bifurcation point, and in its neighborhood, the solution set is the $C ^ { 1 }$ curve

$$
(- \delta , \delta) \mapsto (s \varphi_ {1} (x) + \psi (s) (x), \lambda_ {1} (s))
$$

plus the trivial solution set $( \theta , \lambda )$ , where $\psi ( s ) \in Z$ , and $Z$ is the complement of $\operatorname { s p a n } \{ \varphi _ { 1 } \}$ in $C ^ { 2 , \gamma } \cap C _ { 0 } ( \Omega )$ .

We have not studied eigenvalues other than $\lambda _ { 1 }$ , because we do not know if they are simple, i.e., if the condition $d = 1$ is satisfied. In fact, if $\lambda$ is a simple eigenvalue, then we have the similar result.

For some special cases, in which $d \neq 1$ , we can extend Theorem 1.3.3 as follows:

Theorem 1.3.4 $L e t \wedge , X$ and $Y$ be Banach spaces. Suppose that $F \in C ^ { 2 } ( X \times$ $\textstyle \bigwedge , Y )$ has the form

$$
F (x, \lambda) = L (\lambda) x + P (x, \lambda),
$$

where $L ( \lambda ) \in L ( X , Y ) \ \forall \lambda \in \Lambda$ , and

$$
P (\theta , \lambda) = \theta , P _ {x} (\theta , \lambda) = \theta , P _ {x \lambda} (\theta , \lambda_ {0}) = \theta f o r s o m e \lambda_ {0} \in \wedge .
$$

If there exist $u _ { 0 } \in \ker L ( \lambda _ { 0 } ) \backslash \{ \theta \}$ and a closed linear subspace $Z \subset X$ such that $( z , \lambda ) \mapsto L ( \lambda _ { 0 } ) z + \lambda L ^ { \prime } ( \lambda _ { 0 } ) u _ { 0 } : Z \times \Lambda \to Y$ is a linear homeomorphism, then there exist a neighborhood $U$ of $( \theta , \lambda _ { 0 } )$ in $( s p a n \{ u _ { 0 } \} \times Z ) \times \Lambda , \delta > 0$ , and a $C ^ { 1 }$ map: $( - \delta , \delta ) \to Z \times \wedge$ , defined by $s \mapsto ( \varphi ( s ) , \lambda ( s ) )$ satisfying

$$
F ^ {- 1} (\theta) \cap U \backslash \left\{\left(\theta , \lambda\right) \mid \lambda \in \Lambda \right\} = \left\{\left(s \left(u _ {0} + \varphi (s)\right), \lambda (s)\right) \mid | s | <   \delta \right\},
$$

and

$$
(\varphi (0), \lambda (0)) = (\theta , \lambda_ {0}).
$$

Proof. Similar to Theorem 1.3.3, we define

$$
\Phi (s, z, \lambda) = \left\{ \begin{array}{l l} \frac {1}{s} F (s (u _ {0} + z), \lambda) & \quad \text {a s} s \neq 0 \\ F _ {x} (\theta , \lambda) (u _ {0} + z) & \quad \text {a s} s = 0  . \end{array} \right.
$$

One wants to verify that $\Phi \in C ^ { 1 } ( ( \mathbb { R } ^ { 1 } \times Z ) \times \wedge , Y )$

It is sufficient to verify the continuous differentiability at $s = 0$ . Since

$$
\begin{array}{l} \Phi (s, z, \lambda) - \Phi (0, z, \lambda) = s ^ {- 1} [ F (s (u _ {0} + z), \lambda) - F (\theta , \lambda) - F _ {x} (\theta , \lambda) (s (u _ {0} + z)) ] \\ = s \int_ {0} ^ {1} \int_ {0} ^ {1} F _ {x x} (r t s (u _ {0} + z), \lambda) t d t d r \cdot (u _ {0} + z) ^ {2}, \\ \end{array}
$$

we have

$$
\Phi_ {s} (0, z, \lambda) = \frac {1}{2} F _ {x x} (\theta , \lambda) (u _ {0} + z) ^ {2}.
$$

Furthermore as $s \to 0$ ,

$$
\begin{array}{l} \Phi_ {s} (s, z, \lambda) - \Phi_ {s} (0, z, \lambda) \\ = - s ^ {- 2} \left[ F (s (u _ {0} + z), \lambda) - s F _ {x} (\theta , \lambda) (u _ {0} + z) - \frac {s ^ {2}}{2} F _ {x x} (\theta , \lambda) (u _ {0} + z) ^ {2} \right] \\ + s ^ {- 1} \left[ F _ {x} \left(s \left(u _ {0} + z\right), \lambda\right) - F _ {x} (\theta , \lambda) \right] \left(u _ {0} + z\right) \\ = o (1), \\ \end{array}
$$

and

$$
\begin{array}{l} \Phi_ {\lambda} (s, z, \lambda) - \Phi_ {\lambda} (0, z, \lambda) \\ = s ^ {- 1} \left[ F _ {\lambda} \left(s \left(u _ {0} + z\right), \lambda\right) - F _ {\lambda} (\theta , \lambda) - F _ {x \lambda} (\theta , \lambda) s \left(u _ {0}, z\right) \right] \\ = o (1). \\ \end{array}
$$

Thus, $\Phi \in C ^ { 1 }$ . Now, we have

$$
\begin{array}{l} \Phi (0, z, \lambda) = L (\lambda) (u _ {0} + z) + P _ {x} (\theta , \lambda) (u _ {0} + z), \\ \Phi (0, \theta , \lambda_ {0}) = L (\lambda_ {0}) u _ {0} + P _ {x} (\theta , \lambda_ {0}) u _ {0} = \theta , \\ \end{array}
$$

and

$$
\begin{array}{l} \Phi_ {(z, \lambda)} (0, \theta , \lambda_ {0}) (\bar {z}, \bar {\lambda}) = L (\lambda_ {0}) \bar {z} + L ^ {\prime} (\lambda_ {0}) u _ {0} \cdot \bar {\lambda} + P _ {\lambda x} (\theta , \lambda_ {0}) u _ {0} + P _ {x} (\theta , \lambda_ {0}) \bar {z}, \\ = L \left(\lambda_ {0}\right) \bar {z} + \bar {\lambda} L ^ {\prime} \left(\lambda_ {0}\right) u _ {0}, \\ \end{array}
$$

$\forall ( { \overline { { z } } } , { \overline { { \lambda } } } ) \in Z \times \Lambda$ . By the assumption, the last linear operator is a homeomorphism; then the IFT is applied. Therefore $\Finv$ a neighborhood $U$ of $( \theta , \lambda _ { 0 } )$ , and a unique $C ^ { 1 }$ -curve: $s \mapsto ( \varphi ( s ) , \lambda ( s ) ) \in Z \times \Lambda \ \forall \vert s \vert < \varepsilon$ , satisfying

$$
\left\{ \begin{array}{l} (\varphi (0), \lambda (0)) = (\theta , \lambda_ {0})  , \\ \Phi (s, \varphi (s), \lambda (s)) = \theta , i. e., F (s (u _ {0} + \varphi (s)), \lambda (s)) = \theta  . \end{array} \right.
$$

The proof is complete.

The above theorem is applied to Hopf bifurcations in ODE.

Example 3. (Hopf bifurcation)

We study the periodic solutions for a linear ordinary differential system:

$$
\dot {x} = A x, \tag {1.29}
$$

where $x \in C ^ { 1 } ( [ 0 , 2 \pi ] , \mathbb { R } ^ { n } )$ and $A \in M ( n , \mathbb { R } )$ , the $n \times n$ matrix. Let

$$
A = \left( \begin{array}{c c} B & 0 \\ 0 & C \end{array} \right)  ,
$$

where

$$
B = \left( \begin{array}{c c} 0 & 1 \\ - 1 & 0 \end{array} \right), \text {a n d} C \in M (n - 2, \mathbb {R})  .
$$

We assume that $( e ^ { 2 \pi C } - \mathrm { I } )$ is invertible. Obviously the linear system (1.29) admits a family of periodic solutions with two real parameters $a$ and $b$ :

$$
x (t) = \left( \begin{array}{c} a \cos t + b \sin t \\ - a \sin t + b \cos t \\ 0 \\ \vdots \\ 0 \end{array} \right),   \forall a, b \in \mathbb {R} ^ {1}  .
$$

Now, we perturb $A$ by introducing a parameter $\mu \in ( - 1 , 1 )$ , and consider the nonlinear differential system

$$
\dot {x} = A (\mu) x + P (x, \mu), \tag {1.30}
$$

where

$$
A (\mu) = \left( \begin{array}{c c} B (\mu) & 0 \\ 0 & C (\mu) \end{array} \right),   B (\mu) = \left( \begin{array}{c c} \mu & \beta (\mu) \\ - \beta (\mu) & \mu \end{array} \right)  ,
$$

and $C ( \mu ) \in M ( n - 2 , \mathbb { R } )$ , satisfying $e ^ { 2 \pi C ( \mu ) } - \mathrm { I } \in G L ( n - 2 , \mathbb { R } )$ . We assume that

$$
P: \mathbb {R} ^ {n} \times (- 1, 1) \to \mathbb {R} ^ {n}
$$

satisfies

$$
P (\theta , \mu) = P _ {x} (\theta , 0) = P _ {x, \mu} (\theta , 0) = \theta .
$$

and that $\beta : ( - 1 , 1 ) \to \mathbb { R } ^ { 1 }$ , satisfies

$$
\beta (0) = 1, \text {a n d} \beta^ {\prime} (0) \neq 0.
$$

Obviously, $\forall \mu \in ( - 1 , 1 ) , \ x = \theta$ is always a solution. We are interested in nontrivial solutions bifurcating from the branch of trivial solutions.

Theorem 1.3.5 Suppose that $A ( \mu )$ and $P ( x , \mu )$ are $C ^ { 2 }$ functions satisfying the above assumptions. Then $\Finv$ positive constants $a _ { 0 } , \delta _ { 0 }$ and a $C ^ { 1 }$ map

$$
(\mu , \omega , x): \left(- a _ {0}, a _ {0}\right) \mapsto \mathbb {R} ^ {1} \times \mathbb {R} ^ {1} \times C ^ {1} \left(\mathbb {R} ^ {1}, \mathbb {R} ^ {n}\right),
$$

satisfying

$$
\mu (0) = 0, \omega (0) = 1,
$$

where $x ( a )$ is a $2 \pi \omega ( a )$ -periodic function, which satisfies (1.30), and is of the form:

$$
(x (a)) (t) = \left( \begin{array}{c} a \sin \omega (a) ^ {- 1} t \\ a \cos \omega (a) ^ {- 1} t \\ 0 \\ \vdots \\ 0 \end{array} \right) + \circ (| a |).
$$

Furthermore, every $2 \pi \omega ( a )$ -periodic solution $y ( t )$ of (1.30), satisfying $| y ( t ) | <$ $\delta _ { 0 } ~ \forall t$ , coincides with $x ( a ) ( t )$ modulating a phase shift when $| \mu | < \alpha _ { 0 }$ and $| \omega - 1 | < \delta _ { 0 }$ .

Proof. Since $\omega$ depends on $\mu$ , we introduce a new scale $\tau$ , and let $t = \omega \tau$ ; (1.30) is rewritten as

$$
\frac {d x}{d \tau} = \omega A (\mu) x + \omega P (x, \mu). \tag {1.31}
$$

We find the $2 \pi$ -periodic solution of (1.31). Set

$$
\begin{array}{l} \wedge = \mathbb {R} ^ {2}, \lambda = (\omega , \mu) \in \wedge , \\ X = C _ {2 \pi} ^ {1} (\mathbb {R} ^ {n}) = \left\{u \in C ^ {1} (\mathbb {R}, \mathbb {R} ^ {n}) \mid u \text {i s} 2 \pi \text {p e r i o d i c} \right\}, \\ Y = C _ {2 \pi} (\mathbb {R} ^ {n}) = \left\{u \in C (\mathbb {R}, \mathbb {R} ^ {n}) | u \text {i s} 2 \pi \text {p e r i o d i c} \right\}, \\ \end{array}
$$

and set

$$
\begin{array}{l} L (\lambda) x = \frac {d x}{d \tau} - \omega A (\mu) x, \\ \mathbb {P} (x, \lambda) = \omega P (x, \mu), \\ \lambda_ {0} = (1, 0). \\ \end{array}
$$

Let

$$
u _ {0} = \left( \begin{array}{c} \sin \tau \\ \cos \tau \\ 0 \\ \vdots \\ 0 \end{array} \right), a n d u _ {1} = \left( \begin{array}{c} \cos \tau \\ - \sin \tau \\ 0 \\ \vdots \\ 0 \end{array} \right);
$$

then we have

$$
\ker L \left(\lambda_ {0}\right) = \operatorname {s p a n} \left\{u _ {0}, u _ {1} \right\}.
$$

Define

$$
Z = \left\{z \in X \mid \int_ {0} ^ {2 \pi} z (t) u _ {j} (t) d t = 0, j = 0, 1 \right\}.
$$

It remains to verify the following conditions: $\forall y \in Y$ , the linear equation

$$
\begin{array}{l} L (\lambda_ {0}) z + \lambda L ^ {\prime} (\lambda_ {0}) u _ {0} \\ = \frac {d z}{d \tau} - A (0) z - \omega A (0) u _ {0} - \mu A ^ {\prime} (0) u _ {0} \\ = y \\ \end{array}
$$

has a unique solution $( \omega , \mu , z ) \in \mathbb { R } ^ { 2 } \times Z$ .

![](images/0da1e32fa843f910fb6d795b0ced1e3c9356f7f07dac59bcb700c3d756257bfc.jpg)  
Fig. 1.7. Nontrivial solutions

According to the Fredholm alternative, this is equivalent to saying that $\forall y \in Y , \exists | ( \omega , \mu ) \in \mathbb { R } ^ { 2 }$ such that

$$
y + \omega A (0) u _ {0} + \mu A ^ {\prime} (0) u _ {0} \in \ker L ^ {*} (\lambda_ {0}) ^ {\perp}.
$$

However, $\ker L ^ { * } ( \lambda _ { 0 } ) = \ker L ( \lambda _ { 0 } )$ ; this is again equivalent to

$$
\det  \left| \begin{array}{c c} \int_ {0} ^ {2 \pi} (A (0) u _ {0} (t)) u _ {0} (t) d t & \int_ {0} ^ {2 \pi} (A ^ {\prime} (0) u _ {0} (t)) u _ {0} (t) d t \\ \int_ {0} ^ {2 \pi} (A (0) u _ {0} (t)) u _ {1} (t) d t & \int_ {0} ^ {2 \pi} (A ^ {\prime} (0) u _ {0} (t)) u _ {1} (t) d t \end{array} \right| \neq 0  .
$$

Computing the determinant directly, it equals $- 4 \pi ^ { 2 } \beta ^ { \prime } ( 0 )$ . Thus all assumptions in Theorem 1.3.4 are satisfied. We obtain our conclusion. □

This kind of bifurcation phenomenon is shown in Fig. 1.7.

# 1.3.3 A Perturbation Problem

We present here an example of perturbation problems. This is a nonlinear Schr¨odinger equation (NSE); we look for a nonspreading wave packet solution. Assume that the potential $V \in C ^ { 2 } ( R ^ { 1 } )$ is bounded, and the standing wave $\psi = \psi ( t , x )$ satisfies the following NSE:

$$
i h \frac {\partial \psi}{\partial t} = - \frac {h ^ {2}}{2} \frac {\partial^ {2} \psi}{\partial x ^ {2}} + V (x) \psi - \psi^ {3}.
$$

where $h > 0$ is a small constant, $\psi ( t , x ) = \exp ( - i \lambda t / h ) \varphi ( x )$ with $\varphi ( { \boldsymbol { x } } ) \to 0$ as $| x | \to \infty$ , and $\lambda < \operatorname { i n f } V$ . It is reduced to the following equation:

$$
- \frac {h ^ {2}}{2} \varphi^ {\prime \prime} (x) + V (x) \varphi (x) = \lambda \varphi (x) + \varphi (x) ^ {3}.
$$

If $V$ has a nondegenerate critical point $x _ { 0 }$ , i.e., $V ^ { \prime } ( x _ { 0 } ) = 0 , V ^ { \prime \prime } ( x _ { 0 } ) \ne 0$ , we make a change of variables $\begin{array} { r } { y = \frac { x - x _ { 0 } } { h } } \end{array}$ , and the equation is reduced to

$$
- \frac {1}{2} \varphi^ {\prime \prime} (y) + \left(V _ {h} (y) - \lambda\right) \varphi (y) = \varphi (y) ^ {3}, \tag {1.32}
$$

where $V _ { h } ( y ) = V ( x _ { 0 } + h y )$ and $\varphi ( y ) \to 0$ , as $| y | \to \infty$

This is a perturbation problem, because $h > 0$ is small. The special case where $h = 0$ reads as:

$$
- \frac {1}{2} \varphi^ {\prime \prime} + E \varphi = \varphi^ {3}, \tag {1.33}
$$

where $E = V ( x _ { 0 } ) - \lambda$ , and $\varphi ( y ) \to 0$ as $| y | \to \infty$ . The equation has a special solution:

$$
z (y) = \alpha \operatorname {s e c h} (\alpha y), \alpha = (2 E) ^ {1 / 2}.
$$

Let $\tau _ { \theta } \varphi ~ = ~ \varphi ( y - \theta )$ . Since equation (1.33) is autonomous, it has a onedimensional solution manifold $Z = \{ z _ { \theta } = \tau _ { \theta } z | \theta \in R ^ { 1 } \}$ . It is easily check: $T _ { z } Z = \mathrm { s p a n } \{ e \}$ , where $e = z ^ { \prime }$ , and then $T _ { z _ { \theta } } Z = \operatorname { s p a n } \{ \tau _ { \theta } e \}$ . We write $e _ { \theta } = \tau _ { \theta } e$ .

Let us define $X = H ^ { 2 } ( R ^ { 1 } ) , Y = L ^ { 2 } ( R ^ { 1 } )$ , and

$$
F (h, u) = - \frac {1}{2} u ^ {\prime \prime} + (V _ {h} - \lambda) u - u ^ {3}.
$$

Since $V$ is bounded, $F : R ^ { 1 } \times X \to Y$ .

Thus, $F ( 0 , z _ { \theta } ) = 0 , \forall \theta \in R ^ { 1 }$ . Since

$$
L _ {\theta} := F _ {u} ^ {\prime} (0, z _ {\theta}) = - \frac {1}{2} \frac {d ^ {2}}{d t ^ {2}} + (E - 3 z _ {\theta} ^ {2}) \cdot , \mathrm {w i t h} D (L _ {\theta}) = X ,
$$

$L _ { \theta }$ is a self-adjoint operator on $Y$ . We verify that $L _ { \theta }$ is a Fredholm operator:

(1) Im (Lθ) is closed.   
(2) $\mathrm { K e r } \left( { L _ { \theta } } \right) = \mathrm { C o k e r } ( L _ { \theta } ) = T _ { z _ { \theta } } Z$

In fact, only (1) remains to be verified; it follows from:

Lemma 1.3.6 Assume $\operatorname { i n f } \left( V \right) ~ > ~ \lambda$ . Then $\exists \gamma > 0$ , such that $\Vert L _ { \theta } v \Vert _ { Y } ~ \geq$ $\gamma \| v \| _ { X } ~ \forall v \in ( T _ { z _ { \theta } } Z ) ^ { \perp }$ .

Proof. We prove it by contradiction. Suppose that $\exists \phi _ { n } \in ( T _ { z _ { \theta } } Z ) ^ { \perp }$ such that $\| \phi _ { n } \| _ { X } = 1$ , and $\| L _ { \theta } \phi _ { n } \| _ { Y } \to 0$ . Then $\phi _ { n }  \phi$ in $X$ . We claim that $\phi = 0$ . In fact,

(1) by weakly convergence, $\phi \in ( T _ { z _ { \theta } } Z ) ^ { \perp }$

and

(2) since $X$ is dense in $Y , \forall \psi \in X$

$$
\langle L _ {\theta} \phi , \psi \rangle = \langle \phi , L _ {\theta} \psi \rangle = \lim  \langle \phi_ {n}, L _ {\theta} \psi \rangle = \lim  \langle L _ {\theta} \phi_ {n}, \psi \rangle = 0.
$$

Therefore, $L _ { \theta } \phi = 0$ . Combining (1) and (2), $\phi = 0$ .

By the assumption $E > 0$ , let $\begin{array} { r } { B = - \frac { 1 } { 2 } \frac { d ^ { 2 } } { d t ^ { 2 } } + E } \end{array}$ − 1 d22 + E; we have a constant c > 0 $c > 0$ such that

$$
\| B u \| _ {Y} \geq 2 c \| u \| _ {X}.
$$

Noticing that $| z ( y ) | \sim \exp ( - \alpha | y | )$ as $| y | \to \infty$ , and that $\phi _ { n } \to 0$ in $X$ , in combination with the Sobolev embedding theorem, which implies that $\| \phi _ { n } \| _ { \infty }$ is bounded, and $\phi _ { n } \Rightarrow 0$ uniformly on any bounded interval, we obtain:

$$
\| L _ {\theta} \phi_ {n} \| _ {Y} = \| B \phi_ {n} - 3 z _ {\theta} ^ {2} \phi_ {n} \| _ {Y} \geq \| B \phi_ {n} \| _ {Y} - \| 3 z _ {\theta} ^ {2} \phi_ {n} \| _ {Y} \geq 2 c \| \phi_ {n} \| _ {X} - c = c,
$$

as $n$ is large. This is a contradiction.

We have the following Lyapunov–Schmidt reduction: $X = T _ { z _ { \theta } } Z \oplus X _ { 1 } , Y =$ $T _ { z _ { \theta } } Z \oplus Y _ { 1 }$ , and let $P : Y  Y _ { 1 }$ be the orthogonal projection. The equation $F ( h , u ) = 0$ is equivalent to the system:

$$
P F (h, z _ {\theta}, z _ {\theta} + \xi) = 0, \langle F (h, z _ {\theta} + \xi), e _ {\theta} \rangle = 0.
$$

From $F ( 0 , z _ { \theta } ) = 0$ and the IFT, for $h > 0$ small, $\Finv$ a unique $\xi = \xi ( h , \theta )$ solving the first equation, $P F ( h , z _ { \theta } + \xi ( h , \theta ) ) = 0$ , and satisfying $\| \xi ( h , \theta ) \| \leq$ $C \| F ( h , z _ { \theta } ) \|$ , for some constant $C > 0$ . It remains to solve:

$$
w _ {h} (\theta) := \langle F (h, z _ {\theta} + \xi (h, \theta)), e _ {\theta} \rangle = 0.
$$

Once $\theta = \theta ( h )$ is obtained, $u _ { h } = \xi ( h , \theta ( h ) ) + z _ { \theta ( h ) }$ solves $F ( h , u ) = 0$ .

We write

$$
F (h, u _ {h}) = \left(F (h, u _ {h}) - F (0, u _ {h})\right) + F (0, u _ {h}),
$$

and by the Taylor formula,

$$
F (0, u _ {h}) = F (0, z _ {\theta}) + L _ {\theta} \xi (h, \theta) - N (h, \theta),
$$

where $N ( h , \theta ) = 3 z _ { \theta } \xi ^ { 2 } ( h , \theta ) + \xi ^ { 3 } ( h , \theta )$ , and $\theta = \theta ( h )$ . Since $F ( 0 , z _ { \theta } ) = L _ { \theta } e _ { \theta } =$ $0$ , we have

$$
| \langle F (0, u _ {h}), e _ {\theta} \rangle | = | \langle N (h, \theta), e _ {\theta} \rangle | \leq C \| \xi (h, \theta (h)) \| _ {Y} ^ {2},
$$

and

$$
\begin{array}{l} \langle F (h, u _ {h}) - F (0, u _ {h}), e _ {\theta} \rangle | = \int (V (x _ {0} + h y) - V (x _ {0})) (z _ {\theta} (h) \\ + \xi (h, \theta (h)) e _ {\theta} d y = I _ {1} + I _ {2}. \\ \end{array}
$$

The following estimates hold:

$$
\begin{array}{l} I _ {1} = \left\langle \left(V _ {h} - V _ {0}\right) z _ {\theta}, e _ {\theta} \right\rangle \\ = - h \left\langle z _ {\theta}, V _ {h} ^ {\prime} z _ {\theta} \right\rangle - \left\langle z _ {\theta}, \left(V _ {h} - V _ {0}\right) e _ {\theta} \right\rangle \\ = - \frac {1}{2} h \langle z _ {\theta}, V _ {h} ^ {\prime} z _ {\theta} \rangle \\ = - \frac {1}{2} h \int V _ {h} ^ {\prime} (y) | z (y - \theta) | ^ {2} d y \\ = - \frac {1}{2} h \int V ^ {\prime} \left(x _ {0} + h (y + \theta)\right) | z (y) | ^ {2} d y, \\ \end{array}
$$

$$
\begin{array}{l} I _ {2} = | \langle (V _ {h} - V _ {0}) \xi (h, \theta), e _ {\theta} \rangle | \\ \leq \left\| \left(V _ {h} - V _ {0}\right) e _ {\theta} \right\| \left\| \xi (h, \theta) \right\|, \\ \end{array}
$$

and

$$
\begin{array}{l} \| F (h, z _ {\theta}) \| _ {Y} ^ {2} = \| F (h, z _ {\theta}) - F (0, z _ {\theta}) \| _ {Y} \\ = \int \left| V _ {h} (y + \theta) - V _ {0} \right| ^ {2} | z (y) | ^ {2} d y \\ \leq \int_ {B _ {\rho}} \left| V _ {h} (y + \theta) - V _ {0} \right| ^ {2} | z (y) | ^ {2} d y + 4 \operatorname {M a x} | V | ^ {2} \int_ {R ^ {1} \setminus B _ {\rho}} | z (y) | ^ {2} d y \\ \leq \operatorname {M a x} _ {| y | \leq \rho} | V (h (y + \theta)) - V _ {0} | ^ {2} + 4 \operatorname {M a x} | V | ^ {2} e ^ {- \mu \rho} \\ \leq C _ {1} \left(h (\rho + | \theta |)\right) ^ {4} + e ^ {- \mu \rho} \\ \end{array}
$$

for every $\rho > 0$ , and for some constant $C _ { 1 } > 0$ . Let us denote the right-hand side by Mh,θ,ρ. $M _ { h , \theta , \rho }$

Similarly, we have

$$
\left\| \left(V _ {h} - V _ {0}\right) e _ {\theta} \right\| ^ {2} \leq M _ {h, \theta , \rho}.
$$

Since $z ( y )$ is even,

$$
\begin{array}{l} \left| \frac {1}{h} I _ {1} + \frac {1}{2} V ^ {\prime \prime} (x _ {0}) \| z \| ^ {2} h \theta \right| = \frac {1}{2} \left| \int [ V ^ {\prime \prime} (x _ {0}) h \theta - V ^ {\prime} (x _ {0} + h (y + \theta)) ] | z (y) | ^ {2} d y \right| \\ = \frac {1}{2} \left| \int \left[ V ^ {\prime \prime} \left(x _ {0}\right) h (y + \theta) - V ^ {\prime} \left(x _ {0} + h (y + \theta)\right) \right| z (y) | ^ {2} d y \right| \\ \leq C _ {2} \int | h (y + \theta) | ^ {2} | z (y) | ^ {2} d y \\ \leq C _ {2} \left(h \left(\left| \theta \right| + \rho\right) \right| ^ {2} + e ^ {- \mu \rho}). \\ \end{array}
$$

By rescaling $\begin{array} { r } { \theta \ = \ \frac { s } { h } , v ( s ) \ = \ \frac { 1 } { h } w _ { h } ( \theta ) , v _ { 0 } ( s ) \ = \ \frac { 1 } { 2 } V ^ { \prime \prime } ( x _ { 0 } ) \| z \| ^ { 2 } s } \end{array}$ , and $v _ { h } ( s ) ~ =$ $h ^ { - \nu } v ( h ^ { \nu } s )$ , with $\nu \in ( 1 , 2 )$ , we obtain

$$
\left| v _ {h} (s) - v _ {0} (s) \right| \leq C _ {3} \left(h ^ {- \nu} \left(\left(h ^ {\nu} | s | + h \rho\right) ^ {2} + e ^ {- \mu \rho}\right) + h ^ {- 1 - \nu} \left(\left(h ^ {\nu} | s | + h \rho\right) ^ {4} + e ^ {- \mu \rho}\right)\right).
$$

Choosing $\rho = h ^ { - \tau }$ with $\tau > 0$ , for $| s | \leq 1$ , we obtain

$$
\begin{array}{l} \left| v _ {h} (s) - v _ {0} (s) \right| \leq C _ {3} \left(h ^ {- \nu} \left(\left(h ^ {\nu} + h ^ {1 - \tau}\right) ^ {2} + h ^ {- 1 - \nu} \left(\left(h ^ {\nu} + h ^ {1 - \tau}\right) ^ {4} \right. \right. \right. \\ + \left(h ^ {- \nu} + h ^ {- 1 - \nu}\right) e ^ {- \mu h ^ {- \tau}}) \rightarrow 0, \\ \end{array}
$$

uniformly as $h  0$

Since $v _ { 0 } ( s )$ changes sign at $s = 0$ , we have a zero of $v _ { h }$ in $[ - 1 , 1 ]$ as $h > 0$ small. This proves the existence of a zero $s _ { 0 } \in [ 0 , 1 ]$ of $v _ { h }$ and then of $w _ { h } ( \boldsymbol { \theta } )$ . Thus we have proved the following theorem:

Theorem 1.3.7 Assume that $V \in C ^ { 3 } ( R ^ { 1 } )$ is bounded and that $x _ { 0 }$ is a nondegenerate critical point of $V$ . If $\lambda < \operatorname { i n f } V$ , then $\exists h _ { 0 } > 0$ such that $\forall h \in ( 0 , h _ { 0 } )$ a nonzero solution (1.32), with $s _ { 0 } \in [ - h ^ { \nu } , h ^ { \nu } ] ~ \nu \in ( 1 , 2 )$ $\begin{array} { r } { \phi ( x ) \ = \ z ( \frac { x - x _ { 0 } - s _ { 0 } } { h } ) + \xi ( h , s _ { 0 } ( h ) ) ( \frac { x - x _ { 0 } } { h } ) } \end{array}$ , and $\| \xi ( h , s _ { 0 } ( h ) ) \| \to 0$ , as of the equation $h  0$ .

This solution $\phi$ , which is close to $\scriptstyle z \left( { \frac { x - x _ { 0 } } { h } } \right)$ , becomes more concentrated about $x _ { 0 }$ as $h  0$ . It is called a nonspreading wave packet with width $\alpha =$ $( 2 ( V ( x _ { 0 } ) - \lambda ) ) ^ { - 1 / 2 }$ .

# 1.3.4 Gluing

Gluing is an important technique in nonlinear analysis. It is a method of joining two heteroclinic trajectories to form a new trajectory such that the end point of the first is glued to the starting point of the second, and the new trajectory is closed to the union of the two. For instance, on a Riemannian manifold $M$ , let $f \in C ^ { 1 } ( M , \mathbf { R } ^ { 1 } )$ . Given $( x , y ) \in K \times K$ , where $K$ is the critical point set of $f$ , i.e., $K = \{ x \in M \vert f ^ { \prime } ( x ) = \theta \}$ , let $\operatorname { M } ( x , y )$ be the space of all trajectories $c \in H ^ { 1 } ( \mathbf { R } ^ { 1 } , M )$ satisfying

$$
\dot {c} = - \nabla f \circ c, \quad c (- \infty) = x, c (+ \infty) = y.
$$

The importance of the gluing technique is in the study of the compactness of the totality of the trajectory spaces: $\{ \mathrm { M } ( x , y ) | ( x , y ) \in K \times K \}$ .

It is proved that the trajectory manifolds $\operatorname { M } ( x , y )$ are compact up to the existence of sequences which converge to broken trajectories in the $C _ { l o c } ^ { \infty }$ topology. And the “gluing” means to map such broken trajectories equipped with an additional suitable parametrization into the appropriate trajectory space:

$$
\mathrm {M} (x, z) \times \mathrm {M} (z, y) \rightarrow \mathrm {M} (x, y).
$$

This technique is crucial in Floer homology theory (see Hofer and Zehnder [HZ 2], Floer [Fl 2,3], Taubes [Tau]). In order not to involve over specialized knowledge in that theory, it will suffice to introduce the technique by an example of a nonspreading wave packet, which we met in the previous subsection.

We have known the existence of a nonspreading wave packet, if $V$ has a single nondegenerate critical point. However, if $V$ has several nondegenerate critical points $x _ { 1 } , x _ { 2 } , \ldots , x _ { n }$ , there are at least n nonspreading wave packets with widthes $\alpha _ { j } = ( 2 ( V ( x _ { j } ) - \lambda ) ) ^ { - 1 / 2 } , j = 1 , 2 , \ldots , n$ . As $h$ becomes small, these wave packets are separated. Are there multi-peak wave packets? More precisely, are there new solutions of equation (1.32), which are closed to some of these wave packets on their peak intervals simultaneously? For simplicity, we only consider the case $n = 2$ ; the result can be extended to any $n$ . Assume that $V$ has only two nondegenerate critical points $x _ { \pm } = \pm R$ . Let $\lambda < \operatorname { i n f } ( V )$ . Define $\alpha _ { \pm } = \sqrt { 2 ( V ( \pm R ) - \lambda ) }$ , $z _ { \pm } ( y ) = \alpha _ { \pm } \mathrm { s e c h } \left( \alpha _ { \pm } y \right)$ , and $\forall ( \theta _ { + } , \theta _ { - } ) \in R ^ { 2 } , z ( \theta _ { + } , \theta _ { - } ) = z _ { + } ( \theta _ { + } ) + z _ { - } ( \theta _ { - } )$ , where $z _ { \pm } ( \theta _ { \pm } ) = \tau _ { \pm ( R + \theta _ { \pm } ) / h } z _ { \pm }$ . Again let

$$
F (h, u) = - \frac {1}{2} \frac {d ^ {2}}{d y ^ {2}} u + \left(V _ {h} - \lambda\right) u - u ^ {3}.
$$

We are looking for a solution of the form $u = z ( \theta _ { + } , \theta _ { - } ) + \phi \in X$ for the equation $F ( h , \cdot ) = 0$ , in which $\phi$ is small as $h > 0$ is.

Define a two-dimensional manifold: $Z _ { h } = \{ ( z _ { + } ( \theta _ { + } ) , z _ { - } ( \theta _ { - } ) ) ~ | ( \theta _ { + } , \theta _ { - } ) ~ \in$ $| ( \theta _ { + } , \theta _ { - } ) \in$ $R ^ { 2 } \}$ , then $T _ { z ( \theta _ { + } , \theta _ { - } ) } Z _ { h } = \mathrm { s p a n } \{ \tau _ { ( R + \theta _ { + } ) / h } e _ { + } , \tau _ { - ( R + \theta _ { - } ) / h } e _ { - } \}$ , where $e \pm = z _ { \pm } ^ { \prime }$ .

In contrast with the one-peak solution, now $z ( \theta _ { + } , \theta _ { - } )$ are not zeroes of $F ( 0 , u )$ , therefore the zeroes of $F ( h , u )$ cannot be obtained directly by the IFT.

Instead, we appeal to the contraction mapping theorem. However, Lyapunov– Schmidt reduction is again useful. Fixing $( \theta _ { + } . \theta _ { - } ) \in R ^ { 2 }$ , let $u _ { 0 } = z ( \theta _ { + } , \theta _ { - } )$ , we write down the orthogonal decomposition:

$$
X = T _ {u _ {0}} Z _ {h} \bigoplus X _ {1}, Y = T _ {u _ {0}} Z _ {h} \bigoplus Y _ {1},
$$

where the spaces $X$ and $Y$ were introduced in the previous subsection, and let $P : Y  Y _ { 1 }$ be the orthogonal projection.

$$
F (h, u) = 0 \Leftrightarrow \left\{ \begin{array}{l} P F (h, u _ {0} + \xi) = 0 \\ (I - P) F (h, u _ {0} + \xi) = 0  . \end{array} \right.
$$

In fact, by Taylor’s formula,

$$
F (h, u _ {0} + \xi) = F (h, u _ {0}) + F _ {u} (h, u _ {0}) \xi + N (h, u _ {0}, \xi),
$$

where $N ( h , u _ { 0 } , \xi ) = 3 u _ { 0 } \xi ^ { 2 } + \xi ^ { 3 }$ . First, fixing $( \theta _ { + } , \theta _ { - } ) \in R ^ { 2 }$ , we solve the first equation in $X _ { 1 }$ for $h > 0$ small. Let $L ( \theta _ { + } , \theta _ { - } , h ) = P F _ { u } ( h , u _ { 0 } )$ . We have the following lemmas:

Lemma 1.3.8 $\exists \gamma > 0 , \exists h _ { 0 } > 0 , \exists \alpha _ { 0 } \in ( 0 , 1 / 2 )$ such that

$$
\| L (\theta_ {+}, \theta_ {-}, h) \| _ {Y} \geq \gamma \| u \| _ {X} \forall u \in X _ {1}.
$$

as $h \in ( 0 , h _ { 0 } )$ , and $| \alpha _ { \pm } | < \alpha _ { 0 }$ .

Lemma ${ \bf 1 . 3 . 9 } \ \forall \rho > 0 , \| F ( h , u _ { 0 } ) \| _ { Y } \leq C _ { 1 } ( \Sigma _ { \pm } \mathrm { M a x } _ { | y | \leq \rho h } ( V ( y ) - V ( \pm R ) ) ^ { 2 } | \pm$ $R \pm \theta _ { \pm } | + e ^ { - 2 \mu \rho } + e ^ { - \mu R / h } )$ , where $\mu = \mathrm { M i n } \{ \alpha _ { + } , \alpha _ { - } \}$ .

# Lemma 1.3.10

$$
\| N (h, u _ {0}, \xi) \| _ {Y} \leq C _ {2} \| \xi \| _ {X} ^ {2},
$$

$$
\left\| N \left(h, u _ {0}, \xi_ {1}\right) - N \left(h, u _ {0}, \xi_ {2}\right) \right\| _ {Y} \leq C _ {2} \operatorname {M a x} \left(\left\| \xi_ {1} \right\| _ {X}, \left\| \xi_ {2} \right\| _ {X}\right) \left\| \xi_ {1} - \xi_ {2} \right\| _ {X}.
$$

Once these lemmas have been proved, according to the contraction mapping theorem, one finds a fixed point $\xi$ of the operator $- L ^ { - 1 } ( \theta _ { + } , \theta _ { - } , h ) N ( h ,$ $u _ { 0 } , \xi ) - F ( h , u _ { 0 } )$ in a neighborhood of $u _ { 0 }$ on $X _ { 1 }$ . Namely, $\exists h _ { 0 } > 0 , \exists \alpha _ { 0 } >$ $0 , \exists C _ { 1 } > 0$ such that $\forall h \in ( 0 , h _ { 0 } ) , \forall | \theta _ { \pm } | < \alpha _ { 0 } , \exists$ a unique $\xi = \xi ( \theta _ { + } , \theta _ { - } , h ) \in X _ { 1 }$ such that

$$
P F (h, u _ {0} + \xi) = 0, \text {a n d} \| \xi \| \leq C _ {1} \| F (h, u _ {0}) \| _ {Y}.
$$

Now the second equation is a two system in two variables $( \theta _ { + } , \theta _ { - } ) \in$ $[ - \alpha _ { 0 } , \alpha _ { 0 } ] ^ { 2 }$ :

$$
v _ {h} \left(\theta_ {+}, \theta_ {-}\right) := h ^ {- 1} \left(\left\langle e _ {+}, F (h, z \left(\theta_ {+}, \theta_ {-}\right) + \xi \left(\theta_ {+}, \theta_ {-}, h\right) \right\rangle , \right.
$$

$$
\langle e _ {-}, F (h, z (\theta_ {+}, \theta_ {-}) + \xi (\theta_ {+}, \theta_ {-}, h) \rangle) = (0, 0).
$$

Let us define a two-vector function:

$$
v _ {0} (\theta_ {+}, \theta_ {-}) = \frac {1}{2} (| z _ {+} | ^ {2} V ^ {\prime \prime} (R) \theta_ {+}, | z _ {-} | ^ {2} V ^ {\prime \prime} (- R) \theta_ {-}).
$$

Again we need a Lemma:

Lemma 1.3.11 For $\nu \in ( 1 , 2 )$ and $0 < h < \mathrm { M i n } ( h _ { 0 } , \alpha _ { 0 } )$ , we have

$$
h ^ {- \nu} v _ {h} \left(h ^ {\nu} \theta_ {+}, h ^ {\nu} \theta_ {-}\right)\rightarrow v _ {0} \left(\theta_ {+}, \theta_ {-}\right),
$$

uniformly on $[ - 1 , 1 ] ^ { 2 }$ as $h  0$ .

The difference between these proofs of the Lemmas 1.3.8 to 1.3.11 and those in the previous subsection lie in the interaction terms (for details refer to [Oh]).

From the Brouwer degree theory (cf. Sect. 3.1),

$$
\deg (h ^ {- \nu} v _ {h} (h ^ {\nu} \theta_ {+}, h ^ {\nu} \theta_ {-}), [ - 1, 1 ] ^ {2}, (0, 0)) = \deg (v _ {0} (\theta_ {+}, \theta_ {-}), [ - 1, 1 ] ^ {2}, (0, 0)) \neq 0.
$$

This proves the existence of a zero of the above system. Finally we arrive at:

Theorem 1.3.12 Suppose that $V \in C ^ { 3 }$ is bounded, and $\lambda < \operatorname { i n f } \left( V \right)$ . Then for each pair $( x _ { 1 } , x _ { 2 } )$ of nondegenerate critical points of $V , \exists h _ { 0 } ~ > ~ 0$ , such that for $h \in ( 0 , h _ { 0 } )$ equation (1.32) has a nonzero solution $u _ { h }$ of the form $\tau _ { ( x _ { 1 } + s _ { + } ) / h } z _ { + } + \tau _ { ( x _ { 2 } + s _ { - } ) / h } z _ { - } + \xi ( h )$ , where $| s _ { \pm } | \le h ^ { - \nu }$ with $\nu \in ( 1 , 2 )$ , and $\| \xi \|  0$ as $h \to 0$ .

# 1.3.5 Transversality

Transversality is an important notion in differential geometry. It is a condition, induced by the IFT, on a map between two manifolds, under which the preimage of a submanifold is again a submanifold. $f : X \to Y$ is said to be transversal to the submanifold $W \subset Y$ if at every point $x \in f ^ { - 1 } ( W )$ ,

$$
\operatorname {I m} f ^ {\prime} (x) + T _ {f (x)} (W) = T _ {f (x)} (Y),
$$

and is denoted by $f \pitchfork W$ .

Related notions are regular points and regular values defined as follows.

Definition 1.3.13 Suppose that $X$ and $Y$ are $C ^ { 1 }$ Banach manifolds and $f \in$ $C ^ { 1 } ( X , Y )$ . A point $x \in X$ is called a regular point of $f$ , if $f ^ { \prime } ( x ) : T _ { x } ( X ) \to$ $T _ { f ( x ) } ( Y )$ is surjective, and is singular (or critical) if it is not regular. The images of the singular points under $f$ are called singular values (or critical values), and their complement, regular values. If $y \in Y$ is not in the image of $f$ , i.e., $f ^ { - 1 } ( y ) = \varnothing$ , then y is a regular value.

The following Sard theorem reveals the smallness of the set of critical values:

![](images/f889d0ab1d259726eec0ca9d8e276d7712d1217d2820877131c03272510e6bc6.jpg)

![](images/14dd4437cb769b395070a7af06f9c2f5dccfe66ef3873baaaecc56de33d262f6.jpg)  
Fig. 1.8.

Theorem 1.3.14 (Sard) Suppose that $X$ and $Y$ are differential manifolds with dimensions $n$ and m respectively, and that $U ~ \subset ~ X$ is open. If $f \in$ $C ^ { r } ( X , Y )$ , where $r \geq 1$ and $r > \operatorname* { m a x } \{ 0 , \dim X - \dim Y \}$ , then the set of critical values of $f$ has measure zero in $Y$ .

The proof is purely measure theoretic, we shall not give it here. The special case $\mathrm { d i m } X = \mathrm { d i m } Y < \infty$ will be given in Sect. 3.1, but for the general case refer to Milnor [Mi 2].

We shall extend the study to maps between infinite-dimensional manifolds. Recall a map $f$ between two Banach spaces $X$ and $Y$ is called Fredholm in $U \subset X$ , if $f \in C ^ { 1 } ( U , Y )$ and $f ^ { \prime } ( x )$ is a Fredholm operator $\forall x \in U$ . The index of $f$ is defined to be

$$
\operatorname {i n d} f ^ {\prime} (x) = \dim \ker f ^ {\prime} (x) - \dim \operatorname {c o k e r} f ^ {\prime} (x).
$$

It is known that if $U$ is connected, then $\operatorname { i n d } f ^ { \prime } ( x )$ is a constant integer, and then is denoted by $\operatorname { i n d } ( f )$ .

Lemma 1.3.15 If $f \in C ^ { 1 } ( U , Y )$ is a Fredholm map, then the set of critical points is closed.

Proof. Since $x \mapsto \dim \ker f ^ { \prime } ( x )$ is u.s.c., and $\operatorname { i n d } ( f )$ is locally a constant, dim cokerf -(x) is also u.s.c. Let $S$ be the critical point set of $f$ , then the set

$$
S = \{x \in U | f ^ {\prime} (x) \text {i s n o t s u r j e c t i v e} \} = \{x \in U | \dim \operatorname {c o k e r} f ^ {\prime} (x) \geq 1 \}
$$

is closed.

Theorem 1.3.16 (Sard–Smale) Suppose that $X$ is a separable Banach space and $Y$ is a Banach space. Let $f \in C ^ { r } ( U , Y )$ be a Fredholm map, where $U \subset$ $X$ is open. If $r > \operatorname* { m a x } { ( 0 , \operatorname { i n d } ( f ) ) }$ , then the set of critical values is of first category.

Proof. Since the first category set is a countable union of closed nowhere dense sets, and $U$ is separable, it is sufficient to prove that $\forall x \in U$ , there is a neighborhood of $x , V \subset U$ such that the set of critical values of $f | _ { V } , S ( f , V )$ is closed and nowhere dense.

By the definition of Fredholm operators, one has the decomposition $X =$ $\ker f ^ { \prime } ( x ) \oplus X _ { 1 }$ . Let $Q$ be the projection of $Y$ onto $\operatorname { I m } f ^ { \prime } ( x )$ ; from the IFT, there is a neighborhood $U _ { 0 } \times V _ { 0 } \subset \ker f ^ { \prime } ( x ) \times X _ { 1 }$ , a neighborhood $W \subset \mathrm { I m } f ^ { \prime } ( x )$ of $f ( x )$ , and $h \in C ^ { \mathrm { 1 } } ( U _ { 0 } \times W , V _ { 0 } )$ such that

$$
Q f (u + h (u, w)) = w, \quad \forall (u, w) \in U _ {0} \times W,
$$

and $\forall u \in U _ { 0 } , h ( u , \cdot ) : W \to V _ { 0 }$ is a diffeomorphism. Since $U _ { 0 }$ is finite dimensional, it can be chosen compact. We set $V = U _ { 0 } \times V _ { 0 }$ ,

1. First we show that $S ( f , V )$ is closed. Due to Lemma 1.3.15 it is sufficient to show that the map $f | _ { V }$ is closed. In fact, let $x _ { n } = u _ { n } + v _ { n } \in U _ { 0 } \times V _ { 0 }$ be such that $y _ { n } = f ( x _ { n } ) \to y$ . From the compactness of $U _ { 0 }$ , $u _ { n }$ is subconvergent, and $v _ { n } = h ( u _ { n } , Q y _ { n } )$ is convergent, therefore $f | _ { V }$ is closed.

2. Next we show that $S ( f , V )$ is a nowhere dense set. Define $H : U _ { 0 } \times$ $W  V$ by $H ( u , v ) = ( u , h ( \underline { { u } } , w ) )$ . Then $H$ is a diffeomorphism satisfying $Q f \circ H ( u , w ) = w$ . Setting $f = f \circ H$ , we have $S ( f , V ) = S ( \vec { f } , U _ { 0 } \times W )$ , and $Q \bar { f } ^ { \prime } | _ { W _ { - } } = i d _ { W }$ , therefore $\forall w \ \in \ W$ , $( x , w ) \in S ( \bar { f } , U _ { 0 } \times W ) \Longleftrightarrow x \in$ $S ( ( I - Q ) \bar { f } ( \cdot , w ) , U _ { 0 } )$ . From $r \geq \operatorname { i n d } f ^ { \prime } ( x ) = \dim \ker f ^ { \prime } ( x ) - \dim \operatorname { c o k e r } f ^ { \prime } ( x )$ $r \mathrm { ~ > ~ }$ and Sard theorem, $S ( ( I - Q ) \bar { f } ( \cdot , w ) , U _ { 0 } )$ is a null set. Since $S ( f , V )$ is also closed, it is a nowhere dense set.

We conclude that the critical set of $f$ is of the first category.

Corollary 1.3.17 Let $X$ and $Y$ be Banach spaces, and $X$ be separable. If $f \in C ^ { 1 } ( X , Y )$ is Fredholm with negative index, then $f ( X )$ does not contain interior points.

Proof. If not, say $y _ { 0 } \in f ( X )$ is an interior point, i.e., $\Finv$ a neighborhood $V$ of $y _ { 0 }$ such that $V \subset f ( X )$ . According to the Sard–Smale lemma, $\exists y \in V$ such that $f \pitchfork \{ y \}$ , i.e., $\forall x \in f ^ { - 1 } ( y ) , f ^ { \prime } ( x ) : X \to Y$ is surjective, or codim I $\operatorname { m } ( f ^ { \prime } ( x ) ) =$ $0$ , thus $\operatorname { i n d } ( f ) ( x ) = \dim \ker f ^ { \prime } ( x ) \geq 0$ . A contradiction. □

The following transversality theorem will be often used in the sequel.

Theorem 1.3.18 Suppose that $X , Z$ are Banach spaces, where $X$ is separable, and that $S$ is a ${ \cal C } ^ { r }$ Banach manifold. Assume that ${ \cal F } \in C ^ { r } ( { \cal X } \times { \cal S } , { \cal Z } )$ satisfies (1) $F \pitchfork \{ \theta \}$ , and (2 $\rangle \forall s \in S , f _ { s } ( u ) = F ( u , s )$ is a Fredholm map with index satisfying $\operatorname* { m a x } \{ 0 , \operatorname { i n d } ( f _ { s } ) \} < r$ .

Then a residual set $\Sigma \subset S$ (i.e., the countable intersection of open dense sets) such that $\forall s \in \Sigma , f _ { s } \cap \{ \theta \}$ .

Proof. Define the injection: $i : V \longrightarrow X \times S$ and the projections $\pi : X \times S \longrightarrow$ $S , p : X \times S \to X$ . Let $V = F ^ { - 1 } ( \theta )$ . In the sequel, both of them are restricted on $V$ . We claim that $\pi \circ i$ is a Fredholm map with $\operatorname { i n d } \left( \pi \circ i \right) = \operatorname { i n d } \left( f _ { s } \right)$ .

In fact, $\begin{array} { r } { \ker V ^ { \prime } ( v ) = T _ { v } ( V ) = \operatorname { I m } \big ( ( p \circ i ) ^ { \prime } ( v ) \big ) \oplus \operatorname { I m } \big ( ( \pi \circ i ) ^ { \prime } ( v ) \big ) \ \forall v = ( x , s ) \in \mathcal { } } \end{array}$ $T _ { v } ( V )$ . Since $F \pitchfork \{ \theta \}$ , we have a direct sum decomposition: $X \times T _ { s } ( S ) =$ $Y \oplus T _ { v } ( V )$ such that $F ^ { \prime } ( v ) : Y \to Z$ is an isomorphism. By the assumption that $f _ { s } ^ { \prime } ( x )$ is Fredholm, we have direct sum decompositions: $X = { \mathrm { I m } } ( ( p \circ$ $i ) ^ { \prime } ( v ) ) \oplus Y _ { 1 } , Z = Z _ { 1 } \oplus Z _ { 2 }$ , such that $f _ { s } ^ { \prime } ( x ) : Y _ { 1 }  Z _ { 1 }$ . Thus, $Y = Y _ { 1 } \oplus Y _ { 2 }$ , where $T _ { s } ( S ) = \mathrm { I m } ( \pi \circ i ) ^ { \prime } ( v ) \oplus Y _ { 2 }$ . From $F ^ { \prime } ( v ) = f _ { s } ^ { \prime } ( x ) \oplus \partial _ { s } F ( v )$ , we have $\partial _ { s } F ( v ) : Y _ { 2 } \to Z _ { 2 }$ is an isomorphism.

Thus, ker f -(x) = Im(p  i)-(v) = ker (π  i)-(v), coke $\cdot f _ { s } ^ { \prime } ( x ) = Z _ { 2 } \simeq Y _ { 2 } \simeq$ $\operatorname { c o k e r } ( \pi \circ i ) ^ { \prime } ( v )$ . Therefore $\operatorname { i n d } \left( f _ { s } \right) = \operatorname { i n d } \left( \pi \circ i \right)$ .

According to the Sard–Smale Theorem, the set of regular values of π i $\Sigma$ is a residual set. We are going to prove that for a regular value $s , f _ { s } \pitchfork \{ \theta \}$ . From $F ( x , s ) = \theta$ , and $F \pitchfork \{ \theta \}$ , we have $\quad \operatorname { I m } F ^ { \prime } ( x , s ) = Z$ , i.e., $\forall a \in Z , \exists ( \alpha , \beta ) \in$ $X \times T _ { s } ( S )$ such that $\begin{array} { r } { a = ( \frac { \partial F } { \partial x } \alpha + \frac { \partial F } { \partial s } \beta ) } \end{array}$ .

We are going to prove $\operatorname { I m } f _ { s } ^ { \prime } ( x ) = Z$ , i.e., $\forall \mathrm { ~ a ~ } \in Z , \exists y \in X$ such that $a = f _ { s } ^ { \prime } ( x ) y$ .

Thus, if $\beta = 0$ , we take $y = \alpha$ . Otherwise, since $\pi$ is a projection, (π $i ) ^ { \prime } ( x , s ) : X \times T _ { s } ( S ) \longrightarrow T _ { s } ( S )$ is a projection. That $s$ is a regular value means that $( \pi \circ i ) ^ { \prime } ( x , s ) : T _ { ( x , s ) } ( V ) \longrightarrow T _ { s } ( S )$ is surjective, i.e., to the given $\beta \in T _ { s } ( S ) , \exists w \in X$ such that $( w , \beta ) \in T _ { ( x , s ) } ( V )$ . Since $F ^ { - 1 } ( \theta ) = V$ , we have $F ^ { \prime } ( x , s ) ( w , \beta ) = \theta$ .

Setting $y = \alpha - w$ , we obtain

$$
\begin{array}{l} f _ {s} ^ {\prime} (x) y - a = F ^ {\prime} (x, s) [ (\alpha , \beta) - (w, \beta) ] - a = \left[ \frac {\partial F}{\partial x} \alpha + \frac {\partial F}{\partial s} \beta \right] \\ - a - F ^ {\prime} (x, s) (w, \beta) = \theta . \\ \end{array}
$$

![](images/60fbafc9c4cf9c37913620912f32eb8b68c19a1553bbfa8d5cecec8f827d62cf.jpg)

The same conclusion holds if $X$ and $Z$ are Banach manifolds.

As an application we study the simplicity of eigenvalues of the Laplacian on bounded domains with Dirichlet data. It is well known that all eigenvalues of second-order ODEs on bounded intervals with Dirichlet data are simple, but it is not true for PDE, e.g., the Laplacian on a ball. However, we shall show that for most domains, this is true. What is the meaning of most domains?

Given a bounded open domain $\Omega \subset R ^ { n }$ with smooth boundary, we consider the manifold $S \ = \ \mathrm { D i f f } ^ { 3 } ( \Omega _ { 0 } ) \ : = \ \{ g \ \in \ C ^ { 3 } ( \Omega _ { 0 } , R ^ { n } ) \vert \operatorname * { d e t } \left( g ^ { \prime } ( x ) \right) \ \neq$

$0 \forall x \in \overline { { \Omega } } _ { 0 } \}$ . Thus as $g \in S$ , the domain $\Omega = g ( \Omega _ { 0 } )$ is $C ^ { 3 } . \forall \Omega$ , let $X ( \Omega ) =$ $H ^ { 2 } ( \Omega ) \cap H _ { 0 } ^ { 1 } ( \Omega )$ and $Y ( \Omega ) = L ^ { 2 } ( \Omega )$ . $\forall g \in S$ , define $g ^ { * } : X ( \Omega _ { 0 } ) \to X ( \Omega )$ by

$$
(g ^ {*} u) (x) = u \left(g ^ {- 1} (x)\right) \forall x \in \Omega = g \left(\Omega_ {0}\right).
$$

One defines a map $F \in C ^ { 1 } ( ( X ( \Omega _ { 0 } ) \backslash \{ \theta \} ) \times S , Y ( \Omega _ { 0 } ) )$ ) by

$$
F (u, \lambda , g) = g ^ {* - 1} (\Delta + \lambda I) g ^ {*} u.
$$

We want to show that there exists a residual set $\Sigma$ of $S$ such that $\forall g \in \Sigma$ , all eigenvalues of the problem:

$$
(\Delta + \lambda) u = 0 \text {o n} H _ {0} ^ {1} (\Omega),
$$

are simple, where $\Omega = g ( \Omega _ { 0 } )$ . Since there are, at most, countable eigenvalues, it is sufficient to show that for each eigenvalue $\lambda$ after suitable perturbation of the domain, i.e., for after a suitable $g \in S$ , it is simple. To this end, let us define $f _ { g } ( u , \lambda ) = F ( u , \lambda , g )$ . We claim that it remains to show $f _ { g } \pitchfork \{ \theta \}$ , or equivalently, $f _ { q } ^ { \prime } ( u , \lambda ) : X ( \Omega ) \times R ^ { 1 } \to Y ( \Omega )$ is surjective, i.e., $\{ ( \varDelta + \lambda ) w + $ $\mu u | ( w , \mu ) \in X ( \Omega ) \times R ^ { 1 } \} = Y ( \Omega ) \}$ , or equivalently, codim $\mathrm { I m } ( \varDelta + \lambda I ) \leq 1$ . Since we have assumed that $\lambda$ is an eigenvalue, i.e., codim $\mathrm { I m } ( \varDelta + \lambda I ) \geq 1$ . Therefore $\lambda$ is simple.

We know that $f _ { g } ^ { \prime } ( u , \lambda )$ is a Fredholm map, from Theorem 1.3.18, it is sufficient to verify that $F \pitchfork \ \{ \theta \}$ . To this end, $\forall ( u _ { 0 } , \lambda _ { 0 } , g _ { 0 } ) \ \in \ F ^ { - 1 } ( \theta )$ , let $L = F ^ { \prime } ( u _ { 0 } , \lambda _ { 0 } , g _ { 0 } )$ , and $\Omega = g _ { 0 } ( \Omega _ { 0 } )$ . With no loss of generality we may assume $g _ { 0 } = I d$ . It remains to verify that $L$ is surjective. Since

$$
L (w, \mu , h) = (\Delta + \lambda_ {0}) w + \mu u _ {0} + [ h \cdot \nabla , \Delta + \lambda_ {0} ] u _ {0} = (\Delta + \lambda_ {0}) (w - h \cdot \nabla u _ {0}) + \mu u _ {0},
$$

$L$ is Fredholm, where $[ A , B ] = A B - B A$ . Suppose it is not surjective, then $\mathrm { c o d i m } \operatorname { I m } ( L ) > 0$ , there exists a nontrivial element $\phi \bot \mathrm { I m } ( L )$ . First by taking $w = h = \theta$ , we have

$$
\int_ {\Omega} \phi u _ {0} = 0.
$$

Second, we take $h = \theta$ ; it follows that

$$
\int_ {\Omega} \phi (\Delta + \lambda_ {0} I) w = 0 \forall w \in X (\Omega).
$$

Then $\phi \in C ^ { 2 , \alpha } ( \Omega )$ , $\alpha \in ( 0 , 1 )$ is a solution of the equation:

$$
(\varDelta + \lambda_ {0}) \phi = 0,
$$

with boundary value zero. Then we have

$$
\begin{array}{l} 0 = \int_ {\Omega} \phi (\Delta + \lambda_ {0}) (h \cdot \nabla u _ {0}) \\ = \int_ {\Omega} \phi (\Delta + \lambda_ {0}) (h \cdot \nabla u _ {0}) - (h \cdot \nabla u _ {0}) (\Delta + \lambda_ {0} I) \phi \\ = \int_ {\partial \Omega} \phi \partial_ {n} (h \cdot \nabla u _ {0}) - \partial_ {n} \phi (h \cdot \nabla u _ {0}) \\ = - \int_ {\partial \Omega} \partial_ {n} \phi (h \cdot \nabla u _ {0}) \\ \end{array}
$$

for all $h \in C ^ { 3 } ( \Omega , R ^ { n } )$ . Thus,

$$
\partial_ {n} \phi \nabla u _ {0} = 0 \mathrm {o n} \partial \Omega .
$$

According to the uniqueness of the Cauchy problem (see [Hor 1]) for the equations $( \varDelta + \lambda _ { 0 } ) \phi = 0$ , and $( \varDelta + \lambda _ { 0 } ) u _ { 0 } = 0$ for $\phi , u _ { 0 } \in C ^ { 2 , \alpha } \cap C _ { 0 } ( \Omega )$ , this contradicts with $u _ { 0 } \neq \theta$ and $\phi \neq \theta$ . We have proved a result due to K. Uhlenbeck [Uh]:

Theorem 1.3.19 There exists a residual set $\Sigma \subset \mathrm { D i f f } ^ { 3 } ( \Omega _ { 0 } )$ such that $\forall g \in \Sigma$ , all eigenvalues of the equation

$$
(\Delta + \lambda I) u = 0 o n H _ {0} ^ {1} (g (\Omega_ {0}))
$$

are simple.

# 1.4 Hard Implicit Function Theorem

The inverse function theorem is established for $C ^ { 1 }$ mappings between Banach spaces, $f : X \to Y$ , where $X$ and $Y$ are Banach spaces, under the assumption $f ^ { \prime } ( x _ { 0 } ) ^ { - 1 } \in L ( Y , X )$ . However, if $f ^ { \prime } ( x _ { 0 } ) ^ { - 1 }$ is not bounded, then the problem will be very complicated. The small divisor problem, arising in the study of the long-time behavior of oscillatory motions in the solar system, is a typical example. The celebrated KAM theory, which states that most (in the sense of measure) quasi-periodic solutions of an integrable smooth Hamiltonian system persist under small perturbations of that Hamiltonian, provided that the Hessian is nondegenerate, is a breakthrough in the progress in this direction. Not only is the result an important conclusion in celestial mechanics, but also the methods developed in this theory have great impact in other nonlinear problems. In fact, Nash–Moser iteration, which was introduced in the study of the isometric embedding problem for Riemann manifolds, see [Na 1,2], and [Mos 1,2], is extensively applied in problems arising in geometry (see for instance Hamilton [Ham]) and in physics (see for instance Hormander [Hor 2]). In recent years, KAM theory has been extended to Hamiltonian systems with infinitely many freedoms (see Poschel [Po], Wayne [Way], Kuksin [Kuk], Bourgain [Bou]etc.)

In avoiding too many complicated computations, we take an approach due to Hormander by appealing to the Leray–Schauder degree theory in the proof of the existence of a solution. Readers may read this subsection after Chap. 3. In fact there are other approaches without using topological arguments (see for instance J. Moser [Mos 1,2,3], Hormander [Hor 2] or J. T. Schwartz [Scw]).

Actually, the boundedness assumption on $f ^ { \prime } ( x _ { 0 } ) ^ { - 1 }$ guarantees the convergence of the simple iteration method in solving the nonlinear equation:

$$
y = f (x) \tag {1.34}
$$

$$
\Leftrightarrow \quad x = x - f ^ {\prime} (x _ {0}) ^ {- 1} (y - f (x)).
$$

One produces the iteration process as follows:

$$
x _ {n + 1} = x _ {n} - f ^ {\prime} \left(x _ {0}\right) ^ {- 1} \left(y - f \left(x _ {n}\right)\right), \quad n = 1, 2, \dots . \tag {1.35}
$$

However, if we consider the nonlinear wave equation:

$$
\left\{ \begin{array}{l} u _ {t t} - F (u _ {x x}) = f \\ u | _ {t = 0} = u _ {t} | _ {t = 0} = 0  , \end{array} \right.
$$

where $F$ is a function with $F ^ { \prime } ( 0 ) = 1$ and $F ( 0 ) = 0$ , then the above simple iteration process fails, because the linearized equation reads as

$$
\left\{ \begin{array}{l} u _ {t t} - u _ {x x} = g \\ u | _ {t = 0} = u _ {t} | _ {t = 0} = 0  . \end{array} \right.
$$

when $g \in H ^ { s }$ , with compact support for some $s > 0$ . We can at most estimate the boundedness of $H ^ { s + 1 }$ norm of $u$ , but not $H ^ { s + 2 }$ , as that in the elliptic case. If we use the simple iteration, then we lose derivatives in each step; after a finite number of steps, the iteration cannot go on. This phenomenon is called “loss of derivatives”. It occurs in nonelliptic problems.

In the analytic category, occurs phenomenon whereby the inverse maps reduce the convergence radii (e.g. the small divisor problem, see below); this is called “loss of convergence radii”. As before, this is also the case that the linearized operator has no bounded inverse.

# 1.4.1 The Small Divisor Problem

For a given $f$ , analytic in a neighborhood of $\theta$ , with $f ( 0 ) = 0$ , and $f ^ { \prime } ( 0 ) = \sigma$ , find $u$ , analytic in a neighborhood of $\theta$ with $u ( 0 ) = 0$ , and $u ^ { \prime } ( 0 ) = 1$ , satisfying

$$
f (u (z)) = u (\sigma z). \tag {1.36}
$$

We write it in the IFT form:

Let

$$
f (z) = \sigma z + \hat {f} (z),
$$

$$
u (z) = z + \hat {u} (z).
$$

Equation (1.36) is equivalent to

$$
\Phi (\hat {f}, \hat {u}) = \hat {f} (z + \hat {u} (z)) + \sigma \hat {u} (z) - \hat {u} (\sigma z) = \theta . \tag {1.37}
$$

Obviously, $\Phi ( \theta , \theta ) = \theta$ , and

$$
\Phi_ {\hat {u}} (\hat {f}, \hat {u}) \hat {w} = \hat {f} ^ {\prime} (z + \hat {u} (z)) \hat {w} (z) + \sigma \hat {w} (z) - \hat {w} (\sigma z).
$$

$\Phi _ { \hat { u } } ( \theta , \theta )$ is injective, if $\sigma = e ^ { 2 \pi i \tau }$ , where $\tau$ is irrational. In fact, we write

$$
\hat {v} = \sum_ {j = 2} ^ {\infty} v _ {j} z ^ {j}, \hat {w} = \sum_ {j = 2} ^ {\infty} w _ {j} z ^ {j}.
$$

The equation

$$
\Phi_ {\hat {u}} (\theta , \theta) \hat {w} = \hat {v}
$$

has a unique solution $\hat { w }$ , in which

$$
w _ {j} = \frac {v _ {j}}{\sigma - \sigma^ {j}} j = 2, 3, \dots .
$$

Since the denominator tends to zero for a subsequence, one cannot expect a bounded inverse of $\Phi _ { \hat { u } } ( \theta , \theta )$ .

A real number $\tau$ is of type $( b , \nu ) , b > 0 , \nu > 2$ , if

$$
\left| \tau - \frac {p}{q} \right| \geq \frac {b}{q ^ {\nu}} \quad \forall p, q \in \mathbb {Z} \backslash \{0 \}. \tag {1.38}
$$

Claim. For almost all real numbers $\tau$ , there exists $( b , \nu )$ depending on $\tau$ , such that $\tau$ is of type $( b , \nu )$ .

Indeed, for given $( b , \nu )$ , fixing $q$ , the set of all real numbers $\tau \in \mathsf { \Gamma } [ 0 , 1 ]$ 1 such that (1.38) does not hold has a measure less than $2 b q ^ { - \nu + 1 }$ . Therefore, the set of all real numbers $\tau$ such that (1.38) does not hold has the measure $\leqslant 2 b \sum q ^ { - \nu + 1 } < \infty$ . Since $b$ can be arbitrarily small, the conclusion follows.

Suppose $\sigma = e ^ { 2 \pi i \tau }$ , where $\tau$ is a number of type $( b , \nu )$ . Then

$$
\begin{array}{l} | \sigma - \sigma^ {j} | = \left| e ^ {2 \pi i (j - 1) (\tau - \frac {p}{j - 1})} - 1 \right| \\ \geq \left| \sin 2 \pi \left(\tau - \frac {p}{j - 1}\right) (j - 1) \right| \\ \geq \frac {2}{\pi} \cdot 2 \pi (j - 1) \left| \tau - \frac {p}{j - 1} \right| \\ \geq 4 \frac {b}{(j - 1) ^ {\nu - 1}} , \\ \end{array}
$$

therefore

$$
\frac {1}{| \sigma - \sigma^ {j} |} \leqslant \frac {j ^ {\nu - 1}}{4 b}.
$$

In this case, even if $\hat { v }$ has the convergent radius $r$ , $\hat { w }$ can only have a smaller convergent radius. In other words, if we introduce a family of Banach spaces:

$$
A (r) = \left\{\hat {w} (z) \mid \text {b o u n d e d a n d a n a l y t i c i n} | z | <   r, \hat {w} (0) = \hat {w} ^ {\prime} (0) = 0 \right\}
$$

with the norm

$$
| \hat {w} | _ {r} = \operatorname * {S u p} _ {| z | <   r} | \hat {w} (z) |,
$$

then we have, $\forall \ : \hat { v } \in A ( r )$

$$
\begin{array}{l} \left| \Phi_ {\hat {u}} (\theta , \theta) ^ {- 1} \hat {v} \right| _ {r - \delta} \leqslant \operatorname {S u p} _ {| z | <   r - \delta} \sum_ {j = 2} ^ {\infty} \left| \frac {v _ {j} z ^ {j}}{\sigma - \sigma^ {j}} \right| \\ \leqslant C \sum_ {j = 2} ^ {\infty} j ^ {\nu - 1} (r - \delta) ^ {j} | v _ {j} |, \\ \end{array}
$$

but

$$
| v _ {j} | = \frac {1}{2 \pi} \left| \int_ {| z | = r} \frac {v (z)}{z ^ {j + 1}} d z \right| \leqslant r ^ {- j} | v | _ {r},
$$

therefore

$$
\begin{array}{l} \left| \Phi_ {\hat {u}} (\theta , \theta) ^ {- 1} \hat {v} \right| _ {r - \delta} \leqslant C | v | _ {r} \sum_ {j = 2} ^ {\infty} j ^ {\nu - 1} \left(1 - \frac {\delta}{r}\right) ^ {j} (1.39) \\ \leqslant C \delta^ {- \nu} | v | _ {r} (1.40) \\ \end{array}
$$

where $C = C ( \nu , r )$ is a constant.

The loss of radius prevent us from simple iterations. One intends to gain back some of the loss by accelerating the convergence. It is known that the Newton method provides more efficient convergence speed. The iteration scheme, in contrast with (1.35), reads as follows:

$$
x _ {n + 1} = x _ {n} - f ^ {\prime} \left(x _ {n}\right) ^ {- 1} (y - f \left(x _ {n}\right)) \tag {1.41}
$$

We are led to the computation of $\Phi _ { \hat { u } } ( \hat { f } , \hat { u } ) ^ { - 1 }$ , but the later is too complicated. We intend to find an approximate inverse as a replacement. Of course, the approximation should match the convergence speed.

In doing so, let us perform some computations:

$$
(\Phi (\hat {f}, \hat {u})) ^ {\prime} (z) = \hat {f} ^ {\prime} (z + \hat {u} (z)) (1 + \hat {u} ^ {\prime} (z)) + [ \sigma \hat {u} ^ {\prime} (z) - \sigma \hat {u} ^ {\prime} (\sigma z) ],
$$

and

$$
\Phi_ {\hat {u}} (\hat {f}, \hat {u}) \hat {v} = \hat {f} ^ {\prime} (z + \hat {u} (z)) v (\hat {z}) + \sigma \hat {v} (z) - \hat {v} (\sigma z).
$$

Thus

$$
\Phi_ {\hat {u}} (\hat {f}, \hat {u}) \hat {v} = \frac {\hat {v}}{1 + \hat {u} ^ {\prime}} \Phi (\hat {f}, \hat {u}) ^ {\prime} + (1 + \hat {u} ^ {\prime} \circ \sigma) \Phi_ {\hat {u}} (\theta , \theta) \frac {\hat {v}}{1 + \hat {u} ^ {\prime}}.
$$

One takes

$$
T (\hat {f}, \hat {u}) \hat {w} = (1 + \hat {u} ^ {\prime}) \Phi_ {\hat {u}} (\theta , \theta) ^ {- 1} \left(\frac {\hat {w}}{1 + \hat {u} ^ {\prime} \circ \sigma}\right), \tag {1.42}
$$

as the approximate inverse with $\hat { v } ( z ) = ( 1 + \hat { u } ( z ) ) \hat { w } ( z )$ . In fact,

$$
\Phi_ {\hat {u}} (\hat {f}, \hat {u}) T (\hat {f}, \hat {u}) \hat {w} - \hat {w} = \Phi (\hat {f}, \hat {u}) ^ {\prime} \Phi_ {\hat {u}} (\theta , \theta) ^ {- 1} \left(\frac {\hat {w}}{1 + \hat {u} ^ {\prime} \circ \sigma}\right),
$$

and from the analyticity,

$$
\left| \Phi (\hat {f}, \hat {u}) ^ {\prime} \right| _ {r - \delta} \leqslant C \delta^ {- 1} \left| \Phi (\hat {f}, \hat {u}) \right| _ {r}.
$$

We take $\gamma \in ( 0 , 1 )$ , and $| \hat { u } ^ { \prime } | _ { r } < \gamma$ , then $1 + \hat { u } ^ { \prime } \circ \sigma \neq 0$ , and

$$
\left| \left(\Phi_ {\hat {u}} (\hat {f}, \hat {u}) T (\hat {f}, \hat {u}) - I d\right) \hat {w} \right| _ {r - \delta} \leqslant C \delta^ {- (\nu + 1)} \left| \Phi (\hat {f}, \hat {u}) \right| _ {r} \left| \hat {w} \right| _ {r}, \tag {1.43}
$$

where $C = C ( \gamma , \nu , r )$ . Similarly,

$$
\left| T \left(\hat {f}, \hat {u}\right) \hat {w} \right| _ {r - \delta} \leqslant C \delta^ {- \nu} | \hat {w} | _ {r}. \tag {1.44}
$$

Let us introduce a new norm:

$$
\left\| \hat {w} \right\| _ {r} = \operatorname {S u p} _ {| z | <   r} \left| \hat {w} ^ {\prime} (z) \right|. \tag {1.45}
$$

Obviously the following inequality holds:

$$
| \hat {w} | _ {r} \leqslant r \| \hat {w} \| _ {r}.
$$

Moreover, we estimate the error:

$$
\begin{array}{l} \Phi (\hat {f}, \hat {u}) - \Phi (\hat {f}, \hat {v}) - \Phi_ {\hat {u}} (\hat {f}, \hat {v}) (\hat {u} - \hat {v}) \\ = \hat {f} (z + \hat {u} (z)) - \hat {f} (z + \hat {v} (z)) - \hat {f} ^ {\prime} (z + \hat {v} (z)) (\hat {u} (z) - \hat {v} (z)) \\ = \int_ {0} ^ {1} \int_ {0} ^ {1} s f ^ {\prime \prime} (z + \hat {v} (z) + s t (\hat {u} (z) - \hat {v} (z))) d t d s (\hat {u} (z) - \hat {v} (z)) ^ {2}. \\ \end{array}
$$

By the Cauchy formula, if $f \in A ( r )$ with $| f | _ { r } < \gamma$ ,

$$
| f ^ {\prime \prime} (z) | = \frac {1}{\pi} \left| \int_ {| \zeta - z | = R} \frac {f (\zeta)}{(\zeta - z) ^ {3}} d \zeta \right| \leqslant \frac {2}{R ^ {2}} | f | _ {r}
$$

for $0 < R \leqslant r - | z |$ , we obtain

$$
\left| \Phi (\hat {f}, \hat {u}) - \Phi (\hat {f}, \hat {v}) - \Phi_ {\hat {u}} (\hat {f}, \hat {v}) (\hat {u} - \hat {v}) \right| _ {r - \delta} \leqslant \frac {2 \gamma}{\delta^ {2}} \| \hat {u} - \hat {v} \| _ {r} ^ {2}. \tag {1.46}
$$

The iteration scheme is now modified to

$$
\hat {u} _ {n + 1} = \hat {u} _ {n} - T (\hat {f}, \hat {u} _ {n}) \Phi (\hat {f}, u _ {n}). \tag {1.47}
$$

In order to show that $\hat { u } _ { n }$ converges to some point $\hat { u } ^ { * }$ , one needs to show that $| \Phi ( \hat { f } , \hat { u } _ { n } ) | _ { r _ { n } } \to 0$ , while the radius $r _ { n }$ depends on $n$ , very rapidly tends to a certain positive number $r ^ { * }$ .

Theorem 1.4.1 Let $X _ { \lambda }$ , $Y _ { \lambda }$ , $Z _ { \lambda }$ , $\lambda \in ( 0 , 1 ]$ be three families of Banach spaces with nondecreasing norms, i.e., $\| \cdot \| _ { \mu } \leqslant \| \cdot \| _ { \lambda }$ for $\mu \leqslant \lambda$ . Let $( { \overline { { x } } } , { \overline { { y } } } ) \in X _ { 1 } \times$ $Y _ { 1 } , r ~ > ~ 0$ , and $\Omega _ { r } ^ { \lambda } = B _ { r } ^ { \lambda } ( \overline { { { x } } } ) \times B _ { r } ^ { \lambda } ( \overline { { { y } } } ) \subset X _ { \lambda } \times Y _ { \lambda }$ , where $B _ { r } ^ { \lambda } ( { \overline { { x } } } )$ is the ball centered at $x$ , with radius $r$ in $X ^ { \lambda }$ , similarly for the notations $B _ { r } ^ { \lambda } ( { \overline { { y } } } )$ etc. Let $\Phi : \Omega _ { r } ^ { \lambda }  Z _ { \lambda } \ \forall \lambda \in ( 0 , 1 ]$ be $C ^ { 1 }$ w.r.t. $y$ , and satisfy $\Phi ( { \overline { { x } } } , { \overline { { y } } } ) = \theta$ . We assume:

(1) $| \Phi ( x , y ) - \Phi ( x , y ^ { \prime } ) - \Phi _ { y } ( x , y ^ { \prime } ) ( y - y ^ { \prime } ) | _ { \lambda - \delta } \leqslant M \delta ^ { - 2 \alpha } \| y - y ^ { \prime } \| _ { \lambda } ^ { 2 } , \forall y ^ { \prime }$ - ∈ Bλr (y),   
(2) $\exists T ( x , y ) \in L ( Z _ { \lambda } , Y _ { \lambda - \delta } )$ such that $\mid T ( x , y ) z \vert _ { \lambda - \delta } \leqslant M \delta ^ { - \tau } \vert z \vert _ { \lambda }$   
(3) $| ( \Phi _ { y } ( x , y ) T ( x , y ) - I ) z | _ { \lambda - \delta } \leqslant M \delta ^ { - 2 ( \alpha + \tau ) } | z | _ { \lambda } | \Phi ( x , y ) | _ { \lambda }$

where $M \geq 1 , \alpha \geq 0 , \tau > 1$ are constants, and $0 < \delta < \lambda$ . One concludes that $\exists C = C ( M , \alpha , \tau ) > 0$ such that $\forall ( x , y ) \in \ \Omega _ { r } ^ { \overline { { \lambda } } } , \forall \overline { { \lambda } } \in ( 0 , 1 ]$ with $| \Phi ( x , y ) | _ { \overline { { \lambda } } } \leqslant$ $C \overline { { \lambda } } ^ { - 2 ( \alpha + \tau ) }$ , we have $y _ { * } = u ( x ) \in Y _ { \frac { \overline { { \lambda } } } { 2 } }$ satisfying

$$
\Phi (x, u (x)) = \theta .
$$

Proof. One chooses sequences

$$
\lambda_ {n} = \frac {\bar {\lambda}}{2} (1 + 2 ^ {- n}), \quad n = 0, 1, 2, \dots ,
$$

and

$$
\mu_ {n + 1} = \frac {1}{2} (\lambda_ {n} + \lambda_ {n + 1}) = \frac {\bar {\lambda}}{2} (1 + 3 \cdot 2 ^ {- n - 2}), \quad n = 0, 1, 2, \dots .
$$

Then $\lambda _ { 0 } = { \overline { { \lambda } } } , \ \lambda _ { n } \downarrow \ { \frac { \lambda } { 2 } }$ .

For simplicity, we write $\Phi ( y _ { n } ) = \Phi ( x , y _ { n } )$ , $T ( y _ { n } ) = T ( x , y _ { n } )$ . The Newton iteration sequence reads as

$$
\left\{ \begin{array}{l} y _ {n + 1} = y _ {n} - T \left(y _ {n}\right) \Phi \left(y _ {n}\right) \quad n = 0, 1, 2, \dots \\ y _ {0} = y. \end{array} \right. \tag {1.48}
$$

We want to prove:

(1)- $y _ { n } \in B _ { r } ^ { \lambda _ { n } } ( \overline { { y } } )$   
$( 2 ) ^ { \prime }$   
(3)-

If they are proved, then

$$
\sum_ {n = 1} ^ {\infty} \left\| y _ {n + 1} - y _ {n} \right\| _ {\frac {\lambda}{2}} \leqslant M \overline {{\lambda}} ^ {- \tau} 2 ^ {3 \tau} \sum_ {n = 1} ^ {\infty} 2 ^ {n \tau} c _ {n} <   \infty .
$$

$\exists y _ { * } = u ( x )$ $y _ { n } \to y _ { * }$ $Y _ { \frac { \overline { { \lambda } } } { 2 } }$

$$
\left| \Phi (x, y _ {*}) \right| _ {\bar {\lambda}} \leqslant \lim  _ {n \rightarrow \infty} \left| \Phi (y _ {n}) \right| _ {\lambda_ {n}} = \lim  _ {n \rightarrow \infty} c _ {n} = 0,
$$

i.e., $\Phi ( x , y _ { * } ) = \theta$

First, we prove $( 3 ) ^ { \prime }$ . In fact, From (1.48) and assumption (2),

$$
\begin{array}{l} \left\| y _ {n + 1} - y _ {n} \right\| _ {\mu_ {n + 1}} \leqslant \left\| T (y _ {n}) \Phi (y _ {n}) \right\| _ {\mu_ {n + 1}} \\ \leqslant M \left(2 ^ {(n + 3) \tau} \bar {\lambda} ^ {- \tau}\right) | \Phi (y _ {n}) | _ {\lambda_ {n}} \\ = M c _ {n} \left(2 ^ {(n + 3) \tau} \bar {\lambda} ^ {- \tau}\right). \\ \end{array}
$$

Next, we turn to proving $( 2 ) ^ { \prime }$ . By (1.48), we have

$$
\begin{array}{l} \Phi (y _ {n + 1}) = \left[ \Phi (y _ {n + 1}) - \Phi (y _ {n}) - \Phi_ {y} (x, y _ {n}) (y _ {n + 1} - y _ {n}) \right] \\ - \left[ \Phi_ {y} \left(x, y _ {n}\right) T \left(y _ {n}\right) - I \right] \Phi \left(y _ {n}\right). \tag {1.49} \\ \end{array}
$$

Combining $( 3 ) ^ { \prime }$ , with assumptions (1) and (3), for $n$ large,

$$
\begin{array}{l} c _ {n + 1} = \left| \Phi \left(y _ {n + 1}\right) \right| _ {\lambda_ {n + 1}} \leqslant M 2 ^ {2 \alpha (n + 3)} \bar {\lambda} ^ {- 2 \alpha} \| y _ {n + 1} - y _ {n} \| _ {\mu_ {n + 1}} ^ {2} \\ + M c _ {n} ^ {2} \left(2 ^ {n + 3} \bar {\lambda} ^ {- 1}\right) ^ {2 (\alpha + \tau)} \leq a q ^ {n} c _ {n} ^ {2}, \\ \end{array}
$$

where $a = \overline { { { \lambda } } } ^ { - 2 ( \alpha + \tau ) } q ^ { 3 } ( M + M ^ { 3 } )$ and $q = 4 ^ { \alpha + \tau }$

Set

$$
\alpha_ {n} = a q ^ {n} c _ {n};
$$

we obtain

$$
\alpha_ {n + 1} = a q ^ {n + 1} c _ {n + 1} \leqslant a ^ {2} q ^ {2 n + 1} c _ {n} ^ {2} = q \alpha_ {n} ^ {2} \quad \forall n.
$$

One chooses $k \in ( 1 , 2 )$ , $\epsilon _ { 0 } \in ( 0 , 1 )$ satisfying

$$
q ^ {\frac {1}{k - 1}} \epsilon_ {0} <   1,
$$

and $\epsilon _ { n + 1 } = q \epsilon _ { n } ^ { k } ~ \forall n$ . Thus

$$
\epsilon_ {n + 1} = q ^ {1 + k + k ^ {2} + \dots + k ^ {n}} \epsilon_ {0} ^ {k ^ {n + 1}} = q ^ {- \frac {1}{k - 1}} \left(q ^ {\frac {1}{k - 1}} \epsilon_ {0}\right) ^ {k ^ {n + 1}} <   1.
$$

For sufficiently small $c _ { 0 } = | \Phi ( x , y ) | _ { \overline { { \lambda } } }$ , such that $\alpha _ { 0 } = a c _ { 0 } \leqslant \epsilon _ { 0 }$ , by mathematical induction, it is easy to show that

$$
\alpha_ {n} \leqslant \epsilon_ {n} \quad \forall n.
$$

Thus

$$
c _ {n + 1} \leqslant \alpha_ {n} c _ {n} \leqslant \epsilon_ {n} c _ {n}. \tag {1.50}
$$

It follows $c _ { n + 1 } \leqslant c _ { n }$ and $c _ { n + 1 } \leqslant c _ { 0 } \epsilon _ { n }$ . However, $\epsilon _ { n }$ converges to zero hypergeometrically. That $( 2 ) ^ { \prime }$ is proved.

Finally, we verify $( 1 ) ^ { \prime }$ . From

$$
\begin{array}{l} \left\| y _ {n + 1} - y _ {0} \right\| _ {\mu_ {n + 1}} \leqslant \sum_ {j = 0} ^ {n} \left\| y _ {j + 1} - y _ {j} \right\| _ {\mu_ {j + 1}} \\ \leqslant a \sum_ {j = 0} ^ {n} c _ {j} q ^ {\frac {j}{2}}, \\ \end{array}
$$

one sees that $\Finv$ a constant $M _ { 1 } > 0$ , such that

$$
\left\| y _ {n + 1} - y _ {0} \right\| _ {\mu_ {n + 1}} \leqslant a c _ {0} M _ {1}.
$$

If $c _ { 0 }$ is so small that

$$
\left\| y _ {n + 1} - y _ {0} \right\| _ {\mu_ {n + 1}} <   r - \left\| y - \bar {y} \right\| _ {\bar {\lambda}} \quad \forall n,
$$

then

$$
\left\| y _ {n} - y \right\| _ {\lambda_ {n}} \leqslant \left\| y _ {n} - y \right\| _ {\mu_ {n}} <   r - \left\| y - \bar {y} \right\| _ {\lambda_ {n}}.
$$

Therefore

$$
\left\| y _ {n} - \overline {{y}} \right\| _ {\lambda_ {n}} <   r.
$$

The proof is complete.

As an application, now we return to the small divisor problem. We have the following theorem.

Theorem 1.4.2 (Siegel) Suppose that $\tau$ is of type $( b , \nu ) , b > 0 , \nu > 2$ . Let $\sigma ~ = ~ e ^ { 2 \pi i \tau }$ . If $f ( z ) ~ = ~ \sigma z + { \hat { f } } ( z )$ is an analytic function on $| z | < 1$ , with ${ \hat { f } } ( 0 ) = { \hat { f } } ^ { \prime } ( 0 ) = 0$ , then there exist γ, $r _ { 0 } \in ( 0 , 1 )$ and an analytic function u on |z| < 4(1+γ) $\begin{array} { r } { | z | < \frac { 3 r _ { 0 } } { 4 ( 1 + \gamma ) } } \end{array}$ 3r0 such that

$$
\sup  _ {| z | <   r _ {0}} | \hat {f} (z) | \leqslant \gamma
$$

and $f \circ u = u \circ \sigma$

Proof. We introduce Banach space families: $\forall r > 0$ ,

$$
A (r) = \left\{w (z) \text {a n a l y t i c a n d b o u n d e d i n} | z | <   r, w (0) = w ^ {\prime} (0) = 0 \right\},
$$

with norm

$$
| w | _ {r} = \sup  _ {| z | <   r} | w (z) |,
$$

and

$$
\widetilde {A} (r) = \left\{w (z) \in A (r) \mid w ^ {\prime} (z) \text {i s b o u n d e d i n} | z | <   r \right\},
$$

with norm

$$
\| w \| _ {r} = \sup  _ {| z | <   r} | w ^ {\prime} (z) |.
$$

Set

$$
X _ {\lambda} = A (r _ {0}), \quad Y _ {\lambda} = \widetilde {A} (r _ {\lambda}) \text {a n d} Z _ {\lambda} = A (r _ {\lambda})
$$

where $\begin{array} { r } { \lambda \in ( 0 , 1 ] , r _ { \lambda } = \frac { 1 + \lambda } { 2 } \rho , \ \rho = \frac { r _ { 0 } } { 1 + \gamma } } \end{array}$ , $\gamma \in \mathsf { \Gamma } ( 0 , 1 ]$ and $0 < \gamma < 1$ is to be determined. Set $\Phi ( \hat { f } , \hat { u } ) = \hat { f } ( z + \hat { u } ( z ) ) + \sigma \hat { u } ( z ) - \hat { u } ( \sigma z )$ as in (1.37). Set $\overline { { \lambda } } = 1$ . For $\| \hat { f } \| _ { A ( r _ { 0 } ) } < \gamma$ , $\| \hat { u } \| _ { \widetilde { A } ( \rho ) } < \gamma$ , we have

$$
\left| \Phi (\hat {f}, \hat {u}) \right| _ {A (\rho)} \leqslant \gamma + 2 \left| \hat {u} \right| _ {A (\rho)} \leqslant \gamma (1 + 2 \rho).
$$

If $r _ { 0 } > 0$ is small, we may choose $\gamma > 0$ small. Since all assumptions (1)–(3) of Theorem 1.4.1 are satisfied (cf. (1.46), (1.44), and (1.43) respectively), the conclusion follows from Theorem 1.4.1. □

# 1.4.2 Nash–Moser Iteration

Let us turn to the “loss of derivative” problem. Besides the simple iteration method, there are other ways to solve equation (1.34). Given $c > 0$ and a curve $x ( t )$ satisfying

$$
f (x (t)) = (1 - e ^ {- c t}) y,
$$

$x ( t )$ satisfies the ODE:

$$
\dot {x} (t) = \left(f ^ {\prime} (x (t))\right) ^ {- 1} (y - f (x (t))).
$$

If, for some initial data $x ( 0 ) = x _ { 0 }$ , the solution globally exists, and has a limit $x ( t ) \to x _ { \infty }$ , then

$$
f (x _ {\infty}) = y.
$$

Discretizing the equation, we return to Newton’s approximation scheme:

$$
x _ {n + 1} - x _ {n} = c f ^ {\prime} \left(x _ {n}\right) ^ {- 1} \left(y - f \left(x _ {n}\right)\right).
$$

This is (1.41) for $c = 1$ .

There is enough room to generalize the above method. Let $f ( x _ { 0 } ) = y _ { 0 }$ be a special solution. In order to overcome the problem of “loss of derivatives”, we introduce a family of smoothing operators $S _ { t } , t \in [ 1 , \infty )$ , which regularizes the function $x$ , and satisfies $S _ { t }  I d$ , as $t \to \infty$ . Find a suitable function $g ( t )$ , which will be described later so that the curve $x ( t )$ tends to a solution $x _ { \infty }$ of the equation $f ( x ) = y$ . We intend to solve the ODE:

$$
\left\{ \begin{array}{l} \dot {x} (t) = f ^ {\prime} (v (t)) ^ {- 1} g (t) \\ v (t) = S _ {t} x (t) \\ x (0) = x _ {0}  . \end{array} \right.
$$

Since

$$
\begin{array}{l} \frac {d}{d t} f (x (t)) = f ^ {\prime} (x (t)) \dot {x} (t) \\ = \left(f ^ {\prime} (x (t)) - f ^ {\prime} (v (t))\right) \dot {x} (t) + g (t), \\ \end{array}
$$

we should have

$$
f (x _ {\infty}) - f (x _ {0}) = \int_ {0} ^ {\infty} e (t) d t + \int_ {0} ^ {\infty} g (t) d t,
$$

where $e ( t ) = ( f ^ { \prime } ( x ( t ) ) - f ^ { \prime } ( v ( t ) ) ) \dot { x } ( t )$ .

Design the iteration scheme:

$$
\left\{ \begin{array}{l} \delta_ {n} := x _ {n + 1} - x _ {n} = \triangle_ {n} f ^ {\prime} \left(v _ {n}\right) ^ {- 1} g _ {n}, \\ v _ {n} = S _ {\theta_ {n}} x _ {n}, \\ \triangle_ {n} = \theta_ {n + 1} - \theta_ {n}, \end{array} \right. \tag {1.51}
$$

where $\theta _ { 1 } < \theta _ { 2 } < . . . < \theta _ { n } \to \infty$ . Then

$$
f \left(x _ {n + 1}\right) - f \left(x _ {n}\right) = \triangle_ {n} \left(e _ {n} + g _ {n}\right),
$$

where

$$
\triangle_ {n} e _ {n} = f \left(x _ {n} + \delta_ {n}\right) - f \left(x _ {n}\right) - f ^ {\prime} \left(x _ {n}\right) \delta_ {n} + \left(f ^ {\prime} \left(x _ {n}\right) - f ^ {\prime} \left(v _ {n}\right)\right) \delta_ {n}.
$$

It follows that

$$
f \left(x _ {n + 1}\right) - f \left(x _ {0}\right) = \sum_ {j = 0} ^ {n} \triangle_ {j} \left(g _ {j} + e _ {j}\right).
$$

Since we are only interested in the limiting result, sometimes we modify it to be:

$$
\sum_ {j = 0} ^ {n} \triangle_ {j} g _ {j} + S _ {\theta_ {n}} E _ {n} = S _ {\theta_ {n}} (y - f (x _ {0})) ,
$$

where $\begin{array} { r } { E _ { n } = \sum _ { j = 0 } ^ { n - 1 } \triangle _ { j } e _ { j } } \end{array}$ , so that $g _ { n }$ can be determined step by step.

$$
g _ {n} = \bigtriangleup_ {n} ^ {- 1} \left[ \left(S _ {\theta_ {n}} - S _ {\theta_ {n - 1}}\right) \left(y - f \left(x _ {0}\right) - E _ {n - 1}\right) - \bigtriangleup_ {n - 1} S _ {\theta_ {n}} e _ {n - 1} \right], \quad n = 1, 2, \dots , \tag {1.52}
$$

and

$$
g _ {0} = \triangle_ {0} ^ {- 1} S _ {\theta_ {0}} (y - f (x _ {0})).
$$

We start with an abstract framework. $\{ E _ { a } \} _ { a \ge 0 }$ is called a family of Banach spaces with smoothing operators, if $E _ { b } \hookrightarrow E _ { a }$ $\operatorname { f o r } b \geq a$ is an injection, and $\exists C = C ( a , b )$ such that

$$
\left\| u \right\| _ {a} \leqslant C \left\| u \right\| _ {b}.
$$

Let $E _ { \infty } = \cap _ { a \ge 0 } E _ { a }$ be endowed with the weakest topology, such that $E _ { \infty } \hookrightarrow$ $E _ { a }$ is continuous. Moreover, we assume that $\Finv$ a family of linear operators $S _ { \theta } : E _ { 0 }  E _ { \infty }$ , depending on a parameter $\theta \geq 1$ , such that

(1)   
(2)

where $C$ is a constant depending on $a$ and $b$ .

The following inequalities hold:

(1) $\| S _ { \theta } u \| _ { b } \leqslant C \theta ^ { b - a } \| u \| _ { a } , \forall a < b$   
(2) $\| ( I - S _ { \theta } ) u \| _ { b } \leqslant C \theta ^ { b - a } \| u \| _ { a } \forall b < a .$   
(3)

Both (1) and (2) follow from the second inequality of the definition. In fact, if $a > b$ , then

$$
\begin{array}{l} \| u - S _ {\theta} u \| _ {b} = \left\| \int_ {\theta} ^ {\infty} \frac {d}{d t} S _ {t} u \right\| _ {b} \\ \leqslant C \int_ {\theta} ^ {\infty} t ^ {b - a - 1} \| u \| _ {a} d t = C \theta^ {b - a} \| u \| _ {a} \quad \text {i f} b <   a. \\ \end{array}
$$

and if $a < b$ , then

$$
\begin{array}{l} \| S _ {\theta} u \| _ {b} = \left\| \int_ {1} ^ {\theta} \frac {d}{d t} S _ {t} u \right\| _ {b} + \| S _ {1} u \| _ {b} \\ \leqslant C \int_ {1} ^ {\theta} t ^ {b - a - 1} d t \| u \| _ {a} + C \| u \| _ {a} \leqslant C \theta^ {b - a} \| u \| _ {a} \\ \end{array}
$$

where $C$ denotes various constants.

(3) is a consequence of (1) and (2). In fact, may assume $a < b$ . For $c =$ $\lambda a + ( 1 - \lambda ) b$ ,

$$
\begin{array}{l} \left\| u \right\| _ {c} \leqslant \left\| S _ {\theta} u \right\| _ {c} + \left\| (I - S _ {\theta}) u \right\| _ {c} \\ \leqslant C \left(\theta^ {c - a} \| u \| _ {a} + \theta^ {c - b} \| u \| _ {b}\right) \\ \end{array}
$$

By choosing $\theta = ( { \frac { C \| u \| _ { b } } { \| u \| _ { a } } } ) ^ { \frac { 1 } { b - a } }$ , we obtain the desired inequality.

# Example. H¨older Spaces with Smoothing Operators

Let $\Omega$ be a bounded domain in $R ^ { n }$ we write $H ^ { \alpha } ( \Omega )$ as the H¨older space, defined as follows: $H ^ { 0 } ( \Omega ) = C ( \Omega )$ . If $k \geq 0$ is an integer, $k < a \leqslant k + 1$ ,

$$
H ^ {\alpha} (\overline {{\Omega}}) = \left\{ \begin{array}{l} C ^ {\alpha} (\overline {{\Omega}}) \text {i f} a <   k + 1 \\ C ^ {\alpha - 0} (\overline {{\Omega}}) \text {i f} a = k + 1. \end{array} \right.
$$

The seminorm is defined to be

$$
| u | _ {a} = \sum_ {| \alpha | = k} | \partial^ {\alpha} u | _ {a - k},
$$

and for $0 < a \leqslant 1$ ,

$$
| u | _ {a} = \sup  _ {x, y \in \overline {{\Omega}}} \frac {| u (x) - u (y) |}{| x - y | ^ {\alpha}},
$$

and the norm is defined as:

$$
\left\| u \right\| _ {a} = \left\| u \right\| _ {C} + \left| u \right| _ {a}.
$$

We now come to define the smoothing operators $S _ { \theta } , \theta \geq 1$ . For the sake of simplicity, we assume $\Omega = R ^ { n }$ , the bounded domain case can be modified by standard argument. For a compact set $K$ in $\mathbb { R } ^ { n }$ , one chooses ${ \mathcal { X } } \in C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { n } )$ to be 1 in a neighborhood of $K$ , a function $\psi \in C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { n } )$ to be 1 in a neighborhood of 0, and let $\varphi$ be the Fourier transform of $\psi , \varphi = { \mathcal { F } } \psi$ , i.e.,

$$
\varphi (x) = \int_ {R ^ {n}} \exp {(2 \pi i x \xi)} \psi (\xi) d \xi .
$$

Let

$$
\varphi_ {\theta} (x) = \theta^ {n} \varphi (\theta x) \theta \geq 1.
$$

If $u$ has support in $K$ , we define

$$
S _ {\theta} u = \mathcal {X} (\varphi_ {\theta} * u) \in C _ {0} ^ {\infty} (\mathbb {R} ^ {n}).
$$

Since $\varphi _ { \theta } ( x ) = { \mathcal { F } } \psi ( \xi / \theta )$ , and $\psi ( \mathfrak { \xi } ) \to 1$ (in the distribution sense), we have $\varphi _ { \boldsymbol { \theta } }  \boldsymbol { \delta }$ , so $S _ { \theta } u  u$ , as $\theta  + \infty$ . The operators $S _ { \theta }$ , which approximate the identity and map to smooth functions, are call smoothing operators.

Theorem 1.4.3 The smoothing operators $S _ { \theta }$ have the following properties, for $\theta > 1$ and $u \in H ^ { \alpha }$ :

(1) $\| S _ { \theta } u \| _ { b } \leqslant C \| u \| _ { a } \quad b \leqslant a ,$   
(2) $\begin{array} { r } { \| \frac { d } { d \theta } S _ { \theta } u \| _ { b } \leqslant C \theta ^ { b - a - 1 } \| u \| _ { a } . } \end{array}$

Proof. (1) following directly from the translation invariance and the convexity of the norms:

$$
\left\| \varphi_ {\theta} * u \right\| _ {a} \leqslant \left\| \varphi \right\| _ {L ^ {1}} \| u \| _ {a}.
$$

We verify (2). Noticing

$$
\frac {d}{d \theta} S _ {\theta} u = \mathcal {X} \cdot \mathcal {F} ^ {- 1} (\theta^ {- 1} \psi_ {1} \left(\frac {\xi}{\theta}\right) \cdot (\mathcal {F} u) (\xi))
$$

where

$$
\psi_ {1} (\xi) = - \xi \cdot \nabla \psi (\xi),
$$

and ${ \mathcal { F } } ^ { - 1 }$ is the inverse Fourier transform. Again $\psi _ { 1 } \in C _ { 0 } ^ { \infty }$ , and vanishes in the neighborhood of the origin.

According to (1), (2) holds for $b = a$ . We shall only need to verify (2) for $b = 0$ and $b = a + k , \ k \in \mathbb { N }$ , because the remaining cases follow from the interpolation inequality.

For $b = a + k$ ,

$$
\begin{array}{l} \left\| \frac {d}{d \theta} S _ {\theta} u \right\| _ {a + k} = \sum_ {| \alpha | = k} \left\| \partial^ {\alpha} \left(\frac {d}{d \theta} S _ {\theta} u\right) \right\| _ {a} \\ = \sum_ {| \alpha | = k} \left\| \mathcal {F} ^ {- 1} \left(\theta^ {k - 1} \left(\frac {\xi^ {\alpha}}{\theta^ {k}}\right) \psi_ {1} \left(\frac {\xi}{\theta}\right) \cdot (\mathcal {F} u) (\xi)\right) \right\| _ {a} \\ \leqslant C \theta^ {k - 1} \| u \| _ {a} \\ \end{array}
$$

because $\xi ^ { \alpha } \psi _ { 1 } ( \xi )$ is again in $L ^ { 1 }$ .

For $b = 0$ , we only need to prove for $a = k \in \mathbb { N }$ . Let us write

$$
\psi_ {1} (\xi) = \sum_ {| \alpha | = k} \xi^ {\alpha} \psi_ {\alpha} (\xi) \text {w h e r e} \psi_ {\alpha} (\xi) = \frac {\xi^ {\alpha}}{\sum_ {| \alpha | = k} | \xi^ {\alpha} | ^ {2}} \psi_ {1} (\xi),
$$

then $\psi _ { \alpha } \in C _ { 0 } ^ { \infty }$ and vanishes in the neighborhood of the origin:

$$
\begin{array}{l} \left\| \frac {d}{d \theta} S _ {\theta} u \right\| _ {0} \leqslant \sum_ {| \alpha | = k} \left\| \mathcal {F} ^ {- 1} \left(\psi_ {\alpha} \left(\frac {\xi}{\theta}\right) (\partial^ {\alpha} u) (\xi)\right) \right\| _ {0} \theta^ {- k - 1} \\ \leqslant \theta^ {- k - 1} \sum_ {| \alpha | = k} \left\| \int [ \partial^ {\alpha} u (x - y) - \partial^ {\alpha} u (x) ] (\psi_ {\alpha}) _ {\theta} (y) d y \right\| \\ = \theta^ {- k - 1} \cdot \int | y | | \left(\psi_ {\alpha}\right) _ {\theta} (y) | d y \| u \| _ {k + 1} \\ = C \theta^ {- k - 2} \| u \| _ {k + 1}. \\ \end{array}
$$

The proof is complete.

Now we are going to introduce a family of Banach spaces associated with $E _ { a }$ as follows: Let $\theta _ { j } = 2 ^ { \jmath } , \ j = 0 , 1 , 2 , \ldots , \ \triangle _ { j } = \theta _ { j + 1 } - \theta _ { j } , \ j \geq 1 , \triangle _ { 0 } = 1$ :

$$
R _ {0} = \triangle_ {0} ^ {- 1} S _ {\theta_ {1}}, R _ {j} = \triangle_ {j} ^ {- 1} (S _ {\theta_ {j + 1}} - S _ {\theta_ {j}}), j \geq 1
$$

According to (2),

$$
\| R _ {j} u \| _ {b} \leqslant \triangle_ {j} ^ {- 1} \left\| \int_ {\theta_ {j}} ^ {\theta_ {j + 1}} \frac {d}{d t} S _ {t} u \right\| _ {b} \leqslant C \frac {\theta_ {j + 1} ^ {b - a} - \theta_ {j} ^ {b - a}}{(b - a) \triangle_ {j}} \| u \| _ {a}.
$$

We obtain:

(4) $\| R _ { j } u \| _ { b } \leqslant C _ { a b } \theta _ { j } ^ { b - a - 1 } \| u \| _ { a } ,$

and

(5) $\begin{array} { r } { \sum _ { j = 0 } ^ { \infty } \triangle _ { j } R _ { j } u = u . } \end{array}$

The series is convergent in $E _ { b }$ if $u \in E _ { a } , a > b$

This is called the Paley–Littlewood decomposition.

Conversely, suppose we have a sequence $\{ u _ { j } \} \subset E _ { a } , a \in [ a _ { 1 } , a _ { 2 } ]$ , satisfying

$$
\left\| u _ {j} \right\| _ {a _ {i}} \leqslant C \theta_ {j} ^ {a _ {i} - a - 1}, \quad i = 1, 2 \forall j;
$$

then, by interpolation inequality,

(6) $\| u _ { j } \| _ { b } \leqslant C { \theta } _ { j } ^ { b - a - 1 } \forall b \in [ a _ { 1 } , a _ { 2 } ] , \forall j ,$

and then the series $\textstyle \sum _ { j } \triangle _ { j } u _ { j }$ converges in $E _ { b }$ if $b < a$

Definition 1.4.4 For $a \in [ a _ { 1 } , a _ { 2 } ]$ , we define $\begin{array} { r } { E _ { a } ^ { \prime } = \{ u = \sum _ { j = 0 } ^ { \infty } \triangle _ { j } u _ { j } \ | \ \| u _ { j } \| _ { a _ { i } } \leqslant } \end{array}$ $C \theta _ { j } ^ { a _ { i } - a - 1 } \quad i = 1 , 2 , \ \forall j \}$ with norm:

$$
\| u \| _ {a} ^ {\prime} = \inf  _ {u = \sum_ {j} \triangle_ {j} u _ {j}} \sup  _ {i, j} \theta_ {j} ^ {- a _ {i} + a + 1} \| u _ {j} \| _ {a _ {i}}.
$$

Then $E _ { a } ^ { \prime }$ is a family of Banach spaces. The following properties hold:

(7) $\| u \| _ { b } \leqslant C \| u \| _ { a } ^ { \prime } \leqslant C _ { 1 } \| u \| _ { a }$ if $b < a$

To prove the first inequality. $\forall \epsilon > 0 \forall u \in E _ { a } ^ { \prime } \exists \{ u _ { j } \} \subset E _ { a _ { i } } , i = 1 , 2$ such that $\begin{array} { r } { u = \sum _ { j } \triangle _ { j } u _ { j } } \end{array}$ , and

$$
\left\| u _ {j} \right\| _ {a _ {i}} \leqslant C \theta_ {j} ^ {a _ {i} - a - 1} \left(\left\| u \right\| _ {a} ^ {\prime} + \epsilon\right).
$$

From (6), $\| u _ { j } \| _ { b } \leqslant C \theta _ { j } ^ { b - a - 1 }$ . It follows that $\| u \| _ { b } \leqslant C ( \| u \| _ { a } ^ { \prime } + \epsilon )$

The second inequality follows from (4) and (5).

(8) The space $E _ { a } ^ { \prime }$ does not depend on $a _ { 1 } , ~ a _ { 2 }$ . It follows from the interpolation inequality directly.

(9) $\| ( I - S _ { \theta } ) u \| _ { b } \leqslant C \theta ^ { b - a } \| u \| _ { a } ^ { \prime }$ for $b < a$ . Indeed, $\forall u = E _ { a } ^ { \prime }$ one chooses $a _ { 1 } < a _ { 2 }$ such that $b < a _ { 1 } < a < a _ { 2 }$ . $\forall \epsilon > 0$ , $\exists \{ u _ { j } \} \subset E _ { a _ { i } } , i = 1 , 2$ , such that $u = \Sigma \triangle _ { j } u _ { j }$ and $\| u _ { j } \| _ { a _ { i } } \le C { \theta } _ { j } ^ { a _ { i } - a - 1 } ( \| u \| _ { a } ^ { \prime } + \epsilon )$ . Then

$$
\left\| \left(I - S _ {\theta}\right) u \right\| _ {b} \leqslant \sum \triangle_ {j} \| u _ {j} - S _ {\theta} u _ {j} \| _ {b},
$$

and

$$
\| (I - S _ {\theta}) u _ {j} \| _ {b} \leqslant C \theta^ {b - a _ {i}} \| u _ {j} \| _ {a _ {i}} \leqslant C \theta^ {b - a _ {i}} \theta_ {j} ^ {a _ {i} - a - 1} (\| u \| _ {a} ^ {\prime} + \epsilon), i = 1, 2,
$$

therefore

$$
\begin{array}{l} \left\| (I - S _ {\theta}) u \right\| _ {b} \leqslant C (\left\| u \right\| _ {a} ^ {\prime} + \epsilon) \\ \times \left(\sum_ {\theta_ {j} > \theta} \triangle_ {j} \theta^ {b - a _ {1}} \theta_ {j} ^ {a _ {1} - a - 1} + \sum_ {\theta_ {j} <   \theta} \triangle_ {j} \theta^ {b - a _ {2}} \theta_ {j} ^ {a _ {2} - \theta - 1}\right). \\ \end{array}
$$

Thus,

$$
\left\| \left(I - S _ {\theta}\right) u \right\| _ {b} \leqslant C M \theta^ {b - a} \| u \| _ {a} ^ {\prime}.
$$

Theorem 1.4.5 Suppose that $\{ E _ { a } \}$ and $\{ F _ { a } \}$ are two families of Banach spaces with smoothing operators, and that the embedding $F _ { b } \hookrightarrow F _ { a }$ is compact when $b > a$ . Let $\alpha , \beta , r > 0 , a n d B _ { r } ^ { \alpha }$ be a ball with radius $T$ centered at the origin in $E _ { \alpha } ^ { \prime }$ . Assume that $f : B _ { r } ^ { \alpha } \to F _ { \beta }$ is $C ^ { 2 }$ with $f ( \theta ) = \theta$ , and satisfies

(1) $f ^ { \prime } ( v ) ^ { - 1 }$ exists for $v \in B _ { r } ^ { \alpha } \cap E _ { \infty }$ , and $\forall g \in F _ { \infty }$ , the map $( v , g ) \mapsto f ^ { \prime } ( v ) ^ { - 1 } g$ : $\left( E _ { \infty } \cap B _ { r } ^ { \alpha } \right) \times F _ { \infty } \to E _ { a _ { 2 } }$ is continuous for some $a _ { 2 } > \alpha > a _ { 1 } \geqslant 0$ , and satisfies

$$
\| f ^ {\prime} (v) ^ {- 1} g \| _ {a} \leq C (\| g \| _ {\beta + a - \alpha} + \| g \| _ {0} \| v \| _ {\beta + \alpha}) \quad \forall a \in [ a _ {1}, a _ {2} ],
$$

(2) $\begin{array} { r } { \| f ^ { \prime \prime } ( u ) ( v , w ) \| _ { \beta + \delta } \leqslant C \sum _ { \operatorname* { m a x } ( l - \alpha , 0 ) + \operatorname* { m a x } ( m , a _ { 1 } ) + n < 2 \alpha } ( 1 + \| u \| _ { l } ) \| v \| _ { m } \| w \| _ { n } , } \end{array}$

for some $\delta > 0$ . Then $\forall y \in F _ { \beta } ^ { \prime }$ with $\| y \| _ { \beta } ^ { \prime }$ small, $\exists u \in E _ { \alpha } ^ { \prime }$ satisfying

$$
f (u) = y.
$$

Proof. One uses the iteration scheme (1.51), so one should determine $g$ first. As we have seen from above, after decomposition, it satisfies a series of recursive equations.

$\forall g \in F _ { \beta } ^ { \prime }$ we have the decomposition:

$$
g = \sum_ {j} \triangle_ {j} g _ {j},
$$

with

$$
\left\| g _ {j} \right\| _ {b} \leqslant C _ {b} \theta_ {j} ^ {b - \beta - 1} \| g \| _ {\beta} ^ {\prime}. \tag {1.53}
$$

Define

$$
\left\{ \begin{array}{l} u _ {j + 1} = u _ {j} + \triangle_ {j} f ^ {\prime} \left(v _ {j}\right) ^ {- 1} g _ {j}, u _ {0} = \theta , \\ v _ {j} = S _ {\theta_ {j}} u _ {j}, \\ \delta_ {j} = \triangle_ {j} f ^ {\prime} \left(v _ {j}\right) ^ {- 1} g _ {j}. \end{array} \right. \tag {1.54}
$$

We are going to prove

$$
\left\| f ^ {\prime} \left(v _ {j}\right) ^ {- 1} g _ {j} \right\| _ {a} \leqslant C _ {1} \| g \| _ {\beta} ^ {\prime} \theta_ {j} ^ {a - \alpha - 1} \quad a \in \left[ a _ {1}, a _ {2} \right], \tag {1.55}
$$

$$
\left\| v _ {j} \right\| _ {a} \leqslant C _ {2} \| g \| _ {\beta} ^ {\prime} \theta_ {j} ^ {a - \alpha} \quad a \in (\alpha , a _ {2} ], \tag {1.56}
$$

$$
\left\| u _ {j} - v _ {j} \right\| _ {a} \leqslant C _ {3} \left\| g \right\| _ {\beta} ^ {\prime} \theta_ {j} ^ {a - \alpha} \quad a \leqslant a _ {2}, \tag {1.57}
$$

inductively.

Suppose (1.56) and (1.57) are true for $j \leqslant k$ and (1.55) holds for $j < k$ , we prove (1.55) for $j = k$ . By the assumption (1) and (1.53):

$$
\begin{array}{l} \| f ^ {\prime} (v _ {k}) ^ {- 1} g _ {k} \| _ {a} \leqslant C (\| g _ {k} \| _ {\beta + a - \alpha} + \| g _ {k} \| _ {0} \| v _ {k} \| _ {\beta + a}) \\ \leqslant C \left(\theta_ {k} ^ {a - \alpha - 1} \| g \| _ {\beta} ^ {\prime} + \theta_ {k} ^ {- \beta - 1} \| g \| _ {\beta} ^ {\prime} \cdot \theta_ {k} ^ {\beta + a - \alpha} \| g \| _ {\beta} ^ {\prime}\right) \\ \leqslant C \theta_ {k} ^ {a - \alpha - 1} \| g \| _ {\beta} ^ {\prime}, \\ \end{array}
$$

if $\| g \| _ { \beta } ^ { \prime }$ is small.

Now we prove (1.57) for $j = k + 1$ , from (9), in the case $a < \alpha$

$$
\begin{array}{l} \| u _ {k + 1} - v _ {k + 1} \| _ {a} = \| u _ {k + 1} - S _ {\theta_ {k + 1}} u _ {k + 1} \| _ {a} \\ \leqslant C \theta_ {k + 1} ^ {a - \alpha} \| u _ {k + 1} \| _ {\alpha} ^ {\prime}. \\ \end{array}
$$

Since

$$
u _ {k + 1} = \sum_ {j = 0} ^ {k} \triangle_ {j} f ^ {\prime} (v _ {j}) ^ {- 1} g _ {j},
$$

and by the definition of $F _ { \alpha } ^ { \prime }$ norm, we have

$$
\left\| u _ {k + 1} \right\| _ {\alpha} ^ {\prime} \leqslant C \| g \| _ {\beta} ^ {\prime}, \forall \alpha \in [ a _ {1}, a _ {2} ].
$$

Thus

$$
\left\| u _ {k + 1} - v _ {k + 1} \right\| _ {a} \leqslant C \theta_ {k + 1} ^ {a - \alpha} \| g \| _ {\beta} ^ {\prime}.
$$

In the case $a = a _ { 2 }$

$$
\begin{array}{l} \left\| u _ {k + 1} - v _ {k + 1} \right\| _ {a _ {2}} \leqslant C \| u _ {k + 1} \| _ {a _ {2}} \\ \leqslant C \sum_ {j = 0} ^ {k} \| \triangle_ {j} f ^ {\prime} (v _ {j}) ^ {- 1} g _ {j} \| _ {a _ {2}} \\ \leqslant C \| g \| _ {\beta} ^ {\prime} \sum_ {j = 0} ^ {k} \triangle_ {j} \theta_ {j} ^ {a _ {2} - \alpha - 1} \\ \leqslant C \theta_ {k + 1} ^ {a _ {2} - \alpha} \| g \| _ {\beta} ^ {\prime}. \\ \end{array}
$$

The other cases, $\alpha \leqslant a \leqslant a _ { 2 }$ , are verified by the interpolation property. Finally, we prove (1.56) for $j = k + 1$ :

$$
\begin{array}{l} \left\| v _ {k + 1} \right\| _ {a} \leqslant \left\| u _ {k + 1} \right\| _ {a} + \left\| v _ {k + 1} - u _ {k + 1} \right\| _ {a} \\ \leqslant 2 C \| g \| _ {\beta} ^ {a - \alpha} \theta_ {k + 1} ^ {a}. \\ \end{array}
$$

Thus the construction of the sequence $u _ { k }$ is possible. And $u _ { k } , v _ { k }$ are all in $B _ { r } ^ { \alpha }$ if $\| g \| _ { \beta } ^ { \prime }$ is small.

Now

$$
\begin{array}{l} f \left(u _ {j + 1}\right) - f \left(u _ {j}\right) = \left(f \left(u _ {j} + \delta_ {j}\right) - f \left(u _ {j}\right) - f ^ {\prime} \left(u _ {j}\right) \delta_ {j}\right) \\ + \left(f ^ {\prime} \left(u _ {j}\right) - f ^ {\prime} \left(v _ {j}\right)\right) \delta_ {j} + \triangle_ {j} g _ {j} \\ = \triangle_ {j} \left(e _ {j} ^ {\prime} + e _ {j} ^ {\prime \prime} + g _ {j}\right) \tag {1.58} \\ \end{array}
$$

where

$$
e _ {j} ^ {\prime} = \triangle_ {j} ^ {- 1} (f (u _ {j + 1}) - f (u _ {j}) - f ^ {\prime} (u _ {j}) (u _ {j + 1} - u _ {j})),
$$

and

$$
e _ {j} ^ {\prime \prime} = \int_ {0} ^ {1} f ^ {\prime \prime} (v _ {j} + t (u _ {j} - v _ {j})) (\triangle_ {j} ^ {- 1} \delta_ {j}, u _ {j} - v _ {j}) d t.
$$

We obtain from the assumption (2), for $n < \alpha < l$ ,

$$
\begin{array}{l} \left\| e _ {j} ^ {\prime \prime} \right\| _ {\beta + \delta} \leqslant C \sum (1 + \| t u _ {j} + (1 - t) v _ {j} \| _ {l}) \| f ^ {\prime} \left(v _ {j}\right) ^ {- 1} g _ {j} \| _ {n} \| u _ {j} - v _ {j} \| _ {n} (1.59) \\ \leqslant C \theta_ {j} ^ {- 1 - \epsilon} \| g \| _ {\beta} ^ {\prime} ^ {2} (1.60) \\ \end{array}
$$

where $\epsilon = 3 \alpha - l - 2 n$ . Similarly by Taylor’s formula,

$$
\left\| e _ {j} ^ {\prime} \right\| _ {\beta + \delta} \leqslant \theta_ {j} ^ {- 1 - \epsilon} \| g \| _ {\beta^ {\prime}} ^ {\prime} ^ {2} \tag {1.61}
$$

Let $\begin{array} { r } { T ( g ) = \sum _ { j } \triangle _ { j } ( e _ { j } ^ { \prime } + e _ { j } ^ { \prime \prime } ) . } \end{array}$ . Then $\| T ( g ) \| _ { \beta + \delta } ^ { \prime } \leqslant C \| g \| _ { \beta } ^ { \prime } { } ^ { 2 }$ . According to (7), and the assumption on the compactness of embedding, $F _ { b }  F _ { a }$ as $b > a$ , we conclude that $T : F _ { \beta } ^ { \prime } \to F _ { \beta } ^ { \prime }$ is compact. According to the recursive formula (1.52), $g$ is uniquely determined by $y$ , therefore $I + T$ is locally injective. Now one can apply the Leray–Schauder invariance of domain theorem (see Chap. 3, Corollary 3.4.12) to conclude that $\forall y \in F _ { \beta } ^ { \prime }$ with small $\| y \| _ { \beta } ^ { \prime }$ , there exists $g \in F _ { \beta } ^ { \prime }$ such that $( I + T ) ( g ) = y$ .

Substituting this $g$ into the iteration scheme (1.54), $u _ { n }$ is convergent to some $u$ in $E _ { a } ^ { \prime }$ , provided by (1.55), and (7). Again, by (1.58), $f ( u ) =$ $( I + T ) ( g ) = y$ . □

# Fixed-Point Theorems

It is well known that the contraction mapping theorem is one of the most important fixed-point theorems in analysis. It is based on the metric of the underlying space. It is simple and is strongly dependent on the chosen metric, but useful. In particular, the unique solution can be computed by iteration. In fact, the implicit function theorem and then the first three Sects. of Chap. 1 are based on it.

The Brouwer fixed theorem (1911), which says that every continuous selfmapping on a closed ball $B ^ { n }$ has a fixed-point, is a fundamental fixed-point theorem in topology. It is based on the notion of retraction. Because of its importance, there are a lot of proofs, roughly speaking, divided into three classes according to methods: (1) Combinatorics (Sperner lemma) by which computing methods are developed. (2) Algebraic topology; the topological degree and other algebraic topological invariants are introduced. (3) Differential topology; the proofs are simple and beautiful (see Dunford and Schwartz [DS], Milnor [Mi 2] etc.). In the study of analysis, we need infinite-dimensional versions of this theorem. A new ingredient – the compactness – is added, while the ball is replaced by its topological equivalent – the convex set. The Schauder fixedpoint theorem and its extensions are all based on convexity and compactness. They are widely used in combining with a priori estimates for solutions in differential equations.

In an ordered space, the Bourbaki–Kneser principle is another basic fixedpoint theorem with applications in analysis, in case compactness is unavailable.

The chapter is organized as follows: The order method is studied in Sect. 2.1. Several fixed-point theorems based on the Bourbaki–Kneser principle are derived, by which, the sub- and super-solutions method in ordered Banach space are developed with applications in PDEs. The convexitycompactness method is developed in detail in Sect. 2.3. We start with the KKM map and the Ky Fan inequality. All other fixed-point theorems, including the Schauder fixed-point theorem and its generalizations, the Nash equilibrium, and the Von Neumann–Sion saddle point theorem, are derived

as consequences. Various applications are studied, in particular, the Ky Fan fixed-point theorem for set valued mappings is applied to free boundary problems. Section 2.4 is devoted to the existence and iteration method for fixed points of nonexpansive maps, which are on the borderline of the contraction mappings. The prototype of the monotone mapping is the subdifferential of a convex function. Due to the special feature of monotonicity, the compactness requirement can be reduced considerably. Since monotone mappings map a Banach space into its dual, one studies the surjectivity of monotone mappings. The main results in this aspect are due to Minty [Min] and Browder [Bd 3,4]. Applications to variational inequalities and quasi-linear elliptic problems are studied as well. We shall show how the monotonicity is applied instead of the compactness in the existence theory. These are the contents of Sects. 2.5 and 2.6. Convex sets and convex functions will be used form time to time; we collect their most important properties in Sect. 2.2.

# 2.1 Order Method

A kind of nonlinear operator defined on ordered spaces is order preserving; this special feature makes the fixed-point problem easy to handle.

A set $E$ is said to be ordered if a partial order $\leqslant$ is defined by the following axioms: $\forall x , y , z \in E$

(i) $x \leqslant x$   
(ii) $x \leqslant y$ , and $y \leqslant x$ imply $x = y$   
(iii) $x \leqslant y$ , and $y \leqslant z$ imply $x \leqslant z$ .

A chain (or totally ordered set) $E$ is an ordered set, on which $\forall x , y \in E$ either $x \leqslant y$ or $y \leqslant x$ .

The following terminologies are introduced: The smallest (greatest) element $\underline { { x } }$ ( $\scriptstyle { \overline { { x } } }$ resp.): $\underline { { x } } \leqslant x$ ( $x \leqslant { \overline { { x } } }$ ), $\forall x \in E$ ; the minimum (maximum) element $x _ { * }$ ( $x ^ { * }$ resp.): if $x \leqslant x \ast$ (x∗  x), x E then $x _ { * } = x$ $x ^ { * } = x$ ); the lower (upper) bound of a subset $F \subset E$ : $\underline { { a } } \in E$ ${ \overline { { a } } } \in E$ ) and $\underline { { a } } \leqslant x$ (x  a) $\forall x \in F$ ; the infimum (supremum) of $F$ inf $F$ ( $\operatorname { s u p } F$ respectively) is the greatest (smallest) element of the subset, which consists of all lower (upper) bounds of $F$ .

$\forall a \in E$ , we denote

$$
S _ {+} (a) = \left\{x \in E \mid a \leqslant x \right\},
$$

and call it the right section of $a$ . Similarly we define the left section $S _ { - } ( a )$ . Set

$$
[ a, b ] = S _ {+} (a) \cap S _ {-} (b)
$$

for $a \leqslant b$ , we call it an order interval.

Zorn’s lemma, which is equivalent to the Zermelo selection axiom, is our starting point: In a nonempty ordered set $( E , \leq )$ , if every chain has an upper bound in $E$ , then the set has a maximum element.

Let $( E , \leqslant )$ and $( F , \leqslant )$ be two ordered sets. A map $f : E \to F$ is called (strict) order preserving, if $x \leqslant ( < ) y$ implies $f ( x ) \leqslant ( < ) f ( y )$ . Similarly, we define the (strict) order reversing map.

By definition, if $f : E \to F$ is order preserving, and $a \leqslant f ( a )$ $( f ( a ) \leqslant a )$ , then $f ( S _ { \pm } ( a ) ) \subset S _ { \pm } ( a )$ . Thus $a \leqslant f ( a )$ and $f ( b ) \leqslant b$ imply $f ( [ a , b ] ) \subset [ a , b ]$ .

Theorem 2.1.1 (Bourbaki–Kneser principle) Let $( E , \leqslant )$ be an ordered set, in which every chain has an upper bound. If $f : E \to E$ satisfies $x \leqslant f ( x ) \forall x \in$ $E$ , then $f$ has a fixed point.

Proof. By Zorn’s lemma, $E$ has a maximum element $a$ . Since $a \leqslant f ( a )$ , and $a$ is maximal, $a = f ( a )$ . □

To a self-map $f$ on $E$ , an element $x \in E$ , satisfying $x \leqslant f ( x )$ $f ( x ) \leqslant x )$ ), is called a sub-(or super-) solution of $f$ .

The Bourbaki–Kneser fixed-point theorem is a general principle. Many important fixed point theorems can be derived from it.

Theorem 2.1.2 (Caristi) Suppose that $( E , \rho )$ is a complete metric space and that $\varphi : E \to \mathbb { R } ^ { 1 }$ is a (l.s.c.) function bounded from below. Assume $f : E \to E$ is a mapping satisfying

$$
\rho (x, f (x)) \leqslant \varphi (x) - \varphi (f (x)), \tag {2.1}
$$

then $f$ has a fixed point.

Proof. (1) An order structure on $E$ is introduced by $\varphi$ :

$$
\forall x, y \in E x \leqslant y \text {i f f} \rho (x, y) \leqslant \varphi (x) - \varphi (y).
$$

It is easy to verify that $( E , \leqslant )$ is an ordered set.

(2) It is sufficient to verify that every chain $X$ has an upper bound. By definition, $\varphi$ is a monotone decreasing function on $X \colon x \leqslant y \Longrightarrow \varphi ( y ) \leqslant$ $\varphi ( x ) \ \forall x , y \in X$ . Let $c = \operatorname* { i n f } _ { X } \varphi$ ; we can find an increasing sequence $x _ { 1 } \leqslant x _ { 2 } \leqslant$ $\cdots \leq x _ { n } \leq \cdots$ in $X$ such that $\varphi ( x _ { n } ) \to c$ .

We conclude that there is at most one $a \in X$ such that $x _ { n } \leqslant a \forall n$ . Indeed, if $\exists a _ { 1 } , a _ { 2 } \in X$ satisfy $x _ { n } \leqslant a _ { i } , i = 1 , 2$ , then

$$
c = \varphi (a _ {1}) = \varphi (a _ {2}), \text {b e c a u s e} c \leqslant \varphi (a _ {i}) \leqslant \varphi (x _ {n}) \forall n.
$$

But $X$ is a chain, either $a _ { 1 } \leqslant a _ { 2 }$ or $a _ { 2 } \leqslant a _ { 1 }$ , it follows that

$$
\rho \left(a _ {1}, a _ {2}\right) \leqslant \left| \varphi \left(a _ {1}\right) - \varphi \left(a _ {2}\right) \right| = 0,
$$

i.e., $a _ { 1 } = a _ { 2 }$ .

Noticing that $\{ x _ { n } \}$ is a Cauchy sequence:

$$
\rho \left(x _ {n + m}, x _ {n}\right) \leqslant \varphi \left(x _ {n}\right) - \varphi \left(x _ {n + m}\right) \quad \forall n, \forall m,
$$

there exists a limit $b \in E$ . We conclude that $X$ has an upper bound. In fact, from the l.s.c. of $\varphi$ , one has $x _ { n } \leqslant b \forall n$ . Now, $\forall x \in X$ , if $\exists n \in \mathbb { N }$ , such that $x \leqslant x _ { n }$ , then $x \leqslant b$ , so $b$ is an upper bound, otherwise, $x$ itself is the unique upper bound in $X$ .

(3) The condition (2.1) on $f$ means $x \leqslant f ( x ) \forall x \in E$ . Applying Theorem 2.1.1, $f$ has a fixed point. □

It is worth noting that in the Caristi fixed-point theorem, there is no continuity assumption on the map $f$ . It has many applications.

The following version of the Ekeland variational principle is often used.

Theorem 2.1.3 (Ekeland) Suppose that $( E , \rho )$ is a complete metric space and that $\varphi : E \to \mathbb { R } \cup \{ + \infty \}$ is l.s.c. and bounded from below. $\forall \varepsilon > 0$ if $x \in E$ satisfies $\varphi ( x ) \leqslant \operatorname* { i n f } _ { E } \varphi + \varepsilon$ , then $\forall \lambda > 0$ $\exists y _ { \lambda } \in E$ satisfying

$$
\varphi \left(y _ {\lambda}\right) \leqslant \varphi (x) \tag {2.2}
$$

$$
\rho (x, y _ {\lambda}) \leqslant \frac {1}{\lambda} \tag {2.3}
$$

$$
\varphi (y) > \varphi \left(y _ {\lambda}\right) - \varepsilon \lambda \rho (y, y _ {\lambda}) \quad \forall y \in E \backslash \left\{y _ {\lambda} \right\}. \tag {2.4}
$$

Proof. Define a subset of $E$ :

$$
E _ {1} = \left\{y \in E \mid \varphi (y) + \lambda \varepsilon \rho (x, y) \leqslant \varphi (x) \right\},
$$

$E _ { 1 }$ is closed ( $\varphi$ is l.s.c.) and nonempty $x \in E _ { 1 }$ ), so is complete. We claim that $\exists y _ { \lambda } \in E _ { 1 }$ such that

$$
\varphi (y) > \varphi (y _ {\lambda}) - \lambda \varepsilon \rho (y, y _ {\lambda}) \quad \forall y \in E _ {1} \backslash \{y _ {\lambda} \}.
$$

Indeed, if not, $\exists \lambda > 0 , \forall y _ { \lambda } \in E _ { 1 } , \exists y \in E _ { 1 } \backslash \{ y _ { \lambda } \}$ such that

$$
\varphi (y) \leqslant \varphi (y _ {\lambda}) - \lambda \varepsilon \rho (y, y _ {\lambda}).
$$

Define $f ( y _ { \lambda } ) = y \neq y _ { \lambda }$ , then $f : E _ { 1 } \to E _ { 1 }$ satisfying $\lambda \varepsilon \rho ( y _ { \lambda } , f ( y _ { \lambda } ) ) \leqslant \varphi ( y _ { \lambda } ) -$ $\varphi ( f ( y _ { \lambda } ) )$ . According to the Caristi theorem, $f$ has a fixed point; this is a contradiction.

Since (2.2) and (2.3) are trivially true, it remains to verify (2.4) for $y \in$ $E \backslash E _ { 1 }$ . In fact, if

$$
\varphi (y) \leqslant \varphi (y _ {\lambda}) - \lambda \varepsilon \rho (y, y _ {\lambda}) \text {a n d} y \in E \backslash E _ {1},
$$

then

$$
\begin{array}{l} \varphi (y) \leqslant \varphi (x) - \lambda \varepsilon \rho (x, y _ {\lambda}) - \lambda \varepsilon \rho (y, y _ {\lambda}) \\ \leqslant \varphi (x) - \lambda \varepsilon \rho (x, y). \\ \end{array}
$$

It follows that $y \in E _ { 1 }$ . This is a contradiction.

□

Theorem 2.1.4 (Amann) Assume that every chain of $( E , \leqslant )$ has a supremum, and that $f : E \to E$ is order-preserving mapping. If $f$ has a subsolution $a \leqslant f ( a )$ then $f$ has a least fixed point in $S _ { + } ( a )$ .

Proof. (1) We verify the existence of a fixed point. Define $E _ { + } = \{ x \in E | x \leqslant$ $f ( x ) \} \cap S _ { + } ( a )$ . According to the Bourbaki–Kneser principle, we only want to show: (i) $E _ { + } \neq \emptyset$ , (ii) $f ( E _ { + } ) \subset E _ { + }$ , (iii) every chain of $E _ { + }$ has an upper bound in $E _ { + }$ .

Indeed, $a \in E _ { + }$ ; (i) follows.

Since $f$ is order preserving, $f ( E _ { + } ) \subset E _ { + }$

Let $X$ be a chain of $E _ { + }$ . By assumption, it has a supremum $b \in E$ . From $x \leqslant f ( x ) \leqslant f ( b ) \forall x \in X$ , we have $b \leqslant f ( b )$ , and then $b \in E _ { + }$ , i.e., $b$ is an upper bound of $X$ in $E _ { + }$ .

(2) Let $\operatorname { F i x } _ { S + ( a ) } ( f )$ be the fixed point set of $f$ in $S _ { + } ( a )$ . Define

$$
G _ {+} = \left\{y \in E _ {+} \mid y \leqslant z \forall z \in \operatorname {F i x} _ {S _ {+} (a)} (f) \right\}.
$$

We claim that $\operatorname { F i x } _ { G _ { + } } ( f ) \neq \varnothing$ , then $\forall y _ { 0 } \in \operatorname { F i x } _ { G _ { + } } ( f )$ , $y _ { 0 }$ is the least fixed point in $S _ { + } ( a )$ . To this end, from the Bourbaki–Kneser principle, it is sufficient to verify: (i) $G _ { + } \neq \emptyset$ , (ii) $f ( G _ { + } ) \subset G _ { + }$ , (iii) every chain of $G _ { + }$ has an upper bound in $G _ { + }$ . In fact, $a \in G _ { + }$ ; (i) follows. $\forall y \in G _ { + } , \forall z \in \operatorname { F i x } _ { S _ { + } ( a ) } ( f ) , f ( y ) \leqslant$ $f ( z ) = z$ , which implies $f ( y ) \in G _ { + }$ , and then (ii) follows.

Let $X$ be a chain of $G _ { + }$ , by assumption, it has a supremum $b \in E$ , so is $b \in E _ { + }$ as in (1) and $b \leqslant z \forall z \in \operatorname { F i x } _ { S _ { + } ( a ) } ( f )$ . i.e., $b \in G _ { + }$ . This is an upper bound of $X$ in $G _ { + }$ . □

An ordered set is called chain complete if every chain has an infimum and a supremum.

Corollary 2.1.5 Let $E$ be a chain complete ordered set, and let $f : E \to E$ be order preserving. Suppose that $\exists$ a pair of sub- and super- solutions $\underline { { a } } \leqslant \overline { { a } }$ . Then $f$ has at least a least and a greatest fixed point in $[ \underline { { a } } , \overline { { a } } ]$ .

An ordered set $E$ is called a lattice, if for every pair $x , y \in E , \ x \vee y =$ $\operatorname* { s u p } \{ x , y \}$ and $x \wedge y = \operatorname* { i n f } \{ x , y \}$ are in $E$ .

A lattice is called complete if every nonempty subset possesses an infimum and a supremum.

Corollary 2.1.6 (Birkhoff–Tarski) Let $E$ be a complete lattice and let $f$ be an order-preserving self-map on $E$ , then there exist a smallest and a greatest fixed point of $f$ .

Proof. Let $\underline { { a } } = \operatorname { i n f } E$ and ${ \overline { { a } } } = \operatorname* { s u p } E$ , then $\underline { { \boldsymbol { a } } }$ and $a$ are sub- and super- solutions of $f$ respectively, satisfying $\underline { { a } } \leqslant \overline { { a } }$ . □

We study chain complete ordered sets.

Let $X$ be a Banach space, and let $\mathbb { P } \subset X$ be a nonempty closed convex positive cone with ${ \mathbb { P } } \cap ( - { \mathbb { P } } ) = \{ \theta \}$ . It reduces a partial order structure on $X$ :

$$
x \leqslant y \text {i f a n d o n l y i f} y - x \in \mathbb {P}.
$$

The order defined above matches the linear structure and the topology on $X$ , but not necessarily the magnitude of the norm.

$$
x _ {i} \leqslant y _ {i}, i = 1, 2 \Longrightarrow x _ {1} + x _ {2} \leqslant y _ {1} + y _ {2},
$$

$$
x \leqslant y, \lambda \geqslant 0 \Longrightarrow \lambda x \leqslant \lambda y,
$$

and

$$
x _ {n} \leqslant y _ {n}, x _ {n} \rightarrow x, y _ {n} \rightarrow y \Longrightarrow x \leqslant y.
$$

A Banach space $X$ with a closed convex positive cone $\mathbb { P }$ , induces an ordered Banach space (OBS) $( X , \mathbb { P } )$ . In particular $\forall a \in X$ , $S _ { \pm } ( a )$ is closed.

Lemma 2.1.7 Every compact subset $E$ of an OBS is chain complete.

Proof. For every chain $X \subset E$ , we want to show that it has a supremum. In fact $\forall x \in X$ , the family of sets: $\{ S _ { + } ( x ) \mid x \in X \}$ is a family of closed subsets with finite intersection property, i.e., $\forall x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \in X$ , $\bigcap _ { i = 1 } ^ { n } S _ { + } ( x _ { i } ) =$ $S _ { + } ( x ^ { * } )$ , where $x ^ { * } = \operatorname* { s u p } \{ x _ { 1 } , \dots , x _ { n } \} \in X$ , provided that $X$ is a chain. Since $\overline { { X } }$ is compact, $\cap _ { x \in X } S _ { + } ( x ) \cap X \neq \emptyset$ . Let $\textstyle { \overline { { x } } }$ be an element in the intersection. Then (1) $x \leqslant \overline { { x } } \forall x \in X$ , i.e., $x$ is an upper bound of $X$ , and (2) $\forall$ upper bound $d$ of $X$ , from $x \leq d$ and ${ \bar { x } } \in { \overline { { X } } }$ , it follows $\bar { x } \leqslant d$ . Thus $x$ is a supremum. Similarly, one proves that it has an infimum. □

A positive cone $\mathbb { P }$ is called normal if every ordered interval $[ a , b ]$ in $X$ is bounded in the norm.

Lemma 2.1.8 $\mathbb { P }$ is normal if and only if $\Finv$ a constant $C > 0$ such that $0 \leqslant$ $x \leqslant y \Longrightarrow \| \ x \| \leqslant C \| \ y \|$ .

Proof. “=⇒” If not, $\exists \theta \leqslant x _ { n } \leqslant y _ { n }$ such that $\parallel \perp \parallel \geqslant n ^ { 3 } \parallel \} _ { n } \parallel \neq = \parallel \pmb { n }$ $1 , 2 , \ldots$ Let $\begin{array} { r } { z _ { n } ~ = ~ \frac { x _ { n } } { n ^ { 2 } \| y _ { n } \| } } \end{array}$ x , then $\Vert { \textbf { \em z } } _ { n } \parallel \geqslant n$ . But $\theta \leqslant z _ { n } \leqslant \frac { y _ { n } } { n ^ { 2 } \| y _ { n } \| }$ . Define $\begin{array} { r } { y = \sum _ { n = 1 } ^ { \infty } \frac { 1 } { n ^ { 2 } } \frac { y _ { n } } { \left\| y _ { n } \right\| } } \end{array}$ ; we have $\theta \leqslant z _ { n } \leqslant y$ , i.e., $z _ { n } \in [ \theta , y ]$ . But $z _ { n }$ is unbounded. A contradiction.

$\mathrm { ~ \because ~ } \forall x \in [ a , b ]$ , we have $\theta \leqslant x - a \leqslant b - a$ . Since

$$
\parallel x - a \parallel \leqslant C \parallel b - a \parallel ,
$$

it follows that

$$
\| x \| \leqslant \| x - a \| + \| a \| \leqslant \| a \| + C \| b - a \|.
$$

□

Corollary 2.1.9 Suppose that $( X , \mathbb { P } )$ is a normal OBS, and that $\boldsymbol { D } = [ a , b ]$ is an order interval. If $f$ is an order-preserving compact self-mapping on $D$ , then it has a smallest and a greatest fixed point.

Proof. By normality, $D$ is bounded in the norm. Let $E = f ( D )$ . $E$ is a compact subset of an OBS, provided by the compactness of $f$ . Since $f$ is a self-mapping on $D$ , $a \leq f ( a ) , f ( b ) \leq b$ , so $a < b$ is a pair of sub- and super-solutions of $f$ . The conclusion follows from Corollary 2.1.5 and Lemma 2.1.7. □

Noticing that a bounded closed convex set of a reflexive Banach space is weakly compact, when we make use of the weak topology, Corollary 2.1.9 can be modified as follows:

Corollary 2.1.10 Suppose that $( X , \mathbb { P } )$ is a reflexive normal OBS, and that $D = [ a , b ]$ is an order interval. If $f$ is an order-preserving self-mapping on $D$ , then $f$ has a smallest and a greatest fixed point.

Example 1. Let $M$ be a compact space, and let $X = C ( M )$ with norm

$$
\parallel x \parallel = \max  _ {\xi \in M} | x (\xi) |.
$$

Define $P = \{ x \in X | x ( \xi ) \geqslant 0 \}$ . Then $P$ is normal and $( X , P )$ is an OBS.

Example 2. Let $( \Omega , B , \mu )$ be a measure space and let $L ^ { \boldsymbol { p } } ( \Omega , B , \mu )$ , $1 \leqslant p \leqslant \infty$ be the $L ^ { p }$ space over $( \Omega , B , \mu )$ . Define $P = \left\{ x \in L ^ { p } ( \Omega , B , \mu ) | x ( \xi ) \geqslant 0 \right.$ a.e. $\xi \in \Omega \}$ . Then $( X , P )$ is an OBS, and $P$ is normal.

Example 3. Let $X = C ^ { 1 } ( [ 0 , 1 ] )$ with norm

$$
\parallel x \parallel = \max  _ {t \in [ 0, 1 ]} | x (t) | + \max  _ {t \in [ 0, 1 ]} | x ^ {\prime} (t) |,
$$

and $P = \{ x \in X | x ( t ) \geqslant 0 \forall t \in [ 0 , 1 ] \}$ . Then $( X , P )$ is an OBS, but $P$ is not normal.

Corollaries 2.1.9 and 2.1.10 are the foundation of the sub- and supersolutions method in the theory of differential equations.

Example 4. Consider the following semi-linear elliptic BVP:

$$
\left\{ \begin{array}{l} - \triangle u (x) = \varphi (x, u (x)) \quad \text {i n} \Omega , \\ u | _ {\partial \Omega} = 0 \end{array} \right. \tag {2.5}
$$

where $\Omega$ is a bounded domain with smooth boundary in $\mathbb { R } ^ { n }$ , and $\varphi : \overline { { \Omega } } \times$ $\mathbb { R } ^ { 1 } \to \mathbb { R } ^ { 1 }$ is continuous. Assume that $\forall x \in \Omega$ , the function $t \mapsto \varphi ( x , t )$ is nondecreasing. Define $\mathbb { K } = ( - \triangle ) ^ { - 1 }$ with 0-Dirichlet condition; it is known that $\mathbb { K }$ is a positive linear operator from the maximum principle. Let $F$ be the mapping $u ( x ) \mapsto \varphi ( x , u ( x ) )$ . Since $\varphi$ is nondecreasing in $u$ , the map $F$ is order preserving.

Setting $X = C _ { 0 } ( \Omega )$ , we consider the following mappings:

$$
C _ {0} (\overline {{\Omega}}) \stackrel {F} {\rightarrow} C _ {0} (\overline {{\Omega}}) \stackrel {i} {\rightarrow} L ^ {p} (\Omega) \stackrel {\mathbb {K}} {\rightarrow} W _ {p} ^ {2} \cap \stackrel {\circ} {W} _ {p} ^ {1} (\Omega) \stackrel {j} {\rightarrow} C _ {0} (\overline {{\Omega}}),
$$

where $i$ and $j$ are injections, and $p > \frac { \pi } { 2 }$ . The later is compact, provided by the Sobolev embedding theorem. Define $f = j \circ \mathbb { K o } i \circ F : C _ { 0 } ( \Omega )  C _ { 0 } ( \Omega )$ ; the problem (2.5) is equivalent to finding the fixed point of $f$ . Since $f$ is compact and order preserving, it has a fixed point if one can find a sub-solution $\underline { { u } }$ and 。 super-solution $\bar { u }$ of (2.5) in $W _ { p } ^ { 2 } \cap W _ { p } ^ { 1 } ( \Omega )$ . i.e., $u \leqslant \overline { { u } }$ satisfying

$$
- \triangle \underline {{u}} \leqslant \varphi (x, \underline {{u}} (x)) \text {a n d} - \triangle \bar {u} \geqslant \varphi (x, \bar {u} (x)) \text {a . e . i n} \Omega .
$$

Namely, we have proved:

Statement 2.1.11 Suppose that $\varphi : \Omega \times \mathbb { R } ^ { 1 } \to \mathbb { R } ^ { 1 }$ is continuous and $\forall x \in$ $\Omega , \ t \mapsto \varphi ( x , t )$ is nondecreasing. If there is a pair of sub- and super-solutions $u \leqslant \overline { { u } }$ in $W _ { p } ^ { 2 } \cap W _ { p } ^ { 1 } ( \Omega ) , \ \frac { n } { 2 } < p < \infty$ , then equation (2.5) has the smallest and 。 the greatest solutions in the order interval $[ \underline { { u } } , \overline { { u } } ] \subset W _ { p } ^ { 2 } \cap W _ { p } ^ { 1 } ( \Omega )$ .

Remark 2.1.12 The nondecreasing condition on $\varphi$ can be weakened as follows: $\exists \omega \geqslant 0$ , such that $\forall x \in \overline { { \Omega } }$ , the function:

$$
\varphi_ {\omega} (x, t) = \varphi (x, t) + \omega t
$$

is nondecreasing in $t$ .

Indeed, instead of $\mathbb { K }$ and $F$ , we introduce the linear positive operator $\mathbb { K } _ { \omega } =$ $( - \triangle + \omega I ) ^ { - 1 }$ and the order-preserving mapping $F _ { \omega } : u ( x ) \mapsto \varphi _ { \omega } ( x , u ( x ) )$ . Then we find the fixed point of $f _ { \omega } = j \circ \mathbb { K } _ { \omega } \circ i \circ F _ { \omega }$ .

Moreover, the continuity condition on $\varphi$ can be replaced by the Caratheodory condition. In this case, we consider the map $f : L ^ { p } ( \Omega ) \to L ^ { p } ( \Omega )$ , with $f = i \circ j \circ \circ K \circ F$ instead, where $F$ is the Nemytski operator.

Remark 2.1.13 The sub- and super-solutions method has an advantage in numerical analysis, because one can get the smallest and the greatest solutions in the order interval $[ \underline { { u } } , \overline { { u } } ]$ by iteration. Setting

$$
\begin{array}{l} \underline {{u}} _ {i + 1} = f (\underline {{u}} _ {i}), \overline {{u}} _ {i + 1} = f (\overline {{u}} _ {i}) \quad i = 0, 1, 2, \dots , \\ \underline {{u}} _ {0} = \underline {{u}}, \quad \overline {{u}} _ {0} = \overline {{u}}. \\ \end{array}
$$

Then one has

$$
\underline {{u}} _ {0} \leqslant \underline {{u}} _ {1} \leqslant \dots \leqslant \underline {{u}} _ {n} \leqslant \overline {{u}} _ {n} \leqslant \dots \leqslant \overline {{u}} _ {1} \leqslant \overline {{u}} _ {0}.
$$

The sequences $\{ \underline { { u } } _ { n } \}$ and $\overline { { u } } _ { n }$ converge to the least solution $u _ { * }$ and the greatest solution $u ^ { * }$ respectively. If $u _ { * } = u ^ { * }$ , then this is the unique solution in $[ \underline { { u } } , \overline { { u } } ]$ ; and the two sequences provide lower and upper bounds estimates for $u _ { * } = u ^ { * }$ .

Remark 2.1.14 In contrast to Corollaries 2.1.9 and 2.1.10, we consider an order-reversing compact mapping $f$ on a normal OBS $( X , \mathbb { P } )$ . Let $a \in X$ be $a$

sub- (or super-) solution of both $f$ and $f ^ { 2 }$ . Setting $x _ { i + 1 } = f ( x _ { i } ) , i = 0 , 1 , 2 , . . .$ , and $x _ { 0 } = a$ , we have either

$$
x _ {0} \leqslant x _ {2} \leqslant x _ {4} \leqslant \dots \quad \dots \leqslant x _ {3} \leqslant x _ {1} \quad i f a \leqslant f ^ {2} (a) \leqslant f (a),
$$

or

$$
x _ {1} \leqslant x _ {3} \leqslant x _ {5} \leqslant \dots \quad \dots \leqslant x _ {2} \leqslant x _ {0} \quad i f f (a) \leqslant f ^ {2} (a) \leqslant a, r e s p e c t i v e l y.
$$

Thus

$$
x _ {2 n} \nearrow \underline {{x}} a n d x _ {2 n + 1} \searrow \bar {x} i f a \leqslant f (a),
$$

$$
(x _ {2 n + 1} \nearrow \underline {{x}} a n d x _ {2 n} \searrow \bar {x} i f f (a) \leqslant a r e s p e c t i v e l y.)
$$

If the fixed-point set of $f$ , $\operatorname { F i x } ( f ) \cap S _ { + } ( a )$ (or $\operatorname { F i x } ( f ) \cap S _ { - } ( a ) )$ is not empty, then it is contained in $[ \underline { { x } } , \overline { { x } } ]$ .

Example 5. Let $( M , g )$ be a compact Riemannian manifold without boundary, and let

$$
\varDelta_ {M} = \frac {1}{\sqrt {\det (g)}} \sum_ {i, j = 1} ^ {2} \partial_ {i} (g ^ {i j} \sqrt {\det (g)} \partial_ {j})
$$

be the Laplace–Beltrami operator with respect to $g$ . Assume the Gaussian curvature $k ( x ) < 0 \forall x \in M$ . Given a function $K ( x ) < 0$ on $M$ , we consider the following equation:

$$
\triangle_ {M} u (x) - k (x) + K (x) e ^ {2 u (x)} = 0 \quad x \in M. \tag {2.6}
$$

This equation arises from geometry. The solution $u$ defines a conformal metric $\widetilde g = e ^ { 2 u } g$ . Under the new metric $\widetilde g$ , $K ( x )$ is the Gaussian curvature of $M$ .

We construct a pair of sub- and super-solutions: Choose $C > 0$ sufficiently large such that

$$
- k (x) + K (x) e ^ {2 C} \leqslant 0 \leqslant - k (x) + K (x) e ^ {- 2 C} \quad \forall x \in M.
$$

Then $( - C , C )$ is a pair of sub- and super-solutions.

Statement 2.1.15 Suppose that $K , k \in { \cal C } ( M )$ satisfy $K ( x ) \ < \ 0$ , $k ( x ) \ <$ $0 \forall x \in M$ , then equation (2.6) has a unique solution.

Proof. Since $\varphi ( x , u ) = - k ( x ) + K ( x ) e ^ { 2 u }$ is decreasing in $u$ , one cannot apply Statement 2.1.11 directly. Assume K = max( K(x)) and set $K = \operatorname* { m a x } _ { x \in M } ( - K ( x ) )$ $\omega = 2 K e ^ { 2 C }$ , then the function $t \mapsto \omega t + K ( x ) e ^ { 2 t }$ is nondecreasing. After Remark 2.1.13, Statement 2.1.11 is applied, and we conclude that there is a smallest and a greatest solution of (2.6) in the order interval $[ - C , C ]$ of $C ( M )$ for any sufficiently large constant $C > 0$ .

However, since $K ( x ) < 0$ and $e ^ { 2 u }$ is increasing in $u$ , from the maximum principle, the solution is unique. References can be found in Kazdan [Ka], Kazdan and Warner [KW 1][KW 2] and Aubin [Au 1]. □

Example 6. Consider the following nonlinear heat equation:

$$
\left\{ \begin{array}{l l} \frac {\partial u}{\partial t} - \triangle u = \lambda u - a u ^ {k} & \text {i n} \Omega \times \mathbb {R} _ {+} ^ {1} \\ u = 0 & \text {o n} \partial \Omega \times \mathbb {R} _ {+} ^ {1} \\ u (\cdot , 0) = \varphi & \text {o n} \Omega \times \{0 \} \end{array} \right. \tag {2.7}
$$

where $\lambda > 0 , a > 1 , k > 1$ and $\varphi \in W _ { p } ^ { 2 } ( \Omega )$ is nonnegative, for some $p > \frac { \pi } { 2 }$ . Notice that one can find a large constant $M > 0$ such that $\lambda M - a M ^ { k } < 0$ and $\operatorname* { m a x } \varphi \leqslant M$ . Since $\exists \omega > 0$ such that $( \lambda + \omega ) u - a u ^ { k }$ is increasing in the interval $[ 0 , M ]$ , one can use the sub- and super-solution method to prove the existence of a positive solution of (2.7).

In fact, $\forall T > 0$ define a mapping $A : g \mapsto v = A g$ , $L ^ { p } ( \Omega \times [ 0 , T ] ) \to$ $W _ { p } ^ { 2 , 1 } ( \Omega \times [ 0 , T ] ) \hookrightarrow C ( \overline { { \Omega } } \times [ 0 , T ] )$ as follows:

$$
\left\{ \begin{array}{l l} \frac {\partial v}{\partial t} - \triangle v + \omega v = g & \text {i n} \Omega \times [ 0, T ] \\ v = 0 & \text {o n} \partial \Omega \times [ 0, T ] \\ v (\cdot , 0) = \varphi & \text {o n} \Omega \times \{0 \}  . \end{array} \right.
$$

From the maximum principle for heat equations, $A$ is order preserving. Define $F ( u ) \ : = \ : ( \lambda + \omega ) u - a u ^ { k }$ ; again $F$ is order-preserving in the order interval $D = [ \theta , M ]$ of the OBS $C ( { \overline { { \Omega } } } \times [ 0 , T ] )$ . Setting $f = A \circ F \circ i$ , where $_ i$ is the injection $C ( \Omega \times [ 0 , T ] )  L ^ { p } ( \Omega \times [ 0 , T ] )$ , $f$ is a compact order-preserving map on $D$ . Now $\theta \leqslant f ( \theta )$ , and $f ( M ) \leqslant M .$ From the parabolic maximum principle, we have a fixed point $u = f ( u )$ in $[ \theta , M ]$ . This is a solution of (2.7).

The sub- and super-solutions method is extensively used in the study of nonlinear differential equations. The analytic basis is the maximum principle. As to systems, the method is also applicable if the associated maximum principle is extended to the linearized operator. However, the construction of a pair of sub- and super-solutions is technical, but crucial. It depends on the special feature of the nonlinear term.

# 2.2 Convex Function and Its Subdifferentials

# 2.2.1 Convex Functions

Let $X$ be a vector space, a function $f : X \to \mathbb { R } \cup \{ + \infty \}$ is convex, if $\forall x , y \in$ $X$ , $\forall \lambda \in [ 0 , 1 ]$ , we have

$$
f (\lambda x + (1 - \lambda) y) \leqslant \lambda f (x) + (1 - \lambda) f (y).
$$

It is called concave, if $- f$ is convex. $f$ is called affine if $f$ is both convex and concave. The domain, the epigraph and the level set of $f$ are defined as follows:

$$
\begin{array}{l} \operatorname {d o m} (f) = \left\{x \in X \mid f (x) <   + \infty \right\}, \\ \operatorname {e p i} (f) = \left\{\left(x, \lambda\right) \in X \times \mathbb {R} ^ {1} \mid f (x) \leqslant \lambda \right\}, \\ f _ {a} = \{x \in X | f (x) \leqslant a \} \forall a \in \mathbb {R} ^ {1}. \\ \end{array}
$$

$f$ is said to be proper if $f \not \equiv + \infty$ .

By definition,

$$
\begin{array}{l} f \text {i s} \quad \Leftrightarrow \forall x _ {1}, \dots , x _ {n} \in X, \forall \lambda_ {1}, \dots , \lambda_ {n} \geqslant 0, \sum_ {i = 1} ^ {n} \lambda_ {i} = 1, \\ f \left(\sum_ {i = 1} ^ {n} \lambda_ {i} x _ {i}\right) \leqslant \sum_ {i = 1} ^ {n} \lambda_ {i} f (x _ {i}) \Longleftrightarrow \operatorname {e p i} (f) \text {i s c o n v e x}. \\ \end{array}
$$

Also $f$ is $\mathrm { c o n v e x } \Longrightarrow \forall a \in \mathbb { R } ^ { 1 }$ , $f _ { a }$ is convex, but the converse is not true.

A function $f$ is called quasi convex (quasi concave) if $f _ { a }$ is convex (concave resp.) $\forall a \in \mathbb { R } ^ { 1 }$ .

For a subset $C$ of $X$ , the indicator function is defined to be

$$
\chi_ {C} (x) = \left\{ \begin{array}{l l} 0 & \text {i f} x \in C \\ + \infty & \text {i f} x \notin C  . \end{array} \right.
$$

Thus

$$
C \text {i s} \quad C \text {i s} \quad C \text {i s} \quad C \text {i s} \quad C \text {i s} \quad C \text {i s} \quad C \text {i s}
$$

The following simple propositions hold:

(1) If $f$ and $g$ are convex, then $\forall \alpha , \beta \geqslant 0 \alpha f + \beta g$ is convex.   
(2) If $A$ is a linear mapping: $X  Y$ and if $f : Y \to \mathbb { R } \cup \{ + \infty \}$ is convex, then $f \circ A : X \to \mathbb { R } \cup \{ + \infty \}$ is convex.   
(3) If $\{ f _ { \iota } | \ \iota \in \wedge \}$ is a set of convex functions, then $\operatorname* { s u p } \{ f _ { \iota } | \ \iota \in \wedge \}$ is convex.

We study the continuity of convex functions on Banach spaces.

Theorem 2.2.1 Suppose that $f : X \to \mathbb { R } \cup \{ + \infty \}$ is a convex function on a Banach space $X$ . If $f$ is bounded from above in a neighborhood of a point $x$ , then $f$ is continuous at $x$ . Moreover, $f$ is locally Lipschitzian in $\operatorname { i n t } ( \operatorname { d o m } ( f ) )$ .

Proof. (1) We may assume $x = \theta$ and $f ( \theta ) = 0$ . From the assumption, $\exists \eta >$ $0 , \beth M > 0$ such that

$$
f (y) \leqslant M \quad \forall y \in B _ {\eta} (\theta). \tag {2.8}
$$

On one hand, $\forall z \in B _ { \eta } ( \theta ) \backslash \{ \theta \}$ , we have

$$
f (z) = f \left(\frac {\| z \|}{\eta} \left(\frac {\eta z}{\| z \|}\right)\right) \leqslant \frac {\| z \|}{\eta} f \left(\frac {\eta z}{\| z \|}\right) \leqslant \frac {M}{\eta} \| z \| . \tag {2.9}
$$

One the other hand, $\forall z \in X$

$$
\begin{array}{l} 0 = f (\theta) = f \left(\frac {\eta}{\eta + \| z \|} z + \frac {\| z \|}{\eta + \| z \|} \left(- \frac {\eta}{\| z \|} z\right)\right) \\ \leqslant \frac {\eta}{\eta + \| z \|} f (z) + \frac {M \| z \|}{\eta + \| z \|}, \\ \end{array}
$$

It follows that

$$
f (z) \geqslant - \frac {M}{\eta} \| z \|. \tag {2.10}
$$

Combining (2.9) with (2.10), we obtain

$$
| f (z) | \leqslant \frac {M}{\eta} \| z \| \quad \forall z \in B _ {\eta} (\theta).
$$

(2) To prove the second assertion, we first show that $\forall y \in \operatorname { i n t } ( \operatorname { d o m } ( f ) )$ , $f$ is bounded above in a neighborhood of $y$ , i.e., $\exists \eta _ { 1 } > 0 , \exists M _ { 1 } > 0$ such that

$$
f (z) \leqslant M _ {1} \quad \forall z \in B _ {\eta_ {1}} (y). \tag {2.11}
$$

To this end one chooses $w = ( 1 + t ) y$ such that the segment ${ \overline { { y w } } } \subset \operatorname { i n t } ( \operatorname { d o m } ( f ) )$ . Setting $\textstyle \eta _ { 1 } = { \frac { t } { 1 + t } } \eta$ and $M _ { 1 } = \operatorname* { m a x } \left\{ M , f ( w ) \right\}$ , we have $f ( w ) \leqslant M _ { 1 }$ , and

$$
\left\| \frac {1 + t}{t} (z - y) \right\| \leqslant \eta ,
$$

if $z \in B _ { \eta _ { 1 } } ( y )$ . Since

$$
z = \frac {t}{1 + t} \left(\frac {1 + t}{t} (z - y)\right) + \frac {w}{1 + t}
$$

it follows from the convexity that

$$
f (z) \leqslant \frac {t}{1 + t} f \left(\frac {1 + t}{t} (z - y)\right) + \frac {1}{1 + t} f (w) \leqslant M _ {1}.
$$

Thus (2.11) holds.

(3) We turn to proving that $f$ is locally Lipschitzian in $\operatorname { i n t } ( \operatorname { d o m } ( f ) )$ . In fact, after (2), we only want to show that

$$
\left| f (z) - f \left(z ^ {\prime}\right) \right| \leqslant \frac {M}{\eta - \delta} \| z - z ^ {\prime} \| \quad \forall z, z ^ {\prime} \in B _ {\delta} (\theta), 0 <   \delta <   \eta .
$$

Dividing the segment $z z ^ { \prime }$ into $n$ equal length subsegments ${ \overline { { y _ { i } y _ { i + 1 } } } } , i = 1 , \dots , n$ with y1 = z, yn+1 = z-, and n > z−z $y _ { 1 } = z , y _ { n + 1 } = z ^ { \prime }$ $\begin{array} { r } { n > \frac { \| z - z ^ { \prime } \| } { \eta - \delta } } \end{array}$ , we have $y _ { i + 1 } \in B _ { \eta - \delta } ( y _ { i } ) \subset B _ { \eta } ( \theta )$ . Applying (1), it follows that

$$
\left| f \left(y _ {i + 1}\right) - f \left(y _ {i}\right) \right| \leqslant \frac {M}{\eta - \delta} \| y _ {i + 1} - y _ {i} \| \quad i = 1, 2, \dots , n.
$$

Then

$$
\left| f (z) - f \left(z ^ {\prime}\right) \right| \leqslant \frac {M}{\eta - \delta} \| z - z ^ {\prime} \| .
$$

We also need to study lower semi-continuity of functions defined on a topological space $X$ . □

Definition 2.2.2 A function $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is said to be lower semicontinuous (l.s.c.), if $\forall \lambda \in \mathbb { R } ^ { 1 }$ , the level set $f _ { \lambda } = \{ x \in X | f ( x ) \leqslant \lambda \}$ is closed. It is called sequentially lower semi-continuous (s.l.s.c.), if for any sequence $\{ x _ { n } \}$ , with $x _ { n } \to x \in X$ , we have

$$
\varlimsup_ {n \rightarrow \infty} f (x _ {n}) \geqslant f (x).
$$

From the definition, it follows directly that

(1) $f$ is l.s.c. $\iff \operatorname { e p i } ( f )$ is closed in $X \times ( \mathbb { R } ^ { 1 } \cup \{ + \infty \} )$ .   
(2) If $\{ f _ { \alpha } | \alpha \in \wedge \}$ is a family of l.s.c. functions. Then

$$
f (x) = \sup  \left\{f _ {\alpha} (x) | \alpha \in \wedge \right\}
$$

is l.s.c.

(3) If $f , g$ are l.s.c. and $\lambda , \mu \geqslant 0$ , then $\lambda f + \mu g$ is l.s.c.

Theorem 2.2.3 Let $X$ be a Banach space and let $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ be quasi convex. Then

(1) $f$ is l.s.c.  f is weakly lower semi-continuous (i.e., l.s.c. in weak topology).   
(2) If $X ^ { * }$ is separable, $f$ is weakly l.s.c. $\Longleftrightarrow \ f$ is sequentially weakly l.s.c.   
(3) If $X = Y ^ { * }$ , where $Y$ is a separable $B$ -space, then $f$ is $w ^ { \ast }$ l.s.c. $\Longleftrightarrow \mathrm f$ is sequentially $w ^ { \ast }$ l.s.c.

Proof. (1) “ =” is trivial; we prove “= ”. Since $\forall \lambda \in R ^ { 1 }$ , f is convex and closed. By the Hahn–Banach theorem, it is also weakly closed, i.e., $f$ is weakly l.s.c.

(2) In this case, the weak topolopy restricted to a normed bounded set is metrizable.   
(3) In this case, the w $^ *$ -topology restricted to a normed bounded set is metrizable. □

We collect the following theorems on weak compactness, w $^ *$ -compactness, weak sequential compactness and w $^ *$ -sequential compactness for further reference.

Theorem 2.2.4 (Banach–Alaoglu) Let $X$ be a Banach space, and let $E \subset$ $X ^ { * }$ . Then $E$ is $w ^ { * }$ -compact if and only if $E$ is normed bounded and $w ^ { * }$ -closed.

In particular, if further, $X$ is reflexive, then for any $E \subset X$ , $E$ is weakly compact if and only if $E$ is normed bounded and weakly closed.

See for instance Larsen [La], pp. 254–257.

Theorem 2.2.5 Let $E$ be a weakly closed set in a Banach space, then it is weakly compact if and only if it is weakly sequentially compact.

See for instance Larsen [La], pp. 303–309.

# 2.2.2 Subdifferentials

Convex functions on Banach spaces are not differentiable in general. However, the notion of subdifferentials is introduced as a replacement of derivatives of differentiable functions.

Definition 2.2.6 (Subdifferential) Let $f : X \to \mathbb { R } \cup \{ + \infty \}$ be a convex function on a vector space $X$ . $\forall x _ { 0 } \in \operatorname { d o m } ( f )$ , $x ^ { * } \in X ^ { * }$ is called a subgradient of $f$ at $x _ { 0 }$ if

$$
\langle x ^ {*}, x - x _ {0} \rangle + f (x _ {0}) \leqslant f (x) \quad \forall x \in X.
$$

The set of all subgradients at $x _ { 0 }$ is called the subdifferential of $f$ at $x _ { 0 }$ , and is denoted by $\partial f ( x _ { 0 } )$ .

Geometrically, $x ^ { * } \in \partial f ( x _ { 0 } )$ if and only if the hyperplane

$$
y = \langle x ^ {*}, x - x _ {0} \rangle + f (x _ {0})
$$

lies below the epigraph of $f$ , i.e., it is a support of $\operatorname { e p i } ( f )$ .

![](images/78412ad16ac0cdac1c142ed245e2ba3568602302030531cf808e3997ddedd155.jpg)  
Fig. 2.1.

Obviously, $\partial f ( x _ { 0 } )$ may contain more than one point. The following propositions hold, if $X$ is a Banach space.

(1) $\partial f ( x _ { 0 } )$ is a w∗-closed convex set.   
(2) If $x _ { 0 } \in \operatorname { i n t } ( \operatorname { d o m } ( f ) )$ , then $\partial f ( x _ { 0 } ) \neq 0$ .

Proof. We apply the Hahn–Banach separation theorem to the convex set $\operatorname { e p i } ( f ) \colon \exists ( x ^ { * } , \lambda ) \in X ^ { * } \times \mathbb { R } ^ { 1 } \backslash \{ ( \theta , 0 ) \}$ such that

$$
\langle x ^ {*}, x _ {0} \rangle + \lambda f (x _ {0}) \geqslant \langle x ^ {*}, x \rangle + \lambda t \quad \forall (x, t) \in \operatorname {e p i} (f).
$$

Since $( x _ { 0 } , f ( x _ { 0 } ) + 1 ) \in \mathrm { e p i } ( f )$ , it follows that $\lambda \leqslant 0$ . However, $\lambda \neq 0$ . Otherwise, we would have $\langle x ^ { * } , x - x _ { 0 } \rangle \leqslant 0 \forall x \in \mathrm { d o m } ( f )$ . From $x _ { 0 } ~ \in$ $\operatorname { i n t } ( \operatorname { d o m } ( f ) )$ , we conclude that $x ^ { * } = \theta$ . It contradicts with $( x ^ { * } , \lambda ) \neq ( \theta , 0 )$ . Setting $\begin{array} { r } { x _ { 0 } ^ { * } = \frac { 1 } { - \lambda } x ^ { * } } \end{array}$ , we obtain $x _ { 0 } ^ { * } \in \partial f ( x _ { 0 } )$ . □

(3) ∀ $\lambda \geqslant 0 \partial ( \lambda f ) ( x _ { 0 } ) = \lambda \partial f ( x _ { 0 } )$

(4) If $f , g : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ are convex, then $\forall x _ { 0 } \in \operatorname { i n t } ( \operatorname { d o m } ( f ) \cap \operatorname { d o m } ( g ) )$

$$
\partial (f + g) (x _ {0}) = \partial f (x _ {0}) + \partial g (x _ {0}).
$$

Proof. “ $\supset$ ” is trivial; we are going to prove “ $\subset$ ”. We may assume $x _ { 0 } = \theta$ , $f ( \theta ) = g ( \theta ) = 0$ , and $\theta \in \partial ( f + g ) ( \theta )$ . We want to show that $\exists x _ { 0 } ^ { * } \in$ $\partial f ( \theta )$ such that $- x _ { 0 } ^ { \ast } \in \partial g ( \theta )$ . Since the set $C = \{ ( x , t ) \in \mathrm { d o m } ( g ) \times$ $\mathbb { R } ^ { 1 } | \ t \ \leqslant \ - g ( x ) \}$ is convex, and $C \cap \operatorname { i n t } ( \operatorname { e p i } ( f ) ) \ : = \ : ( \theta , 0 )$ , from the fact that $f ( x ) + g ( x ) \geqslant 0$ . According to the Hahn–Banach separation theorem, $\exists ( x ^ { * } , \lambda ) \in X ^ { * } \times \mathbb { R } ^ { 1 }$ such that

$$
\langle x ^ {*}, x \rangle + \lambda f (x) \geqslant 0 \geqslant \langle x ^ {*}, x \rangle + \lambda t \quad \forall (x, t) \in C.
$$

In the same manner as the proof of (2), we verify $\lambda > 0$ . Setting $\begin{array} { r } { x _ { 0 } ^ { * } = \frac { - x ^ { * } } { \lambda } } \end{array}$ x∗ , we have

$$
\langle x _ {0} ^ {*}, x \rangle \leqslant f (x), - \langle x _ {0} ^ {*}, x \rangle \leqslant g (x) \quad \forall x \in X.
$$

i.e., $x _ { 0 } ^ { * } \in \partial f ( \theta )$ , and $- x _ { 0 } ^ { * } \in \partial g ( \theta )$

(5) If $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is convex, and is G-differentiable at a point $x _ { 0 } \in \operatorname { i n t } ( \operatorname { d o m } ( f ) )$ , then $\partial f ( x _ { 0 } )$ is a single point $x _ { 0 } ^ { * }$ satisfying $\langle x _ { 0 } ^ { * } , h \rangle =$ $d f ( x _ { 0 } , h ) \ \forall h \in X$ .

Proof. We may assume $x _ { 0 } = \theta$ and $f ( \theta ) = 0$ . Define the functional on $X$

$$
L (h) = d f (\theta , h).
$$

It is homogeneous. From the convexity of $f$ , we have

$$
L \left(h _ {1} + h _ {2}\right) \leqslant L \left(h _ {1}\right) + L \left(h _ {2}\right), \forall h _ {1}, h _ {2} \in X.
$$

Then $L$ is linear. Again by the convexity:

$$
- f (- h) \leqslant L (h) \leqslant f (h) \quad \forall h \in X.
$$

Combining with Theorem 2.2.1, $L$ is continuous. Therefore $\exists x _ { 0 } ^ { * } \in X ^ { * }$ such that

$$
L (h) = \left\langle x _ {0} ^ {*}, h \right\rangle .
$$

It follows that $x _ { 0 } ^ { * } \in \partial f ( \theta )$ .

Now, suppose $x ^ { * } \in \partial f ( \theta )$ , i.e., $\langle x ^ { * } , h \rangle \leqslant f ( h ) \ \forall h \in X$ , and then $\langle x ^ { * } , h \rangle \leqslant$ $\frac { 1 } { t } f ( t h ) \ \forall h \in X , \ \forall t > 0$ . By taking limit $t  + 0$ , it follows that $\langle x ^ { * } , h \rangle \leqslant$ $\langle x _ { 0 } ^ { * } , h \rangle \ \forall h \in X$ . Then $x ^ { * } = x _ { 0 } ^ { * }$ . □

(6) If $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is convex and attains its minimum at $x _ { 0 }$ , then $\theta \in \partial f ( x _ { 0 } )$ .

Let us present a few examples in computing the subdifferentials of convex functions.

Example 1. (Normalized duality map) Let $X$ be a real Banach space, and let $f ( x ) = { \begin{array} { l } { { \frac { 1 } { 2 } } } \end{array} } \| { \textbf { \em x } } \| ^ { 2 }$ . Then $\partial f ( x ) = F ( x ) : = \{ x ^ { * } \in X ^ { * } | \parallel x ^ { * } \parallel = \parallel x \parallel , \langle x ^ { * } , x \rangle = \parallel$ $x \parallel ^ { 2 } \}$ . It is called the normalized duality map.

In the particular case where $X$ is a real Hilbert space, it is known that $f ^ { \prime } ( x ) = x$ , and indeed, $F ( x ) = x$ . However, for Banach spaces, $f$ may not be differentiable. We verify $\partial f ( x ) = F ( x )$ as follows:

“ ” $\forall x ^ { * } \in \partial f ( x )$ , we have

$$
\langle x ^ {*}, y - x \rangle \leqslant \frac {1}{2} (\| y \| ^ {2} - \| x \| ^ {2}),
$$

and then $\forall h \in X$

$$
\langle x ^ {*}, h \rangle \leqslant \frac {1}{2 t} \left(\| x + t h \| ^ {2} - \| x \| ^ {2}\right) \leqslant \| x \| \| h \| + \frac {t}{2} \| h \| ^ {2},
$$

as $t > 0$ . It follows that $\langle x ^ { * } , h \rangle \leqslant \parallel { \boldsymbol { x } } \parallel \parallel h \parallel$ , and then $\parallel x ^ { * } \parallel \leqslant \parallel x \parallel$ . On the other hand, setting $y = \lambda x$ , we have $( \lambda - 1 ) \langle x ^ { * } , x \rangle \leqslant { \frac { 1 } { 2 } } ( \lambda ^ { 2 } - 1 ) \parallel x \parallel ^ { 2 }$ . Dividing by $( \lambda - 1 )$ as $\lambda \in ( 0 , 1 )$ , and then letting $\lambda = 1$ we obtain $\langle x ^ { * } , x \rangle \geq \parallel x \parallel ^ { 2 }$ ; it follows that $\parallel x \parallel \leqslant \parallel x ^ { \ast } \parallel$ . Thus $\| x \| = \| x ^ { * } \|$ , and $\langle x ^ { * } , x \rangle = \| x \| ^ { 2 }$ .

“ $\ l ) ^ { \mathfrak { N } } \forall x ^ { \ast } \in F ( x )$ , one has

$$
\begin{array}{l} \langle x ^ {*}, y - x \rangle = \langle x ^ {*}, y \rangle - \| x \| ^ {2} \\ \leqslant \| x ^ {*} \| \| y \| - \| x \| ^ {2} \\ \leqslant \frac {1}{2} (\| y \| ^ {2} - \| x \| ^ {2}) \\ = f (y) - f (x) \\ \end{array}
$$

$\forall y \in X$ , i.e., $x ^ { * } \in \partial f ( x )$ .

Example 2. Let $C$ be a convex subset of $X$ . The support function of $C$ is defined to be $S _ { C } ( x ^ { * } ) = \operatorname* { s u p } { \langle x ^ { * } , x \rangle }$ . Geometrically, $S _ { C } ( x ^ { * } ) = \langle x ^ { * } , x _ { 0 } \rangle$ if and x C only if $\{ x \in X | \ \langle x ^ { * } , x \rangle = S _ { C } ( x ^ { * } ) \}$ is a support hyperplane of $C$ at $x _ { 0 }$ .

We consider the subdifferential of the indicator function of $C$ :

$$
\partial_ {\chi_ {C}} (x _ {0}) = \left\{ \begin{array}{l l} \{\theta \} & \text {i f} x _ {0} \in \overset {\circ} {C} \\ \{x ^ {*} \in X ^ {*} | S _ {C} (x ^ {*}) = \langle x ^ {*}, x _ {0} \rangle \} & \text {i f} x _ {0} \in \partial C \\ \varnothing & \text {i f} x _ {0} \not \in \overline {{C}}  . \end{array} \right.
$$

Example 3. Let $\beta : \mathbb { R } ^ { 1 }  \mathbb { R } ^ { 1 }$ be a monotone nondecreasing function. Then

$$
\varphi (t) = \int_ {0} ^ {t} \beta (s) d s
$$

is a continuous convex function, and $\partial \varphi ( t _ { 0 } ) = [ \beta ( t _ { 0 } - 0 ) , \beta ( t _ { 0 } + 0 ) ] \ \forall t _ { 0 } \in \mathbb { R } ^ { 1 }$ . If further, $\exists p > 1 , C _ { 1 } , C _ { 2 } > 0$ such that $| \beta ( t ) | \leqslant C _ { 1 } + C _ { 2 } | t | ^ { p - 1 }$ , then the functional

$$
J (u) = \int_ {\Omega} \varphi (u (x)) d x
$$

is a convex functional on $L ^ { p } ( \Omega )$ , where $\Omega$ is a measurable set in $\mathbb { R } ^ { n }$ with bound measure. We conclude that

$$
\begin{array}{l} \partial J (u _ {0}) = [ \beta (u _ {0} (x) - 0), \beta (u _ {0} (x) + 0) ] \\ := \left\{v \in L ^ {p ^ {\prime}} (\Omega) \mid \beta \left(u _ {0} (x) - 0\right) \leqslant v (x) \leqslant \beta \left(u _ {0} (x) + 0\right) \forall \text {a . e .} \Omega \right\} \\ \end{array}
$$

where $\begin{array} { r } { \frac { 1 } { p ^ { \prime } } + \frac { 1 } { p } = 1 } \end{array}$ . In fact,

$$
\begin{array}{l} v \in \partial J (u _ {0}) \Longleftrightarrow \int_ {\Omega} v (x) (u (x) - u _ {0} (x)) \leqslant \int_ {\Omega} [ \varphi (u (x)) - \varphi (u _ {0} (x)) ] \forall u \in L ^ {p} (\Omega) \\ \Longleftrightarrow v (x) (u (x) - u _ {0} (x)) \leqslant \varphi (u (x)) - \varphi (u _ {0} (x)) \quad \text {a . e .} \\ \Longleftrightarrow v (x) \in \partial \varphi \left(u _ {0} (x)\right) \quad \text {a . e .} \\ \end{array}
$$

# 2.3 Convexity and Compactness

The Schauder fixed-point theorem is one of the most important fixed-point theorems in nonlinear analysis. In this section we shall study a series of fixedpoint theorems on compact convex set, including the Schauder fixed-point theorem and its extensions. All these theorems are set up by considering the Ky Fan inequality and KKM mapping from the outset.

Definition 2.3.1 Suppose that $X$ is a vector space, $E \subset X$ is a subset of $X$ . A set-valued mapping $G : E \to 2 ^ { X }$ is called a KKM mapping, if $\forall x _ { 1 } , \ldots , x _ { n } \in$ $E$ , $\mathrm { c o n v } \{ x _ { 1 } , \ldots , x _ { n } \} \subset \bigcup _ { i = 1 } ^ { n } G ( x _ { i } )$ .

Knaster, Kuratowski, and Mazurkiweicz (1929) discovered the following (KKM) theorem: Let $[ p _ { 0 } , \ldots , p _ { n } ]$ be an $n$ -simplex generated by $n + 1$ points $p _ { 0 } , p _ { 1 } , \ldots , p _ { n }$ in a vector topological space, and let $M _ { 0 } , \ldots , M _ { n }$ be $( n + 1 )$ closed sets, satisfying $[ p _ { i _ { 0 } } , \dotsc , p _ { i _ { k } } ] \subset \bigcup _ { j = 0 } ^ { k } M _ { i _ { j } } \forall $ k index subset $\{ i _ { 0 } , \ldots , i _ { k } \} \subset$ $\{ 0 , 1 , \ldots , n \}$ . Then $\bigcap _ { i = 0 } ^ { n } M _ { i } \neq \emptyset$ .

The following version of the KKM theorem is due to Ky Fan.

![](images/a2cbe9c4246e75f03d9b6eb991a271c6c9cc7f68e20e797347ae244dcc4f7b83.jpg)  
Fig. 2.2.

Theorem 2.3.2 (FKKM) Suppose that $X$ is a locally convex Hausdorff space (LCS), $E \subset X$ , and that $G : E \to 2 ^ { X }$ is a closed-valued KKM map. If $\exists x _ { 0 } \in E$ such that $G ( x _ { 0 } )$ is compact, then $\textstyle \bigcap _ { x \in E } G ( x ) \neq \emptyset$ .

Proof. Define $F ( x ) = G ( x ) \cap G ( x _ { 0 } )$ . Then $\{ F ( x ) | \ x \ \in \ E \}$ is a family of closed subsets of $G ( x _ { 0 } )$ . If one can show the finite intersection property of $\{ F ( x ) | x \in E \}$ , then our conclusion is proved.

To this end, we only want to verify the finite intersection property for $\{ G ( x ) | \ x \ \in \ E \}$ . If not, $\exists \{ x _ { 1 } , \dots , x _ { n } \} \subset E$ , such that $\bigcap _ { i = 1 } ^ { n } G ( x _ { i } ) \ = \ \varnothing$ . Let us introduce the Euclidean metric $d$ on $L = \operatorname { s p a n } \{ x _ { 1 } , \ldots , x _ { n } \}$ . Since $L$ is finite dimensional, the metric derives the same topology on $L$ , from the following facts: Each finite-dimensional LCS is normable (a consequence of the Kolmogorov normable theorem), and all norms on a finite-dimensional space are equivalent. Then the function

$$
\lambda (x) = \sum_ {i = 1} ^ {n} d (x, L \cap G (x _ {i})) > 0 \quad \forall x \in L.
$$

Define

$$
\beta_ {i} (x) = \frac {1}{\lambda (x)} d (x, L \cap G (x _ {i})) \quad i = 1, \dots , n.
$$

$\forall x \in L$ , we have

$$
\beta_ {i} (x) > 0 \Longleftrightarrow x \notin G (x _ {i}) \quad i = 1, \dots , n.
$$

Again, define $C = \operatorname { c o n v } \{ x _ { 1 } , \ldots , x _ { n } \}$ and a map $\varphi : C \to C$ to be

$$
\varphi : x \mapsto \sum_ {i = 1} ^ {n} \beta_ {i} (x) x _ {i}.
$$

According to the Brouwer fixed-point theorem, $\exists x _ { 0 } \in C$ such that $x _ { 0 } = \varphi ( x _ { 0 } )$ . Letting $I ( x _ { 0 } ) = \{ i \mid \beta _ { i } ( x _ { 0 } ) > 0 \}$ , we have

$$
\varphi (x _ {0}) \in \operatorname {c o n v} \left\{x _ {i} \mid i \in I (x _ {0}) \right\}.
$$

But $x _ { 0 } \notin G ( x _ { i } ) \forall i \in I ( x _ { 0 } )$ implies x0  G(xi). This contradicts with $x _ { 0 } \not \in \bigcup _ { i \in I ( x _ { 0 } ) } G ( x _ { i } )$ the KKM mapping. □

Ky Fan’s inequality is the foundation of this section, by which all fixedpoint theorems are derived.

Theorem 2.3.3 (Ky Fan’s inequality) Suppose that $X$ is an LCS and that $E \subset X$ is a nonempty convex set. Assume $\phi : E \times E \to \mathbb { R } ^ { 1 }$ satisfying

(1) $\forall y \in E$ $x \mapsto \phi ( x , y )$ is l.s.c.,   
(2) $\forall x \in E$ $y \mapsto \phi ( x , y )$ is quasi-concave,   
(3) $\exists y _ { 0 } \in E$ such that

$$
\{x \in E | \phi (x, y _ {0}) \leqslant \sup  _ {x \in E} \phi (x, x) \} i s c o m p a c t.
$$

Then $\exists x _ { 0 } \in E$ such that

$$
\sup  _ {y \in E} \phi (x _ {0}, y) \leqslant \sup  _ {x \in E} \phi (x, x).
$$

Before going to the proof of Theorem 2.3.3, we first introduce a geometric version of the statement. Given a set $E$ , let $A \subset E \times E$ be a subset. Define the sections:

$$
\forall x \in E, A _ {1} (x) = \left\{y \in E \mid (x, y) \in A \right\},
$$

and

$$
\forall y \in E, A _ {2} (y) = \left\{x \in E \mid (x, y) \in A \right\}.
$$

Denote the diagonal $\{ ( x , x ) | x \in E \}$ by $\bigtriangleup$ .

![](images/33adc382ef5fbf00b379a5b15ab1f5e675395d4f720686a804e6bf0d49421614.jpg)  
Fig. 2.3.

In this manner, we define two set-valued mappings: $E  2 ^ { E }$ ,

$$
A _ {1}: x \mapsto A _ {1} (x) \text {a n d} A _ {2}: y \mapsto A _ {2} (y).
$$

Theorem 2.3.4 Suppose that $X$ is an LCS and that $E \subset X$ is a nonempty convex set. Assume that $\Gamma \subset E \times E$ is a subset satisfying

(1-) $\forall y \in E$ , $\Gamma _ { 2 } ( y )$ is open,   
(2-) $\forall x \in E$ , $\Gamma _ { 1 } ( x )$ is convex,   
(3-) $\triangle \cap \Gamma = \emptyset$   
(4-) $\exists y _ { 0 } \in E$ such that $E \backslash \Gamma _ { 2 } ( y _ { 0 } )$ is compact.

Then $\exists x _ { 0 } \in E$ such that $\Gamma _ { 1 } ( x _ { 0 } ) = 0$ .

We are going to show that Theorem 2.3.3  Theorem 2.3.4.

Proof. “ = ” Define a function on $E \times E$ :

$$
\phi (x, y) = \left\{ \begin{array}{l l} 1 & (x, y) \in \Gamma \\ 0 & (x, y) \notin \Gamma \end{array} \right.
$$

$\forall y \in E$ , we see

$$
\{x \in E | \phi (x, y) > \lambda \} = \left\{ \begin{array}{l l} E & \text {i f} \lambda <   0 \\ \Gamma_ {2} (y) & \text {i f} 0 \leqslant \lambda <   1 \\ \emptyset & \text {i f} 1 \leqslant \lambda , \end{array} \right.
$$

then $x \mapsto \phi ( x , y )$ is l.s.c., from (1-).

Similarly, $\forall x \in E$

$$
\{y \in E | \phi (x, y) > \lambda \} = \left\{ \begin{array}{l l} E & \text {i f} \lambda <   0 \\ \Gamma_ {1} (x) & \text {i f} 0 \leqslant \lambda <   1 \\ \varnothing & \text {i f} 1 \leqslant \lambda  , \end{array} \right.
$$

then $y \mapsto \phi ( x , y )$ is quasi-concave, from ( $2 ^ { \prime }$ ) .

Since $\triangle \cap \Gamma = \emptyset , \phi ( x , x ) = 0 \forall x \in E$ . The set

$$
\{x \in E | \phi (x, y _ {0}) \leqslant 0 \} = E \backslash \Gamma_ {2} (y _ {0})
$$

is compact, from (4-). Theorem 2.3.3 is applied to conclude that $\exists x _ { 0 } \in E$ such that $\phi ( x _ { 0 } , y ) \leqslant 0 \forall y \in E$ , i.e., $\Gamma _ { 1 } ( x _ { 0 } ) = 0$ .

“ =” Define

$$
\Gamma = \{(x, y) \in E \times E | \phi (x, y) > \mu \},
$$

where $\mu = \operatorname* { s u p } _ { x \in E } \phi ( x , x )$ . Then (1) and (2) imply ( $1 ^ { \prime }$ ) and ( $2 ^ { \prime }$ ) respectively. By definition $\Gamma \cap \triangle = \emptyset$ . Also

$$
E \backslash \Gamma_ {2} (y _ {0}) = \{x \in E | \phi (x, y _ {0}) \leqslant \mu \}
$$

is compact. We then apply Theorem 2.3.4, and conclude that $\exists x _ { 0 } \in E$ such that $\Gamma _ { 1 } ( x _ { 0 } ) = \emptyset$ , i.e., $\phi ( x _ { 0 } , y ) \leqslant \mu \forall y \in E$ . □

Now we turn to giving a proof of Theorem 2.3.4.

Proof. Define a set-valued mapping

$$
G: y \mapsto E \backslash \Gamma_ {2} (y).
$$

From (1-), it is a closed set-valued mapping.

( $4 ^ { \prime }$ ) implies that $G ( y _ { 0 } )$ is compact. If we can show that $G$ is a KKM map, then the FKKM theorem is applied to conclude that ∩ G(y) = Ø, i.e., $\bigcap _ { y \in E } G ( y ) \neq \emptyset$

$$
\exists x _ {0} \in \bigcap_ {y \in E} G (y) \iff (x _ {0}, y) \in (E \times E) \backslash \Gamma \forall y \in E \iff \Gamma_ {1} (x _ {0}) = \emptyset .
$$

Verification of the KKM map.

If not, $\exists \{ y _ { 1 } , \dots , y _ { n } \} \subset E$ and $\exists \lambda _ { 1 } \geq 0 , \ldots , \lambda _ { n } \geq 0$ , with $\Sigma _ { 1 } ^ { n } \lambda _ { i } \leq 1$ , such that

$$
\begin{array}{l} w = \sum_ {i = 1} ^ {n} \lambda_ {i} y _ {i} \notin \bigcup_ {i = 1} ^ {n} G (y _ {i}) \iff w \in \bigcap_ {i = 1} ^ {n} \Gamma_ {2} (y _ {i}), \\ \iff y _ {i} \in \Gamma_ {1} (w) \forall i, \\ \stackrel {{\text {b y} \left(2 ^ {\prime}\right)}} {{\Longrightarrow}} w \in \Gamma_ {1} (w)  , \\ \Rightarrow \triangle \cap \Gamma \neq \emptyset . \\ \end{array}
$$

This contradicts with ( $3 ^ { \prime }$ ) .

□

First we mention that this theorem is very useful in game theory. In particular, the Nash equilibrium theorem and Von Neumann–Sion saddle point theorem can be derived from it.

In an $n$ -person game, there are $n$ players $\{ 1 , 2 , \ldots , n \}$ . The $i$ th person has a strategy set $E _ { i }$ and a payoff function $f _ { i } , { i = 1 , 2 , \dots , n }$ . The payoff functions depend on the strategies of all players, i.e., $f _ { i } , { i = 1 , 2 , \dots , n }$ are functions of $x = ( x _ { 1 } , \ldots , x _ { n } )$ , where $x _ { j } \in E _ { j } , j = 1 , \dotsc , n$ . Nash’s solution of the game is such a strategy $( x _ { 1 } ^ { * } , \ldots , x _ { n } ^ { * } ) \in E _ { 1 } \times \cdot \cdot \cdot \times E _ { n }$ : no player has any incentive to change his strategy as long as his enemies don’t change theirs.

Mathematically, let $X _ { 1 } , \ldots , X _ { n }$ be $n$ -LCS, let $E _ { i } \subset X _ { i }$ be nonempty compact convex sets, $\forall i = 1 , \ldots , n$ , and let $f _ { 1 } , \ldots , f _ { n }$ be $n$ continuous functions on $E = E _ { 1 } \times \cdot \cdot \cdot \times E _ { n }$ . A point $x ^ { * } = ( x _ { 1 } ^ { * } , \ldots , x _ { n } ^ { * } ) \in E$ is called a Nash equilibrium if

$$
f _ {i} \left(x ^ {*}\right) \geqslant f _ {i} \left(x _ {1} ^ {*}, \dots , y _ {i}, \dots , x _ {n} ^ {*}\right) \quad \forall i = 1, 2, \dots , n \forall y = \left(y _ {1}, \dots , y _ {n}\right) \in E.
$$

Corollary 2.3.5 (Nash) If the continuous functions

$$
x _ {i} \mapsto f _ {i} (x _ {1}, \dots , x _ {i}, \dots , x _ {n}) \quad \forall i = 1, \dots , n
$$

are quasi-concave (x1, . . . , xˆi, . . . , xn) Ej i. Then the Nash equilibrium $\forall ( x _ { 1 } , \dots , \hat { x } _ { i } , \dots , x _ { n } ) \in \underset { j \neq i } { \sqcap } E _ { j } \ \forall i$ exists.

In fact, let us define a function on $E \times E$ :

$$
\phi (x, y) = \sum_ {i = 1} ^ {n} \left(f _ {i} \left(x _ {1}, \dots , y _ {i}, \dots , x _ {n}\right) - f _ {i} \left(x _ {1}, \dots , x _ {n}\right)\right)
$$

It is easy to verify that all the assumptions of the Ky Fan inequality are satisfied (in particular, $y \to \phi ( x , y )$ is quasi-concave), and $\phi ( x , x ) = 0 \forall x \in E$ . Then $\exists x ^ { * } \in E$ satisfying $\phi ( x ^ { * } , y ) \leqslant 0 \forall y \in E$ . This is the Nash equilibrium.

Corollary 2.3.6 (Von Neumann–Sion) Suppose that $X , Y$ are reflexive Banach spaces, $E \subset X$ and $F \subset Y$ are nonempty closed convex sets. Assume that $f : E \times F \to \mathbb { R } ^ { 1 }$ satisfies

(1) $\forall y \in F$ , $x \mapsto f ( x , y )$ is l.s.c. and quasi-convex.   
(2) $\forall x \in E$ , $y \mapsto f ( x , y )$ is u.s.c. and quasi-concave.   
(3) $\exists ( x _ { 0 } , y _ { 0 } ) \in E \times F$ such that $f ( x _ { 0 } , y )  - \infty$ as $\parallel y \parallel \to + \infty$ and $f ( x , y _ { 0 } ) $ $+ \infty$ as $\parallel x \parallel \to + \infty$ .

Then there exist $( x ^ { * } , y ^ { * } ) \in E \times F$ such that

$$
f \left(x ^ {*}, y\right) \leqslant f \left(x ^ {*}, y ^ {*}\right) \leqslant f \left(x, y ^ {*}\right) \quad \forall (x, y) \in E \times F. \tag {2.12}
$$

Proof. Define

$$
\phi (u, v) = f (x, w) - f (z, y),
$$

where $u = ( x , y ) , v = ( z , w ) \in E \times F$ . Then we have the following:

From Theorem $2 . 2 . 3 , \forall v \in E \times F , u \mapsto \phi ( u , v )$ is w.l.s.c.

It is easy to verify directly ${ \mathrm { h a t } } \forall u \in E \times F , v \mapsto \phi ( u , v )$ is quasi-concave,

$$
\begin{array}{l} \phi (u, v _ {0}) = f (x, y _ {0}) - f (x _ {0}, y) \to + \infty \text {a s} \| x \| + \| y \| \to + \infty , \text {w h e r e} \\ v _ {0} = (x _ {0}, y _ {0})  . \end{array}
$$

The set $\{ u \in E \times F | \ \phi ( u , v _ { 0 } ) \leqslant 0 \}$ is weakly compact. We apply Ky Fan’s inequality to the space $E \times F$ with weak topology. There exists $\boldsymbol { u } ^ { * } = ( x ^ { * } , y ^ { * } )$ such that

$$
f \left(x ^ {*}, w\right) - f \left(z, y ^ {*}\right) \leqslant 0 \quad \forall (z, w) \in E \times F ,
$$

which implies

$$
f \left(x ^ {*}, y\right) \leqslant f \left(x ^ {*}, y ^ {*}\right) \leqslant f \left(x, y ^ {*}\right) \quad \forall (x, y) \in E \times F.
$$

□

The solution $( x ^ { * } , y ^ { * } )$ is called the saddle point of $f$ .

As a special case where $\dim Y = 0$ , we have:

Corollary 2.3.7 Suppose that $X$ is a reflexive Banach space. If $f : X \to$ $\mathbb { R } ^ { 1 } \cup \lbrace + \infty \rbrace$ is an l.s.c. convex function, satisfying $f ( x )  + \infty$ as $\parallel x \parallel  \infty$ , then there exists $x _ { 0 } \in X$ such that $f ( x _ { 0 } ) = \operatorname* { m i n } { \{ f ( x ) | x \in X \} }$ .

In the case where $E$ and $F$ are compact, the Von Neumann–Sion theorem is also rewritten as follows:

$$
\min  _ {x \in E} \max  _ {y \in F} f (x, y) = \max  _ {y \in F} \min  _ {x \in E} f (x, y). \tag {2.13}
$$

Indeed, (2.12) ⇐⇒ (2.13).

Proof. “= ” We always have

$$
\beta := \max  _ {y \in F} \min  _ {x \in E} f (x, y) \leqslant \min  _ {x \in E} \max  _ {y \in F} f (x, y) =: \alpha .
$$

However, (2.12) implies

$$
\alpha \leqslant \max  _ {y \in F} f (x ^ {*}, y) \leqslant f (x ^ {*}, y ^ {*}) \leqslant \min  _ {x \in E} f (x, y ^ {*}) \leqslant \beta .
$$

Therefore $\alpha = \beta$ , and then (2.13) holds.

“ =” Since both $E$ and $F$ are compact and the functions $x \mapsto f ( x , y )$ is l.s.c., $y \mapsto f ( x , y )$ is u.s.c., the functions $x \to \operatorname* { m a x } _ { y \in F } f ( x , y )$ is l.c.s, and $y  \operatorname* { m i n } _ { x \in E } f ( x , y )$ is u.s.c., we have $( x ^ { * } , y ^ { * } ) \in E \times F$ such that

$$
\begin{array}{l} \min  _ {x \in E} f (x, y ^ {*}) = \max  _ {y \in F} \min  _ {x \in E} f (x, y) \\ = \min  _ {x \in E} \max  _ {y \in F} f (x, y) \\ = \max  _ {y \in F} f (x ^ {*}, y). \\ \end{array}
$$

Since

$$
\min  _ {x \in E} f (x, y ^ {*}) \leq f (x ^ {*}, y ^ {*}) \leq \max  _ {y \in F} f (x ^ {*}, y),
$$

these three are equal. It (2.12) follows.

In this sense, the Von Neumann–Sion theorem is called the mini-max theorem, which is the fundamental theorem in two-person game theory, and has many applications in other fields of mathematics (linear programming, convex programming, potential theory, the dual variational theory in mechanics etc.)

Next, we turn to fixed-point theorems.

For a set-valued mapping $\Gamma : E  2 ^ { F }$ , where $E , F$ are Hausdorff topological vector spaces, we say $\Gamma$ is upper (or lower) semi-continuous (u.s.c. or l.s.c., respectively for short), if for any closed (open resp.) set $W \subset F$ , the pre-image $\Gamma ^ { - 1 } ( W ) = \{ x \in E | \Gamma ( x ) \cap W \neq \emptyset \}$ is closed (open resp.).

For single-valued mappings, both u.s.c. and l.s.c. are all reduced to continuity.

An alternative description of the upper semi-continuity is that for any open set $W \subset F$ , the set $\{ x \in E | \Gamma ( x ) \subset W \}$ is open. A point $x _ { 0 } \in E$ is called a fixed point of $\Gamma$ if $x _ { 0 } \in \Gamma ( x _ { 0 } )$ .

Theorem 2.3.8 (Ky Fan–Glicksberg) Suppose that $X$ is an LCS, and that $E \subset X$ is a nonempty compact convex set. If $\Gamma : E  2 ^ { E }$ is u.s.c., and $\forall x \in E , \Gamma ( x )$ is a nonempty closed convex subset of $E$ , then $\exists x _ { 0 } \in E$ such that $x _ { 0 } \in \Gamma ( x _ { 0 } )$ .

Proof. We prove by contradiction. If $\forall x \in E , x \notin \Gamma ( x )$ , then from the Hahn– Banach theorem $\exists x ^ { * } \in X ^ { * } \exists t \in \mathbb { R } ^ { 1 }$ such that

$$
\langle x ^ {*}, y \rangle <   t <   \langle x ^ {*}, x \rangle \quad \forall y \in \Gamma (x)  .
$$

Since $\Gamma$ is u.s.c. there exists a neighborhood $U ( x )$ of $x$ such that

$$
\langle x ^ {*}, y \rangle <   t <   \langle x ^ {*}, z \rangle \quad \forall y \in \Gamma (z) \forall z \in U (x).
$$

Since $E$ is compact, we have $x _ { 1 } , \ldots , x _ { n } \in E$ such that $E \subset \bigcup _ { i = 1 } ^ { n } U ( x _ { i } )$ .

Let $\{ x _ { 1 } ^ { * } , \ldots , x _ { n } ^ { * } \} \subset X ^ { * }$ be the associated elements; one has

$$
\langle x _ {i} ^ {*}, y \rangle <   \langle x _ {i} ^ {*}, x \rangle \quad \forall y \in \Gamma (x) \forall x \in U (x _ {i}).
$$

We construct a partition of unity of $E$ :

$$
\left\{\beta_ {i} \colon E \to \mathbb {R} ^ {1}, i = 1, \dots , n \mid \beta_ {i} \geqslant 0, \sum_ {i = 1} ^ {n} \beta_ {i} = 1 \text {o n} E \text {a n d} \beta_ {i} (x) > 0 \text {i f f} x \in U (x _ {i}) \right\}.
$$

Define a continuous map $f : E \to X ^ { * }$ by

$$
f (x) = \sum_ {i = 1} ^ {n} \beta_ {i} (x) x _ {i} ^ {*},
$$

one has

$$
\begin{array}{l} \langle f (x), y \rangle = \sum_ {i = 1} ^ {n} \beta_ {i} (x) \langle x _ {i} ^ {*}, y \rangle <   \sum_ {i = 1} ^ {n} \beta_ {i} (x) \langle x _ {i} ^ {*}, x \rangle \\ = \langle f (x), x \rangle \quad \forall x \in E, \forall y \in \Gamma (x). \tag {2.14} \\ \end{array}
$$

Define $\phi : E \times E \to \mathbb { R } ^ { 1 }$ by

$$
\phi (x, y) = \langle f (x), x - y \rangle ,
$$

All the assumptions of Theorem 2.3.3 are satisfied, and we conclude that $\exists x _ { 0 } \in E$ such that

$$
\phi (x _ {0}, y) \leqslant 0 \quad \forall y \in E;
$$

it follows that

$$
\langle f (x _ {0}), x _ {0} \rangle = \min  _ {y \in E} \langle f (x _ {0}), y \rangle .
$$

This contradicts (2.14), because $\Gamma ( x ) \subset E$ .

□

This is a very general fixed-point theorem concerning convexity and compactness. The very special case is the Schauder fixed-point theorems, which are stated as follows:

Corollary 2.3.9 (Schauder–Tichonov) Suppose that $X$ is an LCS and that $E \subset X$ is a nonempty compact convex set. If $f : E \to E$ is continuous, then $f$ has a fixed point.

Corollary 2.3.10 (Schauder) Suppose that $X$ is a Banach space and that $E \subset X$ is a nonempty bounded closed convex set. If $f : E \to E$ is a compact map, then $f$ has a fixed point.

Proof. It is a direct consequence of Corollary 2.3.9. Let us define

$$
E _ {1} = \overline {{\operatorname {c o n v}}} f (E).
$$

Then $E _ { 1 }$ is a nonempty compact convex subset of $E$ , and $f : E _ { 1 } \to E _ { 1 }$ .

We would rather give a direct proof of the second Schauder fixed-point theorem as follows:

Proof. Define $\phi ( x , y ) = - \| y - f ( x ) \|$ on $E _ { 1 } \times E _ { 1 }$ , where $E _ { 1 } = \overline { { \mathrm { c o n v } } } f ( E )$ is compact. Applying Ky Fan’s inequality, we have $x _ { 0 } ~ \in ~ E _ { 1 }$ such that $\begin{array} { r } { \operatorname* { s u p } _ { y \in E _ { 1 } } \phi ( x _ { 0 } , y ) ~ \le ~ \operatorname* { s u p } _ { x \in E _ { 1 } } \phi ( x , x ) } \end{array}$ . Thus, $\begin{array} { r } { \operatorname* { i n f } _ { x \in E _ { 1 } } \| x - f ( x ) \| \ \leq \ \operatorname* { i n f } _ { y \in E _ { 1 } } } \end{array}$ $\| y - f ( x _ { 0 } ) \| = 0$ , i.e., $\exists x \in E _ { 1 }$ such that $x = f ( x )$ . □

Remark 2.3.11 Theorem 2.3.8 was obtained by Ky Fan [FK 1] and Glicksberg independently in 1952. The earlier results for $X \ = \ \mathbb { R } ^ { n }$ and $X$ being a linear normed space are due to Kakutani (1941) [Kak] and Bohnenblust, Karlin (1950) resp.

# Applications

# 1. Convex programming

Let $X$ be a reflexive Banach space. Given l.s.c. convex functions $f , g _ { 1 } , \ldots , g _ { n } \ : \ X \to \mathbb { R } ^ { 1 }$ , assume that the set $C = \{ x \in X | g _ { i } ( x ) \leq 0 , i =$ $1 , 2 , \ldots , n \}$ is nonempty; find $x _ { 0 } \in C$ such that

$$
f \left(x _ {0}\right) = \min  \left\{f (x) \mid x \in C \right\}. \tag {2.15}
$$

Let us introduce Lagrange multipliers $\{ y _ { i } \} _ { 1 } ^ { n } , y _ { i } \geqslant 0 , i = 1 , . . . , n$ , and the following function:

$$
L (x, y) = f (x) + \sum_ {i = 1} ^ {n} y _ {i} g _ {i} (x) \quad (x, y) \in X \times \mathbb {R} _ {+} ^ {n},
$$

which is called the Lagrangian.

Statement 2.3.12 If $( x _ { 0 } , y _ { 0 } )$ is a saddle point of L:

$$
L \left(x _ {0}, y\right) \leqslant L \left(x _ {0}, y _ {0}\right) \leqslant L \left(x, y _ {0}\right) \quad \forall (x, y) \in X \times \mathbb {R} _ {+} ^ {n}. \tag {2.16}
$$

Then $x _ { 0 }$ is a solution of (2.15) and satisfies

$$
y _ {i} ^ {\circ} g _ {i} \left(x _ {0}\right) = 0, i = 1, 2, \dots , n,
$$

where $( y _ { 1 } ^ { \circ } , \ldots , y _ { n } ^ { \circ } ) = y _ { 0 }$ . Conversely, if there exists $x ^ { * } \in X$ satisfying $g _ { i } ( x ^ { * } ) <$ 0, $i = 1 , 2 , \ldots , n$ , and if $x _ { 0 } \in X$ solves (2.15), then $\exists \xi \in \mathbb { R } _ { + } ^ { n }$ such that $( x _ { 0 } , \xi )$ is a saddle point of $L$ .

Proof. “=⇒” From the first inequality of (2.16), we have

$$
\sum_ {i = 1} ^ {n} \left(y _ {i} ^ {\circ} - y _ {i}\right) g _ {i} \left(x _ {0}\right) \geqslant 0.
$$

Letting $y _ { i } \to + \infty$ and $y _ { j } = y _ { j } ^ { \circ } , j \neq i$ , it follows that $g _ { i } ( x _ { 0 } ) \leqslant 0$ , i = $1 , 2 , \ldots , n$ . Then letting $y _ { i } = 0 , i = 1 , 2 , . . . , n$ , it follows that $\textstyle \sum _ { i = 1 } ^ { n } y _ { i } ^ { \circ } g _ { i } ( x _ { 0 } ) \geqslant$ 0. But for $y _ { i } ^ { \mathrm { o } } \geqslant 0$ , we have either $g _ { i } ( x _ { 0 } ) = 0$ or $y _ { i } ^ { \mathrm { o } } = 0$ , i.e., $y _ { i } ^ { \circ } g _ { i } ( x _ { 0 } ) = 0 , ~ i =$ $1 , 2 , \ldots , n$ .

Returning to the second inequality of (2.16), we obtain

$$
f (x _ {0}) \leqslant f (x) + \sum_ {i = 1} ^ {n} y _ {i} ^ {\circ} g _ {i} (x) \quad \forall x \in X.
$$

Thus

$$
f (x _ {0}) \leqslant f (x),
$$

whenever $g _ { i } ( x ) \leqslant 0 , \ i = 1 , 2 , \ldots , n$

“ =” We consider two convex sets:

$$
E = \left\{\left(y ^ {0}, \dots , y ^ {n}\right) \in \mathbb {R} ^ {n + 1} \mid y ^ {0} <   f \left(x _ {0}\right), y ^ {i} <   0, i = 1, 2, \dots , n \right\},
$$

and

$$
F = \left\{\left(y ^ {0}, \dots , y ^ {n}\right) \in \mathbb {R} ^ {n + 1} \mid \exists x \in X \text {s u c h t h a t} f (x) \leqslant y ^ {0}, g _ {i} (x) \leqslant y ^ {i}, \forall i \right\}.
$$

Claim: $E \cap F = \cup$ . If not, $\exists ( { \overline { { y } } } ^ { \cup } , \dotsc , { \overline { { y } } } ^ { n } ) \in E \cap F$ , i.e., $\exists x _ { 1 } \in X$ satisfying $f ( x _ { 1 } ) \leq \overline { { y } } ^ { \cup } < f ( x _ { 0 } )$ , g $\ L _ { i } ( x _ { 1 } ) \leqslant \overline { { y } } ^ { i } < 0 , \ L = 1 , \dots , n$ . Then $x _ { 0 }$ cannot be a solution of (2.15). This is a contradiction.

Applying the Hahn–Banach theorem, $\exists \omega = ( \omega _ { 0 } , \ldots , \omega _ { n + 1 } ) \in \mathbb { R } ^ { n + 1 } \backslash \{ \theta \}$ such that

$$
\sum_ {i = 0} ^ {n} \omega_ {i} y ^ {i} \leqslant \sum_ {i = 0} ^ {n} \omega_ {i} z ^ {i} \tag {2.17}
$$

$\forall y ~ = ~ ( y ^ { \mathsf { U } } , \ldots , y ^ { n } ) ~ \in ~ E$ , $\forall z \ = \ ( z ^ { \mathsf { U } } , \ldots , z ^ { n } ) \ \in \ F$ . By setting $z ~ = ~ ( f ( x _ { 0 } )$ , $g _ { 1 } ( x _ { 0 } ) , \ldots , g _ { n } ( x _ { 0 } ) )$ ,

$y = ( f ( x _ { 0 } ) - \epsilon _ { 0 } , g _ { 1 } ( x _ { 0 } ) - \epsilon _ { 1 } , \ldots , g _ { n } ( x _ { 0 } ) - \epsilon _ { n } ) , \forall ( \epsilon _ { 0 } , \epsilon _ { 1 } , \ldots , \epsilon _ { n } ) \in R _ { + } ^ { n + 1 }$ , it follows that $\omega _ { i } \geqslant 0 , \ i = 0 , 1 , \ldots , n$ .

Again, $\forall \varepsilon > 0$ , substituting $y ^ { 0 } = f ( x _ { 0 } ) - \varepsilon , z ^ { 0 } = f ( x ) , y ^ { i } = - \varepsilon , z ^ { i } =$ $g _ { i } ( x ) \ i = 1 , . . . , n$ into (2.17) $\forall x \in X$ , and letting $\varepsilon \to 0$ , we have

$$
\omega_ {0} f \left(x _ {0}\right) \leqslant \sum_ {i = 1} ^ {n} \omega_ {i} g _ {i} (x) + \omega_ {0} f (x) \quad \forall x \in X. \tag {2.18}
$$

Claim $\omega _ { 0 } ~ > ~ 0$ . Otherwise, $\omega _ { 0 } ~ = ~ 0$ , thus $\textstyle \sum _ { i = 1 } ^ { n } \omega _ { i } g _ { i } ( x ) \ \geqslant \ 0$ . Since we have assumed $g _ { i } ( x ^ { * } ) < 0$ , combining with the fact that $\omega _ { i } \geqslant 0 , \ i = 0 , 1 , \ldots , n$ , it follows that $\omega _ { i } = 0 , \ i = 1 , \dots , n$ . This is impossible.

Dividing (2.18) by $\omega _ { 0 }$ , we obtain

$$
f (x _ {0}) \leqslant \sum_ {j = 1} ^ {n} \xi_ {j} g _ {j} (x) + f (x) \quad \forall x \in X,
$$

where $\begin{array} { r } { \xi _ { j } = \frac { \omega _ { j } } { \omega _ { 0 } } } \end{array}$ ωj . Then

$$
\sum_ {j = 1} ^ {n} \xi_ {j} g _ {j} (x _ {0}) = 0,
$$

it follows that $\xi _ { j } g _ { j } ( x _ { 0 } ) = 0$ $j = 1 , \dotsc , n$ . Thus

$$
f (x _ {0}) + \sum_ {j = 1} ^ {n} y _ {j} g _ {j} (x _ {0}) \leqslant f (x _ {0}) + \sum_ {j = 1} ^ {n} \xi_ {j} g _ {j} (x _ {0}) \leqslant f (x) + \sum_ {j = 1} ^ {n} \xi_ {j} g _ {j} (x)
$$

$\forall ( x , y ) \in X \times \mathbb { R } _ { + } ^ { n }$ , i.e., $( x _ { 0 } , \xi )$ is a saddle point of $L$ .

Statement 2.3.13 In the convex programming problem, assume

(1) $f ( x )  + \infty$ as $\parallel x \parallel  \infty$   
(2) $\exists x _ { 0 } \in X$ such that $g _ { i } ( x _ { 0 } ) < 0 , ~ i = 1 , 2 , \ldots , n$ .

The problem (2.16) has a solution.

Proof. Consider the Lagrangian on $X \times \mathbb { R } _ { + } ^ { n }$

$$
L (x, y) = f (x) + \sum_ {i = 1} ^ {n} y _ {i} g _ {i} (x).
$$

Obviously,

$$
\forall y \in \mathbb {R} _ {+} ^ {n}, x \mapsto L (x, y) \text {i s l . s . c . a n d c o n v e x ,}
$$

$$
\forall x \in X, y \mapsto L (x, y) \text {i s l i n e a r ,}
$$

and by (2) and (1),

$$
L (x _ {0}, y) \rightarrow - \infty \text {a s} \| y \| \rightarrow \infty ,
$$

$$
L (x, \theta) = f (x) \rightarrow + \infty \text {a s} \| x \| \rightarrow \infty .
$$

Applying Corollary 2.3.6, there is a saddle point of $L$ , which is the solution of the problem (2.15), according to Statement 2.3.12. □

# 2. Periodic solutions of ODE

We study the $2 \pi$ -periodic solutions of the following ODE:

$$
\ddot {x} + g (x) = f (t) \tag {2.19}
$$

where $g \in C ( \mathbb { R } ^ { 1 } )$ and $f \in L ^ { 2 } ( [ 0 , 2 \pi ] )$ . Assume that $\exists n \in \mathbb { N }$ and $\varepsilon > 0$ such that

$$
n ^ {2} + \varepsilon \leqslant \frac {g (x)}{x} \leqslant (n + 1) ^ {2} - \varepsilon \quad \text {f o r} | x | \text {l a r g e}. \tag {2.20}
$$

Statement 2.3.14 The problem (2.19) has a $2 \pi$ -periodic solution under the assumption (2.20).

Proof. Let $\gamma = n ^ { 2 } + n + 1 / 2$ . Then $\exists R > 0$ such that

$$
| g (x) - \gamma x | \leqslant \left(n + \frac {1}{2} - \epsilon\right) | x | \quad \text {a s} | x | \geqslant R.
$$

Rewrite (2.19) as

$$
\ddot {x} + \gamma x + g (x) - \gamma x - f (t) = 0 \tag {2.21}
$$

and define the linear operator

$$
L: x \mapsto - (\ddot {x} + \gamma x)
$$

with domain $D ( L ) = \{ x \in H ^ { 2 } ( 0 , 2 \pi ) |$ $x ( 0 ) = x ( 2 \pi )$ , ${ \dot { x } } ( 0 ) = { \dot { x } } ( 2 \pi ) \}$ . Then $L$ has a bounded inverse: $L ^ { - 1 } : L ^ { 2 } ( 0 , 2 \pi )  D ( L )$ . Since the injection $D ( L ) \to$ $L ^ { 2 } ( 0 , 2 \pi )$ is compact, (2.21) is equivalent to

$$
x = L ^ {- 1} \cdot G (x),
$$

where $G : x \mapsto g ( x ) - \gamma x - f$ is a bounded continuous mapping on $L ^ { 2 } ( 0 , 2 \pi )$ . Then $F = L ^ { - 1 } \circ G$ is a compact map on $L ^ { 2 } ( 0 , 2 \pi )$ .

Moreover $\begin{array} { r } { \| ~ L ^ { - 1 } ~ \| \leqslant ~ \frac { 1 } { n + 1 / 2 } } \end{array}$ , $\parallel G x \parallel \leqslant ( n + \textstyle { \frac { 1 } { 2 } } - \varepsilon ) \parallel x \parallel + C$ , where $C = \sqrt { 2 \pi } \operatorname* { m a x } _ { | x | \leqslant R } | g ( x ) - \gamma x | + \parallel \textit { f } \parallel _ { 2 }$ . Setting $\begin{array} { r } { R _ { 1 } \geqslant \frac { C } { \varepsilon } } \end{array}$ , we have $F : \overline { { B } } _ { R _ { 1 } } ( \theta ) $

${ \overline { { B } } } _ { R _ { 1 } } ( \theta )$ . The Schauder fixed-point theorem is applied to ensure the existence of $x _ { 0 } \in B _ { R _ { 1 } } ( \theta )$ satisfying $F ( x _ { 0 } ) = x _ { 0 }$ , thus $x _ { 0 } \in D ( L )$ and satisfies (2.21).

# 3. Obstacle problem

Find the equilibrium position $u$ of a membrane with a fixed boundary, acted upon by an external force $f$ , and obstructed by an fixed obstacle $\psi$ .

The problem is posed as a minimizing problem on a closed convex set. Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded domain, $f \in L ^ { 2 } ( \Omega ) , \ \psi \in H ^ { 1 } ( \Omega )$ . Find $u \in H _ { 0 } ^ { 1 } ( \Omega )$ such that $u$ is the minimizer of the problem: $\operatorname* { m i n } \left\{ J ( v ) | \ v \in E \right\}$ , where

$$
J (v) = \int_ {\Omega} \left(\frac {1}{2} | \nabla v | ^ {2} - f \cdot v\right),
$$

and

$$
E = \left\{v \in H _ {0} ^ {1} (\Omega) \mid v (x) \leqslant \psi (x) \text {a . e .} \right\}.
$$

From $J ( u ) \leqslant J ( u + t ( v - u ) ) \forall v \in E$ and $t \in ( 0 , 1 )$ , and letting $t  0$ , we obtain the following variational inequality:

$$
\int_ {\Omega} \nabla u \cdot \nabla (v - u) \geqslant \int_ {\Omega} f (v - u) \quad \forall v \in E. \tag {2.22}
$$

The existence of a weak solution for (2.22) can be studied by variational method, see Chap. 4. However, we present here a fixed-point approach.

First we reduce the problem to a system of inequalities. One has

$$
u \leqslant \psi \text {a . e .}, \tag {2.23}
$$

Denote the positive cone of $H _ { 0 } ^ { 1 } ( \Omega )$ by $C = \{ w \in H _ { 0 } ^ { 1 } ( \Omega ) |$ $w ( x ) \geqslant 0$ a.e. . Let $v \in H _ { 0 } ^ { 1 } ( \Omega )$ , and $v \leq u$ , then $v \in E$ , and let $w = u - v$ , then (2.22) implies that $\begin{array} { r } { \int _ { \Omega } \nabla u \nabla w \leqslant \int _ { \Omega } f w \ \forall w \in C } \end{array}$ i.e.,

$$
- \triangle u - f \leqslant 0 \text {a . e .}, \tag {2.24}
$$

Substituting $v = \psi$ and $2 u - \psi$ , respectively into (2.22), we obtain

$$
\int_ {\Omega} (- \triangle u - f) \cdot (\psi - u) \geq 0,
$$

and

$$
\int_ {\Omega} (- \triangle u - f) \cdot (u - \psi) \geq 0,
$$

respectively, therefore $\begin{array} { r } { \int _ { \Omega } ( - \triangle u - f ) ( u - \psi ) = 0 } \end{array}$ .

If $u \in W _ { p } ^ { 2 } ( \Omega )$ then,

$$
(- \triangle u - f) (u - \psi) = 0 \text {a . e .} \tag {2.25}
$$

Conversely, from (2.24), (2.25) and $v \leq \psi$ , we obtain $\begin{array} { r } { \int _ { \Omega } ( - \Delta u - f ) ( v - u ) \geq } \end{array}$ 0, i.e., (2.22)

Thus, for $u \in W _ { p } ^ { 2 } \cap H _ { 0 } ^ { 1 } ( \Omega )$ , it is a solution of the variational inequality (2.22) if and only if it satisfies the system of inequalities (2.23), (2.24), (2.25).

Second, the system of inequalities is equivalent to a PDE with discontinuous nonlinearity, i.e., for $\psi \in W _ { p } ^ { 2 } ( \Omega )$ the system of inequalities (2.23), (2.24), (2.25) is again equivalent to

$$
- \triangle u = \left\{ \begin{array}{l l} \min  \left\{f (x), - \triangle \psi (x) \right\} & \text {i f} u (x) \geqslant \psi (x) \\ f (x) & \text {i f} u (x) <   \psi (x). \end{array} \right. \tag {2.26}
$$

Verification:

“=⇒” On the set $\{ x \in \Omega | u ( x ) < \psi ( x ) \}$ the equality follows from (2.25). On the complement set $\{ x \in \Omega | u ( x ) = \psi ( x ) \}$ we have $- \triangle \psi ( x ) = - \varDelta u ( x ) \leqslant f ( x )$ a.e., and then $- \varDelta u ( x ) = - \varDelta \psi ( x ) = \mathrm { m i n } \left\{ f ( x ) , - \triangle \psi ( x ) \right\}$ .

“⇐=” (2.24) is trivial.

If the set $U = \{ x \in \Omega | \ u ( x ) > \psi \} \neq \emptyset$ , (2.26) implies that $- \triangle ( u -$ $\psi ) \leqslant 0$ on $U$ . This contradicts the maximum principle; then (2.23) holds, and consequently (2.25).

Now, (2.26) is a PDE with discontinuous nonlinearity:

$$
- \triangle u = \phi (x, u) \tag {2.27}
$$

where

$$
\phi (x, u) = \left\{ \begin{array}{l l} \min  \left\{f (x), - \triangle \psi (x) \right\} & \text {i f} u \geqslant \psi (x) \\ f (x) & \text {i f} u <   \psi (x)  . \end{array} \right.
$$

We shall solve the equation by the Ky Fan fixed-point theorem.

First, let us define a set-valued mapping

$\begin{array} { r c l } { { } } & { { } } & { { F : C ( \overline { { { \Omega } } } )  2 ^ { L ^ { p } ( \Omega ) } : u \mapsto F u = [ \underline { { { \phi } } } ( x , u ( x ) ) , \overline { { { \phi } } } ( x , u ( x ) ) ] } } \\ { { } } & { { } } & { { \mathrm { i n ~ } L ^ { p } ( \Omega ) ~ , } } \end{array}$ , the order interval

where $\underline { { \phi } } ( x , t ) = \operatorname* { l i m } _ { t ^ { \prime } \to t } \phi ( x , t ^ { \prime } )$ , and $\overline { { \phi } } ( x , t ) = \overline { { \operatorname* { l i m } _ { t ^ { \prime }  t } } } \phi ( x , t ^ { \prime } )$ .

→ We consider the following maps:

$$
C (\overline {{\Omega}}) \xrightarrow {F} 2 ^ {L ^ {p} (\Omega)} \xrightarrow {\mathbb {K}} 2 ^ {W _ {p} ^ {2} \cap H _ {0} ^ {1} (\Omega)} \xrightarrow {j} 2 ^ {C (\overline {{\Omega}})},
$$

where $\mathbb { K } = ( - \triangle ) ^ { - 1 }$ and $j$ is the embedding map.

It is not difficult to verify that $\Gamma : = j \circ \mathbb { K } \circ F$ is u.s.c., and $\forall u \in C ( \overline { { \Omega } } )$ , $\Gamma ( u )$ is a nonempty closed convex set of $C ( { \overline { { \Omega } } } )$ .

Second, we consider the set $\begin{array} { r } { E \ = \ \mathbb { K } ( { \overline { { B } } } _ { R } ( \theta ) ) } \end{array}$ of $C ( { \overline { { \Omega } } } )$ , where $R \ =$ $\operatorname* { m a x } \left\{ \parallel f \parallel _ { p } , \parallel \triangle \psi \parallel _ { p } \right\}$ . It is a nonempty compact convex set in $C ( \Omega )$ , and $\Gamma : E  2 ^ { E }$ . Ky Fan’s fixed-point theorem is applied to ensure the existence of $u _ { 0 } \in E$ such that $u _ { 0 } \in \Gamma ( u _ { 0 } )$ , i.e., $u _ { 0 } \in W _ { p } ^ { 2 } \cap H _ { 0 } ^ { 1 } ( \Omega )$ satisfying

$$
- \triangle u _ {0} (x) \in \left\{ \begin{array}{l l} \min  \{f (x), - \triangle \psi (x) \} & \text {i f} u _ {0} (x) > \psi (x) \\ [ \min  \{f (x), - \triangle \psi (x) \}, f (x) ] & \text {i f} u _ {0} (x) = \psi (x) \\ f (x) & \text {i f} u _ {0} (x) <   \psi (x)  . \end{array} \right.
$$

Third, however, on the set $U = \{ x \in \Omega |$ $u _ { 0 } ( x ) = \psi ( x ) \}$ , $- \triangle u _ { 0 } ( x ) =$ $- \triangle \psi ( x )$ a.e. This implies that $- \triangle \psi ( x ) \leqslant f ( x )$ a.e., i.e.,

$$
- \triangle u _ {0} (x) = \left\{ \begin{array}{l l} \min  \left\{f (x), - \triangle \psi (x) \right\} & \text {i f} u _ {0} (x) \geqslant \psi (x) \\ f (x) & \text {i f} u _ {0} (x) <   \psi (x)  . \end{array} \right.
$$

This is exactly the solution of the PDE with discontinuous nonlinearity (2.26).

Statement 2.3.15 If $\psi \in W _ { p } ^ { 2 } ( \Omega )$ , $f \in L ^ { p } ( \Omega ) , p \mathrm { ~ > ~ } \frac { n } { 2 }$ , then there exists 。 $u \in W _ { p } ^ { 2 } \cap W _ { p } ^ { 1 } ( \Omega )$ satisfying (2.26).

The obstacle problem is a free boundary problem for PDEs. The boundary where the membrane detaches from the obstacle is unknown, and is to be determined simultaneously with the position of the membrane. A kind of free boundary problem, having the feature that the boundary values and the normal derivatives of the unknown function coincide on both sides of the unknown free boundary, can be studied via the above approach, i.e., pose the free boundary problem as a PDE with discontinuous nonlinearity, in which the unknown boundary is automatically absorbed into the unknown function. Fixed-point theorems for set-valued mappings can be used to study the existence of the solutions. Examples of the obstacle problem are the seepage surface in dams, the free boundary of confined plasma, water cones in oil reservoirs etc. (see K.C. Chang [Ch 1,2]).

# 4. Stefan problem

Consider a container $\Omega$ with boundaries $\Gamma ^ { \prime }$ and $\Gamma ^ { \prime \prime }$ , filled with ice. Cooling on $\Gamma ^ { \prime \prime }$ and heating on $\Gamma ^ { \prime }$ (see Fig. 2.4), we study the melting process.

![](images/b5d5715be327acded3812cfebbe1562f3d878f5bd760415f981232432727b8aa.jpg)  
Fig. 2.4.

At time $t$ , the domain occupied by water is denoted by $C ( t )$ , and let $\theta ( x , t ) \geqslant 0$ be the temperature at the space–time $( x , t )$ as $x \in C ( t )$ . Both $\theta$ and $C$ are unknown, but $g ( x , t ) = \theta ( x , t ) | _ { \Gamma ^ { \prime } \times [ 0 , T ] }$ is given $\forall T \ > \ 0$ . Let $S ( t ) = \partial { \cal { C } } ( t )$ be the unknown boundary. Let us suppose that $S ( t )$ is given by an equation $\sigma ( x ) = t$ . The temperature distribution is governed by the following equations:

$$
\left\{ \begin{array}{l l} \frac {\partial}{\partial t} \theta - \triangle \theta = 0 & \forall t \in [ 0, T ], \forall x \in C (t), \\ \theta (x, t) | _ {\Gamma^ {\prime} \times [ 0, T ]} = g (x, t), \\ \theta (x, t) | _ {S (t)} = 0 & \forall t \in [ 0, T ], \\ \nabla \theta \cdot \nabla \sigma | _ {S (t)} = - L, \\ \theta (x, 0) = 0 & \forall x \in \Omega , \end{array} \right. \tag {2.28}
$$

where $L ~ > ~ 0$ is a constant. The boundary conditions on $S ( t )$ are derived from the continuity of the temperature and the conservation of the energy, respectively.

We set

$$
\widetilde {\theta} (x, t) = \left\{ \begin{array}{l l} \theta (x, t) & \text {i f} x \in C (t) \\ 0 & \text {i f} x \not \in C (t)  , \end{array} \right.
$$

and

$$
u (x, t) = \int_ {0} ^ {t} \widetilde {\theta} (x, s) d s.
$$

The latter is called the modified Baiocchi transformation of $\theta$ . Let

$$
\psi (x, t) = \int_ {0} ^ {t} g (x, s) d s.
$$

The problem (2.28) is reduced to a parabolic equation with discontinuous nonlinearity:

$$
\left\{ \begin{array}{l l} \frac {\partial u}{\partial t} - \triangle u = - L H (u) & \text {i n} Q _ {T} := \Omega \times (0, T), \\ u | _ {\Gamma^ {\prime} \times [ 0, T ]} = \psi , \\ u | _ {\Gamma^ {\prime \prime} \times [ 0, T ]} = 0, \\ u (x, 0) = 0, & \text {o n} \Omega . \end{array} \right. \tag {2.29}
$$

Again, the advantage of the formulation (2.29) is that the unknown domain $C ( t )$ is implicitly involved. The price we paid is the nonlinear term being a discontinuous nonlinear function: the Heaviside function $H ( u ) = 1$ as $u > 0$ , and = 0 as $u \leqslant 0$ .

The problem can be solved by the Ky Fan fixed-point theorem as before. Let us introduce the anisotropic Sobolev space

$$
W _ {p} ^ {1, 2} (Q _ {T}) = \left\{u \in L ^ {p} (Q _ {T}) | \partial_ {t} u, \nabla_ {x} u, \nabla_ {x} ^ {2} u \in L ^ {p} (Q _ {T}) \right\},
$$

with norm:

$$
\parallel u \parallel_ {W _ {p} ^ {1, 2}} = \parallel u \parallel_ {p} + \parallel \partial_ {t} u \parallel_ {p} + \parallel \nabla_ {x} ^ {2} u \parallel_ {p}.
$$

The boundary values of functions in $W _ { p } ^ { 1 , 2 } ( Q _ { T } )$ span a fractional Sobolev space $W _ { p } ^ { 1 - \frac { 1 } { 2 p } , 2 - \frac { 1 } { p } } ( \Gamma \times ( 0 , T ) )$ , where $\Gamma = \Gamma ^ { \prime } \cup \Gamma ^ { \prime \prime }$ . In particular, $C ^ { 1 , 2 } ( \Gamma \times$ $[ 0 , T ] ) = \{ u \in C ( \Gamma \times [ 0 , T ] ) | \partial _ { t } u$ , $\nabla _ { x } u \ : \nabla _ { x } ^ { 2 } u \in C ( \Gamma \times [ 0 , T ] ) \} \subset W _ { p } ^ { 1 - \frac { 1 } { 2 p } , 2 - \frac { 1 } { p } } ( \Gamma \times$ $( 0 , T ) \}$ ).

Statement 2.3.16 Assume $\psi \in W _ { p } ^ { 1 - \frac { 1 } { 2 p } , 2 - \frac { 1 } { p } } ( \Gamma \times ( 0 , T ) )$ − 12p ,2− 1p 1 , p > n, and $\psi \geqslant 0$ ; ∈ the equation (2.29) has a unique solution $u \in W _ { p } ^ { 1 , 2 } ( Q _ { T } )$ .

Proof. Define a set-valued mapping

$$
u \mapsto F (u) = \left\{ \begin{array}{l l} 1 & u (x, t) > 0  , \\ {[ 0, 1 ]} & u (x, t) = 0  , \\ 0 & u (x, t) <   0  , \end{array} \right.
$$

then ${ \cal F } : { \cal C } ( \overline { { { Q } } } _ { T } )  2 ^ { L ^ { p } ( Q _ { T } ) }$ is u.s.c. Let $\mathbb { K } _ { \psi } : L ^ { p } ( Q _ { T } ) \mapsto W _ { p } ^ { 1 , 2 } ( Q _ { T } )$ be the affine operator $f \mapsto u$ , where $u$ satisfies the linear parabolic equation:

$$
\left\{ \begin{array}{l l} \frac {\partial u}{\partial t} - \triangle u = f & \text {i n} Q _ {T}  , \\ u | _ {\Gamma^ {\prime} \times (0, T)} = \psi  , \\ u | _ {\Gamma^ {\prime \prime} \times (0, T)} = 0, \\ u (x, 0) = 0, & \text {o n} \Omega  . \end{array} \right.
$$

Then the set-valued mapping $\Gamma : = \mathbb { K } _ { \psi } \circ ( L \circ F ) : C ( \overline { { \Omega } } ) \to 2 ^ { C ( \overline { { \Omega } } ) }$ is a u.s.c. closed convex set-valued mapping. Define a compact convex set:

$$
E = \left\{\mathbb {K} _ {\psi} u \mid u \in L ^ {p} \left(Q _ {T}\right), \| u \| _ {p} \leqslant \operatorname {L m} (\Omega) ^ {\frac {1}{p}} \right\}.
$$

Obviously $\Gamma : E  2 ^ { E }$ . According to Ky Fan’s fixed-point theorem, one finds $u \in W _ { p } ^ { 1 , 2 } ( Q _ { T } )$ satisfying:

$$
\left\{ \begin{array}{l} \frac {\partial u}{\partial t} - \triangle u \in - L F (u) \\ u | _ {\Gamma^ {\prime} \times (0, T)} = \psi ,   u | _ {\Gamma^ {\prime \prime} \times (0, T)} = 0,   u | _ {\Omega \times \{0 \}} = 0  . \end{array} \right.
$$

On the set $u ^ { - 1 } ( 0 ) = \{ ( x , t ) \in Q _ { T } | \ u ( x , t ) = 0 \}$ , we have $\begin{array} { r } { \frac { \partial u } { \partial t } - \triangle u = 0 } \end{array}$ . This means $u$ automatically satisfies (2.29).

Now, we prove the uniqueness of the solution $u$ of equation (2.29). In fact, let $u _ { 1 }$ and $u _ { 2 }$ be two solutions of (2.29), and $u = u _ { 1 } - u _ { 2 }$ , then

$$
\left\{ \begin{array}{l} \frac {\partial u}{\partial t} - \triangle u = - L (H (u _ {1}) - H (u _ {2})) \quad \text {i n} Q _ {T}, \\ u | _ {\partial Q _ {T} \setminus (\Omega \times \{T \})} = 0. \end{array} \right.
$$

By the maximum principle for parabolic equations, both sets $\{ ( x , t ) \in$ $Q _ { T } | \pm u ( x , t ) > 0 \}$ are null sets. Thus $u _ { 1 } = u _ { 2 }$ a.e.

Returning to the original problem (2.28), we define $C ( t ) = \{ x \in \Omega | u ( x , t ) >$ $0 \}$ , $\begin{array} { r } { \tilde { \theta } ( x , t ) = \frac { \partial } { \partial t } u ( x , t ) } \end{array}$ , and $\theta ( \cdot , t ) = \widetilde { \theta } | _ { C ( t ) } \ \forall t \in ( 0 , T )$ .

The verification of (2.28) is omitted.

# 2.4 Nonexpansive Maps

Let $X$ be a Banach space, and $E$ be a closed subset of $X$ . A map $T : E  X$ is called a nonexpansive map, if

$$
\| T (x) - T (y) \| \leqslant \| x - y \| \quad \forall x, y \in E.
$$

In contrast with the fact that a contraction mapping $T : E  E$ , (i.e., $\exists \alpha \in ( 0 , 1 )$ such that

$$
\| T (x) - T (y) \| \leqslant \alpha \| x - y \|)
$$

always has a fixed point, the nonexpansive map does not in general. This can be seen by the following example:

Example 1. $X = c _ { 0 }$ , the space consisting of all sequences $x = \{ \xi _ { i } \} _ { 1 } ^ { \infty }$ converging to zero, with the norm

$$
\parallel x \parallel = \max  _ {i \geqslant 1} \left| \xi_ {i} \right|.
$$

Let

$$
T (x) = \left(1 - \| x \|, \xi_ {1}, \xi_ {2}, \dots\right),
$$

and $E$ be the closed unit ball. Then $T : E  E$ is non-expansive:

$$
\| T (x) - T (y) \| = \max  \left\{\left| \| x \| - \| y \| \right|, \quad \max  _ {i \geqslant 1} \left| \xi_ {i} - \eta_ {i} \right| \right\} \leqslant \| x - y \| .
$$

However, $T$ does not have a fixed point. In fact, if $x = T ( x )$ , then $\xi _ { 1 } = \xi _ { 2 } =$ $\dots = 1 - \parallel x \parallel$ . This is impossible.

We are going to study the fixed points for a nonexpansive map on a closed convex subset of a Banach space $X$ with a normal structure, which means that for every closed convex subset $E$ of $X$ containing at least two points, there exists a point $x _ { 0 } \in E$ such that

$$
\sup  _ {x \in E} \| x - x _ {0} \| <   \operatorname {d i a m} (E). \tag {2.30}
$$

It is easy to see that Hilbert spaces are Banach spaces with normal structure: Assume $a , b \in E$ , then $\forall x \in E$ , by parallelogram identity,

$$
\left\| x - \frac {a + b}{2} \right\| ^ {2} + \left\| \frac {a - b}{2} \right\| ^ {2} = \frac {1}{2} (\| x - a \| ^ {2} + \| x - b \| ^ {2}) \leqslant \operatorname {d i a m} (E).
$$

Taking $\begin{array} { r } { x _ { 0 } = \frac { 1 } { 2 } ( a + b ) } \end{array}$ , (2.30), is satisfied.

Now, for any closed bounded convex subset $E$ , one defines a number

$$
r_{E} = \inf_{x\in E}\sup_{y\in E}\left\| x - y\right\| .
$$

By definition, $r _ { E } \ \leqslant \ \mathrm { d i a m } \left( E \right)$ . If further, $X$ is a Banach space with a normal structure, then $r _ { E } < \mathrm { d i a m } ( E )$ .

Lemma 2.4.1 If $X$ is a reflexive Banach space and if $E$ is a bounded closed convex set, then $\exists x _ { 0 } \in E$ such that

$$
\sup  _ {y \in E} \| x _ {0} - y \| = r _ {E}.
$$

Proof. The function f (x) = sup  x − y  is convex, l.s.c., and coercive. We $f ( x ) = \operatorname* { s u p } _ { y \in E } \parallel x - y \parallel$ now apply Corollary 2.3.7; there exists $x _ { 0 } \in E$ such that $f ( x _ { 0 } ) = \operatorname* { m i n } _ { x \in E }$ $f ( x )$ . □

Setting $G ( y ) = \left\{ x \in E | \ \parallel y - x \parallel \leqslant r _ { E } \right\} \forall y \in E$ , we define

$$
\operatorname {c t} \left(E\right) = \bigcap_ {y \in E} G (y)  .
$$

Provided by Lemma 2.4.1, $x _ { 0 } \in \operatorname { c t } \left( E \right)$ . Therefore $\operatorname { c t } \left( E \right)$ is a nonempty closed convex subset of $E$ . It is called the center of $E$ . By definition, we have

$$
\sup  _ {x, y \in c t (E)} \| x - y \| \leq r _ {E}. \tag {2.31}
$$

Theorem 2.4.2 Suppose that $E$ is a bounded closed convex set of a reflexive Banach space with a normal structure, and that $T : E  E$ is nonexpansive, then the fixed-point set of $T \operatorname { F i x } \left( T \right)$ is closed and nonempty.

Proof. (1) Let $\mathsf { X }$ be the set of all nonempty closed convex subsets of $E$ . Define the partial order $\leqslant$ on $\mathsf { X }$ by inclusion:

$$
C _ {1} \leqslant C _ {2} \text {i f} C _ {2} \subset C _ {1} \quad \forall C _ {1}, C _ {2} \in X.
$$

Then $( \mathsf { X } , \leqslant )$ is an ordered set, in which every chain $\mathsf { X } _ { 1 }$ has a supremum: $\smash { \underset { C \in \mathbf { X } _ { 1 } } { \bigcap } C }$ . The intersection is not empty, because $E$ is weakly compact and $C$ is $C { \in } { \mathsf { X } } _ { 1 }$ weakly closed.

(2) Define

$$
f (C) = \operatorname {c t} \left(\overline {{\operatorname {c o n v}}} (T (C))\right),
$$

then $f : \mathsf { X } \to \mathsf { X }$ is order preserving and $E \leq f ( E )$ . Applying the Amann theorem, $f$ has a fixed point $C _ { 0 } \in \mathsf X$ , i.e., $C _ { 0 } \subset E$ is a nonempty closed convex $T$ -invariant set satisfying:

$$
C _ {0} = \operatorname {c t} (\overline {{\operatorname {c o n v}}} (T (C _ {0}))).
$$

(3) It remains to verify that $C _ { 0 }$ consists of a single point. If not, from the fact that $r _ { C } < \mathrm { d i a m } ( C )$ for all bounded closed convex sets $C$ containing more than one point, we obtain:

$$
\operatorname {d i a m} \left(C _ {0}\right) = \operatorname {d i a m} \left(\operatorname {c t} \left(\overline {{\operatorname {c o n v}}} (T \left(C _ {0}\right))\right)\right)
$$

$$
\begin{array}{l} = \sup  _ {x, y \in \operatorname {c t} (\overline {{\operatorname {c o n v}}} (T (C _ {0})))} \| x - y \| \\ \leq r _ {\overline {{\mathrm {c o n v}}} (T (C _ {0}))} \\ <   \operatorname {d i a m} (\overline {{\operatorname {c o n v}}} (T (C _ {0}))) \\ = \operatorname {d i a m} \left(T \left(C _ {0}\right)\right) \\ \leqslant \operatorname {d i a m} \left(C _ {0}\right). \\ \end{array}
$$

This is a contradiction.

Thus, $C _ { 0 }$ is a single-point set in $E$ , say $C _ { 0 } = \{ x _ { 0 } \}$ . It follows that

$$
x _ {0} = f \left(\left\{x _ {0} \right\}\right) = c t (\overline {{\operatorname {c o n v}}} \left(T x _ {0}\right)) = T x _ {0}.
$$

Obviously $\operatorname { F i x } \left( T \right)$ is closed.

Remark 2.4.3 A Banach space $X$ is called uniformly convex, if ε > 0 δ > 0 such that $\forall x , y \in X \quad \parallel \ x \parallel , \parallel y \parallel \leqslant 1 , \parallel x - y \parallel \geqslant \varepsilon$ $\|  { x - y } \| \geqslant \varepsilon$ implies that $\parallel$ ${ \frac { x + y } { 2 } } \parallel \leqslant 1 - \delta$ ∀ ∈      −  . Obviously, a Hilbert space is a uniformly convex Banach space. Moreover, $L ^ { p } ( \Omega , B , \mu ) , \ 1 < p < + \infty$ are also, but $C ( \Omega )$ is not.

It is known [DS] that a uniformly convex Banach space is a reflexive Banach space with normal structure. One has the following:

Theorem 2.4.4 (Browder–G¨ohde) Suppose that $X$ is a uniformly convex Banach space, and that $E$ is a nonempty bounded closed convex set of $X$ . If $T : E  E$ is nonexpansive, then $T$ has a fixed point.

A natural question: can we find out a fixed point of a nonexpansive map $T$ by iteration as we did for contraction mappings?

Starting from any point $x _ { 0 } \in E$ , set $x _ { i } = T ^ { i } x _ { 0 }$ , i, from

$$
\left\| x _ {i + 1} - x _ {i} \right\| \leqslant \left\| x _ {i} - x _ {i - 1} \right\| \leqslant \dots \leqslant \left\| x _ {1} - x _ {0} \right\|,
$$

we know that $\{ \| \ x _ { i + 1 } - x _ { i } \| \} _ { i = 0 } ^ { \infty }$ is a nonincreasing sequence. It is obviously true that even if $\{ x _ { i } \}$ has a convergent subsequence $x _ { n _ { i } } \to x ^ { * }$ , we still cannot conclude that $x ^ { * }$ is a fixed point of $T$ , e.g., $T x = - x$ , and $x _ { 0 } \neq \theta$ .

To this end, Mann [Man] introduced an iteration method for the following perturbed mappings: $\forall \alpha \in ( 0 , 1 )$ , set

$$
T _ {\alpha} x = \alpha x + (1 - \alpha) T x.
$$

One easily verifies that if $E$ is convex and $T : E  E$ is nonexpansive then:

(1) $T _ { \alpha } : E \to E$ .   
(2) $T _ { \alpha }$ is nonexpansive.   
(3) $T _ { \alpha }$ shares the same fixed point set with $T$ .   
(4) Let $X$ be a uniformly convex Banach space, and $E$ be a bounded closed convex subset of $X$ . Let $p$ be the fixed point of $T$ . If further, $I - T$ is proper (a map $f$ is called proper if the inverse image of any compact set $C , f ^ { - 1 } ( C )$ is compact), then $\forall \alpha \in ( 0 , 1 )$ , $\parallel T _ { \alpha } ^ { i } x { - p } \parallel \to 0$ as $i  \infty \forall x \in E$ .

The proofs of (1)–(3) are trivial. In order to give a proof of (4). Let $z _ { i } =$ $T _ { \alpha } ^ { i } x$ . Since the sequence $\lVert z _ { i + 1 } - z _ { i } \rVert$ is nonincreasing, either (1) $\exists \epsilon > 0$ such that $\| z _ { i + 1 } - z _ { i } \| \geq \epsilon$ , or (2) $\| z _ { i + 1 } - z _ { i } \| \to 0$ .

But case (1) is impossible. In fact, from the definition of the uniform convexity $\exists \delta _ { 0 } = \delta ( \alpha , \epsilon ) > 0$ , such that

$$
\begin{array}{l} \parallel z _ {i + 1} - p \parallel = \parallel \alpha (z _ {i} - p) + (1 - \alpha) (T z _ {i} - T p) \parallel \\ \leqslant \left(1 - \delta_ {0}\right) \max  \left\{\| z _ {i} - p \|, \| T z _ {i} - T p \| \right\} \\ = \beta \| z _ {i} - p \| \\ \leqslant \dots \\ \leqslant \beta^ {i} \| x - p \|, \\ \end{array}
$$

where $\beta = 1 - \delta _ { 0 }$ . This proves that $z _ { i }  p$ , a contradiction. In case (2), by the assumption that $I - T$ is proper, there exists a convergent subsequence $z _ { n _ { i } }  p$ . It follows that $p = T p$ , i.e., $p \in \operatorname { F i x } \left( T \right) = \operatorname { F i x } \left( T _ { \alpha } \right)$ . Thus $\| z _ { n } - p \| \leq$ $\| z _ { n _ { i } } - p \| \to 0$ as $n \geq n _ { i } \to \infty$ .

We have thus proved:

Theorem 2.4.5 Assume that $X$ is a uniformly convex Banach space, and that $E$ is a nonempty bounded closed convex set of $X$ . If $T : E  E$ is nonexpansive and $I - T$ is proper, $\forall \alpha \in ( 0 , 1 )$ , let $T _ { \alpha } = \alpha I + ( 1 - \alpha ) T$ , then $\forall x \in E$ , the sequence $T _ { \alpha } ^ { i } x$ converges to a fixed point of $T$ .

# Applications

(1) The iteration method is an effective computing method in the convex feasibility problem:

Given a family of closed convex sets $\{ E _ { i } | \ i = 1 , \ldots , n \}$ in a Hilbert space $H$ , find a point $x \in \bigcap _ { i = 1 } ^ { n } E _ { i }$ , if the intersection is nonempty.

In fact, given a closed convex set $E$ in $H$ , $\forall x \in H$ , there is a projection onto $E$ : $P _ { E } x \in E$ , which is defined by the minimizer of the problem:

$$
c = \min _ {y \in E} \left\| x - y \right\|.
$$

The existence of $P _ { E } x$ follows from the parallelogram identity:

$$
\parallel y _ {n} - y _ {m} \parallel^ {2} = 2 (\parallel x - y _ {n} \parallel^ {2} + \parallel x - y _ {m} \parallel^ {2}) - 4 \left\| x - \frac {y _ {n} + y _ {m}}{2} \right\| ^ {2},
$$

where $\left\{ y _ { n } \right\}$ is a minimizing sequence: $\parallel x - y _ { n } \parallel \to c$ . Again by the parallelogram identity, the minimizer is unique. We define the minimizer $y ^ { * }$ to be the projection $P _ { E } x$ .

The projection can be characterized by the following variational inequality:

$$
\left(x - P _ {E} x, P _ {E} y - P _ {E} x\right) \leqslant 0 \quad \forall x, y \in H. \tag {2.32}
$$

![](images/68c2717e4fe453c1eea4a10d7b39aece20116955481d3d1a1eae3ff1b4b03a38.jpg)  
Fig. 2.5.

In fact, $\forall t \in ( 0 , 1 )$ , $t P _ { E } y + ( 1 - t ) P _ { E } x \in E$ . One has

$$
\left\| x - t P _ {E} y - (1 - t) P _ {E} x \right\| ^ {2} \geqslant \left\| x - P _ {E} x \right\| ^ {2}.
$$

i.e.,

$$
- 2 t \left(x - P _ {E} x, P _ {E} y - P _ {E} x\right) + t ^ {2} \| P _ {E} y - P _ {E} x \| ^ {2} \geqslant 0.
$$

Letting $t \to 0$ , we obtain the desired result (2.32).

Statement 2.4.6 $P _ { E }$ is a nonexpansive map on $H$ .

Proof. By definition, $\forall x , y \in H$

$$
\left(x - P _ {E} x, P _ {E} y - P _ {E} x\right) \leqslant 0,
$$

$$
\left(y - P _ {E} y, P _ {E} x - P _ {E} y\right) \leqslant 0.
$$

Summing the two inequalities:

$$
(x - y + \left(P _ {E} y - P _ {E} x\right), P _ {E} y - P _ {E} x) \leqslant 0,
$$

it follows that

$$
\left\| P _ {E} y - P _ {E} x \right\| ^ {2} \leqslant (y - x, P _ {E} y - P _ {E} x) \leqslant \| x - y \| \| P _ {E} y - P _ {E} x \| .
$$

Therefore $P _ { E }$ is nonexpansive.

One may use Mann iteration to compute a feasible solution. Various algorithms have been introduced to improve the convergent rate and the computation.

(2) Periodic solution for nonlinear evolution equations:

Let $X$ be a real Hilbert space. Given $\varphi \in X$ , and $f : \mathbb { R } _ { + } ^ { 1 } \times X \to X$ . We consider the following nonlinear evolution equation: Find $x \in C ^ { 1 } ( \mathbb { R } _ { + } ^ { 1 } , X )$ satisfying

$$
\dot {x} (t) = f (t, x (t)) \quad t \in \mathbb {R} _ {+} ^ {1}, \tag {2.33}
$$

$$
x (0) = \varphi . \tag {2.34}
$$

Statement 2.4.7 Assume $\omega > 0$ and that

(1) $f$ is $\omega$ -periodic in $t$ , i.e., $f ( t , x ) = f ( t + \omega , x )$ ,   
(2) $( f ( t , x ) - f ( t , y ) , x - y ) \leqslant 0 \forall t \in \mathbb { R } _ { + } ^ { 1 }$ , $\forall x , y \in X$   
(3) $\exists R > 0$ such that $( f ( t , x ) , x ) < 0 , \ \forall t \in [ 0 , \omega ] , \ \forall x \in \partial B _ { R } ( \theta )$ $\forall x \in \partial B _ { R } ( \theta )$ ,   
(4) $\forall \varphi \in \overline { { B } } _ { R } ( { \theta } )$ , the initial-value problem (2.33) (2.34) has a solution.

Then (2.33) has a unique $\omega$ -periodic solution, i.e., $x ( t + \omega ) = x ( t )$ .

Proof. From (4), we consider the Poincar´e map on $X$ :

$$
T: x (0) \mapsto x (\omega),
$$

and want to show that $T$ has a unique fixed point.

First we show that $T : \overline { { { B } } } _ { R } ( \theta )  \overline { { { B } } } _ { R } ( \theta )$ is a well-defined nonexpansive map:

1. We show that the solution for the initial-value problem is unique, i.e., if $x ( t )$ , $y ( t )$ are solutions with the same initial data $\varphi$ , then $x ( t ) = y ( t )$ . Indeed, from

$$
\begin{array}{l} \frac {d}{d t} \| x (t) - y (t) \| ^ {2} = 2 \left(x ^ {\prime} (t) - y ^ {\prime} (t), x (t) - y (t)\right) \\ = 2 (f (t, x (t)) - f (t, y (t)), x (t) - y (t)) \\ \leqslant 0, \\ \end{array}
$$

it follows that

$$
\left\| x (t) - y (t) \right\| \leqslant \left\| x (0) - y (0) \right\|. \tag {2.35}
$$

Thus $x ( t ) = y ( t )$ , and then $T$ is well defined.

2. Moreover, (2.35) implies that $T$ is nonexpansive.

3. We turn to showing that $T$ maps $\overline { { B } } _ { R } ( \theta )$ into itself. Because of assumption (3)

$$
\frac {d}{d t} \parallel x \parallel^ {2} = 2 (x ^ {\prime} (t), x (t)) = 2 (f (t, x (t)), x (t)) <   0,
$$

whenever $x ( t ) \in \partial B _ { R } ( \theta )$ . If the conclusion is not true, then $\exists t \ > \ 0$ such that $x ( \underline { { t } } ) \notin B _ { R } ( \theta )$ and then $\exists t _ { 0 } ~ \in ~ ( 0 , t )$ , such that $x ( t _ { 0 } ) ~ \in ~ \partial B _ { R } ( \theta )$ and $x ( t ) \notin B _ { R } ( \theta )$ as $t _ { 0 } < t < t _ { 0 } + \delta$ for some $\delta > 0$ . But this contradicts the differential inequality.

Now, we apply Theorem 2.4.2; there is a fixed point of $T$ in ${ \overline { { B } } } _ { R } ( \theta )$ . Again applying step 1, the $\omega$ -periodic solution is also unique. □

# 2.5 Monotone Mappings

Let us consider the subdifferential of a convex function $f$ on a vector space $X \colon \forall x , y \in \operatorname { d o m } ( f ) , \ \forall x ^ { * } \in \partial f ( x ) , \forall y ^ { * } \in \partial f ($ $\forall x ^ { * } \in \partial f ( x )$ $\forall y ^ { * } \in \partial f ( y ^ { * } )$ , we have

$$
\langle x ^ {*}, y - x \rangle + f (x) \leqslant f (y),
$$

$$
\langle y ^ {*}, x - y \rangle + f (y) \leqslant f (x).
$$

By addition,

$$
\langle y ^ {*} - x ^ {*}, y - x \rangle \geqslant 0 \tag {2.36}
$$

This is what we call the monotonicity of $\partial f$ .

Definition 2.5.1 Suppose that $X$ is a real Banach space and that $E \subset X$ is a nonempty subset. A set-valued mapping $A : E \to 2 ^ { X ^ { * } }$ is called monotone if $\forall x , y \in E , \forall x ^ { * } \in A ( x ) , \forall y ^ { * } \in A ( y )$ $\forall y ^ { * } \in A ( y )$ , one has

$$
\langle y ^ {*} - x ^ {*}, y - x \rangle \geqslant 0.
$$

The set $\{ x \in E | A ( x ) \neq \varnothing \}$ is called the domain of $A$ , denoted by $D ( A )$ , and the set $\Gamma _ { A } = \{ ( x , x ^ { * } ) \in E \times X ^ { * } |$ $x \in D ( A )$ , $x ^ { * } \in A ( x ) \}$ is called the graph of A. A single-valued monotone mapping is called a monotone operator.

Example 1. Suppose that $X$ is a real Hilbert space, and that $A$ is a linear positive operator, i.e., $\forall x \in D ( A )$ , $( A x , x ) \geqslant 0$ . Then $A$ is a monotone operator.

Example 2. Suppose that $X$ is a real Hilbert space, and that $T$ is a nonexpansive map, then $A = \operatorname { i d } - T$ is a monotone operator:

$$
\left(A y - A x, y - x\right) = \parallel y - x \parallel^ {2} - \left(T y - T x, y - x\right) \geqslant 0.
$$

Example 3. Let $X$ be a real Banach space, and let $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ b e convex, then $\partial f$ is a monotone mapping.

Example 4. ( $p$ -Laplacian) Let $\Omega$ be a bounded domain of $\mathbb { R } ^ { n }$ . For $1 < p < \infty$ , the operator

$$
A u = - \operatorname {d i v} \left(| \nabla u | ^ {p - 2} \nabla u\right) = - \sum_ {i = 1} ^ {n} \frac {\partial}{\partial x _ {i}} \left(\left[ \sum_ {j = 1} ^ {n} \left(\frac {\partial u}{\partial x _ {j}}\right) ^ {2} \right] ^ {\frac {p - 2}{2}} \frac {\partial u}{\partial x _ {i}}\right)
$$

defines a map from $W _ { p } ^ { \mathrm { 1 } } ( \Omega )$ to $W _ { p ^ { \prime } } ^ { - 1 } ( \Omega )$ , $\begin{array} { r } { \frac { 1 } { p } + \frac { 1 } { p ^ { \prime } } = 1 } \end{array}$ , as follows:

$$
\langle A u, v \rangle = \sum_ {i = 1} ^ {n} \int_ {\Omega} \left[ \sum_ {j = 1} ^ {n} \left(\frac {\partial u}{\partial x _ {j}}\right) ^ {2} \right] ^ {\frac {p - 2}{2}} \frac {\partial u}{\partial x _ {i}} \frac {\partial v}{\partial x _ {j}} d x \quad \forall v \in \overset {\circ} {W} _ {p} ^ {1} (\Omega).
$$

In fact, by the H¨older inequality:

$$
\int_ {\Omega} | \nabla u | ^ {p - 1} | \nabla v | \leqslant \left(\int_ {\Omega} | \nabla u | ^ {p}\right) ^ {\frac {1}{p ^ {\prime}}} \left(\int_ {\Omega} | \nabla v | ^ {p}\right) ^ {\frac {1}{p}},
$$

$$
A u \in (\stackrel {\circ} {W _ {p} ^ {1}} (\Omega)) ^ {*} = W _ {p ^ {\prime}} ^ {- 1} (\Omega).
$$

We verify that $A$ is monotone: According to the elementary inequalities:

$$
(| b | ^ {p - 2} b - | a | ^ {p - 2} a) (b - a) \geqslant c _ {p} \left\{ \begin{array}{l l} | b - a | ^ {p} & \text {i f} p \geqslant 2  , \\ (1 + | b | + | a |) ^ {p - 2} | b - a | ^ {2} & \text {i f} 1 <   p <   2  , \end{array} \right.
$$

where $c _ { p } > 0$ is a constant, and $a , b \in \mathbb { R } ^ { n }$ , and the H¨older inequality, we obtain

$$
\begin{array}{l} \langle A u - A v, u - v \rangle \\ \geqslant c _ {p} \left\{ \begin{array}{l l} \int_ {\Omega} | \nabla u - \nabla v | ^ {p} & \text {i f} p \geqslant 2  , \\ (\int_ {\Omega} (1 + | \nabla u | + | \nabla v |) ^ {p}) ^ {1 - \frac {2}{p}} \big (\int_ {\Omega} | \nabla u - \nabla v | ^ {p} \big) ^ {\frac {2}{p}} & \text {i f} 1 <   p <   2  . \end{array} \right. \\ \end{array}
$$

The requirement of the continuity for monotone operators is very weak.

Definition 2.5.2 Let $X$ be a real Banach space and let $E \subset X$ be a nonempty subset. A map $A : E \to X ^ { * }$ is called hemi-continuous at $x _ { 0 } \in E$ , if $\forall y \in X$ , $\forall t _ { n } \downarrow 0$ with $x _ { 0 } + t _ { n } y \in E$ , imply that $A ( x _ { 0 } + t _ { n } y ) \stackrel { * } {  } A ( x _ { 0 } )$ . It is called demicontinuous at $x _ { 0 } \in E$ , if $\forall \{ x _ { n } \} \subset E$ , $x _ { n } \to x _ { 0 }$ implies that $A ( x _ { n } ) \stackrel { * } {  } A ( x _ { 0 } )$ , where $* $ is the $w ^ { * }$ -convergence.

Obviously,

$$
\text {c o n t i n u o u s} ^ {\prime \prime} \Longrightarrow \text {d e m i - c o n t i n u o u s} ^ {\prime \prime} \Longrightarrow \text {h e m i - c o n t i n u o u s} ^ {\prime \prime}.
$$

The monotone operator in Example 4 is hemi-continuous. In fact, $\forall u , v , w \in$ $W _ { p } ^ { \mathrm { 1 } } ( \Omega )$ , we consider the function $t \mapsto \langle A ( ( 1 - t ) x + t y ) , w \rangle$ , and verify the continuity. Now

$$
\begin{array}{l} \langle A ((1 - t) u + t v), w \rangle = \int_ {\Omega} \left[ \sum_ {j = 1} ^ {n} \left((1 - t) \frac {\partial u}{\partial x _ {j}} + t \frac {\partial v}{\partial x _ {j}}\right) ^ {2} \right] ^ {\frac {p - 2}{2}} \\ \times \sum_ {i = 1} ^ {n} \left((1 - t) \frac {\partial u}{\partial x _ {i}} + t \frac {\partial v}{\partial x _ {i}}\right) \frac {\partial w}{\partial x _ {i}}. \\ \end{array}
$$

Since the integrand on the RHS is dominated by an integrable function $( | \nabla u | +$ $| \nabla \boldsymbol { v } | ) ^ { p - 1 } | \nabla \boldsymbol { w } |$ , the Lebesgue dominance theorem is applied.

An important property for monotone operators reads as:

Lemma 2.5.3 Let $E$ be a convex subset of a real Banach space $X$ . If A : $E \to X ^ { * }$ is hemi-continuous and monotone, then for any sequence $\{ x _ { j } \} \subset E$ with $x _ { j } \to x \in E$ and ${ \overline { { \operatorname* { l i m } } } } \langle A ( x _ { j } ) , x _ { j } - x \rangle \leqslant 0$ , we have

$$
\underline {{\lim }} \langle A (x _ {j}), x _ {j} - y \rangle \geqslant \langle A (x), x - y \rangle \quad \forall y \in E.
$$

Proof. First we claim that

$$
\lim  \langle A (x _ {j}), x _ {j} - x \rangle = 0. \tag {2.37}
$$

Indeed, since $A$ is monotone, it follows that

$$
0 = \varlimsup_ {x} \langle A (x), x _ {j} - x \rangle \leqslant \varliminf_ {x} \langle A (x _ {j}), x _ {j} - x \rangle \leqslant \varlimsup_ {x} \langle A (x _ {j}), x _ {j} - x \rangle \leqslant 0.
$$

Again by the monotonicity and (2.37), $\forall z \in E$

$$
\underline {{\lim }} \langle A (x _ {j}), x - z \rangle = \underline {{\lim }} \langle A (x _ {j}), x _ {j} - z \rangle \geqslant \lim  \langle A (z), x _ {j} - z \rangle = \langle A (z), x - z \rangle . \tag {2.38}
$$

Now, $\forall y \in E$ , $\forall t _ { n } \mathrm { ~ \textdownarrow ~ 0 ~ }$ , substituting $z = z _ { n } : = ( 1 - t _ { n } ) x + t _ { n } y$ into (2.38), we obtain

$$
\lim  \langle A (x _ {j}), x - y \rangle \geqslant \langle A (z _ {n}), x - y \rangle . \tag {2.39}
$$

By using the hemi-continuity, the RHS of (2.39) tends to $\langle A ( x ) , x - y \rangle$ . Combining (2.37) (2.39) and the last fact, it follows that

$$
\underline {{\lim }} \langle A (x _ {j}), x _ {j} - y \rangle \geqslant \langle A (x), x - y \rangle . \tag {2.40}
$$

□

Remark 2.5.4 The deduction argument from (2.37) to (2.40) is called Minty’s trick, in which a combination of the monotonicity and the hemicontinuity is applied. This lemma is substantial in the study of monotone operators. The following notion on pseudo-monotonicity is abstracted from it.

Definition 2.5.5 Let $X$ be a reflexive Banach space and let $E \subset X$ be a nonempty closed convex subset. An operator $A : E \to X ^ { * }$ is called pseudo monotone, if

(1)  finite-dimensional linear subspace $L \subset X$ , $A | _ { L \cap E } : L \cap E \to X ^ { * }$ is demi-continuous.   
(2)  sequence $\{ x _ { j } \} \subset E$ with $x _ { j } \to x \in E$ , the condition $\overline { { \operatorname * { l i m } } } \langle A ( x _ { j } ) , x _ { j } - x \rangle \leqslant$ 0 implies that $\underline { { \operatorname* { l i m } } } \langle A ( x _ { j } ) , x _ { j } - y \rangle \geqslant \langle A ( x ) , x - y \rangle$ , $\forall y \in E$ .

Thus, a hemi-continuous monotone operator is pseudo monotone. Moreover, a completely continuous mapping $A : X \to X ^ { * }$ (i.e., for any $x _ { j } \to x$ i n $X$ , we have $A x _ { j }  A x$ in $X ^ { * }$ ) is pseudo monotone.

In contrast with the fixed-point problem for compact maps and nonexpansive mappings, we shall study the surjection of pseudo monotone operators, because the latter maps a subset of a Banach space into its dual space.

The following generalized Ky Fan’s inequality is the basis of this section.

Theorem 2.5.6 (Brezis, Nirenberg, Stampacchia) Assume that $E$ is a nonempty convex set of an LCS X. Assume $\Phi : E \times E \to \mathbb { R } ^ { 1 }$ satisfying

(1) ∀ finite-dimensional linear subspace $L$ of $X$ , $\forall y \in E \cap L$ , $x \mapsto \Phi ( x , y ) | _ { L \cap E }$ is l.s.c.   
(2) $\forall x \in E , \ y \mapsto \Phi ( x , y )$ is quasi concave.   
(3)  a compact set $K \subset X$ , $\exists y _ { 0 } \in E$ such that $\{ x \in E | \ \Phi ( x , y _ { 0 } ) \leqslant 0 \} \subset K$   
(4) sup $\Phi ( x , x ) = 0$ $x \in E$   
(5) $\forall x , y \in E$ and for any net xα x, t  [0, 1], $\Phi ( x _ { \alpha } , ( 1 - t ) x + t y ) \ \leqslant 0$ implies that $\Phi ( x , y ) \leqslant 0$ .

Then $\exists x _ { 0 } \in E$ such that $\operatorname* { s u p } _ { y \in E } \Phi ( x _ { 0 } , y ) \leqslant 0$

Proof. Again we transform the problem into its geometric version by setting $\Gamma = \{ ( x , y ) \in E \times E | \ \Phi ( x , y ) > 0 \}$ . Then (1)–(5) are transformed into:

$( 1 ^ { \prime } ) ~ \forall L$ , letting $E _ { L } = E \cap L$ and $\Gamma ^ { L } = \Gamma \cap ( L \times L ) , \forall y \in E _ { L } \ \Gamma _ { 2 } ^ { L } ( y )$ $\Gamma _ { 2 } ^ { L } ( y )$ is open in $E$ .   
$( 2 ^ { \prime } )$ $\forall x \in E$ , $\Gamma _ { 1 } ( x )$ is convex.   
$( 3 ^ { \prime } )$ $\exists y _ { 0 } \in E$ such that $E \backslash \Gamma _ { 2 } ( y _ { 0 } ) \subset K$   
(4-) $\triangle \cap \Gamma = \emptyset$ , where $\bigtriangleup$ is the diagonal of $E \times E$   
(5-) Let $\overline { { x y } }$ be the segment connecting $x$ and $y$ , then $\Gamma _ { 1 } ( x _ { \alpha } ) \cap { \overline { { x y } } } = \emptyset$ implies $y \not \in \Gamma _ { 1 } ( x ) \ \forall y \in E$ for any net $x _ { \alpha } \to x$ .

Now we set $\begin{array} { r } { \mathrm { ~ \sf ~ X ~ } = \mathrm { ~ \{ ~ \sf ~ L | ~ } ~ }  \end{array}$ finite-dimensional linear subspace, with $y _ { 0 } ~ \in ~ L$ and $E _ { L } \ \ne \ \mathcal { O } \big \}$ . $\forall L \ \in \ \mathsf { X }$ , applying Ky Fan’s inequality, $\exists x _ { L } ~ \in ~ E _ { L }$ such that $\Gamma _ { 1 } ^ { L } ( x _ { L } ) ~ = ~ \emptyset$ . From (3-), $x _ { L } ~ \in ~ K$ as $L \in \mathsf { X } . \forall L \in \mathsf { X }$ $L \in \mathsf { X }$ , setting $N ^ { L } \ = \ \{ x \ \in \ K |$ ΓL(x) = Ø , from the above discussion, it is nonempty. We claim that the family $\{ \overline { N } ^ { L } | L \in \times \}$ has the finite intersection property: Indeed, $\forall L _ { 1 } , \ldots , L _ { n } \in \mathsf { X }$ , setting $L \ = \ \operatorname { s p a n } \left\{ L _ { 1 } , \ldots , L _ { n } \right\}$ we have $L \in \mathsf { X }$ . Then $\Gamma _ { 1 } ^ { L } ( x _ { L } ) = \bigcup _ { i = 1 } ^ { n } \Gamma _ { 1 } ^ { L _ { i } } ( x _ { L } )$ and $N ^ { L } = \bigcap _ { i = 1 } ^ { n } N ^ { L _ { i } }$ . Since $K$ is compact, Z := NL = Ø. $Z : = \bigcap _ { L \in \mathsf { X } } \overline { { N } } ^ { L } \neq \emptyset$

∈We shall prove that $\forall x \in Z$ , $\Gamma _ { 1 } ( x ) = 0$ . To this end, $\forall y \in E$ , we choose $L \in \mathsf { X }$ such that the segment $\overline { { x y } } \subset L$ . Since $x \in \overline { { N } } ^ { L }$ , $\exists$ a net $x _ { \alpha } ^ { L } \in N ^ { L }$ , such that $\Gamma _ { 1 } ( x _ { \alpha } ^ { L } ) \cap \overline { { x y } } = \emptyset$ , and $x _ { \alpha } ^ { L } \to x$ . From assumption (5-), $y \not \in \Gamma _ { 1 } ( x )$ . Since $y \in E$ is arbitrary, we obtain $\Gamma _ { 1 } ( x ) = 0$ , Therefore there exists $x _ { 0 } \in Z$ such that sup Φ(x0, y)  0. $\operatorname* { s u p } _ { y \in E } \Phi ( x _ { 0 } , y ) \leqslant 0$ □

In many of the applications, the LCS is taken to be a reflexive Banach space with its weak topology. Since then for weakly closed set it is weakly compact if and only if it is sequentially weakly compact (cf. Theorem 2.2.5), the net convergence in condition (5) of the above theorem can be replaced by the sequential convergence.

The following theorem is the foundation of the theory of variational inequalities, which arise in free boundary problems for partial differential equations.

Theorem 2.5.7 (Hartman–Stampacchia) Suppose that $X$ is a real reflexive Banach space, and that $E \subset X$ is a nonempty closed convex set. Assume

(1) $A : E \to X ^ { * }$ is pseudo monotone,   
(2) $\varphi : E \to \mathbb { R } ^ { 1 }$ is convex and l.s.c.,   
(3) $\exists y _ { 0 } \in E$ $\exists R _ { 0 } > 0$ such that

$$
\langle A x, x - y _ {0} \rangle + \varphi (x) - \varphi (y _ {0}) > 0 \quad a s \| x \| \geqslant R _ {0} a n d x \in E.
$$

Then $\exists x _ { 0 } \in X$ such that

$$
\langle A x _ {0}, x _ {0} - y \rangle + \varphi (x _ {0}) - \varphi (y) \leqslant 0, \quad \forall y \in E.
$$

Proof. (1) Assign the weak topology on $X$ , and define

$$
\Phi (x, y) = \langle A x, x - y \rangle + \varphi (x) - \varphi (y).
$$

It is easy to see that conditions (1)–(4) of Theorem 2.5.6 are satisfied. Let us verify (5).

$\forall x , y \in E \ \forall t \in [ 0 , 1 ]$ , assume $x _ { j } \to x$ and $\Phi ( x _ { j } , ( 1 - t ) x + t y ) \ \leqslant 0$ . Setting $t = 0$ and 1 separately, we have

$$
\overline {{\lim }} \left\langle A \left(x _ {j}\right), x _ {j} - x \right\rangle \leqslant \overline {{\lim }} \left[ \varphi (x) - \varphi \left(x _ {j}\right) \right] \leqslant 0 \tag {2.41}
$$

and

$$
\overline {{\lim }} \left\langle A \left(x _ {j}\right), x _ {j} - y \right\rangle \leqslant \overline {{\lim }} \left[ \varphi (y) - \varphi \left(x _ {j}\right) \right] \leqslant \varphi (y) - \varphi (x). \tag {2.42}
$$

From the pseudo monotonicity, it follows that

$$
\varlimsup_ {j \rightarrow \infty} \langle A (x _ {j}), x _ {j} - y \rangle \geqslant \langle A (x), x - y \rangle . \tag {2.43}
$$

Combining (2.42) and (2.43), we obtain $\Phi ( x , y ) \leqslant 0$ .

(2) Now we apply Theorem 2.5.6 and conclude the existence of $x _ { 0 } \in E$ satisfying $\Phi ( x _ { 0 } , y ) \leqslant 0$ . This is the desired conclusion. □

Theorem 2.5.8 ( $F$ . Browder) Suppose that $X$ is a real reflexive Banach space, and that $A : X \to X ^ { * }$ is pseudo monotone and coercive, i.e., $\parallel x \parallel ^ { - 1 }$ $\langle A ( x ) , x \rangle \to + \infty$ as $\parallel x \parallel \to + \infty$ . Then $A$ is surjective.

Proof. We shall prove that $\forall z \in X ^ { * } , \ \exists x _ { 0 } \in X$ $\forall z \in X ^ { * }$ satisfying $A ( x _ { 0 } ) = z$ . Define $T : x \mapsto A ( x ) - z$ . Then $T$ is pseudo monotone and satisfies

$$
\langle T x, x \rangle > 0 \quad \text {a s} \| x \| > R _ {0},
$$

for some $R _ { 0 } > 0$ , provided by the coerciveness of $A$ . We apply the Hartman– Stampacchia theorem to conclude the existence of $x _ { 0 } \in X$ satisfying $\langle T x _ { 0 } , x _ { 0 } -$ $y \rangle \leqslant 0 \forall y \in X$ . Since $y$ is arbitrary in the linear space $X$ , it follows that $A x _ { 0 } = z$ . □

As a consequence, we return to Example 4, the $p -$ Laplacian $- \varDelta _ { p } :$ $W _ { 0 } ^ { 1 , p } ( \Omega ) \ \to \ W ^ { - 1 , p ^ { \prime } } ( \Omega ) , 1 \ < \ p < \ \infty$ , is a homeomorphism. The surjection follows from Browder’s theorem, and the injection as well as the continuity of the inverse mapping follow from the inequality in the verification of the monotonicity.

Corollary 2.5.9 (Hartman–Stampacchia) Suppose that $A$ is $a$ hemicontinuous monotone operator defined on the unit ball $B$ centered at $\theta$ of a Hilbert space $H$ . If $A x \neq \lambda x , \forall x \in \partial B$ , $\forall \lambda < 0$ then $\exists x _ { 0 } \in { \overline { { B } } }$ such that $A x _ { 0 } = \theta$ .

Proof. Since $A$ is pseudo monotone, we apply Theorem 2.5.7 to conclude the existence of $x _ { 0 } \in { \overline { { B } } }$ satisfying

$$
\left(A x _ {0}, x _ {0} - y\right) \leqslant 0 \quad \forall y \in \overline {{B}}.
$$

。 If $\boldsymbol { x } _ { 0 } \in \overset { \circ } { B }$ , then we have $A x _ { 0 } = \theta$ . Otherwise $x _ { 0 } \in \partial B _ { R } \ \forall y \in B _ { R }$ , we decompose $y = ( 1 - t ) x _ { 0 } + y ^ { \perp }$ , and $A x _ { 0 } = \lambda x _ { 0 } + y _ { 0 } ^ { \perp }$ , where $t > 0$ , $y ^ { \perp } \perp x _ { 0 }$ , $y _ { 0 } ^ { \perp } \perp x _ { 0 }$ , and $\boldsymbol { \lambda } \in \mathbb { R } ^ { 1 }$ . Then

$$
- \lambda t \| x _ {0} \| ^ {2} + \left(y _ {0} ^ {\perp}, y ^ {\perp}\right) \geqslant 0. \tag {2.44}
$$

Since $y ^ { \perp }$ is arbitrary, first letting $t  + 0$ , we obtain $y _ { 0 } ^ { \perp } = \theta$ , i.e., $A x _ { 0 } = \lambda x _ { 0 }$ By the assumption, $\lambda \geqslant 0$ . Again by (2.44)

$$
\lambda t \parallel x _ {0} \parallel^ {2} \leqslant 0,
$$

which implies that $\lambda = 0$ . Therefore $A x _ { 0 } = \theta$ .

![](images/e69ba75090c7b02b2ed4e06dcae2ff530da7d9be829784b12f750debc48fa1b2.jpg)  
Fig. 2.6.

Corollary 2.5.10 (Minty) Suppose that $H$ is a real Hilbert space, and that A is a continuous strongly monotone operator, i.e., $\exists c > 0$ such that

$$
\left(A x - A y, x - y\right) \geqslant c \| x - y \| ^ {2} \quad \forall x, y \in H. \tag {2.45}
$$

Then A is a homeomorphism.

Proof. Obviously, $A$ is pseudo monotone and coercive. As a consequence of the Browder theorem, $A$ is surjective. The injectivity of $A$ as well as the continuity of $A ^ { - 1 }$ follows from the inequality (2.45). □

# Applications

1. Elliptic variatiational inequality

Let $A$ be the $p$ -Laplacian defined in Example 4 and let $\Omega \subset \mathbb { R } ^ { n }$ be a 。 bounded domain. We consider the closed convex subset of $X = W _ { p } ^ { 1 } ( \Omega )$ with $1 < p < \infty$ :

$$
E = \left\{u \in \overset {\circ} {W} _ {p} ^ {1} (\Omega) | | \nabla u (x) | \leqslant 1 \text {a . e .} \right\}.
$$

Given $f \in W _ { p ^ { \prime } } ^ { 1 } ( \Omega )$ , where $\begin{array} { r } { p ^ { \prime } = \frac { p } { p - 1 } } \end{array}$ , find $u \in E$ such that

$$
\langle A u, v - u \rangle \geqslant \langle f, v - u \rangle \quad \forall v \in E.
$$

The existence of a solution follows from the Hartman–Stampacchia theorem directly. This is a free boundary problem, in fact the solution $u$ , if it is regular, satisfies the equation:

$$
\left\{ \begin{array}{l l} - \operatorname {d i v} \left(| \nabla u | ^ {p - 2} \nabla u\right) = f & \text {i n} \Omega_ {1}, \\ | \nabla u | = 1 & \text {i n} \Omega_ {2}, \\ \text {u a n d} \nabla u \text {c o i n c i d e o n t h e i n t e r f a c e o f} \Omega_ {1} \text {a n d} \Omega_ {2}  , \end{array} \right.
$$

where

$$
\Omega_ {1} = \left\{x \in \Omega | | \nabla u (x) | <   1 \right\},
$$

and

$$
\Omega_ {2} = \Omega \backslash \Omega_ {1}.
$$

The same method can be applied to the obstacle problem and the Stefan problem studied in Sect. 2.3. But in these problems, the working spaces should be taken to be the following Hilbert spaces: $H _ { 0 } ^ { 1 } ( \Omega )$ and $H ^ { 1 } ( [ 0 , T ] \times \Omega )$ respectively.

2. Weak solutions for quasi-linear elliptic equations

Let $\Omega$ be a bounded domain of $\mathbb { R } ^ { n }$ . We study the weak solution of the quasi-linear elliptic equation:

$$
\left\{ \begin{array}{l l} \sum_ {i = 1} ^ {n} \frac {\partial}{\partial x _ {i}} A _ {i} (x, u (x), \nabla u (x)) + B (x, u (x), \nabla u (x)) = f (x) & \mathrm {i n} \Omega , \\ u | _ {\partial \Omega} = 0, \end{array} \right.
$$

where we make use of the following structural condition on $\stackrel {  } { A } = \{ A _ { i } \} _ { 1 } ^ { n }$ and $B$ :

$$
(1) \quad B \in C (\overline {{\Omega}} \times \mathbb {R} ^ {1} \times \mathbb {R} ^ {n}, \mathbb {R} ^ {1}), \stackrel {\rightharpoonup} {A} \in C (\overline {{\Omega}} \times \mathbb {R} ^ {1} \times \mathbb {R} ^ {n}, \mathbb {R} ^ {n}),
$$

(2) $\exists p \in ( 1 , \infty )$ , $\exists g \in L ^ { p ^ { \prime } } ( \Omega )$ , $\begin{array} { r } { p ^ { \prime } = \frac { p } { p - 1 } } \end{array}$ , $\exists C > 0$ such that

$$
\begin{array}{l} \left\| \overrightarrow {A} (x, u, \xi) \right\| \leqslant g (x) + C (| u | + | \xi |) ^ {p - 1}, \\ | B (x, u, \xi) | \leqslant g (x) + C (| u | + | \xi |) ^ {p - 1}, \\ \end{array}
$$

(3)

$$
[ \stackrel {\rightharpoonup} {A} (x, u, \xi) - \stackrel {\rightharpoonup} {A} (x, u, \xi^ {\prime}) ] \cdot (\xi - \xi^ {\prime}) > 0, \forall (x, u) \in \Omega \times \mathbb {R} ^ {1}, \forall \xi , \xi^ {\prime} \in \mathbb {R} ^ {n}, \xi \neq \xi^ {\prime},
$$

and

(4)

$$
\frac {1}{1 + | \xi | + | \xi | ^ {p - 1}} \vec {A} (x, u, \xi) \cdot \xi \rightarrow + \infty \quad \text {a s} | \xi | \rightarrow \infty , (x, u, \xi) \in \Omega \times \mathbb {R} ^ {1} \times \mathbb {R} ^ {n}.
$$

Remark 2.5.11 Condition (3) (i.e., the monotone condition) implies the ellipticity of the differential operator: $\boldsymbol { u } ( \boldsymbol { x } ) \mapsto \operatorname { d i v } \stackrel {  } { A } ( \boldsymbol { x } , \boldsymbol { u } ( \boldsymbol { x } ) , \boldsymbol { \nabla } \boldsymbol { u } ( \boldsymbol { x } ) )$ .

。 Let $X = W _ { p } ^ { 1 } ( \Omega )$ ; we define a form on $X \times X$ :

$$
\begin{array}{l} a (u, v) = \int_ {\Omega} [ \stackrel {\rightharpoonup} {A} (x, u (x), \nabla u (x)) \cdot \nabla v (x) + B (x, u (x), \nabla u (x)) v (x) ] d x \\ \times \forall (u, v) \in X \times X. \\ \end{array}
$$

From (1) and (2),

$$
\left| a (u, v) \right| \leqslant C (\parallel u \parallel_ {1, p} ^ {p - 1} + \parallel g \parallel_ {p ^ {\prime}}) \parallel v \parallel_ {1, p}.
$$

We define $T : X \to X ^ { * }$ by $\langle T u , v \rangle = a ( u , v )$ , where $\langle , \rangle$ is the duality between $X ^ { * }$ and $X$ .

In order to verify that $T$ is pseudo monotone, we define $A : X \times X \to X ^ { * }$ as follow:

$$
\langle A (u, w), v \rangle = \int_ {\Omega} \vec {A} (x, u, \nabla w) \cdot \nabla v + \int_ {\Omega} B (x, u, \nabla u) v \forall u, v, w \in X.
$$

We conclude:

(a) $\langle A ( u , u ) - A ( u , w ) , u - w \rangle \geqslant 0 , \forall u , w \in X$ .   
(b) $\forall u \in X , w \mapsto A ( u , w )$ is bounded and demi-continuous.   
(c) If $u _ { j } \to u$ in $X$ and if $\langle A ( u _ { j } , u _ { j } ) - A ( u _ { j } , u ) , u _ { j } - u \rangle \to 0$ , then $A ( u _ { j } , w ) $ $A ( u , w )$ in $X ^ { \ast } \forall w \in X$ .   
(a) and (b) are obviously true; we prove (c).

Proof. From $u _ { j }  u$ in $X$ , we have $u _ { j }  u ( \mathrm { i n } ~ L ^ { q } ( \Omega ) )$ and $u _ { j }  u$ a.e. after a subsequence, and then $\forall w \in X$ ,

$$
\overrightarrow {A} (x, u _ {j} (x), \nabla w (x)) \to \overrightarrow {A} (x, u (x), \nabla w (x)) \quad \mathrm {a . e .}
$$

Combining with the growth condition (2),

$$
\overrightarrow {A} (\cdot , u _ {j}, \nabla w) \rightarrow \overrightarrow {A} (\cdot , u, \nabla w) \text {i n} L ^ {p ^ {\prime}} (\Omega , \mathbb {R} ^ {n}).
$$

Thus

$$
\int_ {\Omega} \overrightarrow {A} (x, u _ {j}, \nabla w) \nabla u _ {j} \rightarrow \int_ {\Omega} \overrightarrow {A} (x, u, \nabla w) \nabla u, \tag {2.46}
$$

and

$$
\int_ {\Omega} \vec {A} (x, u _ {j}, \nabla w) \nabla v \rightarrow \int_ {\Omega} \vec {A} (x, u, \nabla w) \nabla v. \tag {2.47}
$$

It remains to verify that $g _ { j } : = B ( \cdot , u _ { j } , \nabla u _ { j } )  g : = B ( \cdot , u , \nabla u )$ $\left( L ^ { p ^ { \prime } } ( \Omega ) \right)$ . The difficulty is that we only have $\nabla \boldsymbol { u } _ { j }  \nabla \boldsymbol { u }$ in $L ^ { p } ( \Omega , \mathbb { R } ^ { n } )$ , and that $B$ is nonlinear with respect to $\nabla u$ . How can we pass the limit?

If one can show that $\nabla u _ { j }  \nabla u$ a.e., then $g _ { j }  g$ a.e., and then we claim that $g _ { j } \to g$ in $L ^ { p ^ { \prime } } ( \Omega )$ .

In fact, let $E _ { k } = \left\{ x \in \Omega | | g _ { j } ( x ) - g ( x ) | \leq 1 , j \geq k \right\}$ ; we have $E _ { 1 } \subset E _ { 2 } \subset . . . ,$ and $\mathrm { m e s } ( E _ { k } ) \to \mathrm { m e s } ( \Omega )$ . Since $\forall \chi \in L ^ { p } ( \Omega )$ with support in $E _ { k }$ , we have $\begin{array} { r } { \int _ { \Omega } ( g _ { j } - g ) \chi \to 0 } \end{array}$ , from the Lebesgue dominance theorem, and those $\chi$ span a dense set in $L ^ { p } ( \Omega )$ ; in combination with the fact that $\| g _ { j } \| _ { p ^ { \prime } }$ is bounded, the conclusion follows.

We turn to verifying that $\nabla u _ { j }  \nabla u$ a.e., and define the function:

$$
F _ {j} (x) = \left[ \overrightarrow {A} (x, u _ {j} (x), \nabla u _ {j} (x)) - \overrightarrow {A} (x, u _ {j} (x), \nabla u (x)) \right] \cdot \nabla (u _ {j} (x) - u (x)).
$$

By (3), $F _ { j } ( x ) \geqslant 0$ . As a consequence of our assumption, $\begin{array} { r } { \int _ { \Omega } F _ { j } ( x ) d x \to 0 } \end{array}$ . From Fatou’s lemma, it follows that $F _ { j } ( x ) \to 0$ a.e. Therefore there exists a null set $Z$ such that $u _ { j } ( x )  u ( x )$ , and $F _ { j } ( x ) \to 0 \forall x \not \in Z$ .

Firstly, we claim that $\Finv$ a measurable function $M ( x ) < + \infty$ , such that

$$
\left| \nabla u _ {j} (x) \right| \leqslant M (x) \quad \forall x \notin Z. \tag {2.48}
$$

From (2),

$$
F _ {j} (x) \geq \overrightarrow {A} (x, u _ {j} (x), \nabla u _ {j} (x)) \nabla u _ {j} (x) - C _ {1} (x) (1 + | \nabla u _ {j} (x) | + | \nabla u _ {j} (x) | ^ {p - 1})
$$

where $C _ { 1 } ( x )$ depends on $u ( x )$ and $\nabla u ( x )$ .

If (2.48) were false, then there would be a positive measure set $E$ , on which $| \nabla u _ { j } ( x ) |  + \infty$ . It follows that $F _ { j } ( x ) \longrightarrow + \infty$ , from (4). But, this is a contradiction.

Secondly, we claim that $\forall x \notin Z$ , $\nabla u _ { j } ( x )  \nabla u ( x )$ .

In fact, let $\xi ( x )$ be a limit point of $\nabla u _ { j } ( x )$ . From $F _ { j } ( x )  0$ it follows that:

$$
[ \overrightarrow {A} (x, u (x), \xi (x)) - \overrightarrow {A} (x, u (x), \nabla u (x)) ] \cdot (\xi (x) - \nabla u (x)) d x = 0.
$$

Applying the monotone relation (3), we have $\xi ( x ) = \nabla u ( x )$ . Since all limit points of $\nabla u _ { j } ( x )$ equal $\nabla u ( x )$ , we obtain $\nabla u _ { j } ( x )  \nabla u ( x )$ a.e.

Combining (2.47) with this fact, we obtain $A ( u _ { j } , w )  A ( u , w )$ in $X ^ { * }$ $\forall w \in X$ . □

(d) If $u _ { j }  u$ in $X$ and $A ( u _ { j } , v )  \psi$ in $X ^ { * }$ , then $\langle A ( u _ { j } , v ) , u _ { j } \rangle \to \langle \psi , u \rangle$ .

In fact, we already have $u _ { j }  u$ $\left( L ^ { p } ( \Omega , \mathbb { R } ^ { 1 } ) \right)$ ) and $\overrightarrow { A } ( \cdot , u _ { j } , \nabla v )  \overrightarrow { A } ( \cdot , u ,$ v) $L ^ { p ^ { \prime } } ( \Omega , \mathbb { R } ^ { n } ) )$ . Thus

$$
\begin{array}{l} \langle A (u _ {j}, v), u _ {j} \rangle = \int_ {\Omega} \overrightarrow {A} (x, u _ {j}, \nabla v) \nabla (u _ {j} - u) + B (x, u _ {j}, \nabla u _ {j}) (u _ {j} - u) \\ + \langle A (u _ {j}, v), u \rangle \rightarrow \langle \psi , u \rangle . \\ \end{array}
$$

After these preparations, we turn to verifying that the operator $T$ , defined by the form $a$ , is pseudo monotone. Since $u \mapsto T ( u )$ is obviously demicontinuous, it is sufficient to prove that $u _ { j }  u$ and $\operatorname* { l i m } \left. T ( u _ { j } ) , u _ { j } - u \right. \leqslant 0$ implies that $\varinjlim \left. T ( u _ { j } ) , u _ { j } - v \right. \geqslant \left. T ( u ) , u - v \right. \forall v \in X$ .

First we show that $\langle T ( u _ { j } ) , u _ { j } - u \rangle \to 0$ , and define the sequence:

$$
b _ {j} = \left\langle A \left(u _ {j}, u _ {j}\right) - A \left(u _ {j}, u\right), u _ {j} - u \right\rangle .
$$

By (a), $b _ { j } \geqslant 0$ . Since $A ( u _ { j } , u )$ is bounded in $X ^ { * }$ , there exists a subsequence, for which we don’t change the subscript, such that $A ( u _ { j } , u )  \psi$ in $X ^ { * }$ . By (d), $\langle A ( u _ { j } , u ) , u _ { j } \rangle \to \langle \psi , u \rangle$ . Thus, $\langle A ( u _ { j } , u ) , u _ { j } - u \rangle \to 0$ . But by assumption,

$$
\overline {{\lim }} \langle A (u _ {j}, u _ {j}), u _ {j} - u \rangle = \overline {{\lim }} \langle T (u _ {j}), u _ {j} - u \rangle \leqslant 0,
$$

therefore $b _ { j } \to 0$ . It follows $\langle T ( u _ { j } ) , u _ { j } - u \rangle \to 0$

It remains to show: $\underline { { \operatorname* { l i m } } } \langle T ( u _ { j } ) , u - v \rangle \geq \langle T ( u ) , u - v \rangle$ , from (a),

$$
\langle T (u _ {j}) - A (u _ {j}, w), u _ {j} - w \rangle \geqslant 0, \forall w \in X.
$$

Letting $w = ( 1 - \theta ) u + \theta v , \ \theta \in ( 0 , 1 ) , \ \forall v \in X$ , we have

$$
\theta \langle T (u _ {j}), u - v \rangle \geqslant \theta \langle A (u _ {j}, w), u - v \rangle + \langle A (u _ {j}, w), u _ {j} - u \rangle - \langle T (u _ {j}), u _ {j} - u \rangle .
$$

By taking limit,

$$
\lim  _ {j \to \infty} \langle T (u _ {j}), u - v \rangle \geqslant \langle A (u, w), u - v \rangle
$$

or

$$
\lim  _ {\theta} \langle T (u _ {j}), u - v \rangle \geqslant \langle A (u, (1 - \theta) u + \theta v), u - v \rangle \quad \forall \theta \in (0, 1)
$$

letting $\theta  0$ , by (b), the $\mathrm { R H S } = \langle T ( u ) , u - v \rangle$ , this is the desired inequality.

Statement 2.5.12 Under the above structural conditions, we assume further:

(5) $\exists C _ { 1 } > 0 \exists h \in L ^ { 1 } ( \Omega )$ such that

$$
\overrightarrow {A} (x, u, \xi) \cdot \xi > C _ {1} | \xi | ^ {p} - h (x) \quad \forall (x, u, \xi) \in \Omega \times \mathbb {R} ^ {1} \times \mathbb {R} ^ {n}.
$$

。 Then $\forall f \in L ^ { p ^ { \prime } } ( \Omega )$ , there exists a weak solution $u \in W _ { p } ^ { 1 } ( \Omega )$ of the quasilinear elliptic equation:

$$
\begin{array}{l} \int_ {\Omega} \overrightarrow {A} (x, u (x), \nabla u (x)) \cdot \nabla v (x) + B (x, u (x), \nabla u (x)) v (x) = \int_ {\Omega} f (x) v (x) \\ \times \forall v \in \overset {\circ} {W _ {p} ^ {1}} (\Omega). \\ \end{array}
$$

Proof. It is reduced to find the fixed point of the pseudo monotone operator $T$ . Since (5) implies the coerciveness:

$$
\frac {1}{\parallel u \parallel_ {1 , p}} \langle T u, u \rangle = \frac {1}{\parallel u \parallel_ {1 , p}} a (u, u) \rightarrow + \infty .
$$

The existence of $u$ follows from Browder’s theorem.

It is worth noting that in the above statement, no a priori estimate has been used. In the verification of the pseudo monotonicity, a crucial point is the ellipticity condition (or the monotone condition (3)). The statement can be extended to high-order quasi-elliptic equations. This is an approach where no compactness is concerned.

# 2.6 Maximal Monotone Mapping

To a set-valued mapping $A$ , $\forall x \in D ( A )$ , one may take any nonempty subset $D ( A _ { 1 } )$ of $D ( A )$ and a nonempty subset $A _ { 1 } ( x )$ of $A ( x )$ , and define a new set-valued mapping $A _ { 1 } : x  A _ { 1 } ( x ) \forall x \in D ( A _ { 1 } )$ . If $A$ is monotone, so is $A _ { 1 }$ . In order to avoid the indetermination of domain and the image of setvalued monotone mappings in the study of their surjectivity, one introduces the notion of maximal monotone mappings.

First, we define the graph $\Gamma _ { A } : = \{ ( x , y ) \in X \times Y | x \in D ( A ) , y \in A ( x ) \}$ of a set-valued mapping $A : X \to 2 ^ { Y }$ .

Definition 2.6.1 Assume that $A _ { 1 }$ and $A _ { 2 }$ are two monotone set-valued mappings from $X$ to $2 ^ { X ^ { \ast } }$ . If $\Gamma _ { A _ { 1 } } \subset \Gamma _ { A _ { 2 } }$ , then we say that $A _ { 2 }$ is a monotone extension of $A _ { 1 }$ . A monotone mapping, which does not have any proper monotone extension, is called a maximal monotone mapping.

In other words, $A : X \to 2 ^ { X ^ { * } }$ is maximal monotone iff for $( x , x ^ { * } ) \in X \times X ^ { * }$

$$
\langle y ^ {*} - x ^ {*}, y - x \rangle \geq 0 \forall (y, y ^ {*}) \in \Gamma_ {A} \Rightarrow (x, x ^ {*}) \in \Gamma_ {A}.
$$

Example 1. Assume that $\phi : \mathbf { R } ^ { 1 }  \mathbf { R } ^ { 1 }$ is a nondecreasing function. The map $A : u \mapsto [ \phi ( u { - } 0 ) , \phi ( u { + } 0 ) ]$ is maximal monotone, but the map $B : u \mapsto \phi ( u { - } 0 )$ is not, if $\phi ( u - 0 ) \neq \phi ( u + 0 )$ .

Example 2. A hemi-continuous monotone operator $A$ is maximal monotone. In fact, for $( x , x ^ { * } ) \in X \times X ^ { * }$ , if

$$
\langle A y - x ^ {*}, y - x \rangle \geq 0 \forall y \in X,
$$

then $\forall z \in X , \forall t \in [ 0 , 1 ]$ , we have

$$
\langle A ((1 - t) x + t z) - x ^ {*}, z - x \rangle \geq 0.
$$

Letting $t \longrightarrow 0$ , it follows that $\langle A x - x ^ { * } , z - x \rangle \geq 0$ . Since $z \in X$ is arbitrary, we obtain $A x = x ^ { * }$ .

Moreover, we shall prove later that the subdifferential of a proper l.s.c. convex function is maximal monotone.

Since maximal monotone mappings occur in convex analysis, free boundary boundary problems in mathematical physics, and nonlinear semigroups. we shall extend most of the results of monotone operators to their set-valued counterpart-maximal monotone mappings. The central problem is on the surjectivity.

Now we are going to extend the notions of the demi-continuity and the hemi-continuity for single-valued mappings to set-valued mappings.

Definition 2.6.2 Suppose that $E$ is a nonempty subset of a real Banach space $X$ . A mapping $A : E \to 2 ^ { X ^ { * } }$ is called weakly $^ *$ upper hemi-continuous (w∗u.h.c.,) if $\forall x , y \in E \ \forall z \in X$ , the function $t \mapsto \langle A ( ( 1 - t ) x + t y ) , z \rangle$ is a u.s.c. set-valued function at $t = 0$ . It is called upper demi-continuous (u.d.c.) if $\forall z \in X , \ x \mapsto \langle A ( x ) , z \rangle$ is u.s.c. from $E$ to $2 ^ { \mathbb { R } ^ { 1 } }$ .

Obviously, u.d.c. implies w∗u.h.c., and for single-valued mappings, w∗u.h.c. is hemi-continuous, and u.d.c. is demi-continuous.

Note that for any set-valued mapping $A : D ( A ) \to 2 ^ { X ^ { * } }$ , the inverse mapping $A ^ { - 1 } : X ^ { * } \to 2 ^ { X }$ , $x ^ { * } \mapsto \{ x \in D ( A ) | \ x ^ { * } \in A ( x ) \}$ is well defined (in case $x ^ { * } \not \in \mathrm { r a n g } \left( A \right)$ , one defines $A ^ { - 1 } ( x ^ { * } ) = 0$ ). In particular, if $X$ is reflexive then $A$ is (maximal) monotone $\iff A ^ { - 1 }$ is (maximal) monotone.

Lemma 2.6.3 Assume that $A : X \to 2 ^ { X ^ { * } }$ is monotone. Then $\forall x _ { 0 } \in X$ $\exists \varepsilon > 0$ such that $A$ is bounded on $B _ { \varepsilon } ( x _ { 0 } )$ .

Proof. We may assume $x _ { 0 } = \theta$ . We prove it by contradiction. Assume that $A$ is not bounded in any neighborhood of $\theta$ , i.e., $\exists x _ { n }  \theta \ \exists x _ { n } ^ { * } \in A ( x _ { n } )$ such that $\parallel x _ { n } ^ { * } \parallel  \infty$ . We claim that $\forall \varepsilon > 0 \exists z \in B _ { \varepsilon } ( \theta )$ and a subsequence $\{ n ^ { \prime } \}$ such that

$$
\langle x _ {n ^ {\prime}} ^ {*}, x _ {n ^ {\prime}} - z \rangle \rightarrow - \infty . \tag {2.49}
$$

Then from the monotonicity, $\forall z ^ { * } \in A ( z )$ , we have

$$
\langle x _ {n ^ {\prime}} ^ {*} - z ^ {*}, x _ {n ^ {\prime}} - z \rangle \geqslant 0.
$$

(2.49) implies that $\langle z ^ { * } , z \rangle = + \infty$ . Obviously, this is impossible.

Let us return to proving the existence of $z$ satisfying (2.49). If it is not true, then $\exists \varepsilon > 0 \forall z \in B _ { \varepsilon } ( \theta ) \exists ($ $\exists C _ { z } \in \mathbb { R } ^ { 1 }$ such that

$$
\left\langle x _ {n} ^ {*}, x _ {n} - z \right\rangle \geqslant C _ {z} \quad \forall n  .
$$

Now, $\forall k \in \mathbb N$ , let

$$
E _ {k} = \left\{u \in \bar {B} _ {\varepsilon} (\theta) \mid \langle x _ {n} ^ {*}, x _ {n} - u \rangle \geqslant - k, \forall n \right\}.
$$

Obviously, $E _ { k }$ are closed, and $\stackrel { \infty } { \mathop { \left| \mathrm { \scriptsize \omega } \right| } } { } _ { k = 1 } E _ { k } = \overline { { B } } _ { \varepsilon } ( \theta )$ . From the Baire category argument, $\exists r > 0 , \exists y _ { 0 } \in \overline { { B } } _ { \epsilon } ( \theta )$ and $\exists k _ { 0 } \in \mathbb { N }$ such that $B _ { r } ( y _ { 0 } ) \subset E _ { k _ { 0 } }$ . Setting $C = C _ { - y _ { 0 } }$ , we have

$$
\langle x _ {n} ^ {*}, x _ {n} + y _ {0} \rangle \geqslant C \quad \forall n  ,
$$

and

$$
\left\langle x _ {n} ^ {*}, x _ {n} - u \right\rangle \geq - k _ {0}, \forall u \in \overline {{B}} _ {r} (y _ {0}).
$$

By addition,

$$
\left\langle x _ {n} ^ {*}, 2 x _ {n} + y _ {0} - u \right\rangle \geqslant C - k _ {0} \quad \forall u \in \overline {{B}} _ {r} (y _ {0}).
$$

Setting $v = 2 x _ { n } + y _ { 0 } - u$ , for sufficiently large $n$ , such that $\Vert { \begin{array} { l } { x _ { n } } \end{array} } \Vert < { \frac { r } { 4 } }$ , we obtain

$$
\langle x _ {n} ^ {*}, v \rangle \geqslant C - k _ {0} \quad \forall v \in B _ {\frac {r}{2}} (\theta).
$$

This means $\begin{array} { r } { \left\| \begin{array} { l l l } { x _ { n } ^ { * } } & { \| \leqslant } & { { \frac { 2 | C - k _ { 0 } | } { r } } } \end{array} \right. } \end{array}$ , which contradicts $\| x _ { n } ^ { * } \| ~ \to ~ \infty$ ; (2.49) is proved. □

Proposition 2.6.4 Assume that $A : X \to 2 ^ { X ^ { * } }$ is a maximal monotone mapping with $D ( A ) = X$ on a reflexive real Banach space $X$ . Then:

(1) $\forall x \in X \ A ( x )$ is a nonempty closed convex set.   
(2) $A$ is u.d.c.   
(3) If there is a sequence $\{ x _ { j } \}$ , and a sequence $x _ { j } ^ { * } \in A ( x _ { j } )$ satisfying $x _ { j } \to x$ and $\overline { { \operatorname* { l i m } } } \left. x _ { j } ^ { * } , x _ { j } - x \right. \leqslant 0$ , then $\exists x ^ { * } \in A ( x )$ such that

$$
\underline {{\lim }} \left\langle x _ {j} ^ {*}, x _ {j} - y \right\rangle \geqslant \left\langle x ^ {*}, x - y \right\rangle \quad \forall y \in X.
$$

Proof. (1) If $x _ { i } ^ { * } \in A ( x )$ $i = 1 , 2$ , then $\forall y \in X$ , $\forall y ^ { * } \in A ( y )$ we have

$$
\left\langle y ^ {*} - x _ {i} ^ {*}, y - x \right\rangle \geqslant 0, \quad i = 1, 2.
$$

Therefore

$$
\langle y ^ {*} - \left(\lambda x _ {1} ^ {*} + (1 - \lambda) x _ {2} ^ {*}\right), y - x \rangle \geqslant 0 \quad \forall \lambda \in [ 0, 1 ].
$$

By the maximality, $\lambda x _ { 1 } ^ { * } + ( 1 - \lambda ) x _ { 2 } ^ { * } \in A ( x )$ . Then $A ( x )$ is convex. Similarly, $A ( x )$ is closed.

(2) In order to show that $A$ is u.d.c., i.e., $\forall z \in X , x \mapsto \langle A ( x ) , z \rangle$ is a u.s.c. set-valued mapping: $X \to 2 ^ { \mathbb { R } ^ { 1 } }$ . We prove it by contradiction. If $\exists z _ { 0 } \in X \exists \varepsilon _ { 0 } >$ $0 \exists x _ { n }  x _ { 0 }$ and $\exists x _ { n } ^ { * } \in A ( x _ { n } )$ such that $\langle x _ { n } ^ { * } , z _ { 0 } \rangle \not \in \langle A ( x _ { 0 } ) , z _ { 0 } \rangle + ( - \varepsilon _ { 0 } , \varepsilon _ { 0 } )$ . From Lemma 2.6.3, $\{ x _ { n } ^ { * } \}$ is bounded, and then it possesses a weakly convergent subsequence: $x _ { n } ^ { * } \to x _ { 0 } ^ { * }$ in $X ^ { * }$ . Since $A$ is maximal, $x _ { 0 } ^ { * } \in A ( x _ { 0 } )$ . This is a contradiction.   
(3) The proof is the same as that for single-valued monotone operators; cf. Lemma 2.5.3. □

Conversely, we have:

Proposition 2.6.5 Assume that $X$ is a reflexive real Banach space and that $A : X \to 2 ^ { X ^ { * } }$ is a w∗.u.h.c. monotone mapping. If $\forall x \in X$ , $A ( x )$ is a nonempty closed convex set, then A is maximal monotone.

Proof. We shall verify that for $( x , x ^ { * } ) \in X \times X ^ { * }$ , if

$$
\langle y ^ {*} - x ^ {*}, y - x \rangle \geqslant 0 \quad \forall (y, y ^ {*}) \in \Gamma_ {A} , \tag {2.50}
$$

then $( x , x ^ { * } ) \in \Gamma _ { A }$ . For otherwise, according to the Ascoli separation theorem, $\exists z \in X$ such that

$$
\langle x ^ {*}, z \rangle > \sup  \left\{\langle w ^ {*}, z \rangle \mid w ^ {*} \in A (x) \right\}.
$$

Since $A$ is w∗.u.h.c. $\exists \delta > 0$ such that

$$
\langle x ^ {*}, z \rangle > \langle y _ {t} ^ {*}, z \rangle \quad \forall y _ {t} ^ {*} \in A (y _ {t}) \forall t \in (0, \delta),
$$

where $y _ { t } = x + t z$ . But from the monotonicity, we have

$$
t \left\langle y _ {t} ^ {*} - x ^ {*}, z \right\rangle = \left\langle y _ {t} ^ {*} - x ^ {*}, y _ {t} - x \right\rangle \geqslant 0.
$$

This is a contradiction.

□

The following theorem is a characterization of maximal monotone mappings on Hilbert spaces.

Theorem 2.6.6 (Minty) Suppose that $H$ is a Hilbert space, and that A : $H \to 2 ^ { H }$ is a set-valued mapping. The following statements are equivalent:

(1) A is maximal monotone.   
(2) $A$ is monotone and $I + A$ is surjective.   
(3) $\forall \lambda > 0$ $( I + \lambda A ) ^ { - 1 }$ is nonexpansive.

Before going on to prove Theorem 2.6.6, we now introduce the following notion.

Definition 2.6.7 Suppose that $E$ is a nonempty subset of a real Hilbert space $H$ . A set-valued mapping $T : E \to 2 ^ { H }$ is called expansive if $\forall x , y \in E , \ \forall x ^ { * } \in$ $T ( x ) , \ \forall y ^ { * } \in T ( y ) , \ \parallel \ x ^ { * } - y ^ { * } \ \parallel \geqslant \parallel x - y \ \parallel$ $\forall y ^ { * } \in T ( y )$ .

Obviously, $T : H \to 2 ^ { H }$ is surjective and expansive iff $T ^ { - 1 }$ is nonexpansive. In this case, $T ^ { - 1 }$ is single valued and injective.

Lemma 2.6.8 $A : E \to 2 ^ { H }$ is monotone if and only if $\forall \lambda > 0$ , $T _ { \lambda } = I + \lambda A$ is expansive.

Proof. Since $\forall x , y \in E , \forall x ^ { * } \in A ( x ) , \forall y ^ { * } \in A ( y )$ , we have

$$
\| y - x + \lambda \left(y ^ {*} - x ^ {*}\right) \| ^ {2} = \| y - x \| ^ {2} + 2 \lambda \left(y ^ {*} - x ^ {*}, y - x\right) + \lambda^ {2} \| y ^ {*} - x ^ {*} \| ^ {2}. \tag {2.51}
$$

“= ” If $A$ is monotone, then the RHS of $( 2 . 5 1 ) \geqslant \parallel y - x \parallel ^ { 2 }$ , i.e., $T _ { \lambda }$ is expansive.

“⇐=” If $T _ { \lambda }$ is expansive, then the LHS of $( 2 . 5 1 ) \ \geqslant \parallel \ y - x \ \parallel ^ { 2 }$ . Letting $\lambda  0$ , $A$ is monotone. □

Lemma 2.6.9 (Deburnner–Flor) Suppose that $E$ is a nonempty closed convex subset of a real Hilbert space $H$ . If $A : E \to 2 ^ { H }$ is monotone, then $\forall y \in H$ , $\exists x \in E$ such that

$$
(\eta + x, \xi - x) \geqslant (y, \xi - x) \quad \forall (\xi , \eta) \in \Gamma_ {A}.
$$

Proof. $\forall ( \xi , \eta ) \in \Gamma _ { A }$ , let us define

$$
C (\xi , \eta) = \left\{x \in E \mid (\eta + x - y, \xi - x) \geqslant 0 \right\}.
$$

Our conclusion is equivalent to $\cap \{ C ( \xi , \eta ) | ( \xi , \eta ) \in \Gamma _ { A } \} \neq \emptyset$ . Since $C ( \xi , \eta )$ is a bounded closed convex set, it is weakly closed and $E$ is weakly compact. It is sufficient to verify that $\{ C ( \xi , \eta ) | \ ( \xi , \eta ) \in \Gamma _ { A } \}$ has the finite intersection property. Indeed, $\forall \xi _ { 1 } , \ldots , \xi _ { n } \in E \ \forall \eta _ { i } \in A ( \xi _ { i } ) , \ i = 1 , \ldots , n$ , we consider the simplex

$$
\triangle_ {n} = \left\{\lambda = (\lambda_ {1}, \dots , \lambda_ {n}) | \lambda_ {i} \geqslant 0 \forall i, \sum \lambda_ {i} = 1 \right\},
$$

and the function

$$
\Phi (\lambda , \mu) = \sum_ {i = 1} ^ {n} \mu_ {i} (x (\lambda) + \eta_ {i} - y, x (\lambda) - \xi_ {i}) \quad \forall (\lambda , \mu) \in \triangle_ {n} \times \triangle_ {n},
$$

where $\begin{array} { r } { x ( \lambda ) = \sum _ { i = 1 } ^ { n } \lambda _ { i } \xi _ { i } } \end{array}$ . It is easy to see that

$$
\forall \mu \in \triangle_ {n}, \lambda \mapsto \Phi (\lambda , \mu) \text {i s c o n t i n u o u s},
$$

$$
\forall \lambda \in \triangle_ {n}, \mu \mapsto \Phi (\lambda , \mu) \text {i s l i n e a r ,}
$$

and

$$
\begin{array}{l} \Phi (\lambda , \lambda) = \sum_ {i = 1} ^ {n} \left[ \lambda_ {i} (x (\lambda) - y, x (\lambda) - \xi_ {i}) + \lambda_ {i} \left(\eta_ {i}, x (\lambda) - \xi_ {i}\right) \right] \\ = \sum_ {i, j = 1} ^ {n} \lambda_ {i} \lambda_ {j} \left(\eta_ {i}, \xi_ {j} - \xi_ {i}\right) \\ = \frac {1}{2} \sum_ {i, j = 1} ^ {n} \lambda_ {i} \lambda_ {j} \left(\eta_ {i} - \eta_ {j}, \xi_ {j} - \xi_ {i}\right) \leq 0 , \\ \end{array}
$$

because $A$ is monotone. According to Ky Fan’s inequality, $\exists \lambda _ { 0 } \ \in \ \triangle _ { n }$ such that

$$
\Phi (\lambda_ {0}, \mu) \leqslant 0 \quad \forall \mu \in \triangle_ {n} ,
$$

i.e.,

$$
\left(x \left(\lambda_ {0}\right) + \eta_ {i} - y, x \left(\lambda_ {0}\right) - \xi_ {i}\right) \leqslant 0 \quad \forall i.
$$

Therefore $x ( \lambda _ { 0 } ) \in \bigcap _ { i = 1 } ^ { n } C ( \xi _ { i } , \eta _ { i } )$ n . This proves the finite intersection property for $\{ C ( \xi , \eta ) | ( \xi , \eta ) \in \Gamma _ { A } \}$ , and thus the lemma. □

Proof of Minty’s theorem.

Proof. (1)= (2) We only want to show that $\forall y \in H$ , $\exists x \in H$ such that $y - x \in A ( x )$ . In fact, according to the Deburnner–Flor lemma, $\exists x \ \in \ H$ satisfying $( \eta - ( y - x ) , \xi - x ) \geqslant 0$ , $\forall ( \xi , \eta ) \in \Gamma _ { A }$ . By maximality, $y - x \in A ( x )$ .

(1) =(2) Assume that $B$ is a monotone mapping with $\Gamma _ { A } \subset \Gamma _ { B }$ ; we want to show that $B = A$ . Since $I + A$ is surjective, $\forall ( x , y ) \in \Gamma _ { B }$ , $\exists x ^ { \prime } \in D ( A )$ such that $x + y \in ( I + A ) ( x ^ { \prime } )$ , so is $x + y \in x ^ { \prime } + B ( x ^ { \prime } )$ , i.e, $y ^ { \prime } = y + x - x ^ { \prime } \in B ( x ^ { \prime } )$ . Also from $( x , y ) \in \Gamma _ { B }$ , $x + y \in x + B ( x )$ . Provided by the monotonicity of $B$ , we have

$$
\left\| x - x ^ {\prime} \right\| ^ {2} = \left(y ^ {\prime} - y, x - x ^ {\prime}\right) \leqslant 0,
$$

i.e., $x = x ^ { \prime }$ . Therefore $B = A$ and then $A$ is maximal.

Since $A$ is maximal monotone if and only if $\lambda A$ is $\forall \lambda > 0$ , we may assume $\lambda = 1$ in proving the equivalence of (2) and (3). Also, by Lemma 2.6.8, $( I + A )$ is expansive $\Longleftrightarrow A$ is monotone.

$( 2 ) \Longleftrightarrow ( 3 )$ follows from the fact that $( I + A )$ is expansive and surjective $\iff ( I + A ) ^ { - 1 }$ is nonexpansive. □

In the literature, for a maximal monotone mapping $A$ , $\forall \lambda > 0$ , setting $J _ { \lambda } = ( I + \lambda A ) ^ { - 1 }$ we call the operator $A _ { \lambda } ( x ) = { \textstyle \frac { 1 } { \lambda } } ( I - J _ { \lambda } ) ( x )$ the Yosida regularization of $A$ . By definition, $A _ { \lambda } ( x ) \in A ( J _ { \lambda } x )$ , but in general, $A _ { \lambda } ( x ) \neq$ $A ( J _ { \lambda } x )$ .

Theorem 2.6.10 If $f : H \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is a proper, l.s.c., convex function, then $\partial f$ is maximal monotone.

Proof. The monotonicity has been known since the beginning of the last section. Due to Minty’s theorem, it remains to verify the surjection of $I + \partial f$ , i.e., $\forall y _ { 0 } \in H , \ \exists x _ { 0 } \in H$ such that $y _ { 0 } - x _ { 0 } \in \partial f ( x _ { 0 } )$ .

To this end, we define a function:

$$
\varphi (x) = \frac {1}{2} \parallel x \parallel^ {2} + f (x) - \langle y _ {0}, x \rangle .
$$

It is proper, l.s.c., and convex, and $\varphi ( x )  + \infty$ as $\parallel x \parallel \to \infty$ . There exists $R _ { 1 } > 0$ s.t. $\varphi ( x ) > \varphi ( \theta )$ as $\parallel x \parallel \geqslant R _ { 1 }$ . Applying the Hartman–Stampacchia theorem, $\exists x _ { 0 } \in D ( \varphi ) = D ( f )$ such that

$$
\varphi (x _ {0}) \leqslant \varphi (x) \quad \forall x \in H.
$$

Therefore, $\theta \in \partial \varphi ( x _ { 0 } )$ , i.e., $y _ { 0 } - x _ { 0 } \in \partial f ( x _ { 0 } )$ .

In fact, the above conclusion can be extended to general real Banach spaces. In this extension, the normalized duality map $J : X \to 2 ^ { X ^ { * } }$ plays an important role, $J ( x ) \ = \ \{ x ^ { * } \ \in \ X ^ { * } \mid \ < \ x ^ { * } , x \ > = \ \| x \| ^ { 2 } \ = \ \| x ^ { * } \| ^ { 2 } \ \}$ , (cf. Example 1 in Sect. 2.2). It is easy to verify that $J$ is an odd, positively homogeneous, bounded set-valued mapping, and it is known that $J ( x ) = \partial \varphi ( x )$ , where $\textstyle \varphi ( x ) = { \frac { 1 } { 2 } } \| x \| ^ { 2 }$ , so it is monotone.

The Browder theorem (Theorem 2.5.8) is also extended to maximal monotone mappings: Let $X$ be a uniformly convex Banach space, if $D ( A ) \subset X$ , and $A : D ( A ) \to X ^ { * }$ is maximal monotone and coercive: ${ \frac { \langle y , x \rangle } { \| x \| } } \ \to \ + \infty$ $\forall ( x , y ) \in \Gamma _ { A }$ , as $\| x \| \to \infty$ , then $A$ is surjective.

For a proper l.s.c. convex function on a real Banach space, $f : X \to$ $\mathbb { R } ^ { 1 } \cup \{ + \infty \}$ , the conjugate function of $f , f ^ { * } ~ : ~ { \cal { X } } ^ { * } ~  ~ \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ will be defined in Sect. 4.1.3 to be

$$
f ^ {*} (x ^ {*}) = \sup  _ {x \in X} \left\{\langle x ^ {*}, x \rangle - f (x) \right\}.
$$

It will be proved therein that $f ^ { * }$ is again a proper l.s.c. convex function, satisfying

$$
f (x) + f ^ {*} \left(x ^ {*}\right) = \left\langle x ^ {*}, x \right\rangle \Leftrightarrow x ^ {*} \in \partial f (x) \Leftrightarrow x \in \partial f ^ {*} \left(x ^ {*}\right).
$$

Thus we have the following:

Corollary 2.6.11 If $f : X \to \mathbb { R } ^ { 1 }$ is proper, l.s.c., and convex, then $\partial f ^ { * } =$ $( \partial f ) ^ { - 1 }$ is maximal monotone.

Proof. According to the above discussion, $\partial f ^ { * } = ( \partial f ) ^ { - 1 }$ . The maximality of $\partial f ^ { * }$ follows from Theorem 2.6.10. □

# Degree Theory and Applications

The Leray–Schauder degree is an important topological tool introduced by Leray and Schauder in the study of nonlinear partial differential equations in the early 1930s. The nontriviality of the degree ensures the existence of a fixed point of the compact mapping in the domain. It enjoys the properties of homotopy invariance and additivity, which make the topological tool more convenient in application, and provides more information on fixed points. In Sects. 3.5 and 3.6 we shall see how better results could be obtained by using this tool than by using any of the other methods that we discussed previously.

The Leray–Schauder degree is an extension of the Brouwer degree from finite–dimensional spaces to infinite–dimensional Banach spaces, while the Brouwer degree is a powerful tool in algebraic topology. We introduce the notion of the Brouwer degree in Sect. 3.1, and investigate its fundamental properties in Sect. 3.2. On the one hand, we introduce the definition of the Brouwer degree from the point of view of differential topology so that it closely ties with the counting of zeroes of mappings; on the other hand, we build up the relationship between this definition and that used in algebraic topology. The applications to the Brouwer fixed-point theorem and Borsuk–Ulam theorem, as well as the intersection number etc. are studied in Sect. 3.3.

The notion of the Leray–Schauder degree is defined by approximation. All its fundamental properties are transferred from those of the Brouwer degree directly. These are the contents of Sect. 3.4. We emphasize the computation of the degree, because the more precisely we know the degree the sharper we can estimate the number of fixed points. This opens a door to the study of multiple solutions in nonlinear analysis.

Rabinowitz’s global bifurcation theorem, based on the computation of the Leray–Schauder degree, is an important part of nonlinear functional analysis and probably the only global result in bifurcation theory. It provides global information on the branch of solutions emanating from a bifurcation point. The applications to nonlinear Sturm–Liouville problems as well as nonlinear elliptic problems are presented. They are studied in Sect. 3.5.

In Sect. 3.6, we introduce various applications of the Leray–Schauder degree. Schaefer’s fixed-point theorem is more convenient in application to partial differential equations. Relying on the degree arguments, multiple solutions for semilinear elliptic equations are studied. In particular, some interesting multiple results are obtained in combination with the super- and sub-solutions method. When we are concerned with positive solutions of differential equations, one modifies the degree theory to that for compact mappings defined on closed convex sets. Moreover, we present some other applications: The Krein–Rutmann theory for positive linear operators, the existence of a positive solution for the superlinear elliptic problem, and bridging solutions of disjoint domains etc. in order to expose the extensive applicability of the degree theory.

In Sect. 3.7, various extensions of the Leray–Schauder degree are discussed, including the degrees for set contraction mappings, condensing mappings, setvalued mappings, Fredholm mappings, etc.

# 3.1 The Notion of Topological Degree

The topological degree was originally introduced by H. Poincar´e in the qualitative study of ODEs. Firstly, he defined the index of a singularity $P _ { 0 }$ as follows: Let $P _ { 0 } = ( x _ { 0 } , y _ { 0 } )$ be a singularity of the plane vector field (F(x,y), G(x,y)), i.e., $F ( x _ { 0 } , y _ { 0 } ) = G ( x _ { 0 } , y _ { 0 } ) = 0$ . Let $C$ be a closed curve surrounding $P _ { 0 }$ in the phase plane. We count the variation of the angles of the tangents of the flow:

$$
\left\{ \begin{array}{l} \dot {x} = F (x, y) \\ \dot {y} = G (x, y) \end{array} \right.
$$

generated by $( F , G )$ along $C$ counterclockwise. This amount should be a multiple of $2 \pi$ , and is denoted by $\operatorname { i n d } ( P _ { 0 } )$ . One may measure this amount by observing the compass moving along the curve $C$ in a magnetic field.

Afterwards, he defined the winding number of a closed curve $C$ , on which there are no singularities of the vector field. Different from the index, at this time, $C$ is not necessarily just enclosing one single singularity. He noticed:

(1) The winding number is invariant under deformation of curves $C$ , if there is no singularity on these curves $C$ .   
(2) The winding number for a given curve $C$ is invariant under deformation of vector fields, if there is no singularity on $C$ .   
A direct consequence follows:   
(3) Let $D$ be the domain enclosed by $C$ . If the winding number of $C$ is nonzero, then there must be a singularity of the vector field in $D$ .

For a given vector field $f = ( F , G )$ , a closed curve $C$ (or equivalently, the enclosed domain $D$ ), we denote the winding number by $\deg \left( f , D \right)$ , More generally, instead of the zeroes of $f$ , sometimes we study the solutions of

$$
f (x, y) = P _ {0}
$$

for a given $P _ { 0 } \in \mathbb { R } ^ { 2 }$ ; we denote the winding number of $f - P _ { 0 }$ by $\deg \left( f , D , P _ { 0 } \right)$ .

In order to extend the concept of winding numbers to higher-dimensional vector fields, we would rather make some stronger restrictions on $f$ at first. It makes the geometric characterization of winding numbers easy to understand.

Let us first assume that $f$ is an analytic function: $f ( z ) = F ( z ) + i G ( z )$ , where $z = x + i y$ and $w _ { 0 } = p _ { 0 } + i q _ { 0 }$ are complex numbers. Let $\Omega$ be a bounded open domain in $\mathbb { C }$ with boundary $\partial \Omega$ .

Suppose $f ( z ) \neq w _ { 0 } \forall z \in \partial \Omega$ , then we have

$$
\begin{array}{l} \deg (f, \Omega , w _ {0}) = \frac {1}{2 \pi} \int_ {\partial \Omega} d \arg (f (z) - w _ {0}) \\ = \frac {1}{2 \pi i} \int_ {\partial \Omega} d \log (f (z) - w _ {0}) \\ = \frac {1}{2 \pi i} \int_ {\partial \Omega} \frac {f ^ {\prime} (z)}{f (z) - w _ {0}} d z \\ = \sum_ {z _ {j} \in f ^ {- 1} \left(w _ {0}\right) \cap \Omega} \sigma_ {j}, \tag {3.1} \\ \end{array}
$$

where $\sigma _ { j }$ is the multiplicity of $z _ { j }$ , i.e.,

$$
f (z) = w _ {0} + c _ {j} \left(z - z _ {j}\right) ^ {\sigma_ {j}} + \circ \left(\left| z - z _ {j} \right| ^ {\sigma_ {j}}\right), \tag {3.2}
$$

as $| z - z _ { j } | \to 0$ , and $c _ { j } \neq 0$ .

If $f$ is not analytic but differentiable, let $f = ( F , G ) , z = ( x , y )$ and $w _ { 0 } =$ $( p _ { 0 } , q _ { 0 } )$ . Assume $f ( z ) \neq w _ { 0 } , \forall z \in \partial \Omega$ and

$$
\det  f ^ {\prime} (z) = \det  \frac {\partial (F , G)}{\partial (x , y)} (z) \neq 0 \quad \forall z \in f ^ {- 1} (w _ {0})  ,
$$

then the point set $\Omega \cap f ^ { - 1 } ( w _ { 0 } )$ is finite, from the IFT; and

$$
\begin{array}{l} \deg (f, \Omega , w _ {0}) = \frac {1}{2 \pi} \int_ {\partial \Omega} d \arg (f (z) - w _ {0}) \\ = \frac {1}{2 \pi} \int_ {\partial \Omega} d \arctan \frac {G - q _ {0}}{F - p _ {0}} \\ = \frac {1}{2 \pi} \int_ {\partial \Omega} \frac {(F - p _ {0}) d G - (G - q _ {0}) d F}{(F - p _ {0}) ^ {2} + (G - q _ {0}) ^ {2}}. \\ \end{array}
$$

Applying Green’s formula, it equals

$$
\sum_ {z _ {j} \in f ^ {- 1} \left(w _ {0}\right) \cap \Omega} \int_ {\partial B _ {\varepsilon} \left(z _ {j}\right)} \frac {\left(F - p _ {0}\right) d G - \left(G - q _ {0}\right) d F}{\left(F - p _ {0}\right) ^ {2} + \left(G - q _ {0}\right) ^ {2}} = \sum_ {z _ {j} \in f ^ {- 1} \left(w _ {0}\right) \cap \Omega} \operatorname {s g n} \det  f ^ {\prime} \left(z _ {j}\right), \tag {3.3}
$$

where $\varepsilon > 0$ is small enough that $B _ { \varepsilon } ( z _ { i } ) \cap B _ { \varepsilon } ( z _ { j } ) = \emptyset \forall z _ { i } \neq z _ { j }$ in $f ^ { - 1 } ( w _ { 0 } ) \cap \Omega$ , provided $\begin{array} { r } { \operatorname* { d e t } f ^ { \prime } ( z ) \neq 0 \forall z \in \bigcup _ { z _ { i } \in f _ { \ . } ^ { - 1 } ( w _ { 0 } ) \cap \Omega } B _ { \varepsilon } ( z _ { i } ) } \end{array}$ .

∈ ∩The last formula provides a clue to defining the topological degree for higher-dimensional mappings. For each root of $f ( z ) = w _ { 0 }$ , we assign a signed number, which is the signature of the determinant at this point, and then the topological degree is the summation of all these signed numbers.

Before going to the definition, we need a special case of the Sard theorem. Since we have not proved the Sard theorem in Chap. 1, now we shall present a proof for the special case, due to its simplicity. However, we refer the reader to Milnor [Mi 2] for a proof of the general theorem, which can also be obtained by combining our proof for the special case with an inductive argument.

Theorem 3.1.1 (Sard) Let $\Omega \subset \mathbb { R } ^ { n }$ be an open set, and let $f : \Omega \to \mathbb { R } ^ { n }$ be a $C ^ { 1 }$ map. Write

$$
\mathbf {Z} = \left\{x \in \Omega \mid \det  f ^ {\prime} (x) = 0 \right\}.
$$

Then $f ( \mathbf { Z } )$ is a zero measure set.

Proof. Consider a closed cube $C \subset \Omega$ , each side with length $a$ . We divide $C$ into $N ^ { n }$ configuration cubes $C _ { i } ^ { ( 1 ) } , i = 1 , . . . , N ^ { N }$ , with $\mathrm { i n t } ( C _ { i } ^ { ( 1 ) } ) \cap \mathrm { i n t } ( C _ { j } ^ { ( 1 ) } ) =$ $\varnothing , i \neq j$ , each side with length $\frac { u } { N }$ . Thus on one hand,

$$
\parallel f (x) - f \left(x _ {0}\right) \parallel \leqslant K _ {C} \parallel x - x _ {0} \parallel \leqslant K _ {C} \frac {\sqrt {n} a}{N},
$$

$\forall x , x _ { 0 } \in C _ { i } ^ { ( 1 ) }$ for some $i$ , where $K _ { C } = \operatorname* { m a x } { \{ \| ~ f ^ { \prime } ( x ) ~ \| ~ | ~ x \in C \} }$

On the other hand, $\forall \varepsilon > 0 \exists$ an integer $N$ such that

$$
\| f (x) - f \left(x _ {0}\right) - f ^ {\prime} \left(x _ {0}\right) \left(x - x _ {0}\right) \| <   \varepsilon \| x - x _ {0} \| \leqslant \varepsilon \frac {\sqrt {n} a}{N}.
$$

Let $x _ { 0 } \in \mathbf { Z }$ , $f ^ { \prime } ( x _ { 0 } )$ is not invertible. Thus $f ^ { \prime } ( x _ { 0 } ) \mathbb { R } ^ { n }$ cannot span the whole space $\mathbb { R } ^ { n }$ , but is included in a $( n - 1 )$ -dimensional linear subspace $H$ . It follows that

$$
\operatorname {d i s t} (f (x), f (x _ {0}) + H) \leqslant \frac {\varepsilon \sqrt {n} a}{N} \quad \forall x \in C _ {i} ^ {(1)},
$$

in which C(1) ${ C } _ { i } ^ { ( 1 ) }$ is the small cube including $x _ { 0 }$ . We conclude that $f ( C _ { i } ^ { ( 1 ) } )$ is included in a cube, whose sides have length $\frac { 2 \varepsilon { \sqrt { n } } a } { N }$ , $\frac { 2 K _ { C } { \sqrt { n } } a } { N }$ $\cdot { \frac { 2 K _ { C } { \sqrt { n } } a } { N } }$ , respectively. Therefore

$$
\begin{array}{l} m ^ {*} (f (C \cap \mathbf {Z})) \leqslant \sum_ {C _ {i} ^ {(1)} \cap \mathbf {Z} \neq \emptyset} m ^ {*} (f (C _ {i} ^ {(1)} \cap \mathbf {Z})) \\ \leqslant \sum_ {C _ {i} ^ {(1)} \cap \mathbf {Z} \neq \emptyset} m ^ {*} (f \left(C _ {i} ^ {(1)}\right)) \\ \leqslant 2 ^ {n} K _ {C} ^ {n - 1} n ^ {\frac {n}{2}} a ^ {n} \varepsilon , \\ \end{array}
$$

where $m ^ { * }$ is the Lebesgue outer measure. Since $\varepsilon > 0$ and the cube $C$ are arbitrary, we obtain $m ^ { * } ( f ( \mathbf { Z } ) ) = 0$ . □

Suppose that $Y$ is an $n$ -manifold, a subset $W \subset Y$ is called a null set, if for each chart $( \varphi , U )$ of $Y$ , the set $\varphi ( U \cap W ) \subset \mathbb { R } ^ { n }$ is a null set.

Let us recall the definitions of regular/critical values/points of a map between two Banach manifolds. Sard’s theorem asserts that if $X$ and $Y$ are $n$ -manifolds, then for any $f \in C ^ { 1 } ( X , Y )$ , the set of critical values of $f$ is a null set.

Let $X _ { 0 } , Y$ be two oriented smooth $n$ -dimensional manifolds, and $X \subset X _ { 0 }$ be an open subset satisfying the condition that ${ \overline { { X } } } = X \cup \partial X$ is compact. If $f \in C ( { \overline { { X } } } , Y ) \cap C ^ { 1 } ( X , Y )$ , and if $y _ { 0 }$ is a regular value of $f$ then

$$
f ^ {- 1} (y _ {0}) = \{x \in X | f (x) = y _ {0} \}
$$

must be a finite set, from the IFT. Define

$$
f ^ {- 1} (y _ {0}) = \left\{x _ {1}, \dots , x _ {k} \right\}.
$$

Assuming that $y _ { 0 } \not \in f ( \partial X )$ is a regular value, we define

$$
\deg (f, X, y _ {0}) = \sum_ {j = 1} ^ {k} \operatorname {s g n} \det  f ^ {\prime} \left(x _ {j}\right). \tag {3.4}
$$

Again we let $\mathbf { Z }$ denote the set of critical points of $f$ .

We shall extend the definition to continuous mappings in three steps:

I. The special case $f \in C ( { \overline { { X } } } , Y ) \cap C ^ { 2 } ( X , Y ) , y _ { 0 } \not \in f ( { \mathbf { Z } } ) \cup f ( \partial X ) .$

In order to remove the assumptions on the regularity and the condition $f \in C ^ { 1 } ( X , Y )$ , we would rather express (3.4) in an integration form.

Let $U _ { j }$ be a neighborhood of $x _ { j }$ , such that

$$
f: U _ {j} \to f (U _ {j})
$$

is a diffeomorphism, and that $U _ { i } \cap U _ { j } = \emptyset , \ \forall i \neq j$ , where $i , j = 1 , 2 , \ldots , k$ . Set

$$
V = \bigcap_ {j = 1} ^ {k} f (U _ {j}) .
$$

This is a neighborhood of $y _ { 0 }$ . Let us choose a $C ^ { \infty }$ $n$ -form on $Y$ :

$$
\mu = \psi (y) d y _ {1} \wedge \dots \wedge d y _ {n},
$$

(in the following, we write simply $d y = d y _ { 1 } \wedge . . . \wedge d y _ { n } )$ such that

$$
\operatorname {s u p p} \psi \subset V \cap (Y \backslash f (\partial X)),
$$

and

$$
\int_ {Y} \mu = 1.
$$

Since $\operatorname* { d e t } f ^ { \prime } ( x )$ has the same sign in each $U _ { j }$ , we have

$$
\begin{array}{l} \int_ {X} \mu \circ f = \sum_ {j = 1} ^ {k} \int_ {U _ {j}} \mu \circ f \\ = \sum_ {j = 1} ^ {k} \int_ {U _ {j}} \psi (f (x)) \det f ^ {\prime} (x) d x \\ = \sum_ {j = 1} ^ {k} \operatorname {s g n} \det  f ^ {\prime} (x _ {j}) \int_ {U _ {j}} \psi \circ f (x) | \det  f ^ {\prime} (x) | d x \\ = \sum_ {j = 1} ^ {k} \operatorname {s g n} \det  f ^ {\prime} (x _ {j}) \int_ {f (U _ {j})} \psi (y) d y \\ = \sum_ {j = 1} ^ {k} \operatorname {s g n} \det  \left(f ^ {\prime} \left(x _ {j}\right)\right) \\ = \deg (x, X, y _ {0}). \tag {3.5} \\ \end{array}
$$

In the integration form, $\mu$ is arbitrarily chosen. We shall prove that the integral $\int _ { X } \mu \circ f$ does not depend on the special choice of $\mu$ .

We call the chart $( g , \Omega )$ a good chart at $y _ { 0 } \in Y$ , if

(1) $g ( y _ { 0 } ) = \theta$   
(2)

A $C ^ { \infty } - n$ -form $\mu$ on $Y$ is called admissible if $\mu = \psi ( y ) d y$ satisfies

(1) supp $\psi \subset \Omega \cap ( Y \backslash f ( \partial X ) )$ for a good chart $( g , \Omega )$ at $y _ { 0 }$   
(2) $\textstyle \int _ { Y } \mu = 1$

Lemma 3.1.2 Suppose that $\mu = \psi ( y ) d y$ is a $C ^ { \infty } - n$ -form on $Y$ satisfying

(1) supp $\psi \subset \Omega$ , and $( g , \Omega )$ is a good chart at $y _ { 0 }$   
(2) $\textstyle \int _ { Y } \mu = 0$

Then there exists an $( n - 1 )$ form $\omega$ such that supp $\omega \subset \Omega$ and $d \omega = \mu$

Proof. In the coordinates,

$$
\begin{array}{l} \Omega \xrightarrow {g} \mathrm {I} _ {n} \\ \begin{array}{c c} \cap & \cap \\ \hline \end{array} \\ Y \qquad \mathbb {R} ^ {n} \\ \end{array}
$$

one may assume $\operatorname { s u p p } \psi \subset \operatorname { I } ^ { \prime \prime }$ and $\begin{array} { r } { \int { \psi ( y ) d y } = 0 } \end{array}$ . What we want to prove is the existence of a function $\nu \in C ^ { 1 } ( \mathbb { R } ^ { n } , \mathbb { R } ^ { n } )$ such that

supp $\nu \subset \mathbf { l } _ { n }$ and div $\nu = \psi$ .

Indeed, let $\nu = ( \nu _ { 1 } , \nu _ { 2 } , \ldots , \nu _ { n } )$ , then

$$
\omega = \nu_ {1} d y _ {2} \wedge \dots \wedge d y _ {n} + \dots + (- 1) ^ {n} \nu_ {n} d y _ {1} \wedge \dots \wedge d y _ {n - 1} \text {s a t i s f i e s} d \omega = \mu .
$$

We prove the conclusion by mathematical induction. For $n = 1$ , one takes

$$
\nu (y) = \int_ {- \infty} ^ {y} \psi (t) d t,
$$

then $\nu$ is as required.

Assume the lemma is true for $n = k$ . Let

$$
\phi (\hat {y}) = \int_ {- \infty} ^ {\infty} \psi (\hat {y}, y _ {k + 1}) d y _ {k + 1},
$$

where $y = ( \hat { y } , y _ { k + 1 } ) , \ \hat { y } = ( y _ { 1 } , \dots , y _ { k } )$ . Then

$$
\operatorname {s u p p} \phi \subset \mathrm {I} _ {k} \triangleq (- 1, 1) ^ {k},
$$

and

$$
\int \phi (\hat {y}) d \hat {y} = \int \psi (y) d y = 0.
$$

By the hypothesis of induction, there exists $\omega \in C ^ { 1 } ( \mathbb { R } ^ { k } , \mathbb { R } ^ { k } )$ satisfying

$$
\operatorname {s u p p} \omega \subset \mathrm {I} _ {k}, \operatorname {d i v} \omega = \phi .
$$

Choose

$$
\tau \in C ^ {1} (\mathbb {R} ^ {1}), \operatorname {s u p p} \tau \subset \mathrm {I} _ {1}, \int_ {- \infty} ^ {\infty} \tau (t) d t = 1,
$$

then

$$
\int_ {- \infty} ^ {\infty} [ \psi (\hat {y}, y _ {k + 1}) - \phi (\hat {y}) \tau (y _ {k + 1}) ] d y _ {k + 1} = 0.
$$

Let

$$
\nu_ {k + 1} (\hat {y}, y _ {k + 1}) = \int_ {- \infty} ^ {y _ {k + 1}} [ \psi (\hat {y}, t) - \phi (\hat {y}) \tau (t) ] d t,
$$

then

$$
\frac {\partial}{\partial y _ {k + 1}} \nu_ {k + 1} (\hat {y}, y _ {k + 1}) = \psi (\hat {y}, y _ {k + 1}) - \phi (\hat {y}) \tau (y _ {k + 1}).
$$

This means that $\psi = \mathrm { d i v } \nu$ , where

$$
\nu = \left(\omega_ {1} (\hat {y}) \tau \left(y _ {k + 1}\right), \dots , \omega_ {k} (\hat {y}) \tau \left(y _ {k + 1}\right), \nu_ {k + 1} (\hat {y}, y _ {k + 1})\right),
$$

and

$$
\left(\omega_ {1}, \dots , \omega_ {k}\right) = \omega .
$$

□

In order to prove that the integral $\int _ { X } \mu \circ f$ does not depend on $\mu$ , we assume that $\nu , \mu$ are two admissible $n$ forms with respect to a good chart $( g , \Omega )$ at $y _ { 0 }$ . According to Lemma 3.1.2, there exists an $( n - 1 )$ form $\omega$ such that $d \omega = \nu - \mu$ , and then

$$
\int_ {X} \nu \circ f - \int_ {X} \mu \circ f = \int_ {X} d \omega \circ f.
$$

If we can prove

$$
\int_ {X} (d \omega) \circ f = \int_ {X} d (\omega \circ f), \tag {3.6}
$$

then by the Stokes theorem, we conclude:

$$
\int_ {X} \nu \circ f = \int_ {X} \mu \circ f.
$$

We are going to prove (3.6). First, we need

Lemma 3.1.3 If $f \in C ( X , Y ) \cap C ^ { 2 } ( X , Y )$ , then for any $( n - 1 )$ form $\omega$ , we have

$$
d (\omega \circ f) = d \omega \circ f.
$$

Proof. In local coordinates,

$$
\omega = \sum_ {j = 1} ^ {n} \nu_ {j} (y) d y _ {1} \wedge \dots \wedge d \hat {y} _ {j} \wedge \dots \wedge d y _ {n},
$$

where

$$
d y _ {1} \wedge \dots \wedge d \hat {y} _ {j} \wedge \dots \wedge d y _ {n} = d y _ {1} \wedge \dots \wedge d y _ {j - 1} \wedge d y _ {j + 1} \wedge \dots \wedge d y _ {n}.
$$

Thus

$$
\omega \circ f = \sum_ {j = 1} ^ {n} \left(\sum_ {k = 1} ^ {n} \nu_ {k} \circ f (x) J _ {f} ^ {k j} (x)\right) d x _ {1} \wedge \dots \wedge d \hat {x} _ {j} \wedge \dots \wedge d x _ {n},
$$

where $J _ { f } ^ { k i }$ is the $( k , i )$ cofactor of the Jacobian determinant $\operatorname* { d e t } f ^ { \prime } ( x ) = J _ { f } ( x )$ . Hence

$$
\begin{array}{l} d (\omega \circ f) \\ = \sum_ {i, k = 1} ^ {n} \left[ \sum_ {l = 1} ^ {n} \frac {\partial \nu_ {k} \circ f (x)}{\partial y _ {l}} J _ {f} ^ {k i} (x) \cdot \frac {\partial f _ {l}}{\partial x _ {i}} + \nu_ {k} \circ f (x) \frac {\partial J _ {f} ^ {k i} (x)}{\partial x _ {i}} \right] d x _ {1} \wedge \dots \wedge d x _ {n} \\ = \left[ \sum_ {k, l = 1} ^ {n} \frac {\partial \nu_ {k} \circ f (x)}{\partial y _ {l}} \sum_ {i = 1} ^ {n} \frac {\partial f _ {l}}{\partial x _ {i}} J _ {f} ^ {k i} (x) + \sum_ {k = 1} ^ {n} \nu_ {k} \circ f (x) \sum_ {i} \frac {\partial J _ {f} ^ {k i} (x)}{\partial x _ {i}} \right] \\ \times d x _ {1} \wedge \dots \wedge d x _ {n} \\ \end{array}
$$

in which

$$
f = \left(f _ {1}, \dots , f _ {n}\right).
$$

Let

$$
g = (- 1) ^ {k - 1} \left(f _ {1}, \dots , \hat {f} _ {k}, \dots , f _ {n}\right),
$$

then

$$
J _ {f} ^ {k i} = (- 1) ^ {i - 1} \det \left(\frac {\partial g}{\partial x _ {1}}, \dots , \frac {\hat {\partial g}}{\partial x _ {i}}, \dots , \frac {\partial g}{\partial x _ {n}}\right).
$$

Hence

$$
\begin{array}{l} \sum_ {i = 1} ^ {n} \frac {\partial}{\partial x _ {i}} J _ {f} ^ {k i} = \sum_ {i = 1} ^ {n} (- 1) ^ {i - 1} \sum_ {l \neq i} \det  \left(\frac {\partial g}{\partial x _ {1}}, \dots , \frac {\partial^ {2} g}{\partial x _ {i} \partial x _ {l}}, \dots , \frac {\partial g}{\partial x _ {i}}, \dots , \frac {\partial g}{\partial x _ {n}}\right) \\ = \sum_ {i = 1} ^ {n} \left(\sum_ {l > i} (- 1) ^ {i - 1 + l - 2} \det  \left\{\frac {\partial^ {2} g}{\partial x _ {i} \partial x _ {l}}, \frac {\partial g}{\partial x _ {1}}, \dots , \frac {\partial \hat {g}}{\partial x _ {i}}, \dots , \frac {\partial g}{\partial x _ {n}} \right\} \right. \\ \left. + \sum_ {i > l} (- 1) ^ {i - 1 + l - 1} \det  \left\{\frac {\partial^ {2} g}{\partial x _ {l} \partial x _ {i}}, \frac {\partial g}{\partial x _ {1}}, \dots , \frac {\partial \hat {g}}{\partial x _ {i}}, \dots , \frac {\partial g}{\partial x _ {n}} \right\}\right) = 0. \\ \end{array}
$$

From this, together with

$$
\sum_ {i = 1} ^ {n} \frac {\partial f _ {l}}{\partial x _ {i}} J _ {f} ^ {k i} (x) = \delta_ {k l} J _ {f} (x)
$$

follows

$$
d (\omega \circ f) = \sum_ {k = 1} ^ {n} \frac {\partial \nu_ {k} \circ f (x)}{\partial y _ {k}} J _ {f} (x) d x _ {1} \wedge \dots \wedge d x _ {n} = (d \omega) \circ f.
$$

Now we arrive at:

Theorem 3.1.4 Assume $f \in C ( { \overline { { X } } } , Y ) \cap C ^ { 2 } ( X , Y )$ . If $y _ { 0 } \not \in f ( \mathbf { Z } ) \cap f ( \partial X )$ and $\mu$ is an admissible $C ^ { \infty }$ n form with respect to a good chart $( g , \Omega )$ at $y _ { 0 }$ , with $s u p p ( \mu ) \subset \Omega \cap ( Y \backslash f ( \partial X ) )$ , then $\scriptstyle \int _ { X } \mu \circ f$ is a constant independent of $\mu$ .

II. Removal of the restriction $y _ { 0 } \not \in f ( \mathbf { Z } )$

Combining Theorem 3.1.4 and (3.5), we know that

$$
\deg (f, X, y _ {0}) = \int_ {X} \mu \circ f ,
$$

if $f \in C ( { \overline { { X } } } , Y ) \cap C ^ { 2 } ( X , Y )$ and $y _ { 0 } \not \in f ( \mathbf { Z } ) \cup f ( \partial X )$ , where $\mu$ is an arbitrary admissible $C ^ { \infty }$ $n$ form with respect to a good chart at $y _ { 0 }$ . Note that the integral on the right-hand side of the above formula does not contain $y _ { 0 }$ explicitly.

Further recall that the set of regular values is dense in $f ( X )$ according to the Sard theorem, thus if we take the integral $\int _ { X } \mu \circ f$ to be the definition of $\deg ( f , X , y _ { 0 } )$ , the assumption $y _ { 0 } \not \in f ( \mathbf { Z } )$ is not necessary. In fact, this integral is defined for all $y _ { 0 } \not \in f ( \partial X )$ , and is integer valued (the left-hand side of the formula (3.5)) if $y _ { 0 }$ belongs to the dense (regular value) set. Hence $\scriptstyle \int _ { X } \mu \circ f$ is integer valued. Therefore the restriction $y _ { 0 } \not \in f ( \mathbf { Z } )$ can be removed when we use $\int _ { X } \mu \circ f$ as the definition of the $\deg \left( f , X , y _ { 0 } \right)$ .

# III. Extension to continuous mappings

We first prove:

Lemma 3.1.5 Let $\phi : { \overline { { X } } } \times [ 0 , 1 ] \ { \overset { C } { \longrightarrow } } \ Y$ satisfy $\phi ( \cdot , t ) \in C ^ { 2 } ( X , Y ) \ \forall t \in [ 0 , 1 ]$ If $y _ { 0 } \not \in \phi ( \partial X \times [ 0 , 1 ] )$ , then $\deg ( \phi ( \cdot , t ) , X , y _ { 0 } )$ is a constant independent of $t$ .

Proof. Since $\phi ( \partial X \times [ 0 , 1 ] )$ is closed, there is a good neighborhood $\Omega$ at $y _ { 0 }$ such that $\Omega \cap \phi ( \partial X \times [ 0 , 1 ] ) = \emptyset$ . If we choose $\psi \in C ^ { \infty } ( Y , \mathbb { R } ^ { 1 } )$ such that $\mathrm { s u p p } \psi \subset \Omega , \ \int _ { Y } \psi d y = 1$ , then $\mu = \psi ( y ) d y$ is admissible, and

$$
\deg (\phi (\cdot , t), X, y _ {0}) = \int_ {X} \mu \circ \phi (\cdot , t). \tag {3.7}
$$

The right-hand side is integer valued and is continuous in $t$ , hence it must be a constant. □

Now we may extend the definition of degree to mappings in $C ( { \overline { { X } } } , Y )$ by approximation.

By the use of the Whitney embedding theorem, we embed $Y$ into $\mathbb { R } ^ { N }$ as a regular submanifold for large $N$ . Thus there exist a tubular neighborhood $W$ of $Y$ and a $C ^ { \infty }$ retract $r : W \to Y$ . Since we assumed $y _ { 0 } \not \in f ( \partial X )$ , one may choose $0 < \varepsilon < d _ { R ^ { N } } ( r ^ { - 1 } ( y _ { 0 } ) , f ( \partial X ) )$ such that the $\varepsilon$ neighborhood of $Y$ is in $W$ , where $d _ { R ^ { N } } ( \cdot , \cdot )$ is the distance in $R ^ { N }$ . According to the Weierstrass approximation theorem and the partition of unity, there exists $g \in C ^ { 2 } ( X , \mathbb { R } ^ { N } )$ satisfying

$$
\| f - g \| _ {C (\overline {{X}}, \mathbb {R} ^ {N})} <   \varepsilon . \tag {3.8}
$$

Thus for such $g$ , $\deg { ( r \circ g , X , y _ { 0 } ) }$ is well defined.

Moreover, we shall prove that

$$
\deg (r \circ g _ {1}, X, y _ {0}) = \deg (r \circ g _ {2}, X, y _ {0})
$$

for $g _ { 1 } , g _ { 2 } \in C ^ { 2 } ( \overline { { X } } , \mathbb { R } ^ { N } )$ both satisfying (3.8).

Indeed, define $h : \overline { { X } } \times [ 0 , 1 ] \longrightarrow \mathbb { R } ^ { N }$ by

$$
h (x, t) = (1 - t) g _ {1} (x) + t g _ {2} (x),
$$

and $\phi : \overline { { { X } } } \times [ 0 , 1 ] \to Y$ by $\phi = r \circ h$ . Then

$$
\operatorname {d i s t} _ {\mathbb {R} ^ {N}} \left(\phi (x, t), y _ {0}\right) = 0 \Leftrightarrow \operatorname {d i s t} _ {\mathbb {R} ^ {N}} \left(h (x, t), r ^ {- 1} \left(y _ {0}\right)\right) = 0.
$$

But,

$$
\operatorname {d i s t} _ {\mathbb {R} ^ {N}} \left(h (\partial X, t), r ^ {- 1} (y _ {0})\right) \geqslant \operatorname {d i s t} _ {\mathbb {R} ^ {N}} \left(f (\partial X), r ^ {- 1} (y _ {0})\right) - \varepsilon > 0,
$$

this proves that $y _ { 0 } \not \in \phi ( \partial X \times [ 0 , 1 ] )$ . Applying Lemma 3.1.5, we obtain

$$
\deg (r \circ g _ {1}, X, y _ {0}) = \deg (r \circ g _ {2}, X, y _ {0}).
$$

We are now in a position to define the Brouwer degree for continuous mappings.

Definition 3.1.6 (Brouwer degree) Let $X _ { 0 } , Y$ be two oriented smooth $n -$ manifolds, and let $X \subset X _ { 0 }$ be an open subset with compact closure $\overline { { X } }$ . For $f \in C ( { \overline { { X } } } , Y ) , y _ { 0 } \notin f ( \partial X )$ , to the triple $( f , X , y _ { 0 } )$ we define

$$
\deg (f, X, y _ {0}) = \deg (r \circ g, X, y _ {0}),
$$

where $T$ is a $C ^ { \infty }$ retract of a tubular neighborhood $W$ of $Y$ to $Y$ , and $g$ is $a$ map defined in (3.8). This is called the Brouwer degree of $f$ .

The Brouwer degree is integer valued.

It is not difficult to verify that Brouwer degree does not depend on the special choice of $W$ and $r$ .

By definition, if $X = \varnothing$ , then $\deg \left( f , X , y _ { 0 } \right) = 0$ .

In dealing with mappings between complex manifolds, we identify $C ^ { n }$ with $R ^ { 2 n }$ by the canonical isomorphism $z \mapsto ( x , y )$ , where $z \ = \ x + i y$ for $z \in$ $C ^ { n }$ , $x , y \in R ^ { n }$ . Similarly for the map $f \mapsto ( u , v )$ , where $f : \Omega \to C ^ { n }$ , and $f = u + i v$ .

As an exercise, readers can verify that if $f$ is analytic then $\operatorname* { d e t } ( f ^ { \prime } ( z ) ) > 0$ .

# 3.2 Fundamental Properties and Calculations of Brouwer Degrees

The Brouwer degree has the following fundamental properties.

(1) (Homotopy invariance). If $\phi : \overline { { { X } } } \times [ 0 , 1 ] \to Y$ is continuous and $y _ { 0 } \notin$ $\phi ( \partial X \times [ 0 , 1 ] )$ , then

$$
\deg (\phi (\cdot , t), X, y _ {0}) = \text {c o n s t a n t}.
$$

Proof. Using the above notations, we choose

$$
0 <   \epsilon <   d _ {R ^ {N}} (r ^ {- 1} (y _ {0}), \phi (\partial X \times [ 0, 1 ])),
$$

where $d _ { R ^ { N } } ( \cdot , \cdot )$ is the distance in $R ^ { N }$ . By the above approximation, there is $\hat { \phi } \in C ^ { 2 } ( \overline { { X } } \times [ 0 , 1 ] , Y )$ such that

$$
d _ {C (\overline {{X}} \times [ 0, 1 ], Y)} (\phi , \hat {\phi}) <   \varepsilon .
$$

From Definition 3.1.6 and Lemma 3.1.5 it follows that

$$
\deg (\phi (\cdot , t), X, y _ {0}) = \deg (\hat {\phi} (\cdot , t), X, y _ {0}) = \text {c o n s t a n t}.
$$

More generally, if $ \phi ~ : ~ X \times [ 0 , 1 ] ~ \to ~ Y$ is continuous, and if $t \mapsto$ $X _ { t } ~ \subset ~ \stackrel { \circ } { X } \forall t \in [ 0 , 1 ]$ 。 are continuously deformed open domains, assume $y _ { 0 } \not \in \bigcup _ { t \in [ 0 , 1 ] } \phi ( \partial X _ { t } , t )$ , then

$$
\deg (\phi (\cdot , t), X _ {t}, y _ {0}) = \text {c o n s t a n t}.
$$

(2) If $y _ { 1 } , y _ { 2 } \in Y \backslash f ( \partial X )$ are in the same component, then

$$
\deg (f, X, y _ {1}) = \deg (f, X, y _ {2}).
$$

In particular, if $\partial X = 0$ , and $Y$ is connected, then $\deg ( f , X , y _ { 0 } )$ is independent of $y _ { 0 }$ . (In this case, it is written as $\deg ( f , X ) ,$ ).

Proof. By definition, $\deg ( f , X , y _ { 0 } )$ is continuous in $y _ { 0 } \in Y \backslash f ( \partial X )$ , and then is a constant, if $f \in C ( X , Y ) \cap C ^ { 2 } ( X , Y )$ . By approximation, we obtain the conclusion. □

Let $\Omega$ be a connected component of $Y \backslash f ( \partial X )$ . According to (2), $\forall y _ { 0 } \in$ $\Omega , \deg ( f , X , y _ { 0 } )$ is constant. One may write $\deg ( f , X , \Omega )$ instead, if there is no confusion.

Corollary 3.2.1 If $Y ~ = ~ \mathbb { R } ^ { n }$ , $f \in C ( { \overline { { X } } } , Y )$ and $y _ { 0 } \notin f ( \partial X )$ , then the degree $\deg ( f , X , y _ { 0 } )$ only depends on the restriction of $f$ on $\partial X$ , i.e., ${ \hat { f } } =$ $f | _ { \partial X }$ .

Proof. Let $g \in C ( { \overline { { X } } } , Y )$ satisfy $\hat { g } = g | _ { \partial X } = \hat { f }$ . Define

$$
F (x, t) = (1 - t) f (x) + t g (x) \quad t \in [ 0, 1 ].
$$

Then $y _ { 0 } \not \in F ( \partial X \times [ 0 , 1 ] )$ . By homotopy invariance, we have

$$
\deg (f, X, y _ {0}) = \deg (g, X, y _ {0}).
$$

Therefore, sometimes we write $\deg ( \hat { f } , \partial X , y _ { 0 } )$ in place of $\deg ( f , X , y _ { 0 } )$ . Returning to Lemma 3.1.5), we may rewrite (3.7) in a global version:

Corollary 3.2.2 Suppose that $f \in C ( { \overline { { X } } } , Y ) \cap C ^ { 2 } ( X , Y )$ and that $\Omega$ is a connected component of $Y \backslash f ( \partial X )$ . If $\mu$ is a smooth $n$ form with support in $\Omega$ and $\textstyle { \int _ { Y } \mu \neq 0 }$ , then

$$
\deg (f, X, \Omega) = \frac {\int_ {X} \mu \circ f}{\int_ {Y} \mu}.
$$

Proof. Let $\{ \psi _ { \alpha } | \alpha \in \wedge \}$ be a partition of unity of $Y$ , such that $\operatorname { s u p p } \psi _ { \alpha }$ $\psi _ { \alpha }$ is in a “good” neighborhood $\Omega _ { \alpha }$ . Set $\mu _ { \alpha } = \psi _ { \alpha } \cdot \mu$ , and choose $y _ { \alpha } \in$ supp $\mu _ { \alpha }$ . In the case where $\int _ { \Omega _ { \alpha } } \mu _ { \alpha } \neq 0$ , from (3.7) one has

$$
\deg (f, X, \Omega) = \deg (f, X, y _ {\alpha}) = \frac {\int_ {X} \mu_ {\alpha} \circ f}{\int_ {\Omega_ {\alpha}} \mu_ {\alpha}},
$$

i.e.,

$$
\deg (f, X, \Omega) \int_ {\Omega_ {\alpha}} \mu_ {\alpha} = \int_ {X} \mu_ {\alpha} \circ f. \tag {3.9}
$$

In the case where $\int _ { \Omega _ { \alpha } } \mu _ { \alpha } = 0$ , by Lemma 3.1.2, there exists an $n - 1$ form $\omega _ { \alpha }$ on $Y$ such that

$$
\operatorname {s u p p} \omega_ {\alpha} \subset \operatorname {s u p p} \mu_ {\alpha} \text {a n d} d \omega_ {\alpha} = \mu_ {\alpha}.
$$

Following Lemma 3.1.3,

$$
\mu_ {\alpha} \circ f = (d \omega_ {\alpha}) \circ f = d (\omega_ {\alpha} \circ f),
$$

we obtain $\int _ { X } \mu _ { \alpha } \circ f = 0$ provided by the Stokes theorem. Again (3.9) holds. By addition, we have

$$
\deg (f, X, \Omega) \int_ {Y} \mu = \int_ {X} \mu \circ f.
$$

This is the conclusion.

□

(3) (Translation invariance). If $Y = \mathbb { R } ^ { n }$ , and $y _ { 0 } \notin f ( \partial \Omega )$ , then

$$
\deg (f, X, y _ {0}) = \deg (f - y _ {0}, X, \theta).
$$

Proof. Let $f \in C ^ { 1 } ( { \overline { { X } } } , \mathbb { R } ^ { n } )$ and $y _ { 0 } \not \in f ( \mathbf { Z } )$ , then

$$
\begin{array}{l} \deg (f, X, y _ {0}) = \sum_ {x _ {i} \in f ^ {- 1} (y _ {0})} \operatorname {s g n} \det  f ^ {\prime} \left(x _ {i}\right) \\ = \sum_ {x _ {i} \in (f - y _ {0}) ^ {- 1} (\theta)} \operatorname {s g n} \det  f ^ {\prime} (x _ {i}) \\ = \deg (f - y _ {0}, X, \theta). \\ \end{array}
$$

The assertion can be obtained by approximations to $y _ { 0 }$ and $f$ .

(4) (Additivity). Let $X _ { 1 } , X _ { 2 } \subset X$ be open subsets in $X$ such that $X _ { 1 } \cap X _ { 2 } =$ $\mathrm { \ o }$ and $y _ { 0 } \not \in f ( { \overline { { X } } } \backslash ( X _ { 1 } \cup X _ { 2 } ) )$ , then

$$
\deg (f, X, y _ {0}) = \deg (f, X _ {1}, y _ {0}) + \deg (f, X _ {2}, y _ {0}).
$$

Proof. By the Whitney embedding theorem mentioned above, let

$$
\varepsilon = d _ {R ^ {N}} \left(r ^ {- 1} \left(y _ {0}\right), f (\bar {X} \backslash \left(X _ {1} \cup X _ {2}\right))\right) (> 0).
$$

One may choose $g _ { 0 } \in C ^ { 2 } ( \overline { { X } } , R ^ { N } )$ such that $d _ { C ( \overline { { { X } } } , R ^ { N } ) } ( g , f ) < \frac { \varepsilon } { 2 }$ for $g =$ $r \cup { } g _ { 0 }$ , and choose $y _ { 1 } \not \in g ( \mathbf { Z } )$ satisfying $d _ { R ^ { N } } ( y _ { 0 } , y _ { 1 } ) < \frac { \varepsilon } { 2 }$ , then

$$
\operatorname {d i s t} \left(y _ {1}, g (\bar {X} \backslash (X _ {1} \cup X _ {2}))\right) > 0.
$$

On the one hand, by (3.5) we have

$$
\begin{array}{l} \deg (g, X, y _ {1}) = \sum_ {x _ {i} \in g ^ {- 1} (y _ {1})} \operatorname {s g n} \det  g ^ {\prime} \left(x _ {i}\right) \\ = \sum_ {x _ {i} \in g ^ {- 1} (y _ {1}) \cap X _ {1}} + \sum_ {x _ {i} \in g ^ {- 1} (y _ {1}) \cap X _ {2}} \operatorname {s g n} \det  g ^ {\prime} (x _ {i}) \\ = \deg (g, X _ {1}, y _ {1}) + \deg (g, X _ {2}, y _ {1}). \\ \end{array}
$$

On the other hand, by property (2),

$$
\begin{array}{l} \deg (f, X, y _ {0}) = \deg (g, X, y _ {1}), \\ \deg (f, X _ {i}, y _ {0}) = \deg (g, X _ {i}, y _ {1}), i = 1, 2. \\ \end{array}
$$

This is due to the fact that

$$
\theta \neq t f (x) + (1 - t) g (x) - t y _ {0} - (1 - t) y _ {1},
$$

as

$$
x \in \partial X \cup \partial X _ {1} \cup \partial X _ {2} \subset \overline {{X}} \backslash (X _ {1} \cup X _ {2}), t \in [ 0, 1 ].
$$

Corollary 3.2.3 (Excision) If $K \subset { \overline { { X } } }$ is compact and $y _ { 0 } \notin \ f ( K ) \cup$ $f ( \partial X )$ , then

$$
\deg (f, X, y _ {0}) = \deg (f, X \backslash K, y _ {0}).
$$

Proof. Let $X _ { 1 } = X \backslash K , X _ { 2 } = \emptyset$ , then

$$
\overline {{X}} \backslash (X _ {1} \cup X _ {2}) = \partial X \cup K.
$$

Corollary 3.2.4 (Kronecker existence) If $y _ { 0 } \not \in f ( \partial X )$ and $\deg ( f , X ,$ $y _ { 0 } ) \neq 0$ , then $f ^ { - 1 } ( y _ { 0 } ) \neq \emptyset$ .

Proof. Assume the contrary: $f ^ { - 1 } ( y _ { 0 } ) = \varnothing , \mathrm { i . e . }$ , $y _ { 0 } \not \in \ f ( { \overline { { X } } } )$ . By Corollary 3.2.3,

$$
\deg (f, X, y _ {0}) = \deg (f, X \backslash \overline {{X}}, y _ {0}) = 0.
$$

□

Kronecker existence theorem is often used in the study of the solvability of the equation $f ( x ) = y _ { 0 }$ .

(5) (Normality). If $X \subset Y$ , and $y _ { 0 } \not \in \partial X$ , then

$$
\deg (i d, X, y _ {0}) = \left\{ \begin{array}{l l} 1, & y _ {0} \in \overset {\circ} {X} \\ 0, & y _ {0} \notin \overline {{X}}. \end{array} \right.
$$

More generally, we have:

(6) If $L$ is a $n \times n$ nondegenerate real matrix, and if $X$ is a bounded open subset and $\theta \in X \subset \mathbb { R } ^ { n }$ , then

$$
\deg (L, X, \theta) = (- 1) ^ {\beta},
$$

where $\begin{array} { r } { \beta = \sum _ { \lambda _ { j } < 0 } \beta _ { j } } \end{array}$ and $\beta _ { j }$ are the algebraic multiplicities of the negative eigenvalues $\lambda _ { j }$ of $L$ , i.e.,

$$
\beta_ {j} = \dim \cup_ {k = 1} ^ {\infty} \ker \left(\lambda_ {j} I - L\right) ^ {k}, j = 1, 2, \dots .
$$

Proof. Let $\lambda _ { 1 } , \ldots , \lambda _ { n }$ be all the eigenvalues of $L$ . By definition,

$$
\deg \left(L, X, \theta\right) = \operatorname {s g n} \det L = \operatorname {s g n} \prod_ {j = 1} ^ {n} \lambda_ {j}.
$$

Since complex roots appear in pairs and positive eigenvalues have no contribution to $\mathrm { s g n d e t } L$ , hence

$$
\operatorname {s g n} \prod_ {j = 1} ^ {n} \lambda_ {j} = \operatorname {s g n} \prod_ {\lambda_ {j} <   0} \lambda_ {j} = \prod_ {\lambda_ {i} <   0} (- 1) ^ {\beta_ {j}} = (- 1) ^ {\beta}.
$$

Remark 3.2.5 In the case where $X = Y = \mathbb { R } ^ { n }$ , the fundamental properties (1),(3),(4),(5) uniquely determine the Brouwer degree. All the other properties are their consequences.

(7) Let $f _ { i } \in C ( \Omega _ { i } , \mathbb { R } ^ { n _ { i } } )$ be open and bounded, $p _ { i } \notin f _ { i } ( \partial \Omega _ { i } ) , \ i = 1 , 2$ . Then

$$
\deg (f _ {1} \times f _ {2}, \Omega_ {1} \times \Omega_ {2}, (p _ {1}, p _ {2})) = \deg (f _ {1}, \Omega_ {1}, p _ {1}) \deg (f _ {2}, \Omega_ {2}, p _ {2}).
$$

Proof. By approximation, we may assume that $f _ { i } \in C ^ { 1 } ( \overline { { \Omega } } _ { i } , \mathbb { R } ^ { n _ { i } } )$ and that $p _ { i }$ is not a critical value of $f _ { i }$ , $i = 1 , 2$ .

Let $\mu _ { i }$ be an admissible $n _ { i }$ form with support in a good chart $( g _ { i } , \Omega _ { i } )$ at $p _ { i }$ , $i = 1 , 2$ , respectively. Then $\mu _ { 1 } \times \mu _ { 2 }$ is an admissible $n _ { 1 } + n _ { 2 }$ form with support in a good chart $( g _ { 1 } \times g _ { 2 } , \Omega _ { 1 } \times \Omega _ { 2 } )$ at $( p _ { 1 } , p _ { 2 } )$ . We have

$$
\int_ {\Omega_ {1} \times \Omega_ {2}} \left(\mu_ {1} \times \mu_ {2}\right) \circ \left(f _ {1} \times f _ {2}\right) = \int_ {\Omega_ {1}} \mu_ {1} \circ f _ {1} \int_ {\Omega_ {2}} \mu_ {2} \circ f _ {2}.
$$

□

(8) (Leray product formula). Assume that $\Omega , M \subset \mathbb { R } ^ { n }$ are open subsets with compact closures. Let $f : \overline { { \Omega } }  \mathbb { R } ^ { n } , g : \overline { { M } }  \mathbb { R } ^ { n }$ be continuous, with $f ( \overline { \Omega } ) \subset M$ . Let

$$
\triangle = M \backslash f (\partial \Omega) = \cup_ {j = 1} ^ {\infty} \triangle_ {j},
$$

where each $\triangle _ { j }$ is a connected component of $\bigtriangleup$ . Suppose $p \not \in g \circ f ( \partial \Omega ) \cup$ $g ( \partial M )$ . Then

$$
\deg (g \circ f, \Omega , p) = \sum_ {j} \deg (g, \triangle_ {j}, p) \deg (f, \Omega , \triangle_ {j}).
$$

Proof. Before going to the proof, we should explain:

(a) The meaning of $\deg ( f , \Omega , \triangle _ { j } )$ is understood according to (2).

(b) There are at most finitely many terms on the right-hand side of the formula different from 0. In fact, $g ^ { - 1 } ( p )$ is compact and

$$
g ^ {- 1} (p) \cap (\partial M \cup f (\partial \Omega)) = \varnothing .
$$

This means $\triangle = \cup _ { j = 1 } ^ { \infty } \triangle _ { j }$ covers $g ^ { - 1 } ( p )$ . Since $\triangle _ { i } \cap \triangle _ { j } = \emptyset , \ i \neq j$ , provided by finite covering, there are at most finitely many that $\triangle _ { j }$ cover $g ^ { - 1 } ( p )$ .

Now we begin the proof.

1. First suppose $f \in C ^ { 1 } ( { \overline { { \Omega } } } ) , \ g \in C ^ { 1 } ( { \overline { { M } } } )$ and that $p$ is a regular value of $g \circ f$ . We have

$$
J _ {g \circ f} (x) = J _ {g} (f (x)) \cdot J _ {f} (x).
$$

If $x \in ( g \circ f ) ^ { - 1 } ( p )$ , then $y = f ( x )$ is a regular value of $f$ , and $p$ is a regular value of $g : \overline { { M } } \cap f ( \overline { { \Omega } } ) \to \mathbb { R } ^ { n }$ , hence

$$
\begin{array}{l} \deg (g \circ f, \Omega , p) = \sum_ {x _ {j} \in (g \circ f) (p)} \operatorname {s g n} J _ {g \circ f} (x _ {j}) \\ = \sum_ {x _ {j} \in (g \circ f) (p)} \operatorname {s g n} J _ {g} (f (x _ {j})) \cdot \operatorname {s g n} J _ {f} (x _ {j}) \\ = \sum_ {y _ {k} \in g ^ {- 1} (p) \cap f (\Omega)} \operatorname {s g n} J _ {y} \left(y _ {k}\right) \left(\sum_ {x _ {j} \in f ^ {- 1} \left(y _ {k}\right)} \operatorname {s g n} J _ {f} \left(x _ {j}\right)\right) \\ = \sum_ {y _ {k} \in g ^ {- 1} (p) \cap f (\Omega)} \operatorname {s g n} J _ {y} (y _ {k}) \cdot \deg (f, \Omega , y _ {k}). \\ \end{array}
$$

Putting together all these terms, in which $y _ { k }$ belongs to the same $\triangle _ { j }$ from

$$
\deg (f, \Omega , y _ {k}) = \deg (f, \Omega , \bigtriangleup_ {j}),
$$

we have

$$
\deg \left(g \circ f, \Omega , p\right) = \sum_ {y _ {k} \in g ^ {- 1} (p) \cap \triangle_ {j}} \operatorname {s g n} J _ {g} \left(y _ {k}\right) \deg \left(f, \Omega , \triangle_ {j}\right)
$$

$$
= \sum_ {j} \deg (g, \bigtriangleup_ {j}, p) \deg (f, \Omega , \bigtriangleup_ {j}).
$$

2. Passing to continuous mappings $f \in C ( \Omega )$ , $g \in C ( { \overline { { M } } } )$ , the trouble lies in the fact that both $\bigtriangleup$ and $\{ \triangle _ { j } \}$ change as $f$ varies. We have to be careful in making the $C ^ { 1 } ( \Omega ) -$ approximations. $\forall y \in f ( \partial \Omega )$ let

$$
O _ {k} = \left\{y \in M \mid \deg (f, \Omega , y) = k \right\}.
$$

Then

$$
O _ {k} = \cup \left\{\triangle_ {j} \mid \deg (f, \Omega , \triangle_ {j}) = k \right\}, k = 0, \pm 1, \pm 2, \dots .
$$

Let

$$
\varepsilon = \operatorname {d i s t} (g ^ {- 1} (p), f (\partial \Omega)).
$$

We choose ${ \hat { f } } \in C ^ { 1 } ( { \overline { { \Omega } } } )$ such that

$$
\| \hat {f} - f \| _ {C (\overline {{\Omega}})} <   \frac {\varepsilon}{2},
$$

then

$$
p \notin g (\partial M) \cup g \circ \hat {f} (\partial \Omega). \tag {3.10}
$$

Let

$$
\hat {O} _ {k} = \{y \in M | \deg (\hat {f}, \Omega , y) = k \}.
$$

Again we choose $\hat { g } \in C ^ { 1 } ( \overline { { M } } )$ such that

$$
\| \hat {g} - g \| _ {C (\overline {{M}})} <   \operatorname {d i s t} (p, g (\partial M) \cup g \circ \hat {f} (\partial \Omega)).
$$

Since $\partial \hat { O } _ { k } \subset \hat { f } ( \partial \Omega ) \cup \partial M$ , it follows that

$$
\parallel \hat {g} - g \parallel_ {C (\overline {{M}})} <   \operatorname {d i s t} (p, g (\partial \hat {O} _ {k})) .
$$

Therefore $p \not \in \hat { g } ( \partial \hat { O } _ { k } )$ and

$$
\deg (g, \hat {O} _ {k}, p) = \deg (\hat {g}, \hat {O} _ {k}, p). \tag {3.11}
$$

It is easy to verify that

$$
g ^ {- 1} (p) \cap O _ {k} = g ^ {- 1} (p) \cap \hat {O} _ {k}.
$$

By excision,

$$
\deg (g, O _ {k}, p) = \deg (g, O _ {k} \cap \hat {O} _ {k}, p) = \deg (g, \hat {O} _ {k}, p) \tag {3.12}
$$

Combining (3.11) and (3.12) it follows that

$$
\deg (g, O _ {k}, p) = \deg (\hat {g}, \hat {O} _ {k}, p). \tag {3.13}
$$

Finally, we obtain

$$
\begin{array}{l} \deg (g \circ f, \Omega , p) = \deg (\hat {g} \circ \hat {f}, \Omega , p) \\ = \sum_ {k} \deg (\hat {g}, \hat {O} _ {k}, p) \deg (\hat {f}, \Omega , \hat {O} _ {k}) \\ = \sum_ {k} k \deg (g, O _ {k}, p) \\ = \sum_ {k} \deg (g, O _ {k}, p) \deg (f, \Omega , O _ {k}) \\ = \sum_ {j} \deg (g, \triangle_ {j}, p) \deg (f, \Omega , \triangle_ {j}). \\ \end{array}
$$

□

Fixing the map $f$ and the point $y _ { 0 }$ , the Brouwer degree is a function on the domain $X$ . It provides the information on the behavior of the mapping $f$ by all solutions included in $X$ of the equation:

$$
f (x) = y _ {0}. \tag {3.14}
$$

However, if we want to localize the notion to studying the local behavior of $f$ at an isolated solution of equation (3.14), we are led to:

Definition 3.2.6 (Brouwer index) Let $x _ { 0 }$ be an isolated solution of (3.14), i.e., ε > 0 such that there is no solution of (3.14) in $B _ { \varepsilon } ( x _ { 0 } ) \backslash \{ x _ { 0 } \}$ . We call

$$
i (f, x _ {0}, y _ {0}) = \deg (f, B _ {\varepsilon} (x _ {0}), y _ {0})
$$

the index of $f$ at $x _ { 0 }$ with respect to $y _ { 0 }$

The excision property guarantees that the definition is well defined, i.e., it does not depend on the special choice of $\varepsilon > 0$ .

We have the following properties:

(9) Suppose that $f \in C ( { \overline { { X } } } , Y ) \cap C ^ { 1 } ( X , Y )$ , and that $x _ { 0 }$ is an isolated solution of (3.14). If $\operatorname* { d e t } f ^ { \prime } ( x _ { 0 } ) \neq 0$ , then

$$
i (f, x _ {0}, y _ {0}) = (- 1) ^ {\beta},
$$

where $\beta$ is the sum of the algebraic multiplicities of the negative eigenvalues of $f ^ { \prime } ( x _ { 0 } )$ .

Proof. Since $f ^ { \prime } ( x _ { 0 } )$ is invertible, there exists $\varepsilon > 0$ such that

$$
\deg \left(f, B _ {\varepsilon} \left(x _ {0}\right), y _ {0}\right) = \deg \left(f ^ {\prime} \left(x _ {0}\right), B _ {\varepsilon} (\theta), \theta\right)
$$

from the homotopy invariance and translation invariance. The conclusion then follows from (6). □

(10) If $f \in C ( { \overline { { X } } } , Y )$ and $y _ { 0 } \not \in f ( \partial X )$ with $f ^ { - 1 } ( y _ { 0 } ) = \{ x _ { 1 } , . . . , x _ { p } \}$ , then

$$
\deg \left(f, X, y _ {0}\right) = \sum_ {i = 1} ^ {p} i \left(f, x _ {j}, y _ {0}\right).
$$

Proof. It follows from the excision property plus the additivity property. □

At the end of this section we establish the connection between the definition of the Brouwer degree given above with that in algebraic topology. Following Corollary 3.2.2, for $f \in C ^ { 2 } ( X , Y )$ , we have

$$
\deg \left(f, X, y _ {0}\right) = \frac {\int_ {X} \mu \circ f}{\int_ {Y} \mu}.
$$

Let $f ^ { * } : \Omega ^ { n } ( Y ) \to \Omega ^ { n } ( X ) , \mu \mapsto \mu \circ f$ be the pullback of $f$ , where $\Omega ^ { n } ( X )$ is the space of $n -$ forms over $X$ . This means that $\deg \left( f , X , y _ { 0 } \right)$ is the ratio of the integrations of $f ^ { * } \mu$ and $\mu$ , $\forall \mu \in \Omega ^ { n } ( Y )$ .

In algebraic topology, the Brouwer degree for maps $f$ from the sphere into itself is defined to be the multiplier of the homomorphism $f ^ { * } : H ^ { n } ( S ^ { n } ) \to$ $H ^ { n } ( S ^ { n } )$ , in which $H ^ { n } ( S ^ { n } )$ stands for the cohomology group of the $n -$ sphere, and the map $f$ maps the generator $[ \omega ]$ into $\lambda [ \omega ]$ for some integer $\lambda$ .

According to de Rham theory, for a compact, oriented, connected $n$ manifold without boundary $X$ , the cohomology group $H ^ { k } ( X )$ is defined to be

$$
\ker d ^ {k} / \operatorname {I m} d ^ {k - 1},
$$

where $d ^ { k } : \Omega ^ { k } ( X ) \to \Omega ^ { k + 1 } ( X )$ is the exterior differentiation, $k = 0 , 1 , \ldots , n$ .

We shall prove that $\Omega ^ { n } ( S ^ { n } )$ and then $H ^ { n } ( S ^ { n } )$ is one dimensional. Namely:

Lemma 3.2.7 For a compact, oriented, connected $n$ -manifold $X$ without boundary, $\dim H ^ { n } ( X ) = 1$ .

Proof. We want to show that $H ^ { n } ( X )$ is generated by one generator. Let us choose an atlas $\{ ( U _ { i } , \psi _ { i } ) | \ i = 1 , \dots , p \}$ such that

$$
U _ {i} \cap U _ {j} \neq \emptyset \Rightarrow U _ {i} \cup U _ {j} \text {i s c o n t a i n e d i n a g o o d c h a r t}.
$$

It is sufficient to prove that for $\forall \mu , \mu _ { 0 } \ \in \ \Omega ^ { n } ( X )$ with $\operatorname { s u p p } \mu _ { 0 } \subset U _ { 1 }$ , and $\int _ { X } \mu _ { 0 } = 1$ , there exist $\lambda \in \mathbb { R } ^ { 1 }$ and $\omega \in \Omega ^ { n - 1 } ( X )$ such that

$$
\mu - \lambda \mu_ {0} = d \omega .
$$

Since $X$ is connected, for $\forall i , \beth$ a curve $C$ starting from $U _ { 1 }$ ending at $U _ { i }$ . Let $\{ U _ { i _ { 0 } } , U _ { i _ { 1 } } , \ldots , U _ { i _ { l } } \}$ be a chain of neighborhoods in $\{ U _ { i } \} _ { 1 } ^ { p }$ , such that

$$
U _ {i _ {0}} = U _ {1}, U _ {i _ {l}} = U _ {i} \text {a n d} U _ {i _ {k}} \cap U _ {i _ {k - 1}} \neq \emptyset k = 1, 2, \dots , l.
$$

We choose $\alpha _ { k } \in \Omega ^ { n } ( X )$ satisfying

$$
\operatorname {s u p p} \alpha_ {k} \subset U _ {i _ {k}}, \int_ {X} \alpha_ {k} = 1, k = 1, 2, \ldots , l,
$$

and $\alpha _ { 0 } = \mu _ { 0 }$ . Since

$$
\operatorname {s u p p} \left(\alpha_ {k} - \alpha_ {k - 1}\right) \subset U _ {i _ {k}} \cup U _ {i _ {k - 1}}
$$

is contained in a good neighborhood, and

$$
\int_ {X} (\alpha_ {k} - \alpha_ {k - 1}) = 0,
$$

we have $\omega _ { k } ^ { i } \in \Omega ^ { n - 1 } ( X )$ satisfying

$$
\alpha_ {k} - \alpha_ {k - 1} = d \omega_ {k} ^ {i}, k = 1, 2, \dots , l,
$$

provided by Lemma 3.1.2. Hence

$$
\alpha_ {l} = \mu_ {0} + d \sum_ {k = 1} ^ {l} \omega_ {k} ^ {i}. \tag {3.15}
$$

Since $U _ { i _ { l } } = U _ { i } , \ \alpha _ { l }$ depends on the index $_ i$ , we write it as $\beta _ { i } , i = 1 , \ldots , p$

Now, let $\{ \chi _ { i } \} _ { 1 } ^ { p }$ be a partition of unity with respect to $\{ U _ { i } \} _ { 1 } ^ { p }$ , i.e.,

$$
\operatorname {s u p p} \chi_ {i} \subset U _ {i} \chi_ {i} \geqslant 0, \sum_ {i = 1} ^ {p} \chi_ {i} \equiv 1.
$$

Set

$$
\mu_ {i} = \chi_ {i} \mu ,
$$

then

$$
\operatorname {s u p p} \mu_ {i} \subset U _ {i}.
$$

Denote $c _ { i } = \int _ { X } \mu _ { i }$ . There exists ${ \widetilde { \omega } } _ { i } \in \Omega ^ { n - 1 } ( X )$ such that

$$
\mu_ {i} - c _ {i} \beta_ {i} = d \widetilde {\omega} _ {i}, \tag {3.16}
$$

from Lemma 3.1.2, $i = 1 , 2 , \ldots , p$ .

Combining (3.15) and (3.16), we have

$$
\begin{array}{l} \mu = \sum_ {i = 1} ^ {p} \chi_ {i} \mu = \sum_ {i = 1} ^ {p} \mu_ {i} \\ = \sum_ {i = 1} ^ {p} c _ {i} \beta_ {i} + d \sum_ {i = 1} ^ {p} \widetilde {\omega} _ {i} \\ \end{array}
$$

$$
\begin{array}{l} = \sum_ {i = 1} ^ {p} c _ {i} \mu_ {0} + d \sum_ {i = 1} ^ {p} \left(\widetilde {\omega} _ {i} + c _ {i} \sum_ {k = 1} ^ {l} \omega_ {k} ^ {i}\right) \\ = \lambda \mu_ {0} + d \omega , \\ \end{array}
$$

where $\begin{array} { r } { \ \lambda ~ = ~ \sum _ { i = 1 } ^ { p } c _ { i } \ = \ \int _ { X } \mu _ { i } } \end{array}$ , and $\begin{array} { r c l } { \omega } & { = } & { \sum _ { i = 1 } ^ { p } ( \widetilde { \omega } _ { i } + c _ { i } \sum _ { k = 1 } ^ { l } \omega _ { k } ^ { i } ) } \end{array}$ . Thus we have proved that all elements in $H ^ { n } ( X )$ !  are multipliers of $[ \mu _ { 0 } ]$ , i.e., $\dim H ^ { n } ( X ) = 1$ . □

Combining Corollary 3.2.2 and Lemma 3.2.7, we obtain

Theorem 3.2.8 Let $X$ and $Y$ be compact, oriented, connected $n$ -manifolds without boundaries. Then for any $f \in C ^ { 2 } ( X , Y )$ , the following diagram commutes:

$$
\begin{array}{ccc}H^{n}(Y) & \xrightarrow{f^{*}} & H^{n}(X)\\ \int_{Y}\Big{\downarrow} & & \int_{X}\Big{\downarrow}\\ \mathbb{R}^{1} & \xrightarrow{} & \mathbb{R}^{1} \end{array}
$$

Finally, we turn to studying the relationship between degrees on balls and those on spheres. Let $f : B ^ { n } \to \mathbb { R } ^ { n }$ be a continuous map satisfying $\theta \not \in f ( \partial B ^ { n } )$ , so $\deg { ( f , B ^ { n } , \theta ) }$ is well defined. Let us define

$$
\Phi (x) = \frac {f (x)}{\| f (x) \|} \quad \forall x \in \partial B ^ {n}.
$$

Then $\Phi : S ^ { n - 1 } \to S ^ { n - 1 }$ defines a Brouwer degree: $\deg { \left( \Phi , S ^ { n - 1 } \right) }$

What is the relationship between $\deg ( f , B ^ { n } , \theta )$ and $\deg ( \Phi , S ^ { n - 1 } )$ ?

Define a homotopy

$$
\phi (x, t) = \left\{ \begin{array}{l l} \theta & \text {i f} x = \theta \\ \frac {\| x \| ^ {2}}{\| f (\frac {x}{\| x \|}) \| ^ {t}} f (\frac {x}{\| x \|}) & \text {i f} x \neq \theta  . \end{array} \right.
$$

Since $f | _ { \partial B ^ { n } } = \phi ( \cdot , 0 ) | _ { \partial B ^ { n } }$ , from Corollary 3.2.1 and the homotopy invariance,

$$
\begin{array}{l} \deg (f, B ^ {n}, \theta) = \deg (\phi (\cdot , 0), B ^ {n}, \theta) \\ = \deg (\phi (\cdot , 1), B ^ {n}, \theta). \\ \end{array}
$$

In the case where $f \in C ^ { 1 }$ , and $y _ { 0 } \in S ^ { n - 1 }$ is a regular value of $\Phi$ , for $\forall \varepsilon > 0$ small, $\varepsilon ^ { 2 } y _ { 0 }$ is a regular value of $\phi ( \cdot , 1 )$ , and $\Phi ^ { - 1 } ( y _ { 0 } ) = \{ x _ { 1 } , \dots , x _ { k } \}$ if and only if $\phi ( \cdot , 1 ) ^ { - 1 } ( \varepsilon ^ { 2 } y _ { 0 } ) = \{ \varepsilon x _ { 1 } , \ldots , \varepsilon x _ { k } \}$ .

Since

$$
\operatorname {s g n} J _ {\Phi} \left(x _ {j}\right) = \operatorname {s g n} J _ {\phi (\cdot , 1)} \left(\varepsilon x _ {j}\right), j = 1, 2, \dots , k,
$$

we obtain $\deg ( \phi ( \cdot , 1 ) , B ^ { n } , \theta ) = \deg ( \Phi , S ^ { n - 1 } )$ . The assumptions on $f$ and on $y _ { 0 }$ can easily be dropped. Namely, we have proved:

Corollary 3.2.9 $\deg ( f , B ^ { n } , \theta ) = \deg ( \Phi , S ^ { n - 1 } ) = \deg \left( \Phi \right)$

Thus the notation $\deg { \left( \Phi , S ^ { n - 1 } \right) }$ is in coincidence with the notation $\deg { ( \hat { f } , \partial B ^ { n } , \theta ) }$ introduced after Corollary 3.2.1.

# 3.3 Applications of Brouwer Degree

# 3.3.1 Brouwer Fixed-Point Theorem

The Brouwer degree is a fundamental tool in algebraic topology. It is widely used in topological arguments. We are satisfied having a glimpse of these applications.

Theorem 3.3.1 (Brouwer fixed-point theorem) A continuous map $f$ from ${ \overline { { B } } } ^ { \prime \iota }$ into itself has a fixed point $\boldsymbol { x } _ { 0 } \in \overline { { B } } ^ { \prime \iota }$ , i.e., $f ( x _ { 0 } ) = x _ { 0 }$ .

Proof. Let $g = \mathrm { i d } - f$ . With no loss of generality, we may assume $\theta \not \in g ( \partial B ^ { n } )$ . Since $\forall x \in \partial B ^ { n }$ , λ - 1, f (x)  λx, we have $\theta \not \in \phi ( \partial B ^ { n } ) \times [ 0 , 1 ]$ , where

$$
\phi (x, t) = (1 - t) g (x) + t x.
$$

Thus by the homotopy invariance and the normality,

$$
\deg \left(g, \overset {\circ} {B} ^ {n}, \theta\right) = \deg \left(\mathrm {i d}, \overset {\circ} {B} ^ {n}, \theta\right) = 1.
$$

Our conclusion follows from the Kronecker existence theorem.

![](images/376375f06a6ca71553180bb9f7d4d5beea4a48eb92e1995fc5233f7e24597952.jpg)

The following topological fact follows from a simple degree argument:

Theorem 3.3.2 There is no continuous map $f ~ : ~ \overline { { { B } } } ^ { n } ~  ~ \partial B ^ { n }$ such that $f | _ { \partial B ^ { n } } = i d | _ { \partial B ^ { n } }$ , i.e., $\partial B ^ { n }$ is not a retraction of ${ \overline { { B } } } ^ { \prime \iota }$ .

Proof. If not, there is a continuous map $f : \overline { { { B } } } ^ { n } \to \partial B ^ { n }$ . $f | _ { \partial B ^ { n } } = \operatorname { i d } | _ { \partial B ^ { n } }$ . On one hand, $\deg \left( f , B ^ { n } , \theta \right) = 0$ due to the Kronecker existence theorem. On the other hand, by Corollary 3.2.1 and the normality

$$
\deg \left(f, B ^ {n}, \theta\right) = \deg (\operatorname {i d}, B ^ {n}, \theta) = 1.
$$

This is a contradiction.

![](images/ada77cbc9b6faee0012aff28eae9535767b7b0cef86d8b1ba8b9aa3892b23f41.jpg)

# 3.3.2 The Borsuk-Ulam Theorem and Its Consequences

Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open set, which is symmetric with respect to the origin $\theta$ , i.e., $- \Omega = \Omega$ . A map $f : \Omega \to \mathbb { R } ^ { n }$ is called odd, if

$$
f (- x) = - f (x) \quad \forall x \in \Omega .
$$

We are going to study the Brouwer degree of odd mappings.

Theorem 3.3.3 (Borsuk) Suppose that $\Omega \subset \mathbb { R } ^ { n }$ is a bounded open set containing θ and is symmetric with respect to $\theta$ . If $f : \Omega \to { \mathbb { R } } ^ { n }$ is an odd continuous map, then

$$
\deg (f, \Omega , \theta) = o d d,
$$

whenever $\theta \not \in f ( \partial \Omega )$ .

Proof. One chooses $\varepsilon > 0$ such that $B _ { \varepsilon } = B _ { \varepsilon } ( \theta ) \subset \Omega$ . According to Tietze’s theorem, there exists a continuous map $f _ { \varepsilon } : \overline { { \Omega } }  \mathbb { R } ^ { n }$ satisfying

$$
f _ {\varepsilon} (x) = \left\{ \begin{array}{l l} x & x \in \overline {{B}} _ {\varepsilon} \\ f (x) & x \in \partial \Omega  . \end{array} \right.
$$

If we use $\begin{array} { r } { \frac { 1 } { 2 } ( f _ { \varepsilon } ( x ) - f _ { \varepsilon } ( - x ) ) } \end{array}$ to replace $f _ { \varepsilon } ( x )$ , then one may assume that $f _ { \varepsilon }$ is odd. Since $f _ { \varepsilon } | _ { \partial \Omega } = f | _ { \partial \Omega }$ , according to Corollary 3.2.1, the additivity and the normality,

$$
\begin{array}{l} \deg (f, \Omega , \theta) = \deg \left(f _ {\varepsilon}, \Omega , \theta\right) \\ = \deg \left(f _ {\varepsilon}, B _ {\varepsilon}, \theta\right) + \deg \left(f _ {\varepsilon}, \Omega \backslash \bar {B} _ {\varepsilon}, \theta\right) \\ = 1 + \deg \left(f _ {\varepsilon}, \Omega \backslash \bar {B} _ {\varepsilon}, \theta\right). \\ \end{array}
$$

In the following, we want to prove that $\deg \left( f _ { \varepsilon } , \Omega \backslash \overline { { B } } _ { \varepsilon } , \theta \right)$ is even. Noticing that for a $C ^ { 1 }$ odd map $g$ with regular value $\theta$ , elements in $g ^ { - 1 } ( \theta ) \cap ( \Omega \backslash \overline { { B } } _ { \varepsilon } )$ occur in pairs. Our strategy is to approximate $f _ { \varepsilon }$ by a $C ^ { 1 }$ odd map $g$ with regular value $\theta$ . Once it is constructed, we have

$$
\deg \left(f _ {\epsilon}, \Omega \backslash \bar {B} _ {\epsilon}, \theta\right) = \deg \left(g, \Omega \backslash \bar {B} _ {\epsilon}, \theta\right) \equiv 0 (\mathrm {m o d} 2).
$$

By the Weierstrass approximation theorem, we may assume that $f _ { \varepsilon } \in C ^ { \infty }$ . It remains to approximate it such that $\theta$ is a regular value. To this end we consider the map $F : ( \Omega \backslash B _ { \epsilon } ( \theta ) ) \times \mathbf { M } ^ { n \times n } \longrightarrow \mathrm { R } ^ { n }$ defined by $F ( x , A ) = f _ { \epsilon } ( x ) +$ $A x$ .

If we can show that $F \pitchfork \{ \theta \}$ , then according to the transversality theorem (Theorem 1.3.14), $g _ { A } \pitchfork \left\{ \theta \right\}$ , for almost every $A \in \mathrm { M } ^ { n \times n }$ , where $g _ { A } ( x ) =$ $F ( x , A )$ . In fact,

$$
F ^ {\prime} (x, A) (y, B) = f _ {\epsilon} ^ {\prime} (x) y + A y + B x, \forall (y, B) \in \mathrm {R} ^ {n} \times \mathrm {M} ^ {n \times n}.
$$

Since $x ~ \neq ~ \theta , \forall z ~ \in ~ \mathrm { R } ^ { \pi }$ , we take $\begin{array} { r } { B \ = \ \frac { < \cdot , x > } { \Vert x \Vert ^ { 2 } } z } \end{array}$ <·,x>x 2 z, and y = θ, we have <,x> $y ~ = ~ \theta$ $f ^ { \prime } ( x , A ) ( y , B ) = z$ , i.e., $F ^ { \prime } ( x , A )$ is surjective, or $F \pitchfork \{ \theta \}$ . Since $\| A \|$ can be chosen arbitrarily small, the proof is complete. □

Theorem 3.3.4 (Borsuk–Ulam) Suppose that $\Omega ~ \subset ~ \mathbb { R } ^ { n }$ is a symmetric bounded open set including $\theta$ , and that $g : \partial \Omega  \mathbb { R } ^ { m }$ , $m < n$ , is odd and continuous. Then there exists $x _ { 0 } \in \partial \Omega$ such that $g ( x _ { 0 } ) = 0$ .

Proof. We prove by contradiction. Suppose $\theta \not \in g ( \partial \Omega )$ . We define a continuous extension $\widetilde { g } : \Omega \to \mathbb { R } ^ { m }$ by the Tietze theorem. With no loss of generality, one may assume $\widetilde g$ is odd.

Now we apply Theorem 3.3.3, $\mathrm { d e g } \left( \widetilde { g } , \Omega , \theta \right) \neq 0$ . Choosing $y _ { 0 } \in \mathbb { R } ^ { n } \backslash \mathbb { R } ^ { m }$ , with small norm, it follows that

$$
\deg \left(\widetilde {g}, \Omega , y _ {0}\right) \neq 0
$$

Thus, $\widetilde g ^ { - 1 } ( y _ { 0 } ) \neq \emptyset$ , but this is impossible.

□

Corollary 3.3.5 Suppose that $\Omega \subset \mathbb { R } ^ { n }$ is a symmetric bounded open set including $\theta$ , and that $g : \partial \Omega  \mathbb { R } ^ { m }$ , with $m < n$ , is continuous. Then there exists $x _ { 0 } \in \partial \Omega$ such that $g ( x _ { 0 } ) = g ( - x _ { 0 } )$

Proof. Set $g _ { 1 } ( x ) = g ( x ) - g ( - x )$ , then apply Theorem 3.3.4 directly.

Remark 3.3.6 We consider a continuous vector field on $S ^ { n }$ , i.e., a continuous map $g : S ^ { n } \to \mathbb { R } ^ { n }$ . Corollary 3.3.5 means that there exists a pair of antipodal points $\pm x _ { 0 }$ , which have the same vector $g ( x _ { 0 } ) = g ( - x _ { 0 } )$ .

Theorem 3.3.7 (Ljusternik–Schnirelmann–Borsuk) Suppose that $\Omega \subset$ $\mathbb { R } ^ { n }$ is a symmetric bounded open set including $\theta$ , and that $\{ A _ { 1 } , \dotsc , A _ { p } \}$ is a closed covering of $\partial \Omega$ , satisfying $A _ { i } \cap ( - A _ { i } ) = \emptyset \ \forall i$ . Then $p \geqslant n + 1$ .

Proof. We may assume that $\cap _ { i = 1 } ^ { p } A _ { i } = \emptyset$ . For otherwise, if $\exists x _ { 0 } \in \cap _ { i = 1 } ^ { p } A _ { i }$ , then by covering $\partial \Omega = \cup _ { i = 1 } ^ { p } ( - A _ { i } )$ , there must be a $j$ such that $x _ { 0 } \in A _ { j } \cap ( - A _ { j } )$ . But this is impossible. Now we set

$$
d _ {i} (x) = \operatorname {d i s t} \left(x, A _ {i}\right), i = 1, 2, \dots , p,
$$

and $f : \partial \Omega \to \mathbb { R } ^ { p - 1 } \subset \mathbb { R } ^ { n - 1 }$ as follows:

$$
f (x) = \left(d _ {1} (x), d _ {2} (x), \dots , d _ {p - 1} (x)\right).
$$

Supposing $p \ \leqslant n$ , we apply Corollary 3.3.5, $\exists x _ { 0 } ~ \in ~ \partial \Omega$ satisfying $f ( x _ { 0 } ) =$ $f ( - x _ { 0 } )$ . Since $\{ A _ { 1 } , A _ { 2 } , \ldots , A _ { p } \}$ is a covering, there exists $j ~ \leqslant p$ such that $x _ { 0 } \in A _ { j }$ . There are two possibilities:

1. $j ~ \leqslant ~ p - 1$ . It follows that $d _ { j } ( - x _ { 0 } ) = d _ { j } ( x _ { 0 } ) = 0$ , thus $- x _ { 0 } \in { \cal { A } } _ { j }$ or $x _ { 0 } \in A _ { j } \cap ( - A _ { j } )$ ; this is a contradiction.   
2. $x _ { 0 } \notin \cup _ { j = 1 } ^ { p - 1 } A _ { j }$ , thus $j ~ = ~ p$ . In this case, $d _ { j } ( - x _ { 0 } ) = d _ { j } ( x _ { 0 } ) > 0$ , $\forall j =$ $1 , 2 , \ldots , p - 1$ , so $x _ { 0 } \notin \bigcup _ { j = 1 } ^ { p - 1 } ( - A _ { j } )$ . Again, by covering, $x _ { 0 } \in A _ { p } \cap ( - A _ { p } )$ , a contradiction.

The following theorem is an extension of the open mapping theorem to continuous mappings in finite-dimensional spaces.

Theorem 3.3.8 (Invariance of domains) If $\Omega \subset \mathbb { R } ^ { n }$ is a nonempty open set, and if $f : \Omega \to \mathbb { R } ^ { n }$ is continuous and locally injective, then $f$ is an open map.

Proof. We want to show that $\forall x _ { 0 } \in \{ l , \forall \varepsilon > 0$ with $B _ { \varepsilon } ( x _ { 0 } ) \subset \Omega , \exists \delta > 0$ such that $f ( B _ { \varepsilon } ( x _ { 0 } ) ) \supset B _ { \delta } ( f ( x _ { 0 } ) )$ .

One may assume $x _ { 0 } = \theta = f ( \theta )$ , and that $f : \overline { { B } } _ { \epsilon } ( \theta ) \mapsto f ( \overline { { B } } _ { \epsilon } ( \theta ) )$ is 1–1.

Since $\theta \notin \int ( \partial \overline { { B } } _ { \epsilon } ( \theta ) )$ , $\exists \delta \ > \ 0$ such that $B _ { \delta } ( \theta ) \subset \mathbb { R } ^ { n } \backslash f ( \partial B _ { \epsilon } ( \theta ) )$ . Our problem is reduced to proving that $\forall y _ { 0 } \in B _ { \delta } ( \theta )$ , $\exists x \in B _ { \epsilon } ( \theta )$ such that $f ( x ) =$ $y _ { 0 }$ . Due to the Kronecker existence theorem, it is sufficient to prove that

$$
\deg \left(f, B _ {\epsilon} (\theta), y _ {0}\right) \neq 0.
$$

Since $\delta > 0$ can be chosen arbitrarily small, according to property (2) of the Brouwer degree,

$$
\deg \left(f, B _ {\epsilon} (\theta), y _ {0}\right) = \deg \left(f, B _ {\epsilon} (\theta), \theta\right).
$$

Let

$$
H (x, t) = f \left(\frac {x}{1 + t}\right) - f \left(\frac {- t x}{1 + t}\right) \quad \forall (x, t) \in \overline {{B}} _ {\epsilon} (\theta) \times [ 0, 1 ],.
$$

It is continuous on $\overline { { B } } _ { \epsilon } ( \theta ) \times [ 0 , 1 ]$ , and satisfies $H ( x , 0 ) = f ( x )$ , $H ( x , 1 ) =$ $f ( { \frac { 1 } { 2 } } x ) - f ( - { \frac { 1 } { 2 } } x )$ . We claim that $\theta \not \in H ( \partial B _ { \varepsilon } \times [ 0 , 1 ] )$ . For otherwise, there are $( x _ { 0 } , t ) \in \partial B _ { \epsilon } ( \theta ) \times [ 0 , 1 ]$ satisfying

$$
f \left(\frac {x _ {0}}{1 + t}\right) = f \left(\frac {- t x _ {0}}{1 + t}\right).
$$

Since $f$ is $1 - 1$ on $B _ { \epsilon } ( \theta )$ , we would have

$$
\frac {x _ {0}}{1 + t} = \frac {- t x _ {0}}{1 + t},
$$

i.e., $x _ { 0 } = \theta$ . But, this is impossible.

Therefore by the homotopy invariance and Borsuk–Ulam theorem

$$
\deg \left(f, B _ {\varepsilon} (\theta), \theta\right) = \deg \left(H (\cdot , 1), B _ {\varepsilon}, \theta\right) \neq 0.
$$

and then $\deg \left( f , B _ { \varepsilon } ( \theta ) , y _ { 0 } \right) \neq 0$ . This proves our conclusion.

Comparing with the global implicit function theorem, the following theorem is an extension to continuous mappings in finite-dimensional spaces:

Corollary 3.3.9 If $f : \mathbb { R } ^ { n }  \mathbb { R } ^ { n }$ is continuous and locally injective, and if $\parallel f ( x ) \parallel  + \infty$ as $\parallel x \parallel \to + \infty$ , then $f$ is surjective.

Proof. From Theorem 3.3.8, $f ( \mathbb { R } ^ { n } )$ is open. We shall prove that $f ( \mathbb { R } ^ { n } )$ is also closed. Suppose $y _ { n } \in f ( \mathbb { R } ^ { n } )$ with $y _ { n } \to y _ { 0 }$ . We shall prove that $\exists x _ { 0 } \in \mathbb { R } ^ { n }$ satisfying $f ( x _ { 0 } ) = y _ { 0 }$ . Indeed we have $x _ { n } \in \mathbb { R } ^ { n }$ satisfying $f ( x _ { n } ) = y _ { n } \ \forall n$ . According to the assumption, $\{ x _ { n } \}$ is bounded, and then there exists a convergent subsequence $x _ { n } ^ { \prime }  x _ { 0 }$ . Thus $y _ { 0 } = f ( x _ { 0 } )$ .

$f ( \mathbb { R } ^ { n } )$ is both open and closed. Therefore $f ( \mathbb { R } ^ { n } ) = \mathbb { R } ^ { n }$ .

# 3.3.3 Degrees for $S ^ { 1 }$ Equivariant Mappings

We now turn to a computation of the degree of an $S ^ { 1 }$ group action equivariant map. Let us consider an $S ^ { 1 }$ representation on $C ^ { n }$ : $T _ { \phi } = e ^ { i \mathrm { d i a g } \{ \lambda _ { 1 } \phi , \ldots , \lambda _ { n } \phi \} }$ , where $\phi \in [ 0 , 2 \pi ]$ , and $\lambda _ { 1 } , \ldots , \lambda _ { n } \in \mathbf { Z }$ .

Theorem 3.3.10 Suppose that $\Omega \subset C ^ { n }$ is a $T ( S ^ { 1 } )$ invariant open domain containing $\theta$ , and that $f \in C ( \partial \Omega , C ^ { n } \backslash \{ \theta \} )$ satisfies

$$
f (T _ {\phi} z) = e ^ {i k \phi} f (z), \forall z \in \partial \Omega , \forall \phi \in [ 0, 2 \pi ].
$$

Then

$$
\deg \left(f, \Omega , \theta\right) = \frac {k ^ {n}}{\lambda_ {1} \dots \lambda_ {n}}.
$$

Proof. 1. We may assume that $\lambda _ { 1 } = \cdot \cdot \cdot = \lambda _ { n } = 1$ . In fact, if it is proved for the special case, then for general $\lambda _ { 1 } , . . . , \lambda _ { n } \in \mathbf { Z } _ { + }$ , we define the transform $g ( z ) = ( z _ { 1 } ^ { \lambda _ { 1 } } , \ldots , z _ { n } ^ { \lambda _ { n } } )$ , let $\tilde { \Omega } = g ^ { - 1 } ( \Omega )$ ∈, then the map $f \circ g : \tilde { \Omega }  C ^ { n }$ satisfies $T _ { \phi } \circ g ( z ) = g \circ T _ { \phi } z$ , where $\displaystyle \ddot { T } _ { \phi } = e ^ { i \phi } I _ { n }$ . Therefore

$$
f \circ g (\tilde {T} _ {\phi} z) = f (T _ {\phi} \circ g (z)) = e ^ {i k \phi} f \circ g (z).
$$

Provided by the product formula, we have

$$
\deg \left(f \circ g, \tilde {\Omega}, \theta\right) = \deg \left(f, \Omega , \theta\right) \cdot \deg \left(g, \tilde {\Omega}, \theta\right).
$$

We assume the conclusion holds for $\lambda _ { 1 } = \cdot \cdot \cdot = \lambda _ { n } = 1$ , therefore

$$
\deg \left(f \circ g, \tilde {\Omega}, \theta\right) = k ^ {n}.
$$

But,

$$
\deg \left(g, \tilde {\Omega}, \theta\right) = \lambda_ {1} \dots \lambda_ {n},
$$

we obtain our conclusion.

If $\lambda _ { 1 } , \ldots , \lambda _ { n }$ are not all positive, then we introduce a transform $\textit { g } :$ $( z _ { 1 } , \ldots , z _ { j } , \ldots , z _ { n } ) \to ( z _ { 1 } , \ldots , { \bar { z } } _ { j } , \ldots , z _ { n } ) $ for all negative $\lambda _ { j }$ s. Again by the product formula, we have

$$
\deg \left(f \circ g, g ^ {- 1} (\Omega), \theta\right) = (- 1) ^ {\operatorname {s g n} \lambda_ {1} \dots \lambda_ {n}} \deg \left(f, \Omega , \theta\right).
$$

Thus it is sufficient to prove the theorem for $\lambda _ { 1 } = \cdot \cdot \cdot = \lambda _ { n } = 1$ .

2. We may assume that $f$ is smooth. This can be done by a mollifier; the argument is standard, so we omit it. The only thing that has to be verified is the equivariance of the mollified map.

3. Taking a smooth function on $R _ { + } ^ { 1 } : \eta ( t ) = 1$ , as $t \leq \epsilon$ , and 0, as $t > 2 \epsilon$ , where $\begin{array} { r } { \epsilon < \frac { 1 } { 2 } \mathrm { d i s t } \left( \partial \Omega , \theta \right) } \end{array}$ , and define

$$
\hat {f} = \eta (| z |) (z _ {1} ^ {k}, \dots , z _ {n} ^ {k}) + (1 - \eta (| z |)) f (z).
$$

Then $\hat { f }$ has the same degree as $f$ on $\Omega$

In summary, we may assume that $f$ is smooth and $S ^ { 1 }$ -equivariant, with $\lambda _ { 1 } = . . . = \lambda _ { n } = 1$ , and has the form $( z _ { 1 } ^ { k } , \ldots , z _ { n } ^ { k } )$ in $B _ { \epsilon } ( \theta )$ .

4. Now, we consider the degree of $f$ on $\Omega \backslash B _ { \varepsilon } ( \theta )$ . Define a map $F$ : $( \Omega \backslash B _ { \varepsilon } ( \theta ) ) \times \mathrm { M } ^ { n \times n }  C ^ { n }$ by

$$
F (z, A) = f (z) + A z ^ {k},
$$

where $z ^ { k } = ( z _ { 1 } ^ { k } , \dots , z _ { n } ^ { k } )$ . We show that $F \pitchfork \{ \theta \}$ . In fact,

$$
F ^ {\prime} (z, A) (w, \bar {w}, B) = F _ {z} \cdot w + F _ {\bar {z}} \cdot \bar {w} + \left(\sum_ {1} ^ {n} z _ {i} ^ {k} B _ {i j}\right) _ {1} ^ {n}.
$$

Since $z \not \in B _ { \epsilon } ( \theta ) , \forall \xi \in C ^ { n }$ , the system

$$
\sum_ {1} ^ {n} B _ {i j} z _ {j} ^ {k} = \xi_ {i}
$$

has a solution $B _ { i j }$ . By taking $w = \bar { w } = 0$ the map $F ^ { \prime } ( z , A )$ is surjective.

Applying the transversality theorem, $f _ { A } : = F ( \cdot , A ) \pitchfork \{ \theta \}$ for almost all $A \in \mathrm { M } ^ { n \times n }$ .

Since $\| A \|$ can be chosen small, the critical set $S ( f _ { A } , \Omega \backslash B _ { \epsilon } ( \theta ) )$ of $f _ { A }$ on $\Omega \backslash B _ { \epsilon } ( \theta )$ consists of isolated points, but $\forall z ^ { * } \in S ( f _ { A } , \Omega \backslash B _ { \epsilon } ( \theta ) ) , T _ { \phi } z ^ { * }$ $T _ { \phi } z ^ { * } \in$ $S ( f _ { A } , \Omega \backslash B _ { \epsilon } ( \theta ) )$ . This is impossible if it is nonempty. This proves that $\mathrm { d e g } ( f _ { A } , \Omega \backslash \bar { B } _ { \varepsilon } ( \theta ) , \theta ) = 0$ . Therefore

$$
\deg \left(f _ {A}, \Omega , \theta\right) = \deg \left(f _ {A}, B _ {\epsilon} (\theta), \theta\right).
$$

But on $B _ { \varepsilon } ( \theta ) , f _ { A } ( z ) = z ^ { k } + A z ^ { k }$ , for sufficiently small $\| A \|$ , we obtain:

$$
\deg \left(f _ {A}, B _ {\epsilon} (\theta), \theta\right) = \deg \left(f, B _ {\epsilon} (\theta), \theta\right) = k ^ {n}.
$$

Combining all together, we have proved the conclusion.

□

# 3.3.4 Intersection

The Brouwer degree is useful in the intersection theory. We briefly introduce it here. Let $\Omega _ { 1 } , \Omega _ { 2 } \subset \mathbb { R } ^ { n }$ be two compact manifolds with $\partial \Omega _ { 1 } \neq \emptyset$ and $\partial \Omega _ { 2 } = \emptyset$ , where $\dim \Omega _ { 1 } = k + 1 , \dim \Omega _ { 2 } = n - k - 1$ . Let $\phi _ { 1 } : \partial \Omega _ { 1 } \to \mathbb { R } ^ { n } , \phi _ { 2 } : \Omega _ { 2 } \to \mathbb { R } ^ { n }$ be two continuous mappings with $\phi _ { 1 } ( \partial \Omega _ { 1 } ) \cap \phi _ { 2 } ( \Omega _ { 2 } ) = \emptyset$ . We say $\phi _ { 1 }$ and $\phi _ { 2 }$ link, if for any continuous extension of $\phi _ { 1 } , { \tilde { \phi } } _ { 1 } : \Omega _ { 1 } \to R ^ { n }$ , we have

$$
\widetilde {\phi} _ {1} (\Omega_ {1}) \cap \phi_ {2} (\Omega_ {2}) \neq \emptyset .
$$

In other words,

$$
\exists (x, y) \in \Omega_ {1} \times \Omega_ {2}, \text {s u c h} \widetilde {\phi_ {1}} (x) = \phi_ {2} (y).
$$

Define a mapping $F : \Omega _ { 1 } \times \Omega _ { 2 } \to \mathbb { R } ^ { n }$ by $F ( x , y ) = \widetilde { \phi } _ { 1 } ( x ) - \phi _ { 2 } ( y )$ ; it is equivalent to saying that $F$ has a zero in $\Omega _ { 1 } \times \Omega _ { 2 }$ . Since $\theta \notin F ( \partial \Omega _ { 1 } \times \Omega _ { 2 } ) =$ $F ( \partial ( \Omega _ { 1 } \times \Omega _ { 2 } ) ) , \mathrm { d e g } \left( F , \Omega _ { 1 } \times \Omega _ { 2 } , \theta \right)$ is well defined and depends on $( \phi _ { 1 } , \phi _ { 2 } )$ only,

on account of Corollary 3.2.1. It is sufficient to verify $\deg \left( F , \Omega _ { 1 } \times \Omega _ { 2 } , \theta \right) \neq 0$ . Thus the Brouwer degree is a tool in the study of linking.

Example 1. Let $r _ { 1 } > 1 , r _ { 2 } > 0$ . Let $\Omega _ { 1 } , \Omega _ { 2 } \subset \mathbb { R } ^ { n }$ be defined as follows:

$$
\begin{array}{l} \Omega_ {1} = B _ {r _ {2}} ^ {k} (\theta) \times [ 0, r _ {1} ] \\ := \left\{\left(x _ {1}, x _ {2}, \dots , x _ {k + 1}, 0, \dots , 0\right) \mid x _ {1} ^ {2} + \dots x _ {k} ^ {2} = r _ {2} ^ {2}, 0 \leq x _ {k + 1} \leq r _ {1} \right\}, \\ \end{array}
$$

$$
\Omega_ {2} = S ^ {n - k - 1} := \left\{\left(0, 0, \dots , x _ {k + 1}, \dots , x _ {n}\right) \mid x _ {k + 1} ^ {2} + \dots + x _ {n} ^ {2} = 1 \right\},
$$

and let $\phi _ { 1 } = \mathrm { i d } | _ { \partial \Omega _ { 1 } } , \phi _ { 2 } = \mathrm { i d } _ { \Omega _ { 2 } }$ . Then $\phi _ { 1 }$ and $\phi _ { 2 }$ link.

In fact, let $\begin{array} { r } { \overleftrightarrow { \boldsymbol { \varsigma } } ( x _ { 1 } , x _ { 2 } , . . . , x _ { n } ) = ( x _ { 1 } , . . . , x _ { k } , x _ { k + 1 } - \sqrt { ( 1 - x _ { k + 2 } ^ { 2 } - . . . - x _ { n } ^ { 2 } ) } , } \end{array}$ $x _ { k + 2 } , \ldots , x _ { n } )$ , it has a unique zero in $\{ l _ { 1 } \times \{ l _ { 2 } : x _ { 1 } = \cdot \cdot \cdot = x _ { k } = x _ { k + 2 } = \cdot \cdot \cdot =$ $x _ { n } = 0 , x _ { k + 1 } = 1$ . Therefore,

$$
\deg (F, \Omega_ {1} \times \Omega_ {2}, \theta) = 1.
$$

![](images/b10cbf68dcc0ff5e2bb7aa4c24e03fcc563aa6d0d19581087d576b4e1b731043.jpg)  
Fig. 3.1.

The Borsuk–Ulam theorem is applied to the study of the intersection of symmetric sets. Assume that $X$ is a Banach space. To a closed symmetric set $A \subset X \backslash \{ \theta \}$ , we define the genus $\gamma ( A )$ of $A$ to be $\operatorname* { i n f } \{ k \in \mathbb { N } | \exists$ an odd $\phi \in { }$ $C ( A , \mathbb { R } ^ { k } \backslash \{ \theta \} ) \}$ .

Example 2. Assume that $\Omega \subset \mathbb { R } ^ { n }$ is a symmetric bounded open set containing $\theta$ , and that there is an odd homeomorphism $h : A  \partial \Omega$ . Then $\gamma ( A ) = n$ .

In fact, by definition, $\gamma ( A ) ~ \leq ~ n$ . Suppose $\gamma ( A ) ~ < ~ n$ , then there exist $k < n$ and an odd $\phi$ such that $\phi \in C ( A , \mathbb { R } ^ { k } \backslash \{ \theta \} )$ . Let $g = \phi \circ h ^ { - 1 }$ , then $g : \partial \Omega \to \mathbb { R } ^ { k } \backslash \{ \theta \}$ is continuous and odd. According to the Borsuk–Ulam theorem, there exists $x _ { 0 } \in A$ such that $g ( x _ { 0 } ) = \theta$ . This is a contradiction.

# 3.4 Leray–Schauder Degrees

In analysis we study continuous mappings in infinite-dimensional spaces, so we should be concerned with extending the Brouwer degree from finitedimensional spaces to infinite-dimensional Banach spaces. However, the generalization cannot suit arbitrary continuous mappings. This can be seen from the fact that if this sort of degree theory, which possesses the fundamental properties homotopy invariance, additivity and normality established in Sect. 3.2, had been built up, then the Brouwer fixed-point theorem would be extended directly to Banach spaces as follows.

Let $B$ be the unit open ball at the origin in an infinite-dimensional real Banach space $\mathbb { X }$ , $\phi \in C ( \overline { { B } } , \overline { { B } } )$ . Let $f _ { t } = \mathrm { i d } - t \phi$ , $t \in [ 0 , 1 ]$ and $\theta \notin f _ { t } ( \partial B )$ . If $\deg \left( f _ { t } , B , \theta \right)$ had been extended such that all the above properties of the Brouwer degree hold, then there would be a zero of $f _ { 1 } = \mathrm { i d } - \phi$ , i.e., a fixed point $x \in B$ of $\phi$ :

$$
\phi (x) = x.
$$

The “proof” is the same as in the previous section; we repeat it as follows: From the hypothesis $\theta \not \in f _ { t } ( \partial B )$ , according to the homotopy invariance and the normality,

$$
\deg \left(f _ {1}, B, \theta\right) = \deg (\operatorname {i d}, B, \theta) = 1.
$$

Hence by the Kronecker existence, $f _ { 1 } ^ { - 1 } ( \theta ) \cap B \neq \emptyset$ , i.e., $\exists x \in B$ such that $\phi ( x ) = x$ .

However, for an infinite-dimensional Banach space $\mathbb { X }$ , the above conclusion cannot be true for any continuous mapping $\phi$ .

Example. Let

$$
\mathbb {X} = l ^ {2} = \left\{x = (x _ {1}, x _ {2}, \dots , x _ {n}, \dots) | \| x \| ^ {2} = \sum_ {n = 1} ^ {\infty} x _ {n} ^ {2} <   \infty \right\}.
$$

and let $\phi$ be the mapping

$$
x \mapsto \left(\sqrt {1 - \parallel x \parallel^ {2}}, x _ {1}, x _ {2}, \ldots\right).
$$

Then $\phi : \overline { { B } } \ :  \ : \overline { { B } }$ is continuous. But $\phi$ has no fixed point in $\overline { B }$ . In fact, $\phi ( { \overline { { B } } } ) \subset \partial B$ , if $\phi ( { \overline { { x } } } ) = { \overline { { x } } }$ for some ${ \overline { { x } } } \in { \overline { { B } } }$ , then $\parallel \overline { { x } } \parallel = 1$ , but it is easily seen that ${ \overline { { x _ { 1 } } } } = 0 , { \overline { { x _ { 2 } } } } = { \overline { { x _ { 1 } } } } = 0$ , . . ., therefore ${ \overline { { x } } } = \theta$ , a contradiction.

We are forced to restrict ourselves to a subfamily of continuous mappings. Let us recall:

Definition 3.4.1 Let $\Omega \subset \mathbb { X }$ be a subset of a real Banach space. A continuous mapping $K : \Omega \to \mathbb { X }$ is said to be compact if it maps a bounded closed set into a compact set.

As before, in the following we suppose that $\Omega$ is a bounded open set in the real Banach space $\mathbb { X }$ .

Theorem 3.4.2 If $K : \Omega \to \mathbb { X }$ is a compact map, and $\Omega \subset X$ is a bounded open set, then $\forall \varepsilon > 0$ , there is a continuous operator $K _ { \varepsilon }$ taking values in a finite-dimensional linear subspace $E _ { m _ { \varepsilon } }$ such that

$$
\| K (x) - K _ {\varepsilon} (x) \| \leqslant \varepsilon \quad \forall x \in \overline {{\Omega}}.
$$

Proof. Let $B _ { \varepsilon } ( y _ { j } )$ $j = 1 , \dots , m _ { \varepsilon }$ , be finitely many $\varepsilon -$ balls covering $K ( { \overline { { \Omega } } } )$ . Let

$$
\psi_ {j} (x) = (\varepsilon - \| x - y _ {j} \|) _ {+}
$$

where

$$
\lambda_ {+} = \left\{ \begin{array}{l l} \lambda , & \lambda \geqslant 0  , \\ 0, & \lambda <   0  . \end{array} \right.
$$

Let

$$
\varphi_ {i} (x) = \frac {\psi_ {i} (x)}{\sum_ {j = 1} ^ {m _ {\varepsilon}} \psi_ {j} (x)}, i = 1, 2, \ldots , m _ {\varepsilon},
$$

and

$$
K _ {\varepsilon} (x) = \sum_ {i = 1} ^ {m _ {\varepsilon}} \varphi_ {i} (K (x)) y _ {i},
$$

then Kε(x) ∈ span $\{ y _ { 1 } , \ldots , y _ { m _ { \varepsilon } } \}$ and

$$
\begin{array}{l} \| K (x) - K _ {\varepsilon} (x) \| \leqslant \sum_ {i = 1} ^ {m _ {\epsilon}} \varphi_ {i} (K (x)) \| K (x) - y _ {i} \| \\ \leqslant \varepsilon \sum_ {i = 1} ^ {m _ {\varepsilon}} \varphi_ {i} (K (x)) = \varepsilon . \\ \end{array}
$$

The idea in extending the degree to mapping $f = \operatorname { i d } - K$ on bounded open set $\Omega \subset X$ , where $K$ is a compact map, is by approximation: $\forall \varepsilon > 0$ arbitrarily small, we already have the Brouwer degree for $f _ { \varepsilon } = \mathrm { i d } - K _ { \varepsilon }$ , then we want to define the degree for $f$ by Brouwer degrees for these $f _ { \varepsilon }$ . In doing so, we should verify:

(1) If $y _ { 0 } \in \mathbb { X }$ satisfies $y _ { 0 } \notin f ( \partial \Omega )$ , then for small $\varepsilon > 0 , \ y _ { 0 } \notin f _ { \varepsilon } ( \partial \Omega )$   
(2) For any two such $f _ { \varepsilon _ { 1 } }$ and $f _ { \varepsilon _ { 2 } }$ , the Brouwer degrees for $f _ { \varepsilon _ { 1 } } , f _ { \varepsilon _ { 2 } }$ are equal.

In proving (1), we need:

Lemma 3.4.3 Let $K : \overline { { \Omega } } \to \mathbb { X }$ be compact, $f = i d - K$ and $S \subset { \overline { { \Omega } } }$ be closed. Then $f ( S )$ is closed.

Proof. Let $\{ x _ { n } \} \subset S$ with $f ( x _ { n } ) \to z ^ { * }$ . We want to show that $z ^ { * } \in f ( S )$ . Indeed, there are a subsequence $\{ n _ { i } \}$ and $y ^ { \ast } \in \mathbb { X }$ such that $K x _ { n _ { i } }  y ^ { * }$ , and then $x _ { n _ { i } } \to z ^ { * } - y ^ { * }$ , which we write as $x ^ { * }$ . Then $x ^ { * } \in S$ . By the continuity of K, $K x ^ { * } = y ^ { * }$ , and hence $f ( x ^ { * } ) = z ^ { * }$ , i.e., $z ^ { * } \in f ( S )$ . □

Thus, $y _ { 0 } \notin \ f ( \partial \Omega )$ implies dist $( y _ { 0 } , f ( \partial \Omega ) ) \ > \ 0$ . Choosing $0 ~ < ~ \varepsilon ~ <$ < dist $( y _ { 0 } , f ( \partial \Omega ) )$ , we have

$$
\operatorname {d i s t} \left(y _ {0}, f _ {\varepsilon} (\partial \Omega)\right) > 0.
$$

Now we turn to (2). Consider the mapping

$$
f _ {\varepsilon}: \overline {{\Omega}} \cap \mathbb {R} ^ {m _ {\varepsilon}} \longrightarrow \mathbb {R} ^ {m _ {\varepsilon}}
$$

where $m _ { \varepsilon } = \mathrm { d i m s p a n } \{ \ K _ { \varepsilon } ( \Omega ) \}$ $K _ { \varepsilon } ( \Omega ) \}$ . The Brouwer degree $\deg \left( f _ { \epsilon } , \Omega \cap \mathbb { R } ^ { m _ { \epsilon } } , y _ { 0 } \right)$ is well defined. Since ${ m } _ { \varepsilon }$ depends on $\varepsilon$ , we should compare all the degrees for these $f _ { \varepsilon }$ .

Namely, we have:

Lemma 3.4.4 Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open set, $\mathbb { R } ^ { m } \subset \mathbb { R } ^ { n }$ , and $i : \mathbb { R } ^ { m } $ $\mathbb { R } ^ { n }$ be the canonical immersion:

$$
x = (x _ {1}, \dots , x _ {m}) \mapsto \hat {x} = (x _ {1}, \dots , x _ {m}, 0, \dots , 0).
$$

Let $K : \overline { { \Omega } }  \mathbb { R } ^ { m }$ be continuous, $f = i d - K$ and $p \in \mathbb { R } ^ { m }$ satisfying ${ \hat { p } } \not \in f ( \partial \Omega )$ . Then

$$
\deg \left(f, \Omega , \hat {p}\right) = \deg \left(f | _ {\mathbb {R} ^ {m} \cap \overline {{\Omega}}}, \mathbb {R} ^ {m} \cap \Omega , p\right).
$$

Proof. Let ε = dist $\left( \hat { p } , f ( \partial \Omega ) \right) . > 0$ . Choose $\hat { K } \in C ^ { 1 } ( \overline { { \Omega } } , \mathbb { R } ^ { m } )$ satisfying $\parallel$ $K - \hat { K } \parallel _ { C ( \overline { { \Omega } } ) } < \frac { \varepsilon } { 2 }$ . Let $g = \operatorname { i d } - { \hat { K } }$ . Then

$$
J _ {g} (y) = \det  \left( \begin{array}{c c} \mathrm {i d} _ {m} - \frac {\partial \hat {K} _ {i}}{\partial x _ {j}} & - \frac {\partial \hat {K} _ {i}}{\partial x _ {k}} \\ 0 & \mathrm {i d} _ {n - m} \end{array} \right) \begin{array}{c} i, j = 1, \ldots , m, \\ k = m + 1, \ldots , n, \forall y \in \bar {\Omega}. \end{array}
$$

By the Sard theorem the critical value set of $g \big | _ { \mathbb { R } ^ { m } \cap \overline { { \Omega } } } : \mathbb { R } ^ { m } \cap \overline { { \Omega } } \to \mathbb { R } ^ { m }$ is an $m$ ∩ -dimensional set of measure zero. It has a regular value $q \in \mathbb { R } ^ { m }$ such that $\parallel p - q \parallel < \varepsilon / 2$ , and $\hat { q } \not \in \mathfrak { g } ( \partial \Omega )$ . Thus

$$
\begin{array}{l} \deg (f, \Omega , \hat {p}) = \deg (g, \Omega , \hat {q}) (\text {h o m o t o p y i n v a r i a n c e}) \\ = \sum_ {y _ {i} \in g ^ {- 1} (\hat {q})} \operatorname {s g n} J _ {g} (y _ {i}) \\ = \sum_ {y _ {i} \in g | _ {\mathbb {R} ^ {m} \cap \overline {{\Omega}}} ^ {- 1} (\hat {q})} \operatorname {s g n} J _ {g} | _ {\mathbb {R} ^ {m} \cap \overline {{\Omega}}} (y _ {i}) \left(g ^ {- 1} (\hat {q}) \subset \mathbb {R} ^ {m} \cap \Omega\right) \\ = \deg \left(g | _ {\mathbb {R} ^ {m} \cap \overline {{\Omega}}}, \mathbb {R} ^ {m} \cap \Omega , q\right) \\ = \deg (f | _ {\mathbb {R} ^ {m} \cap \overline {{\Omega}}}, \mathbb {R} ^ {m} \cap \Omega , p) (\text {h o m o t o p y i n v a r i a n c e}) \\ \end{array}
$$

□

Now we are ready to define the topological degree for $f = \operatorname { i d } - K$ when $K$ is compact.

Definition 3.4.5 (Leray–Schauder degree) Let $\mathbb { X }$ be a real Banach space, and $\Omega \subset { \overline { { \mathbb { X } } } }$ be bounded and open. Let $K : \overline { { \Omega } } \to \mathbb { X }$ be compact, $f = i d - K$ and $p \in \mathbb { X } \backslash f ( \partial \Omega )$ . Define

$$
\deg \left(f, \Omega , p\right) = \deg \left(f _ {\varepsilon}, \Omega \cap E _ {\varepsilon}, p\right),
$$

where

$$
\varepsilon \in (0, \operatorname {d i s t} (p, f (\partial \Omega))), f _ {\varepsilon} = i d - K _ {\varepsilon},
$$

and $K _ { \varepsilon }$ is a continuous operator assuming values in a finite-dimensional space $E _ { \varepsilon }$ with $p \in E _ { \varepsilon }$ and satisfies

$$
\| K x - K _ {\varepsilon} x \| \leqslant \varepsilon \quad \forall x \in \overline {{\Omega}}.
$$

Let us explain the legitimacy of the definition.

1. By Lemma 3.4.3, $\mathrm { d i s t } ( p , f ( \partial \Omega ) ) > 0$ , according to Theorem 3.4.2, there exist such $K _ { \varepsilon }$ and $E _ { \varepsilon }$ .   
2. We verify that $\deg \left( f _ { \varepsilon } , \Omega \cap E _ { \varepsilon } , p \right)$ is well defined for all such $f _ { \varepsilon }$ and $E _ { \varepsilon }$ and takes the same value. Since

$$
\operatorname {d i s t} \left(p, f _ {\varepsilon} \left(\partial \Omega \cap E _ {\varepsilon}\right)\right) \geqslant \operatorname {d i s t} (p, f (\partial \Omega)) - \varepsilon > 0,
$$

$\deg \left( f _ { \varepsilon } , \Omega \cap E _ { \varepsilon } , p \right)$ is well defined. Let $( f _ { \varepsilon _ { 0 } } , E _ { \varepsilon _ { 0 } } )$ and $( f _ { \varepsilon _ { 1 } } , E _ { \varepsilon _ { 1 } } )$ be two arbitrary pairs of mappings and finite-dimensional linear subspaces satisfying the hypotheses of the definition, and let

$$
\hat {E} = \operatorname {s p a n} \left\{E _ {\varepsilon_ {0}}, E _ {\varepsilon_ {1}} \right\}, \hat {f} _ {\varepsilon_ {i}} = \mathrm {i d} - \hat {K} _ {\varepsilon_ {i}},
$$

where $\hat { K } _ { \varepsilon _ { i } } ( x ) = ( K _ { \varepsilon _ { i } } ( x ) , 0 )$ and 0 is the zero element in $\hat { E } \ominus E _ { \varepsilon _ { i } } , \ i = 0 , 1$ . By Lemma 3.4.4, we have

$$
\deg \left(f _ {\varepsilon_ {i}}, E _ {\varepsilon_ {i}} \cap \Omega , p\right) = \deg \left(\hat {f} _ {\varepsilon_ {i}}, \hat {E} \cap \Omega , p\right), i = 0, 1.
$$

Let us define $\boldsymbol \phi : ( \Omega \cap \hat { E } ) \times [ 0 , 1 ]  \hat { E }$ by

$$
\phi (x, t) = t \hat {f} _ {\varepsilon_ {0}} (x) + (1 - t) \hat {f} _ {\varepsilon_ {1}} (x), \quad \forall t \in [ 0, 1 ].
$$

Applying the homotopy invariance of the Brouwer degree we obtain

$$
\deg \left(\hat {f} _ {\varepsilon_ {0}}, \hat {E} \cap \Omega , p\right) = \deg \left(\hat {f} _ {\varepsilon_ {1}}, \hat {E} \cap \Omega , p\right).
$$

This proves

$$
\deg \left(f _ {\varepsilon_ {0}}, E _ {\varepsilon_ {0}} \cap \Omega , p\right) = \deg \left(f _ {\varepsilon_ {1}}, E _ {\varepsilon_ {1}} \cap \Omega , p\right).
$$

Remark 3.4.6 A set $\Omega \subset \mathbb { X }$ is called finitely bounded, if for all linear finitedimensional subspaces $E \subset \mathbb { X } , E \cap \Omega$ is bounded. In Definition 3.4.5, one may assume that $\Omega$ is finitely bounded.

More generally, for a given $\{ e _ { 1 } , \dotsc , e _ { k } \} \subset E$ , if for all linear finitedimensional subspaces $E \subset \mathbb { X }$ with $\{ e _ { 1 } , \ldots , e _ { k } \} \subset E , E \cap \Omega$ is bounded, then $\Omega$ is called $\{ e _ { 1 } , \ldots , e _ { k } \}$ - finitely bounded. Again, the Leray–Schauder degree is well defined on open sets, which are $\{ e _ { 1 } , \ldots , e _ { k } \}$ -finitely bounded.

Similarly to the Brouwer degree, the Leray–Schauder degree enjoys the following fundamental properties:

(1) (Homotopy invariance) Let $K : \overline { { \Omega } } \times [ 0 , 1 ] \to \mathbb { X }$ be compact and p ∈ $( \mathrm { i d } - K ) ( \partial \Omega \times [ 0 , 1 ] )$ , then

$$
\deg \left(\operatorname {i d} - K (\cdot , t), \Omega , p\right) = \text {c o n s t a n t}.
$$

(2) (Translation invariance)

$$
\deg \left(\operatorname {i d} - K, \Omega , p\right) = \deg \left(\operatorname {i d} - K - p, \Omega , \theta\right).
$$

(3) (Additivity) Let $\Omega _ { 1 } , \Omega _ { 2 } \subset \{ 2 , \Omega _ { 1 } \cap \Omega _ { 2 } = \mathcal { O }$ and $p \notin ( \mathrm { i d } - K ) ( \Omega \backslash ( \Omega _ { 1 } \cup \Omega _ { 2 } ) )$ ), then

$$
\deg \left(\operatorname {i d} - K, \Omega , p\right) = \deg \left(\operatorname {i d} - K, \Omega_ {1}, p\right) + \deg \left(\operatorname {i d} - K, \Omega_ {2}, p\right).
$$

(4) (Normality)

$$
\deg \left(\operatorname {i d}, \Omega , p\right) = \left\{ \begin{array}{l l} 1, & p \in \Omega  , \\ 0, & p \not \in \overline {{\Omega}}  . \end{array} \right.
$$

Similarly, all the other properties are consequences of these properties. Since the proofs are standard (reducing to finite-dimensional spaces by $\varepsilon$ -approximations, and applying the corresponding properties of the Brouwer degree), we omit them.

In particular, we have:

(5) (Kronecker existence) Let $\Omega \subset \mathbb { X }$ be a bounded open set and let $K : \Omega \to \mathbb { X }$ be compact. If $y _ { 0 } \notin ( \mathrm { i d } - K ) ( \partial \Omega )$ and $\mathrm { d e g } ( \mathrm { i d } - K , \Omega , y _ { 0 } ) \neq 0$ , then there exists $x _ { 0 } \in \Omega$ satisfying $x _ { 0 } = K x _ { 0 } + y _ { 0 }$ .

(6) Let $K$ be a compact linear operator, $1 \not \in \sigma ( K )$ (the spectrum of $K$ ) and $\theta \in \Omega$ , then

$$
\deg \left(\operatorname {i d} - K, \Omega , \theta\right) = (- 1) ^ {\beta},
$$

where

$$
\beta = \sum_ {\lambda_ {j} > 1, \lambda_ {j} \in \sigma (K)} \beta_ {j}, \beta_ {j} = \dim \cup_ {k = 1} ^ {\infty} \ker \left(\lambda_ {j} \mathrm {I} - K\right) ^ {k}.
$$

Proof. According to the Riesz–Schauder theory, $\sigma ( K )$ has only a point spectrum except 0. Let $E _ { 1 }$ be the finite-dimensional subspace spanned by all the generalized eigenvectors corresponding to eigenvalues $> 1$ , then we have the direct sum decomposition ${ \mathbb X } = E _ { 1 } \oplus E _ { 2 }$ . Both $E _ { 1 }$ and $E _ { 2 }$ are invariant subspaces of $K$ . Define $K _ { E _ { i } } = K _ { i }$ , then, for $\forall t \in [ 0 , 1 ]$ , $\mathrm { f o r } \forall x \in$ $E _ { 2 } \backslash \{ \theta \}$ , $t K _ { 2 } x \neq x$ . The homotopy invariance and the excision ensure that $\exists \varepsilon > 0$ such that

$$
\begin{array}{l} \deg \left(\operatorname {i d} - K, \Omega , \theta\right) = \deg \left(\operatorname {i d} - K, B _ {\varepsilon}, \theta\right) \\ = \deg (\mathrm {i d} - (K _ {1} \oplus t K _ {2}), B _ {\varepsilon}, \theta) \\ = \deg \left(\operatorname {i d} - K _ {1}, B _ {\varepsilon}, \theta\right) = (- 1) ^ {\beta}. \\ \end{array}
$$

□

Now, let us return to the discussion in the beginning of this section. We have the special form of the Schauder fixed-point theorem: if $K : { \overline { { B } } }  { \overline { { B } } }$ is compact, then $K$ has a fixed point in $\overline { B }$ .

However, the unit ball $B$ may be replaced by any bounded closed convex set. Because it is known from Dugundji’s theorem (see Sect. 3.6) that for any closed convex subset $C$ of $\mathbb { X }$ , there is a retract $r : \mathbb { X } \to C$ , i.e., $r$ is continuous, and satisfies $r \circ i = \operatorname { i d } | _ { C }$ , and $i \circ r \sim \mathrm { i d } | _ { \mathbb { X } }$ , where $_ i$ is the injection $C  \mathbb { X }$ . The following Schauder fixed-point theorem, which we have met in Sect. 2.2 is a consequence of the Kronecker existence property of the Leray–Schauder degree.

Theorem 3.4.7 (Schauder fixed point) Let $C$ be a bounded closed convex set in $\mathbb { X }$ . If $K : C  C$ is compact. Then $K$ has a fixed point.

Proof. We choose $R > 0$ large enough such that $C \subset B _ { R } ( \theta )$ . Let $r : \overline { { B } } _ { R } ( \theta ) \to$ $C$ be a retract, and $i : C \to { \overline { { B } } } _ { R } ( \theta )$ be the injection. We have a compact mapping $f = i \circ K \circ r : \overline { { B } } _ { R } ( \theta )  \overline { { B } } _ { R } ( \theta )$ :

$$
\begin{array}{ccc}\overline{B}_{R}(\theta) & & \overline{B}_{R}(\theta)\\ r\Big{\downarrow} & & i\Big{\uparrow}\\ C & \xrightarrow{K} & C \end{array}
$$

Following the steps in the proof of the Brouwer fixed-point theorem, there is a fixed point $x _ { 0 } \in \overline { { B } } _ { R }$ of $f$ , i.e., $x _ { 0 } = i \circ K \circ r ( x _ { 0 } )$ . Thus $x _ { 0 } \in C$ and then $r ( x _ { 0 } ) = x _ { 0 }$ . It follows that $x _ { 0 } = K x _ { 0 }$ . □

Another fixed-point theorem, which is also frequently used in the theory of differential equations in studying the existence of solutions by a priori estimates, reads as:

Theorem 3.4.8 (Schaefer) Suppose that $K : \mathbb { X } \times [ 0 , 1 ] \to \mathbb { X }$ is a compact mapping, satisfying $K ( x , 0 ) \ = \ \theta$ . If the set $S ~ = ~ \{ x ~ \in ~ \mathbb { X } | ~ \exists t ~ \in$ $[ 0 , 1 ]$ such that $x = K ( x , t ) \}$ is bounded. Then $K = K ( \cdot , 1 )$ has a fixed point.

Proof. Taking $r > 0$ large such that $S \subset \mathop { B _ { r } } ^ { \circ }$ , we define

$$
f (x, t) = x - K (x, t) \quad \forall (x, t) \in \overline {{B}} _ {r} \times [ 0, 1 ]
$$

then $\theta \not \in f ( \partial B _ { r } \times [ 0 , 1 ] )$ . According to the homotopy invariance

$$
\deg \left(\operatorname {i d} - K, B _ {r}, \theta\right) = \deg \left(\operatorname {i d}, B _ {r}, \theta\right) = 1.
$$

![](images/67c874396b25e3e4b8f33dcddfab92981d6322c324e9f36d43b9400a0d89848a.jpg)

Corollary 3.4.9 Suppose that $K : \mathbb { X } \to \mathbb { X }$ is compact and that the set

$$
S = \{x \in \mathbb {X} | \exists t \in [ 0, 1 ] \text {s u c h t h a t} x = t K (x) \}
$$

is bounded. Then K has a fixed point.

Proof. Set

$$
K (x, t) = t K (x)
$$

![](images/32a95a68226ad28cd480bb0259f396877a7b7f6f162b5244ff04ff486f4610b6.jpg)

As an application of Schaefer’s fixed-point theorem, we study the following semi-linear elliptic BVP, which we have met in Sect. 1.2.

Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open domain with smooth boundary.

Theorem 3.4.10 Suppose that $f \in C ^ { \gamma } ( \overline { \Omega } \times \mathbb R ^ { 1 } \times \mathbb R ^ { n } , \mathbb R ^ { 1 } )$ , for some $\gamma \in ( 0 , 1 )$ , satisfies

(1)  an increasing function $c : \mathbb { R } _ { + } ^ { 1 } \to \mathbb { R } _ { + } ^ { 1 }$ such that

$$
\left| f (x, \eta , \xi) \right| \leqslant c \left(\left| \eta \right|\right) \left(1 + \left| \xi \right| ^ {2}\right) \quad \forall (x, \eta , \xi) \in \overline {{\Omega}} \times \mathbb {R} ^ {1} \times \mathbb {R} ^ {n}.
$$

(2)  a constant $M > 0$ such that

$$
f (x, \eta , \theta) \left\{ \begin{array}{l l} <   0 & a s   \eta > M \\ > 0 & a s   \eta <   - M. \end{array} \right.
$$

Assume $\phi \in C ^ { \cdot 2 , \gamma } ( \partial \Omega )$ . Then the equation

$$
\left\{ \begin{array}{l} - \triangle u = f (x, u (x), \nabla u (x)) \\ u | _ {\partial \Omega} = \phi \end{array} \right. \tag {3.17}
$$

has a solution $u \in C ^ { 2 , \gamma } ( \overline { { \Omega } } )$ .

Proof. Fixing $\overline { \gamma } \in ( 0 , \gamma )$ , we define a map $T$ from $C ^ { 2 , \overline { { \gamma } } } ( \Omega ) = X$ into itself by $u \longrightarrow v$ , where $v$ is the solution of the following BVP:

$$
\left\{ \begin{array}{l} - \triangle v = f (x, u (x), \nabla u (x)) \\ v | _ {\partial \Omega} = \phi . \end{array} \right.
$$

By the Schauder estimate, we obtain

$$
\| v \| _ {C ^ {2, \gamma} (\overline {{\Omega}})} \leqslant C (\| u \| _ {C ^ {2, \overline {{\gamma}}} (\overline {{\Omega}})}, \| \phi \| _ {C ^ {2, \gamma} (\partial \Omega)}).
$$

Thus, $T$ is compact.

Let us introduce a parameter $t$ , and define $v = T ( u , t )$ to be the solution of the equation:

$$
\left\{ \begin{array}{l} - \triangle v = t f (x, u (x), \nabla u (x)) \\ v | _ {\partial \Omega} = \phi . \end{array} \right.
$$

We intend to apply Schaefer’s fixed-point theorem; it is sufficient to verify the boundedness of the set

$$
S = \left\{u \in X \mid \exists t \in [ 0, 1 ] \text {s u c h t h a t} u = T (u, t) \right\},
$$

i.e., $\exists$ a constant $C > 0$ such that solutions $u _ { t }$ of the equations

$$
\left\{ \begin{array}{l} - \triangle u _ {t} = t f (x, u _ {t} (x), \nabla u _ {t} (x)) \\ u _ {t} | _ {\partial \Omega} = \phi \end{array} \right. \tag {3.18}
$$

satisfy the following a priori estimate of the solution of (3.17):

$$
\| u _ {t} \| _ {C ^ {2, \gamma}} \leqslant C \tag {3.19}
$$

However, this has been done in Sect. 1.2.

Comparing the continuity method with the fixed-point method based on Schaefer’s theorem, the latter does not require the invertible of the linearized equation. Moreover, less regularity on the nonlinear term is assumed. Of course, there is only the existence of a solution but not uniqueness.

The Borsuk theorem is also extended.

Theorem 3.4.11 Let $\Omega \subset \mathbb { X }$ be a symmetric bounded open set including $\theta$ . If $K : \Omega \to \mathbb { X }$ is an odd compact map with $\theta \not \in f ( \partial \Omega )$ , where $f = i d - K$ , then $\deg \left( f , \Omega , \theta \right)$ is odd.

Proof. We approximate $f$ by finite-dimensional odd maps. Indeed, in Theorem 3.4.2, let $K$ be approximated by $K _ { \varepsilon }$ , with Im $K _ { \varepsilon } \subset \operatorname { s p a n } \{ y _ { 1 } , . . . , y _ { m _ { \varepsilon } } \}$ , and let

$$
\hat {K} _ {\varepsilon} (x) = \frac {1}{2} [ K _ {\varepsilon} (x) - K _ {\varepsilon} (- x) ].
$$

Then

$$
\| K (x) - \hat {K} _ {\varepsilon} (x) \| \leqslant \frac {1}{2} \| K (x) - K _ {\varepsilon} (x) \| + \frac {1}{2} \| K (- x) - K _ {\varepsilon} (- x) \| <   \varepsilon ,
$$

and

$$
\operatorname {I m} \hat {K} _ {\varepsilon} \subset \operatorname {s p a n} \left\{y _ {1}, \dots , y _ {m _ {\varepsilon}} \right\}.
$$

The conclusion follows from Theorem 3.3.3 directly.

□

Corollary 3.4.12 Let $\Omega \subset \mathbb { X }$ be a symmetric bounded open set including $\theta$ . If $K : \overline { { \Omega } } \to \mathbb { X }$ is compact, $f = i d - K$ , and if

$$
f (x) \neq t f (- x) \quad \forall (x, t) \in \partial \Omega \times [ 0, 1 ],
$$

then $\deg \left( f , \Omega , \theta \right)$ is odd.

Proof. Define

$$
\begin{array}{l} \phi (x, t) = \operatorname {i d} - \left[ \frac {1}{1 + t} K (x) - \frac {t}{1 + t} K (- x) \right] \\ = \frac {1}{1 + t} f (x) - \frac {t}{1 + t} f (- x). \\ \end{array}
$$

Thus $\theta \not \in ( \partial \Omega \times [ 0 , 1 ] )$ , $\phi ( \cdot , 0 ) = f$ and $\begin{array} { r } { \phi ( \cdot , 1 ) = \frac { 1 } { 2 } ( f ( x ) - f ( - x ) ) } \end{array}$ is odd. We obtain

$$
\deg (f, \Omega , \theta) = \deg (\phi (\cdot , 1), \Omega , \theta) = \text {o d d n u m b e r}.
$$

□

By the same proof of the invariance of domains theorem for the Brouwer degree, we have its infinite-dimensional version:

Corollary 3.4.13 (Invariance of domains) Assume that $\Omega \subset \mathbb { X }$ is a nonempty open set, and that $K : \overline { { \Omega } }  \mathbb { X }$ is compact. If $f = i d - K$ is locally injective, then $f$ is an open map.

The notion of the index of an isolated solution is also extended. For $f =$ $\mathrm { i d } - K$ , where $K$ is a compact map, we define the index of $f$ at an isolated fixed point $x _ { 0 }$ as follows:

$$
i (f, x _ {0}, \theta) = \deg (f, B _ {\varepsilon} (x _ {0}), \theta)
$$

for sufficiently small $\varepsilon > 0$ .

(7) In particular, if $K$ is differentiable at $x _ { 0 }$ , where $f ( x _ { 0 } ) = \theta$ , and if $f ^ { \prime } ( x _ { 0 } )$ is invertible, then we have

$$
i (f, x _ {0}, \theta) = (- 1) ^ {\beta},
$$

where $\begin{array} { r } { \beta = \sum _ { \{ \lambda _ { j } > 1 | \lambda _ { j } \in \sigma ( K ^ { \prime } ( x _ { 0 } ) ) \} } \beta _ { j } \ \mathrm { a n d } \ \beta _ { j } = \dim \cup _ { k = 1 } ^ { \infty } \ker { \big ( } \lambda _ { j } \mathrm { I } - K ^ { \prime } ( x _ { 0 } ) { \big ) } ^ { k } . } \end{array}$

Proof. By homotopy invariance, $\exists \varepsilon > 0$ such that

$$
\begin{array}{l} i (f, x _ {0}, \theta) = \deg (f, B _ {\varepsilon} (x _ {0}), \theta) \\ = \deg (\operatorname {i d} - K ^ {\prime} (x _ {0}), B _ {\varepsilon} (x _ {0}), \theta). \\ \end{array}
$$

It is sufficient to verify that $K ^ { \prime } ( x _ { 0 } )$ is a compact linear operator, because if it is so, then the conclusion follows directly from (6). The verification is as follows:

If $T = K ^ { \prime } ( x _ { 0 } )$ were not compact, then there would be $\{ x _ { j } \} \subset B _ { 1 } ( \theta )$ and $\varepsilon > 0$ such that

$$
\parallel T x _ {i} - T x _ {j} \parallel \geqslant \varepsilon \quad \text {a s} i \neq j.
$$

Choose $\delta > 0$ small such that $\forall k = 1 , 2 , \ldots$

$$
\parallel K (x _ {0} + \delta x _ {k}) - K (x _ {0}) - \delta T x _ {k} \parallel \leqslant \frac {\varepsilon \delta}{4},
$$

thus

$$
\begin{array}{l} \frac {\varepsilon \delta}{2} \geqslant \| K (x _ {0} + \delta x _ {i}) - K (x _ {0} + \delta x _ {j}) - \delta T (x _ {i} - x _ {j}) \| \\ \geqslant \delta \| T (x _ {i} - x _ {j}) \| - \| K (x _ {0} + \delta x _ {i}) - K (x _ {0} + \delta x _ {j}) \| \\ \geqslant \delta \varepsilon - \| K (x _ {0} + \delta x _ {i}) - K (x _ {0} + \delta x _ {j}) \|, \\ \end{array}
$$

that is

$$
\| K (x _ {0} + \delta x _ {i}) - K (x _ {0} + \delta x _ {j}) \| \geqslant \frac {1}{2} \varepsilon \delta , i \neq j.
$$

It contradicts with the compactness of $K$ .

The Leray-Schauder degree theory is also applied to the study of the intersections of infinite-dimensional manifolds, in which the continuous mappings should be replaced by compact vector fields.

# 3.5 The Global Bifurcation

We have studied in Sect. 1.3, the local bifurcation phenomenon, which describes a branch of solutions splitting into several branches. Having the topological tools at hand, we are able to provide more precise local information and to investigate the global behavior of bifurcating branches. Consider $F : X \times \mathbb { R } ^ { 1 } \to X$ in the following form:

$$
F (x, \lambda) = L x - \lambda x + N (x, \lambda), \tag {3.20}
$$

where $X$ is a real Banach space, $L \in \mathcal { L } ( X , X )$ , $\lambda \in \mathbb { R } ^ { 1 }$ , and $\| N ( x , \lambda ) \| =$ $o (  \boldsymbol { x }   )$ as $x \to \theta$ uniformly in any finite interval of $\lambda$ . As we know, bifurcation points $( \theta , \lambda _ { 0 } )$ only occur at $\lambda _ { 0 } \in \sigma ( L )$ , i.e., $\lambda _ { 0 }$ is a spectrum of $L$ . In particular, if $L$ is compact, and $\lambda _ { 0 } \neq 0$ , then $\lambda _ { 0 }$ is an eigenvalue of $L$ . Conversely, we have:

Theorem 3.5.1 (Knasnoselski) Suppose that $L$ is a linear compact operator on $X$ , and that $\lambda _ { 0 } \neq 0$ is an eigenvalue of $L$ with odd multiplicity, i.e., $\beta = \dim \bigcup _ { k = 1 } ^ { \infty } k e r ( L - \lambda _ { 0 } I ) ^ { k }$ is odd. $I f \forall \lambda , N ( \cdot , \lambda )$ is compact, and $N$ is continuous in $x$ and $\lambda$ , and satisfies

$$
\| N (x, \lambda) \| = o (\| x \|)
$$

uniformly in any finite interval of $\lambda$ , then $( \theta , \lambda _ { 0 } )$ is a bifurcation point of the equation $F ( x , \lambda ) = \theta$ .

Proof. Set

$$
S = \left\{\left(x, \lambda\right) \in X \times \mathbb {R} ^ {1} \mid F (x, \lambda) = \theta \right\},
$$

and

$$
S _ {+} = \overline {{S \setminus \left(\{\theta \} \times \mathbb {R} ^ {1}\right)}}.
$$

If $( \theta , \lambda _ { 0 } )$ is not a bifurcation point, then there is a closed interval $[ \lambda _ { - } , \lambda _ { + } ]$ not including 0 and satisfying:

1. $\sigma ( L ) \cap [ \lambda _ { - } , \lambda _ { + } ] = \left\{ \lambda _ { 0 } \right\}$ ,   
2. $\begin{array} { r l } & { \sigma ( L ) \cap | \lambda _ { - } , \lambda _ { + } | = \{ \lambda _ { 0 } \} , } \\ & { \exists r > 0 \qquad \mathrm { s u c h ~ t h a t } \qquad ( \overline { { B _ { r } } } ( \theta ) \times [ \lambda _ { - } , \lambda _ { + } ] ) \cap S _ { + } = \emptyset . } \end{array}$ $\exists r > 0$

In the sequel we write $B _ { r } = B _ { r } ( \theta )$ briefly. Define a deformation:

$$
\Phi (x, t) = x - \frac {1}{\lambda (t)} (L x + N (x, \lambda (t))) \qquad t \in [ 0, 1 ],
$$

where $\lambda ( t ) = t \lambda _ { - } + ( 1 - t ) \lambda _ { + }$ . Hence $\theta \not \in \Phi ( \partial B _ { r } \times [ 0 , 1 ] )$ .

It follows that

$$
\deg \left(\mathrm {i d} - \frac {1}{\lambda_ {+}} (L + N (\cdot , \lambda_ {+})), B _ {r}, \theta\right) = \deg \left(\mathrm {i d} - \frac {1}{\lambda_ {-}} (L + N (\cdot , \lambda_ {-})), B _ {r}, \theta\right).
$$

For sufficiently small $r > 0$ , again by the homotopy invariance, we obtain

$$
\deg \left(\mathrm {i d} - \frac {1}{\lambda_ {+}} L, B _ {r}, \theta\right) = \deg \left(\mathrm {i d} - \frac {1}{\lambda_ {-}} L, B _ {r}, \theta\right),
$$

i.e.,

$$
(- 1) ^ {\sum_ {j > \lambda_ {+}} \beta_ {j}} = (- 1) ^ {\sum_ {j > \lambda_ {-}} \beta_ {j}},
$$

where $\lambda _ { j } \in \sigma ( L )$ , and $\beta _ { j }$ is the multiplicity of $\lambda _ { j }$ . This is

$$
(- 1) ^ {\beta} = 1.
$$

By assumption $\beta$ is odd. This is impossible.

The oddness of the algebraic multiplicity of $\lambda _ { 0 }$ is a sufficient condition for bifurcation points, but not necessary (see Chap. 5, Theorem 5.1.37.)

We turn to studying some global results.

Lemma 3.5.2 Let $K$ be a compact metric space, $K _ { 1 }$ , $K _ { 2 }$ be disjoint closed subsets. Then the following alternatives hold: either

1. ∃ a component of $K$ intersecting $K _ { 1 }$ and $K _ { 2 }$ , or

2. compact subsets $\widehat { K } _ { 1 }$ , $\widehat { K } _ { 2 }$ such that

$$
\begin{array}{l} K _ {i} \subset \widehat {K} _ {i}, i = 1, 2, \\ K = \widehat {K} _ {1} \cup \widehat {K} _ {2}, \quad a n d \quad \widehat {K} _ {1} \cap \widehat {K} _ {2} = \varnothing . \\ \end{array}
$$

Proof. If (1) is not true, then $\exists \varepsilon _ { 0 } \ > \ 0$ such that each $\varepsilon _ { \mathrm { { 0 } } } -$ chain cannot intersect with $K _ { 1 }$ and $K _ { 2 }$ simultaneously. For otherwise, $\forall \varepsilon > 0 \exists a _ { i } ^ { \varepsilon } \in K _ { i }$ and $\varepsilon$ -chain $C _ { \varepsilon }$ connecting $a _ { i } ^ { \circ } , i = 1 , 2$ . Since $K _ { i }$ , $i = 1 , 2$ , are compact, $\{ a _ { i } ^ { \frac { 1 } { n } } \}$ has a limiting point $a _ { i } \in K _ { i } , i = 1 , 2$ .

Set

$C _ { a _ { 1 } } = \{ x \in K | \forall \varepsilon > 0$ a1 can be connected with $x$ by an $\varepsilon$ -chain} .

Since $K$ is compact, $C _ { a _ { 1 } }$ is a closed connected set. Therefore, $a _ { 2 } \in C _ { a _ { 1 } }$ . This means that (1) holds. We arrive at a contradiction.

To the $\varepsilon _ { 0 }$ , let ${ \widehat { K } } _ { 1 } = \{ y \in K | \exists x \in K _ { 1 }$ and $\exists \varepsilon _ { 0 }$ chain connecting $x$ and $y \}$ . Obviously, $\widehat { K } _ { 1 } \cap K _ { 2 } = \varnothing$ and $K _ { 1 } \subset \widehat { K } _ { 1 }$ .

We shall prove that $\widehat { K } _ { 1 }$ is both open and closed.

On the one hand, $\forall x \in \widehat { K } _ { 1 }$ , $B _ { \varepsilon _ { 0 } } ( x ) \cap \widehat { K } _ { 1 } \neq \varnothing$ , which implies $x \in { \widehat { K } } _ { 1 }$ , therefore $\widehat { K } _ { 1 }$ is closed. On the other hand, $\forall x \in \widehat { K } _ { 1 } , B _ { \varepsilon _ { 0 } } ( x ) \subset \widehat { K } _ { 1 }$ , so it is also open. We set $\widehat { K } _ { 2 } = K \backslash \widehat { K } _ { 1 }$ . Then $\widehat { K } _ { 1 }$ and $\widehat { K } _ { 2 }$ meet all conditions we need.

Theorem 3.5.3 (Leray-Schauder) Let $X$ be a real Banach space, $T : X \times$ $\mathbb { R } ^ { 1 } \to X$ be a compact map satisfying $T ( x , 0 ) = \theta$ , and $f ( x , \lambda ) = x - T ( x , \lambda )$ . Let

$$
S = \left\{\left(x, \lambda\right) \in X \times \mathbb {R} ^ {1} \mid f (x, \lambda) = \theta \right\},
$$

![](images/b41156b143f2b37da525fb866b2575a5b56ad4b8197eeeaf1b421fbc5c4421cb.jpg)  
Fig. 3.2.

and let $\zeta$ be the component of $S$ passing through $( \theta , 0 )$ . If

$$
\zeta^ {\pm} = \zeta \cap (X \times \mathbb {R} _ {\pm} ^ {1}),
$$

then both $\zeta ^ { + }$ and $\zeta ^ { - }$ are unbounded.

Proof. Since $T$ is compact, if $\zeta ^ { + }$ (or $\zeta ^ { - }$ ) is bounded, then it is compact. We consider a $\varepsilon -$ neighborhood $\zeta _ { \varepsilon }$ of $\zeta ^ { + }$ . Let $K = \overline { { \zeta _ { \varepsilon } } } \cap S$ . This is a compact metric space.

Set $K _ { 1 } = \zeta ^ { + }$ , $K _ { 2 } = \partial \zeta _ { \varepsilon } \cap S$ . According to Lemma 3.5.2, $\exists \widehat { K } _ { i } , i = 1 , 2$ , such that

$$
\begin{array}{l} K _ {i} \subset \widehat {K} _ {i} \qquad i = 1, 2, \\ K = \widehat {K} _ {1} \cup \widehat {K} _ {2}, \qquad \text {a n d} \qquad \widehat {K} _ {1} \cap \widehat {K} _ {2} = \varnothing . \\ \end{array}
$$

Choosing

$$
0 <   \delta <   \min  \left\{\operatorname {d i s t} \left(\widehat {K} _ {1}, \widehat {K} _ {2}\right), \operatorname {d i s t} \left(\widehat {K} _ {1}, \partial \zeta_ {\varepsilon}\right) \right\},
$$

and defining a $\delta / 2 -$ neighborhood $O$ of $\widehat { K } _ { 1 }$ , we have

$$
\begin{array}{l} \partial O \cap S = \varnothing , \\ \zeta^ {+} \subset O. \\ \end{array}
$$

Setting

$$
U = O \cup \left\{\left(x, \lambda\right) \in X \times \mathbb {R} _ {-} ^ {1} \mid \| x \| ^ {2} + \lambda^ {2} <   \delta^ {2} \right\},
$$

and $\lambda ^ { * }$ larger than the projection of $O$ onto $\mathbb { R } _ { + } ^ { 1 }$ , we consider the map $F$ : $( X \times \mathbb { R } ^ { 1 } ) \times [ 0 , 1 ] \to X \times \mathbb { R } ^ { 1 }$ as follows:

$$
(x, \lambda , t) \mapsto (f (x, \lambda), \lambda - t \lambda^ {*}).
$$

Then

$$
F (x, \lambda , t) = (\theta , 0) \quad \Longleftrightarrow \quad \left\{ \begin{array}{c} f (x, \lambda) = \theta \\ \lambda = t \lambda^ {*} \end{array} \right..
$$

Now,

$$
\begin{array}{l} \forall (x, \lambda) \in \partial O, \text {b y} f (x, \lambda) \neq \theta ; \\ \forall (x, 0) \in U, \text {i f} f (x, 0) = \theta , \text {t h e n} x = \theta ; \\ \forall (x, \lambda) \in X \times \mathbb {R} _ {-} ^ {1} \text {w i t h} \| x \| ^ {2} + \lambda^ {2} = \delta^ {2}, \text {w e h a v e} \lambda \neq t \lambda^ {*}. \\ \end{array}
$$

Since

$$
\partial U \subset \partial O \cup \left\{(x, 0) \mid \frac {\delta}{2} \leq \| x \| \leq \delta \right\} \cup \left\{\left(x, \lambda\right) \in X \times \mathbb {R} _ {-} ^ {1} \mid \| x \| ^ {2} + \lambda^ {2} = \delta^ {2} \right\},
$$

we have $( \theta , 0 ) \notin F ( \partial U \times [ 0 , 1 ] )$ . Thus

$$
\deg (F (\cdot , \cdot , 0), U, (\theta , 0)) = \deg (F (\cdot , \cdot , 1), U, (\theta , 0)).
$$

On the one hand, since $\lambda \neq \lambda ^ { * }$ on $U$ , from Knonecker existence theorem, we have

$$
\deg (F (\cdot , \cdot , 1), U, (\theta , 0)) = 0.
$$

On the other hand, $\forall t$ ,

$$
\left\{ \begin{array}{c} x - T (x, t \lambda) = \theta , \\ \lambda = 0, \end{array} \right. \qquad \Longleftrightarrow \qquad \left\{ \begin{array}{c} x = \theta , \\ \lambda = 0. \end{array} \right.
$$

This implies $( x - T ( x , t \lambda ) , \lambda ) \neq ( \theta , 0 )$ on $\partial U$ . Then, by the homotopy invariance and the excision property,

$$
\deg (F (\cdot , \cdot , 0), U, (\theta , 0)) = \deg (i d _ {X \times \mathbb {R} ^ {1}}, B _ {\frac {\delta}{2}} (\theta , 0), (\theta , 0)) = 1.
$$

This is the contradiction.

Although the Leray–Schauder theorem is not directly related to bifurcation problems (if $T ( \theta , \lambda ) = \theta$ , then the conclusion is trivial: $\zeta = \{ \theta \} \times \mathbb { R } ^ { 1 }$ ), but in applications, it can be used in the spirit of the following global bifurcation theorem due to P. Rabinowitz and improved by Ize [Iz].

Theorem 3.5.4 (Rabinowitz) Let $X$ be a real Banach space and $F ( x , \lambda ) =$ $x - \lambda L x - N ( x , \lambda )$ , where $L \in { \mathcal { L } } ( X , X )$ and $N : X \times \mathbb { R } ^ { 1 } \to X$ are compact. Let $S$ be the solution set of $F ( x , \lambda ) = \theta$ , $S _ { + } = \overline { { S \backslash ( \{ \theta \} \times \mathbb { R } ^ { 1 } ) } }$ , and let $\zeta$ be the component of $S _ { + }$ , containing $( \theta , \lambda _ { 1 } )$ . Assume that $N ( x , \lambda ) = o ( \left\| x \right\| )$ uniformly on any finite interval in $\lambda$ and that $\lambda _ { 1 } ^ { - 1 } \in \sigma ( L )$ is an eigenvalue of odd multiplicity. Then the following alternatives hold: Either

1. $\zeta$ is unbounded; or   
2. there are only finite number of points $\{ ( \theta , \lambda _ { i } ) | i = 1 , \ldots , l \}$ lying on $\zeta$ , where $\lambda _ { i } ^ { - 1 } \in \sigma ( L ) , i = 1 , 2 , \ldots , l$ . Furthermore, if $\beta _ { i }$ is the algebraic multiplicity of $\lambda _ { i } ^ { - 1 }$ , then $\textstyle \sum _ { i = 1 } ^ { l } \beta _ { i }$ is even.

Proof. If $\zeta$ is bounded, then $\zeta$ is compact, because both $L$ and $N$ are compact. Since $L$ is a compact linear operator, there are at most finitely many points $\{ ( \theta , \lambda _ { j } ) | \lambda _ { j } \in \sigma ( L ^ { - 1 } ) , j = 1 , \ldots , l \}$ lying on $\zeta$ . Let $0 < \varepsilon < \mathrm { d i s t } ( \zeta , \left\{ \lambda \vert \lambda ^ { - 1 } \in \right.$ $\sigma ( L ) , \lambda \neq \lambda _ { 1 } , \lambda _ { 2 } , . . . , \lambda _ { l } \} $ ), and let $\zeta _ { \varepsilon }$ be the $\varepsilon -$ neighborhood of $\zeta$ . Set $K =$ $\overline { { \zeta _ { \varepsilon } } } \cap { \cal S } _ { + }$ . Then $K$ is a compact metric space, and $\zeta \cap \partial \zeta _ { \varepsilon } = \varnothing$ . Set $K _ { 1 } = \zeta$ , $K _ { 2 } = S _ { + } \cap \partial \zeta _ { \varepsilon }$ . According to Lemma 3.5.2, $\exists$ compact sets $\widehat { K } _ { i } , i = 1 , 2$ , such that $K _ { i } \subset \widehat { K } _ { i } , i = 1 , 2$ , $K = \widehat { K } _ { 1 } \cup \widehat { K } _ { 2 }$ and $\widehat { K } _ { 1 } \cap \widehat { K } _ { 2 } = \varnothing$ . Set $0 ~ < ~ \delta ~ <$ $\operatorname* { m i n } \{ \mathrm { d i s t } ( \widehat { K } _ { 1 } , \widehat { K } _ { 2 } ) , \mathrm { d i s t } ( \widehat { K } _ { 1 } , \partial \zeta _ { \varepsilon } ) \}$ , and let $O$ be the $\textstyle { \frac { 1 } { 2 } } \delta$ neighborhood of $\widehat { K } _ { 1 }$ . Then we have

$$
\left\{ \begin{array}{l} \zeta \subset O \subset \overline {{O}} \subset \zeta_ {\varepsilon}, \\ S _ {+} \cap \partial O = \varnothing \end{array} \right..
$$

If $\lambda ^ { - 1 } \in \sigma ( L )$ , and $\lambda \ne \lambda _ { 1 } , \lambda _ { 2 } , . . . , \lambda _ { l }$ , then $( \theta , \lambda ) \not \in { \cal O }$ . We consider the following map:

![](images/c1e166d11390f2abae365b732e0dbb51c70a0b2df151b2c75095ead8d1d38ea0.jpg)  
Fig. 3.3.

$$
\Phi : O \times \mathbb {R} ^ {1} \to X \times \mathbb {R} ^ {1},
$$

$$
\Phi (x, \lambda ; t) = \left(F (x, \lambda), \| x \| ^ {2} - t ^ {2}\right).
$$

We claim that $( \theta , 0 ) \notin \Phi ( \partial O \times \{ t \} )$ as $t \neq 0$ . Indeed, if $\Phi ( x , \lambda ; t ) = ( \theta , 0 )$ , i.e.,

$$
\left\{ \begin{array}{l} F (x, \lambda) = \theta \\ \| x \| ^ {2} = t ^ {2} \neq 0 \end{array} ; \right.
$$

then $( x , \lambda ) \in S _ { + }$ , but $S _ { + } \cap \partial { \cal O } = \emptyset$ .

By the homotopy invariance, for $0 < r < R$ , we have

$$
\deg (\Phi (\cdot , R), O, (\theta , 0)) = \deg (\Phi (\cdot , r), O, (\theta , 0)).
$$

We choose $R$ large enough such that $O \subset B _ { R } ( \theta , 0 )$ , the ball in $X \times \mathbb { R } ^ { 1 }$ . From the Knonecker existence theorem,

$$
\deg (\Phi (\cdot , R), O, (\theta , 0)) = 0.
$$

Then we choose $\varepsilon > 0$ such that the $\varepsilon$ -balls $B _ { \varepsilon } ( ( \theta , \lambda _ { j } ) ) \subset O , j = 1 , 2 , \ldots , l$ . For small $r > 0$ , one wishes to prove:

$$
\deg (\Phi (\cdot , r), O, (\theta , 0)) = \sum_ {i = 1} ^ {l} \deg (\Phi (\cdot , r), B _ {\sqrt {r ^ {2} + \varepsilon^ {2}}} ((\theta , \lambda_ {j})), (\theta , 0)). \tag {3.21}
$$

Let $K > 0$ be an upper bound:

$$
\left\| (\mathrm {i d} - \lambda L) ^ {- 1} \right\| \leqslant K,
$$

for $\lambda$ in the projection of $O$ onto $\mathbb { R } ^ { 1 }$ subtracting the set $\bigcup _ { j = 1 } ^ { l } ( \lambda _ { j } - \varepsilon , \lambda _ { j } + \varepsilon ) .$ We choose $r > 0$ such that

$$
\| N (x, \lambda) \| <   \frac {1}{2 K} \| x \| \quad \forall (x, \lambda) \in O, \| x \| \leqslant r.
$$

In order to prove (3.21), according to the excision property, it is sufficient to prove:

$$
\Phi (x, \lambda , r) \neq \theta \qquad \forall (x, \lambda) \in O \backslash \bigcup_ {j = 1} ^ {l} B _ {\sqrt {r ^ {2} + \varepsilon^ {2}}} (\theta , \lambda_ {j}) .
$$

However, if $\Phi ( x , \lambda , r ) = \theta$ and $( x , \lambda ) \not \in \bigcup _ { j = 1 } ^ { l } B _ { \sqrt { r ^ { 2 } + \varepsilon ^ { 2 } } } ( \theta , \lambda _ { j } ) , \mathrm { t h e n } 0 = \| x \| ^ { 2 } -$ $r ^ { 2 } \geqslant \varepsilon ^ { 2 } - ( \lambda - \lambda _ { j } ) ^ { 2 }$ . This implies $| \lambda - \lambda _ { j } | \geqslant \varepsilon$ .

Since $( x , \lambda ) \in O$ , we have

$$
\| x \| = \| (\mathrm {i d} - \lambda L) ^ {- 1} N (x, \lambda) \| \leqslant \frac {1}{2} \| x \| ,
$$

therefore $x = \theta$ . This is impossible.

Finally, we compute $\deg ( \Phi ( \cdot , r ) , B _ { \sqrt { r ^ { 2 } + \varepsilon ^ { 2 } } } ( ( \theta , \lambda _ { j } ) ) , ( \theta , 0 ) )$ .

Let us define

$$
\Psi (x, \lambda , s) = (x - \lambda L x - s N (x, \lambda), \| x \| ^ {2} - r ^ {2}), \qquad s \in [ 0, 1 ].
$$

Provided by the homotopy invariance, we obtain

$$
\begin{array}{l} \deg (\Phi (\cdot , r), B _ {\sqrt {r ^ {2} + \varepsilon^ {2}}} ((\theta , \lambda_ {j})), (\theta , 0)) \\ = \deg ((\mathrm {i d} - \lambda L, \| \cdot \| ^ {2} - r ^ {2}), B _ {\sqrt {r ^ {2} + \varepsilon^ {2}}} (\theta , \lambda_ {j}), (\theta , 0)). \\ \end{array}
$$

Again, we define

$$
\Psi_ {1} (x, \lambda , t) = (x - \lambda L x, t (\| x \| ^ {2} - r ^ {2}) + (1 - t) (\varepsilon^ {2} - (\lambda - \lambda_ {j}) ^ {2})) \qquad \forall t \in [ 0, 1 ].
$$

Now, $\forall ( x , \lambda ) \in \partial B _ { \sqrt { r ^ { 2 } + \varepsilon ^ { 2 } } } \left( \theta , \lambda _ { j } \right)$ , if

$$
0 = t (\left\| x \right\| ^ {2} - r ^ {2}) + (1 - t) \left(\varepsilon^ {2} - (\lambda - \lambda_ {j}) ^ {2}\right) = \varepsilon^ {2} - (\lambda - \lambda_ {j}) ^ {2},
$$

then $| \lambda - \lambda _ { j } | = \varepsilon$ and $\| { \boldsymbol { x } } \| = r$ , we have $x - \lambda L x \neq \theta$ .

Therefore by the homotopy invariance,

$$
\begin{array}{l} \deg \left(\left(\operatorname {i d} - \lambda L, \| \cdot \| ^ {2} - r ^ {2}\right), B _ {\sqrt {r ^ {2} + \varepsilon^ {2}}} \left(\theta , \lambda_ {j}\right), (\theta , 0)\right) \\ = \deg \left(\left(\operatorname {i d} - (\lambda + \lambda_ {j}) L, \left(\varepsilon^ {2} - \lambda^ {2}\right)\right), B _ {\sqrt {r ^ {2} + \varepsilon^ {2}}} (\theta , 0), (\theta , 0)\right) \\ = i (\mathrm {i d} - (\lambda_ {j} - \varepsilon) L, \theta , \theta) - i (\mathrm {i d} - (\lambda_ {j} + \varepsilon) L, \theta , \theta) \\ = \left\{ \begin{array}{l l} 1 & \text {i f} \beta_ {j} \text {i s o d d}, \\ 0 & \text {i f} \beta_ {j} \text {i s e v e n}. \end{array} \right. \\ \end{array}
$$

If $\begin{array} { r } { \sum _ { j = 1 } ^ { l } \beta _ { j } = \mathrm { o d d } } \end{array}$ , then

$$
\sum_ {j = 1} ^ {l} \deg \left(\Phi (\cdot , r), B _ {\sqrt {r ^ {2} + \varepsilon^ {2}}} (\theta , \lambda_ {j}), (\theta , 0)\right) = 1 \pmod {2}.
$$

This contradicts (3.21).

In many PDE and ODE problems, which we shall see later, the second alternative is excluded. But here we shall present an example showing that the second alternative occurs. Let $X = R ^ { 2 } , L = { \binom { 1 } { 0 } }$ , and $N ( x , \lambda ) = L \left( \begin{array} { c } { { - x _ { 2 } ^ { 3 } } } \\ { { x _ { 1 } ^ { 3 } } } \end{array} \right)$ −x32 for $x = { \binom { x _ { 1 } } { x _ { 2 } } } .$  x1 . Then $\lambda _ { 1 } = 1 , \lambda _ { 2 } = 2$ . For $\lambda \in \left[ 1 , 2 \right]$ , we have a pair of solutions passing through the points $( \theta , 1 )$ and $( \theta , 2 )$ :

$$
\left(x _ {1}, x _ {2}\right) = \pm \left(\left(\lambda - 1\right) ^ {\frac {1}{8}} (2 - \lambda) ^ {\frac {3}{8}}, (2 - \lambda) ^ {\frac {1}{8}} (\lambda - 1) ^ {\frac {3}{8}}\right).
$$

The above global bifurcation theorem has a powerful application in the nonlinear Sturm–Liouville problem.

Recall the equation:

$$
\left\{ \begin{array}{l} \ddot {y} + \lambda \sin y = 0 \qquad \text {i n} (0, \pi)  , \\ y (0) = y (\pi) = 0  , \end{array} \right.
$$

which we studied in Sect. 1.3. We know that $\lambda = n ^ { 2 } , n = 1 , 2 , . . .$ are bifurcation points, i.e., in the neighborhood of $( \theta , n ^ { 2 } )$ there are nontrivial solutions. However, we did not know how is the global behavior of those components of solutions passing through these bifurcation points. Now, we shall apply the Rabinowitz bifurcation theorem to understand it better.

We start with slightly general equations. Let

$$
A u = - \frac {d}{d x} \left(p (x) \frac {d u}{d x}\right) + q (x) u \qquad x \in (0, \pi),
$$

where $p \in C ^ { 1 } ( [ 0 , \pi ] )$ $p ~ \in ~ C ^ { 1 } ( [ 0 , \pi ] ) , ~ p ( x ) ~ > ~ 0 , ~ q ~ \in ~ C ^ { 0 } ( [ 0 , \pi ] )$ , with $q ~ \geq ~ 0$ , and let $D ( A ) = H ^ { 2 } \cap H _ { 0 } ^ { 1 } ( [ 0 , \pi ] )$ . Then $A$ is a self-adjoint operator with simple

eigenvalues $0 < \lambda _ { 1 } < \lambda _ { 2 } < . . .$ , and $L = A ^ { - 1 }$ is compact from $L ^ { 2 } ( [ 0 , \pi ] )$ to $C _ { 0 } ^ { 1 } ( [ 0 , \pi ] )$ , provided by the Sobolev embedding theorem.

Suppose $h \in C \left( [ 0 , \pi ] \times \mathbb { R } ^ { 2 } \times \mathbb { R } ^ { 1 } , \mathbb { R } ^ { 1 } \right)$ . We consider the nonlinear eigenvalue problem:

$$
\left\{ \begin{array}{l} A u = \lambda u + h (x, u (x), u ^ {\prime} (x), \lambda) \quad x \in (0, \pi) \\ u \in D (A) \end{array} \right. \tag {3.22}
$$

The equation is reduced to the following:

$$
u = \lambda L u + N (u, \lambda) \quad u \in C _ {0} ^ {1} ([ 0, \pi ], \mathbb {R} ^ {1}). \tag {3.23}
$$

where $N ( u , \lambda ) = L h ( x , u ( x ) , u ^ { \prime } ( x ) , \lambda )$ is compact from $C _ { 0 } ^ { 1 } ( [ 0 , \pi ] , \mathbb { R } ^ { 1 } ) \times \mathbb { R } ^ { 1 }$ t o $C _ { 0 } ^ { 1 } ( [ 0 , \pi ] , \mathbb { R } ^ { 1 } )$ . If further, we assume:

$$
h (x, \xi , \eta ; \lambda) = o \left(| \xi | ^ {2} + | \eta | ^ {2}\right) ^ {\frac {1}{2}} \tag {3.24}
$$

as $( \xi , \eta ) \to ( 0 , 0 )$ uniformly in $x$ and $\lambda$ (in finite intervals), then in the Banach space $C _ { 0 } ^ { 1 } ( [ 0 , \pi ] , \mathbb { R } ^ { 1 } )$

$$
\| N (u, \lambda) \| = o (\| u \|).
$$

According to the global bifurcation theorem, the component $\zeta _ { j }$ of nontrivial solutions passing through $( \theta , \lambda _ { j } )$ , $\lambda _ { j } ^ { - 1 } \in \sigma ( L )$ (or equivalently $\lambda _ { j } \in \sigma ( A )$ ), $j = 1 , 2 , \dots$ has two possibilities: either $\zeta _ { j }$ is unbounded or $\zeta _ { j }$ meets some $\zeta _ { i } , i \neq j$ .

We shall prove that the latter possibility actually does not occur in this problem.

Let us recall the linear ODE (i.e., $h = 0$ )

$$
\left\{ \begin{array}{l} A u = - (p u ^ {\prime}) ^ {\prime} + q u = \lambda u \hskip 2 8. 4 5 2 7 5 6 p t \text {i n} (0, \pi)  , \\ u (0) = u (\pi) = 0  . \end{array} \right.
$$

The nontrivial solution set consists of $\bigcup _ { j = 1 } ^ { \infty } \phi _ { j } \mathbb { R } ^ { 1 }$ 8 , where $\phi _ { j }$ is the eigenfunction associate with $\lambda _ { j } , j = 1 , 2 , . . .$ according to the Sturm-Liouville Theory, $\phi _ { j }$ has exactly $j - 1$ simple roots in the open interval $( 0 , \pi )$ . We may assume $\phi _ { j } ^ { \ \prime } ( 0 ) > 0$ . Let us extend this conclusion to nonlinear problems (3.22). Set

$S _ { j } = \{ v \in C _ { 0 } ^ { 1 } ( [ 0 , \pi ] ) \mid v \mathrm { ~ h a s ~ e x a c t l y ~ } ( j - 1 ) \mathrm { s i m p l e ~ r o o t s ~ i n ~ } ( 0 , \pi ) , \mathrm { ~ a n d ~ } v ^ { \prime } ( 0 ) ,$ $\mathrm { a n d } v ^ { \prime } ( 0 )$

$$
v ^ {\prime} (\pi) \neq 0 \}
$$

$$
j = 1, 2, \dots
$$

Thus $S _ { j }$ is an open set in $C _ { 0 } ^ { 1 } ( [ 0 , \pi ] )$ , and

$$
S _ {i} \cap S _ {j} = \varnothing \qquad \text {a s} \qquad i \neq j ,
$$

$$
i, j = 1, 2, \dots
$$

We shall prove that $\zeta _ { i } \cap \zeta _ { j } = \emptyset$ as $i \neq j$ .

It is sufficient to prove that

$$
\zeta_ {j} \subset (S _ {j} \times \mathbb {R} ^ {1}) \cup \left\{(\theta , \lambda_ {j}) \right\}, \qquad j = 1, 2, \dots
$$

Lemma 3.5.5 There exists a neighborhood $O _ { j }$ of $( \theta , \lambda _ { j } )$ such that if $( u , \lambda ) \in$ $O _ { j } \backslash ( \{ \theta \} \times \mathbb { R } ^ { 1 } )$ is a solution of (3.22) then $u \in S _ { j } , \forall j$ .

Proof. We prove by contradiction. If $\exists$ solutions $( u _ { n } , \alpha _ { n } )$ of (3.23), with $u _ { n } \neq$ $\theta$ , satisfying $( u _ { n } , \alpha _ { n } )  ( \theta , \lambda _ { j } )$ , but $u _ { n } \notin S _ { j }$ . Let $\begin{array} { r } { v _ { n } = \frac { u _ { n } } { \| u _ { n } \| } } \end{array}$ un , then

$$
v _ {n} = \alpha_ {n} L v _ {n} + \| u _ {n} \| ^ {- 1} N (u _ {n}, \alpha_ {n}).
$$

Since $\| v _ { n } \| = 1 , \alpha _ { n } \to \lambda _ { j }$ , $\| u _ { n } \| ^ { - 1 } \| N ( u _ { n } , \alpha _ { n } ) \| = o ( 1 )$ , and $L$ is compact, we have $v _ { n }  v , \| v \| = 1$ , satisfying

$$
v = \lambda_ {j} L v.
$$

According to the Sturm-Liouville theorem, $v \in S _ { j }$ . This contradicts the openness of $S _ { j }$ . □

Now, we are going to prove:

Theorem 3.5.6 Suppose that h satisfies (3.24). Then the branch of nontrivial solutions $\zeta _ { j }$ passing through $( \theta , \lambda _ { j } )$ is included in $( S _ { j } \times \mathbb { R } ^ { 1 } ) \cup \{ ( \theta , \lambda _ { j } ) \}$ . In particular, $\zeta _ { j }$ is unbounded.

Proof. From Lemma 3.5.5, $\zeta _ { j } \cap O _ { j } \subset \ ( S _ { j } \times \mathbb { R } ^ { 1 } ) \cup \{ ( \theta , \lambda _ { j } ) \}$ . We shall prove that $\zeta _ { j }$ is entirely contained in the above set. If not, $\exists ( u ^ { * } , \lambda ^ { * } ) \in \zeta _ { j } \cap \partial ( S _ { j } \times$ $\mathbb { R } ^ { 1 } ) \backslash ( \theta , \lambda _ { j } )$ . Since $\partial ( S _ { j } \times \mathbb { R } ^ { 1 } ) = \partial S _ { j } \times \mathbb { R } ^ { 1 }$ , and $u ^ { \ast } \in \partial S _ { j }$ implies that $u ^ { * }$ has at least a double zero, i.e., $\exists \xi \in [ 0 , \pi ]$ such that $u ^ { * } ( \xi ) = u ^ { * \prime } ( \xi ) = 0$ , it follows from the uniqueness of ODE, $u ^ { * } = \theta$ . Thus $( u ^ { * } , \lambda ^ { * } ) = ( \theta , \lambda _ { i } ) , \lambda _ { i } ^ { - 1 } \in \sigma ( L ) , i \ne j$ , according to the necessary condition of bifurcation. Again, by Lemma 3.5.5, $i \ : = \ : j$ . This is a contradiction. Our conclusion follows from Theorem 3.5.4 directly. □

Remark 3.5.7 One may consider other boundary conditions than the Dirichlet condition.

Let us return to the example in Sect. 1.3:

$$
\left\{ \begin{array}{l} - u ^ {\prime \prime} = \lambda \sin u \quad x \in (0, \pi), \\ u (0) = u (\pi) = 0. \end{array} \right. \tag {3.25}
$$

Applying Theorem 3.5.6, we conclude that $\forall n = 1 , 2 , \ldots$ , there is an unbounded branch of solutions $\zeta _ { n }$ passing through the bifurcation point $( \theta , n ^ { 2 } )$ .

Again, one may prove that if $( u , \lambda ) \in \zeta _ { n }$ , then

$$
\lambda \geqslant n ^ {2}. \tag {3.26}
$$

Indeed, if $\lambda < n ^ { 2 }$ , then

$$
\lambda \frac {\sin u}{u} <   n ^ {2}.
$$

Comparing (3.25) with the equation

$$
\left\{ \begin{array}{l} - v ^ {\prime \prime} = n ^ {2} v \quad x \in (0, \pi) \\ v (0) = v (\pi) = 0, \end{array} \right. \tag {3.27}
$$

we would have $v \in S _ { n + 1 }$ , on account of Sturm’s theorem. But $v \in S _ { n }$ . This is a contradiction.

Moreover, $\forall ( u , \lambda ) \in \zeta _ { n }$ ,

$$
u ^ {\prime} (x) = \lambda \int_ {x} ^ {\xi} \sin u (t) d t,
$$

where $\xi \in ( 0 , \pi )$ is a root of $u ^ { \prime } ( x ) = 0$ . Thus

$$
\| u ^ {\prime} \| _ {C} \leqslant \lambda \pi ,
$$

and

$$
\| u \| _ {C ^ {1}} \leqslant \lambda \pi (1 + \pi). \tag {3.28}
$$

Combining (3.26) with (3.28), we have

$$
\zeta_ {n} \subset \left\{\left(u, \lambda\right) \in C _ {0} ^ {1} ([ 0, \pi ]) \times \mathbb {R} ^ {1} \mid \lambda \geqslant n ^ {2}, \| u \| _ {C ^ {1}} \leqslant \lambda \pi (1 + \pi) \right\}.
$$

Hence, the projection of $\zeta _ { n }$ onto $\mathbb { R } ^ { 1 }$ is the interval $( n ^ { 2 } , \infty )$ . For any $\lambda _ { 0 } \in$ $( n ^ { 2 } , \infty )$ , there exist at least there exists at least a $n$ branches air of nont $\zeta _ { 1 } , \zeta _ { 2 } , \ldots , \zeta _ { n }$ $\lambda = \lambda _ { 0 }$ $\zeta _ { j }$ $\pm u _ { \lambda _ { 0 } } ^ { ( j ) } , j = 1 , 2 , \ldots , n$

![](images/76be4728d3baf33d518886f921d88470dce3263b9cd94a0d61c399fd5aab0f90.jpg)  
Fig. 3.4.

Theorem 3.5.8 $\forall \lambda \in ( n ^ { 2 } , ( n + 1 ) ^ { 2 } ]$ equation (3.26) has at least n distinct pairs of nontrivial solutions.

Remark 3.5.9 It is natural to ask if we can extend the above method to study elliptic nonlinear eigenvalue problems as that for ODE. The main obstruction is that there is no counterpart of the Sturm–Liouville theory for PDEs. There are only a few partial results in this direction.

# Remark 3.5.10 (Bifurcation at infinity)

We study the bifurcation at infinity by transforming the problem into the bifurcation at zero. We consider the equation:

$$
x = \lambda L u + M (x, \lambda), \tag {3.29}
$$

where $L$ is a linear compact operator on $X$ , and $M : X \times R ^ { 1 } \to X$ satisfies $\| M ( x , \lambda ) \| = o ( \| x \| )$ uniformly in any finite interval as $\| x \| \to \infty$ .

$( \lambda _ { 0 } , \infty )$ is called a bifurcation point at infinity, if there exists a sequence of solutions $( \lambda _ { n } , x _ { n } )$ of (3.29), such that $\| x _ { n } \| \to \infty$ , and $\lambda _ { n }  \lambda _ { 0 }$ .

Now we define $y = \| x \| ^ { - 2 } x$ , then $\| y \| = \| x \| ^ { - 1 }$ . Equation (3.29) is transformed into:

$$
y = \lambda L y + N (y, \lambda), \tag {3.30}
$$

where $N ( y , \lambda ) = \| y \| ^ { 2 } M ( \| y \| ^ { - 2 } y , \lambda )$ . Since

$$
\left\| N (y, \lambda) \right\| = o (\left\| y \right\|) \text {a s} y \rightarrow \theta ,
$$

(3.30) is exactly the equation we studied previously.

# 3.6 Applications

# 3.6.1 Degree Theory on Closed Convex Sets

In applications of the Leray–Schauder degree theory, boundary conditions should be carefully investigated, i.e., on the boundary there is no fixed point of the compact vector field. However, for mappings with images in a closed convex set, the notion of the boundary can be reduced. Let us present an extension version of the Tietze theorem, Dugundji’s theorem, concerning the extension of continuous mappings on closed convex sets, which we have met in Sect. 3.4.

Theorem 3.6.1 (Dugundji) Suppose that A is a closed subset of a metric space $X$ , and that $C$ is a convex subset of a Banach space $Y$ . Then for any continuous map $f : A  C$ , there exists a continuous extension ${ \tilde { f } } : X \to C$ .

Proof. Since $X \setminus A$ is a metric subspace, it is paracompact. $\forall x \in X \backslash A$ , let $0 < r ( x ) < \frac { 1 } { 2 } \mathrm { d i s t } ( x , A )$ , and let $B _ { r ( x ) } ( x )$ be the ball with radius $r ( x )$ and

center $x$ . All these balls form a covering of $X \backslash A$ . The covering has a locally finite refinement $\{ U _ { \alpha } | \alpha \in \wedge \}$ . On the set $X \backslash A$ , we define

$$
\lambda_ {\alpha} (x) = \frac {\operatorname {d i s t} (x , X \backslash U _ {\alpha})}{\sum_ {(\beta \in \Lambda)} \operatorname {d i s t} (x , X \backslash U _ {\beta})} \quad \forall \alpha \in \Lambda .
$$

They satisfy:

$$
\operatorname {s u p p} \lambda_ {\alpha} \subset U _ {\alpha}, 0 \leqslant \lambda_ {\alpha} (x) \leqslant 1, \text {a n d} \sum_ {\alpha \in \Lambda} \lambda_ {\alpha} (x) = 1.
$$

$\forall \alpha \in A$ we choose $a _ { \alpha } \in A$ , such that

$$
\operatorname {d i s t} \left(a _ {\alpha}, U _ {\alpha}\right) <   2 \operatorname {d i s t} \left(A, U _ {\alpha}\right),
$$

and let

$$
\widetilde {f} (x) = \left\{ \begin{array}{l l} \sum \lambda_ {\alpha} (x) f (a _ {\alpha}) & x \in X \backslash A \\ f (x) & x \in A  . \end{array} \right.
$$

Since $\{ U _ { \alpha } | \alpha \in A \}$ is a locally finite covering, the sum is finite. We claim that $\hat { \boldsymbol f }$ is continuous. It is sufficient to prove that $\hat { \boldsymbol f }$ is continuous on the boundary $\partial A$ , i.e., for $\forall x _ { 0 } \in \partial A$ . $\forall \varepsilon > 0$ , $\exists \delta > 0$ such that

$$
\| \widetilde {f} (x) - f (x _ {0}) \| <   \varepsilon ,
$$

whenever $\mathrm { d i s t } ( x , x _ { 0 } ) < \delta$ .

In fact, $\exists \delta > 0$ , such that if $\mathrm { d i s t } ( x , x _ { 0 } ) < 6 \delta$ and $x \in A$ , then

$$
\| f (x) - f (x _ {0}) \| <   \varepsilon .
$$

Now, $\forall x \in X$ , if $\mathrm { d i s t } ( x , a _ { \alpha } ) < 3 \delta$ , from

$$
\operatorname {d i s t} \left(x _ {0}, a _ {\alpha}\right) \leq \operatorname {d i s t} \left(x _ {0}, x\right) + \operatorname {d i s t} \left(x, a _ {\alpha}\right),
$$

it follows that $\mathrm { d i s t } ( x _ { 0 } , a _ { \alpha } ) < 4 \delta$ , then

$$
\left\| f \left(a _ {\alpha}\right) - f \left(x _ {0}\right) \right\| <   \epsilon . \tag {3.31}
$$

Otherwise, $\mathrm { d i s t } ( x , a _ { \alpha } ) \geq 3 \delta$ , then

$$
\operatorname {d i s t} (x, a _ {\alpha}) \geq 3 \operatorname {d i s t} (x, x _ {0}) \geq 3 \operatorname {d i s t} (x, A).
$$

In this case we claim that $x \not \in U _ { \alpha }$ . If not, i.e., $x \in U _ { \alpha }$ , then we would have

$$
\begin{array}{l} \operatorname {d i s t} (x, a _ {\alpha}) \leqslant \operatorname {d i a m} (U _ {\alpha}) + \operatorname {d i s t} (U _ {\alpha}, a _ {\alpha}) \\ \leqslant 2 r (x) + 2 \operatorname {d i s t} (A, U _ {\alpha}) \\ \leqslant \operatorname {d i s t} (A, x) + 2 \operatorname {d i s t} (A, x) \\ = 3 \operatorname {d i s t} (A, x) \\ \end{array}
$$

This is a contradiction, therefore, $x \not \in U _ { \alpha }$ and then

$$
\lambda_ {\alpha} (x) = 0. \tag {3.32}
$$

Combining equations (3.31) and (3.32), we have

$$
\| \widetilde {f} (x) - f (x _ {0}) \| \leqslant \sum_ {\operatorname {d i s t} (x, a _ {\alpha}) <   3 \delta} \lambda_ {\alpha} (x) \| f (a _ {\alpha}) - f (x _ {0}) \| <   \varepsilon .
$$

The proof is finished.

Suppose that $C$ is a closed convex subset of a real Banach space $\mathbb { X }$ , and that $U \subset C$ is a bounded relatively open set. Assume that $K : \overline { { U } } \to C$ is compact, and that $\theta \not \in ( \mathrm { i d } - K ) ( \partial U )$ , where $\partial U$ is the boundary of $U$ relative to $C$ . Then we can define

$$
\deg_ {C} (\operatorname {i d} - K, U, \theta) = \deg (\operatorname {i d} - K \circ r, B _ {R} \cap r ^ {- 1} (U), \theta),
$$

where $r$ is the retraction of $C$ , and $R > 0$ is a large number such that $U \subset$ $B _ { R } ( \theta )$ .

Firstly, we point out that the definition makes sense, i.e, $\theta \not \in ( \mathrm { i d } - K \circ$ $r ) ( \partial ( B _ { R } ( \theta ) \cap r ^ { - 1 } ( U ) )$ ). In fact, if it is not true, then $\exists p \in \partial ( B _ { R } ( \theta ) \cap r ^ { - 1 } ( U ) )$ ) such that $p = K \circ r ( p )$ . Thus, $p \in C$ and then $p = K p$ , which implies $p \in$ $C \cap \partial ( B _ { R } ( \theta ) \cap r ^ { - 1 } ( U ) )$ , i.e., $p \in \partial U$ . This is a contradiction.

Secondly, one verifies that the degree so defined does not depend on the retraction $r$ , and the radius $R$ . In fact, let $R _ { 1 } < R _ { 2 }$ such that $U \subset \operatorname { i n t } ( B _ { R _ { 1 } } ( \theta ) )$ . All fixed points of $\mathrm { i d } - K$ in $U$ must be in $B _ { R _ { 1 } } ( \theta )$ . By excision,

$$
\deg \left(\operatorname {i d} - K \circ r, r ^ {- 1} (U) \cap B _ {R _ {2}} (\theta), \theta\right) = \deg \left(\operatorname {i d} - K \circ r, r ^ {- 1} (U) \cap B _ {R _ {1}} (\theta), \theta\right).
$$

Again let $r _ { 1 } , r _ { 2 }$ be two retractions: $\mathbb { X } \to C$ , by excision,

$$
\begin{array}{l} \deg \left(\operatorname {i d} - K \circ r _ {i}, r _ {i} ^ {- 1} (U) \cap B _ {R} (\theta), \theta\right) \\ = \deg \left(\operatorname {i d} - K \circ r _ {i}, r _ {1} ^ {- 1} (U) \cap r _ {2} ^ {- 1} (U) \cap B _ {R} (\theta), \theta\right). \\ \end{array}
$$

for $i = 1 , 2$ . We define

$$
F (x, t) = \operatorname {i d} - (t K \circ r _ {1} + (1 - t) K \circ r _ {2}) \forall t \in [ 0, 1 ].
$$

From the homotopy invariance, $\deg _ { C } \left( \operatorname { i d } - K , U , \theta \right)$ is independent of $r _ { i } , \ i = $ $1 , 2$ .

In particular, if $\mathbb { X }$ is a real OBS, i.e., $\mathbb { X }$ is a real Banach space with a closed positive cone $P$ (see Sect. 2.1), then $P$ is a closed convex subset, and then $\deg _ { P } \left( \operatorname { i d } - K , U , \theta \right)$ makes sense.

Now, we present a few degree computations for special boundary conditions in an OBS.

Lemma 3.6.2 Suppose that $( \mathbb { X } , P )$ is an OBS, and that $U \subset P$ is bounded and open. Assume that $K : { \overline { { U } } }  P$ is compact, satisfying

$$
\exists y \in P \backslash \{\theta \}, \text {s u c h t h a t} x - K x \neq t y \forall t \geq 0, \forall x \in \partial U;
$$

then

$$
\deg_ {P} (\operatorname {i d} - K, U, \theta) = 0.
$$

Proof. From the homotopy invariance,

$$
\deg \left(\operatorname {i d} - K - t y, U, \theta\right) = \text {c o n s t a n t}, \forall t \geq 0.
$$

$K$ is co then $\exists C > 0$ that . It fo $\| x - K x \| \leq C \forall x \in U$ oses. $\begin{array} { r } { t > \frac { C } { \| y \| } } \end{array}$ $x - K x \neq t y \forall x \in U$ $\deg _ { P } { ( \mathrm { i d } - K , U , \theta ) } = 0$

Theorem 3.6.3 Suppose that $( \mathbb { X } , P )$ is an OBS, $U \subset P$ is bounded open and contains $\theta$ . Assume that $\exists \rho > 0$ such that $B _ { \rho } ( \theta ) \cap P \subset U$ and that $K : { \overline { { U } } }  P$ is compact and satisfies:

(1) $\forall x \in P$ , with $\| x \| = \rho , \forall \lambda \in [ 0 , 1 ) , x \neq \lambda K ( x )$ ,   
(2) $\exists y \in P \backslash \{ \theta \}$ , such that $x - K ( x ) \neq t y , \forall x \in \partial U , \forall t \geq 0$ .

Then $K$ possesses a fixed point on $\overline { { U _ { \rho } } }$ , where $U _ { \rho } = U \backslash B _ { \rho } ( \theta )$ .

Proof. We may assume that $K$ has no fixed point on $\partial B _ { \rho } ( \theta )$ then

$$
\deg_ {P} (\mathrm {i d} - K, U, \theta) = \deg_ {P} (\mathrm {i d} - K, U _ {\rho}, \theta) + \deg_ {P} (\mathrm {i d} - K, B _ {\rho} (\theta) \cap P, \theta).
$$

According to Lemma 3.6.2 and assumption (2), LHS = 0, and from assumption (1) and the homotopy invariance, the second term on the RHS = 1. Therefore, $\mathrm { d e g } _ { P } \left( \mathrm { i d } - K , U _ { \rho } , \theta \right) = - 1$ . □

Theorem 3.6.4 Suppose that $( \mathbb { X } , P )$ is an OBS, $U \subset P$ is bounded and open, and contains $\theta$ . Assume that $\exists \rho > 0$ such that $U \subset B _ { \rho } ( \theta ) \cap P$ , and that $U _ { \rho } : = ( B _ { \rho } ( \theta ) \cap P ) \backslash U$ possesses an interior point. If $K : \overline { { B } } _ { \rho } ( \theta ) \cap P \to P$ is compact and satisfies:

(1) $\forall x \in P , \| x \| = \rho , \forall \lambda \in [ 0 , 1 ) , x \neq \lambda K ( x )$ .   
(2) $\exists y \in P \backslash \{ \theta \}$ such that $x - K ( x ) \neq t y , \forall t \geq 0 , \forall x \in \partial U .$

Then $K$ has a fixed point in $U _ { \rho }$ .

Proof. The proof is similar to the previous theorem: $\deg { ( \mathrm { i d } - K , U _ { \rho } , \theta ) } = 1$ . We omit it. □

Remark 3.6.5 Let $( X , P )$ be an OBS, and let $0 ~ < ~ \rho _ { 1 } ~ < ~ \rho _ { 2 }$ , and $P _ { \rho _ { i } } =$ $P \cap B _ { \rho _ { i } } ( \theta )$ . We write $\partial P _ { \rho _ { i } } = P \cap \partial B _ { \rho _ { i } } ( \theta )$ , $i = 1 , 2$ . A map $f : P _ { \rho _ { 2 } } \backslash P _ { \rho _ { 1 } }  P$ is ∩ i i  called a cone compression, if $x \not \leqslant f ( x )$ , $\forall x \in \partial P _ { \rho _ { 2 } }$ , and $f ( x ) \not \leqslant x$ \, $\forall x \in \partial P _ { \rho _ { 1 } }$ . It is called a cone expansion if $f ( x ) \not \leqslant x$ ∀, $\forall x \in \partial P _ { \rho _ { 2 } }$ , and $x \not \leqslant f ( x ) \forall x \in \partial P _ { \rho _ { 1 } }$ . Noticing that

$$
f (x) \not \leq x i m p l i e s \forall y \in P \backslash \{\theta \}, \forall t \geq 0, x - f (x) \neq t y,
$$

and

$$
x \not \leqslant f (x) \text {i m p l i e s} \forall \lambda \in [ 0, 1 ], x \neq \lambda f (x),
$$

Theorems 3.6.3 and 3.6.4 extend the earlier results of Krasnoselski on cone compression and cone expansion mappings.

Let $( X , P )$ be an OBS satisfying $X = P - P$ . A map $T : P  P$ is called positive. We shall study the bifurcation problem for positive mappings on OBS. Let $R _ { + } = { } | 0 , + \infty )$ and $f : P \times R _ { + } \to P$ be a continuous mapping satisfying $f ( \theta , \lambda ) = \theta ~ \forall \lambda \in \mathcal { R } _ { + }$ . We are looking for $\lambda _ { 0 } ~ \in ~ R _ { + }$ , such that  > 0, (x, λ)  (B+ θ )  (λ0 , λ0 + ) satisfying

$$
x = f (x, \lambda),
$$

where $B _ { \epsilon } ^ { + } = B _ { \epsilon } ( \theta ) \cap P$ . At this time, $( \theta , \lambda _ { 0 } )$ is a bifurcation point of positive solutions.

Lemma 3.6.6 Let $( \mathbb { X } , P )$ be defined above. Let $f : P \times R _ { + } \to P$ be of the form:

$$
f (x, \lambda) = \lambda T x + g (x, \lambda),
$$

where $T \in \mathcal { L } ( \mathbb { X } , \mathbb { X } )$ is a positive and compact operator, and $g : P \times R _ { + } \to P$ is compact, satisfying $\| g ( x , \lambda ) \| = o ( \| x \| )$ as $\| x \|  0$ and $g ( x , 0 ) = \theta$ . If $( \theta , \lambda _ { 0 } )$ is a bifurcation point with $\lambda _ { 0 } \geq 0$ , then $\lambda _ { 0 } > 0$ , and ${ \lambda } _ { 0 } ^ { - 1 }$ is an eigenvalue of $T$ with positive eigenvector.

Proof. By definition, $\exists ( x _ { n } , \lambda _ { n } ) \in ( P \backslash \{ \theta \} ) \times R _ { + }$ , such that $( x _ { n } , \lambda _ { n } ) \to ( \theta , \lambda _ { 0 } )$ , with $x _ { n } = f ( x _ { n } , \lambda _ { n } )$ . Setting $\begin{array} { r } { y _ { n } = \frac { x _ { n } } { \| x _ { n } \| } } \end{array}$ x , we have

$$
y _ {n} - \lambda_ {0} T y _ {n} = (\lambda_ {n} - \lambda_ {0}) T y _ {n} + g (x _ {n}, \lambda_ {n}) \| x _ {n} \| ^ {- 1}.
$$

Define $S _ { + } = \partial B _ { 1 } ( \theta ) \cap P$ , then $y _ { n } \in S _ { + }$ , and then, after a subsequence we have $T y _ { n } \to z _ { 0 }$ . Let $y _ { 0 } = \lambda _ { 0 } z _ { 0 }$ , then $y _ { 0 } \in S _ { + }$ . Obviously, $y _ { 0 } = \lambda _ { 0 } T y _ { 0 }$ , and $\lambda _ { 0 } > 0$ . □

One has the following variant of the global bifurcation theorem.

Theorem 3.6.7 (Dancer) Let $( X , B )$ and $f , T , g$ be as in Lemma 3.6.6. Assume that $T$ has finitely many positive eigenvalues with positive eigenvectors. Let $\Sigma = \{ ( x , \lambda ) \in P { \times } R _ { + } | x = f ( x , \lambda ) \}$ and $\Sigma ^ { + } = \overline { { \Sigma } } \cap \left( ( P \backslash \{ \theta \} ) \times ( R ^ { + } \backslash \{ 0 \} ) \right)$ . Then $\Sigma ^ { + }$ contains an unbounded connected branch emanating from the reciprocal of one of these eigenvalues.

Proof. Let $\lambda _ { 0 } ^ { - 1 }$ be the smallest positive eigenvalue of $T$ , and let $\zeta$ be the connected component of $\Sigma ^ { + } \cup ( \{ \theta \} \times [ 0 , \lambda _ { 0 } ] )$ , containing $\{ \theta \} \times [ 0 , \lambda _ { 0 } ]$ . If $\zeta$ is bounded, then $\exists \mu > \lambda _ { 0 }$ such that $\zeta \subset Q _ { \mu } : = B _ { \mu } ^ { + } \times [ 0 , \mu ]$ , and $\zeta \cap \partial Q _ { \mu } = \cup$ , where $B _ { \mu } ^ { + } = B _ { \mu } ( \theta ) \cap P$ .

Since $\mu ^ { - 1 }$ is not an eigenvalue with positive eigenvector of $T$ , $\exists \alpha > 0$ such that

$$
\left\| x - \mu T x \right\| \geq \alpha \mu \| x \|, \forall x \in P.
$$

One chooses $\rho \in ( 0 , \operatorname* { m i n } \{ \frac { \alpha \mu } { 2 } , \mu \} )$ , such that $\begin{array} { r } { \| g ( x , \mu ) \| \leq \frac { \alpha \mu } { 2 } \| x \| } \end{array}$ as $x \in B _ { \rho } ^ { + }$ . Choose $\epsilon \in ( 0 , \operatorname* { m i n } \{ \mu , \rho \} )$ . Let $\zeta _ { 1 } = \zeta \cup ( \{ \theta \} \times [ 0 , \mu ] ) , D = ( ( B _ { \mu } ^ { + } \backslash B _ { \epsilon } ^ { + } \times \{ 0 \} ) \cup$ $( ( \partial B _ { \mu } ( \theta ) \cap P ) \times [ 0 , \mu ] ) \cup ( ( B _ { \mu } ^ { + } \backslash B _ { \epsilon } ^ { + } ) \times \{ \mu \} )$ . Then $D \cap \zeta _ { 1 } = \emptyset$ . Similar to the proof in the global bifurcation theorem, $\exists U \subset P \times [ 0 , \mu ]$ open, such that $\Sigma \cap { \dot { o } } U = \emptyset , \zeta _ { 1 } \subset U$ , and $\overline { { U } } \cap D = \emptyset$ . Thus,

$$
\begin{array}{l} 1 = \deg_ {P} (\operatorname {i d}, B _ {\epsilon} ^ {+}, \theta) \\ = \deg_ {P} (\mathrm {i d} - f (\cdot , 0), U _ {0}, \theta) \\ = \deg_ {P} (\operatorname {i d} - f (\cdot , \mu), U _ {\mu}, \theta) \\ = \deg_ {P} (\mathrm {i d} - f (\cdot , \mu), B _ {\epsilon} ^ {+}, \theta), \\ \end{array}
$$

where $U _ { \lambda } : = \{ x \in P | ( x , \lambda ) \in U \}$ .

Let $\phi \in S ^ { + }$ be a positive eigenvector associated with $\lambda _ { 0 } ^ { - 1 }$ with norm 1. We claim that the equation

$$
x - \mu T x = \beta \phi \forall \beta > 0
$$

has no positive solution. For otherwise, if $\Rightarrow \beta _ { 0 } > 0 , x _ { 0 } \in P$ , satisfying

$$
x _ {0} - \mu T x _ {0} = \beta_ {0} \phi .
$$

Then

$$
x _ {0} \geq (\mu \tau_ {0} \lambda_ {0} ^ {- 1} + \beta_ {0}) \phi > (\tau_ {0} + \beta_ {0}) \phi ,
$$

where $\tau _ { 0 } = \operatorname* { s u p } \{ \tau \in R ^ { + } | x _ { 0 } \geq \tau \phi \}$ . This contradicts the definition of $\tau _ { 0 }$ . Setting $\beta \in ( 0 , \frac { \epsilon \rho } { 2 } )$ , we have

$$
\| x - \mu T x + (1 - \lambda) \beta \phi + \lambda g (x, \mu) \| \geq \alpha \mu \| x \| - \frac {\epsilon \rho}{2} - \frac {\alpha \mu}{2} \| x \| > 0, \text {a s} \| x \| = \varepsilon .
$$

Then, by the homotopy invariance,

$$
\deg_ {P} (\mathrm {i d} - f (\cdot , \mu), B _ {\epsilon} ^ {+}, \theta) = \deg_ {P} (\mathrm {i d} - \mu T - \beta \phi , B _ {\epsilon} ^ {+}, \theta) = 0.
$$

The contradiction shows that $\zeta$ is unbounded.

# 3.6.2 Positive Solutions and the Scaling Method

The above degree computation is applied to the study of the positive solution of certain superlinear elliptic equations. Let $\Omega \subset \mathbb { R } ^ { n }$ be an open bounded domain with smooth boundary. Assume that $a _ { i j } , b _ { j } \in C ( \Omega ) , i , j = 1 , \dots , n , \ \phi \in$ $C ^ { 1 } ( \partial \Omega )$ is nonnegative, and $f \in C ( \overline { { \Omega } } \times \mathbb { R } ^ { 1 } )$ . Let

$$
L u (x) = \sum_ {i, j = 1} ^ {n} \frac {\partial}{\partial x _ {i}} \left(a _ {i, j} (x) \frac {\partial u}{\partial x _ {j}}\right) + \sum_ {j = 1} ^ {n} b _ {j} (x) \frac {\partial u}{\partial x _ {j}}. \tag {3.33}
$$

We study positive solutions of the equation:

$$
\left\{ \begin{array}{c} L u + f (x, u) = 0, \text {i n} \Omega , \\ u = \phi , \quad \text {o n} \partial \Omega , \end{array} \right. \tag {3.34}
$$

in which $f$ is superlinear in $u$ . In order to apply degree theory, we should estimate a priori bounds for positive solutions $u$ . We introduce here a useful method – the scaling method – based on the following global results of Liouville-type theorems, cf. Gidas and Spruck [GS], Chen and Li [ChL 1] and Y. Li [Li 1].

Theorem. Assume $p \in ( 1 , \frac { n + 2 } { n - 2 } )$ for $n \geq 3$ , and $p > 1$ for $n = 1 , 2$ . If $u \in$ $C ^ { 2 } ( \mathbb { R } ^ { n } )$ is a nonnegative solution of the equation:

$$
\Delta u + u ^ {p} = 0 \text {i n} \mathbb {R} ^ {n}, \tag {3.35}
$$

then $u \equiv 0$ .

Proof. (We only prove the theorem under a stronger condition: $\textstyle 1 < p < { \frac { \pi } { n - 2 } }$ , for general case see the above-mentioned references.) Let $\varphi _ { 1 } > 0$ − be the first eigenfunction of the Laplacian on $B _ { 1 } ( \theta ) \subset R ^ { n }$ , i.e.,

$$
\left\{ \begin{array}{c} - \Delta \varphi_ {1} = \lambda_ {1} \varphi_ {1}, \quad \text {i n} B _ {1} (\theta), \\ \varphi_ {1} = 0 \quad \text {o n} \partial B _ {1} (\theta). \end{array} \right. \tag {3.36}
$$

For $\forall R > 0$ , let $\begin{array} { r } { \varphi _ { R } ( x ) = \varphi _ { 1 } ( \frac { x } { R } ) } \end{array}$ , we have

$$
\left\{ \begin{array}{c} - \Delta \varphi_ {R} = \frac {\lambda_ {1}}{R ^ {2}} \varphi_ {R}, \quad \text {i n} B _ {R} (\theta), \\ \varphi_ {R} = 0 \quad \text {o n} \partial B _ {R} (\theta). \end{array} \right. \tag {3.37}
$$

Integrating by parts,

$$
\begin{array}{l} \int_ {B _ {R}} u ^ {p} \varphi_ {R} ^ {p} = - \int_ {B _ {R}} \Delta u \varphi_ {R} ^ {p} (3.38) \\ = \int_ {B _ {R}} \nabla u \nabla \varphi_ {R} ^ {p} (3.39) \\ = - \int_ {B _ {R}} u \Delta \varphi_ {R} ^ {p} + p \int_ {\partial B _ {R}} u \frac {\partial \varphi_ {R}}{\partial n} \varphi_ {R} ^ {p - 1} (3.40) \\ = - \int_ {B _ {R}} u \Delta \varphi_ {R} ^ {p}. (3.41) \\ \end{array}
$$

Since

$$
\int_ {B _ {R}} u \varDelta \varphi_ {R} ^ {p} = p (p - 1) \int_ {B _ {R}} u \varphi_ {R} ^ {p - 2} | \nabla \varphi_ {R} | ^ {2} - \frac {p \lambda_ {1}}{R ^ {2}} \int_ {B _ {R}} u \varphi_ {R} ^ {p}, \qquad (3. 4 3)
$$

it follows that

$$
\begin{array}{l} \int_ {B _ {R}} (u \varphi_ {R}) ^ {p} \leq \frac {\lambda_ {1} p}{R ^ {2}} \int_ {B _ {R}} u \varphi_ {R} ^ {p} (3.44) \\ \leq \frac {\lambda_ {1} p}{R ^ {2}} \left(\int_ {B _ {R}} (u \varphi_ {R}) ^ {p}\right) ^ {\frac {1}{p}} \left(\int_ {B _ {R}} \varphi_ {R} ^ {p}\right) ^ {\frac {p - 1}{p}}. (3.45) \\ \end{array}
$$

Therefore,

$$
\int_ {B _ {R}} (u \varphi_ {R}) ^ {p} \leq \left(\frac {\lambda_ {1} p}{R ^ {2}}\right) ^ {\frac {p}{p - 1}} \int_ {B _ {R}} \varphi_ {R} ^ {p} = (\lambda_ {1} p) ^ {\frac {p}{p - 1}} R ^ {n - \frac {2 p}{p - 1}} \int_ {B _ {1}} \varphi_ {1} ^ {p}. \qquad (3. 4 6)
$$

We obtain $\begin{array} { r } { \operatorname* { l i m } _ { R  0 } \int _ { B _ { R } } ( u \varphi _ { R } ) ^ { p } = 0 } \end{array}$ . Thus $u \equiv 0$ .

![](images/93df0e9b65f8d63f47c34c5674d7c9d81fda66a5f691c2f17839b078ced35cfc.jpg)

Similarly, one has the Liouville theorem for half space:

Theorem. Assume $p \in ( 1 , \frac { n + 2 } { n - 2 } )$ for $n \geq 3$ , and $p > 1$ for $n = 1 , 2$ . If $u \in$ $C ^ { 2 } ( \mathbb { R } ^ { n } ) \cap C ( \overline { { \mathbb { R } } } _ { + } ^ { n } )$ is a nonnegative solution of the equation:

$$
\left\{ \begin{array}{c} \Delta u + u ^ {p} = 0, \quad \text {i n} \quad \overline {{\mathbb {R}}} _ {+} ^ {n}, \\ u = 0 \quad \text {o n} \quad x _ {n} = 0, \end{array} \right. \tag {3.47}
$$

then $u \equiv 0$ .

Theorem 3.6.8 Assume $p \in ( 1 , \frac { n + 2 } { n - 2 } )$ for $n \geq 3$ , and $p > 1$ for $n = 1 , 2$ . Suppose that $f \in C ( \Omega \times R ^ { 1 } )$ satisfies

$$
\lim  _ {t \rightarrow + \infty} \frac {f (x , t)}{t ^ {p}} = h (x) \text {u n i f o r m l y i n} x \in \overline {{\Omega}}, \tag {3.48}
$$

where h is a positive function. If $u \geq 0$ is a solution of equation (3.34), then there exists a constant $C$ depending on $p , \Omega , \phi$ and $f$ only such that $u \leq C$ .

Proof. One proves the theorem by contradiction. If the conclusion is not true, then there exist a sequence of solutions $u _ { k }$ of equation (3.34) and points $P _ { k } \in \Omega$ satisfying

$$
M _ {k} := \sup  _ {x \in \overline {{\Omega}}} u _ {k} (x) = u _ {k} (P _ {k}) \to + \infty \text {a s} k \to \infty .
$$

We may assume $P _ { k } \to P \in { \overline { { \Omega } } }$ as $k  \infty$ . There are two possibilities: either (1) $P \in \Omega ^ { \circ }$ , or (2) $P \in \partial \Omega$ .

(1): Let $\begin{array} { r } { d = \frac 1 2 \mathrm { d i s t } ( P , \partial \Omega ) , \lambda _ { k } ^ { \frac { 2 } { p - 1 } } M _ { k } = 1 , y = \frac { x - P _ { k } } { \lambda _ { k } } } \end{array}$ $\begin{array} { r } { d = \frac { 1 } { 2 } \mathrm { d i s t } ( P , \partial \Omega ) } \end{array}$

$$
v _ {k} (y) = \lambda_ {k} ^ {\frac {2}{p - 1}} u _ {k} (x).
$$

It is defined on B d (θ) and satisfies $B _ { \frac { d } { \lambda _ { k } } } ( \theta )$ λ k

$$
\sup_{y\in B_{\frac{d}{\lambda_{k}}}(\theta)}v_{k}(y) = v_{k}(\theta) = 1  .
$$

and

$$
\begin{array}{l} \sum_ {i, j = 1} ^ {n} \frac {\partial}{\partial y _ {j}} \left(a _ {i j} ^ {k} (y) \frac {\partial v _ {k} (y)}{\partial y _ {i}}\right) \\ + \lambda_ {k} \sum_ {j = 1} ^ {n} b _ {j} ^ {k} (y) \frac {\partial v _ {k} (y)}{\partial y _ {j}} + \lambda_ {k} ^ {\frac {2 p}{p - 1}} f \left(\lambda_ {k} y + P _ {k}, \lambda_ {k} ^ {\frac {- 2}{p - 1}} v _ {k} (y)\right) = 0, \\ \end{array}
$$

where $a _ { i j } ^ { k } ( y ) = a _ { i j } ( \lambda _ { k } y + P _ { k } ) \to a _ { i j } ( P )$ , $\lambda _ { k } b _ { j } ^ { k } ( y ) = \lambda _ { k } b _ { j } ( \lambda _ { k } y + P _ { k } ) \to 0$ , and the last term is asymptotic to $h ( \lambda _ { k } y + P _ { k } ) v _ { k } ^ { p }$ .

$\forall R > 0$ , $\exists k$ such that BR(θ)  B d (θ). Since $B _ { R } ( \theta ) \subset B _ { \frac { d } { \lambda _ { k } } } ( \theta )$ $v _ { k }$ is uniformly bounded λ on $B _ { R } ( \theta )$ , from the $L ^ { q }$ estimates, $1 < q < \infty$ , we have $\| v _ { k } \| _ { W ^ { 2 , q } ( B _ { R } ( \theta ) ) } \leq C$ , a uniform constant. According to the Sobolev embedding theorem and the diagonal principle, one has a subsequence $\boldsymbol { v } _ { k _ { j } }  \boldsymbol { v }$ in $C ^ { 1 , \beta } \cap W ^ { 2 , q } ( B _ { R } ( \theta ) )$ for some $\beta \in ( 0 , 1 ) , \forall R > 0$ , and $v ( \theta ) = 1$ . Thus as a weak solution $v$ satisfies

$$
\left\{ \begin{array}{c} \sum_ {i, j = 1} ^ {n} a _ {i j} (P) \frac {\partial^ {2} v (y)}{\partial y _ {i} \partial y _ {j}} + h (P) v ^ {p} (y) = 0, \text {i n} \mathbb {R} ^ {n}  . \\ v (\theta) = 1. \end{array} \right.
$$

After scaling and rotation, it is reduced to equation (3.35). Again by bootstrap iteration, $v \in C ^ { 2 } ( \mathbb { R } ^ { n } )$ , it follows from the Liouville-type theorem that $v \equiv 0$ . This contradicts $v ( \theta ) = 1$ .

In case (2), $P \in \partial \Omega$ . Without loss of generality, we may assume $\exists \delta > 0$ such that $B _ { \delta } ( P ) \cap \partial \Omega$ is contained in the hyperplane $x _ { n } = 0$ . One defines $v _ { k } , \lambda _ { k }$ as before. Let $d _ { k } = d ( P _ { k } , \partial \Omega )$ , then $d _ { k } = P _ { k } \cdot e _ { n }$ , where $e _ { n }$ is the unit normal vector of $\partial \Omega$ near $P$ . Therefore $v _ { k }$ is well defined in $\begin{array} { r } { B _ { \frac { \delta } { \lambda _ { k } } } ( \theta ) \cap \{ y _ { n } > - \frac { d _ { k } } { \lambda _ { k } } \} } \end{array}$ λ Again we have $\operatorname* { s u p } v _ { k } ( y ) = v _ { k } ( \theta ) = 1$ .

We claim that there is a constant $C _ { 1 } > 0$ such that $\begin{array} { r } { \frac { d _ { k } } { \lambda _ { k } } \geq C _ { 1 } } \end{array}$ 入 . In fact, according to elliptic regularity up to the boundary, $| \nabla v _ { k } |$ is uniformly bounded, so is

$$
\left| v _ {k} (\theta) - v _ {k} \left(0, \dots , 0, - \frac {d _ {k}}{\lambda_ {k}}\right) \right| \leq C _ {1} ^ {- 1} \frac {d _ {k}}{\lambda_ {k}},
$$

i.e.,

$$
1 - \lambda_ {k} ^ {\frac {2}{p - 1}} \sup  _ {x \in \partial \Omega} \phi (x) \leq C _ {1} ^ {- 1} \frac {d _ {k}}{\lambda_ {k}}.
$$

Since $\lambda _ { k } \to 0$ , our claim is proved.

If there is a subsequence $\frac { d _ { k _ { j } } } { \lambda _ { k _ { j } } }  \infty$ dk λk , then the discussion is reduced to case (1). Therefore we may assume that there is another constant $C _ { 2 }$ such that $\begin{array} { r } { C _ { 1 } \le \frac { d _ { k } } { \lambda _ { k } } \le C _ { 2 } } \end{array}$ , after a subsequence, $\begin{array} { r } { \frac { d _ { k } } { \lambda _ { k } } \longrightarrow s } \end{array}$ λ . By taking the limit, we obtain:

$$
\left\{ \begin{array}{c c} \sum_ {i, j = 1} ^ {n} a _ {i j} (P) \frac {\partial^ {2} v}{\partial y _ {i} \partial y _ {j}} + h (P) v ^ {p} (y) = 0 & \mathrm {i n} y _ {n} > - s, \\ v (y) = 0 & \mathrm {o n} y _ {n} = - s. \end{array} \right.
$$

After scaling and rotation, it is reduced to equation (3.47). By the same reasoning, we have $v \equiv 0$ , which contradicts $v ( \theta ) = 1$ .

Combining these two cases, we have proved the theorem.

Lemma 3.6.9 Under the assumptions of Theorem 3.6.8, if further $f ( x , t ) \geq$ 0, and

$$
\lim  _ {t \rightarrow + \infty} \inf  _ {t} \frac {f (x , t)}{t} > \lambda_ {1}, \text {u n i f o r m l y i n} x \in \overline {{\Omega}} ， \tag {3.49}
$$

where $\lambda _ { 1 }$ is the first eigenvalue of the linear elliptic operator $- L$ (see equation (3.33)) with eigenfunction $\varphi _ { 1 }$ . Then there exists a constant $C > 0$ such that for all solutions u of the equation:

$$
\left\{ \begin{array}{c} L u + f (x, u) + t \varphi_ {1} = 0, i n \Omega , \forall t \geq 0, \\ u = 0 \quad o n \partial \Omega , \end{array} \right. \tag {3.50}
$$

we have

$$
\int_ {\Omega} f (x, u (x)) \varphi_ {1} (x) d x \leq C,
$$

and

$$
t \leq C.
$$

Proof. We may assume $\| \varphi _ { 1 } \| _ { L ^ { 2 } } = 1$ . By the condition (3.49), there are constants $\begin{array} { r } { M > 0 , k \in ( \lambda _ { 1 } , \operatorname* { l i m } \operatorname* { i n f } _ { t  + \infty } \frac { f ( x , t ) } { t } ) } \end{array}$ f(x,t) ) such that

$$
f (x, t) \geq k t - M, \forall t > 0.
$$

For any solution $u$ of equation (3.50), by integration, we have

$$
- \int_ {\Omega} \varphi_ {1} (x) L u (x) d x = \int_ {\Omega} \varphi_ {1} (x) f (x, u (x)) d x + t ,
$$

thus

$$
\lambda_ {1} \int_ {\Omega} \varphi_ {1} (x) u (x) d x \geq k \int_ {\Omega} \varphi_ {1} (x) u (x) d x - M \int_ {\Omega} \varphi_ {1} (x) + t,
$$

i.e.,

$$
t + (k - \lambda_ {1}) \int_ {\Omega} \varphi_ {1} (x) u (x) d x \leq M \int_ {\Omega} \varphi_ {1} (x) d x.
$$

Since $f ( x , t ) ~ \geq ~ 0$ , from the maximum principle, $u ~ \geq ~ 0$ . The conclusions follow. □

Theorem 3.6.10 Suppose that $f \in C ( \overline { { \Omega } } \times \mathbb { R } ^ { 1 } )$ is positive, which satisfies the conditions (3.48) (3.49) and

$$
\lim  _ {t \rightarrow 0} \sup  _ {t <   x} \frac {f (x , t)}{t} <   \lambda_ {1} \quad u n i f o r m l y \quad i n \quad x \in \overline {{\Omega}}. \tag {3.51}
$$

Then equation (3.34) has a positive solution in which $\phi = 0$ .

Proof. Define $F : u ( x ) \mapsto f ( x , u ( x ) )$ from $C ( \Omega )$ into itself, and $K = ( - L ) ^ { - 1 }$ being the linear compact operator on $C ( \overline { { \Omega } } )$ .

Then $T = K \circ F$ is a compact mapping from $C ( \overline { { \Omega } } )$ into its positive cone. Applying Lemma 3.6.9 and Theorem 3.6.8 we take $R > 0$ large enough such that equation (3.50) has no positive solution $u$ with $C -$ norm $\| u \| = R$ , and it follows from Lemma 3.6.2 that $\deg _ { P } ( i d - T , B _ { R } ( \theta ) \cap P , \theta ) = 0$ . According to condition (3.51) and the homotopy invariance, we have $\rho > 0$ small such that $\deg _ { P } ( i d - T , B _ { \rho } ( \theta ) \cap P , \theta ) = 1$ . Therefore there is a nontrivial positive solution. □

# 3.6.3 Krein–Rutman Theory for Positive Linear Operators

Let $X$ be a real Banach space, and $P \subset X$ be a positive closed cone with nonempty interior $\overset { \circ } { P }$ . A linear continuous operator $L \in \mathcal { L } ( X , X )$ is called 。 positive if $L ( P ) \subset P$ ; it is called strictly positive if $L ( P \backslash \{ \theta \} ) \subset { \overset { \circ } { P } }$ .

Example. Let $M$ be a compact topological space with a Radon measure $\mu$ . Let $X = C ( M ) , P = \{ u \in C ( M ) | u \geq 0 \}$ , then $( X , P )$ is an OBS. Given a nonnegative continuous function $K : M \times M \longrightarrow R ^ { 1 }$ , we define:

$$
(L u) (x) = \int_ {M} K (x, y) u (y) d \mu .
$$

Then $L$ is a positive operator.

Example. We consider the second-order elliptic operator:

$$
L u = - \Sigma_ {i, j = 1} ^ {n} a _ {i j} \partial_ {i j} ^ {2} u + \Sigma_ {i = 1} ^ {n} b _ {i} \partial_ {i} u + c u, \tag {3.52}
$$

where $a _ { i j } , b _ { i } , c \in C ( \Omega )$ , $i , j = 1 , 2 , \ldots , n$ , Ω Rn is an open domain with smooth boundary (cf. Protter and Weinberger [PW]). Assume the ellipticity condition, i.e., $\exists \alpha > 0$ such that

$$
\Sigma_ {i, j = 1} ^ {n} a _ {i j} (x) \xi_ {i} \xi_ {j} \geq \alpha | \xi | ^ {2}, \forall \xi = (\xi_ {1}, \ldots , \xi_ {n}) \in R ^ {n}, \forall x \in \overline {{\Omega}},
$$

and

$$
c \geq 0.
$$

It is known that $\forall f \in L ^ { p } , 1 < p < \infty$ , the equation:

$$
\left\{ \begin{array}{l} L u = f  , \\ u | _ {\partial \Omega} = 0  , \end{array} \right.
$$

has a unique solution $u \in W ^ { 2 , p } ( \Omega )$ ; the inverse operator $K = L ^ { - 1 } : f \to u$ is positive and bounded on $L ^ { p } ( \Omega )$ (and also on $C _ { 0 } ( \overline { { \Omega } } )$ the subspace of $C ( { \overline { { \Omega } } } )$ with boundary value zero), but not strictly positive. However, we have:

Lemma 3.6.11 Let $X = C _ { 0 } ^ { 1 } ( \Omega )$ , and $P = \{ u \in X | u \geq 0 \}$ . Then $K = L ^ { - 1 }$ is a strictly positive compact operator.

Proof. We only want to prove the strict positivity, i.e., $\forall f \in C _ { 0 } ^ { 1 } ( \overline { { \Omega } } )$ if $f \geq 0$ , but $f \neq \theta$ , then $u = K f \in { \overset { \circ } { P } }$ .

In fact, by the strong maximum principle, $u ( x ) > 0 \ \forall x \in \ \Omega$ . Again by the Hopf maximum principle, $\partial _ { n } u | _ { \partial \Omega } ~ < ~ 0$ , where $\partial _ { n }$ is the outer normal derivative. Since $\partial \Omega$ is compact, $\exists \delta > 0$ such that $\begin{array} { r } { \operatorname* { s u p } _ { x \in \partial \Omega } \partial _ { n } u ( x ) < - \delta < 0 } \end{array}$ . Now, $u \in C ^ { 1 } ( \overline { { \Omega } } )$ , there is a neighborhood $N$ of $\partial \Omega$ on which $\partial _ { \nu } u \le - \delta / 2$ , where $\nu$ is the direction connecting $x \in N$ to the closest point on $\partial \Omega$ . Setting $\alpha = \operatorname* { i n f } \left\{ u ( x ) | x \in \Omega \backslash N \right\}$ and $\beta = \mathrm { M i n } \{ \alpha , \delta / 2 \}$ , then the open ball, centered at $u$ with radius $\beta$ , is contained in $\overset { \circ } { P }$ 。 . □

The first eigenvalue $\lambda _ { 1 }$ and the first eigenfunction $\phi$ play important role in the study of second-order elliptic equations. It is known that $\lambda _ { 1 } > 0$ is algebraically simple, and that the normalized $\phi > 0$ is unique. In fact, the conclusion is a special case of the following Krein–Rutman theorem:

Theorem 3.6.12 (Krein–Rutman) Suppose that $( X , P )$ is an OBS with non-。 empty interior $P$ , and that $T$ is a compact strictly positive operator. Then $T$ possesses a unique positive eigenvector $\phi \in \mathring { P }$ 。 with $\| \phi \| = 1$ , associated with a algebraically simple eigenvalue $\begin{array} { r } { \frac { 1 } { \lambda _ { 1 } } = \operatorname* { l i m } _ { n \to \infty } \| T ^ { n } \| ^ { \frac { 1 } { n } } > 0 } \end{array}$ , satisfying:

$$
\lambda_ {1} \leq | \lambda | \quad \forall \lambda^ {- 1} \in \sigma (T). \tag {3.53}
$$

Proof. (1). (The existence of a positive eigenvector.) $\forall x \in P \backslash \{ \theta \} , \exists M > 0$ , such that $M T x \geq x$ . For otherwise, it must be $\textstyle T x - { \frac { 1 } { n } } x \not \in P , \forall n \in { \mathcal { N } }$ large, and then $T x \notin { \overset { \circ } { P } }$ 。 ; this contradicts the strict positiveness of $T$ . According to Dugundji’s theorem, there exists a retraction $r : X  P$ . We define $\tau$ : $X \times R ^ { 1 } \to P$ as follows:

$$
\mathcal {T} (y, \lambda) = \lambda r \circ T (y + x).
$$

Thus, $\forall \lambda , T ( \cdot , \lambda )$ is compact, and satisfies $\boldsymbol { \mathcal { T } } ( \boldsymbol { y } , 0 ) = \boldsymbol { \theta }$ , $\forall y \in X$ . We notice that for $\lambda \ge 0 , y = \mathcal { T } ( y , \lambda )$ is equivalent to $y \in P$ , and $y = \lambda T ( y + x )$ . From the Leray–Schauder Theorem, there is an unbounded connected branch of solutions $\zeta ^ { + } \subset P \times \mathbb { R } _ { + } ^ { 1 }$ passing through $( \theta , 0 )$ .

We claim that if $( y , \lambda ) \in \zeta ^ { + }$ , then $\lambda \in \vert 0 , M \vert$ . In fact, since $M T x \geq x$ , $y =$ $\begin{array} { r } { \lambda T ( y + x ) \ge \frac { \lambda } { M } x } \end{array}$ , we obtain

$$
y \geq \lambda T y \geq \left(\frac {\lambda}{M}\right) ^ {n} x, \forall n \in \mathcal {N}.
$$

If $\lambda > M$ , then $x = \theta$ . This is a contradiction. Therefore, $\exists ( y _ { n } , \lambda _ { n } ) \in \zeta _ { + }$ such that $\lambda _ { n } \to \lambda ^ { * } \in [ 0 , M ] , \| y _ { n } \| > n$ . Setting $\begin{array} { r } { z _ { n } = \frac { y _ { n } } { \left\| y _ { n } \right\| } } \end{array}$ , we have

$$
z _ {n} = \lambda_ {n} T z _ {n} + \frac {\lambda_ {n}}{\| y _ {n} \|} T x.
$$

Since $T$ is compact, we have $z _ { n }  z ^ { * } \neq \theta$ satisfying $z ^ { * } = \lambda ^ { * } T z ^ { * }$ . This implies that $z ^ { * } \in \stackrel { \circ } { P } , \lambda ^ { * } > 0 , \| z ^ { * } \| = 1$ 。 . Let $\phi = z ^ { * }$ , and $\lambda _ { 1 } = \lambda ^ { * }$ ; these are what we need.

(2). (The uniqueness of the positive eigenvector.) $\forall x \notin P , \forall y \in P$ , let us define $\delta _ { y } ( x ) = \operatorname* { s u p } { \{ \lambda \geq 0 | y + \lambda x \in P \} }$ . It is easily seen that $\delta _ { y } ( x ) > 0$ is a continuous function of $x \in X \backslash P$ . By definition,

$$
\left\{ \begin{array}{l} \lambda \in [ 0, \delta_ {y} (x) ] \text {i m p l i e s} y + \lambda x \in P  , \\ \lambda > \delta_ {y} (x) \text {i m p l i e s} y + \lambda x \notin P  . \end{array} \right.
$$

。 Now, suppose that $\phi _ { i } \in \breve { P }$ satisfy $\| \phi _ { i } \| = 1 , \ \phi _ { i } = \lambda _ { i } T \phi _ { i } , \ i = 1 , 2$ . Set

$$
\gamma_ {1} = \delta_ {\phi_ {1}} (- \phi_ {2}), \gamma_ {2} = \delta_ {\phi_ {2}} (- \phi_ {1}).
$$

Then

$$
\left\{ \begin{array}{l} T (\phi_ {1} - \gamma_ {1} \phi_ {2}) = \frac {1}{\lambda_ {1}} (\phi_ {1} - \gamma_ {1} \frac {\lambda_ {1}}{\lambda_ {2}} \phi_ {2}), \\ T (\phi_ {2} - \gamma_ {2} \phi_ {1}) = \frac {1}{\lambda_ {2}} (\phi_ {2} - \gamma_ {2} \frac {\lambda_ {2}}{\lambda_ {1}} \phi_ {1}). \end{array} \right.
$$

If $\phi _ { 2 } - \gamma _ { 2 } \phi _ { 1 } \neq \theta$ , then $T ( \phi _ { 2 } - \gamma _ { 2 } \phi _ { 1 } ) \in \stackrel { \cup } { P }$ . This implies $\lambda _ { 2 } ~ < ~ \lambda _ { 1 }$ . But $T ( \phi _ { 1 } - \gamma _ { 1 } \phi _ { 2 } ) \in P$ implies $\lambda _ { 1 } \leq \lambda _ { 2 }$ . This is a contradiction. Therefore, $\phi _ { 1 } = \phi _ { 2 }$ . Again, by the normality, $\lambda _ { 1 } = \lambda _ { 2 }$ .

(3). (The inequality (3.52)) Assume that $\psi \in X \backslash ( P \cup ( - P ) )$ satisfying $\lambda T \psi = \psi$ , and $\| \psi \| = 1$ .

In the case where $\lambda$ is real: From $\phi \pm \delta _ { \phi } ( \pm \psi ) \psi \neq \theta$ , it follows that

$$
\frac {1}{\lambda_ {1}} \left(\phi \pm \frac {\lambda_ {1}}{\lambda} \delta_ {\phi} (\pm \psi) \psi\right) = T (\phi \pm \delta_ {\phi} (\pm \psi) \psi) \in \mathring {P}.
$$

Therefore $\lambda _ { 1 } < | \lambda |$

In the case where $\lambda$ is not real: We write $\lambda = | \lambda | e ^ { i \theta }$ , then $\exists x _ { 1 } , x _ { 2 } \in X$ , such that

$$
x _ {1} + x _ {2} = \lambda \left(T x _ {1} + i T x _ {2}\right).
$$

Let $H = \operatorname { s p a n } \{ x _ { 1 } , x _ { 2 } \}$ , then $\begin{array} { r } { T | _ { H } = \frac { 1 } { | \lambda | } R _ { \theta } } \end{array}$ , where $R _ { \theta }$ is the following matrix:

$$
\left( \begin{array}{c c} \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta \end{array} \right)
$$

We claim that $P \cap H = \{ \theta \}$ . For otherwise, since both $P$ and $H$ are invariant with respect to $T$ , $T | _ { P \cap H }$ is positive, according to the conclusion of the first paragraph, it should have a positive eigenvalue. But this contradicts the representation of $R _ { \theta }$ .

Now, $\forall z \in H \backslash \{ \theta \}$ ,

$$
\frac {1}{\lambda_ {1}} \left(\phi + \frac {\lambda_ {1}}{| \lambda |} \delta_ {\phi} (z) R _ {\theta} z\right) = T (\phi + \delta_ {\phi} (z) z) \in \stackrel {\circ} {P}.
$$

Thus ${ \frac { \lambda _ { 1 } } { | \lambda | } } \delta _ { \phi } ( z ) ~ < ~ \delta _ { \phi } ( R _ { \theta } z )$ . We choose $z _ { 0 } ~ \in ~ H \backslash \{ \theta \}$ such that $\delta _ { \phi } ( z _ { 0 } ) ~ =$ $\operatorname* { s u p } \left\{ \delta _ { \phi } ( z ) | z \in C \right\}$ , where $C = \{ \cos \theta x _ { 1 } + \sin \theta x _ { 2 } | \theta \in [ 0 , 2 \pi ] \}$ , it follows that $\lambda _ { 1 } < | \lambda |$ .

(4). (The algebraic simplicity of $\lambda _ { 1 }$ ). From (1), we have shown that $\mathrm { k e r ( i d - }$ $\lambda _ { 1 } T ) = \operatorname { s p a n } \{ \phi \}$ . According to the Riesz–Schauder theory, it is sufficient to show that $\ker ( \mathrm { i d } - \lambda _ { 1 } T ) ^ { 2 } = \ker ( \mathrm { i d } - \lambda _ { 1 } T )$ , i.e., if $x \in X$ satisfies $x - \lambda _ { 1 } T x = - \phi$ , then $x = \theta$ . To this end, it is sufficient to show that $x \in P \cap ( - P )$ .

In fact, if $x \notin P$ , let $\gamma = \delta _ { \phi } ( - x )$ , then

$$
T (\phi - \gamma x) = \frac {1}{\lambda_ {1}} \phi - \frac {\gamma}{\lambda_ {1}} (x + \phi).
$$

It follows that

$$
\frac {1}{\lambda_ {1}} (\phi - \gamma x) = T (\phi - \gamma x) + \frac {\gamma}{\lambda_ {1}} \phi \in \stackrel {\circ} {P}.
$$

It contradicts the definition of $\gamma$ , therefore $x \in P$ . Similarly, $x \in - P$ . Therefore $x \in P \cap ( - P ) = \theta$ . □

Corollary 3.6.13 Let $L$ be the linear second-order elliptic operator (3.52) with the Dirichlet boundary condition, defined on a bounded open domain $\Omega$ in $R ^ { n }$ , the boundary ∂Ω being smooth. Then the eigenvalue problem:

$$
\left\{ \begin{array}{c} L u = \lambda u \quad i n   \Omega \\ u = 0 \quad o n   \partial \Omega  . \end{array} \right.
$$

has a positive eigenfuction $\phi$ with positive, algebraically and geometrically simple eigenvalue $\lambda _ { 1 }$ , satisfying

$$
\lambda_ {1} \leq | \lambda | \quad \forall \lambda \in \sigma (L).
$$

Corollary 3.6.14 ([Tu]). Let $( X , P )$ be an OBS, and let $f , T , g , \Sigma ^ { + }$ be as in Theorem 3.6.7, in which $T$ is a strictly positive compact operator. Then $( \theta , \lambda _ { 1 } )$ is a bifurcation point and $\Sigma ^ { + }$ contains an unbounded connected component in $P$ emanating from $( \theta , \lambda _ { 1 } )$ .

Also, we have the following result on the bifurcation at infinity:

Corollary 3.6.15 Let $( X , P )$ be an OBS. Let $T \in { \mathcal { L } } ( X , X )$ be strictly positive and compact, and $g : P \times R ^ { + } \to P$ be compact, satisfying $g ( x , 0 ) = \theta , g ( x , \lambda ) =$ $o ( \| x \| )$ , as $\| x \| \to \infty$ uniformly on each finite interval. Define

$$
f (x, \lambda) = \lambda T x + g (x, \lambda) \forall (x, \lambda) \in P \times R ^ {+}.
$$

Let $\Sigma = \{ ( x , \lambda ) \in P \times R ^ { + } | f ( x , \lambda ) = x \}$ and $\Sigma ^ { + } = \overline { { \Sigma \cap ( P \backslash \{ \theta \} \times R ^ { + } \backslash \{ 0 \} ) } }$ . Then there are only two possibilities for the connected component $\zeta$ of $\Sigma ^ { + }$ , passing through the point $( \infty , \lambda _ { 1 } )$ , where $\lambda _ { 1 }$ is the eigenvalue of $T$ with positive eigenvector:

(1) The projection of $\zeta$ onto $R _ { + } ^ { 1 }$ is unbounded.   
(2) $\zeta \cap ( \{ \theta \} \times R _ { + } ^ { 1 } ) \neq \emptyset$ .

# 3.6.4 Multiple Solutions

In this subsection, we present a few examples showing that degree theory is applicable to the study of multiple solutions for certain nonlinear elliptic equations.

Example 1. Let $L$ be the second-order elliptic operator defined in (3.52), and let $g \in C ( R ^ { 1 } )$ satisfy $g ( 0 ) = 0 , | g ( t ) | \leq C$ , and $g ( t ) - \lambda t = o ( t )$ , as $| t | $ $0$ , where $C$ is a constant, and $\lambda \in \mathbb { R } ^ { 1 }$ is between the first and the second eigenvalues of $L$ . We consider the equation:

$$
\left\{ \begin{array}{c} L u = g (u) \text {i n} \Omega , \\ u = 0. \quad \text {o n} \partial \Omega . \end{array} \right. \tag {3.54}
$$

Obviously, $u = 0$ is a trivial solution. We shall show that there exists a nontrivial solution. This can be seen by the following degree computation.

Let $X = C _ { 0 } ^ { 1 } ( \overline { { \Omega } } )$ , since $\lambda$ is between $\lambda _ { 1 }$ and $\lambda _ { 2 }$ , by homotopy invariance of the index,

$$
i (\mathrm {i d} - L ^ {- 1} g, \theta) = i (\mathrm {i d} - \lambda L ^ {- 1}, \theta) = - 1.
$$

Since all solutions are bounded in $X$ , again by the homotopy invariance of the degree,

$$
\deg \left(\mathrm {i d} - L ^ {- 1} g, B _ {R} (\theta), \theta\right) = \deg \left(\mathrm {i d}, B _ {R} (\theta), \theta\right) = 1.
$$

If there were no nontrivial solution, the above identities would contradict the additivity of the degree.

If one slightly changes the above assumptions then there exists a positive solution.

Example 2. Let $L$ be as in (3.52), and let $g \in C ^ { 1 } ( R ^ { 1 } )$ satisfy $g ( 0 ) = 0 , 0 \leq g \leq$ $C$ , and $g ( t ) - \lambda t = o ( t )$ , as $t  0$ with $\lambda > \lambda _ { 1 }$ . Then there exists a positive solution of (3.54).

This can be shown by the sub- and super-solutions method. Obviously, $\overline { { u } } =$ $L ^ { - 1 } C$ is an super-solution. Let $\underline { { u } } = \epsilon \phi _ { 1 }$ , where $\phi _ { 1 }$ is the positive eigenfunction

of $L$ , and $\epsilon > 0$ small. By the condition of $g$ near zero, it is a sub-solution of $L$ . From the Hopf maximum principle, we have $u \leq \overline { { u } }$ . Thus we obtain a positive solution.

We are inspired by Example 2 to compute the degree of an order-preserving compact map on an order interval.

。 Theorem 3.6.16 Let $( X , P )$ be an OBS with ${ \check { P } } \neq \emptyset$ , and let $[ \underline { { u } } , \overline { { u } } ] \subset X$ be a $\{ e _ { 1 } , \ldots , e _ { k } \}$ finitely bounded order interval for some $\{ e _ { 1 } , \ldots , e _ { k } \} \subset X$ . If $f : [ { \underline { { u } } } , { \overline { { u } } } ] \to X$ is order preserving, compact and satisfying:

$$
f (\underline {{u}}) \in \underline {{u}} + \stackrel {\circ} {P}, f (\overline {{u}}) \in \overline {{u}} - \stackrel {\circ} {P};
$$

Then $U : = [ \underline { { u } } , \overline { { u } } ] ^ { \circ } \neq \emptyset$ and

$$
\deg (i d - f, U, \theta) = 1.
$$

Proof. Since $f$ is order preserving, $\forall u \in [ \underline { { u } } , \overline { { u } } ]$ , $f ( { \underline { { u } } } ) \leq f ( u ) \leq f ( { \overline { { u } } } )$

Moreover, we have $f ( \left[ { \underline { { u } } } , { \overline { { u } } } \right] ) \subset U$ . In fact, $\exists \epsilon > 0$ , such that

$$
f (\underline {{u}}) + B _ {\epsilon} (\theta) \subset \underline {{u}} + P, f (\bar {u}) + B _ {\epsilon} (\theta) \subset \bar {u} - P.
$$

Therefore

$$
f (u) + B _ {\epsilon} (\theta) \in (\underline {{u}} + P) \cap (\bar {u} - P),
$$

i.e., $f ( u ) \in U$ . This implies that $U$ is a nonempty open convex set.

Choosing arbitrarily $x \in U , \forall t \in \lfloor 0 , 1 \rfloor$ , define

$$
\phi (u, t) = t (u - f (u)) + (1 - t) (u - x).
$$

Since $f ( \partial U ) \subset U$ , $t f ( u ) + ( 1 - t ) x \in U \forall ( u , t ) \in \partial U \times [ 0 , 1 ]$ ]. Thus $\theta \not \in$ $\phi ( \partial U \times [ 0 , 1 ] )$ . From the homotopy invariance of the degree,

$$
\deg (\operatorname {i d} - f, U, \theta) = \deg (\phi (\cdot , 1), U, \theta) = \deg (\phi (\cdot , 0), U, \theta) = \deg (\operatorname {i d}, U, x) = 1.
$$

□

The above theorem is due to Amann [Am 3].

${ \overset { \circ } { P } } \neq \emptyset$ Corollary 3.6.17 (Amann) Let $( X , P )$ be an OBS with , and let $\underline { { u } } _ { i } , \boldsymbol { u } _ { i }$ , $i = 1 , 2$ , satisfy $u _ { 1 } \leq \overline { { u _ { 1 } } } \leq \bar { u _ { 2 } } , \underline { { u _ { 1 } } } \leq \underline { { u _ { 2 } } } \leq \overline { { u _ { 2 } } }$ ; $a n d [ \underline { { u _ { 1 } } } , \overline { { u _ { 1 } } } ] \cap [ \underline { { u _ { 2 } } } , \overline { { u _ { 2 } } } ] = \emptyset$ . If $[ \underline { { u _ { 1 } } } , \bar { u _ { 2 } } ]$ is $\{ e _ { 1 } , \ldots , e _ { k } \}$ - finitely bounded for some $\{ e _ { 1 } , \ldots , e _ { k } \} \subset X$ , and if $f : [ \underline { { { u _ { 1 } } } } , \overline { { { u _ { 2 } } } } ] \to X$ is an order-preserving compact map satisfying:

$$
f (\underline {{u}} _ {i}) \in \underline {{u}} _ {i} + \stackrel {\circ} {P}, f (\overline {{u _ {i}}}) \in \overline {{u _ {i}}} - \stackrel {\circ} {P}, i = 1, 2.
$$

Then there are at least three distinct fixed points of $f$ on the order interval $[ \underline { { u _ { 1 } } } , \overline { { u _ { 2 } } } ]$ .

Proof. Set $U _ { i } = [ \underline { { { u _ { i } } } } , \overline { { { u _ { i } } } } ] ) ^ { \circ } , ~ i = 1 , 2$ , and $U = [ \underline { { u _ { 1 } } } , \overline { { u _ { 2 } } } ] ^ { \circ }$ . From the additivity,

$$
\begin{array}{l} \deg (\mathrm {i d} - f, U, \theta) = \deg (\mathrm {i d} - f, U _ {1}, \theta) + \deg (\mathrm {i d} - f, U _ {2}, \theta) \\ + \deg (\mathrm {i d} - f, U \backslash \overline {{\left(U _ {1} \cup U _ {2}\right)}}, \theta). \\ \end{array}
$$

Applying the above theorem, we have

$$
\deg \left(\operatorname {i d} - f, U \backslash \left(U _ {1} \cup U _ {2}\right), \theta\right) = - 1.
$$

According to the Kronecker existence property, $\exists$ fixed points $u _ { i } , \ i = 1 , 2 , 3$ , such that $u _ { i } \in U _ { i } , \ i = 1 , 2$ . and $u _ { 3 } \in U \backslash ( U _ { 1 } \cup U _ { 2 } )$ . □

In the applications to PDEs, let $\Omega \subset R ^ { n }$ be a bounded domain with smooth boundary. We take $X = C _ { 0 } ^ { 1 } ( \overline { { \Omega } } )$ , and $e \in X$ is a positive function in $\Omega$ and satisfies $\partial _ { n } e ( x ) < 0$ , $\forall x \in \partial \Omega$ , where $\partial _ { n }$ is the outer normal derivative. Then it is easy to verify that any order interval $[ u , v ]$ is $e \mathrm { - }$ finitely bounded.

Example 3. Let $L$ be the operator defined in (3.52), and let $g \in C ^ { 1 } ( R , \mathbb { R } _ { + } ^ { 1 } )$ be nondecreasing and satisfying $g ( 0 ) = 0 , g ^ { \prime } ( 0 ) = 1 , t < g ( t ) \forall t \in ( 0 , t _ { 0 } ]$ for some small $t _ { 0 } > 0$ , and $g ( t ) \leq C$ as $t > 0$ , where $C > 0$ is a constant. Then $\exists \lambda ^ { * } \in ( 0 , \lambda _ { 1 } )$ such that the equation:

$$
\left\{ \begin{array}{c c} L u = \lambda g (u), & \text {i n} \Omega \\ u = 0, & \text {o n} \partial \Omega , \end{array} \right. \tag {3.55}
$$

possesses at least one positive solution $\forall \lambda \geq \lambda ^ { * }$ and at least two distinct positive solutions $\forall \lambda \in \left( \lambda ^ { * } , \lambda _ { 1 } \right)$ .

Indeed, we extend $g$ to $R ^ { 1 }$ oddly; the mapping $f ( u ) = L ^ { - 1 } g ( u )$ is order preserving. $\forall \lambda > 0$ , it has a pair of strict sub- and super-solutions $\{ - \lambda C e , + \lambda C e \}$ , where $e = L ^ { - 1 } 1$ . Thus, $v _ { \lambda } : = \operatorname* { l i m } _ { n \to \infty } ( \lambda f ) ^ { n } ( \lambda C )$ exists and is the maximal solution of (3.55). Obviously, $v _ { \lambda }$ is upper semi-continuous and monotone nondecreasing in $\lambda$ . Define $\lambda ^ { * } = \operatorname* { i n f } \{ \lambda \in R _ { + } ^ { 1 } | v _ { \lambda } > 0 \}$ . We claim that $\lambda ^ { * } < \lambda _ { 1 }$ , where $\lambda _ { 1 }$ is the first eigenvalue of $L$ . In fact, near the bifurcation point $( \theta , \lambda _ { 1 } )$ we find $u _ { \lambda } > 0$ , a solution of (3.55) with small $\| u _ { \lambda } \| _ { C } < t _ { 0 }$ . One has

$$
\int_ {\Omega} L u _ {\lambda} \phi_ {1} d x = \lambda \int_ {\Omega} g (u _ {\lambda}) \phi_ {1} d x,
$$

where $\phi _ { 1 } > 0$ is the first normalized eigenfunction of $L ^ { * }$ . Then

$$
\lambda_ {1} \int_ {\Omega} u _ {\lambda} \phi_ {1} d x > \lambda \int u _ {\lambda} \phi_ {1} d x.
$$

It follows that $\lambda < \lambda _ { 1 }$ , Since $v _ { \lambda } \ge u _ { \lambda } > 0$ , then $\lambda ^ { * } < \lambda _ { 1 }$

For $\forall \lambda \in ( \lambda ^ { * } , \lambda _ { 1 } ) \ \exists \delta > 0$ , such that $\lambda g ( t ) ~ < ~ \lambda _ { 1 } t$ , as $| t | < \delta$ . Let $\phi _ { 1 }$ be the normalized first eigenfunction; as $\epsilon \in ( 0 , \delta )$ , $\pm \epsilon \phi _ { 1 }$ is a pair of suband super-solutions of (3.55). However, as $\begin{array} { r } { \lambda ^ { \prime } = \frac { \lambda ^ { * } + \lambda } { 2 } } \end{array}$ ±, $v _ { \lambda ^ { \prime } }$ is a sub-solution of (3.55). Set $\epsilon > 0$ small enough such that $\epsilon \phi _ { 1 } \ < \ v _ { \lambda ^ { \prime } }$ . Then we find two pairs of sub- and super solutions: $[ - \epsilon \phi _ { 1 } , + \epsilon \phi _ { 1 } ] , [ v _ { \lambda ^ { \prime } } , \lambda C e ]$ . According to the Amann theorem, there must be two distinct nontrivial solutions of (3.55): $u _ { \lambda } ^ { \mathrm { 1 } } \in \lbrack - \epsilon \phi _ { 1 } , \lambda C e ] \backslash ( \lbrack - \epsilon \phi _ { 1 } , \epsilon \phi _ { 1 } ] \cup \lbrack v _ { \lambda ^ { \prime } } , \lambda C e \rbrack )$ , and $u _ { \lambda } ^ { 2 } \in [ v _ { \lambda ^ { \prime } } , \lambda C e ]$ . Since $\epsilon > 0$ can be arbitrarily small, both of $u _ { \lambda } ^ { 1 } , u _ { \lambda } ^ { 2 }$ are positive. This is our conclusion.

# 3.6.5 A Free Boundary Problem

We turn to an application of the bifurcation at infinity to a free boundary problem for the flux equation in the confined plasma. Let $\Omega \subset R ^ { 2 }$ be the section of the container, a bounded domain with smooth boundary rotated along and off the $z$ axis in $R ^ { 3 }$ , which contains plasma confined by an external magnetic field. Let $u$ be the flux function of the magnetic field. The current $I$ is a given positive constant. The following equation is derived from the Maxwell equation:

$$
\left\{ \begin{array}{c} L u := - \left[ \frac {\partial}{\partial r} \left(\frac {1}{r} \frac {\partial}{\partial r}\right) + \frac {1}{r} \frac {\partial^ {2}}{\partial z} \right] u = \lambda u _ {+} \text {i n} \Omega , \\ u = C, \text {o n} \partial \Omega , \\ - \int_ {\partial \Omega} \frac {1}{r} \frac {\partial u}{\partial n} = I, \end{array} \right. \tag {3.56}
$$

where $C$ is a constant to be determined, and $u _ { + } = \operatorname* { m a x } { \{ u , 0 \} }$

The domain $\Omega _ { p } : = \{ x \in \Omega | u ( x ) > 0 \}$ , which is the space occupied by the plasma, is unknown. It will be solved simultaneously with the flux function $u$ . Physically, we are interested in the case where $\Omega _ { p } \subset \overline { { \Omega _ { p } } } \subset \Omega$ , and $\Omega _ { p } \neq \emptyset$ .

From Green’s formula,

$$
\lambda \int_ {\Omega} u _ {+} = \int_ {\Omega} L u = - \int_ {\partial \Omega} \frac {1}{r} \frac {\partial u}{\partial n} = I.
$$

We see that the problem is solvable only if $\lambda > 0$ .

Again, from Green’s formula,

$$
\lambda \int_ {\Omega} u _ {+} \phi_ {1} = \lambda_ {1} \int_ {\Omega} u \phi_ {1} + C \int_ {\partial \Omega} \frac {1}{r} \frac {\partial \phi_ {1}}{\partial n},
$$

where $\phi _ { 1 }$ is the first eigenfunction of $L$ with respect to Dirichlet boundary condition.

Combining with the maximum principle it is seen that $C > 0 \Rightarrow \lambda < \lambda _ { 1 }$ , and $C = 0 \Rightarrow \lambda = \lambda _ { 1 }$ . In all these cases, the solution $u ( x ) > 0 \forall x \in \Omega$ , from the maximum principle. i.e., $\Omega _ { p } = \Omega$ . They are not of interest in physics.

We reduce the problem (3.56) to the following: Given a constant $C _ { 1 } < 0$ , find $( v , \lambda )$ satisfying

$$
\left\{ \begin{array}{c} L v = \lambda v _ {+} \text {i n} \Omega , \\ v = C _ {1} \text {o n} \partial \Omega , \\ \lambda > 0. \end{array} \right. \tag {3.57}
$$

Indeed, if $( v , \lambda )$ is a solution of (3.57), let $v = \alpha u , C _ { 1 } = \alpha C$ , where $\alpha > 0$ is an adjustment constant satisfying

$$
\int_ {\partial \Omega} \frac {1}{r} \frac {\partial u}{\partial n} = - I.
$$

Then $( u , \lambda )$ is the solution of equation (3.56). We have the following conclusion:

Statement 3.6.1 ${ \bf 8 } \ \forall I > 0 , \exists u \in C ^ { 2 , \gamma } ( \overline { { \Omega } } ) \ \gamma \in ( 0 , 1 )$ with $C < 0$ , which solves equation (3.56) if and only if $\lambda > \lambda _ { 1 }$ .

Proof. After the above discussion, we just take $C _ { 1 } = - 1$ . Let $w = v + 1$ , then equation (3.57) is reduced to

$$
\left\{ \begin{array}{c} L w = \lambda (w - 1) _ {+} \text {i n} \Omega , \\ w = 0 \text {o n} \partial \Omega , \\ \lambda > 0. \end{array} \right. \tag {3.58}
$$

Let us take $X = C ( \overline { { \Omega } } )$ , $F : w \mapsto ( w - 1 ) _ { + }$ , $K = L ^ { - 1 }$ , our problem is reduced to finding the solution of $w = \lambda K \circ F ( w )$ . It is easily seen that there is no bifurcation point located at $\{ \theta \} \times R _ { + } ^ { 1 }$ . According to Corollary 3.6.15, the projection of the connected component $\zeta$ of the positive solution set passing through the bifurcation point at infinity $( \infty , \lambda _ { 1 } )$ onto $R _ { + } ^ { 1 }$ is unbounded. It must include the interval $( \lambda _ { 1 } , + \infty )$ . Therefore, $\forall \lambda \in ( \lambda _ { 1 } , + \infty )$ , there is a solution $w _ { \lambda }$ on $\zeta$ .

On the other hand, if there is a nontrivial solution $w _ { \lambda }$ of equation (3.58), then $w _ { \lambda } \geq 0$ and $F ( w _ { \lambda } ) < w _ { \lambda }$ . Thus

$$
\begin{array}{l} \int_ {\Omega} w _ {\lambda} \phi_ {1} d x = \frac {1}{\lambda_ {1}} \int_ {\Omega} w _ {\lambda} L \phi_ {1} d x \\ = \frac {1}{\lambda_ {1}} \int_ {\Omega} L w _ {\lambda} \phi_ {1} d x \\ = \frac {\lambda}{\lambda_ {1}} \int_ {\Omega} F (w _ {\lambda}) \phi_ {1} d x \\ <   \frac {\lambda}{\lambda_ {1}} \int_ {\Omega} w _ {\lambda} \phi_ {1} d x. \\ \end{array}
$$

This proves $\lambda > \lambda _ { 1 }$ . The proof of the regularity is standard.

# 3.6.6 Bridging

We present an example to express the fact that solutions of a differential equation on disjoint domains can be glued up as a new solution in a larger domain. A typical example we consider here is a dumbbell $D \subset R ^ { 2 }$ , i.e., two disks $B _ { \pm } = B _ { 1 / 2 } ( ( \pm 1 , 0 ) )$ connected by a segment $E = \{ ( x , y ) \in R ^ { 2 } | y =$ $0 , x \in [ - 1 / 2 , 1 / 2 ] \}$ . Let $B = B _ { + } \cup B _ { - }$ , and $D = B \cup E$ . Assuming that $f \in C ^ { 1 }$ is of power growth with $f ( 0 ) = 0$ , and that $u _ { \pm } \in H _ { 0 } ^ { 1 } ( B _ { \pm } )$ are solutions of the equation:

$$
- \triangle u = f (u). \tag {3.59}
$$

We consider a sequence of domains $D _ { n }$ squeezing to $D$ , and want to find a solution $u _ { n } \in H _ { 0 } ^ { 1 } ( D _ { n } )$ of (3.59) such that $u _ { n }$ is closed to $u _ { \pm }$ in certain sense.

![](images/a90fb917b46c1354f2b350d06c97dfd97797867f3e1eaa68d4ecd473a7de2022.jpg)  
Fig. 3.5.

Let $\Omega$ be a bounded domain containing $D _ { n }$ , and $D \subset D _ { n }$ . Assuming that $U \subseteq \Omega$ is a bounded domain, the composition operator $u \mapsto f ( u )$ is $C ^ { 1 }$ from $H ^ { 1 } ( U )$ to $L ^ { q } ( U )$ , $\forall q \in ( 1 , \infty )$ . Let $r : H ^ { 1 } ( \Omega ) \to H ^ { 1 } ( U )$ be the restriction $u  u | _ { U }$ , and $e : H _ { 0 } ^ { 1 } ( U ) \to H _ { 0 } ^ { 1 } ( \Omega )$ be the extension $( e u ) ( x ) = u ( x )$ if $x \in U$ and 0 elsewhere. Let $K = ( - \varDelta ) ^ { - 1 } : L ^ { q } ( U ) \to H _ { 0 } ^ { 1 } ( U ) , 1 \leq q < \infty$ , then $K$ is linear and compact. We define $T = e \cdot K \cdot f \cdot r : H _ { 0 } ^ { 1 } ( \Omega ) \to H _ { 0 } ^ { 1 } ( \Omega )$ . The following statements are easy to verify:

(1) $\forall u \in H _ { 0 } ^ { 1 } ( U )$ , we have $( r \cdot e ) u = u$ .   
(2) $\forall v \in H _ { 0 } ^ { 1 } ( \Omega )$ , if $r v \in H _ { 0 } ^ { 1 } ( U )$ , then $( e \cdot r ) v = v$   
(3) For $u _ { 0 } \in H _ { 0 } ^ { 1 } ( U ) , u _ { 0 }$ is a solution of (3.59) if and only if $e \cdot u _ { 0 }$ is a fixed point of $T$ .   
(4) If $u _ { 0 }$ is nondegenerate (i.e., $\ker - \varDelta - f ^ { \prime } ( u _ { 0 } ) I ) = \{ \theta \}$ ), then $I - T ^ { \prime } ( u _ { 0 } )$ is invertible on $H _ { 0 } ^ { 1 } ( \Omega )$ .

Let us choose $X = H _ { 0 } ^ { 1 } ( \Omega )$ . Let $T _ { n } , T _ { B }$ and $e _ { n } , e _ { B }$ be the operators and the extensions on $D _ { n } \forall n \in \mathcal { N }$ and $B = B _ { + } \cup B _ { - }$ , respectively.

Statement 3.6.19 For any nondegenerate fixed point $e _ { B } u _ { 0 }$ of $T _ { B }$ , $u _ { 0 } ~ \in$ $H _ { 0 } ^ { 1 } ( B )$ , there exists a fixed point $e _ { n } u _ { n }$ of $T _ { n }$ for $n$ large, such that $\Vert e _ { n } u _ { n } -$ $e _ { 0 } u _ { 0 } \rVert _ { X } \to 0$ as $n \longrightarrow \infty$ .

Proof. By the nondegeneracy assumption, $e _ { B } u _ { 0 }$ is isolated and then $i ( I -$ $T , e _ { B } u _ { 0 } ) = \pm 1$ . In order to show the existence of a fixed point $e _ { n } u _ { n }$ of $T _ { n } , u _ { n } \in$ $H _ { 0 } ^ { \mathrm { 1 } } ( D _ { n } )$ , it is sufficient to verify that for all small $\delta > 0 , I - T _ { n }$ is homotopic to $I - T _ { B }$ on $\partial V$ , where $V = B _ { \delta } ( e _ { B } u _ { 0 } )$ is a ball in $H _ { 0 } ^ { 1 } ( \Omega )$ . If it is so, then

$$
\deg (I - T _ {n}, V, \theta) = \deg (I - T _ {B}, V, \theta) \neq 0.
$$

Our conclusion follows.

Now, we verify it by contradiction. Suppose that $\exists t _ { n } ~ \in ~ [ 0 , 1 ] , \exists u _ { n } ~ \in$ $H _ { 0 } ^ { 1 } ( D _ { n } ) , e _ { n } u _ { n } \ \in \ \partial V$ , such that $e _ { n } u _ { n } = t _ { n } T _ { B } e _ { n } u _ { n } + ( 1 - t _ { n } ) T _ { n } e _ { n } u _ { n }$ . Since

$e _ { n } u _ { n }$ is bounded, and $T _ { n } , T _ { B }$ are compact, there exists a subsequence $u _ { n _ { j } }$ such that $e _ { n _ { j } } u _ { n _ { j } }$ converges to some $v$ and then $v \in \partial V$ .

We verify that $v$ is a solution of (3.59), and that $v ( x ) = 0 , \mathrm { a . e . }$ , on $\Omega \backslash B$ . Since $V$ can be chosen arbitrarily close to $e _ { B } u _ { 0 }$ , it contradicts the nondegeneracy of $u _ { 0 }$ .

(1) $\forall \boldsymbol { \phi } \in H _ { 0 } ^ { 1 } ( B )$ , from

$$
\int_ {B} \nabla u _ {n} \nabla \phi = \int_ {B} f (u _ {n}) \phi ,
$$

it follows that

$$
\int_ {B} \nabla v \nabla \phi = \int_ {B} f (v) \phi ;
$$

i.e., $v$ is a solution (3.59) on $B$ .

(2) From $e _ { n } u _ { n } \ = \ 0$ a.e., on $\Omega \backslash D _ { n }$ , we have $v = 0$ a.e., on $\Omega \backslash D$ . Since $| E | = 0 , v = 0$ , a.e., on $\Omega \backslash B$ . Therefore, $v \in H _ { 0 } ^ { 1 } ( \Omega )$ is in fact a solution of (3.59). □

In other words, $u _ { + }$ and $u _ { - }$ are nondegenerate solutions of (3.59) on $H _ { 0 } ^ { 1 } ( B _ { + } )$ and $H _ { 0 } ^ { 1 } ( B _ { - } )$ , respectively. We define $u _ { 0 } ( x ) = u _ { \pm } ( x )$ as $x \in B _ { \pm }$ , then $e _ { B } u _ { 0 }$ is a degenerate fixed point of $T _ { B }$ . By Statement 3.6.19 there are a sequence of domains $D _ { n }$ squeezing to $D$ and a sequence of solutions $u _ { n } \in H _ { 0 } ^ { 1 } ( D _ { n } )$ of the equation (3.59) such that $u _ { n }$ is close to $u _ { 0 }$ .

# 3.7 Extensions

We briefly introduce a few directions in generalizing the Leray-Schauder degree.

# 3.7.1 Set-Valued Mappings

We have used the fixed-point theorem for set-valued mappings in Chap. 2. It is natural to ask if the degree theory for set-valued mappings can be set up.

Let $X$ be a Banach space, let $\Gamma ( X )$ be the set of all nonempty closed convex subsets of $X$ .

Definition 3.7.1 (Compact convex set-valued mapping) Let $\Omega \subset X$ be a bounded subset, and let $\phi : \Omega \to \Gamma ( X )$ . It is called a compact convex setvalued mapping, if (1) it is upper semi-continuous, and (2) $\overline { { \phi ( \overline { { \Omega } } ) } }$ is compact.

Obviously, any compact single-valued mapping is a compact convex setvalued mapping.

The main idea in extending the Leray–Schauder degree to compact convex set-valued mappings is to approximate the set-valued mapping by singlevalued mappings. The following notion is useful:

Definition 3.7.2 (Single-valued approximation) Let $( M , d )$ be a metric space, and let $Y$ be a Banach space. Let $T : M \to 2 ^ { Y }$ and ${ \tilde { T } } : M \to Y$ . We say that $\tilde { T }$ is a $\epsilon -$ single-valued approximation of $T$ for $\epsilon > 0$ , if

1. $\tilde { T } ( M ) \subset c o \bar { n } v T ( M )$

2. $\forall x \in M$ , $\exists y \in M$ and $z \in T ( y )$ satisfying $d ( x , y ) < \epsilon$ , and $\| \tilde { T } ( x ) - z \| < \epsilon$

The following picture shows the $\epsilon -$ approximation of a set-valued mapping.

![](images/01239155cc4c382fc7fbc040eeed7d64fff6cdf1297113643c8b2527b02eae82.jpg)  
Fig. 3.6.

Theorem 3.7.3 Let $( M , d )$ be a metric space, $X$ be a Banach space, and let $\phi : M \to \Gamma ( X )$ be upper semi-continuous. Then $\forall \epsilon > 0$ there exists an  approximation $\phi _ { \epsilon }$ of $\phi$ .

Proof. By the upper semi-continuity, $\forall x \in M , \exists \delta _ { x } > 0$ , such that $\phi ( B _ { \delta _ { x } } ( x ) ) \subset$ $\phi ( x ) + B _ { \epsilon } ( \theta )$ . One may choose $\delta _ { x } < \frac { \epsilon } { 2 }$ . Then $\mathrm { A } = \{ B _ { \delta _ { x } } ( x ) | x \in M \}$ is an open covering of $M$ . It has an open locally finite star-refinement $\mathrm { B } = \{ V _ { \beta } | \beta \in$ $\varLambda \}$ (see Dugundji [Du 2]). The so-called star-refinement means that $\forall \beta \in$ $A , \exists x _ { \beta } \in M$ such that $S t ( V _ { \beta } , \mathrm { B } ) \subset B _ { \delta _ { x _ { \beta } } } ( x _ { \beta } )$ , where ${ \cal S t } ( V _ { \beta } , \mathrm { B } ) = \bigcup \{ V _ { \alpha } \in$ $\mathrm { B } \mid V _ { \alpha } \cap V _ { \beta } \neq \emptyset \}$ . Define a partition of unity with respect to $\mathrm { B } : \{ \lambda _ { \beta } | \beta \in A \}$ , and

$$
\phi_ {\epsilon} (x) = \Sigma_ {\beta \in \Lambda} \lambda_ {\beta} (x) z _ {\beta},
$$

where $z _ { \beta } \in \phi ( V _ { \beta } ) .$ . We are going to verify that $\phi _ { \epsilon }$ is an $\epsilon -$ approximation of $\phi$ . Indeed, $\forall x _ { 0 } \in M$ , if $x _ { 0 } \in V _ { \beta }$ , then there must be $y \in M$ and $\delta _ { y } < \frac { \epsilon } { 2 }$ such that $V _ { \beta } \subset B _ { \delta _ { y } } ( y )$ . Thus, $d ( x _ { 0 } , y ) < 2 \delta _ { y } < \epsilon$ . However, $z _ { \beta } \in \phi ( V _ { \beta } ) \subset \phi ( B _ { \delta _ { y } } ( y ) ) \subset$ $\phi ( y ) + B _ { \epsilon } ( \theta )$ , we have $\phi _ { \epsilon } ( x _ { 0 } ) = \Sigma _ { i = 1 } ^ { n } \lambda _ { \beta _ { i } } ( x _ { 0 } ) z _ { \beta _ { i } } \in \phi ( y ) + B _ { \epsilon } ( \theta )$ , provided by the convexity of $\phi ( y )$ , and the fact $\{ V _ { \beta _ { i } } \} _ { i = 1 } ^ { n } = \{ V _ { \beta } | x _ { 0 } \in V _ { \beta }$ , $\beta \in A \}$ . We may choose $z \in \phi ( y )$ such that $\| \phi _ { \epsilon } ( x _ { 0 } ) - z \| < \epsilon$ . Obviously, $\phi _ { \epsilon } ( M ) \subset \mathrm { c o n v } \phi ( M )$ .

What is the homotopy equivalence for compact convex set-valued mappings?

Definition 3.7.4 Let $\Omega$ be a bounded open set in a Banach space $X$ , and let $\phi _ { 1 } , \phi _ { 2 } : \Omega \to \Gamma ( X )$ be two compact convex set-valued mappings. We say that $\phi _ { 1 }$ is homotopically equivalent to $\phi _ { 2 }$ , denoted by $\phi _ { 1 } \simeq \phi _ { 2 }$ , if there exists a family of compact convex set-valued mappings $\Phi : [ 0 , 1 ] \times { \bar { \Omega } }  \Gamma ( X )$ , such that $\theta \not \in F ( [ 0 , 1 ] \times \partial \Omega )$ , and $\Phi ( i , \cdot ) = \phi _ { i } ( \cdot )$ , where $F ( t , x ) = x - \Phi ( t , x )$ .

Lemma 3.7.5 Suppose that $\phi : \Omega \to \Gamma ( X )$ is a compact convex set-valued mapping and $f = i d - \phi$ . If $\theta \not \in f ( \partial \Omega )$ , then $\exists \epsilon _ { 0 } > 0$ such that $\forall \epsilon \in ( 0 , \epsilon _ { 0 } )$ , for any $\epsilon -$ single-valued approximation $\tilde { \phi }$ of $\phi$ , we have $\theta \notin F ( [ 0 , 1 ] \times \partial \Omega )$ , where $F ( t , x ) = x - ( 1 - t ) \bar { \phi } ( x ) - t \phi ( x )$ .

Proof. We prove the lemma by contradiction. If there exist single-valued continuous mappings $\tilde { \phi } _ { n }$ , and $t _ { n } \in [ 0 , 1 ] , x _ { n } \in \partial \Omega , y _ { n } \in \Omega , z _ { n } \in \phi ( y _ { n } )$ such that:

$$
\begin{array}{l} x _ {n} \in (1 - t _ {n}) \tilde {\phi} _ {n} (x _ {n}) + t _ {n} \phi (x _ {n}), \\ \left\| x _ {n} - y _ {n} \right\| <   \frac {1}{n}, \\ \left\| \tilde {\phi} _ {n} \left(x _ {n}\right) - z _ {n} \right\| <   \frac {1}{2 n}. \\ \end{array}
$$

Since $\overline { { \phi ( \overline { { \Omega } } ) } }$ is compact, and $\ddot { \phi } _ { n } ( \Omega ) \subset \overline { { \mathrm { c o n v } } } \phi ( \overline { { \Omega } } )$ , after a subsequence, we have $x _ { n } \to x ^ { * } \in \partial \Omega$ , $t _ { n } \to t ^ { * }$ , and then $y _ { n } \to x ^ { * }$ . Since $\phi$ is u.s.c., for $n$ large, we have $\phi ( x _ { n } ) \subset \phi ( x ^ { * } ) + B _ { \frac { 1 } { n } } ( \theta ) , \phi ( y _ { n } ) \subset \phi ( x ^ { * } ) + B _ { \frac { 1 } { 2 n } } ( \theta )$ . It follows, $\tilde { \phi } _ { n } ( x _ { n } ) \in$ $\phi ( x ^ { * } ) + B _ { \frac { 1 } { n } } ( \theta )$ , then

$$
(1 - t _ {n}) \tilde {\phi} _ {n} (x _ {n}) + t _ {n} \phi (x _ {n}) \subset \phi (x ^ {*}) + B _ {\frac {1}{n}} (\theta),
$$

and then $x _ { n } \in \phi ( x ^ { * } ) + B _ { \frac { 1 } { n } } ( \theta )$ as $n$ large. Thus $x ^ { * } \in \phi ( x ^ { * } )$ . This is a contradiction.

![](images/2aaeb45e89da181e35657cbe8d25469bf5bd12c090161e492b428ad8bc739705.jpg)

Lemma 3.7.6 If $\phi _ { 1 } , \phi _ { 2 }$ are two homotopically equivalent compact convex setvalued mappings, then there exists $\epsilon > 0$ such that for any $\epsilon -$ approximation $\bar { \phi } _ { 1 } , \bar { \phi } _ { 2 }$ of $\phi _ { 1 }$ and $\phi _ { 2 }$ , respectively, we have $\tilde { \phi } _ { 1 } \simeq \tilde { \phi } _ { 2 }$ .

Proof. According to Lemma 3.7.5, we have $\ddot { \phi } _ { 0 } \simeq \phi _ { 0 } \simeq \phi _ { 1 } \simeq \ddot { \phi } _ { 1 }$ in the sense of set-valued mappings. We shall prove that $\tilde { \phi } _ { 0 } \simeq \tilde { \phi } _ { 1 }$ is also true in single-valued sense.

In fact there exists a compact convex set-valued mapping $h : [ 0 , 1 ] \times \Omega \to$ $\Gamma ( X )$ satisfying $h ( i , x ) = \ddot { \phi } _ { i } ( x ) , i = 0 , 1$ and $\epsilon : = \mathrm { d i s t } ( H ( [ 0 , 1 ] \times \partial \Omega ) , \theta ) > 0$ , where $H = { \mathrm { i d } } - h$ . By Lemma 3.7.3, and Lemma 3.7.5, there exists a $\frac { \epsilon } { 2 } -$ approximation $\bar { h } : [ 0 , 1 ] \times \bar { \Omega }  X$ of $h$ such that $h ( \mu , \cdot ) \simeq \ddot { h } ( \mu , \cdot )$ . More precisely, let

$$
p (\lambda , \mu ; x) = (1 - \lambda) h (\mu , x) + \lambda h (\mu , x),
$$

we have $\theta \notin P ( [ 0 , 1 ] \times [ 0 , 1 ] \times \partial \Omega )$ ), where $P ( \lambda , \mu ; x ) = x - p ( \lambda , \mu ; x )$ .

Define a family of single-valued mappings:

$$
g (t, x) = \left\{ \begin{array}{l l} p (1 - 3 t, 0; x) & \text {i f} 0 \leq t \leq \frac {1}{3} \\ p (0, 3 t - 1; x) & \text {i f} \frac {1}{3} \leq t \leq \frac {2}{3} \\ p (3 t - 2, 1; x) & \text {i f} \frac {2}{3} \leq t \leq 1. \end{array} \right.
$$

It is easy to verify that this is the required homotopy.

![](images/9e7fdd0bb727e408c8dd8d80ac69601243d93c92e68ec2a097e4de3e94da3a01.jpg)

Now, we are ready to define the degree of ${ \mathrm { i d } } - \phi$ , where $\phi$ is a compact convex set-valued mapping.

Definition 3.7.7 Let $\Omega$ be an open bounded set in a Banach space $X$ , let $\phi : \bar { \Omega } \to \Gamma ( X )$ be a compact convex-valued mapping, and $f = i d - \phi$ . If $\theta \not \in f ( \partial \Omega )$ , define

$$
\deg \left(f, \Omega , \theta\right) = \lim _ {\epsilon \rightarrow 0} \deg \left(\tilde {f} _ {\epsilon}, \Omega , \theta\right),
$$

where $\ddot { f } _ { \epsilon } = i d - \ddot { \phi } _ { \epsilon }$ and $\ddot { \phi } _ { \epsilon }$ is an $\epsilon -$ approximation of $\phi$

According to Lemmas 3.7.5 and 3.7.6, the degree is well defined.

Readers can easily verify that the degree enjoys the same basic properties as the Leray–Schauder degree: Homotopy invariance, additivity, translation invariance and normality. So do the Kronecker existence and the excision.

The results of Examples 1–3 in Sect. 3.6 can be extended to case where $g$ has jump discontinuity by the use of the degree of set-valued mappings, see K. C. Chang [Ch 1], [Ch 2].

# 3.7.2 Strict Set Contraction Mappings and Condensing Mappings

The motivation of introducing the strict set contraction mapping is to extend the notion of compactness and then to extend the L-S degree.

Let $X$ be a Banach space, and let $A \subset X$ be a bounded subset. $\forall \epsilon > 0$ , we consider a $\epsilon -$ net $N _ { \epsilon }$ of $A$ . Let

$$
\alpha (A) = \inf  \left\{\epsilon > 0 \mid \exists \text {a f i n i t e} \epsilon - \text {n e t} N _ {\epsilon} \text {o f} A \right\}.
$$

By definition, for $\alpha ( A ) = 0$ if and only if $A$ is compact. It is easy to verify the following simple properties of $\alpha$ :

1. If $A \subset B$ , then $\alpha ( A ) \leq \alpha ( B )$   
2. $\alpha ( A \cup B ) \leq \alpha ( A ) + \alpha ( B )$ .   
3. $\alpha ( A ) = \alpha ( A )$   
4. $\alpha ( A + B ) \leq \alpha ( A ) + \alpha ( B ) , \ \alpha ( \lambda A ) = | \lambda | \alpha ( A ) , \ \forall \lambda \in \quad$ $\forall \lambda \in C$   
5. For a sequence of bounded closed nonempty subsets $\cdots \subset A _ { n } \subset \cdots \subset$ $A _ { 2 } \subset A _ { 1 }$ satisfying $\alpha ( A _ { n } )  0$ , we have $A = \cap _ { n = 1 } ^ { \infty } A _ { n } \neq \emptyset$ , and $\alpha ( A ) = 0$ .   
6. (Mazur) $\alpha ( \mathrm { c o n v } ( A ) ) = \alpha ( A )$

Definition 3.7.8 ( $k$ -set contraction map) Let $X$ be a Banach space, $\Omega \subset$ $X$ be a bounded subset. A continuous map $\phi : \Omega \to X$ is called a $k -$ set contraction mapping, $k \geq 0$ , if

$$
\alpha (\phi (A)) \leq k \alpha (A) \forall \text {b o u n d e d} A \subset \Omega .
$$

Remark 3.7.9 We assign $\infty \cdot 0 = 0$ , then continuous mappings can be considered as $\infty - \mathrm { s e t }$ contraction mappings.

Example 1. If $\phi : \Omega \to X$ is compact, the $\phi$ is a $0 -$ set contraction.

Example 2. If $\phi : \Omega \to X$ is a Lipschitz mapping with Lipschitz constant $L$ , then $\phi$ is an $L -$ set contraction.

Definition 3.7.10 (Strict set contraction mapping and condensing mapping) A k set contraction mapping $\phi$ is called strict if $k \in [ 0 , 1 )$ . It is called a condensing mapping if

$$
\alpha (\phi (A)) <   \alpha (A) \forall \text {b o u n d e d} A.
$$

According to (4): If $\phi , \psi : \Omega \to X$ are $k _ { 1 } -$ and $k _ { 2 } -$ set contraction mappings respectively, then $\phi + \psi$ is a $k _ { 1 } + k _ { 2 } -$ set contraction. In particular, if $\phi$ is a contraction mapping (i.e., a Lipschitz mapping with Lipschitz constant $L < 1$ ), and if $\psi$ is compact, then $\phi + \psi$ is a strict set contraction mapping.

By the definition, if $\phi : \Omega \to X$ is a $k _ { 1 } -$ set contraction, and $\psi : \phi ( \Omega ) \to X$ is a $k _ { 2 } -$ set contraction. (In case, $k _ { 1 } = \infty$ , we assume further $\phi$ maps bounded set to bounded set); then $\psi \circ \phi$ is a $k _ { 1 } k _ { 2 } -$ set contraction.

First we are going to define the degree for strict set contraction mappings. We intend to reduce it to compact mappings. Let $f = \mathrm { i d } - \phi$ , where $\phi$ is a $k -$ contraction mapping, where $0 \leq k < 1$ .

If $\theta \not \in f ( \Omega )$ , then we define $\deg \left( f , \Omega , \theta \right) = 0$ .

Otherwise the fixed-point set of $\phi$ is not empty. Define $A _ { 1 } = \overline { { \mathrm { c o n v } } } \phi ( \Omega )$ , and $A _ { k + 1 } = { \overline { { \operatorname { c o n v } } } } \phi ( \Omega \cap A _ { k } ) , k = 1 , 2 , \dots .$ . Since the fixed point set of $\phi$ is not empty, $\begin{array} { r } { A : = \bigcap _ { n = 1 } ^ { \infty } A _ { n } \neq \emptyset } \end{array}$ . However, we have

$$
\begin{array}{l} \alpha \left(A _ {n + 1}\right) = \alpha (\overline {{\operatorname {c o n v}}} \phi \left(A _ {n} \cap \bar {\Omega}\right)) \\ \leq k \alpha \left(A _ {n}\right) \\ \leq \dots \\ \leq k ^ {n} \alpha (A _ {1}) \rightarrow 0. \\ \end{array}
$$

Therefore $\alpha ( A ) = 0$ , $A$ is a compact convex set, and then $\phi : \Omega \cap A \to A$ .

According to the Dugundji extension theorem, there exists a continuous $\tilde { \phi } : \bar { \Omega }  A$ such that $\tilde { \phi } | _ { \bar { \Omega } \cap A } = \phi | _ { \bar { \Omega } \cap A }$ . Then $\tilde { \phi }$ is compact and shares the same fixed point set with $\phi$ .

We claim that $\theta \not \in ( \mathrm { i d } - \phi ) ( \partial \Omega ) \Rightarrow \theta \not \in ( \mathrm { i d } - \bar { \phi } ) ( \partial \Omega )$ . In fact, if $x \in \partial \Omega$ satisfies $x = \tilde { \phi } ( x )$ , then $x \in \partial \Omega \cap A$ , so is $\phi ( x ) = \phi ( x )$ and then $\theta \in ( \mathrm { i d } -$ $\phi ) ( \partial \Omega )$ .

With the above notations, we define the degree for strict set contraction mappings.

Definition 3.7.11 Let $\phi$ be a strict set contraction mapping; we define

$$
\deg \left(\operatorname {i d} - \phi , \Omega , \theta\right) = \deg \left(\operatorname {i d} - \tilde {\phi}, \Omega , \theta\right).
$$

To verify that the degree is well defined, we shall prove that the definition does not depend on the special choice of $\tilde { \phi }$ , i.e., if $\bar { \phi } _ { 1 } , \bar { \phi } _ { 2 }$ are two such extensions: $\tilde { \phi } _ { i } : \bar { \Omega }  A , \tilde { \phi } _ { i } | _ { \bar { \Omega } \cap A } = \phi | _ { \bar { \Omega } \cap A } , i = 1 , 2$ , then we define $F ( t , x ) = x - [ ( 1 - t ) \phi _ { 1 } ( x ) + t \phi _ { 2 } ( x ) ]$ . From $\theta \not \in ( \mathrm { i d } - \phi ) ( \partial \Omega )$ , it follows that

$\theta \not \in ( \mathrm { i d } - F ) ( [ 0 , 1 ] \times \partial \Omega )$ , i.e., $\ddot { \phi } _ { 1 } \simeq \ddot { \phi } _ { 2 }$ . According to the homotopy invariance of the Leray–Schauder degree, we have $\mathrm { d e g } \left( \mathrm { i d } - \tilde { \phi } _ { 1 } , \Omega , \theta \right) = \mathrm { d e g } \left( \mathrm { i d } - \tilde { \phi } _ { 2 } , \Omega , \theta \right)$ .

It is easy to verify that the degree enjoys the homotopy invariance, additivity, translation invariance and normality. They are left to readers as exercises.

Accordingly, this enables us to apply the degree theory to a map, which can be decomposed into a summation of a contraction mapping and a compact map.

Finally, we extend the degree to condensing mappings. If $\phi$ is a condensing mapping, $\phi ( \Omega )$ is bounded, say it is included in the ball centered at $\theta$ with radius $R > 0$ . For any $\epsilon > 0$ , setting $\lambda \in ( 1 - \frac { \epsilon } { R } , 1 )$ , and $\phi _ { \lambda } = \lambda \phi$ , we have $\| \phi ( x ) - \phi _ { \lambda } ( x ) \| \le \epsilon$ , and

$$
\alpha \left(\phi_ {\lambda} (A)\right) \leq \lambda \alpha (\phi (A)) \leq \lambda \alpha (A) \forall \text {b o u n d e d} A,
$$

i.e., $\phi _ { \lambda }$ is a strict set contraction mapping which is closed to $\phi$ . We define

$$
\deg \left(\mathrm {i d} - \phi , \Omega , \theta\right) = \deg \left(\mathrm {i d} - \phi_ {\lambda}, \Omega , \theta\right).
$$

Again, it is easy to verify that the degree is also well defined and enjoys all basic properties of the Leray–Schauder degree. Again, this are left to readers.

# 3.7.3 Fredholm Mappings

We know that the Leray–Schauder degree can be applied to quasilinear elliptic equations: Find $u \in C ^ { 2 , \gamma } \cap C _ { 0 } ( \Omega ) , \gamma \in ( 0 , 1 )$ satisfying

$$
\sum_ {i, j = 1} ^ {n} a _ {i j} (x, u, \nabla u) \frac {\partial^ {2} u}{\partial x _ {i} \partial x _ {j}} + f (x, u, \nabla u) = 0, \text {i n} \Omega . \tag {3.60}
$$

It is executed as follows. Define a mapping $K : C _ { 0 } ^ { 1 } ( \Omega ) \to C _ { 0 } ^ { 1 } ( \Omega )$ such that $\forall u \in C _ { 0 } ^ { 1 } ( \Omega )$ , $v = K u \in C ^ { 2 , \gamma } \cap C _ { 0 } ( \Omega )$ satisfies the following linear equation:

$$
\sum_ {i, j} ^ {n} a _ {i j} (x, u, \nabla u) \frac {\partial^ {2} v}{\partial x _ {i} \partial x _ {j}} + f (x, u, \nabla u) = 0 \mathrm {i n} \Omega .
$$

Thus the fixed points of $K$ are the solutions of equation (3.60).

However, if we consider a general elliptic equation:

$$
A (u) := A (x, u, \nabla u, \nabla^ {2} u) = g (x) \text {i n} \Omega , \tag {3.61}
$$

where $g$ is a given function and the quadratic form is positive definite:

$$
\sum_ {i, j = 1} ^ {n} \frac {\partial A}{\partial u _ {x _ {i} x _ {j}}} \xi_ {i} \xi_ {j} \geq \alpha | \xi | ^ {2}, \alpha > 0,
$$

then it seems that the Leray–Schauder degree argument is not applicable, because we do not know how to recast it as a fixed-point problem for compact

mappings. However, the linearization of $A ( u )$ is a linear second-order elliptic operator, so is a Fredholm operator with index zero. People have made great efforts in defining an integer-valued degree theory for $C ^ { 1 }$ Fredholm mappings of index 0 between Banach manifolds. Among them we should mention Caccioppoli [Cac 2], Smale [Sm 3](for $\mathbf { Z } _ { 2 }$ valued), Elworthy and Tromba [ElT 1],[ElT 2], Borisovich, Zvyagin, and Sapronov [BZS] (for $C ^ { 2 }$ mappings and $\mathbf { Z }$ valued), and Fitzpatrick, Pejsachowicz, and Rabier [FP],[FPR] and [PR 1] [PR 2] etc. We are satisfied to introduce the idea of the definition; details are to be found in the above-mentioned references.

The main difficulty in extending the Leray–Schauder degree theory to Fredholm mappings lies in the fact discovered by Kuiper [Kui] that the general linear group of infinite-dimensional separable Hilbert space is connected and even contractible. A new ingredient has to be introduced to define the orientation of Fredholm mappings. Following [FPR], one defines the parity of curves of Fredholm operators. Let us denote by $\kappa ( X )$ the space of all compact linear operators, and by $\Phi _ { 0 } ( X , Y ) , ( G L ( X , Y ) )$ the space of all Fredholm operators of index 0 (and isomorphisms resp.) between Banach spaces $X$ and $Y$ .

For a curve of linear compact operators $K \in C ( [ 0 , 1 ] , \mathcal { K } ( X ) )$ , if $I - K ( i ) , i =$ $0 , 1$ are invertible, then we define the parity by

$$
\sigma (I - K) = i _ {L S} (I - K (0), \theta) i _ {L S} (I - K (1), \theta),
$$

where $i _ { L S } ( I - K ( i ) , \theta ) , i = 0 , 1$ , are the Leray–Schauder indices.

The notion of parity is extended to curves of Fredholm mappings with index $\gimel \ \forall A \ \in \ C ( [ 0 , 1 ] , \Phi _ { 0 } ( X , Y ) )$ , if $A ( i ) \ \in \ G L ( X , Y )$ , then there exists $N \in C ( [ 0 , 1 ] , G L ( Y , X ) )$ such that $N ( t ) A ( t ) = I - K ( t ) , \forall t \in [ 0 , 1 ]$ ], where $K \in C ( [ 0 , 1 ] , \mathcal { K } ( X ) )$ . If $A ( i ) \in G L ( X , Y ) , i = 0 , 1$ , then we define

$$
\sigma (A) = \sigma (I - K).
$$

One can show that $\sigma ( A )$ does not depend on the special choice of $N$ .

In particular, if $A \in C ( [ 0 , 1 ] , G L ( X , Y ) )$ , then we take $N ( t ) = A ^ { - 1 } ( t )$ , and then $\sigma ( A ) = 1$ .

If $X = Y$ is a finite-dimensional Banach space, by definition, $\sigma ( A ) = \pm 1$ if and only if $A ( 0 )$ and $A ( 1 )$ lie in the same/different connected component(s) in $G L ( X )$ . But this is not true for infinite-dimensional space due to the above-mentioned Kuiper’s theorem. A geometric interpretation is given in [FP]: Let S = L Φ(X, Y ) dimkerL = j , j = 1, 2, . . ., and $S = \Phi _ { 0 } ( X , Y ) \backslash G L ( X , Y )$ , then $\textstyle S = \bigcup _ { j = 1 } ^ { \infty } S _ { j }$ , and $\sigma ( A )$ is the number of points of transversal intersection of a generic path $A$ with $S _ { 1 }$ .

The parity of the curve $A \in C ( [ 0 , 1 ] , \Phi _ { 0 } )$ enjoys the following homotopy invariance and the invariance under reparametrizations:

(Homotopy invariance) Let $H \ \in \ C ( [ 0 , 1 ] ^ { 2 } , \Phi _ { 0 } ( X , Y ) )$ and suppose that $H ( t , i ) \in G L ( X , Y ) , i = 0 , 1 , \forall t \in [ 0 , 1 ]$ . Then $\sigma ( H ( t , \cdot ) )$ is a constant.

(Invariance under reparametrizations) Let $A \in C ( [ 0 , 1 ] , \Phi _ { 0 } ( X , Y ) )$ with $A ( i ) \in G L ( X , Y ) , i = 0 , 1$ , and let $\gamma \in C ( [ 0 , 1 ] , [ 0 , 1 ] )$ satisfy $\gamma ( i ) = i , i = 0 , 1$ . Then $\sigma ( A \circ \gamma ) = \sigma ( A )$ .

Let $\Omega \subset U \subset X$ be open subsets, and $U$ be connected and simply connected. In the following the closure $\Omega$ and the boundary $\partial \Omega$ are understood relative to $U$ .

Definition 3.7.12 A Fredholm mapping $F \in C ^ { 1 } ( U , Y )$ of index 0 is called $\Omega$ -admissible if $F | _ { \bar { \Omega } }$ is proper. A Fredholm homotopy $H \in C ^ { 1 } ( [ 0 , 1 ] \times U , Y )$ of index 1 is called $\Omega$ -admissible if $H | _ { [ 0 , 1 ] \times \bar { \Omega } }$ is proper.

Let $p \in U$ be a regular point of a $C ^ { 1 }$ $\Omega$ -admissible Fredholm mapping $F$ ; we take it as a base point. Let $y \in Y \backslash F ( \partial \Omega )$ be a regular value of $F | _ { \Omega }$ , then $F ^ { - 1 } ( y ) \cap \Omega$ is a finite set (may be empty): $\{ x _ { 1 } , x _ { 2 } , \ldots , x _ { k } \}$ . If it is not empty, let $\gamma _ { i } \in C ( [ 0 , 1 ] , U )$ , with $\gamma _ { i } ( 0 ) = p , \gamma _ { i } ( 1 ) = x _ { i }$ , be curves connecting $p$ and $x _ { i } , i = 1 , 2 , \ldots , k$ . Then the parities $\sigma _ { i } = \sigma ( F ^ { \prime } \circ \gamma _ { i } ) , i = 1 , 2 , \dotsc , k$ , are all well defined, and are independent of $\gamma _ { i }$ . Thus for a regular value $y$ , we define the degree of $F$ with the base point $p$ by

$$
\deg_ {p} \left(F, \Omega , y\right) = \sum_ {i = 1} ^ {k} \sigma_ {i}.
$$

If $F ^ { - 1 } ( y ) \cap \Omega = \emptyset$ , then we define $\mathrm { d e g } _ { p } \left( F , \Omega , y \right) = 0$ .

Quinn and Sard [QS] avoided the requirement of the separability of the spaces $X$ and $Y$ , and obtained an improved version of Sard–Smale Theorem, by which one shows that for any $y \in Y \backslash F ( \partial \Omega )$ there exists $\epsilon > 0$ such that $B _ { \epsilon } ( y ) \subset Y \backslash F ( \partial \Omega )$ contains a regular value $z$ of $F$ . In combining with an approximation theorem for $C ^ { 1 }$ Fredholm mappings of any index in [PR 2], we can define

Definition 3.7.13 Assume that $F \in C ^ { 1 } ( U , Y )$ is $\Omega$ -admissible, $p \in U$ is a base point of $F$ , and $y \in Y \backslash F ( \partial \Omega )$ . We define the degree $\deg _ { p } { ( F , \Omega , y ) } =$ $\deg _ { p } \left( F , \Omega , z \right)$ , where $z \in B _ { \epsilon } ( y ) \subset Y \backslash F ( \partial \Omega )$ is a regular value of $F | _ { \Omega }$ .

Again, the degree is independent of the choice of $z$

For different base points $p _ { 0 } , p _ { 1 }$ , one has

$$
\deg_ {p _ {0}} (F, \Omega , y) = \sigma \left(F ^ {\prime} \circ \gamma\right) \deg_ {p _ {1}} (F, \Omega , y),
$$

where $\gamma \in C ( [ 0 , 1 ] , U ) , \gamma ( i ) = i , i = 0 , 1$ .

The following fundamental properties hold:

(Homotopy invariance) Let $H$ be an $\Omega$ -admissible homotopy. Suppose that $y \in Y \backslash ( [ 0 , 1 ] \times \partial \Omega )$ and that $p$ is a base point of $H ( t , \cdot ) \forall t \in [ 0 , 1 ]$ . Then

$$
\deg_ {p} \left(H (1, \cdot), \Omega , y\right) = \deg_ {p} \left(H (0, \cdot), \Omega , y\right).
$$

(Additivity) Suppose the $\Omega = \Omega _ { 1 } \cup \Omega _ { 2 }$ , where $\Omega _ { i } , i = 1 , 2$ are disjoint open subsets of $U$ , and that $F \in C ^ { 1 } ( U , Y )$ is $\Omega$ -admissible, $p \in U$ is a base point

of $F$ . Then $F$ is $\Omega _ { i }$ -admissible for $i = 1 , 2$ . Moreover, if $y \in Y \backslash F ( \partial \Omega )$ , then $y \in Y \backslash F ( \partial \Omega _ { i } ) , i = 1 , 2$ , and

$$
\deg_ {p} (F, \Omega , y) = \deg_ {p} (F, \Omega_ {1}, y) + \deg_ {p} (F, \Omega_ {2}, y).
$$

(Normality) Let $\Omega$ be an open subset of $X$ , $\forall p \in X$ , and $y \notin \partial \Omega$ . Then $\mathrm { d e g } _ { p } \left( \mathrm { i d } , \Omega , y \right) = 1$ if $y \in \Omega , = 0$ if $y \not \in \Omega$ .

Obviously, the excision property and the Kronecker existence theorem hold as well.

The generalized degree has been used to extend Rabinowitz global bifurcation theorem [PR1], and is applied to the study of bifurcation problems for semi-linear elliptic equations on $R ^ { n }$ (see [JLS]).

# Minimization Methods

The calculus of variations studies the optimal shape, time, velocity, energy, volume or gain etc. under certain conditions. Laws in astronomy, mechanics, physics, all natural sciences and engineering technologies, as well as in economic behavior obey variational principles. The main object of the calculus of variations is to find out the solutions governed by these principles. Tracing back to Fermat, who postulated that light follows a path of least possible time, this is a subject in finding the minimizers of a given functional. Starting from the brothers Johann and Jakob Bernoulli and L. Euler, the calculus of variations has a long history, and renews itself according to the developments of mathematics and other sciences.

The problem is formulated as follows: Assume that $f : \mathbb { R } ^ { n } \times \mathbb { R } ^ { N } \times \mathbb { R } ^ { n N } \longrightarrow$ $\mathbb { R } ^ { 1 }$ is a continuous function, and that $E$ is a set of $N .$ -vector functions. Let $J$ be a functional defined on $E$ :

$$
J (u) = \int f (x, u (x), \nabla u (x)) d x.
$$

Find $u _ { 0 } \in E$ , such that

$$
J (u _ {0}) = \operatorname {M i n} \left\{J (u) \mid u \in E \right\}.
$$

The central problems in the calculus of variations are the existence and the regularity of the minimizers. These are the 19th and the 20th problems among the 23 problems posed by Hilbert in his famous lecture delivered at International Congress of Mathematicians in 1900.

This chapter is devoted to an introduction of the minimization method. We pay attention only to the existence of minimizers, but not to the regularity, although the latter is a very important and rich part of the theory of the calculus of variations. The direct method, studied in Sect. 4.2, is the core of the minimization method, in which w∗-compactness and w∗ lower semicontinuity (w∗l.s.c.) play crucial roles.

A necessary and sufficient condition on the integrand $f$ for the w∗l.s.c. of the functional $J$ on the Sobolev space $W ^ { 1 , p }$ , $p \in ( 1 , \infty ]$ is studied in Sect. 4.3.

In the case when w∗l.s.c. fails, either the minimizing sequence does not converge or it does not converge to a minimizer. The Young measure and the relaxation functional are introduced in Sect. 4.4.

In the spaces $W ^ { 1 , 1 }$ and $L ^ { 1 }$ the closed balls are no longer w∗ compact. Instead, we consider the $B V$ space and the Hardy space, respectively. They are studied in Sect. 4.5.

Two interesting applications are given in Sect. 4.6. One is on the phase transitions and the other is the segmentation in the image processing.

The concentration phenomenon, which happens in many problems from geometry to physics, concerns the lack of compactness. We give a brief introduction to the method of managing this phenomenon in Sect. 4.7.

The minimax method dealing with saddle points is briefly introduced in Sect. 4.8. With the aid of the Ekeland variational principle and the Palais– Smale condition, it is studied in the spirit of the minimization method.

Section 4.1 is an introduction, where various variational principles and their reductions are introduced.

# 4.1 Variational Principles

Let $X$ be a real Banach space, and $U \subset X$ be an open set. A point $x _ { 0 } \in U$ is called a local maximum (or minimum) point of $f : U \to \mathbb { R } ^ { 1 }$ , if

$$
f (x) \leqslant f (x _ {0}) \quad (\text {o r} f (x) \geqslant f (x _ {0})) \quad \forall x \in B _ {\varepsilon} (x _ {0}) \subset U,
$$

for some $\varepsilon > 0$ .

If further, $f$ is $G$ -differentiable at $x _ { 0 }$ , then

$$
d f (x _ {0}, h) = \frac {d}{d t} f (x _ {0} + t h) \big | _ {t = 0} = \theta \forall h \in X,
$$

or simply

$$
d f \left(x _ {0}\right) = \theta . \tag {4.1}
$$

Moreover, if $f$ has second-order $G$ -derivatives at $x _ { 0 }$ , then

$$
d ^ {2} f (x _ {0}) (h, h) \leqslant 0 \quad (\text {o r} \geqslant 0) \quad \forall h \in X.
$$

In particular, if $X$ is a Hilbert space, and $f \in C ^ { 2 }$ then $d ^ { 2 } f ( x _ { 0 } )$ is a selfadjoint operator. We conclude that $d ^ { 2 } f ( x _ { 0 } )$ is nonnegative (nonpositive) if $x _ { 0 }$ is a local minimum (or maximum) point. Conversely, $x _ { 0 }$ is a local minimum (or maximum) point if $d ^ { 2 } f ( x _ { 0 } )$ is positive (or negative) definite.

# 4.1.1 Constraint Problems

Let $X$ , $Y$ be real Banach spaces, $U \subset X$ be an open set. Suppose that $f$ : $U \to \mathbb { R } ^ { 1 } , g : U \to Y$ are $C ^ { 1 }$ mappings. Let

$$
M = \left\{x \in U \mid g (x) = \theta \right\}.
$$

Find the necessary condition for

$$
\min  _ {x \in M} f (x). \tag {4.2}
$$

Theorem 4.1.1 (Ljusternik) Suppose that $x _ { 0 } ~ \in ~ M$ solves (4.2), and that $\operatorname { I m } g ^ { \prime } ( x _ { 0 } )$ is closed. Then $\exists ( \lambda , y ^ { * } ) \in \mathbb { R } ^ { 1 } \times Y ^ { * }$ such that $( \lambda , y ^ { * } ) \neq ( 0 , \theta )$ , and

$$
\lambda f ^ {\prime} \left(x _ {0}\right) + g ^ {\prime} \left(x _ {0}\right) ^ {*} y ^ {*} = \theta . \tag {4.3}
$$

Furthermore, if Im $g ^ { \prime } ( x _ { 0 } ) = Y$ , then $\lambda \neq 0$ .

Proof. In the case where $Y _ { 1 } = \operatorname { I m } g ^ { \prime } ( x _ { 0 } ) \subsetneq Y$ , the conclusion (4.3) is trivial; one may choose $\lambda = 0$ and $y ^ { * } \in Y _ { 1 } ^ { \bot } : = \left\{ z ^ { * } \in Y ^ { * } \mid \langle z ^ { * } , z \rangle = 0 \forall z \in Y _ { 1 } \right\}$ .

We assume $Y _ { 1 } = Y$ . The tangent space $T _ { x _ { 0 } } ( M )$ of $M$ at $x _ { 0 }$ is as follows:

$$
\begin{array}{l} T _ {x _ {0}} (M) \\ = \left\{h \in X \mid \exists \varepsilon > 0, \exists v \in C ^ {1} ((- \varepsilon , \varepsilon), X), x _ {0} + v (t) \in M, v (0) = \theta , \dot {v} (0) = h \right\}. \\ \end{array}
$$

We want to prove that $T _ { x _ { 0 } } ( M ) = \ker g ^ { \prime } ( x _ { 0 } )$ . In order to avoid technical complication, we make an additional assumption: Either $g ^ { \prime } ( x _ { 0 } )$ is a Fredholm operator, or $X$ is a Hilbert space. The assumption is superfluous, because a modified IFT has been studied in [De] (pp. 334) to improve the proof. Indeed, from

$$
g (x _ {0} + v (t)) = \theta ,
$$

it follows that

$$
g ^ {\prime} (x _ {0}) h = \frac {d}{d t} \left. g (x _ {0} + v (t)) \right| _ {t = 0} = 0 \forall h \in T _ {x _ {0}} (M),
$$

i.e., $T _ { x _ { 0 } } ( M ) \subset \ker g ^ { \prime } ( x _ { 0 } )$ . On the other hand, if $h \in \ker g ^ { \prime } ( x _ { 0 } )$ , one solves the equation:

$$
g (x _ {0} + t h + w (t)) = \theta ,
$$

for $w \in C ^ { 1 } ( ( - \varepsilon , \varepsilon ) , X _ { 1 } ) , w ( 0 ) = \theta$ , where $X _ { 1 }$ is the complement of $\ker g ^ { \prime } ( x _ { 0 } )$ and $\varepsilon > 0$ is small. Since $g ( x _ { 0 } ) = \theta$ , $g ^ { \prime } ( x _ { 0 } ) : X _ { 1 }  Y$ is an isomorphism, one may apply the IFT to obtain such a solution $w$ . Setting $v ( t ) = t h + w ( t )$ , we have $v ( 0 ) = \theta$ and $\dot { v } ( 0 ) = h + \dot { w } ( 0 )$ .

From

$$
g ^ {\prime} (x _ {0}) (h + \dot {w} (0)) = \theta ,
$$

it follows that $\dot { w } ( 0 ) \in \ker g ^ { \prime } ( x _ { 0 } )$ , but $\dot { w } ( 0 ) \in X _ { 1 }$ which implies that $\dot { w } ( 0 ) = \theta$ , i.e., $\dot { v } ( 0 ) = h$ .

Now, $\forall h \in T _ { x _ { 0 } } ( M )$ , $\exists v ( t )$ satisfying $v ( 0 ) = \theta , \dot { v } = h$ and $x _ { 0 } + v ( t ) \in M$ , so that

$$
f (x _ {0} + v (t)) \geqslant f (x _ {0}),
$$

which implies that

$$
\langle f ^ {\prime} (x _ {0}), h \rangle = 0,
$$

i.e. $f ^ { \prime } ( x _ { 0 } ) \in \ker g ^ { \prime } ( x _ { 0 } ) ^ { \perp }$ . By the closed range theorem, $\beth \ b y ^ { * } \in \ b { Y } ^ { * }$ such that

$$
- f ^ {\prime} (x _ {0}) = g ^ {\prime} (x _ {0}) ^ {*} y ^ {*}.
$$

This completes the proof.

Corollary 4.1.2 Suppose that $g _ { 1 } , \dots , g _ { m } : U \to \mathbb { R } ^ { 1 }$ are $C ^ { 1 }$ functions, and that $x _ { 0 }$ solves (4.2) with

$$
M = \{x \in U \mid g _ {i} (x) = 0, i = 1, 2, \dots , m \}.
$$

If $\{ g _ { i } ^ { \prime } ( x _ { 0 } ) \} _ { 1 } ^ { m }$ is linearly independent, then $\exists \lambda _ { 1 } , \ldots , \lambda _ { m }$ such that

$$
f ^ {\prime} (x _ {0}) + \sum_ {i = 1} ^ {m} \lambda_ {i} g _ {i} ^ {\prime} (x _ {0}) = 0.
$$

Moreover, we may also consider inequality constraints: Given $f , g _ { 1 } , \ldots , g _ { m }$ , $h _ { 1 } , \ldots , h _ { l } \in C ^ { 1 } ( X , R ^ { 1 } )$ find

$$
\min  \left\{f (x) | g _ {i} (x) = 0, i = 1, \dots , m; h _ {j} (x) \leq 0, j = 1, \dots , l \right\}.
$$

In the same manner, we find the necessary condition of an extremum point $x _ { 0 }$ :

$$
\exists \lambda_ {1}, \dots , \lambda_ {m} \in \mathbb {R} ^ {1},
$$

$$
\exists \mu_ {0}, \mu_ {1}, \dots , \mu_ {l} \geq 0, \text {b u t n o t a l l z e r o ,}
$$

such that

$$
\mu_ {0} f ^ {\prime} \left(x _ {0}\right) + \sum_ {i = 1} ^ {m} \lambda_ {i} g _ {i} ^ {\prime} \left(x _ {0}\right) + \sum_ {j = 1} ^ {l} \mu_ {j} h _ {j} ^ {\prime} \left(x _ {0}\right) = \theta , \text {a n d} \mu_ {j} h _ {j} \left(x _ {0}\right) = 0, \forall j = 1, \dots , l.
$$

This is called the Kuhn–Tucker condition in the mathematical programming.

In particular, if all the functions $h _ { j } , j = 1 , \ldots , l$ are convex, which implies that the set $C = \{ x \in X | h _ { j } ( x ) \leq 0 , j = 1 , \ldots , l \}$ is convex, then a necessary condition for an extremum $x _ { 0 } \in C$ is the following variational inequality:

$$
\left\langle f ^ {\prime} \left(x _ {0}\right) + \sum_ {1} ^ {m} \lambda_ {i} g _ {i} ^ {\prime} \left(x _ {0}\right), y - x _ {0} \right\rangle \geq 0, \forall y \in C.
$$

Remark 4.1.3 $\lambda _ { 1 } , \ldots , \lambda _ { m } ; \mu _ { 0 } , \mu _ { 1 } , \ldots , \mu _ { l }$ , are called the Lagrangian multipliers.

# 4.1.2 Euler–Lagrange Equation

In case $f$ is a functional of the following form:

$$
f (u) = \int_ {\Omega} \varphi (x, u (x), \nabla u (x)) d x \quad \Omega \subset \mathbb {R} ^ {n}, \tag {4.4}
$$

where $\varphi : \overline { { \Omega } } \times \mathbb { R } ^ { N } \times \mathbb { R } ^ { n N } \to \mathbb { R } ^ { 1 }$ is a $C ^ { 2 }$ function, let us turn to the 1st and 2nd variations of the functional (4.4). $\forall u \in C ^ { 1 } ( \Omega )$

$$
\begin{array}{l} \langle f ^ {\prime} (u), v \rangle \\ = \int_ {\Omega} \left\{\sum_ {i = 1} ^ {N} \sum_ {\alpha = 1} ^ {n} \frac {\partial \varphi (x , u (x) , \nabla u (x))}{\partial \xi_ {\alpha} ^ {i}} \frac {\partial v _ {i}}{\partial x _ {\alpha}} + \sum_ {i = 1} ^ {N} \frac {\partial \varphi (x , u (x) , \nabla u (x))}{\partial u _ {i}} v _ {i} \right\}, \\ \end{array}
$$

and

$$
\begin{array}{l} \langle f ^ {\prime \prime} (u), v \otimes v \rangle = \int_ {\Omega} \left\{\sum_ {i, k = 1} ^ {N} \sum_ {\alpha , \beta = 1} ^ {n} \frac {\partial^ {2} \varphi (x , u (x) , \nabla u (x))}{\partial \xi_ {\alpha} ^ {i} \partial \xi_ {\beta} ^ {k}} \frac {\partial v _ {i}}{\partial x _ {\alpha}} \frac {\partial v _ {k}}{\partial x _ {\beta}} \right. \\ \left. + 2 \sum_ {i, k = 1} ^ {N} \sum_ {\alpha = 1} ^ {n} \frac {\partial^ {2} \varphi}{\partial \xi_ {\alpha} ^ {i} \partial u _ {k}} \frac {\partial v _ {i}}{\partial x _ {\alpha}} \cdot v _ {k} + \sum_ {i, k = 1} ^ {N} \frac {\partial^ {2} \varphi}{\partial u _ {i} \partial u _ {k}} v _ {i} v _ {k} \right\}, \\ \end{array}
$$

$\forall v \in C ^ { 1 } ( \overline { { \Omega } } , \mathbb { R } ^ { N } )$

Thus the Euler–Lagrange equation under certain boundary conditions reads as:

$$
- \sum_ {\alpha = 1} ^ {n} \frac {\partial}{\partial x _ {\alpha}} \left(\frac {\partial \varphi (x , u (x) , \nabla u (x))}{\partial \xi_ {\alpha} ^ {i}}\right) + \frac {\partial \varphi (x , u (x) , \nabla u (x))}{\partial u _ {i}} = 0, i = 1, 2, \dots , N.
$$

It is a second-order differential system.

If the functional $f$ is only defined on a closed convex subset $C$ of the function space $C ^ { 1 } ( \Omega )$ , and $u$ is a minimizer in $C$ , then we only have a variational inequality:

$$
\begin{array}{l} \int_ {\Omega} \left\{\sum_ {i = 1} ^ {N} \sum_ {\alpha = 1} ^ {n} \frac {\partial \varphi (x , u (x) , \nabla u (x))}{\partial \xi_ {\alpha} ^ {i}} \frac {\partial (v _ {i} - u _ {i})}{\partial x _ {\alpha}} + \frac {\partial \varphi (x , u (x) , \nabla u (x))}{\partial u _ {i}} (v _ {i} - u _ {i}) \right\} \\ \geq 0, \\ \end{array}
$$

for all $v \in C$

In case $N = 1$ , the Euler–Lagrange equation reads as

$$
- \operatorname {d i v} \varphi_ {\xi} (x, u (x), \nabla u (x)) + \varphi_ {u} (x, u (x), \nabla u (x)) = 0.
$$

This is a differential equation. We give here a few examples.

1. (Geodesics) Let $U$ be an open set of $R ^ { n }$ , and let $g _ { i j } : U  R ^ { 1 } , \forall i , j =$ $1 , 2 , \ldots , n$ be symmetric and positive definite. For $u \ = \ ( u ^ { 1 } , \ldots , u ^ { n } ) \ \in$ $C ^ { 1 } ( [ 0 , 1 ] , U )$ , one defines

$$
\varphi (u, \xi) = \sum_ {i, j = 1} ^ {n} g _ {i j} (u) \xi_ {i} \xi_ {j}, \forall \xi = (\xi_ {1}, \dots , \xi_ {n}) \in R ^ {n}.
$$

Then the Euler–Lagrange equation is the following differential system:

$$
\frac {d ^ {2} u ^ {i}}{d t ^ {2}} + \Gamma_ {j, k} ^ {i} \frac {d u ^ {j}}{d t} \frac {d u ^ {k}}{d t} = 0, \forall i = 1, 2, \dots , n, t \in [ 0, 1 ],
$$

where

$$
\Gamma_ {j, k} ^ {i} = \frac {1}{2} [ \partial_ {j} g _ {l k} + \partial_ {k} g _ {l j} - \partial_ {l} g _ {j k} ] g ^ {l i},
$$

and ( $g ^ { l i }$ ) is the inverse matrix of $\left( g _ { i l } \right)$ , i.e., $g _ { i j } g ^ { i k } = \delta _ { j } ^ { k }$

2. (Poisson equation) Let $\begin{array} { r } { \varphi ( x , u , \xi ) \ = \ c ( x ) u + \frac { 1 } { 2 } | \xi | ^ { 2 } } \end{array}$ . The Euler–Lagrange equation is the Poisson equation:

$$
\triangle u = c (x) \quad \text {i n} \Omega .
$$

3. (Hamiltonian systems)

For $H \in C ^ { 1 } ( \mathbb { R } ^ { 1 } \times \mathbb { R } ^ { n } \times \mathbb { R } ^ { n } , \mathbb { R } ^ { 1 } )$ , the following ODE system

$$
\left\{ \begin{array}{l} \dot {x} = - H _ {p} (t, x, p), \\ \dot {p} = H _ {x} (t, x, p), \end{array} \right.
$$

with $( t , x , p ) \in \mathbb { R } ^ { 1 } \times \mathbb { R } ^ { n } \times \mathbb { R } ^ { n }$ , is called a Hamiltonian system, and $H$ is called a Hamiltonian function. Sometimes we use the notations:

$$
z = (x, p),
$$

and

$$
J = \left( \begin{array}{c c} 0 & - I \\ I & 0 \end{array} \right)  ,
$$

where $I$ is the $n \times n$ unit matrix. The above system then has a simple form:

$$
\dot {z} = J \operatorname {g r a d} H (t, z).
$$

$J$ grad sometimes is also called a symplectic gradient. The $2 \pi$ -periodic solution can be seen as a critical point of the following functional:

$$
f (z) = \int_ {0} ^ {2 \pi} \frac {1}{2} \langle z, J \dot {z} \rangle_ {\mathbb {R} ^ {2 n}} + H (t, z)
$$

on the space $C ^ { 1 } ( [ 0 , 2 \pi ] , \mathbb { R } ^ { 2 n } )$ . In other words, the Euler–Lagrange equation of this functional is the Hamiltonian system.

4. (Minimal surface) Let $u \in C ^ { 1 } ( \overline { { \Omega } } , \mathbb { R } ^ { 1 } )$ , and $\varphi ( \xi ) = [ 1 + | \xi | ^ { 2 } ] ^ { \frac { 1 } { 2 } }$ . Then the Euler–Lagrange equation reads as

$$
\mathrm {d i v} \left\{\frac {\nabla u}{[ 1 + | \nabla u | ^ {2} ] ^ {\frac {1}{2}}} \right\} = 0.
$$

5. (Obstacle problem revisit) Let $\Omega ~ \subset ~ \mathbb { R } ^ { n }$ be a bounded domain, $g \in$ $L ^ { 2 } ( \Omega ) , \psi \in H ^ { 1 } ( \Omega )$ . Find a minimizer of the problem: $\mathrm { M i n } \{ f ( u ) | u \in E \}$ , where

$$
f (u) = \int_ {\Omega} \left(\frac {1}{2} | \nabla u | ^ {2} - g \cdot u\right),
$$

and

$$
E = \left\{u \in H _ {0} ^ {1} (\Omega) \mid u (x) \leq \psi (x) \text {a . e .} x \in \Omega \right\}.
$$

The variational inequality reads as

$$
\int_ {\Omega} \nabla u \cdot \nabla (v - u) \geq \int_ {\Omega} g (v - u), \forall v \in E.
$$

Now we derive the necessary condition on $\varphi$ for a minimizer of $f$ under Dirichlet boundary conditions. If $u$ is a minimizer, then by Taylor expansion,

$$
\langle f ^ {\prime \prime} (u), v \otimes v \rangle \geqslant 0, \quad \forall v \in W _ {0} ^ {1, \infty} (\Omega , \mathbb {R} ^ {N}). \tag {4.5}
$$

Let $\rho ( t ) = 1 + t$ for $t \in [ - 1 , 0 ]$ , and $\rho ( t ) = 1 - t$ for $t \in [ 0 , 1 ]$ . $\forall x _ { 0 } \ =$ $( x _ { 0 , 1 } \ldots , x _ { 0 , n } ) \in \Omega$ , $\exists \varepsilon > 0$ such that the cube centered at $\begin{array} { r } { x _ { 0 } : \prod _ { \alpha = 1 } ^ { n } [ x _ { 0 , \alpha } - } \end{array}$ $\epsilon , x _ { 0 , \alpha } + \epsilon ] \subset \Omega$ . $\forall \lambda = ( \lambda ^ { 1 } , \ldots , \lambda ^ { n } ) \in \mathbf { S } ^ { n - 1 }$ and $\forall \xi = ( \xi _ { 1 } , \ldots , \xi _ { N } ) \in \mathbb { R } ^ { N }$ , we define

$$
v _ {\varepsilon} (x) = \Pi_ {\alpha = 1} ^ {n} \rho \left(\frac {\lambda^ {\alpha} (x _ {\alpha} - x _ {0 , \alpha})}{\varepsilon}\right) \cdot \xi ,
$$

where $\rho$ is understood to be 0 outside $( - 1 , 1 )$ . Substituting $v _ { \varepsilon }$ into (4.5), and letting $\varepsilon \to 0$ , we obtain

$$
\sum_ {i, k = 1} ^ {N} \sum_ {\alpha , \beta = 1} ^ {n} \frac {\partial^ {2} \varphi (x _ {0} , u (x _ {0}) , \nabla u (x _ {0}))}{\partial \xi_ {\alpha} ^ {i} \partial \xi_ {\beta} ^ {k}} \lambda^ {\alpha} \lambda^ {\beta} \xi_ {i} \xi_ {k} \geq 0 \forall (\lambda , \xi) \in (R ^ {n} \backslash \{\theta \}) \times R ^ {N}.
$$

This is called the Legendre–Hardamard condition, which can be rewritten in a compact form:

$$
\left(\lambda \otimes \xi\right) ^ {T} \partial^ {2} \varphi (x, u (x), \nabla u (x)) (\lambda \otimes \xi) \geqslant 0, \tag {4.6}
$$

where $\lambda \otimes \xi$ denotes the rank-one $n \times N$ matrix. In the next section, this condition on $\varphi$ is called rank-one convexity. Thus, rank-one convexity is the necessary condition for $\varphi$ at a minimizer.

In particular, if $N = 1$ , condition (4.6) is reduced to

$$
\sum_ {j, l = 1} ^ {n} \frac {\partial^ {2} \varphi (x , u (x) , \nabla u (x))}{\partial \xi_ {j} \partial \xi_ {l}} \lambda^ {j} \lambda^ {l} \geqslant 0, \quad \forall \lambda \in \mathbb {R} ^ {n} \backslash \{\theta \}, \forall x \in \Omega . \tag {4.7}
$$

However, if the strict inequality in (4.7) holds for all $\lambda \in \mathbb { R } ^ { n } \backslash \{ \theta \}$ , i.e., $\varphi$ is strictly convex with respect to $\xi$ . This is the ellipticity condition. To the vectorial case, the condition:

$$
\sum_ {i, k = 1} ^ {N} \sum_ {\alpha , \beta = 1} ^ {n} \frac {\partial^ {2} \varphi}{\partial \xi_ {\alpha} ^ {i} \partial \xi_ {\beta} ^ {k}} (u, u (x), \nabla u (x)) \xi_ {\alpha} ^ {i} \xi_ {\beta} ^ {k} > 0, \forall \xi = \{\xi_ {\alpha} ^ {i} \} \in \mathbb {M} ^ {n \times N}, \text {w i t h} | \xi | \neq 0,
$$

is called the strong ellipticity.

# 4.1.3 Dual Variational Principle

The following notion was introduced by Fenchel, and is very important in convex analysis.

Definition 4.1.4 Suppose that $X$ is a real Banach space and that $f : X \to$ $\mathbb { R } ^ { 1 } \cup \lbrace + \infty \rbrace$ is proper. The conjugate function of $f , f ^ { * } : X ^ { * } \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ i s defined by

$$
f ^ {*} (x ^ {*}) = \sup  _ {x \in X} \left\{\langle x ^ {*}, x \rangle - f (x) \right\}.
$$

The notion was initiated by Young’s inequality: if $\begin{array} { r } { f ( x ) = \frac { 1 } { p } | x | ^ { p } , 1 < p < } \end{array}$ $\infty$ , then $\begin{array} { r } { f ^ { * } ( x ) = \frac { 1 } { p ^ { \prime } } | x | ^ { p ^ { \prime } } } \end{array}$ , where $\begin{array} { r } { \frac { 1 } { p } + \frac { 1 } { p ^ { \prime } } = 1 } \end{array}$ .

In particular, if $f ( x ) \ = \ { \textstyle { \frac { 1 } { 2 } } } \langle A x , x \rangle _ { { \cal R } ^ { n } }$ , where $A$ is positive definite, then $f ^ { * } ( p ) = { \textstyle \frac { 1 } { 2 } } \langle p , A ^ { - 1 } p \rangle _ { R ^ { n } }$ .

The following propositions hold:

1. $f ^ { * }$ is convex and l.s.c. Moreover, it is proper, if $f$ is proper l.s.c. and convex.

Proof. Only the properness of $f ^ { * }$ needs to be proved, i.e., one should find $x _ { 0 } ^ { * } \in X ^ { * }$ such that $f ^ { \ast } ( x _ { 0 } ^ { \ast } ) < + \infty$ .

We consider the closed convex set $\operatorname { e p i } ( f ) = \{ ( x , t ) \in X \times \mathbb { R } ^ { 1 } |$ $f ( x ) \ \leqslant$ $t \}$ . Since $f$ is proper, $\exists x _ { 0 } ~ \in ~ X$ such that $f ( x _ { 0 } ) ~ < ~ + \infty$ . One chooses $t _ { 0 } ~ < ~ f ( x _ { 0 } )$ , then $( x _ { 0 } , t _ { 0 } ) \ \notin \ \mathrm { e p i } ( f )$ . By the Ascoli separation theorem, $\exists ( x _ { 0 } ^ { * } , \lambda ) \in X ^ { * } \times \mathbb { R } ^ { 1 }$ , $\exists \alpha \in \mathbb { R } ^ { 1 }$ , satisfying

$$
\langle x _ {0} ^ {*}, x \rangle + \lambda t > \alpha > \langle x _ {0} ^ {*}, x _ {0} \rangle + \lambda t _ {0}, \quad \forall (x, t) \in \operatorname {e p i} (f). \tag {4.8}
$$

In particular,

$$
\langle x _ {0} ^ {*}, x _ {0} \rangle + \lambda f (x _ {0}) > \langle x _ {0} ^ {*}, x _ {0} \rangle + \lambda t _ {0}.
$$

It follows that $\lambda > 0$ , and then

$$
\left\langle - \frac {1}{\lambda} x _ {0} ^ {*}, x \right\rangle - f (x) <   - \frac {\alpha}{\lambda} \quad \forall x \in D (f),
$$

i.e.,

$$
f ^ {*} \left(- \frac {x _ {0} ^ {*}}{\lambda}\right) <   - \frac {\alpha}{\lambda} <   + \infty .
$$

Since $f ^ { * }$ is proper, the conjugate function $f ^ { * * }$ for $f ^ { * }$ is well defined.

![](images/6ed27caeab912519c5bd51fce30b250f1a806c20e4bcfd1e7986506becc0ce13.jpg)

2. If $f \leqslant g$ , then $g ^ { \ast } \leqslant f ^ { \ast }$ .   
3. (Young’s inequality) $\langle x ^ { * } , x \rangle \leqslant f ( x ) + f ^ { * } ( x ^ { * } ) .$   
4. $f ( x ) + f ^ { * } ( x ^ { * } ) = \langle x ^ { * } , x \rangle \Longleftrightarrow x ^ { * } \in \partial f ( x )$ .

Proof. By definition, we have

$$
\begin{array}{l} x ^ {*} \in \partial f (x) \Longleftrightarrow \langle x ^ {*}, y - x \rangle \leqslant f (y) - f (x), \quad \forall y \in X, \\ \Longleftrightarrow \langle x ^ {*}, y \rangle - f (y) \leqslant \langle x ^ {*}, x \rangle - f (x), \quad \forall y \in X, \\ \Longleftrightarrow f ^ {*} \left(x ^ {*}\right) \leqslant \langle x ^ {*}, x \rangle - f (x). \\ \end{array}
$$

Combining with Young’s inequality, the last inequality is equivalent to

$$
f (x) + f ^ {*} (x ^ {*}) = \langle x ^ {*}, x \rangle .
$$

![](images/ba8bcdcf0e92fb318084459a7d464a00cd1a2e7ab7c49086d35fe059ec6e7f62.jpg)

5. If $g ( x ) = f ( x - x _ { 0 } ) + \langle x _ { 0 } ^ { * } , x \rangle + a$ , then $g ^ { * } ( x ^ { * } ) = f ^ { * } ( x ^ { * } - x _ { 0 } ^ { * } ) + \langle x ^ { * } , x _ { 0 } \rangle -$ $( a + \langle x _ { 0 } ^ { * } , x _ { 0 } \rangle )$ .   
6. If $g ( x ) = f ( \lambda x )$ , then $\begin{array} { r } { g ^ { * } ( x ^ { * } ) = f ^ { * } ( \frac { x ^ { * } } { \lambda } ) } \end{array}$ for $\lambda \neq 0$ .

Theorem 4.1.5 (Fenchel–Moreau) If $f$ is a proper, l.s.c., convex function, then $f ^ { * * } = f$ .

Proof. By Young’s inequality, $f ^ { * * } \leqslant f$ . It remains to show the reversed inequality. We prove it by contradiction, i.e., assume that $\exists x _ { 0 } \in X$ such that $f ^ { * * } ( x _ { 0 } ) < f ( x _ { 0 } )$ .

Similar to the proof of proposition 1, we separate $\operatorname { e p i } ( f )$ with the point $( x _ { 0 } , f ^ { * * } ( x _ { 0 } ) )$ , and obtain (4.8), in which $t _ { 0 } = f ^ { * * } ( x _ { 0 } )$ and $\lambda \geq 0$ .

If $\lambda > 0$ , then

$$
\langle x _ {0} ^ {*}, x \rangle + \lambda f (x) > \alpha \quad \forall x \in \operatorname {d o m} (f).
$$

It follows that

$$
f ^ {*} \left(- \frac {x _ {0} ^ {*}}{\lambda}\right) \leqslant - \frac {\alpha}{\lambda}.
$$

However, by the definition of $f ^ { * * }$ ,

$$
f ^ {* *} (x _ {0}) \geqslant \left\langle - \frac {x _ {0} ^ {*}}{\lambda}, x _ {0} \right\rangle - f ^ {*} \left(- \frac {x _ {0} ^ {*}}{\lambda}\right).
$$

It follows that

$$
\langle x _ {0} ^ {*}, x _ {0} \rangle + \lambda f ^ {* *} (x _ {0}) \geqslant \alpha .
$$

This contradicts (4.8), in which $t _ { 0 } = f ^ { * * } ( x _ { 0 } )$ . In the case where $f ( x _ { 0 } ) < + \infty$ , from (4.8), $\lambda > 0$ . It remains to verify that $f ^ { * * } ( x _ { 0 } ) = + \infty$ if $f ( x _ { 0 } ) = + \infty$ and $\lambda = 0$ . Again from (4.8), $\exists \epsilon > 0$ such that

$$
\left\langle x _ {0} ^ {*}, x - x _ {0} \right\rangle \geq \epsilon \forall x \in \operatorname {d o m} (f).
$$

Since $f ^ { * }$ is proper, there is an $x _ { 1 } ^ { * } \in X ^ { * }$ such that $f ^ { \ast } ( x _ { 1 } ^ { \ast } ) < + \infty$ , and

$$
\langle x _ {1} ^ {*}, x \rangle - f (x) - f ^ {*} \left(x _ {1} ^ {*}\right) \leq 0 \forall x \in \operatorname {d o m} (f).
$$

Putting them together, $\forall n \in \mathbf { N }$ , we have

$$
\langle x _ {1} ^ {*} - n x _ {0} ^ {*}, x \rangle + n \langle x _ {0} ^ {*}, x _ {0} \rangle + n \epsilon - f (x) - f ^ {*} \left(x _ {1} ^ {*}\right) \leq 0, \forall x \in \operatorname {d o m} (f),
$$

it follows that

$$
f ^ {*} \left(x _ {1} ^ {*} - n x _ {0} ^ {*}\right) + n \left\langle x _ {0} ^ {*}, x _ {0} \right\rangle + n \epsilon - f ^ {*} \left(x _ {1} ^ {*}\right) \leq 0,
$$

or

$$
n \epsilon + \left\langle x _ {1} ^ {*}, x _ {0} \right\rangle - f ^ {*} \left(x _ {1} ^ {*}\right) \leq \left\langle x _ {1} ^ {*} - n x _ {0} ^ {*}, x _ {0} \right\rangle - f ^ {*} \left(x _ {1} ^ {*} - n x _ {0} ^ {*}\right) \leq f ^ {* *} \left(x _ {0}\right).
$$

Letting $n \longrightarrow \infty$ , we obtain $f ^ { * * } ( x _ { 0 } ) = + \infty$ . Therefore, $f = f ^ { * * }$ .

Corollary 4.1.6 For a proper, l.s.c., convex function $f$ , $x ^ { * } \in \partial f ( x ) \Longleftrightarrow x \in$ $\partial f ^ { * } ( x ^ { * } )$ .

Proof. It is a direct consequence of proposition 4 and Theorem 4.1.5. □

In the case where both $\partial f$ and $\partial f ^ { * }$ are single valued, Corollary 4.1.6 means that they are mutually inverse. In this sense, the conjugate function $f ^ { * }$ of $f$ is called the Legendre transform of $f$ .

Recall the variational formulation of the Hamiltonian system, which describes the motion of particle systems. The functional

$$
f (z) = \int_ {0} ^ {2 \pi} \left[ \frac {1}{2} \langle z, J \dot {z} \rangle_ {\mathbb {R} ^ {2 n}} + H (t, z) \right]
$$

is very indefinite. One cannot pose the minimization problem. However, in some cases, the Legendre transform may help.

(1) Assume that $H ( t , x , p )$ is proper, l.s.c., and convex in the variables $p$ , we define the Legendre transform of $H$ with respect to $p$ , i.e.,

$$
L (t, x, q) = \sup  _ {p} \left\{\langle p, q \rangle_ {\mathbb {R} ^ {n}} - H (t, x, p) \right\}.
$$

It is called the Lagrangian. In other words, $\forall ( x , t )$ as a function of $q , L$ is the conjugate function of the function $H ( t , x , p )$ . From proposition 4 and the Hamiltonian system, one has

$$
\langle - \dot {x}, p \rangle_ {\mathbb {R} ^ {n}} = L (t, x, - \dot {x}) + H (t, x, p).
$$

If both $H$ and $L$ are differentiable, then,

$$
L _ {x} (t, x, - \dot {x}) = - H _ {x} (t, x, p).
$$

But from Corollary 4.1.6,

$$
\dot {x} = - H _ {p} (t, x, p) \Leftrightarrow p = L _ {q} (t, x, - \dot {x}).
$$

This shows that $( x , { \dot { x } } )$ satisfies the system:

$$
\frac {d}{d t} L _ {q} (t, x, - \dot {x}) + L _ {x} (t, x, - \dot {x}) = 0. \tag {4.9}
$$

However, (4.9) is the Euler–Lagrange equation of the functional:

$$
I (x) = \int_ {0} ^ {2 \pi} L (t, x, - \dot {x}) d t.
$$

The system (4.9) is called the associate Lagrange system.

For example, in a system of particles with generalized coordinates $q =$ $( q _ { 1 } , \ldots , q _ { n } )$ , the kinetic energy of the system is a positive definite quadratic form: $T = \textstyle { \frac { 1 } { 2 } } \sum a _ { i j } q _ { i } q _ { j }$ , where $a _ { i j } = a _ { i j } ( t , q )$ , and the potential energy is a continuous function bounded from below: $U = U ( q )$ . The total energy $H =$ $T + U$ is called the Hamiltonian, and $L = T - U$ is called the Lagrangian. In this case, the functional $I$ is bounded from below.

(2) Assume that $H ( t , x , p )$ is strictly convex in $\boldsymbol { z } = ( x , p )$ , then the Legendre transform of $H$ with respect to $z$ reads as

$$
G (t, w) = \sup  _ {z} \left\{\langle z, w \rangle_ {\mathbb {R} ^ {2 n}} - H (t, z) \right\}.
$$

Assume $H _ { z } ( t , \theta ) = \theta$ . We study nontrivial $2 \pi$ -periodic solutions of the Hamiltonian system. Let us consider a constraint variational problem:

$$
I (w) = \int_ {0} ^ {2 \pi} G (t, - J \dot {w}) d t,
$$

with

$$
g (w) = \int_ {0} ^ {2 \pi} \langle w, J \dot {w} \rangle_ {\mathbf {R} ^ {2 n}} d t = c \neq 0,
$$

where $c$ is a parameter. The Euler–Lagrange equation reads as

$$
G _ {w} (t, - J \dot {w}) = \lambda w.
$$

Claim: $\lambda \neq 0$ . If not, provided by

$$
w = H _ {z} (t, z) \Leftrightarrow z = G _ {w} (t, w),
$$

we have $- J \dot { w } = H _ { z } ( t , \theta ) = \theta$ . This contradicts $c \neq 0$ .

Let $z = \lambda w$ . Again by duality, we have

$$
\dot {z} = \lambda J H _ {z} (t, z).
$$

One can adjust $c$ such that $\lambda = 1$ .

For example, if $H ( t , z )$ is bounded by two quadratic functions: $0 < m | z | ^ { 2 } \leq$ $H ( t , z ) \leq M | z | ^ { 2 }$ , then $G ( t , w )$ is also. Thus the functional $I$ is bounded from below.

# 4.2 Direct Method

# 4.2.1 Fundamental Principle

Given a topological space $X$ and a function $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ , which is bounded from below, we seek the minimizers of $f$ on $X$ , if they exist. It is natural to require the l.s.c. of $f$ (i.e., $\forall t \in \mathbb { R } ^ { 1 }$ , the level set $f _ { t } : = \{ x \in$ $X | \ f ( x ) \leqslant t \}$ is closed) and certain compactness on $X$ . However, if $X$ is not compact, the coercive condition on $f$ should be assumed as a replacement: $f$ is proper and $\forall t \in \mathbb { R } ^ { 1 } \ \exists$ a compact subset $K _ { t } \subset X$ such that the level set $f _ { t } \subset K _ { t }$ .

Indeed, we have:

Theorem 4.2.1 If $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is l.s.c. and coercive, then it attains a minimizer on $X$ . In particular, $f$ is bounded from below.

Proof. Since $f$ is coercive, it is proper. Then $- \infty \leq m : = \operatorname* { i n f } f < + \infty$ . By the assumption, $\forall t > m$ , the set $f _ { t }$ is compact. Therefore by the finite intersection property, $_ { t > m } f _ { t } \neq \emptyset$ . A point $x _ { 0 }$ in the intersection achieves $f ( x _ { 0 } ) = m$ . Since $f$ does not assume the value $- \infty , m > - \infty$ , and then $f$ is bounded from below. □

However, in analysis, people prefer to use a minimizing sequence (i.e., a sequence $\{ x _ { j } \} \subset X$ such that $f ( x _ { j } )  m : = \operatorname* { i n f } f .$ ) to approach the minimizer. In this case, it is required that $f$ is sequentially lower semi-continuous (s.l.s.c., for short) and we assume that $K _ { t }$ is sequentially compact, $\forall t \in \mathbb { R } ^ { 1 }$ . These two notions are defined as follows:

$$
f \text {i s}.
$$

and

$$
\begin{array}{l} K \text {i s s e q u e n t i a l l y c o m p a c t} \Longleftrightarrow \text {a n y s e q u e n c e} \left\{z _ {j} \right\} \subset K \\ \text {c o n t a i n s a s u b s e q u e n c e} z _ {j} \to z \in K  . \end{array}
$$

Namely,

Theorem 4.2.2 If $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is s.l.s.c., and that $\forall t \in \mathbb { R } ^ { 1 } \ \exists$ a sequentially compact set $K _ { t } ~ \subset ~ X$ such that $f _ { t } ~ \subset ~ K _ { t }$ , then $f$ attains a minimizer in $X$ .

Recall the Eberlein–Schmulian theorem, for weakly closed subsets of a Banach space,

weakly compact = sequentially weakly compact.

According to the Banach–Alaoglu theorem, every w $^ *$ -closed norm bounded set in the dual space of a Banach space is w∗ compact. Thus, for every weakly closed subset of a reflexive Banach space,

weakly closed plus norm bounded = weakly compact = sequentially weakly compact.

In this case, the coerciveness of $f$ is equivalent to that $\forall t \in R ^ { 1 }$ the level set $f _ { t }$ of $f$ is bounded in norm, or equivalently, that $f$ is proper and $f ( x )  + \infty$ as $\| x \| \to \infty$ .

However, how do we verify the weakly $^ *$ lower semi-continuity (w∗l.s.c., in short) or sequentially weakly* lower semi-continuity (s.w∗l.s.c., in short) of $f$ ?

We notice that in a Banach space, closed convex set = weakly closed convex set (Hahn–Banach theorem) = sequentially weakly closed convex set (Mazur theorem).

Thus, if $f$ is convex, then l.s.c. = w.l.s.c. = s.w.l.s.c.

Combining the above discussions, both Theorem 4.2.1 and Theorem 4.2.2 imply Corollary 2.3.7 as a special case. Because of the importance of this statement, we rewrite it as follow:

Theorem 4.2.3 Let $X$ be a reflexive Banach space and let $E \subset X$ be a weakly (or weakly sequentially) closed nonempty subset. If $f : E \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is $a$ l.s.c., convex and coercive function, then $f$ has a minimizer on $E$ .

In fact, Theorem 4.2.3 is a general principle in the proof of the existence of a minimizer. We shall present several classical examples to show how it works.

# 4.2.2 Examples

Example 1. (Dirichlet problem for Poisson equation)

Given $f \in L ^ { 2 } ( \Omega )$ , where $\Omega \subset \mathbb { R } ^ { n }$ is a bounded domain, find a minimizer of the following functional on $H _ { 0 } ^ { 1 } ( \Omega )$ :

$$
J (u) = \int_ {\Omega} \left(\frac {1}{2} | \nabla u | ^ {2} - f \cdot u\right) d x.
$$

Obviously, $J$ is convex and l.s.c.. We verify the coerciveness: From the Poincar´e inequality, we have a constant $C > 0$ such that

$$
\int_ {\Omega} | u | ^ {2} d x \leqslant C \int_ {\Omega} | \nabla u | ^ {2}.
$$

Thus, we may assume $\begin{array} { r } { \| u \| _ { H _ { 0 } ^ { 1 } } = ( \int _ { \Omega } | \nabla u | ^ { 2 } ) ^ { \frac { 1 } { 2 } } } \end{array}$

Then $\exists C _ { 1 } > 0$ 0, such that

$$
\begin{array}{l} J (u) \geqslant \frac {1}{2} \int_ {\Omega} | \nabla u | ^ {2} - \| f \| _ {2} \cdot \| u \| _ {2} \\ \geqslant \frac {1}{2} \int_ {\Omega} | \nabla u | ^ {2} - \frac {1}{4} \int_ {\Omega} | \nabla u | ^ {2} - C _ {1} \| f \| _ {2} ^ {2} \\ = \frac {1}{4} \| u \| _ {H _ {0} ^ {1}} ^ {2} - C _ {1} \| f \| _ {2} ^ {2} \rightarrow + \infty \\ \end{array}
$$

as $\parallel u \parallel _ { H _ { 0 } ^ { 1 } } \longrightarrow + \infty$

0It is well known that the Euler–Lagrange equation of $J$ is the Poisson equation with Dirichlet boundary conditions:

$$
\left\{ \begin{array}{l l} - \triangle u = f & \text {i n} \Omega  , \\ u = 0 & \text {o n} \partial \Omega  . \end{array} \right.
$$

Example 2. (Harmonic map) Let $\Omega \subset \mathbb { R } ^ { m }$ be a bounded open domain with smooth boundary $\partial \Omega$ . A map $u = ( u ^ { 1 } , \ldots , u ^ { n + 1 } ) : \Omega \to S ^ { n } \subset \mathbb { R } ^ { n + 1 }$ , where $S ^ { n }$ is the unit sphere, is called harmonic if

$$
- \triangle u ^ {k} = u ^ {k} | \nabla u | ^ {2} \quad \text {i n} \Omega , k = 1, 2, \dots , n + 1,
$$

where $| \nabla u | ^ { 2 } = \sum _ { k = 1 } ^ { n + 1 } | \nabla u ^ { k } | ^ { 2 }$ n+1 .

Given $\varphi = ( \varphi ^ { 1 } , \dots , \varphi ^ { n + 1 } ) : \partial \Omega \to S ^ { n }$ , find a harmonic map $u$ with prescribed boundary condition $u | _ { \partial \Omega } = \varphi$ .

Let us consider a subset of the Banach space $X = H ^ { 1 } ( \Omega , \mathbb { R } ^ { n + 1 } )$ :

$$
M = \{u \in X | u | _ {\partial \Omega} = \varphi , u (x) \in S ^ {n} \text {a . e .}, x \in \Omega \}
$$

and define the functional

$$
E (u) = \frac {1}{2} \int_ {\Omega} | \nabla u | ^ {2} d x.
$$

Firstly, $M$ is a weakly sequentially closed set. Indeed, if $u _ { j }  u \mathrm { i n } H ^ { 1 } ( \Omega ,$ $\mathbb { R } ^ { n + 1 } )$ , then modulo a subsequence $\{ u _ { j } \}$ we have $u _ { j } \to u \mathrm { i n } L ^ { 2 } ( \Omega , \mathbb { R } ^ { n + 1 } )$ and then $u _ { j } ( x )  u ( x )$ a.e. From $u _ { j } \in { \cal M } \ \forall j$ , we have $u \in M$ . Obviously, $E$ i s l.s.c., convex and coercive.

We are then able to apply Theorem 4.2.3 to obtain the existence of a minimizer $u ^ { * } \in M$ . What is the Euler–Lagrange equation for $E$ ? From

$$
d E (u, v) = \int_ {\Omega} \nabla u \cdot \nabla v = 0,
$$

$\forall v \in H _ { 0 } ^ { 1 } ( \Omega , \mathbb { R } ^ { n + 1 } )$ satisfying

$$
v (x) \in T _ {u ^ {*} (x)} S ^ {n} \quad \mathrm {a . e . i n} \Omega ,
$$

we obtain

$$
\left(\triangle u ^ {*}\right) ^ {T} (x) = 0 \mathrm {a . e . i n} \Omega ,
$$

where $( \triangle u ^ { * } ) ^ { T } ( x )$ is the tangential projection of $\triangle u ^ { * } ( x )$ at $u ( x )$ .

Noticing that the normal projection of $\triangle u ( x )$ at $u ( x )$ reads as

$$
(\triangle u) ^ {N} (x) = \triangle u (x) \cdot u (x),
$$

and then by differentiation twice of the constraint $| u ( x ) | ^ { 2 } = 1$ , we obtain

$$
\triangle u (x) \cdot u (x) = - | \nabla u (x) | ^ {2},
$$

from which follows

$$
- \triangle u ^ {*} (x) = u ^ {*} (x) | \nabla u ^ {*} (x) | ^ {2}.
$$

However, we have only proved that $u ^ { * } \in H ^ { 1 } ( \Omega , \mathbb { R } ^ { n + 1 } )$ . The rest of the problem is about the regularity of $u ^ { * }$ . For $m = 2$ , the harmonic map $u ^ { * }$ with minimal energy is smooth if $\varphi$ is, according to a result due to Morrey [Mo 2], but for $m > 2$ , generally speaking, there is no such regularity (see Schoen and Uhlenbeck [ScU 1, ScU 2], Lin [Lin]. The definition and an existence result for harmonic maps between two Riemannian manifolds can be found in Eells and Sampson [ES]. The regularity problem for harmonic maps has attracted many authors. As a special case for elliptic systems, this regularity problem is related to the Hilbert 20th problem. It is proved in Helein [Hel 1] that for $m = 2$ any harmonic map is regular, but for $m = 3$ Riviere [Ri] showed that there exists a non-minimal energy harmonic map discontinuous everywhere.

Example 3. Nonlinear eigenvalue problem

Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open domain with smooth boundary, and let $\phi \in C ( \overline { { \Omega } } \times \mathbb { R } ^ { 1 } , \mathbb { R } ^ { 1 } )$ satisfy

(2)

then $\exists c _ { 0 } > 0$ such that $\forall c \in ( 0 , c _ { 0 } ]$ the equation

$$
\left\{ \begin{array}{l l} - \triangle u (x) = \lambda \phi (x, u (x)) & x \in \Omega  , \\ u | _ {\partial \Omega} = 0  , \end{array} \right.
$$

has a solution $( \lambda _ { c } , u _ { c } )$ satisfying

$$
\int_ {\Omega} \int_ {0} ^ {u _ {c} (x)} \phi (x, t) d t d x = c.
$$

Proof. Set $X \ = \ H _ { 0 } ^ { 1 } ( \Omega )$ , $g ( u ) ~ = ~ \int _ { \Omega } \Phi ( x , u ( x ) ) d x$ , where $\begin{array} { r } { \Phi ( x , t ) = \int _ { 0 } ^ { t } \phi ( x , } \end{array}$ $s ) d x = O ( | t | ^ { q } ) , M _ { c } = g ^ { - 1 } ( c )$ , and $\begin{array} { r } { f ( u ) = \frac { 1 } { 2 } \int _ { \Omega } | \nabla u | ^ { 2 } } \end{array}$ .

Since $\Phi ( x , t ) \geqslant 0$ but not identical to $0$ , we have $g ( u ) \geqslant 0$ but not identical to 0. We have $c _ { 0 } > 0$ such that $g ^ { - 1 } ( c _ { 0 } ) \neq \emptyset$ ; let $u _ { 0 } \in g ^ { - 1 } ( c _ { 0 } ) , \ t \mapsto g ( t u _ { 0 } )$ is continuous on $[ 0 , 1 ]$ , and the function ranges over $[ 0 , c _ { 0 } ]$ . Therefore $g ^ { - 1 } ( c ) \neq$ $\emptyset \ \forall c \in [ 0 , c _ { 0 } ]$ . Since the embedding $H _ { 0 } ^ { 1 } ( \Omega )  L ^ { q } ( \Omega )$ is compact, $g$ is completely continuous, i.e., $u _ { j }  u ^ { * } \implies g ( u _ { j } ) \to g ( u ^ { * } )$ . Thus $M _ { c }$ is a sequential weakly closed subset. Obviously, $f$ is l.s.c., convex and coercive, so we have proved the existence of a minimum point $u _ { c }$ .

It remains to verify that $g ^ { \prime } ( u ^ { * } ) = \phi ( x , u ^ { * } ( x ) ) \neq \theta$ . In fact, $\theta \notin M _ { c }$ for $c \neq 0$ , then $\boldsymbol { u } ^ { * } \neq \boldsymbol { \theta }$ , therefore, $\phi ( x , u ^ { * } ( x ) ) \neq \theta$ .

As a corollary, we consider the nontrivial solution of the equation:

$$
\left\{ \begin{array}{l l} - \triangle u = | u | ^ {q - 2} u & \text {i n} \Omega ,   2 <   q <   2 ^ {*} \\ u | _ {\partial \Omega} = 0  . \end{array} \right.
$$

Let $( \lambda _ { 0 } , u _ { 0 } ) \in \mathbb { R } ^ { 1 } \times H _ { 0 } ^ { 1 } ( \Omega )$ be a solution of the nonlinear eigenvalue problem:

$$
\left\{ \begin{array}{l} - \triangle u = \lambda | u | ^ {q - 2} u \\ u | _ {\partial \Omega} = 0  , \end{array} \right.
$$

with

$$
\int_ {\Omega} | u _ {0} | ^ {q} > 0.
$$

Let

$$
\lambda_ {0} = \int_ {\Omega} | \nabla u _ {0} | ^ {2} / \int_ {\Omega} | u _ {0} | ^ {q} > 0.
$$

Setting

$$
u ^ {*} = \lambda_ {0} ^ {\frac {1}{q - 2}} u _ {0},
$$

we obtain

$$
\left\{ \begin{array}{l l} - \triangle u ^ {*} = | u ^ {*} | ^ {q - 2} u ^ {*} & \text {i n} \Omega \\ u ^ {*} | _ {\partial \Omega} = 0. \end{array} \right.
$$

![](images/7cd83eedd2307d4aa88ffec4d2cc00c3b18d1a42a9949966d5668a893b42680d.jpg)

Remark 4.2.4 If $- \varDelta$ is replaced by $- \varDelta + I$ , then the same conclusion holds.

Example 4. (Prescribing constant mean curvature problem)

Given a constant $H > 0$ and a curve $\gamma : S ^ { 1 } \to \mathbb { R } ^ { 3 }$ . Suppose $\mathrm { d i a m } \left( \gamma \right) = R$ with $H R < 1$ . Find a disc-type surface $u : D ^ { 2 } \xrightarrow { C ^ { 2 } } \mathbb { R } ^ { 3 }$ satisfying

$$
\left\{ \begin{array}{l} \triangle u = 2 H u _ {x} \wedge u _ {y} \quad \text {i n} D \\ u | _ {\partial D} = \gamma , \end{array} \right. \tag {4.10}
$$

where

$$
u = (u ^ {1}, u ^ {2}, u ^ {3})
$$

and

$$
u _ {x} \wedge u _ {y} = \left(\left| \begin{array}{c c} u _ {x} ^ {2} & u _ {y} ^ {2} \\ u _ {x} ^ {3} & u _ {y} ^ {3} \end{array} \right|, \left| \begin{array}{c c} u _ {x} ^ {3} & u _ {y} ^ {3} \\ u _ {x} ^ {1} & u _ {y} ^ {1} \end{array} \right|, \left| \begin{array}{c c} u _ {x} ^ {1} & u _ {y} ^ {1} \\ u _ {x} ^ {2} & u _ {y} ^ {2} \end{array} \right|\right)  .
$$

This is a geometric problem, in which $H$ is a prescribed constant mean curvature, and $\gamma$ is a prescribed boundary value.

Write the problem in its variational form; define

$$
J (u) = \int_ {D} | \nabla u | ^ {2} + \frac {4 H}{3} \int_ {D} u \cdot (u _ {x} \wedge u _ {y}) \mathrm {o n} H ^ {1} \cap L ^ {\infty} (D, R ^ {3}).
$$

Since

$$
\langle J ^ {\prime} (u), v \rangle = 2 \langle - \triangle u + 2 H (u _ {x} \wedge u _ {y}), v \rangle ,
$$

$\forall v \in C _ { 0 } ^ { \infty } ( D , \mathbb { R } ^ { 3 } )$ . The Euler–Lagrange equation is exactly (4.10). In appearance, inf $J ( u ) = - \infty$ , there is no minimum. However, if we add a constraint condition:

$$
\parallel u \parallel_ {L ^ {\infty} (D, \mathbb {R} ^ {3})} \leq R ^ {\prime},
$$

where $R < R ^ { \prime }$ , and $H R ^ { \prime } < 1$ , then by

$$
| u _ {x} \wedge u _ {y} | \leqslant | u _ {x} | | u _ {y} | \leqslant \frac {1}{2} (| u _ {x} | ^ {2} + | u _ {y} | ^ {2}),
$$

we have

$$
J (u) \geqslant \int_ {D} \left(| \nabla u | ^ {2} - \frac {2}{3} | \nabla u | ^ {2}\right) = \frac {1}{3} \int_ {D} | \nabla u | ^ {2}.
$$

Moreover, on the convex set

$$
C _ {R ^ {\prime}} = \left\{u \in H ^ {1} (D, \mathbb {R} ^ {3}) | u | _ {\partial D} = \gamma , \| u \| _ {L ^ {\infty}} \leq R ^ {\prime} \right\},
$$

which is sequentially weakly closed in $H ^ { 1 } ( D , R ^ { 3 } )$ , $J$ is nonnegative and coercive. We verify the w.s.l.s.c. of $J$ on $C _ { R ^ { \prime } }$ under $H ^ { 1 }$ norm. Suppose $u _ { j } ~ \to ~ u ^ { * }$ $( H ^ { 1 } ( D , \mathbb { R } ^ { 3 } ) )$ , with $u _ { j } \in \ C _ { R ^ { \prime } }$ ; we have a subsequence, for which we do not change the subscripts:

$$
\left\{\begin{array}{l}u _ {j} \to u ^ {*}, \text {s t r o n g l y i n} L ^ {2} (D, \mathbb {R} ^ {3})  ,\\u _ {j, x} \rightharpoonup u _ {x} ^ {*}, \text {w e a k l y i n} L ^ {2} (D, \mathbb {R} ^ {3})  ,\\u _ {j, y} \rightharpoonup u _ {y} ^ {*}, \text {w e a k l y i n} L ^ {2} (D, \mathbb {R} ^ {3})  .\end{array}\right.
$$

and

$$
u _ {j} (x) \rightarrow u ^ {*} (x) \quad \text {a . e . i n} D.
$$

Rewrite

$$
\begin{array}{l} u _ {j} \cdot \left(u _ {j x} \wedge u _ {j y}\right) - u ^ {*} \cdot \left(u _ {x} ^ {*} \wedge u _ {y} ^ {*}\right) = v _ {j} \cdot \left(u _ {x} ^ {*} \wedge u _ {y} ^ {*}\right) + u _ {j} \cdot \left(v _ {j x} \wedge u _ {y} ^ {*} + u _ {x} ^ {*} \wedge v _ {j y}\right) \\ + u _ {j} \cdot \left(v _ {j x} \wedge v _ {j y}\right), \\ \end{array}
$$

where $\boldsymbol { v } _ { j } = \boldsymbol { u } _ { j } - \boldsymbol { u } ^ { * } .$ . Noticing

$$
\left| u _ {j} \wedge u _ {y} ^ {*} \right| \leqslant R ^ {\prime} \left| u _ {y} ^ {*} \right|,
$$

we obtain

$$
\int_ {D} | u _ {j} \wedge u _ {y} ^ {*} - u ^ {*} \wedge u _ {y} ^ {*} | ^ {2} \rightarrow 0,
$$

provided by Lebesgue’s dominance theorem. Therefore

$$
\int_ {D} u _ {j} \cdot (v _ {j x} \wedge u _ {y} ^ {*}) = - \int_ {D} v _ {j x} \cdot (u _ {j} \wedge u _ {y} ^ {*}) \rightarrow 0.
$$

Similarly

$$
\int_ {D} u _ {j} \cdot (u _ {x} ^ {*} \wedge v _ {j y}) \rightarrow 0.
$$

Since

$$
\int_ {D} \left| u _ {x} ^ {*} \wedge v _ {y} ^ {*} \right| \leqslant \frac {1}{2} \int_ {D} \left| \nabla u ^ {*} \right| ^ {2},
$$

and $| v _ { j } | \leqslant 2 R ^ { \prime }$ , again by Lebesgue’s dominance theorem, we have

$$
\int_ {D} v _ {j} \cdot \left(u _ {x} ^ {*} \wedge u _ {y} ^ {*}\right)\rightarrow 0.
$$

Hence,

$$
\begin{array}{l} J (u _ {j}) - J (u ^ {*}) = \int_ {D} | \nabla v _ {j} | ^ {2} + 2 \nabla v _ {j} \nabla u ^ {*} + \frac {4 H}{3} u _ {j} \cdot (v _ {j x} \wedge v _ {j y}) + \circ (1) \\ \geqslant \frac {1}{3} \int_ {D} | \nabla v _ {j} | ^ {2} + \circ (1); \\ \end{array}
$$

it follows that

$$
\lim  _ {j \rightarrow \infty} J (u _ {j}) \geqslant J (u ^ {*}).
$$

How do we drop the constraint? By definition, if $u ^ { * } \in C _ { R ^ { \prime } }$ achieves the minimum point of $J$ on $C _ { R ^ { \prime } }$ , then $u ^ { * }$ satisfies the variational inequality:

$$
\int_ {D} \nabla u ^ {*} \nabla v + 2 H v \cdot \left(u _ {x} ^ {*} \wedge u _ {y} ^ {*}\right) \geqslant 0, \forall v = w - u ^ {*}, \tag {4.11}
$$

where $w \in C _ { R ^ { \prime } }$ . In particular, for any $\eta \in C _ { 0 } ^ { \infty } ( D , \mathbb { R } ^ { 1 } ) , \eta \ge 0$ , one chooses $\varepsilon > 0$ such that $( 1 - \varepsilon \eta ) u ^ { * } \in C _ { R ^ { \prime } }$ . Substituting $v = - \varepsilon \eta u ^ { * }$ into (4.11), we have

$$
- \frac {1}{2} \triangle | u ^ {*} | ^ {2} + | \nabla u ^ {*} | ^ {2} + 2 H u ^ {*} \cdot \left(u _ {x} ^ {*} \wedge u _ {y} ^ {*}\right) \leqslant 0 \text {i n} D.
$$

It follows that

$$
\left\{ \begin{array}{l} - \triangle | u ^ {*} | ^ {2} \leqslant 0, \text {i n} D  , \\ | u ^ {*} | = | \gamma | \text {o n} \partial D  . \end{array} \right.
$$

Thus, $| \boldsymbol { u } ^ { * } | ^ { 2 }$ is subharmonic. Applying the maximum principle, we have $\| \textit { \textbf { u } } \| _ { L ^ { \infty } } < \textit { R } < \textit { R }$ , i.e., $u ^ { * }$ is strictly in the interior of $C _ { R ^ { \prime } }$ under the $L ^ { \infty } ( D , \mathbb { R } ^ { 3 } )$ norm. Therefore we may choose arbitrarily $v \in C _ { 0 } ^ { \infty } ( D , R ^ { 3 } )$ with $\parallel \boldsymbol { v } \parallel _ { L ^ { \infty } } < R ^ { \prime } - R$ as variations. Thus $u ^ { * }$ is a weak solution of equation (4.10).

# 4.2.3 The Prescribing Gaussian Curvature Problem and the Schwarz Symmetric Rearrangement

We turn to studying a problem arising in differential geometry. Let $( M , g _ { 0 } )$ be a smooth compact two-dimensional Riemannian manifold with metric $g _ { 0 }$ ; let $k ( x )$ be its Gaussian curvature.

Given a function $K ( x )$ on $M$ , does there exist a metric $g$ , which is pointwisely conformal to $g _ { 0 }$ , such that $K$ is the Gaussian curvature with respect to $g$ ?

Setting a function $u$ on $M$ satisfying $g = e ^ { 2 u } g _ { 0 }$ , the problem is reduced to solving the following PDE:

$$
\triangle u = k - K e ^ {2 u} \text {o n} M, \tag {4.12}
$$

where $\bigtriangleup$ is the Laplace–Beltrami operator with respect to $g _ { 0 }$

According to the Gauss–Bonnet formula:

$$
\int_ {M} K d V _ {g} = 2 \pi \chi (M), \tag {4.13}
$$

where $\chi ( M )$ is the Euler characteristic of $M$ , and $V _ { g }$ is the volume form with respect to $g$ .

The curvature $K$ is restricted by the topology $\chi ( M )$ ; e.g., if $\chi ( M ) > 0$ (or $< 0$ ) then max $K > 0$ (or $\operatorname* { m i n } K < 0$ resp.), and if $\chi ( M ) = 0$ , then either $K \equiv 0$ or $K$ changes sign.

A result in the case where $\chi ( M ) < 0$ has been studied in Sect. 2.1, Example 5. Namely:

Statement. Assume $k ( x ) < 0$ and

$$
K (x) <   0 \forall x \in M. \tag {4.14}
$$

Then (4.12) possesses a solution.

However, (4.14) is not a necessary condition; the problem remains open if one merely assumes $\operatorname* { m i n } K < 0$ .

In contrast, the case $\chi ( M ) = 0$ was completely solved by a variational argument. A necessary condition for the solvability of (4.12) with $\chi ( M ) = 0$ reads as

$$
\left\{ \begin{array}{l} \text {e i t h e r} K \equiv 0, \\ \text {o r} K \text {c h a n g e s s i g n a n d} \int_ {M} K e ^ {2 v} d V _ {g _ {0}} <   0, \end{array} \right. \tag {4.15}
$$

where $v$ is the solution of

$$
\triangle v = k, \bar {v} = \frac {1}{\operatorname {V o l} (M)} \int_ {M} v = 0. \tag {4.16}
$$

In fact, if $u$ is a solution of (4.12), and let $w = u - v$ , then

$$
- \triangle w = K e ^ {2 (w + v)}. \tag {4.17}
$$

(4.15) follows from

$$
\begin{array}{l} \int_ {M} K e ^ {2 v} d V _ {g _ {0}} = - \int_ {M} \triangle w e ^ {- 2 w} d V _ {g _ {0}} \\ = - 2 \int_ {M} | \nabla w | ^ {2} e ^ {- 2 w} d V _ {g _ {0}} <   0. \\ \end{array}
$$

Conversely, the condition (4.15) is also sufficient. Without loss of generality, we assume $K \neq 0$ (otherwise, this is a linear problem). Let us consider a variational problem associated with (4.17):

$$
J (w) = \frac {1}{2} \int_ {M} | \nabla w | ^ {2} d V _ {g _ {0}}
$$

defined on

$$
\mathcal {M} = \left\{w \in H ^ {1} (M) \mid g _ {1} (w) = g _ {2} (w) = 0 \right\},
$$

where

$$
g _ {1} (w) = \int_ {M} w d V _ {g _ {0}} ,
$$

and

$$
g _ {2} (w) = \int_ {M} K e ^ {2 (w + v)} d V _ {g _ {0}}.
$$

If $\mathcal { M }$ is a nonempty weakly closed Banach manifold, and if $w$ is a local minimum point, then there exist Lagrange multipliers $\lambda , \mu \in R ^ { 1 }$ such that

$$
\int_ {M} \nabla w \cdot \nabla \varphi + \lambda \varphi + 2 \mu K e ^ {2 (w + v)} \varphi = 0,
$$

$\forall \varphi \in H ^ { 1 } ( M )$ , i.e.,

$$
- \triangle w + \lambda + 2 \mu K e ^ {2 (w + v)} = 0.
$$

From $g _ { 2 } ( w ) = 0$ , it follows that $\lambda = 0$ ; and according to (4.15) and integration by parts, we have $\mu < 0$ . Let us choose $r = { \textstyle { \frac { 1 } { 2 } } } \log ( - 2 \mu )$ , then $u = v + w + r$ solves (4.12).

We introduce the notion of the Schwarz symmetric rearrangement of a function and discuss its main properties, which will be often used in analysis.

For a nonnegative measurable function $u$ defined on a $_ { n }$ -dimensional measurable set $\Omega$ with $m ( \Omega ) < \infty$ , the Schwarz symmetric rearrangement of $u$ i s defined to be the following:

$$
u ^ {*} (y) = \sup \left\{t \geq 0 \mid | y | ^ {n} \leq C _ {n} ^ {- 1} m \{x \in \Omega \mid u (x) \geq t \} \right\},
$$

where $C _ { n }$ is the volume of the unit ball in $R ^ { n }$ . It possesses the following properties:

1. Let $u ^ { * } ( y ) = g ( | y | ) \forall y \in \Omega ^ { * } : = B _ { R } ( \theta )$ , where $\begin{array} { r } { R ^ { n } \ = \ \frac { 1 } { C _ { n } } m ( \Omega ) } \end{array}$ . Then $g$ i s nonincreasing and $g ( R ) = 0$ .

2. Let $u ^ { t } = \{ x \in \Omega | u ( x ) \geq t \} \forall t \geq 0$ . Then $m ( u ^ { t } ) = m ( u ^ { * t } ) \ \forall t \geq 0$ .

3. $\begin{array} { r } { \int _ { \Omega } u ^ { p } = \int _ { \Omega ^ { * } } u ^ { * p } \ \forall p \in [ 1 , \infty ) } \end{array}$ .

Ω Ω4. (Faber–Krahn inequality, [Fa], [Krh]) If $u \in H ^ { 1 } ( \Omega )$ , where $\Omega \subset R ^ { n }$ is a bounded domain, then

$$
\int_ {\Omega^ {*}} | \nabla u ^ {*} | ^ {2} \leq \int_ {\Omega} | \nabla u | ^ {2}.
$$

Proof. Let $H ^ { n - 1 } ( E )$ denote the $( n - 1 )$ -dimensional Hausdorff measure. From 2, the isoperimetric inequality, and the rotational invariance of $u ^ { * }$ , we have

$$
\begin{array}{l} H ^ {n - 1} (u ^ {- 1} (t)) ^ {2} = \left(\int_ {u ^ {- 1} (t)} 1\right) ^ {2} \leq \left(\int_ {u ^ {- 1} (t)} | \nabla u |\right) \left(\int_ {u ^ {- 1}} \frac {1}{| \nabla u |}\right), \\ H ^ {n - 1} \left(u ^ {* - 1} (t)\right) ^ {2} = \left(\int_ {u ^ {* - 1} (t)} 1\right) ^ {2} = \left(\int_ {u ^ {* - 1}} | \nabla u ^ {*} | ^ {2}\right) \left(\int_ {u ^ {* - 1} (t)} \frac {1}{| \nabla u ^ {*} |}\right), \\ \end{array}
$$

and

$$
H ^ {n - 1} \left(u ^ {* - 1} (t)\right) \leq H ^ {n - 1} \left(u ^ {- 1} (t)\right).
$$

By the co-area formula and 2, it follows that

$$
- \int_ {u ^ {- 1} (t)} \frac {1}{| \nabla u |} = \frac {d m (u ^ {t})}{d t} = \frac {d m (u ^ {* t})}{d t} = - \int_ {u ^ {* - 1} (t)} \frac {1}{| \nabla u ^ {*} |}.
$$

Thus

$$
\int_ {u ^ {* - 1} (t)} | \nabla u ^ {*} | \leq \int_ {u ^ {- 1} (t)} | \nabla u |.
$$

Again by the co-area formula

$$
\int_ {\Omega^ {*}} | \nabla u ^ {*} | ^ {2} = \int_ {0} ^ {\infty} \int_ {u ^ {* - 1} (t)} | \nabla u ^ {*} | d t \leq \int_ {0} ^ {\infty} \int_ {u ^ {- 1} (t)} | \nabla u | d t = \int_ {\Omega} | \nabla u | ^ {2}.
$$

We now turn to the variational problem.

Lemma 4.2.5 (Trudinger) Assume that $\Omega$ is a bounded planar domain with smooth boundary. Then $\forall u \in H _ { 0 } ^ { 1 } ( \Omega )$ with $\begin{array} { r } { \|  { u } \| ^ { 2 } { : = } \int _ { \Omega } | \nabla u | ^ { 2 } \le 1 } \end{array}$ , $\forall \beta ~ <$ 4π, $\exists \gamma = \gamma ( \beta )$ such that $\int _ { \Omega } e ^ { \beta u ^ { 2 } } \leq \gamma$ .

Proof. Since $\parallel | u | \parallel \leq \parallel u \parallel$ , we may assume $u \geq 0$ . Let $u ^ { * } ( y )$ be the Schwarz symmetric rearrangement of $u$ .

Thus $u ^ { * } ( y )$ is radially symmetric, nonincreasing and satisfies

$$
\left| \left\{y \in R ^ {2} \mid u ^ {*} (y) \geq t \right\} \right| = \left| \left\{x \in \Omega \mid u (x) \geq t \right\} \right| \forall t \in R ^ {1}.
$$

Let $u ^ { * } ( y ) = g ( | y | )$ , and $R = ( \pi ^ { - 1 } | \Omega | ) ^ { \frac { 1 } { 2 } }$ . We have $g ( R ) = 0$ . By changing variables:

$$
e ^ {- s} = \left(\frac {r}{R}\right) ^ {2} \text {a n d} f (s) = g (r),
$$

we have $\begin{array} { r } { f ( 0 ) = 0 , \ f ^ { \prime } ( s ) = - \frac { R } { 2 } e ^ { - \frac { s } { 2 } } g ^ { \prime } ( r ) } \end{array}$ and

$$
\begin{array}{l} | f (s) | \leq \int_ {0} ^ {s} | f ^ {\prime} (\sigma) | d \sigma \\ \leq \sqrt {s} \left(\int_ {0} ^ {s} | f ^ {\prime} (\sigma) | ^ {2} d \sigma\right) ^ {\frac {1}{2}} \\ \leq \sqrt {\frac {s}{2}} \left(\int_ {0} ^ {R} | g ^ {\prime} (r) | ^ {2} r d r\right) ^ {\frac {1}{2}} \\ = \sqrt {\frac {s}{4 \pi}} \left(\int_ {B _ {R}} | \nabla u ^ {*} | ^ {2}\right) \\ \leq \sqrt {\frac {s}{4 \pi}} \left(\int_ {\Omega} | \nabla u | ^ {2}\right) ^ {\frac {1}{2}}, \\ \end{array}
$$

provided by the Faber–Krahn inequality. Thus

$$
\begin{array}{l} \int_ {\Omega} \exp [ \beta u ^ {2} ] = \int_ {B _ {R}} \exp [ \beta (u ^ {*}) ^ {2} ] \\ = 2 \pi \int_ {0} ^ {R} \exp [ \beta g (r) ^ {2} ] r d r \\ = | \Omega | \int_ {0} ^ {\infty} \exp [ \beta f (s) ^ {2} - s ] d s \\ \leq \left(1 - \frac {\beta}{4 \pi}\right) ^ {- 1} | \Omega |. \\ \end{array}
$$

![](images/b4540363648b623fd8d3fd7c24d61cd462700f4d26bee442290174b7015b7021.jpg)

Corollary 4.2.6 There are constants $\beta > 0$ , and $\gamma > 0$ such that

$$
\int_ {M} \exp [ \beta u ^ {2} ] d V _ {g _ {0}} \leq \gamma ,
$$

for all $u \in H ^ { 1 } ( M )$ with $\begin{array} { r } { \overline { { u } } : = \frac { 1 } { \operatorname { V o l } ( M ) } \int _ { M } u = 0 } \end{array}$ and $\begin{array} { r } { \parallel u \parallel ^ { 2 } = \int _ { M } | \nabla u | ^ { 2 } d V _ { g _ { 0 } } = 1 } \end{array}$

Proof. By the use of the p f unity, we have $\begin{array} { r } { u = \sum _ { j = 1 } ^ { n } \chi _ { \underline { { j } } } u } \end{array}$ $\chi _ { j } \geq 0 \forall j$ $\textstyle \sum \chi _ { j } \equiv 1$ Since $\overline { { u } } = 0$ , from Poincar´e’s inequality, there is a constant $c > 0$ such that $\begin{array} { r } { | u | _ { 2 } : = ( \int _ { M } | u | ^ { 2 } ) ^ { \frac { 1 } { 2 } } \leq c \parallel u \parallel } \end{array}$ , and then $\begin{array} { r } { \parallel { \chi } _ { j } u \parallel \leq \parallel \nabla { \chi } _ { j } \parallel _ { \infty } | u | _ { 2 } + | { \chi } _ { j } | _ { \infty } \parallel u \parallel \leq } \end{array}$ $c _ { 1 } \parallel u \parallel$ for some constant $c _ { 1 }$ . We obtain from Lemma 4.2.5:

$$
\int_ {M} \exp [ \beta u ^ {2} ] \leq \sum_ {j = 1} ^ {n} \frac {1}{n} \int_ {M} \exp [ n ^ {2} \beta (\chi_ {j} u) ^ {2} ] \leq \gamma
$$

for sufficiently small $\beta$ .

Corollary 4.2.7 There exist constants $\beta > 0$ , and $\gamma > 0$ such that

$$
\int_ {M} e ^ {2 u} d V _ {g _ {0}} \leq \gamma \exp \left[ \beta^ {- 1} \| u \| ^ {2} + 2 \bar {u} \right] \forall u \in H ^ {1} (M). \tag {4.18}
$$

Proof. Let $a = \| u \| , \ v = a ^ { - 1 } ( u - \overline { { u } } )$ , then $\overline { { v } } = 0$ and $\lVert \boldsymbol { v } \rVert = 1$ . From $2 a v \leq$ $\textstyle { \beta v ^ { 2 } + { \frac { a ^ { 2 } } { \beta } } }$     − and Corollary 4.2.6, we obtain

$$
\int_ {M} e ^ {2 (u - \overline {{u}})} d V _ {g _ {0}} \leq \gamma \exp (\beta^ {- 1} \| u \| ^ {2}).
$$

The proof is complete.

Lemma 4.2.8 If $u _ { n } \to u _ { 0 }$ in $H ^ { 1 } ( M )$ , then after a subsequence, $\begin{array} { r l } { e ^ { u _ { n } } } & { { } \to } \end{array}$ $e ^ { u _ { 0 } }$ in $L ^ { 2 } ( M )$ .

Proof. We have

$$
\begin{array}{l} | e ^ {u _ {n}} - e ^ {u _ {0}} | _ {2} ^ {2} = \int e ^ {2 u _ {0}} | e ^ {(u _ {n} - u _ {0})} - 1 | ^ {2} \\ \leq \int e ^ {2 u _ {0}} | u _ {n} - u _ {0} | ^ {2} (1 + e ^ {2 (u _ {n} - u _ {0})}) \\ \leq 2 \left| e ^ {2 u _ {0}} \right| _ {4} \left(1 + \left| e ^ {2 \left(u _ {n} - u _ {0}\right)} \right| _ {4}\right) \left| u _ {n} - u _ {0} \right| _ {4} ^ {2}. \\ \end{array}
$$

On account of Corollary 4.2.7 the first two factors are bounded. Then by the compactness of Sobolev embedding, one has a subsequence such that $\left| u _ { n } - \right.$ $u _ { 0 } | _ { 4 } \to 0$ . □

Corollary 4.2.9 The functional $g _ { 2 }$ is $C ^ { 1 }$ and is weakly continuous on $H ^ { 1 } ( M )$ with $\begin{array} { r } { \langle g _ { 2 } ^ { \prime } ( w ) , \varphi \rangle = 2 \int K e ^ { 2 ( w + v ) } \varphi \ \forall \varphi \in H ^ { 1 } ( M ) } \end{array}$ .

It remains to verify the following:

(1) $\mathcal { M }$ is a nonempty weakly closed Banach manifold,   
(2) $J | { \mathcal { M } }$ is weakly lower semi-continuous,   
(3) $J | { \mathcal { M } }$ is coercive.

Indeed, (2), (3) are trivial and the nonemptyness of $\mathcal { M }$ is easy to verify. The weak closedness of $\mathcal { M }$ follows from Corollary 4.2.9. Since the functions 1 and $K e ^ { 2 ( w + v ) }$ are linear independent $\forall w \in \mathcal { M }$ , $\mathcal { M }$ is a submanifold of $H ^ { 1 } ( M )$ . Namely, we have:

Statement. (M. S. Berger) Assume $\chi ( M ) = 0$ . (4.12) is solvable if and only if $K$ satisfies (4.15).

For $\chi ( M ) > 0$ , topologically $M = S ^ { 2 }$ or $R P ^ { 2 }$ . As we have seen in (4.13), a necessary condition of the solvability of the problem (4.12) is $\operatorname* { m a x } K > 0$ . If we follow the above procedure, let $v$ be the solution of

$$
- \triangle v = \bar {k} - k, \bar {v} = 0.
$$

We turn to the equation as before:

$$
- \triangle w = K e ^ {2 (w + v)} - \bar {k}, \tag {4.19}
$$

and introduce a functional

$$
J (w) = \frac {1}{2} \int_ {M} \left(| \nabla w | ^ {2} + 2 \overline {{k}} w\right) d V _ {g _ {0}}
$$

defined on

$$
\mathcal {M} = \{w \in H ^ {1} (M) | g _ {2} (w) = \overline {{k}} \}.
$$

If $w$ is a local minimum point of $J$ , then there exists $\mu \in R ^ { 1 }$ such that

$$
- \triangle w = 2 \mu K e ^ {2 (w + v)} - \bar {k}.
$$

From $w \in \mathcal { M }$ , it follows that $\mu = 1 / 2$ , i.e., (4.19) is the Euler–Lagrange equation for $J$ .

From the Gauss–Bonnet formula, $k > 0$ , therefore if $\operatorname* { m a x } K > 0$ , $\mathcal { M } \neq \emptyset$ . Similarly, one can show that $\mathcal { M }$ is a weakly closed Banach manifold, and that $J$ is weakly lower semi-continuous. If $J$ were coercive, then (4.12) would be solvable. But this is not true in general. In fact, set $\widetilde { w } = w - \overline { { w } }$ , we have

$$
2 \overline {{w}} = \log \overline {{k}} \mathrm {V o l} (M) - \log \int_ {M} K e ^ {2 (v + \widetilde {w})} d V _ {g _ {0}},
$$

and

$$
\begin{array}{l} J (w) \geq \frac {1}{2} \left[ \int_ {M} | \nabla w | ^ {2} + \bar {k} \operatorname {V o l} (M) \log \bar {k} \operatorname {V o l} (M) - \bar {k} \operatorname {V o l} (M) \log \int_ {M} K e ^ {2 (v + \tilde {w})} \right] \\ \geq \frac {1}{2} \left(1 - \frac {4 \pi}{\beta}\right) \int_ {M} | \nabla w | ^ {2} + \text {c o n s t}. \\ \end{array}
$$

due to (4.18) and the Gauss–Bonnet formula

$$
\overline {{k}} \operatorname {V o l} (M) = \int_ {M} k d V _ {g _ {0}} = 4 \pi .
$$

If $\beta > 4 \pi$ , then the coerciveness follows. But Moser [Mos 4] proved that the best constant $\beta = 4 \pi$ for $M = S ^ { 2 }$ with the canonical metric $g _ { 0 }$ . This is the point where our argument breaks down.

However, for some special $K$ , e.g., if $K$ is even, we shall improve the estimate in (4.18). In this case, the sphere $S ^ { 2 }$ is reduced to the real projective space $R P ^ { 2 }$ geometrically.

Lemma 4.2.10 (Aubin) For a function $w \in H ^ { 1 } ( S ^ { 2 } )$ , if there exist $f _ { j } \in$ $C ^ { 1 } ( S ^ { 2 } ) , j = 1 , 2 \dots , k$ and $\alpha > 0$ , satisfying

$$
\int_ {S ^ {2}} f _ {j} e ^ {2 w} d V _ {g _ {0}} = 0, j = 1, 2, \dots , k,
$$

and

$$
\sum_ {j = 1} ^ {k} | f _ {j} (x) | \geq \alpha \forall x \in S ^ {2},
$$

then $\forall \epsilon > 0$ $\exists C _ { \epsilon } > 0$ such that

$$
\int_ {S ^ {2}} e ^ {2 w} \leq C _ {\epsilon} \exp \left[ \frac {1}{8 \pi - \epsilon} \int_ {S ^ {2}} | \nabla w | ^ {2} + 2 \overline {{w}} \right] \quad \forall w \in H ^ {1} (S ^ {2}).
$$

Proof. Set

$$
\Omega_ {j} ^ {\pm} = \left\{x \in S ^ {2} | \pm f _ {j} (x) \geq \frac {\alpha}{k} \right\},
$$

and $g _ { j } ^ { \pm } , \ h _ { j } ^ { \pm } \in C ^ { 1 } ( S ^ { 2 } )$ satisfying

$$
\begin{array}{l} \operatorname {s u p p} g _ {j} ^ {+} \cap \operatorname {s u p p} h _ {j} ^ {-} = \operatorname {s u p p} g _ {j} ^ {-} \cap \operatorname {s u p p} h _ {j} ^ {+} = \emptyset , 0 \leq g _ {j} ^ {\pm}, h ^ {\pm} \leq 1, \\ h _ {j} ^ {\pm} (x) = 1 \forall x \in \Omega_ {j} ^ {\pm}, \text {a n d} g _ {j} ^ {\pm} (x) = 1 \text {i f} \pm f _ {j} (x) \geq 0 \forall j. \\ \end{array}
$$

Then

$$
S ^ {2} = \cup_ {j = 1} ^ {k} \bigl (\Omega_ {j} ^ {+} \cup \Omega_ {j} ^ {-} \bigr) ,
$$

and then

$$
\int_ {S ^ {2}} e ^ {2 w} \leq \sum_ {j = 1} ^ {k} \left(\int_ {\Omega_ {j} ^ {+}} + \int_ {\Omega_ {j} ^ {-}}\right) e ^ {2 w}.
$$

Without loss of generality, one many assume that $\int _ { \Omega _ { 1 } ^ { + } } e ^ { 2 w }$ is the largest among $\begin{array} { r } { \int _ { \Omega _ { j } ^ { \pm } } e ^ { 2 w } , ~ j = 1 , 2 , \ldots . } \end{array}$ .

First, $\forall w \in H ^ { 1 } ( S ^ { 2 } )$ , if $\parallel \ h _ { 1 } ^ { + } w \ \parallel \leq \parallel \ g _ { 1 } ^ { - } w \ \parallel$ , then $\forall \epsilon _ { 1 } > 0 , \exists C _ { 1 } > 0$ , such that

$$
\begin{array}{l} 2 \left\| h _ {1} ^ {+} w \right\| ^ {2} \leq \left\| h _ {1} ^ {+} w \right\| ^ {2} + \left\| g _ {1} ^ {-} w \right\| ^ {2} \\ \leq \| w \| ^ {2} + C _ {1} \left(\| w \| | w | _ {2} + | w | _ {2} ^ {2}\right) \\ \leq \left(1 + \epsilon_ {1}\right) \| w \| ^ {2} + C _ {1} | w | _ {2} ^ {2}. \\ \end{array}
$$

Following Lemma 4.2.5 and Corollary 4.2.7,

$$
\begin{array}{l} \int_ {S ^ {2}} e ^ {2 h _ {1} ^ {+} w} \leq C _ {\epsilon_ {1}} \exp \left[ \frac {1}{4 \pi - \epsilon_ {1}} \| h _ {1} ^ {+} w \| ^ {2} + 2 C _ {2} | w | _ {2} ^ {2} \right] \\ \leq C _ {\epsilon_ {1}} \exp \left[ \frac {1 + \epsilon_ {1}}{2 (4 \pi - \epsilon_ {1})} \| w \| ^ {2} + C _ {3} (\epsilon_ {1}) | w | _ {2} ^ {2} \right]. \\ \end{array}
$$

Otherwise, we have

$$
\begin{array}{l} 2 \| g _ {1} ^ {-} w \| ^ {2} \leq \| g _ {1} ^ {-} w \| ^ {2} + \| h _ {1} ^ {+} w \| ^ {2} \\ \leq \left(1 + \epsilon_ {1}\right) \| w \| ^ {2} + C _ {1} | w | _ {2} ^ {2}, \\ \end{array}
$$

and

$$
\begin{array}{l} \int_ {S ^ {2}} e ^ {2 g _ {1} ^ {-} w} \leq C _ {\epsilon_ {1}} \exp \left[ \frac {1}{4 \pi - \epsilon_ {1}} \| g _ {1} ^ {-} w \| ^ {2} + 2 C _ {2} | w | _ {2} ^ {2} \right] \\ \leq C _ {\epsilon_ {1}} \exp \left[ \frac {1 + \epsilon_ {1}}{2 (4 \pi - \epsilon_ {1})} \| w \| ^ {2} + C _ {3} (\epsilon_ {1}) | w | _ {2} ^ {2} \right]. \\ \end{array}
$$

Now let $u = w - \overline { { w } }$ . $\forall \epsilon _ { 0 } > 0$ , $\exists C _ { 4 } = C _ { 4 } ( \epsilon _ { 0 } )$ such that

$$
\left| u \right| _ {1} \leq \epsilon_ {0} \| u \| ^ {2} + C _ {4}. \tag {4.20}
$$

$\forall \epsilon _ { 2 } > 0$ small, we choose $a = a ( \epsilon _ { 2 } ) > 0$ such that

$$
\left| \left\{x \in S ^ {2} \mid u (x) \geq a \right\} \right| = \epsilon_ {2}.
$$

Therefore, we obtain

$$
a \leq \frac {1}{\epsilon_ {2}} | u | _ {1} \leq \frac {\epsilon_ {0}}{\epsilon_ {2}} \| u \| ^ {2} + \frac {1}{\epsilon_ {2}} C _ {4}, \tag {4.21}
$$

from (4.20), and

$$
\left| (u - a) _ {+} \right| _ {2} ^ {2} \leq \epsilon_ {2} ^ {\frac {1}{2}} \left| (u - a) _ {+} \right| _ {4} ^ {2} \leq C _ {5} \epsilon_ {2} ^ {\frac {1}{2}} \| u \| ^ {2}. \tag {4.22}
$$

Setting $\epsilon _ { 0 } = \textstyle { \frac { 1 } { 2 } } \epsilon _ { 2 } ^ { 2 }$ , and choosing $\epsilon _ { 1 } , \epsilon _ { 2 } > 0$ so small that

$$
\frac {1 + \epsilon_ {1}}{2 (4 \pi - \epsilon_ {1})} + \epsilon_ {2} + C _ {3} (\epsilon_ {1}) C _ {5} \epsilon_ {2} ^ {\frac {1}{2}} <   \frac {1}{8 \pi - \epsilon},
$$

Substituting $w = ( u - a ) _ { + }$ in the estimates of the integrals: $\int _ { S ^ { 2 } } e ^ { 2 h _ { 1 } ^ { + } w }$ and $\int _ { S ^ { 2 } } e ^ { 2 g _ { 1 } ^ { - } w }$ , since $\begin{array} { r } { \int _ { S ^ { 2 } } e ^ { 2 h _ { 1 } ^ { + } u } \le e ^ { 2 a } \int _ { S ^ { 2 } } e ^ { 2 ( h _ { 1 } ^ { + } u - a ) _ { + } } } \end{array}$ and $\begin{array} { r } { \int _ { S ^ { 2 } } e ^ { 2 g _ { 1 } ^ { - } u } \le e ^ { 2 a } \int _ { S ^ { 2 } } } \end{array}$ $e ^ { 2 ( g _ { 1 } ^ { - } u - a ) _ { + } }$ , the right-hand sides become $\begin{array} { r } { B _ { \epsilon } \exp [ { \frac { 1 } { 8 \pi - \epsilon } \| u \| ^ { 2 } } ] } \end{array}$ .

Next, for the specified $w$ −  in the assumption, we have

$$
\int_ {S ^ {2}} e ^ {2 u} \leq 2 k \int_ {\Omega_ {1} ^ {+}} e ^ {2 u} \leq 2 k \int_ {S ^ {2}} e ^ {2 h _ {1} ^ {+} u}.
$$

In particular, according to $\textit { j } f _ { 1 } e ^ { 2 u } = 0$ , we also have

$$
\int_ {\Omega_ {1} ^ {+}} e ^ {2 u} \leq \frac {k}{\alpha} \int_ {S ^ {2}} (f _ {1}) _ {+} e ^ {2 u} = \frac {k}{\alpha} \int_ {S ^ {2}} (f _ {1}) _ {-} e ^ {2 u}
$$

$$
\leq \operatorname {M a x} \left\{\left| f _ {i} \right| \mid i = 1, \dots , k \right\} \frac {k}{\alpha} \int_ {S ^ {2}} e ^ {2 g _ {1} ^ {-} u}.
$$

In both cases,

$$
\int_ {S ^ {2}} e ^ {2 u} \leq C _ {\epsilon} \exp \left[ \frac {1}{8 \pi - \epsilon} \| u \| ^ {2} \right]. \tag {4.23}
$$

i.e.,

$$
\int_ {S ^ {2}} e ^ {2 w} \leq C _ {\epsilon} \exp \left[ \frac {1}{8 \pi - \epsilon} \| w \| ^ {2} + 2 \overline {{w}} \right].
$$

□

Now, we return to the case $\left( M , g _ { 0 } \right) = \left( S ^ { 2 } , g _ { 0 } \right)$ where $g _ { 0 }$ is the canonical metric, but assuming $K ( x ) = K ( - x ) , \forall x \in S ^ { 2 }$ .

Let us define

$$
\mathcal {M} ^ {e} = \left\{w \in H ^ {1} \left(S ^ {2}\right) \mid g _ {2} (w) = \bar {k} \text {a n d} w (x) = w (- x) \right\}.
$$

Then, again, $\mathcal { M } ^ { e }$ is a nonempty weakly closed Banach manifold. The functional

$$
J (w) = \frac {1}{2} \int_ {M} (| \nabla w | ^ {2} + 2 \overline {{k}} w) d V _ {g _ {0}}
$$

is again weakly lower semi-continuous.

Since $\forall w \in \mathcal { M } ^ { e }$

$$
\int_ {S ^ {2}} x _ {i} e ^ {2 w (x)} d V _ {g _ {0}} = 0 \quad i = 1, 2, 3,
$$

where $x _ { i }$ are the Euclidean coordinates, when we embed $S ^ { 2 }$ into $R ^ { 3 }$ canonically, Lemma 4.2.10 is applicable, and then the coerciveness of $J$ on $\mathcal { M } ^ { e }$ follows. Thus, we have:

Statement (J. Moser) For $M = R P ^ { 2 }$ with the canonical metric $g _ { 0 }$ , (4.12) is solvable if and only if max $K > 0$ .

Remark 4.2.11 Under certain symmetric conditions on $K$ other than the evenness, Hong [Hon 2] obtained the existence of solutions for equation (4.12).

Remark 4.2.12 The best constant $\beta = 4 \pi$ is due to Moser; there are several different proofs and extensions, see Adams [Ad], Carleson and Chang [CC] and Ding and Tian [DiT].

# 4.3 Quasi-Convexity

In the calculus of variations, we pay attention to the following functional:

$$
J (u) = \int_ {\Omega} f (x, u (x), \nabla u (x)) d x, \tag {4.24}
$$

where $f : \Omega \times \mathbb { R } ^ { N } \times \mathbb { R } ^ { n N } \to \mathbb { R } ^ { 1 }$ is a Caratheodory funciton, and $\Omega \subset \mathbb { R } ^ { n }$ is an open domain.

As we have seen, the w.l.s.c. (or w $^ *$ l.s.c.) condition plays an important role in the proof of the existence of a minimizer.

# 4.3.1 Weak Continuity and Quasi-Convexity

From the abstract theory, we know that an l.s.c. function is w.l.s.c. and s.w.l.s.c. if it is convex. But to the above integral functional, what is the relationship between the s.w.l.s.c. (or s.w∗.l.s.c.) with the convexity of the function $f$ with respect to its variables $( x , u , \xi )$ ?

To this end we would like to understand better the weak convergence in the spaces $W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { n } ) , \ 1 \leqslant p \leqslant \infty$ . Perhaps the simplest example of a weakly convergent but not strongly convergent sequence in our mind is $\{ \sin ( m t ) \} _ { 1 } ^ { \infty }$ i n $L ^ { 2 } ( [ 0 , 2 \pi ] ) .$ . The phenomenon is caused by the large oscillations of the sequence of functions. It can be extended as follows:

Lemma 4.3.1 Let $\boldsymbol { D } \ = \ \underset { j = 1 } { \overset { n } { \prod } } ( a _ { j } , b _ { j } )$ be a rectangle in $\mathbb { R } ^ { n }$ , and let $\varphi \in$ $L ^ { p } ( D )$ , $1 \leqslant p \leqslant \infty$ , which is extended periodically to $\mathbb { R } ^ { n }$ . Let $\varphi _ { m } ( x ) =$ $\varphi ( m x )$ $\varphi ( m x ) , \ \forall m \in \mathbb { N }$ , and

$$
\overline {{\varphi}} = \frac {1}{m (D)} \int_ {D} \varphi (x) d x.
$$

Then $\varphi _ { m } \to \overline { { \varphi } }$ in $L ^ { p } ( D )$ $1 < p < \infty$ and $\varphi _ { m } \stackrel { * } {  } \overline { { \varphi } }$ in $L ^ { \infty } ( D )$

Proof. First, we may assume $\overline { { \varphi } } = 0$ . Otherwise, we consider ${ \widetilde { \varphi } } = \varphi - { \overline { { \varphi } } }$ instead. Second,

$$
\parallel \varphi_ {m} \parallel_ {p} ^ {p} = \int_ {D} | \varphi (m x) | ^ {p} d x = \frac {1}{m ^ {n}} \int_ {m \cdot D} | \varphi (y) | ^ {p} d y = \parallel \varphi \parallel_ {p} ^ {p}, \forall 1 \leqslant p \leqslant \infty .
$$

Third, we define a signed set function $\begin{array} { r } { \Phi ( E ) = \int _ { E } \varphi } \end{array}$ for any measurable set $E$ . It is $\sigma$ -additive, and satisfies $\begin{array} { r } { \Phi ( x + D ) = 0 , \forall x \in \mathbb { R } ^ { n } } \end{array}$ .

Now, $\forall$ rectangles $Q = \prod _ { i = 1 } ^ { n } ( c _ { i } , d _ { i } )$ , by cancelling the nonoverlapping transi= 1 lations of $D$ in $k Q$ , we have the estimate:

$$
\left| \int_ {D} \varphi_ {k} \chi_ {Q} \right| = \left| \int_ {Q} \varphi_ {k} \right| = \frac {1}{k ^ {n}} | \Phi (k Q) | \leqslant \frac {n}{k} \int_ {D} | \varphi |.
$$

Thus, for simple functions of the form $\xi = \sum \alpha _ { i } \chi _ { Q _ { i } }$ , $Q _ { i } \cap Q _ { j } = \emptyset ~ i \neq j$ , we have

$$
\int_ {D} \varphi_ {k} \cdot \xi \rightarrow 0 \quad \text {a s} k \rightarrow \infty .
$$

∞ subset of Lp (D), p- = pp 1 . ∀f ∈ Lp (D), ∃ξ a simple function as above such As $1 < p \leqslant \infty$ $L ^ { p ^ { \prime } } ( D )$ , the simple functions of the above form consist of a dense $\begin{array} { r } { p ^ { \prime } = \frac { p } { p - 1 } } \end{array}$ $\forall f \in L ^ { p ^ { \prime } } ( D )$ $\exists \xi$ that $\parallel f - \xi \parallel _ { p ^ { \prime } } < \varepsilon / ( 2 \parallel \varphi \parallel _ { p } )$ . Therefore

$$
\left| \int_ {D} \varphi_ {k} \cdot f \right| \leqslant \| \varphi_ {k} \| _ {p} \| f - \xi \| _ {p ^ {\prime}} + \left| \int_ {D} \varphi_ {k} \cdot \xi \right| <   \varepsilon
$$

for $k$ large enough.

Remark 4.3.2 Lemma 4.3.1 also holds for $p = 1$ . In fact, the only problem in the above proof for $p = 1$ is that the simple functions $\xi$ cannot approximate any $f \in L ^ { \infty }$ in $L ^ { \infty }$ norm. But, one can choose $\xi$ such that $\parallel f - \xi \parallel _ { 1 } < \varepsilon$ , with $| \alpha _ { i } | \leqslant 2 \parallel f \parallel _ { \infty } \parallel _ { i }$ . Now, $\forall \lambda > 0$ , let $E _ { k , \lambda } = \{ x \in D | | \varphi _ { k } ( x ) | \geqslant \lambda \}$ , $\forall k$ . Since $\varphi _ { 1 } = \varphi \in L ^ { 1 } ( D )$ , ∀ε > 0, ∃λ = λ(ε) such that $\int _ { E _ { 1 , \lambda } } \left| \varphi \right| < \varepsilon$ . From the definition of $\varphi _ { k }$ , so is $\int _ { E _ { k , \lambda } } \left| \varphi _ { k } \right| < \varepsilon$ . Again,

$$
\left| \int_ {D} \varphi_ {k} \cdot f \right| \leqslant \int_ {D} | \varphi_ {k} (f - \xi) | + \left| \int_ {D} \varphi_ {k} \cdot \xi \right| = I + I I
$$

where $I I  0$ as $k  \infty$ , and

$$
\begin{array}{l} I = \int_ {D} | \varphi_ {k} (f - \xi) | \\ = \int_ {E _ {k, \lambda}} | \varphi_ {k} | | f - \xi | + \int_ {D \backslash E _ {k, \lambda}} | \varphi_ {k} (f - \xi) | \\ \leqslant 3 \| f \| _ {\infty} \cdot \int_ {E _ {k, \lambda}} | \varphi_ {k} | + \lambda \| f - \xi \| _ {1}. \\ \end{array}
$$

Again, we have proved that $\varphi _ { k }  \overline { { \varphi } }$ in $L ^ { 1 } ( D )$ .

Corollary 4.3.3 Let $\Omega = ( 0 , 1 ) , \ 0 < \lambda < 1 , \ \alpha , \ \beta \in \mathbb { R } ^ { 1 }$ , and

$$
\varphi (x) = \left\{ \begin{array}{l l} \alpha & i f x \in (0, \lambda) \\ \beta & i f x \in (\lambda , 1)  , \end{array} \right.
$$

then $\varphi _ { k }  ( *  ) \lambda \alpha + ( 1 - \lambda ) \beta$ in $L ^ { p }$ , $p \in [ 1 , \infty ) , ( p = \infty _ { \mathrm { { \ O } } }$ , resp.).

Now, we turn to studying the s.w.l.s.c. (or s.w∗.l.s.c.) of $J$ in $W ^ { 1 , p } ( \Omega ) , 1 \le$ $p \leq \infty$ . For simplicity, we assume $D = [ 0 , 1 ] ^ { n }$ , and $f = f ( \xi )$ . $\forall k \in \mathcal N$ , let $D _ { l } ^ { k }$ be a sub-cube of $D$ with length $2 ^ { - k }$ on each side and centered at $c _ { l } ^ { k } = 2 ^ { - k } ( y _ { 1 } ^ { l } +$ $\frac { 1 } { 2 } , \ldots , y _ { n } ^ { l } + \frac { 1 } { 2 } )$ , where $( y _ { 1 } ^ { l } , \ldots , y _ { n } ^ { l } )$ runs over the lattice points $( 0 , 1 , \ldots , 2 ^ { k } -$ $1 ) ^ { n } , \ l = 1 , \ldots , 2 ^ { k n }$ . Then $D = \bigcup _ { l = 1 } ^ { 2 ^ { k n } } D _ { l } ^ { k } , \forall k \in \mathcal { N } . \forall v \in C _ { 0 } ^ { \infty } ( D , \mathbb { R } ^ { N } )$ $\forall v \in C _ { 0 } ^ { \infty } ( D , \mathbb { R } ^ { N } )$ . Let us define

$$
w _ {k} (x) = \frac {1}{2 ^ {k}} v \left(2 ^ {k} \left(x - c _ {l} ^ {k}\right)\right), \forall x \in D _ {l} ^ {k}, \forall l = 1, \dots , 2 ^ {k n}. \tag {4.25}
$$

Then

$$
\nabla w _ {k} (x) = \nabla v \left(2 ^ {k} \left(x - c _ {l} ^ {k}\right)\right) \quad \forall x \in D _ {l} ^ {k}, \forall l = 1, \dots , 2 ^ {k n}
$$

and

$$
\left\{\begin{array}{l}w _ {k} \to 0 \quad L ^ {\infty} (D)  ,\\\nabla w _ {k} \rightharpoonup (^ {*} \rightharpoonup) 0 \text {i n} L ^ {p} (D)   p \in [ 1, \infty) (p = \infty \text {r e s p .})  .\end{array}\right.
$$

provided by Lemma 4.3.1. Now we arrive at:

Lemma 4.3.4 Let $\Omega \subset \mathbb { R } ^ { n }$ be a domain, and let $\begin{array} { r } { J ( u ) = \int _ { \Omega } f ( \nabla u ) } \end{array}$ . If $J$ i s s.w.l.s.c. on $W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { N } )$ ， $1 \leqslant p < \infty$ (s.w∗l.s.c. on $W ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } ) )$ , then for $\forall A \in M ^ { n \times N }$ , $\forall$ cube $D \subset \overline { { D } } \subset \Omega$ , we have

$$
f (A) \leqslant \frac {1}{m (D)} \int_ {D} f (A + \nabla v) \quad \forall v \in W _ {0} ^ {1, \infty} (D, \mathbb {R} ^ {N}),
$$

where $M ^ { n \times N }$ denotes the $n \times N$ matrix space.

Proof. We choose a sequence in the Banach space $X = W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { N } )$ as follows:

$$
u _ {k} (x) = A x + w _ {k} (x) \quad k = 1, 2, \dots ,
$$

where the sequence $\{ w _ { k } \}$ is defined in (4.25) and equals to zero outside $D$ . Then it weakly converges to $u ( x ) = A x$ ( $w ^ { * }$ -converges as $p = \infty$ ,). It follows that

$$
m (\Omega) f (A) = J (u) \leqslant \lim  _ {k \rightarrow \infty} \inf  J \left(u _ {k}\right).
$$

But

$$
\begin{array}{l} J (u _ {k}) = \int_ {D} f (A + \nabla w _ {k} (x)) d x + \int_ {\Omega \backslash D} f (A) d x \\ = \sum_ {l = 1} ^ {2 ^ {k n}} \int_ {D _ {l} ^ {k}} f (A + \nabla v (2 ^ {k} (x - c _ {l} ^ {k}))) d x + f (A) m (\Omega \backslash D) \\ = \int_ {D} f (A + \nabla v (x)) d x + f (A) m (\Omega \backslash D). \\ \end{array}
$$

This is our conclusion.

According to Lemma 4.3.4, we introduce the following:

Definition 4.3.5 A Borel measurable and locally integrable function $f$ : $\mathbb { R } ^ { n N } \quad \to \quad \mathbb { R } ^ { 1 }$ is called quasi-convex (in the Morrey sense), if for $\forall A \in$ $M ^ { n \times N } , \ \forall v \in W _ { 0 } ^ { 1 , \infty } ( D , \mathbb { R } ^ { N } )$ , and for any cube $D \subset \mathbb { R } ^ { \ltimes }$ , the inequality

$$
m (D) f (A) \leqslant \int_ {D} f (A + \nabla v (x)) d x \tag {4.26}
$$

holds.

In the sequel of this chapter, we shall specify the terminology “quasiconvexity” to be quasi-convexity in the Morrey sense.

Remark 4.3.6 The cube $D$ in the above definition can be replaced by any bounded domains, from a simple scaling and covering argument.

Let us study the relationship between the convexity and the quasiconvexity.

On the one hand, according to Jessen’s inequality, it is easy to see that if $f$ is convex, then

$$
\frac {1}{m (D)} \int_ {D} f (A + \nabla v (x)) d x \geqslant f \left(\frac {1}{m (D)} \int_ {D} (A + \nabla v (x)) d x\right) = f (A);
$$

i.e., convexity $\Longrightarrow$ quasi-convexity.

On the other hand, $\forall B , C \in M ^ { n \times N }$ , with rank $( B - C ) = 1$ , $\forall \lambda \in ( 0 , 1 )$ , let $A = \lambda B + ( 1 - \lambda ) C$ . After suitable translation and rotation, we may assume $\boldsymbol { B } = ( 1 - \lambda ) \boldsymbol { a } \otimes \boldsymbol { e } _ { 1 }$ and $C = - \lambda a \otimes e _ { 1 }$ for some $a \in \mathbb { R } ^ { N }$ and $\boldsymbol { e } _ { 1 } \in \mathbb { R } ^ { n }$ . Let us introduce a 1-periodic sawtooth function:

$$
\varphi (t) = \left\{ \begin{array}{l l} (1 - \lambda) t, & t \in [ 0, \lambda ], \\ - \lambda (t - 1), & t \in [ \lambda , 1 ]. \end{array} \right.
$$

For $\forall x \in D = [ 0 , 1 ] ^ { n }$ , let

$$
u _ {k} (x) = a k ^ {- 1} \varphi (k x ^ {1}),
$$

then

$$
\nabla u _ {k} (x) = a \otimes e _ {1} \left\{ \begin{array}{l l} (1 - \lambda) & \{k x ^ {1} \} \in (0, \lambda) \\ - \lambda & \{k x ^ {1} \} \in (\lambda , 1), \end{array} \right.
$$

where $\{ y \}$ denotes the fractional part of $y \in \mathbb { R } ^ { 1 }$ ; and let

$$
v _ {k} (x) = a \min  \left\{k ^ {- 1} \varphi \left(k x ^ {1}\right), \operatorname {d i s t} (x, \partial D) \right\},
$$

where dist (x, ∂D) = inf  sup xi yi y = (y1, . . . , yn)  ∂D . Then we $\operatorname { d i s t } \left( x , \partial D \right) = \operatorname* { i n f } \big \{ \operatorname* { s u p } _ { 1 \leqslant i \leqslant n } \| x _ { i } - y _ { i } \| \big | y = ( y _ { 1 } , \ldots , y _ { n } ) \in \partial D \big \}$ have $v _ { k } | _ { \partial D } = 0$ and there is a constant $K > 0$ such that $| v _ { k } ( x ) - v _ { k } ( y ) | \leq$ $\| x - y \|$ . Therefore $v _ { k } \in W _ { 0 } ^ { 1 , p } ( D , R ^ { N } )$ . Furthermore, we have

$$
m \{x \in D | \nabla u _ {k} (x) \neq \nabla v _ {k} (x) \} \rightarrow 0.
$$

as $k \to \infty$

Let us divide $D$ into two disjoint parts $D _ { 1 }$ and $D _ { 2 }$ , where $D _ { 1 } = \{ x \in$ $D | \nabla u _ { k } ( x ) \ : = \ : ( 1 - \lambda ) a \otimes e _ { 1 } \}$ and $D _ { 2 } = \{ x \in D | \nabla u _ { k } = - \lambda a \otimes e _ { 1 } \}$ . Thus $D = D _ { 1 } \cup D _ { 2 }$ .

If $f$ is quasi-convex, then

$$
m (D) f (\lambda B + (1 - \lambda) C) \leq \int_ {D} f (\lambda B + (1 - \lambda) C + \nabla v _ {k}).
$$

On the account of the local integrability of $f$ and the absolute continuity of the integral, we have

$$
\begin{array}{l} \lim  \int_ {D} f (\lambda B + (1 - \lambda) C + \nabla v _ {k}) = \lim  \int_ {D} f (\lambda B + (1 - \lambda) C + \nabla u _ {k}) \\ = \lim  \int_ {D _ {1}} f (B) + \int_ {D _ {2}} f (C) \\ = \lambda m (D) f (B) + (1 - \lambda) m (D) f (C). \\ \end{array}
$$

Finally, we obtain:

$$
f (\lambda B + (1 - \lambda) C) \leqslant \lambda f (B) + (1 - \lambda) f (C),
$$

i.e., $f$ is convex along segments connecting two matrices with rank-1 difference.

Definition 4.3.7 A function $f : \mathbb { M } ^ { n \times N }  \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is said to be rank one convex, if

$$
f (\lambda B + (1 - \lambda) C) \leqslant \lambda f (B) + (1 - \lambda) f (C)
$$

$\forall \lambda \in [ 0 , 1 ]$ , $\forall B , C \in \mathbb { M } ^ { n \times N }$ with rank $\{ B - C \} \leqslant 1$

As a direct consequence, we see that either $n = 1$ or $N = 1$ , rank one convex = convex.

In summary, we have proved:

convex =⇒ quasi-convex $\Longrightarrow$ rank one convex.

In particular, either $n = 1$ or $N = 1$ , quasi-convex = convex.

A natural question: are there any counterexamples of these reverse implications?

1. Quasi-convex in Morrey sense $\nRightarrow$ convex. For $A \ \in \mathbb { M } ^ { n \times n }$ , $\mathrm { d e t } { \cal A }$ is quasi-convex in Morrey sense, but not convex.

It is easily seen that $\operatorname { d e t } A$ is not convex, even for $n = 2$ .

Let us show that it is quasi-convex in Morrey sense. More precisely, we have

$$
\frac {1}{m (D)} \int_ {D} \det  (A + \nabla v) = \det  (A) \quad \forall D \subset \Omega ,   \forall v \in C _ {0} ^ {\infty} (D, \mathbb {R} ^ {n})  .
$$

For simplifying the computation, we only verify the cases $n \leqslant 3$ . Higherdimension cases can be shown by induction.

For $n = 2$ , let

$$
A = \left( \begin{array}{c c} a _ {1 1} & a _ {1 2} \\ a _ {2 1} & a _ {2 2} \end{array} \right),   v = \left( \begin{array}{c} v _ {1} \\ v _ {2} \end{array} \right) \in C _ {0} ^ {\infty} (D, \mathbb {R} ^ {2})  .
$$

From

$$
\det  (\nabla v) = \partial_ {x _ {1}} (v _ {1} \partial_ {x _ {2}} v _ {2}) - \partial_ {x _ {2}} (v _ {1} \partial_ {x _ {1}} v _ {2}),
$$

we have

$$
\int_ {D} \det (\nabla v) = 0,
$$

and then

$$
\begin{array}{l} \frac {1}{m (D)} \int_ {D} \det  (A + \nabla v) \\ = \frac {1}{m (D)} \int_ {D} [ \det  (A) + a _ {1 1} \partial_ {x _ {2}} v _ {2} + a _ {2 2} \partial_ {x _ {1}} v _ {1} - a _ {1 2} \partial_ {x _ {1}} v _ {2} - a _ {2 1} \partial_ {x _ {2}} v _ {1} + \det  (\nabla v) ] \\ = \det  (A). \\ \end{array}
$$

For $n = 3$ , let $A = ( a _ { i j } ) _ { 1 \leqslant i , j \leqslant 3 }$ , $\mathrm { a d j } _ { 2 } ( A ) = ( A _ { i j } ) _ { 1 \leqslant i , j \leqslant 3 }$ where $A _ { i j }$ is the $( i j )$ minor of $A$ ,

$$
\begin{array}{l} \det  (A + \nabla v) = \langle (A + \nabla v) ^ {1}, (\operatorname {a d j} _ {2} (A + \nabla v)) ^ {1} \rangle \\ = \langle A ^ {1}, (\operatorname {a d j} _ {2} (A + \nabla v)) ^ {1} \rangle + \langle (\nabla v) ^ {1}, (\operatorname {a d j} _ {2} (A + \nabla v)) ^ {1} \rangle \\ = \sum_ {i = 1} ^ {3} \left[ A _ {i} ^ {1} (\operatorname {a d j} _ {2} (A + \nabla v)) _ {i} ^ {1} + \frac {\partial v _ {1}}{\partial x _ {i}} (\operatorname {a d j} _ {2} (A + \nabla v)) _ {i} ^ {1} \right], \\ \end{array}
$$

where $B ^ { 1 } = ( b _ { 1 1 } , \ldots , b _ { 1 n } )$ is the first row vector of the matrix $B = ( b _ { i j } ) _ { 1 \leqslant i , j \leqslant n }$ , and $\langle , \rangle$ is the scalar product in $\mathbb { R } ^ { n }$ . By the conclusion for case $n = 2$ , we have

$$
\begin{array}{l} \frac {1}{m (D)} \sum_ {i = 1} ^ {3} \int_ {D} A _ {i} ^ {1} (\mathrm {a d j} _ {2} (A + \nabla v)) _ {i} ^ {1} \\ = \frac {1}{m (D)} \sum_ {i = 1} ^ {3} A _ {i} ^ {1} (\operatorname {a d j} _ {2} (A)) _ {i} ^ {1} \\ = \det  (A). \\ \end{array}
$$

Since

$$
\begin{array}{l} \sum_ {i = 1} ^ {3} \frac {\partial}{\partial x _ {i}} (\mathrm {a d j} _ {2} (A + \nabla v)) _ {i} ^ {1} \\ = \frac {\partial}{\partial x _ {1}} \left| \begin{array}{c c} \frac {\partial v _ {2}}{\partial x _ {2}} & \frac {\partial v _ {2}}{\partial x _ {3}} \\ \frac {\partial v _ {3}}{\partial x _ {2}} & \frac {\partial v _ {3}}{\partial x _ {3}} \end{array} \right| + \frac {\partial}{\partial x _ {2}} \left| \begin{array}{c c} \frac {\partial v _ {2}}{\partial x _ {3}} & \frac {\partial v _ {2}}{\partial x _ {1}} \\ \frac {\partial v _ {3}}{\partial x _ {3}} & \frac {\partial v _ {3}}{\partial x _ {1}} \end{array} \right| + \frac {\partial}{\partial x _ {3}} \left| \begin{array}{c c} \frac {\partial v _ {2}}{\partial x _ {1}} & \frac {\partial v _ {2}}{\partial x _ {2}} \\ \frac {\partial v _ {3}}{\partial x _ {1}} & \frac {\partial v _ {3}}{\partial x _ {2}} \end{array} \right| \\ = 0, \\ \end{array}
$$

after integration by parts, it follows that

$$
\int_ {D} \sum_ {i = 1} ^ {3} \frac {\partial v _ {1}}{\partial x _ {i}} (\mathrm {a d j} _ {2} (A + \nabla v)) _ {i} ^ {1} = 0.
$$

This proves the conclusion.

2. Rank one convex $\nRightarrow$ quasi-convex. There is a counterexample due to Sverak [Sv 1].

# 4.3.2 Morrey Theorem

The importance of the notion of quasi-convexity is due to the following:

Theorem 4.3.8 Suppose that $f ~ : ~ { \cal M } ^ { n \times N } ~ \longrightarrow ~ \mathbb { R } ^ { 1 }$ is continuous and quasiconvex. If the following growth condition holds:

$$
| f (A) | \leqslant \alpha (1 + | A |), \quad a s \quad p = 1,
$$

$$
\left(C _ {p}\right) - \alpha \left(1 + | A | ^ {q}\right) \leqslant f (A) \leqslant \alpha \left(1 + | A | ^ {p}\right), a s 1 \leqslant q <   p <   \infty ,
$$

$$
| f (A) | \leqslant \eta (| A |), a s p = + \infty ,
$$

where $\eta$ is continuous and increasing, and $\alpha > 0$ , $\forall A \in M ^ { n \times N }$ , then for every bounded open domain $\Omega \subset \mathbb { R } ^ { n }$ ,

$$
J (u) = \int_ {\Omega} f (\nabla u) d x
$$

is s.w.l.s.c. in $W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { N } )$ (s.w∗.l.s.c. in $W ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } ) )$ .

Before going to the proof, we need the piecewise affine function approximation in Sobolev spaces.

Definition 4.3.9 (Triangulation) Let $\Omega \subset R ^ { n }$ be a bounded domain with piecewise affine boundary. A triangulation $\ddot { \tau }$ of $\Omega$ is a collection of finitely many n-simplices $\{ K _ { i } | i = 1 , 2 , \ldots , I \}$ , such that $\forall i \neq j , K _ { i } \cap K _ { j }$ is either empty or equal to a $p$ -simplex, $0 \leq p \leq n - 1$ , and $\begin{array} { r } { \Omega = \bigcup _ { 0 \leq i \leq I } K _ { i } } \end{array}$ .

We call $h _ { \tilde { \tau } } = \operatorname* { m a x } \{ \dim ( K _ { i } ) | i = 1 , \dots , n \}$ ≤ ≤ the mesh size of $\ddot { \tau }$ .

To an $n$ -simplex, $K = \{ p _ { 0 } , p _ { 1 } , . . . , p _ { n } \}$ , (in which, $p _ { i } \in R ^ { n } , i = 0 , 1 , . . . , n$ , and $\{ p _ { i } - p _ { 0 } | i = 1 , 2 , \ldots , n \}$ are linearly independent). We define $n + 1$ affine functions $\{ \lambda _ { 0 } , \lambda _ { 1 } , . . . , \lambda _ { n } \}$ such that

$$
\lambda_ {i} (p _ {j}) = \delta_ {i j}, i, j = 0, 1, \dots , n.
$$

$\forall v \in C ( K )$ , we define a piecewise affine function by the following interpolation formula:

$$
\tilde {v} _ {K} (x) = \sum_ {i = 0} ^ {n} v \left(p _ {i}\right) \lambda_ {i} (x). \tag {4.27}
$$

One shows that $\forall v \in C ^ { 2 } ( K ) , \forall x \in K$ ,

$$
\left\| \nabla v - \nabla \tilde {v} _ {K} \right\| _ {\infty} \leq \frac {n ^ {2} (n + 1)}{2} \frac {h _ {K} ^ {2}}{\rho_ {K}} \| v \| _ {C ^ {2} (K)}, \tag {4.28}
$$

where $h _ { K } = \dim K$ , and $\rho _ { K } = \operatorname* { s u p } \{ 2 R | B _ { R } ( x ) \subset K , x \in K \}$ .

In fact, let $p _ { i } = ( p _ { i } ^ { 1 } , \ldots , p _ { i } ^ { n } ) , i = 0 , 1 , \ldots , n$ . We consider the functions $v _ { j } ( x ) = x ^ { j } , j = 1 , 2 , \ldots , n$ , where $x = ( x ^ { 1 } , \ldots , x ^ { n } )$ , and $v _ { 0 } ( x ) \equiv 1$ respectively. We obtain:

$$
x ^ {j} = \sum_ {0} ^ {n} p _ {i} ^ {j} \lambda_ {i} (x), j = 1, 2, \ldots , n, \mathrm {a n d} \sum_ {0} ^ {n} \lambda_ {i} (x) = 1.
$$

Differentiating these formulas, it follows that

$$
\begin{array}{l} \sum_ {i = 0} ^ {n} p _ {i} ^ {j} \frac {\partial \lambda_ {i}}{\partial x ^ {k}} = \delta_ {k} ^ {j}, j, k = 1, 2, \dots , n, \\ \sum_ {i = 0} ^ {n} \frac {\partial \lambda_ {i}}{\partial x ^ {k}} = 0, k = 1, 2, \dots , n. \\ \end{array}
$$

Thus by Taylor expansion,

$$
\begin{array}{l} \frac {\partial \tilde {v} _ {K} (x)}{\partial x ^ {k}} = \sum_ {i = 0} ^ {n} v (p _ {i}) \frac {\partial \lambda_ {i} (x)}{\partial x ^ {k}} \\ = \sum_ {i = 0} ^ {n} \left[ v (x) + \nabla v (x) \left(p _ {i} - x\right) + \frac {1}{2} \nabla^ {2} v \left(\xi_ {x}\right) \left(p _ {i} - x\right) ^ {2} \right] \frac {\partial \lambda_ {i} (x)}{\partial x ^ {k}} \\ = \frac {\partial v (x)}{\partial x ^ {k}} + \sum_ {i = 0} ^ {n} R _ {i} (x) \frac {\partial \lambda_ {i} (x)}{\partial x ^ {k}}, \\ \end{array}
$$

where $\xi _ { x } \in K$ , and $\begin{array} { r } { R _ { i } ( x ) = \frac 1 2 \nabla ^ { 2 } v ( \xi _ { x } ) ( p _ { i } - x ) ^ { 2 } } \end{array}$ . Since

$$
| R _ {i} (x) | \leq \frac {n ^ {2}}{2} \| v \| _ {C ^ {2}} h _ {K} ^ {2} \forall i,
$$

and $\begin{array} { r } { | \nabla \lambda _ { i } | \le \frac { 1 } { \rho _ { K } } } \end{array}$ . We obtain the desired estimate (4.28).

KA triangulation $\ddot { \tau }$ is called regular, if $\Finv$ a constant $C > 0$ , such that $\begin{array} { r } { \frac { h _ { K } } { \rho _ { K } } \leq } \end{array}$ $C , \forall K \in \tilde { \tau }$ .

Lemma 4.3.10 Suppose that $\Omega \subset R ^ { n }$ is a bounded domain with piecewise affine boundary. Then for $\forall u \in W ^ { 1 , p } ( \Omega , R ^ { N } ) , \forall \epsilon > 0 , \exists$ a piecewise affine function $v$ on $\Omega$ , satisfying:

1. $\| u - v \| _ { 1 , p } < \epsilon$   
2. $| \nabla v ( x ) | \leq | \nabla u ( x ) |$ , and $| v ( x ) | \leq | u ( x ) |$ a.e.

Proof. With the aid of the extension operator and the mollifier, we have a function $\tilde { u } \in C ^ { 2 } ( \overline { { \Omega } } , R ^ { N } )$ , such that

1. $\| \tilde { u } - u \| _ { 1 , p } < \frac { \epsilon } { 3 }$   
2. $| \tilde { u } ( x ) | \leq | u ( x ) | , | \nabla \tilde { u } ( x ) | \leq | \nabla u ( x ) | ~ \mathrm { a . e }$   
3. $\| \tilde { u } \| _ { C ^ { 2 } ( \overline { { \Omega } } ) } \leq C \| u \| _ { 1 , p }$

where $C > 0$ is a constant depending on $\Omega$

To any regular triangulation $\ddot { \tau }$ of $\Omega$ , we define a linear operator $\pi$ from $W ^ { 1 , p } ( \Omega , R ^ { N } )$ into itself as follows:

$$
(\pi u) (x) = \tilde {u} _ {K} (x) \forall x \in K, \forall K \in \tilde {\tau},
$$

where $\overset { \vartriangle } { \boldsymbol { u } } _ { K }$ is defined as in equation (4.27).

From $| ( \pi u ) ( x ) | \leq | u ( x ) |$ , and $| \nabla ( \pi u ) ( x ) | \leq | \nabla u ( x ) | a . e .$ , it follows that $\| \pi \| \leq 1$ . On account of (4.28) $\| \tilde { u } - \pi \tilde { u } \| _ { 1 , p }$ is small as $h _ { \tilde { \tau } } ~ > ~ 0$ is small. Therefore,

$$
\left\| u - \pi u \right\| _ {1, p} \leq \left\| u - \tilde {u} \right\| _ {1, p} + \left\| \tilde {u} - \pi \tilde {u} \right\| _ {1, p} + \left\| \pi \tilde {u} - \pi u \right\| _ {1, p} <   \epsilon .
$$

Set $v = \pi u$ . This is what we need.

Now, let us return to the proof of Theorem 4.3.8 for the case $p = \infty$ . Other cases are similar, so are omitted.

Proof. Suppose $u \in W ^ { 1 , \infty } ( \Omega , R ^ { N } )$ and $u _ { j } \ ^ { * }  u$ in $W ^ { 1 , \infty }$ , we shall prove that

$$
\lim  \inf  \int_ {\Omega} f (\nabla u _ {j}) \geq \int_ {\Omega} f (\nabla u).
$$

We may assume $| \nabla u _ { j } | , | \nabla u | \le C , M = \operatorname* { s u p } \{ f ( \xi ) | | \xi | \le 3 C + 1 \}$ . $\forall \epsilon > 0 \exists \delta >$ 0 such that $\begin{array} { r } { | f ( \xi ) - f ( \xi ^ { \prime } ) | \le \frac { \epsilon } { 6 m ( \Omega ) } } \end{array}$ as $| \xi - \xi ^ { \prime } | \le \delta$ and $| \xi | , | \xi ^ { \prime } | \leq 3 C .$ One chooses $\Omega _ { 1 } \subset \overline { { \Omega _ { 1 } } } \subset \Omega$ , in which $\partial \Omega _ { 1 }$ is piecewise affine with $\begin{array} { r } { m ( \Omega \backslash \Omega _ { 1 } ) < \frac { \epsilon } { 8 M } } \end{array}$ . There exists a regular triangulation of $\Omega _ { 1 } : \tau _ { k } = \{ D _ { 1 } ^ { k } , \ldots , D _ { I _ { k } } ^ { k } \} , \exists v _ { k } \in W ^ { 1 , \infty } ( \Omega , R ^ { N } )$ such that $v _ { k }$ is a piecewise affine function on $\Omega _ { 1 }$ with $\ v _ { k } \ = \ u$ on $\partial \Omega$ , and $\| v _ { k } - u \| _ { 1 , \infty } < \epsilon$ . Let $v _ { k } = A _ { i } ^ { k } \ \forall x \in D _ { i } ^ { k }$ . This can be realized by considering a domain $\Omega _ { 2 } : \overline { { \Omega _ { 1 } } } \subset \Omega _ { 2 } \subset \overline { { \Omega _ { 2 } } } \subset \Omega$ , with a piecewise affine boundary $\partial \Omega _ { 2 }$ , which is parallel to $\partial \Omega _ { 1 }$ . We define $v _ { k }$ in $\Omega _ { 1 }$ according to Lemma 4.3.10, $v _ { k } ( x ) = u ( x )$ , $\forall x \in \Omega \backslash \Omega _ { 2 }$ , and linearly interpolate $u | _ { \partial \Omega _ { 2 } }$ and $v _ { k } | _ { \partial \Omega _ { 1 } }$ in $\Omega _ { 2 } \backslash \Omega _ { 1 }$ .

According to Lemma 4.3.10, we may choose $\tau _ { k }$ with $h _ { \tau _ { k } } \ \to \ 0$ such that $\nabla v _ { k }  \nabla u$ in measure.

Let $u _ { j k } = v _ { k } + u _ { j } - u$ , then $u _ { j k } \stackrel { * } { \quad }  v _ { k }$ in $W ^ { 1 , \infty }$ as $j \to \infty$ ; and $| \nabla u _ { j k } | \le$ $_ { 3 C }$ . One has $k _ { 0 } \in \mathcal { N }$ , as $k > k _ { 0 }$ ,

$$
\int_ {\Omega} | f (\nabla v _ {k}) - f (\nabla u) | <   \frac {\epsilon}{6} \tag {4.29}
$$

and

$$
\int_ {\Omega} | f (\nabla u _ {j k}) - f (\nabla u _ {j}) | <   \frac {\epsilon}{6}. \tag {4.30}
$$

Fixing $k > k _ { 0 }$ , let $p _ { i } ^ { k }$ be the center of $D _ { i } ^ { k } , \forall r \in ( 0 , 1 )$ ; we consider a similar subsimplex $C _ { i } ^ { k } = r ( D _ { i } ^ { k } - \{ p _ { i } ^ { k } \} ) + p _ { i } ^ { k } , i = 1 , 2 , \ldots , I _ { k }$ , and $\begin{array} { r } { \Omega _ { 1 } ^ { \prime } = \bigcup _ { 1 } ^ { I _ { k } } C _ { i } ^ { k } . } \end{array}$ . Define a smooth function $\rho$ on $\Omega _ { 1 }$ , such that $\rho ( x ) = 1 \forall x \in \Omega _ { 1 } ^ { \prime } , \rho ( x ) = 0 \forall x \in$ $\cup _ { 1 } ^ { I _ { k } } \partial D _ { i } ^ { k }$ , and $| \nabla \rho | \leq C ( \tilde { \tau _ { k } } , r )$ , a constant depending on $\ddot { \tau _ { k } }$ and $r$ .

Define $v _ { j k } = v _ { k } + \rho ( u _ { j k } - v _ { k } ) = v _ { k } + \rho ( u _ { j } - u )$ ; we have

$$
\begin{array}{l} \int_ {\Omega_ {1}} f (\nabla u _ {j k}) = \int_ {\Omega_ {1} ^ {\prime}} f (\nabla v _ {j k}) + \int_ {\Omega_ {1} \backslash \Omega_ {1} ^ {\prime}} f (\nabla u _ {j k}) \\ = \int_ {\Omega_ {1}} f (\nabla v _ {j k}) + \int_ {\Omega_ {1} \backslash \Omega_ {1} ^ {\prime}} [ f (\nabla u _ {j k}) - f (\nabla v _ {j k}) ] \\ = A + B + C. \\ \end{array}
$$

On account of the quasi-convexity of $f$ ; we have

$$
\begin{array}{l} A = \int_ {\Omega_ {1}} f (\nabla v _ {j k}) = \sum_ {i = 1} ^ {I _ {k}} \int_ {D _ {i} ^ {k}} f \left(A _ {i} ^ {k} + \nabla [ \rho (u _ {j k} - u) ]\right) \\ \geq \sum_ {i = 1} ^ {I _ {k}} f \left(A _ {i} ^ {k}\right) m \left(D _ {i} ^ {k}\right) \\ = \int_ {\Omega_ {1}} f (\nabla v _ {k}) . \\ \end{array}
$$

It is easily seen that

$$
B = \int_ {\Omega_ {1} \backslash \Omega_ {1} ^ {\prime}} f (\nabla u _ {j k}) \geq - M (1 - r ^ {n}) m (\Omega_ {1}).
$$

As to $C$ , from $\| u _ { j } - u \| _ { \infty } \to 0$ as $j \to \infty$ , we obtain

$$
\| \nabla v _ {j k} \| _ {\infty} \leq \| \nabla v _ {k} \| _ {\infty} + \| \nabla u _ {j} \| _ {\infty} + \| \nabla u \| _ {\infty} + \| \nabla \rho \| _ {\infty} \| u _ {j} - u \| _ {\infty} \leq 3 C + 1,
$$

as $j > j _ { 0 } ( \tilde { \tau _ { k } } , r )$ . Thus

$$
\int_ {\Omega_ {1} \backslash \Omega_ {1} ^ {\prime}} f (\nabla v _ {j k}) \leq M (1 - r ^ {n}) m (\Omega_ {1}).
$$

Putting them together, we have

$$
\int_ {\Omega_ {1}} f (\nabla u _ {j k}) \geq \int_ {\Omega_ {1}} f (\nabla v _ {k}) - 2 M m (\Omega_ {1}) (1 - r ^ {n}), \tag {4.31}
$$

as $j > j _ { 0 } ( k , r )$ .

Combining (4.29), (4.30) and (4.31), and suitably choosing $r \in ( 0 , 1 )$ , we have

$$
\begin{array}{l} \int_ {\Omega} f (\nabla u _ {j}) \geq \int_ {\Omega \backslash \Omega_ {1}} f (\nabla u _ {j k}) + \int_ {\Omega_ {1}} f (\nabla u _ {j k}) - \frac {\epsilon}{6} \\ \geq \int_ {\Omega} f (\nabla v _ {k}) + \int_ {\Omega \backslash \Omega_ {1}} [ f (\nabla u _ {j k}) - f (\nabla v _ {k}) ] - \frac {\epsilon}{6} - 2 M (1 - r ^ {n}) m (\Omega) \\ \geq \int_ {\Omega} f (\nabla u) - \epsilon , \\ \end{array}
$$

as $j > j _ { 0 } ( k , r )$ .

Since $\epsilon > 0$ is arbitrarily small, we have proved the s.w∗.l.s.c. of $J$ .

Remark 4.3.11 Theorem 4.3.8 is initially due to Morrey under some additional conditions. It was refined by Meyers, and greatly improved by Acerbi and Fusco [AF] and Marcellini [Mar]. In the final version, $f$ can be a Caratheodory function: $\overline { { \Omega } } \times \mathbb { R } ^ { N } \times \mathbb { M } ^ { n \times N } \longrightarrow \mathbb { R } ^ { 1 }$ , and ( $C _ { p }$ ) is replaced by the following:

$$
\left| f (x, u, A) \right| \leqslant \alpha (1 + | u | + | A |) \quad a s p = 1 ,
$$

$$
- \alpha (1 + | u | ^ {r} + | A | ^ {q}) \leqslant f (x, u, A) \leqslant \alpha (1 + | u | ^ {r} + | A | ^ {p}) a s 1 <   p \leqslant n,
$$

where $1 \leqslant q < p$ , and $\textstyle r \in [ 1 , { \frac { n p } { n - p } } ]$

$$
- \alpha (1 + | A | ^ {q}) \leqslant f (x, u, A) \leqslant \alpha (1 + | A | ^ {p}) a s n <   p <   \infty ,
$$

where $1 \leq q < p$ , and

$$
f (x, u, A) \leqslant \eta (x, | u |, | A |) a s p = \infty ,
$$

where $\eta$ is an increasing function in each of its argument.

Theorem 4.3.12 Suppose that $f : \mathbb { M } ^ { n \times N } \longrightarrow \mathbb { R } ^ { 1 }$ is continuous, quasi-convex and satisfying

$$
c | A | ^ {p} \leqslant f (A) \leqslant C (1 + | A | ^ {p}) \quad f o r C > c > 0,
$$

for $1 < p < \infty$ . Then for $\forall v \in W ^ { 1 , p } ( \Omega , R ^ { N } )$ , the functional

$$
J (u) = \int_ {\Omega} f (\nabla u)
$$

achieves its minimum on $E = W _ { v } ^ { 1 , p } : = \{ u \in W ^ { 1 , p } ( \Omega , R ^ { N } ) | \ u | _ { \partial \Omega } = v | _ { \partial \Omega } \} .$

Proof. In fact, $J$ is s.w.l.s.c. and coercive, and $E$ is weakly closed.

# 4.3.3 Nonlinear Elasticity

The body of a given elastic material occupies a bounded domain $\Omega$ in $\mathbb { R } ^ { 3 }$ ; it is called tformation $u : \Omega \to \mathbb { R } ^ { 3 }$ configuration. Ext. The gradient of $u$ r, $\begin{array} { r } { \nabla u = ( \frac { \partial u ^ { 2 } } { \partial x _ { \alpha } } ) _ { 1 \leq i , \alpha \leq 3 } } \end{array}$ material a de- is called the αdeformation gradient. To the hyper-elastic materials, a stored energy density function $W : \overline { { \Omega } } \times \mathbb { M } _ { + } ^ { 3 \times 3 } \to \mathbb { R } ^ { 1 }$ is introduced with certain symmetric properties (e.g., it is left invariant under rotations and right invariant under certain isotropic groups), where $\mathbb { M } _ { + } ^ { 3 \times 3 }$ denotes all $3 \times 3$ matrices with positive determinants. The elastic energy reads as

$$
I (u) = \int_ {\Omega} W (x, \nabla u) d x.
$$

The basic assumption of the variational approach to this problem is that the observed deformations correspond to minimizers of the elastic energy.

After the reductions from these invariances, the stored energy density is, in fact, dependent on the determinant, the minors and the eigenvalues of the deformation gradient $\nabla u$ only.

A function $f : \mathbb { M } ^ { n \times N }  \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ is called poly-convex if there exists $g : \mathbb { R } ^ { \tau ( n , N ) } \longrightarrow \mathbb { R } ^ { 1 }$ convex such that

$$
f (A) = g (T (A))
$$

where

$$
\tau (n, N) = \sum_ {s = 1} ^ {n \wedge N} \frac {n ! N !}{(s !) ^ {2} (n - s) ! (N - s) !},
$$

and $T : \mathbb { M } ^ { n \times N } \longrightarrow \mathbb { R } ^ { \tau ( n , N ) }$ is of the form:

$$
T (A) = \left(A, \operatorname {a d j} _ {2} A, \dots , \operatorname {a d j} _ {n \wedge N} A\right),
$$

and $\operatorname { a d j } _ { s } A$ denotes the matrix of all $s \times s$ minors of $A$ .

Examples. Let $A = ( a _ { i j } ) \in \mathbb { M } ^ { n \times n }$ , then

$$
\parallel A \parallel^ {2} = \sum_ {i, j = 1} ^ {n} | a _ {i j} | ^ {2},
$$

$\operatorname* { d e t } \left( A \right) ,$

and

$$
\| \operatorname {a d j} _ {2} A \| ^ {p} p \geq 1,
$$

are poly-convex functions.

What are the relationships between poly-convexity and other convexities? We claim that

$$
\mathrm {c o n v e x} \Rightarrow \mathrm {p o l y - c o n v e x} \Rightarrow \mathrm {q u a s i - c o n v e x}.
$$

In fact, the first implication is trivial.

To verify the second implication, let us recall

$$
\frac {1}{m (D)} \int_ {D} \det  (B + \nabla v) = \det  (B), \forall B \in \mathbb {M} ^ {n \times n}, \forall D \subset \Omega , \forall v \in C _ {0} ^ {\infty} (D, R ^ {n}).
$$

It implies that $\forall A \in \mathbb { M } ^ { n \times n }$ ,

$$
\frac {1}{m (D)} \int_ {D} \operatorname {a d j} _ {s} (A + \nabla v) = \operatorname {a d j} _ {s} (A) \quad 2 \leqslant s \leqslant n,
$$

and then

$$
\frac {1}{m (D)} \int_ {D} T (\boldsymbol {A} + \nabla v) = T (\boldsymbol {A}).
$$

Thus if $f$ is poly-convex, and $f ( A ) = g ( T ( A ) )$ , where $g$ is convex, then by Jessen’s inequality

$$
\begin{array}{l} \frac {1}{m (D)} \int_ {D} f (A + \nabla v) = \frac {1}{m (D)} \int_ {D} g (T (A + \nabla v)) \\ \geqslant g \left(\frac {1}{m (D)} \int_ {D} T (A + \nabla v)\right) \\ \end{array}
$$

$$
\begin{array}{l} = g (T (A)) \\ = f (A), \\ \end{array}
$$

i.e., $f$ is quasi-convex.

In summary, we have the following implications:

convex =⇒ poly-convex =⇒ quasi-convex =⇒ rank-one convex .

One may ask: Are these implications strict?

The following example is due to Dacorogna and Marcellini [DM]:

$A \in \mathbb { M } ^ { 2 \times 2 }$ , $f ( A ) = \parallel A \parallel ^ { 4 } - \gamma \parallel A \parallel ^ { 2 } \operatorname* { d e t } A$ , then

$$
\begin{array}{l} f \text {i s} | \gamma | \leqslant \frac {4}{3} \sqrt {2}, \\ f \text {i s} \quad \text {p o l y - c o n v e x} \Longleftrightarrow | \gamma | \leqslant 2, \\ f \text {i s} | \gamma | \leqslant 2 + \varepsilon , \text {f o r s o m e} \varepsilon > 0, \\ f \text {i s r a n k - o n e c o n v e x} \Longleftrightarrow | \gamma | \leqslant \frac {4}{\sqrt {3}}. \\ \end{array}
$$

It is not known whether $\textstyle 2 + \varepsilon = { \frac { 4 } { \sqrt { 3 } } }$ . In addition to the counterexample due to Sverak [Sv 1], all the reverse implications are false.

# 4.4 Relaxation and Young Measure

Not all variational integrands are quasi-convex. In this case a minimizing sequence may not converge to a minimizer, there may even be no minimizer!

For example, let $\Omega = ( 0 , 1 )$ , we consider the functional

$$
J (u) = \int_ {0} ^ {1} [ u ^ {2} (x) + (u ^ {\prime} (x) ^ {2} - 1) ^ {2} ] d x \tag {4.32}
$$

on the Sobolev space $W _ { 0 } ^ { \mathrm { 1 } , 4 } ( \Omega )$ . The integrand is not quasi-convex. It is easily seen that inf $J = 0$ . Indeed, on the one hand, we define a sequence of sawtooth functions:

$$
u _ {n} (x) = \left\{ \begin{array}{l l} x - \frac {k}{n} & \text {i f} x \in \left[ \frac {k}{n}, \frac {2 k + 1}{2 n} \right], \\ - x + \frac {k + 1}{n} & \text {i f} x \in \left[ \frac {2 k + 1}{n}, \frac {k + 1}{n} \right]. \end{array} \right.
$$

From $| u _ { n } ^ { \prime } ( x ) | = 1$ a.e., and $\begin{array} { r } { | u _ { n } | \leqslant \frac { 1 } { 2 n } } \end{array}$ , we conclude that

$$
J (u _ {n}) \leqslant \frac {1}{4 n ^ {2}} \rightarrow 0.
$$

On the other hand, $J \geqslant 0$ . Then $\{ u _ { n } \}$ is a minimizing sequence with inf $J = 0$

However, $u _ { n } \to \theta$ in $W _ { 0 } ^ { \mathrm { 1 } , 4 } ( \Omega )$ , because

$$
\int_ {0} ^ {1} u _ {n} ^ {\prime} \cdot \varphi = - \int_ {0} ^ {1} u _ {n} \cdot \varphi^ {\prime} \rightarrow 0, \forall \varphi \in C _ {0} ^ {\infty} (\Omega), \mathrm {a s} n \rightarrow 0.
$$

But $J ( \theta ) = 1$

Moreover, the functional does not have a minimizer in $W _ { 0 } ^ { 1 . 4 } ( \Omega )$ . Because, if $u _ { 0 } \in W _ { 0 } ^ { 1 , 4 } ( \Omega )$ achieves $J ( u _ { 0 } ) = 0$ , then $u _ { 0 } ( x ) = 0$ , a.e., and $u _ { 0 } ^ { \prime } ( x ) = 0$ , a.e., which imply that $J ( u _ { 0 } ) = 1$ . This is impossible.

Nevertheless, to this kind of problem, the minimizing sequences must provide some useful information in understanding the variational problems. There are several ways to studying these minimizing sequences:

(1) Relaxation method: Introduce a relaxed functional with a quasi-convex integrand. It shares a common minimizing sequence with $J$ .   
(2) Young measure: Extend the working Sobolev space to a measure space so that the minimizing sequences converge in some sense to a measure.   
(3) Change the working space to something other than the Sobolev space $W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { N } )$ .

# 4.4.1 Relaxations

In the first aspect, let us recall the relationship between s.w∗.l.s.c. and the quasi-convexity. We are inspired to study various convex envelopes for nonconvex functions.

Definition 4.4.1 Let $f : \mathbb { M } ^ { n \times N }  \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ , we call

$$
\begin{array}{l} C f = \sup  \left\{g \leqslant f \mid g c o n v e x \right\}, \\ P f = \sup  \left\{g \leqslant f \mid g p o l y - c o n v e x \right\}, \\ Q f = \sup  \left\{g \leqslant f \mid g q u a s i - c o n v e x \right\}, \\ R f = \sup  \left\{g \leqslant f \mid g r a n k - o n e c o n v e x \right\}, \\ \end{array}
$$

the convex, poly-convex, quasi-convex and rank-one convex envelope of $f$ , respectively.

Obviously,

(1) $C f$ , P f, Qf and $R f$ are convex, poly-convex, quasi-convex and rank-one convex functions, respectively.   
(2) $C f \leqslant P f \leqslant Q f \leqslant R f \leqslant f$ .

In particular, if $n = 1$ or $N = 1$ , then $C f = P f = Q f = R f$ . By definition, $f \geqslant C f \geqslant f ^ { * * }$ , the biconjugate function of $f$ . But one can show that if $f$ is proper, then $C f = f ^ { \ast \ast }$ . Since $Q f$ is quasi-convex, in addition to some growth condition, the associated functional

$$
\widetilde {J} (u) = \int_ {\Omega} (Q f) (x, u (x), \nabla u (x)) \tag {4.33}
$$

is s.w∗.l.s.c. in a certain Sovolev space $W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { N } ) , 1 < p \leq \infty$ . The following theorem is very basic in this aspect.

Theorem 4.4.2 Suppose that $f : \mathbb { M } ^ { n \times N } \ : \longrightarrow \ : \mathbb { R } ^ { 1 }$ is measurable and locally integrable, and satisfies

$$
C _ {2} | A | ^ {p} \leqslant f (A) \leqslant C _ {1} (1 + | A | ^ {p}) \tag {4.34}
$$

for some $C _ { 1 } > C _ { 2 } > 0$ and $p \in ( 1 , \infty )$ . For $\forall v \in W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { N } )$ , define $W _ { v } ^ { 1 , p } =$ $\{ u \in W ^ { 1 , p } ( \Omega , \mathbb { R } ^ { N } ) | \ u | _ { \partial \Omega } = v | _ { \partial \Omega } \}$ , then

$$
\inf  \left\{J (u) | u \in W _ {v} ^ {1, p} \right\} = \inf  \left\{\widetilde {J} (u) | u \in W _ {v} ^ {1, p} \right\}.
$$

Moreover, a function $u _ { 0 } \in W _ { v } ^ { 1 , p }$ is a minimizer of $\hat { J }$ if and only if it is a cluster point (with respect to the weak topology of $W ^ { 1 , p }$ ) of a minimizing sequence for $J$ .

Before going to the proof of Theorem 4.4.2, we give a characterization of the quasi-convex envelope.

Lemma 4.4.3 If $f : \mathbb { M } ^ { n \times N } \longrightarrow \mathbb { R } ^ { 1 }$ is locally bounded and Borel measurable, then

$$
\left(Q f\right) (A) = \inf  \left\{\frac {1}{m (D)} \int_ {D} f (A + \nabla \varphi (x)) d x \mid \varphi \in W _ {0} ^ {1, \infty} (D, \mathbb {R} ^ {N}) \right\}, \tag {4.35}
$$

where $D \subset \mathbb { R } ^ { n }$ is a bounded domain with $m ( \partial D ) = 0$

Proof. We denote the RHS of (4.35) by $( q f ) ( A , D )$ . By definition, $f \ \leq \ g$ 7 implies that $q f \leq q g$ ; and if $f$ is quasi-convex, then $q f = f$ .

We want to show:

(1) $( q f ) ( A , D )$ is independent of $D$ , then we denote it by $( q f ) ( A )$ .   
(2) $( q f ) ( A )$ is quasi-convex.

We assume these two conclusions at this moment, then by the definition of the quasi-convex envelope,

$$
(q f) (A) \geqslant (q (Q f)) (A) = (Q f) (A) \forall A \in M ^ {n \times N}.
$$

From (2), $q f$ is quasi-convex and $q f \leqslant f$ . Since $Q f$ is the largest quasi-convex function $\leqslant f$ , it follows that $q f \leqslant Q f$ . Therefore

$$
q f = Q f.
$$

# 1. Verification of (1).

One observes that for any two bounded domains $D _ { 1 }$ and $D _ { 2 }$ , if after translation and scaling, $D _ { 1 }$ is transformed into $D _ { 2 }$ (denoted by $D _ { 1 } \sim D _ { 2 }$ ), then

$$
(q f) (A, D _ {1}) = (q f) (A, D _ {2}).
$$

This proves that $( q f ) ( A , D )$ are equal for all $D \sim D _ { 1 }$ . Moreover, if $D _ { 1 } \subset D _ { 2 }$ then $( q f ) ( A , D _ { 2 } ) \leq ( q f ) ( A , D _ { 1 } )$ .

Now for any two bounded domains $D _ { 1 }$ and $D _ { 2 }$ in $\mathbb { R } ^ { n }$ , we may assume that $D _ { 1 }$ is the unit cube. And for any $\varepsilon > 0$ , we find $\{ E _ { j } | \sim D _ { 1 } , \ j = 1 , 2 , . . . , m \}$ , with $E _ { i } \cap E _ { j } = \emptyset \ i \neq j$ and $m ( D _ { 2 } \backslash \bigcup _ { j = 1 } ^ { m } E _ { j } ) < \varepsilon$ . On each $E _ { j }$ one may find a ∪j=1 piecewise affine function $\varphi _ { j } ^ { \varepsilon } \in W _ { 0 } ^ { 1 , \infty } ( E _ { j } , \mathbb { R } ^ { n } )$ , such that

$$
\int_ {E _ {j}} f (A + \nabla \varphi_ {j} ^ {\varepsilon} (x)) d x \leqslant (\varepsilon + (q f) (A, D _ {1})) m (E _ {j}),
$$

and let

$$
\varphi^ {\varepsilon} (x) = \left\{ \begin{array}{l l} \varphi_ {j} ^ {\varepsilon} (x) & x \in E _ {j} \\ 0 & x \in D _ {2} \backslash \bigcup_ {j = 1} ^ {m} E _ {j}  . \end{array} \right.
$$

Thus

$$
\begin{array}{l} \int_ {D _ {2}} f (A + \nabla \varphi^ {\varepsilon} (x)) d x \leqslant \sum_ {j = 1} ^ {m} \int_ {E _ {j}} f (A + \nabla \varphi_ {j} ^ {\varepsilon} (x)) d x + \int_ {D _ {2} \setminus \bigcup_ {j = 1} ^ {m} E _ {j}} f (A) d x. \\ \leqslant \left(\varepsilon + (q f) (A, D _ {1})\right) \sum_ {j = 1} ^ {m} m (E _ {j}) + f (A) m \left(D _ {2} \backslash \bigcup_ {j = 1} ^ {m} E _ {j}\right). \\ \end{array}
$$

Since $\varepsilon > 0$ is arbitrary, it follows that

$$
(q f) (A, D _ {2}) \leqslant (q f) (A, D _ {1}).
$$

The positions of $D _ { 1 }$ and $D _ { 2 }$ are symmetric, thus

$$
(q f) (A, D _ {1}) = (q f) (A, D _ {2}),
$$

i.e., $( q f ) ( A , D )$ is independent of $D$ , and will be written as $( q f ) ( A )$ .

2. Verification of (2).

First, we verify:

$$
\frac {1}{m (D)} \int_ {D} (q f) (A + \nabla \psi (x)) d x \geqslant (q f) (A),
$$

for all piecewise affine $\psi \in W _ { 0 } ^ { 1 , \infty } ( D , \mathbb { R } ^ { N } )$ , where $D$ is the unit cube. In fact, let $D = \bigcup _ { i = 1 } ^ { m } D _ { i }$ , where $D _ { i } , i = 1 , \ldots , m$ are simplexes with $D _ { j } ^ { \circ } \cap D _ { i } ^ { \circ } = \cup \forall i \neq j$

and $\nabla \psi | _ { D _ { i } } = B _ { i } \forall i$ . By the definition of $q f$ , for $\forall \varepsilon > 0 , \exists \varphi _ { i } \in W _ { 0 } ^ { 1 , \infty } ( D _ { i } , \mathbb { R } ^ { N } )$ such that

$$
(q f) (A + \nabla \psi (x)) = (q f) (A + B _ {i}) \geqslant \frac {1}{m (D _ {i})} \int_ {D _ {i}} f (A + \nabla \psi + \nabla \varphi_ {i}) - \varepsilon , \forall x \in D _ {i}.
$$

One extends $\varphi _ { i }$ to be zero outside $D _ { i }$ , and define $\textstyle \varphi = \psi + \sum _ { i = 1 } ^ { m } \varphi _ { i }$ . Then $\varphi \in W _ { 0 } ^ { 1 , \infty } ( D , \mathbb { R } ^ { N } )$ , and

$$
\begin{array}{l} \int_ {D} (q f) (A + \nabla \psi (x)) d x = \sum_ {i = 1} ^ {m} \int_ {D _ {i}} (q f) (A + B _ {i}) \\ \geqslant \sum_ {i = 1} ^ {m} \int_ {D _ {i}} f (A + \nabla \varphi) - \varepsilon m (D) \\ = \int_ {D} f (A + \nabla \varphi) - \varepsilon m (D) \\ \geqslant m (D) \left[ \left(q f\right) (A) - \varepsilon \right]. \\ \end{array}
$$

Since $\varepsilon > 0$ is arbitrary, it follows that

$$
\begin{array}{l} \inf  \left\{\frac {1}{m (D)} \int_ {D} (q f) (A + \nabla \psi) d x \mid \psi \in W _ {0} ^ {1, \infty} (D, \mathbb {R} ^ {N}) \right. \\ \left. \text {p i e c e w i s e l i n e a r} \right\rbrace \geqslant (q f) (A). \tag {4.36} \\ \end{array}
$$

By the same procedure as in Sect. 4.3, we conclude that $( q f )$ is rank-one convex. Therefore $q f$ is continuous, and in fact, is locally Lipschitzian (Theorem 2.2.1, Sect. 2.2). Since piecewise affine functions are dense in $W _ { 0 } ^ { 1 , \infty } ( D , \mathbb { R } ^ { N } )$ , (4.36) is extended to all $\psi \in W _ { 0 } ^ { 1 , \infty } ( D , \mathbb { R } ^ { N } )$ . Thus $q f$ is quasi-convex. □

# Proof of Theorem 4.4.2.

Proof. 1. Since $Q f \leqslant f$ , inf $\{ \tilde { J } \} \leqslant \operatorname { i n f } \{ J \}$ . We only want to show that any minimizing sequence of $\scriptstyle { \hat { J } }$ is indeed a minimizing sequence of $J$ . Thus it is sufficient to prove that $\forall u \in W _ { v } ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } )$ $\forall u \in W _ { v } ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } ) \ \exists \{ u _ { l } \} _ { 1 } ^ { \infty } \subset W _ { v } ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } )$ such that

$$
J (u _ {l}) \rightarrow \tilde {J} (u) \quad \text {a s} l \rightarrow \infty .
$$

Due to the assumption (4.34) on $f$ and the density of piecewise affine functions in $W _ { 0 } ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } )$ , we may assume that $u$ is piecewise affine in a smaller domain $\Omega _ { 0 } \subset \Omega _ { 0 } \subset \Omega$ . By a standard argument, it is sufficient to replace $\Omega$ by $\Omega _ { 0 }$ .

2. Let $\Omega _ { 0 } = \bigcup _ { i = 1 } ^ { m } D _ { i }$ , where $D _ { i } , i = 1 , \ldots , m$ are simplexes with $D _ { i } ^ { \circ } \cap D _ { j } ^ { \circ } =$ $\mathrm { \Delta } \psi$ $i \neq j$ and let $\nabla \boldsymbol { u } | _ { D _ { i } } = B _ { i } \ \forall i$ . Then we choose small cubes $\{ C _ { i } ^ { ( j ) } \} _ { j = 1 } ^ { p _ { i } } \subset D _ { i }$ with $C _ { i } ^ { ( j ) \circ } \cap C _ { i } ^ { ( k ) \circ } = \emptyset$ as $j \neq k$ , and

$$
\int_ {D _ {i} \backslash \bigcup_ {j = 1} ^ {p _ {i}} C _ {i} ^ {(j)}} (1 + | \nabla u (x) |) ^ {p} d x \leqslant \frac {\varepsilon}{m C _ {1}}.
$$

We are going to construct a sequence $u _ { l }$ on each cube $C = C _ { i } ^ { ( j ) }$ . By Lemma 4.4.3, $\exists \varphi _ { l } \in W _ { 0 } ^ { 1 , \infty } ( C , \mathbb { R } ^ { N } )$ such that for $\forall x \in C$

$$
(Q f) (\nabla u (x)) \leqslant \frac {1}{m (C)} \int_ {C} f (\nabla u (x) + \nabla \varphi_ {l} (x)) d x \leqslant (Q f) (\nabla u (x)) + \frac {1}{l}. \tag {4.37}
$$

Since $C$ is a cube and $\varphi _ { l } \in W _ { 0 } ^ { 1 , \infty } ( C , \mathbb { R } ^ { N } )$ , we may extend $\varphi _ { l }$ periodically to $\mathbb { R } ^ { N }$ . Let

$$
u _ {l} (x) = u (x) + \frac {1}{l} \varphi_ {l} (l x), \quad \forall x \in C.
$$

Then $u _ { l } | _ { \partial C } = u | _ { \partial C }$ , and

$$
\begin{array}{l} \int_ {C} f (\nabla u _ {l} (x)) d x = \int_ {C} f (B _ {i} + \nabla \varphi_ {l} (l x)) d x \\ = \frac {1}{l ^ {n}} \int_ {l C} f (B _ {i} + \nabla \varphi_ {l} (y)) d y \\ = \int_ {C} f \left(B _ {i} + \nabla \varphi_ {l} (y)\right) d y. \tag {4.38} \\ \end{array}
$$

Combining (4.37) and (4.38), we obtain

$$
\int_ {C _ {i} ^ {(j)}} f (\nabla u _ {l} (x)) d x \rightarrow (Q f) (B _ {i}) m (C _ {i} ^ {(j)}),
$$

as $l  \infty$

3. “Putting them together” we define

$$
u _ {l} (x) = \left\{ \begin{array}{l l} u & x \in \Omega_ {0} \backslash \bigcup_ {i = 1} ^ {m} \bigcup_ {j = 1} ^ {p _ {i}} C _ {i} ^ {(j)}  , \\ u _ {l} (x) & x \in \bigcup_ {i = 1} ^ {m} \bigcup_ {j = 1} ^ {p _ {i}} C _ {i} ^ {(j)}  . \end{array} \right.
$$

It follows that

$$
\begin{array}{l} \int_{\Omega_{0}}f(\nabla u_{l}(x))dx = \int_{\Omega_{0}\setminus \bigcup_{i = 1}^{m}\bigcup_{j = 1}^{p_{i}}C_{i}^{(j)}}f(\nabla u(x))dx \\ + \sum_ {i = 1} ^ {m} \sum_ {j = 1} ^ {p _ {i}} m \left(C _ {i} ^ {(j)}\right) (Q f) \left(B _ {i}\right) + o (1) \\ = \int_ {\Omega_ {0}} (Q f) (\nabla u (x)) d x + o (1). \\ \end{array}
$$

After a diagonal process, we obtain

$$
J (u _ {l}) \rightarrow \widetilde {J} (u) \quad \text {a s} l \rightarrow \infty .
$$

4. If $u _ { 0 }$ is a minimizer of $\hat { J }$ , then by the above argument, we obtain a minimizing sequence $\{ u _ { l } \}$ of $J$ , with $u _ { l } | _ { \partial \Omega } = u _ { 0 } | _ { \partial \Omega }$ .

Combining the first inequality of (4.34) with the second inequality of (4.37), we obtain

$$
\begin{array}{l} C _ {2} \int_ {\Omega} | \nabla u _ {l} | ^ {p} \leqslant \int_ {\Omega} f (\nabla u _ {l}) \\ \leqslant \tilde {J} (u _ {0}) + m (\Omega) \\ \leqslant J (u _ {0}) + m (\Omega). \\ \end{array}
$$

Therefore there exists a weakly convergent subsequence $\{ u _ { l _ { i } } \}$ in $W _ { v } ^ { 1 , p }$

Recalling the construction of $\{ u _ { l } \}$ , we have $u _ { l } ( x )  u _ { 0 } ( x )$ , a.e. Therefore, ul  u0 in W 1,p. $u _ { l _ { i } } \to u _ { 0 }$ $W _ { v } ^ { 1 , p }$

Conversely, if $\{ u _ { l } \}$ is a minimizing sequence of $J$ , then $\widetilde J ( u _ { l } ) \leqslant J ( u _ { l } ) \longrightarrow$ in $\mathrm { f } \ J = \operatorname { i n f } \ \tilde { J }$ , i.e., $\{ u _ { l } \}$ is also a minimizing sequence of $\hat { J }$ . Again by the first inequality of (4.34), there exists a subsequence $\{ u _ { l _ { i } } \}$ that weakly converges to $u _ { 0 }$ in $W _ { 0 } ^ { 1 , p }$ . Since $\hat { J }$ is s.w.l.s.c., $u _ { 0 }$ is a minimizer of $\scriptstyle { \hat { J } }$ . □

Now, let us return to the example we met in the beginning of this section. The function

$$
f (u, \xi) = u ^ {2} + (\xi^ {2} - 1) ^ {2}
$$

is obviously not convex in $\xi$ . Since in the case where $N = 1$ , convex = quasiconvex, the convex envelope of $f ( u , \xi )$ is the following:

$$
Q f (u, \xi) = \left\{ \begin{array}{l l} u ^ {2} + (\xi^ {2} - 1) ^ {2} & \text {i f} | \xi | \geqslant 1 \\ 0 & \text {i f} | \xi | <   1  , \end{array} \right.
$$

and then

$$
\widetilde {J} (u) = \int_ {0} ^ {1} Q f (u (x), u ^ {\prime} (x)) d x.
$$

In fact, the minimizing sequence introduced previously weakly converges to 0 in $W ^ { 1 , 4 } ( [ 0 , 1 ] )$ , and $u = 0$ is exactly the minimizer of $\hat { J }$ .

Since the initial functional $J$ is in the lack of weakly lower semi-continuity, we then minimize another functional $\hat { J }$ instead, which is s.w.l.s.c. and shares a common minimizing sequence with the original one. This passage is called relaxation. The solution of $\hat { J }$ is called a generalized solution of $J$ . Noticing that the representation of the quasi-convex envelope in Lemma 4.4.3 can be seen as an average over fine scale oscillations, we give a physical interpretation of this phenomenon: $J$ represents the microscopic energy, while $\hat { J }$ is the macroscopic. There are models in elastic crystals, in which this phenomenon occurs.

The quasi convex envelope of a non-convex function is not easy to compute. People have developed various methods in approaching the computations. Readers who are interested in this problem are referred to the book of

Dacorogna [Dac]. In particular, to the interesting example on the two well problem: $K = \{ A , - A \}$ , the quasi convex envelope of the square distance function $Q d i s t ( P , K ) ^ { 2 }$ has been expressed explicitly by K. Zhang [Zh 2].

# 4.4.2 Young Measure

In a variational problem, there may be many minimizing sequences. If minimizers exist, then the minimizers describe the common feature of these sequences. Otherwise, the sequences do not converge to a minimizer as an element of the given function space. In his study of generalized solutions of optimal control problems, L. C. Young [Yo] introduced a family of measures in certain measure space as a replacement of the minimizer. The idea is as follow: Let $\Omega \subset \mathbb { R } ^ { n }$ be a measurable set, and let $z _ { j } : \Omega \to \mathbb { R } ^ { N } , j = 1 , 2 , \ldots , ,$ be a sequence of measurable functions. To each $z _ { j }$ , we relate a family of probability measures $\nu _ { x } ^ { j } = \delta _ { z _ { j } ( x ) }$ on $\mathbb { R } ^ { N } , \forall x \in \Omega$ . Instead of the convergence of $z _ { j }$ , we consider the convergence of the family of probability measures: $\left\{ \nu _ { x } ^ { \ j } \right\}$ ; i.e., whether there exists a family of measures $\nu _ { x }$ on $\mathbb { R } ^ { N }$ , $\forall x \in \Omega$ , such that for $\forall f \in C _ { 0 } ( \mathbb { R } ^ { N } )$ , the space of continuous functions with compact support,

$$
\int_ {\mathbb {R} ^ {N}} f (y) d \nu_ {x} ^ {j} (y) \to \int_ {\mathbb {R} ^ {N}} f (y) d \nu_ {x} (y),
$$

as $j \to \infty$ . The family of measures $\{ \nu _ { x } | x \in \Omega \}$ is a Young measure.

However, the function $z _ { j }$ is well defined in a.e. sense, what is the meaning of the measure: $\delta _ { z _ { j } ( x ) }$ ? What is the exact meaning of the family of measures $\{ \nu _ { x } | x \in \Omega \}$ ?

In order to clarify the meaning, we appeal to the slicing measures. Let $\mu$ be a finite nonnegative Radon measure on $\mathbb { R } ^ { n + N }$ . Denote by $\sigma$ the canonical projection of $\mu$ onto $\mathbb { R } ^ { n }$ : ${ \boldsymbol { \sigma } } ( E ) = \mu ( E \times \mathbb { R } ^ { N } ) \ \forall $ Borel set $E \subset \mathbb { R } ^ { n }$ .

Lemma 4.4.4 For $\sigma - a . e$ . point $x \in \mathbb { R } ^ { n } , \exists$ a Radon probability measure $\nu _ { x }$ on $\mathbb { R } ^ { N }$ such that for each bounded continuous function $f$ ,

1. $\begin{array} { r } { x \mapsto \int _ { \mathbb { R } ^ { N } } f ( x , y ) d \nu _ { x } ( y ) } \end{array}$ is σ − measurable,   
2. $\begin{array} { r } { \int _ { \mathbb { R } ^ { n } \times \mathbb { R } ^ { N } } f ( x , y ) d \mu ( x , y ) = \int _ { \mathbb { R } ^ { n } } \bigl ( \int _ { \mathbb { R } ^ { N } } f ( x , y ) d \nu _ { x } ( y ) \bigr ) d \sigma ( x ) . } \end{array}$

Proof. Recall that the dual space of $C _ { 0 } ( \mathbb { R } ^ { N } )$ , the space of continuous functions with compact support, is the signed measure space $\mathcal { M } ( \mathbb { R } ^ { N } )$ . The family of measures $\nu _ { x }$ on $\mathbb { R } ^ { N } , \forall x \in \mathbb { R } ^ { n }$ , should be determined with the aid of functions on $C _ { 0 } ( \mathbb { R } ^ { N } )$ .

Since $C _ { 0 } ( \mathbb { R } ^ { N } )$ is separable, there is a countable dense subset $\{ g _ { k } \} _ { 1 } ^ { \infty }$ . Now, $\forall$ Borel sets $E \subset \mathbb { R } ^ { n }$ , we define

$$
\gamma_ {k} (E) = \int_ {E \times \mathbb {R} ^ {N}} g _ {k} (y) d \mu (x, y), \forall k.
$$

Obviously, $\gamma _ { k }$ is absolutely continuous with respect to $\sigma$ . There exists a $\sigma$ - null set $\mathcal { N }$ such that the derivatives exist for $\forall x \notin \mathcal N$ :

$$
D _ {\sigma} \gamma_ {k} (x) = \lim  _ {r \rightarrow 0} \frac {\gamma_ {k} \left(B _ {r} (x)\right)}{\sigma \left(B _ {r} (x)\right)}, \forall k, \tag {4.39}
$$

and then $x \mapsto D _ { \sigma } \gamma _ { k } ( x )$ are bounded and $\sigma$ measurable.

Moreover, for all Borel sets $E \subset \mathbb { R } ^ { n }$ ,

$$
\int_ {E \times \mathbb {R} ^ {N}} g _ {k} (y) d \mu (x, y) = \gamma_ {k} (E) = \int_ {E} D _ {\sigma} \gamma_ {k} (x) d \sigma (x), \forall k. \tag {4.40}
$$

Since $\{ g _ { k } \} _ { 1 } ^ { \infty }$ is dense in $C _ { 0 } ( \mathbb { R } ^ { N } ) , \forall g \in C _ { 0 } ( \mathbb { R } ^ { N } )$ , we have a subsequence $g _ { k _ { j } }  y$ uniformly on $\mathbb { R } ^ { N }$ . Thus for $\begin{array} { r } { \forall x \notin N , \Gamma _ { x } ( g ) = \operatorname* { l i m } _ { j \to \infty } D _ { \sigma } \gamma _ { k _ { j } } ( x ) } \end{array}$ exists, and

$$
\left| \Gamma_ {x} (g) \right| \leq C _ {\mu} \| g \| _ {C _ {0} (\mathbb {R} ^ {N})}, \tag {4.41}
$$

where $C _ { \mu } = \mu ( \mathbb { R } ^ { n } \times \mathbb { R } ^ { N } )$ . Since

$$
| \gamma_ {k} (B _ {r} (x)) - \gamma_ {l} (B _ {r} (x)) | \leq \| g _ {k} - g _ {l} \| _ {C _ {0} (\mathbb {R} ^ {N})} \sigma (B _ {r} (x)),
$$

the limit $\Gamma _ { x } ( g )$ does not depend on the choice of the subsequence converging to $g$ . Also, the map: $g \mapsto \Gamma _ { x } ( g )$ is linear and positive. Provided by the Riesz representation theorem, $\exists \nu _ { x } \in \mathcal { M } ( \mathbb { R } ^ { N } )$ , which is nonnegative, such that for $\forall g \in C _ { 0 } ( \mathbb { R } ^ { N } )$ :

$$
\Gamma_ {x} (g) = \int_ {\mathbb {R} ^ {N}} g (y) d \nu_ {x} (y), \forall x \notin \mathcal {N}.
$$

By taking limits, we see $\nu _ { x } ( \mathbb { R } ^ { N } ) = \Gamma _ { x } ( 1 ) = 1$ . Therefore, it is a probability measure.

As a function of $x$ on $R ^ { N } \backslash \mathcal { N } , x \mapsto \Gamma _ { x } ( g )$ is bounded and $\sigma$ -measurable, because it is the pointwise limit of a sequence of $\sigma$ -measurable functions $D _ { \sigma } \gamma _ { k _ { j } } ( x )$ . Therefore, the conclusion (1) is obtained by approximation.

Following (4.40), by taking limits we obtain

$$
\int_ {E \times \mathbb {R} ^ {N}} g (y) d \mu (x, y) = \int_ {E} \Gamma_ {x} (g) d \sigma (x) = \int_ {E} \left(\int_ {\mathbb {R} ^ {N}} g (y) d \nu_ {x} (y)\right) d \sigma (x). \tag {4.42}
$$

From (4.42) $\forall h \in C _ { 0 } ( \mathbb { R } ^ { n } )$ , we have

$$
\int_ {\mathbb {R} ^ {n} \times \mathbb {R} ^ {N}} g (y) h (x) d \mu (x, y) = \int_ {\mathbb {R} ^ {n}} \left(\int_ {\mathbb {R} ^ {N}} g (y) d \nu_ {x} (y)\right) d \sigma (x). \tag {4.43}
$$

Since $C _ { 0 } ( \mathbb { R } ^ { n } ) \bigotimes C _ { 0 } ( \mathbb { R } ^ { N } )$ is dense in $C _ { 0 } ( \mathbb { R } ^ { n + N } )$ , (2) is proved by approximation. □

Remark 4.4.5 By standard measure theoretic argument, one can show that the statements of Lemma 4.4.4 hold, if the bounded continuous function $f$ is replaced by $f \in L ^ { 1 } ( \mathbb { R } ^ { n } \times \mathbb { R } ^ { N } , \mu )$ . Moreover, we have

$$
f (x, \cdot) \in L ^ {1} \left(\mathbb {R} ^ {N}, \nu_ {x}\right) f o r a. e., x \in \mathbb {R} ^ {n},
$$

and the function $\begin{array} { r } { x \mapsto \int _ { \mathbb { R } ^ { N } } f ( x , y ) d \nu _ { x } ( y ) } \end{array}$ is in $L ^ { 1 } ( \mathbb { R } ^ { n } , \sigma )$ .

A map $\nu : \mathbb { R } ^ { n } \to \mathbb { M } ( \mathbb { R } ^ { N } )$ is called weakly ${ } ^ { * } \sigma$ -measurable, if the functions $\begin{array} { r } { x \mapsto \int _ { \mathbb { R } ^ { N } } f ( y ) d \nu _ { x } ( y ) } \end{array}$ are $\sigma$ -measurable for all $f \in C _ { 0 } ( \mathbb { R } ^ { N } )$ on $\mathbb { R } ^ { n }$ .

In the context of the duality between $\mathcal { M } ( \mathbb { R } ^ { N } )$ and $C _ { 0 } ( \mathbb { R } ^ { N } )$ , the map ν : $\mathbb { R } ^ { n } \to \mathcal { M } ( \mathbb { R } ^ { N } )$ , obtained in Lemma 4.4.4 is $w ^ { \ast } \sigma$ -measurable.

After the preparations, we are able to identify an $L ^ { \infty } ( \Omega , \mathbb { R } ^ { N } )$ function with a $w ^ { * }$ -measurable map $\nu : \Omega \mapsto \mathcal { M } ( \mathbb { R } ^ { N } )$ .

Suppose that $\Omega \subset \mathbb { R } ^ { n }$ is a Lebesgue measurable set with a finite Lebesgue measure. For $\forall z \in L ^ { \infty } ( \Omega , \mathbb { R } ^ { N } )$ , one defines a measure $\mu$ on $\mathbb { R } ^ { n } \times \mathbb { R } ^ { N }$ , provided by the duality:

$$
\int_ {\mathbb {R} ^ {n} \times \mathbb {R} ^ {N}} f (x, y) d \mu (x, y) = \int_ {\Omega} f (x, z (x)) d x, \forall f \in C _ {0} \left(\mathbb {R} ^ {n} \times \mathbb {R} ^ {N}\right), \tag {4.44}
$$

and then a family of measures on $\mathbb { R } ^ { N } : \delta _ { z ( x ) }$ for a.e. $x \in \Omega$ . Since the righthand side of (4.44) is linear and positive in $f$ , and

$$
\left| \int_ {\Omega} f (x, z (x)) d x \right| \leq m (\Omega) \| f \| _ {C _ {0}},
$$

the existence of $\mu$ follows from the Riesz representation theorem.

Obviously, the canonical projection of $\mu$ onto $R ^ { n }$ is the Lebesgue measure restricted on the measurable set $\Omega$ , and the $w ^ { * }$ -measurable map $\nu : \Omega \to$ $\mathcal { M } ( \mathbb { R } ^ { N } )$ satisfies $\nu _ { x } = \delta _ { z ( x ) }$ for a.e. $x \in \Omega$ .

Now we turn to the fundamental theorem for Young measures.

Theorem 4.4.6 Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open domain and let $\{ z _ { k } \} \subset$ $L ^ { \infty } ( \Omega , \mathbb { R } ^ { N } )$ be a bounded sequence. Then there exist a subsequence $\{ z _ { k _ { j } } \}$ and a $w ^ { * }$ -measurable map $\nu : \Omega \to \mathcal { M } ( \mathbb { R } ^ { N } )$ such that

1. $\nu _ { x }$ is a Radon probability measure for a.e. $x \in \Omega$ .   
2. For $\forall g \in C _ { 0 } ( R ^ { N } )$ , one has

$$
g \left(z _ {k _ {j}}\right) ^ {*} \rightharpoonup \overline {{g}} (x) := \langle \nu_ {x}, g \rangle = \int_ {\mathbb {R} ^ {N}} g (y) d \nu_ {x} (y), i n L ^ {\infty} (\Omega).
$$

3. Let $K \subset \mathbb { R } ^ { N }$ be closed, and let $\mathrm { d i s t } ( z _ { k _ { j } } , K )  0$ in measure as $j  \infty$

Then supp $\nu _ { x } \subset K$ for a.e. $x \in \Omega$ .

Proof. According to (4.44), $\forall z _ { k } , \exists \mu _ { k }$ on $\mathbb { R } ^ { n } \times \mathbb { R } ^ { N }$ , for which the canonical projection onto $R ^ { n }$ is the Lebesgue measure restricted to $\Omega$ . Since $\mu _ { k } ( \mathbb { R } ^ { n } \times$ $\mathbb { R } ^ { N } ) \le \mathcal { L } ^ { n } ( \Omega )$ , there exist a measure $\mu$ on $\mathbb { R } ^ { n } \times \mathbb { R } ^ { N }$ and a subsequence such that $\mu _ { k _ { j } } \sp { * } \to \mu$ , according to the Banach–Alaoglu theorem.

We claim that the canonical projection $\sigma$ of $\mu$ into $R ^ { n }$ is the Lebesgue measure $\mathcal { L } ^ { n }$ restricted on $\Omega$ . On one hand, if $U \subset \Omega$ is open, then by the $w ^ { * }$ -convergence:

$$
\sigma (U) = \mu (U \times \mathbb {R} ^ {N}) \leq \lim  \inf  \mu_ {k _ {j}} (U \times \mathbb {R} ^ {N}) = \mathcal {L} ^ {n} (U).
$$

According to the Radon–Nikodym theorem, there exists $a \in L ^ { 1 } ( \Omega , { \mathcal { L } } ^ { n } )$ such that $\sigma = a \mathcal { L } ^ { n }$ .

On the other hand, for any compact subset $K \subset \Omega$ , we show that $\sigma ( K ) \geq \mathcal L ^ { n } ( K )$ . Since $\left\{ z _ { k } \right\}$ is bounded in $L ^ { \infty } ( \Omega , \mathbb { R } ^ { N } ) , \exists R > 0$ , such that $\operatorname { s u p p } ( \mu _ { k _ { j } } )$ , and then $\operatorname { s u p p } ( \mu )$ are contained in $\Omega \times B _ { R } ( \theta ) . \forall \epsilon \ > \ 0 , \exists f \in$ $C _ { 0 } ( \mathbb { R } ^ { n } \times \mathbb { R } ^ { N } ) , \mathrm { w i t h } f = 1$ on $K \times B _ { R } ( \theta )$ and $f \geq 0$ , such that

$$
\int_ {\mathbb {R} ^ {n} \times \mathbb {R} ^ {N}} f (x, y) d \mu (x, y) <   \mu (K \times B _ {R} (\theta)) + \epsilon .
$$

Thus,

$$
\begin{array}{l} \sigma (K) = \mu (K \times \mathbb {R} ^ {N}) \\ = \mu (K \times \overline {{B _ {R} (\theta)})} \\ > \int_ {\mathbb {R} ^ {n} \times \mathbb {R} ^ {N}} f (x, y) d \mu (x, y) - \epsilon \\ = \lim  _ {j \rightarrow \infty} \int_ {\mathbb {R} ^ {n} \times \mathbb {R} ^ {N}} f (x, y) d \mu_ {k _ {j}} (x, y) - \epsilon \\ > \mathcal {L} ^ {n} (K) - \epsilon . \\ \end{array}
$$

Since $\epsilon > 0$ is arbitrary, we have proved that

$$
\sigma (K) \geq \mathcal {L} ^ {n} (K). \tag {4.45}
$$

Therefore $a = 1$ , a.e., and $\sigma$ is the restriction of the Lebesgue measure on $\Omega$ . The first conclusion follows directly from Lemma 4.4.4.

We turn to proving the second conclusion. $\forall h \in C ( \Omega )$ , and $\forall g \in C _ { 0 } ( \mathbb { R } ^ { N } ) ,$ 3 an extension of $h , \bar { h } \in C _ { 0 } ( \mathbb R ^ { n } )$ , from (4.44), we have

$$
\begin{array}{l} \int_ {\mathbb {R} ^ {n + N}} \tilde {h} (x) g (y) d \mu_ {k _ {j}} (x, y) \\ = \int_ {\Omega} h (x) g \left(z _ {k _ {j}} (x)\right) d x. \\ \end{array}
$$

By taking limits, and from (4.43), we have

$$
\begin{array}{l} \int_ {\Omega} h (x) g \left(z _ {k _ {j}} (x)\right) d x \rightarrow \int_ {\mathbb {R} ^ {n} \times \mathbb {R} ^ {N}} \tilde {h} (x) g (y) d \mu (x, y) \\ = \int_ {\Omega} h (x) \left(\int_ {\mathbb {R} ^ {N}} g (y) d \nu_ {x} (y)\right) d x. \\ \end{array}
$$

Since $C ( \Omega )$ is dense in $L ^ { 1 } ( \Omega )$ and $g ( z _ { k _ { j } } )$ is bounded in $L ^ { \infty } ( \Omega )$ , the above equality implies that

$$
g \left(z _ {k _ {j}}\right) ^ {*} \rightharpoonup \int_ {\mathbb {R} ^ {N}} g (y) d \nu (y) = <   \nu , g > \text {i n} L ^ {\infty} (\Omega , \mathbb {R} ^ {N}),
$$

i.e., (2) holds.

We prove the third conclusion. $\forall g \in C _ { 0 } ( R ^ { N } \backslash K ) , \forall \epsilon > 0 , \exists C _ { \epsilon } > 0$ , such that

$$
\left| g (y) \right| \leq \epsilon + C _ {\epsilon} \operatorname {d i s t} (y, K), \forall y \in \mathbb {R} ^ {N},
$$

which implies that

$$
\left| g \left(z _ {k} (x)\right) \right| \leq \epsilon + C _ {\epsilon} \operatorname {d i s t} \left(z _ {k} (x), K\right), \text {a . e .}, x \in \Omega .
$$

From $\mathrm { d i s t } ( z _ { k _ { j } } ( x ) , K )  0$ in measure, we have

$$
| \langle \nu_ {x}, g \rangle | \leq \epsilon , \quad \text {a . e .} x \in \Omega
$$

Since $\epsilon > 0$ is arbitrary, $\operatorname { s u p p } ( \nu _ { x } ) \subset K$ , a.e. $x \in \Omega$ .

Definition 4.4.7 The $w ^ { * }$ -measurable map $\nu : \Omega \to \mathcal { M } ( \mathbb { R } ^ { N } )$ , defined in Theorem 4.4.6, is called a Young measure generated by the sequence $\{ z _ { k _ { j } } \} \subset$ $L ^ { \infty } ( \Omega , \mathbb { R } ^ { N } )$ .

Remark 4.4.8 The conclusion (2) has a probability interpretation: In the limit, $g ( z _ { k _ { j } } )$ takes the value $g ( y )$ with probability $\nu _ { x } ( y )$ at $x$ . Therefore the Young measure can be used to describe the local phase proportions in an infinitesimally fine mixture.

Corollary 4.4.9 Assume that $\nu$ is the Young measure associated with the sequence $\left\{ z _ { k } \right\}$ and that $z _ { k } \longrightarrow z$ in measure. Then $\nu _ { x } = \delta _ { z ( x ) }$ a.e.

Proof. $\forall g \ \in \ C _ { 0 } ( \mathbb { R } ^ { N } ) , g ( z _ { k } ) \ \to \ g ( z )$ in measure. But by Theorem 4.4.6, $g ( z _ { k } ) \mathrel { \mathop {  } } \bgroup \egroup \textstyle \longrightarrow \bar { g } = \langle \nu , g \rangle$ . Therefore,

$$
\langle \nu_ {x}, g \rangle = g (z (x)) = \langle \delta_ {z (x)}, g \rangle , \mathrm {a . e .}
$$

□

Remark 4.4.10 In Theorem 4.4.6, the $L ^ { \infty }$ -bounded sequence $\{ z _ { k } \} \subset L ^ { \infty } ( \Omega ,$ $\mathbb { R } ^ { N }$ ) can be replaced by the $L ^ { 1 }$ bounded sequence $\{ z _ { k } \} \subset L ^ { 1 } ( \Omega , \mathbb { R } ^ { N } )$ . In this case, $\nu _ { x }$ is ${ \mathcal { L } } ^ { n } \lfloor \Omega$ a.e. a probability measure with

$$
\int_ {\Omega} \int_ {\mathbb {R} ^ {N}} | y | d \nu_ {x} (y) d x \leq \lim  _ {k \rightarrow \infty} \inf  _ {k \rightarrow \infty} \| z _ {k} \| _ {L ^ {1}}.
$$

Again we claim that $\sigma$ is the restriction of the Lebesgue measure on $\Omega$ . In fact, only (4.45) should be modified: $\forall R > 0$

$$
\begin{array}{l} \sigma (K) \geq \mu (K \times \overline {{B _ {R} (\theta)}}) \\ \geq \limsup_{k\to \infty}\mu_{k}(K\times \overline{B_{R}(\theta)}) \\ \geq \limsup_{k\to \infty}\mathcal{L}^{n}(\{x\in K\mid |z_{k}(x)|\leq R\}) \\ \end{array}
$$

$$
\geq \mathcal {L} ^ {n} (K) - \frac {1}{R} \sup  _ {k} \| z _ {k} \| _ {L ^ {1}}.
$$

As $R \to \infty$ , it follows that $\sigma ( K ) \geq { \mathcal { L } } ^ { n } ( K )$ . Finally, we use Remark 4.4.5, and choose a sequence of positive functions $f _ { k } ( x , y ) \uparrow | y |$ pointwisely; the monotone convergence theorem implies the conclusion.

Corollary 4.4.11 Assume that $\{ z _ { k } \} \subset L ^ { \infty } ( \Omega , \mathbb { R } ^ { N } )$ , satisfies $z _ { k } \ \longrightarrow \ z , u . e .$ , and that $\{ w _ { k } \} \subset L ^ { \infty } ( \Omega , \mathbb { R } ^ { M } )$ generates the Young measure $\nu$ . Then $\left\{ z _ { k } , w _ { k } \right\} :$ $\Omega \to \mathbb { R } ^ { N + M }$ generates the Young measure $\delta _ { z ( x ) } \otimes \nu _ { x } , x \in \Omega$ .

Proof. $\forall \varphi ~ \in ~ C _ { 0 } ( \mathbb { R } ^ { N } ) , \forall \psi ~ \in ~ C _ { 0 } ( \mathbb { R } ^ { M } ) , \forall \eta ~ \in ~ L ^ { 1 } ( \Omega )$ , by definition and the Lebesgue’s dominance theorem, we have

$$
\begin{array}{l} \varphi (z _ {k}) \rightarrow \varphi (z), \text {a . e .}, \\ \eta \varphi (z _ {k}) \rightarrow \eta \varphi (z) \text {i n} L ^ {1} (\Omega), \\ \psi \left(w _ {k}\right) ^ {*} \rightharpoonup \overline {{\psi}} = \langle \nu , \psi \rangle \text {i n} L ^ {\infty} (\Omega). \\ \end{array}
$$

This implies that

$$
\int_ {\Omega} \eta (\varphi \bigotimes \psi) (z _ {k}, w _ {k}) d x = \int_ {\Omega} \eta \varphi (z _ {k}) \psi (w _ {k}) d x \rightarrow \int_ {\Omega} \eta (x) \varphi (z (x)) \langle \nu_ {x}, \psi \rangle d x.
$$

Then,

$$
\left(\varphi \bigotimes \psi\right)\left(z _ {k}, w _ {k}\right) ^ {*} \rightharpoonup \left\langle \delta_ {z} \bigotimes \nu , \varphi \bigotimes \psi \right\rangle \text {i n} L ^ {\infty} (\Omega).
$$

Since $C _ { 0 } ( \mathbb { R } ^ { N } ) \bigotimes C _ { 0 } ( \mathbb { R } ^ { M } )$ is dense in $C _ { 0 } ( \mathbb { R } ^ { N + M } )$ , it follows that

$$
f \left(z _ {k}, w _ {k}\right) ^ {*} \rightharpoonup \left\langle \delta_ {z} \bigotimes \nu , f \right\rangle \text {i n} L ^ {\infty} (\Omega),
$$

for all $f \in C _ { 0 } ( \mathbb { R } ^ { N + M } )$ .

Example 4.4.12 Let $\varphi$ be the function defined in Corollary 4.3.3, and $\varphi _ { k } ( x ) = \varphi ( k x ) , \forall k \in \mathcal { N }$ . It is known that

$$
\varphi_ {k} ^ {*} \rightharpoonup \int_ {0} ^ {1} \varphi (x) d x = \lambda \alpha + (1 - \lambda) \beta .
$$

Similarly, $\forall f \in C ( \mathbb { R } ^ { 1 } )$ , we have

$$
f \circ \varphi_ {k} ^ {*} \rightharpoonup \int_ {0} ^ {1} f (\varphi (x)) d x = \lambda f (\alpha) + (1 - \lambda) f (\beta).
$$

Therefore the sequence $\{ \varphi _ { k } \}$ is associated with a Young measure: $\nu = \lambda \delta _ { \alpha } +$ $( 1 - \lambda ) \delta _ { \beta }$ ; i.e., $\nu _ { x } = \lambda \delta ( x - \alpha ) + ( 1 - \lambda ) \delta ( x - \beta )$ , $\forall x \in \mathbb { R } ^ { 1 }$ .

Example 4.4.13 Let $\{ u _ { n } \}$ be the sequence of sawtooth functions:

$$
u _ {n} (x) = \left\{ \begin{array}{l l} x - \frac {k}{n} & i f x \in [ \frac {k}{n}, \frac {2 k + 1}{n} ] \\ - x + \frac {k + 1}{n} & i f x \in [ \frac {2 k + 1}{n}, \frac {k + 1}{n} ], \end{array} \right.
$$

which is a minimizing sequence of the functional $J$ (see equation 4.32). Let

$$
z _ {n} (x) = u _ {n} ^ {\prime} (x) = \left\{ \begin{array}{l l} 1 & i f x \in [ \frac {k}{n}, \frac {2 k + 1}{n} ], \\ - 1 & i f x \in [ \frac {2 k + 1}{n} ]. \end{array} \right.
$$

Let ν be the Young measure associated with to the sequence $z _ { k }$ .

Conclusion: $\begin{array} { r } { \nu = \frac { 1 } { 2 } ( \delta _ { - 1 } + \delta _ { + 1 } ) } \end{array}$

In fact,

$$
g \left(z _ {n} (x)\right) ^ {*} \rightharpoonup \left\langle \nu_ {x}, g \right\rangle ,
$$

for all $g \in C _ { 0 } ( \mathbb { R } ^ { 1 } )$ . Let us take $g ( y ) = \operatorname* { m i n } \{ ( y ^ { 2 } - 1 ) ^ { 2 } , 1 \}$ . Since $J ( u _ { n } ) \to 0$ , $\langle \nu _ { x } , g \rangle = 0$ a.e. This implies that $\operatorname { s u p p } ( \nu _ { x } ) \subset \{ - 1 , 1 \}$ , i.e., $\nu _ { x } = \lambda ( x ) \delta _ { - 1 } +$ $( 1 - \lambda ( x ) ) \delta _ { + 1 }$ .

Also, $z _ { n } \ ^ { * } \  \ 0$ in $L ^ { \infty } ( \mathbb { R } ^ { 1 } )$ and the relation $g _ { 1 } ( z _ { n } ( x ) ) \ ^ { * } \  \ \langle \nu _ { x } , g _ { 1 } \rangle$ i n $L ^ { \infty } ( \mathbb { R } ^ { 1 } )$ , holds for all $g _ { 1 } \in C _ { 0 } ( \mathbb { R } ^ { 1 } )$ with $g _ { 1 } ( y ) = y$ , as $| y | < 2$ . Thus

$$
\left\langle \lambda (x) \delta_ {- 1} + (1 - \lambda (x)) \delta_ {+ 1}, g _ {1} \right\rangle = 0.
$$

This implies that $1 - 2 \lambda ( x ) = 0$ , or $\begin{array} { r } { \lambda ( { \boldsymbol x } ) = \frac { 1 } { 2 } } \end{array}$ .

Example 4.4.14 Let $B , C \in M ^ { n \times N }$ satisfy $r a n k ( B - C ) = 1$ . Assume $\exists \lambda \in$ $( 0 , 1 )$ , such that $\lambda B + ( 1 - \lambda ) C = 0$ . It is known that without loss of generality, one may assume $\boldsymbol { B } = ( 1 - \lambda ) \boldsymbol { a } \otimes \boldsymbol { e } _ { 1 }$ , and $C = - \lambda a \otimes e _ { 1 }$ for some $a \in \mathbb { R } ^ { N }$ and $\boldsymbol { e } _ { 1 } \in \mathbb { R } ^ { n }$ . Let

$$
\begin{array}{l} \varphi (t) = \left\{ \begin{array}{l l} (1 - \lambda) t & i f t \in [ 0, \lambda ]  , \\ - \lambda (t - 1) & i f t \in [ \lambda , 1 ]  . \end{array} \right. \\ \forall x = (x _ {1}, \dots , x _ {n}) \in D := [ 0, 1 ] ^ {n}, l e t \\ u _ {k} (x) = a k ^ {- 1} \varphi \left(k x _ {1}\right), \quad k = 1, 2, \dots , \quad a n d \quad z _ {k} = \nabla u _ {k}. \\ \end{array}
$$

Then $\left\{ z _ { k } \right\}$ is associated with the Young measure $\nu = \lambda \delta _ { B } + ( 1 - \lambda ) \delta _ { C }$ , where $\delta _ { B }$ and $\delta _ { C }$ are the probability measures concentrated at the matrices $B$ and $C$ , respectively.

In fact,

$$
z _ {k} (x) = \left\{ \begin{array}{l l} B & \text {i f} \{k x _ {1} \} \in (0, \lambda), \\ C & \text {i f} \{k x _ {1} \} \in (\lambda , 1), \end{array} \right.
$$

so is $\mathrm { d i s t } ( z _ { k } , \{ B , C \} ) ~ = ~ 0$ . By Theorem 4.4.6, the probability measure $\nu$ satisfies $\operatorname { s u p p } ( \nu _ { x } ) \subset \{ B , C \}$ a.e. Therefore, $\exists \mu ( x ) \in [ 0 , 1 ]$ such that $\nu _ { x } =$

$\mu ( x ) \delta _ { B } + ( 1 - \mu ( x ) ) \delta _ { C }$ , and $\forall g \in C _ { 0 } ( M ^ { n \times N } ) , g ( z _ { k _ { j } } ) ^ { * }  \langle \nu , g \rangle$ for some subsequence $\{ z _ { k _ { j } } \}$ . In particular, we take $g ( y ) = y$ in a ball centered at $\theta$ , containing $\boldsymbol { a } \otimes \boldsymbol { e } _ { 1 }$ . Again, from $z _ { k } \ ^ { * } \to 0$ , we obtain $\mu ( x ) B + ( 1 - \mu ( x ) ) C = 0$ , i.e., $\mu ( x ) = \lambda$ . Therefore, $\nu = \lambda \delta _ { B } + ( 1 - \lambda ) \delta _ { C }$ .

A w∗-measurable map $\nu : \Omega \to \mathcal { M } ( M ^ { n \times N } )$ is called a gradient Young measure, if $\exists \{ u _ { j } \} \subset W ^ { 1 , \infty } ( \Omega , R ^ { N } )$ such that

$$
u _ {j} ^ {*} \rightharpoonup u \text {i n} W ^ {1, \infty} (\Omega , R ^ {N}), \text {a n d} \delta_ {\nabla u _ {j}} ^ {*} \rightharpoonup \nu .
$$

A central problem for the gradient Young measure is the following: Given a set $K \subset M ^ { n \times N }$ , how do we characterize all $W ^ { 1 , \infty }$ gradient Young measures $\nu$ such that $\operatorname { s u p p } ( \nu _ { x } ) \subset K$ for a.e. $x \in \Omega$ ?

Indeed, letting $\nu$ be the Young measure generated by $\{ u _ { j } \}$ , the range of $\{ \nabla u _ { j } \}$ must be contained in a big ball $B _ { R } ( \theta )$ of $M ^ { n \times N }$ . Then by taking $K = { \overline { { B } } } _ { R } ( \theta )$ , and according to (3) of Theorem 4.4.6, it follows that:

(1). There exists a compact set $K \subset M ^ { n \times N }$ such that $\operatorname { s u p p } ( \nu _ { x } ) \subset K$ , a.e.

If we choose $g ( y ) = y$ on $B _ { R } ( \theta )$ and $g \in C _ { 0 } ( M ^ { n \times N } )$ , then $g ( \nabla u _ { j } ) =$ $\nabla { u _ { j } } ^ { * }  \nabla u$ in $L ^ { \infty } ( \Omega , M ^ { n \times N } )$ . From the second conclusion of Theorem 4.4.6, we have $g ( \nabla u _ { j } ) ^ { * }  \langle \nu , g \rangle \ln L ^ { \infty } ( \Omega , M ^ { n \times N } )$ . Since $g$ is arbitrary outside the ball $B _ { R } ( \theta )$ , we may simply write $\langle \nu , g \rangle$ as $\langle \nu , \mathrm { i d } \rangle$ . Thus,

(2) $\langle \nu _ { x } , \mathrm { i d } \rangle = \nabla u ( x )$ a.e.

Finally, assume that $g : M ^ { n \times N } \to { \mathbb { R } } ^ { 1 }$ is continuous and quasi-convex, then by the Morrey theorem, for all open sets $U \subset \Omega$ , we have

$$
\lim  _ {k \rightarrow \infty} \inf  _ {U} g (\nabla u _ {k} (x)) d x \geq \int_ {U} g (\nabla u (x)) d x.
$$

Note that

$$
g \left(\nabla u _ {k} (x)\right) ^ {*} \rightharpoonup \langle \nu_ {x}, g \rangle , \text {a n d} g (\nabla u (x)) = g \left(\langle \nu_ {x}, \mathrm {i d} \rangle\right);
$$

we obtain

$$
\int_ {U} \langle \nu_ {x}, g \rangle d x \geq \int_ {U} g (\langle \nu_ {x}, \mathrm {i d} \rangle) d x.
$$

Since $U \subset \Omega$ is arbitrary, we arrive at

(3) $\langle \nu _ { x } , g \rangle \geq g ( \langle \nu _ { x } , \mathrm { i d } \rangle )$ a.e.

In fact, all these propositions together characterize the $W ^ { 1 , \infty }$ gradient Young measure, i.e., the converse is also true. Namely, one has the following:

Theorem 4.4.15 A $w ^ { * }$ -measurable map $\nu : \Omega \to \mathcal { M } ( M ^ { n \times N } )$ is a gradient Young measure if and only if $\nu _ { x } \geq 0$ a.e.,  a compact set $K \subset M ^ { n \times N }$ and $\exists u \in W ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } )$ such that

1. $\operatorname { s u p p } ( \nu _ { x } ) \subset K$ , a.e.   
$\boldsymbol { \mathcal { Z } }$ . $\langle \nu _ { x } , \mathrm { i d } \rangle = \nabla u ( x )$ , a.e.   
3. $\langle \nu _ { x } , g \rangle \geq g ( \langle \nu _ { x } , i d \rangle )$ a.e., $\forall$ continuous quasi-convex g: $M ^ { n \times N } \to \mathbb { R } ^ { 1 }$   
The sufficient part of the proof can be found in [KP 1].

Remark 4.4.16 In the modeling of microstructure, the reason people use $W ^ { 1 , \infty }$ sequences approaching a compact set $K$ is based on a truncation lemma due to Kewei Zhang [Zh 1]: Let $K$ be a compact set in $M ^ { n \times N }$ , and let $\| K \| _ { \infty } =$ $\operatorname* { s u p } \{ | A | \mid A \in K \}$ . Let $\Omega \subset R ^ { \pi }$ be a bounded domain. If $\{ u _ { j } \} \subset W _ { l o c } ^ { 1 , 1 } ( \Omega , R ^ { N } )$ satisfies dist $( \nabla u _ { j } , K ) \to 0$ in $L ^ { 1 } ( \Omega )$ , and $u _ { j }  u$ in $L _ { l o c } ^ { 1 } ( \Omega )$ , then there exists a sequence $\{ v _ { j } \} \subset W _ { l o c } ^ { 1 , 1 } ( \Omega , R ^ { N } )$ such that $| \nabla v _ { j } | \le c _ { n , N } \| K \| _ { \infty }$ , $\mid \{ x \in$ $\Omega | u _ { j } ( x ) \neq v _ { j } ( x ) \} |  0$ , and near $\partial \Omega$ . $v _ { j } = u$

As an application, we extend that Morrey theorem to the case where the integrand is a function depending on $( x , u , \nabla u )$ .

Corollary 4.4.17 Suppose that $f : \Omega \times \mathbb { R } ^ { N } \times M ^ { n \times N } \longrightarrow \mathbb { R } ^ { 1 }$ is a nonnegative Caratheodory function, and that $u _ { j } \to u$ in $W ^ { 1 , \infty } ( \Omega , \mathbb { R } ^ { N } )$ generates a gradient Young measure $\nu$ . Then

$$
\lim  _ {j \rightarrow \infty} \inf  _ {\Omega} f (x, u _ {j} (x), \nabla u _ {j} (x)) d x \geq \int_ {\Omega} \left(\int_ {\mathcal {M} ^ {n \times N}} f (x, u (x), \xi) d \nu_ {x} (\xi)\right) d x
$$

If further, $\forall ( x , u ) \in \Omega \times R ^ { N } , \xi \to f ( x , u , \xi )$ is quasi-convex, then

$$
\lim  _ {j \rightarrow \infty} \inf  _ {\Omega} f (x, u _ {j} (x), \nabla u _ {j} (x)) d x \geq \int_ {\Omega} f (x, u (x), \nabla u (x)) d x,
$$

i.e., the functional $\begin{array} { r } { J ( u ) = \int _ { \Omega } f ( x , u ( x ) , \nabla u ( x ) ) d x } \end{array}$ is s.w∗.l.s.c. on $W ^ { 1 , \infty } ( \Omega ,$ $\mathbb { R } ^ { N }$ ).

Proof. According to Corollary 4.4.13, the sequence $\{ \boldsymbol { z } _ { j } = ( u _ { j } , \nabla { u } _ { j } ) \}$ generates the Young measure $\delta _ { u } \otimes \nu$ .

Since the range of $\{ z _ { j } \}$ are contained in a big ball $B _ { R } ( \theta ) \subset \mathbb { R } ^ { N } \times M ^ { n \times N }$ , we may assume that $f = 0$ outside $B _ { R } ( \theta )$ .

Since $f$ is a Caratheodory function, by the Scorza–Dragoni–Vainberg theorem (a version of the Luzin theorem with parameters, see also Vainberg [Va], there exists an increasing sequence of compact sets $C _ { k } ~ \subset ~ \Omega$ 2 such that $m ( \Omega \backslash C _ { k } )  0$ , and $f | _ { C _ { k } \times \mathbb { R } ^ { N } \times \mathcal { M } ^ { n \times N } }$ is continuous. Let $g _ { k } ( x , \cdot ) =$ $\chi _ { C _ { k } } ( x ) f ( x , \cdot ) : \Omega \to C _ { 0 } ( \mathbb { R } ^ { N } \times \mathcal { M } ^ { n \times N } )$ . We obtain

$$
\begin{array}{l} \int_ {\Omega} f (x, u _ {j} (x), \nabla u _ {j} (x)) d x \geq \int_ {\Omega} g _ {k} (x, u _ {j} (x), \nabla u _ {j} (x)) d x \\ \rightarrow \int_ {\Omega} \langle \delta_ {u (x)} \bigotimes \nu_ {x}, g _ {k} (x, \cdot) \rangle d x, \text {a s} j \rightarrow \infty . \\ \end{array}
$$

But,

$$
\begin{array}{l} \mathrm {R H S} = \int_ {C _ {k}} \left\langle \delta_ {u (x)} \bigotimes \nu_ {x}, f (x, \cdot , \cdot) \right\rangle d x \\ = \int_ {C _ {k}} \left(\int_ {M ^ {n \times N}} f (x, u (x), \xi) d \nu_ {x} (\xi)\right) d x \\ \end{array}
$$

$$
\rightarrow \int_ {\Omega} \left(\int_ {M ^ {n \times N}} f (x, u (x), \xi) d \nu_ {x} (\xi)\right) d x, \text {a s} k \rightarrow \infty .
$$

This proves the first conclusion.

If further, $f$ is quasi-convex in $\xi$ , then from the conclusions (2) and (3) of Theorem 4.4.15, we have

$$
\int_ {\mathcal {M} ^ {n \times N}} f (x, u (x), \xi) d \nu_ {x} (\xi) \geq f (x, u (x), \nabla u (x)), \text {a . e .} x \in \Omega .
$$

The s.w $^ *$ .l.s.c. of $J$ is proved.

Remark 4.4.18 A gradient Young measure ν, if it is constant up to a null set, is called homogeneous. This is a useful notion in studying oscillations of sequences because it isolates the oscillation near a point in a domain. Examples 4.4.13 and 4.4.14 are homogeneous.

# 4.5 Other Function Spaces

We have seen that the $w ^ { * }$ -compactness plays an important role in the calculus of variations. That is why we always choose Sobolev spaces $W ^ { \prime \prime \prime , p } , 1 < p \leq \infty$ as working spaces. The case $p = 1$ is different, even if to the Lebesgue space $L ^ { 1 }$ , the Banach–Alaoglu theorem cannot be applied. In fact, the positive deltatype sequence does not weakly * converge to any $L ^ { 1 }$ function. In the extremal case, we study some other function spaces as replacements. In this section, the BV space, the Hardy space and some other related spaces are introduced in the applications to variational problems.

# 4.5.1 BV Space

We study the minimal surface problem. Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded open domain; a hypersurface $u \in C ^ { 1 } ( \Omega , \mathbb { R } ^ { 1 } )$ with prescribed boundary condition has the area:

$$
A (u) = \int_ {\Omega} \sqrt {\left(1 + | \nabla u | ^ {2}\right)}.
$$

This leads to the following variational problem: $\operatorname { M i n } \{ A ( u ) \}$ in the above set of hypersurfaces. One might expect that the Sobolev space $W ^ { 1 , 1 }$ would be a working space for the direct method. Unfortunately, the closed ball in this space is not weakly closed, and so a minimizing sequence may fail to converge. We are introduced to the space of bounded variations, which is widely used in the so-called free discontinuous boundary problems, in which the boundary of the domain is to be determined in the extremum problems.

Recall that a continuous linear functional defined on the space of continuous functions on $[ 0 , 1 ]$ can be represented by a BV function $u$ as follows:

$$
L (\phi) = \int_ {0} ^ {1} \phi (t) d u (t), \forall \phi \in C ([ 0, 1 ]). \tag {4.46}
$$

If $\phi \in C _ { 0 } ^ { \mathrm { 1 } } ( [ 0 , 1 ] )$ , then

$$
L (\phi) = - \int_ {0} ^ {1} u (t) \phi^ {\prime} (t) d t. \tag {4.47}
$$

We shall extend this relation to higher dimensional space. Let $\Omega \subset \mathbb { R } ^ { n }$ be an open domain, and let $C _ { 0 } ( \Omega )$ be the space of continuous functions of compact support with respect to $\Omega$ .

First, we note that $L$ is a linear continuous functional on $C _ { 0 } ( \Omega )$ if and only if there exists a Radon measure $\mu$ and a $\mu$ -measurable function $\nu$ on $\Omega$ with $| \nu ( x ) | = 1$ , a.e., such that

$$
L (\phi) = \int_ {\Omega} \phi (x) \nu (x) d \mu (x), \phi \in C _ {0} (\Omega).
$$

Second, for each compact subset $K \subset \Omega$ , let $L _ { K } = L | _ { C ( K ) }$ , then

$$
\left\| L _ {K} \right\| := \sup  \left\{L (f) \mid \| f \| _ {C (K)} \leq 1 \right\} = \mu (K).
$$

Motivated by (4.46), and (4.47), one defines the counterpart of a BV function $u$ in higher dimensional spaces, as being a function $u \in L ^ { 1 } ( \Omega )$ such that the functional

$$
L (\phi) := \int_ {\Omega} u \operatorname {d i v} \phi
$$

is defined for all $\phi ~ \in ~ { \cal C } _ { 0 } ^ { 1 } ( \Omega , \mathbb { R } ^ { n } )$ , and can be extended continuously to $C _ { 0 } ( \Omega , \mathbb { R } ^ { n } )$ . Then by Riesz representation theorem, there exist Radon measures $i = 1 , \ldots , n$ $\rho _ { i }$ , and , and let $\rho _ { i }$ measurable functions $\begin{array} { r } { \mu = \Sigma _ { i = 1 } ^ { n } \rho _ { i } , \nu _ { i } = \tau _ { i } \frac { d \rho _ { i } } { d \mu } , i = 1 , \dots , n } \end{array}$ $\tau _ { i } ( x )$ on $\Omega$ , with . such that $| \tau _ { i } ( x ) | \leq 1 , \rho _ { i } -$ a.e.,

$$
\int_ {\Omega} u \operatorname {d i v} \phi = - \int_ {\Omega} \Sigma_ {1} ^ {n} \phi_ {i} \tau_ {i} d \rho_ {i} = - \int_ {\Omega} (\phi \cdot \nu) d \mu , \tag {4.48}
$$

where $\nu ( x ) = ( \nu _ { 1 } ( x ) , \ldots , \nu _ { n } ( x ) )$ with $\| \nu ( x ) \| _ { \infty } \leq 1 \mu -$ a.e. We define $\| \cdot \| : =$ $\| \cdot \| _ { R ^ { n } }$ .

Definition 4.5.1 Let $\Omega \subset \mathbb { R } ^ { n }$ be open. A function $u \in L ^ { 1 } ( \Omega )$ is said to be of bounded variations, if the total variation of u on $\Omega$ is:

$$
\| D u \| (\Omega) := \sup  \left\{\int_ {\Omega} u \operatorname {d i v} \phi \mid \phi \in C _ {0} ^ {1} (\Omega , \mathbb {R} ^ {n}), \| \phi (x) \| \leq 1, f o r a. e. x \in \Omega \right\} <   \infty .
$$

The space $B V ( \Omega )$ consists of all bounded variation functions on $\Omega$ with norm:

$$
\left\| u \right\| _ {B V} = \left\| u \right\| _ {L ^ {1}} + \left\| D u \right\| (\Omega).
$$

Since $\| . \| _ { R ^ { n } }$ and $\| . \| _ { \infty }$ are equivalent, for $u \in B V ( \Omega )$ , the total variation $\lVert D u \rVert$ can be regarded as a measure $\mu$ :

$$
\frac {1}{\sqrt {n}} \mu (\Omega) \leq \| D u \| (\Omega) \leq \sqrt {n} \mu (\Omega). \tag {4.49}
$$

Moreover, the distributional derivative $_ { D u }$ makes sense, it is related to the Radon measure $\mu$ and vector measurable function $\nu ( x )$ .

Example 4.5.2 If $u \in W ^ { 1 , 1 } ( \Omega )$ , then $u \in B V ( \Omega )$ , and

$$
\left\| u \right\| _ {B V} = \left\| u \right\| _ {W ^ {1, 1}}.
$$

In fact, we shall verify that $\begin{array} { r } { \| D u \| ( \Omega ) = \int _ { \Omega } | \nabla u | d x } \end{array}$ . On one hand,

$$
\left| \int_ {\Omega} u \operatorname {d i v} \phi d x \right| = \left| \int_ {\Omega} \nabla u \cdot \phi d x \right| \leq \int_ {\Omega} | \nabla u | d x
$$

for all $\phi \in \cal { C } _ { 0 } ^ { 1 } ( \Omega , R ^ { n } )$ with $\| \phi ( x ) \| \leq 1 \forall x \in \Omega$ , it follows that $\| D u \| ( \Omega ) \leq$ $\int _ { \Omega } | \nabla u | d x$ .

On the other hand, $\forall u \in W ^ { 1 , 1 } ( \Omega ) , \forall \epsilon > 0 , \exists \phi _ { \epsilon } \in C _ { 0 } ^ { 1 } ( \Omega , \mathbb { R } ^ { n } )$ with $\| \phi _ { \epsilon } ( x ) \| \le$ 1, $\forall x \in \Omega$ satisfying:

$$
\int_ {\Omega} | \nabla u | d x \leq \int_ {\Omega} \nabla u \cdot \phi_ {\epsilon} d x + \epsilon = \int_ {\Omega} u \cdot \operatorname {d i v} \phi_ {\epsilon} d x + \epsilon \leq \| D u \| (\Omega) + \epsilon .
$$

Since $\epsilon > 0$ is arbitrary, we obtain $\begin{array} { r } { \int _ { \Omega } | \nabla u | d x \leq \| D u \| ( \Omega ) } \end{array}$ .

Example 4.5.3 Let $S \subset \mathbb { R } ^ { n }$ be a $C ^ { \infty }$ compact $( n - 1 )$ -dimensional hypersurface with the induced metric, and let $H ^ { n - 1 }$ be the $( n { - } 1 )$ -dimensional Hausdorff measure in $\mathbb { R } ^ { n }$ . The area of $S$ is $H ^ { n - 1 } ( S )$ . Let $\Omega$ be the body bounded by $S$ , and let $\chi \Omega$ be the characteristic function of $\Omega$ . Then $\chi \Omega \in B V ( R ^ { n } )$ , and

$$
\left\| D \chi_ {\Omega} \right\| (\mathbb {R} ^ {n}) = H ^ {n - 1} (S).
$$

Indeed, by the Gauss formula, $\forall \phi \in C _ { 0 } ^ { 1 } ( \mathbb { R } ^ { n } , \mathbb { R } ^ { n } )$

$$
\int_ {R ^ {n}} \chi_ {\Omega} \operatorname {d i v} \phi d x = \int_ {\Omega} \operatorname {d i v} \phi d x = \int_ {S} \mathfrak {n} (x) \cdot \phi (x) d \mathsf {H} ^ {n - 1},
$$

where $\mathsf { n } ( x )$ is the unit exterior normal. Thus

$$
\left\| D \chi_ {\Omega} \right\| (\mathbb {R} ^ {n}) \leq \mathrm {H} ^ {n - 1} (S).
$$

On the other hand, one extends $\mathsf { n }$ to be a $C ^ { \infty }$ vector field $V$ over $\mathbb { R } ^ { n }$ with $\| V ( x ) \| \leq 1 \forall x \in \mathbb { R } ^ { n }$ . This can be done by a partition of unity. Then, $\forall \rho \in C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { n } , \mathbb { R } ^ { 1 } )$ with $| \rho ( x ) | \leq 1$ , $\forall x \in \mathbb { R } ^ { n }$ , let $\phi = \rho V$ , we have

$$
\int_ {R ^ {n}} \chi_ {\Omega} \operatorname {d i v} \phi d x = \int_ {S} \rho d \mathsf {H} ^ {n - 1}.
$$

Thus,

$$
\begin{array}{l} \| D _ {\chi_ {\Omega}} \| (\mathbb {R} ^ {n}) \\ = \sup  \left\{\int_ {\mathbb {R} ^ {n}} \chi_ {\Omega} \operatorname {d i v} \phi d x \mid \phi \in C _ {0} ^ {\infty} (\mathbb {R} ^ {n}, \mathbb {R} ^ {n}), \text {w i t h} \| \phi (x) \| \leq 1, \forall x \in \mathbb {R} ^ {n} \right\} \\ \geq s u p \left\{\int_ {S} \rho d \mathsf {H} ^ {n - 1} \mid \rho \in C _ {0} ^ {\infty} (\mathbb {R} ^ {n}, \mathbb {R} ^ {1}), | \rho (x) | \leq 1, \forall x \in \mathbb {R} ^ {n} \right\} \\ = \mathsf {H} ^ {n - 1} (S). \\ \end{array}
$$

These two examples show that $W ^ { \mathrm { 1 } , \mathrm { 1 } } ( \Omega )$ is strictly contained in $B V ( \Omega )$ , since for $n = 1$ , the characteristic function $\chi _ { [ 0 , 1 ] } \in B V ( \mathbb { R } ^ { 1 } )$ but not in $W ^ { 1 , 1 }$ .

This leads us to extend the definition of the co-dimensional one area to the boundary of more general domains.

Definition 4.5.4 Let $E$ be a Borel set in an open domain $\Omega \subset \mathbb { R } ^ { n }$ . We call

$$
\| \partial E \| (\Omega) = \| D _ {\chi_ {E}} \| (\Omega)
$$

the perimeter of $E$ in $\Omega$ .

As a function space with the norm $\| u \| _ { B V } = \| u \| _ { L ^ { 1 } } + \| D u \| ( \Omega )$ , $B V ( \Omega )$ is a Banach space. Only the completeness remains to be verified.

Lemma 4.5.5 (Lower semi-continuity) If $\{ u _ { j } \} \subset B V ( \Omega )$ , and $u _ { j }  u$ in $L ^ { 1 }$ ; then for every open $U \subset \Omega$ 2

$$
\| D u \| (U) \leq \lim  _ {j \rightarrow \infty} \inf  _ {j \rightarrow \infty} \| D u _ {j} \| (U). \tag {4.50}
$$

If further, $\operatorname* { s u p } \{ \| D u _ { j } \| ( \Omega ) | j \in \mathcal { N } \} < \infty$ , then $u \in B V ( \Omega )$ .

Proof. $\forall \phi \in C _ { 0 } ^ { 1 } ( U , \mathbb { R } ^ { n } )$ with $\| \phi ( x ) \| \leq 1$ , one has

$$
\int_ {U} u \operatorname {d i v} \phi d x = \lim  _ {j \rightarrow \infty} \int_ {U} u _ {j} \operatorname {d i v} \phi d x \leq \lim  _ {j \rightarrow \infty} \inf  _ {j \rightarrow \infty} \| D u _ {j} \| (U).
$$

(4.50) is proved.

Theorem 4.5.6 $B V ( \Omega )$ is complete.

Proof. For a Cauchy sequence $\{ u _ { j } \}$ in the BV norm, it is obvious that $u _ { j }  u$ in $L ^ { 1 }$ . By the previous lemma, $\| D u \| ( \Omega ) < \infty$ , and then $u \in B V ( \Omega ) .$ It remains to show that $\| D ( u _ { j } - u ) \| ( \Omega ) \to 0$ . Again, from the lower semi-continuity lemma, $\forall \epsilon > 0 , \exists j _ { 0 } \in \mathcal { N }$ such that

$$
\| D (u _ {j} - u) \| (\Omega) \leq \lim  _ {k \rightarrow \infty} \inf  _ {k \rightarrow \infty} \| D (u _ {j} - u _ {k}) \| (\Omega) <   \epsilon , \text {a s} j \geq j _ {0}.
$$

□

Now we consider the possibility of $C ^ { \infty }$ approximation of the BV functions. Since the $W ^ { 1 , 1 }$ norm equals the BV norm for $C ^ { 1 }$ functions, and $C ^ { \infty }$ is dense in $W ^ { 1 , 1 }$ , we can only have:

Theorem 4.5.7 Let $\Omega$ be an open domain of $\mathbb { R } ^ { n }$ . Then for $\forall u \in B V ( \Omega )$ , $\exists u _ { j } \in B V ( \Omega ) \cap C ^ { \infty } ( \Omega )$ such that

1. $u _ { j }  u$ in $L ^ { 1 } ( \Omega )$   
$\boldsymbol { \mathcal { Z } }$ . $\| D u _ { j } \|  \| D u \|$ in the sense of Radon measure.

In particular, $\lVert D u _ { j } \rVert ( \Omega ) \to \lVert D u \rVert ( \Omega )$ .

We omit the proof, but refer to Giusti [Gi], p. 14.

Theorem 4.5.8 (Compactness) Let $\Omega$ be a bounded open domain of $\mathbb { R } ^ { n }$ . Any sequence $\{ u _ { j } \} \subset B V ( \Omega )$ with $\| u _ { j } \| _ { B V } ~ \le ~ M ~ < ~ \infty \quad$ possesses a convergent subsequence in the $L ^ { 1 }$ norm, and the limit $u \in B V ( \Omega )$ , with $\| u \| _ { B V } \leq M$ .

Proof. We take a sequence $\{ v _ { j } \} \subset B V ( \Omega ) \cap C ^ { \infty } ( \Omega )$ such that $\| u _ { j } - v _ { j } \| _ { L ^ { 1 } } <$ $\textstyle { \frac { 1 } { j } }$ , and $\begin{array} { r } { \| D v _ { j } \| ( \Omega ) \ \leq \ M + \ \frac { 1 } { j } } \end{array}$ . From Example 4.5.2, we have $\| v _ { j } \| _ { W ^ { 1 , 1 } } =$ $\| D v _ { j } \| _ { B V }$ . According to the Rellich–Kondrachev compactness theorem, there is a subsequence $v _ { j _ { k } } L ^ { 1 }$ -converges to $u$ . From the lower semi-continuity lemma, $\| D u \| ( \Omega ) \leq M$ , and $u \in B V ( \Omega )$ . Obviously, in $L ^ { 1 }$ . □ $u _ { j _ { k } } \to u$

Besides the above preparations, in order to study the minimal surface problem stated at the beginning of the subsection we have to define the trace of BV functions. The following theorem can be found in Giusti [Gi] and Evans and Gariepy [EG]:

Theorem 4.5.9 (Trace) Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded domain with Lipschitzian boundary ∂Ω. Then the trace operator $T \ : : \ : B V ( \Omega ) \ :  \ : \ : L ^ { 1 } ( \partial \Omega , H ^ { n - 1 } )$ i s bounded, and we have

$$
\int_ {\Omega} u \operatorname {d i v} \phi = - \int_ {\Omega} (\phi \cdot \nu) d \mu + \int_ {\partial \Omega} T u (\phi \cdot n) d H ^ {n - 1}, \forall \phi \in C _ {0} ^ {1} (\Omega , \mathbb {R} ^ {n}),
$$

where $\mu$ is the Radon measure, $\nu$ is the $\mu$ -measurable vector function with $\| \nu ( x ) \| \leq 1$ , a.e. with respect to $\mu$ , and n is the unit normal vector field over $\partial \Omega$ , which is well defined almost everywhere.

We omit the proof and turn to the existence of the minimal surface problem.

Theorem 4.5.10 (Nonparametric minimal surface) Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded Lipschitzian domain, and let $u _ { 0 } \in B V ( \Omega )$ . Then there exists a minimizer of the problem:

$$
\operatorname {M i n} \left\{A (u) \mid u \in B V (\Omega), T u = T u _ {0} \right\},
$$

where

$$
A (u) = \int_ {\Omega} \sqrt {1 + | D u | ^ {2}}
$$

$$
\begin{array}{l} := \sup  \left\{\int_ {\Omega} \left(\phi_ {0} + u \operatorname {d i v} \phi\right) d x \mid \left(\phi_ {0}, \phi\right) \in C _ {0} ^ {1} (\Omega , \mathbb {R} ^ {1 + n}) \right. \\ \left. \text {w i t h} | \phi_ {0} (x) | + \| \phi (x) \| \leq 1 \forall x \in \Omega \right\}. \\ \end{array}
$$

Proof. One considers the set: $X _ { M } \ : = \ \left\{ u \ \in \ B V ( \Omega ) | T u \ = \ T u _ { 0 } \right.$ , and $\| D u \| ( \Omega ) \leq M \}$ , for any $M \ > \ 0$ . For sufficiently large $M \ > \ 0$ , $X _ { M } \ \ne \ 0$ since we have

$$
\| D u \| (\Omega) \leq A (u) \leq \| D u \| (\Omega) + \operatorname {m e s} (\Omega).
$$

Combining with Theorems 4.5.7 and 4.5.8, $X _ { M }$ is compact in $L ^ { 1 }$ -topology. By the same proof as for Lemma 4.5.5, $A$ is l.s.c. on $X _ { M }$ in $L ^ { 1 }$ -topology. The proof follows from the general principle of the direct method, i.e., Theorem 4.2.1. □

The problem stated above is called the nonparametric minimal surface problem, there is another formulation called parametric minimal surface problem:

Again, let $\Omega$ be a bounded open domain in $R ^ { n }$ , and let $L$ be a set of finite perimeters. Define

$$
\mathsf {F} = \left\{F \text {i s B o r e l m e a s u r a b l e} \mid F \backslash \Omega = L \backslash \Omega \right\}.
$$

Theorem 4.5.11 (De Giorgi) The problem

$$
\operatorname {M i n} \left\{\left\| D _ {\chi_ {F}} \right\| \left(R ^ {n}\right) \mid F \in \mathcal {F} \right\}
$$

has a solution.

Proof. Since $\Omega$ is bounded, $\exists R > 0$ such that $\Omega \subset B _ { R } ( \theta )$ . If $F \langle \Omega = L \langle \Omega$ , then $F \backslash B _ { R } ( \theta ) = L \backslash B _ { R } ( \theta )$ . It follows that

$$
\left\| D \chi_ {F} \right\| \left(R ^ {n}\right) = \left\| D \chi_ {F} \right\| \left(B _ {R} (\theta)\right) + \left\| D \chi_ {L} \right\| \left(R ^ {n} \backslash B _ {R} (\theta)\right).
$$

Thus the problem is reduced to

$$
\operatorname {M i n} \left\{\| D _ {\chi_ {F}} \| \left(B _ {R} (\theta)\right) \mid F \in \mathsf {F} \right\}.
$$

The functional $J ( u ) : = \| D u \| ( B _ { R } ( \theta ) )$ is bounded from below on the set $X : = \{ u = \chi _ { F } | \| D u \| ( B _ { R } ( \theta ) ) \leq \| \partial \chi _ { L } \| ( B _ { R } ( \theta ) ) \}$ .

We shall verify that $X$ is a compact subset under $L ^ { 1 }$ -topology. According to Theorem 4.5.8, it remains to verify that any limit point $u$ of $X$ is a characteristic function of a measurable set of finite perimeter. In fact, it is the a.e. limit of a sequence of $X$ .

Since $J$ is l.s.c. on $X$ under $L ^ { 1 }$ -topology, there exists a minimizer $u$ .

Although we have proved the existence of minimal surfaces, the solutions we obtained are very weak. Thus, in order to verify that the weak solutions are geometric minimal surfaces, the difficulties lie in the regularity, cf E. Giusti [EG] and De Giorgi [DG 1].

# 4.5.2 Hardy Space and BMO Space

It is known that a bounded sequence in $L ^ { 1 }$ space may not have a w∗-convergent subsequence. Therefore $L ^ { 1 }$ is not a good space for the variational method on which we work. Comparing with the $L ^ { p }$ space $p \in ( 1 , \infty )$ , the latter has many important properties in analysis: the uniform convexity, the reflexivity, the smoothness of the norm, the boundedness of singular integral operators in Calderon–Zygmund theory, as well as the $L ^ { p }$ estimates in the elliptic theory, which $L ^ { 1 }$ space does not have. The Hardy space $H ^ { \mathrm { { l } } }$ is in some sense a replacement of $L ^ { 1 }$ . It is defined as follows:

Definition 4.5.12 (Hardy space)

$$
\boldsymbol {H} ^ {1} (\mathbb {R} ^ {N}) = \left\{f \in L ^ {1} (\mathbb {R} ^ {N}) \mid R _ {j} f \in L ^ {1} (\mathbb {R} ^ {N}), j = 1, 2, \dots , N \right\}.
$$

where $R _ { j } = \partial _ { j } ( - \varDelta ) ^ { - \frac { 1 } { 2 } }$ is the Riesz transform, $j = 1 , 2 , \dots , N$ . The norm is defined to be

$$
\left\| f \right\| _ {\boldsymbol {H} ^ {1}} = \left\| f \right\| _ {L ^ {1}} + \sum_ {1} ^ {N} \left\| R _ {j} f \right\| _ {L ^ {1}}.
$$

There are many equivalent characterizations of the Hardy space. The following is one of them. $\forall h \in C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { N } )$ , with $h \geq 0$ , and $\textstyle { \int h = 1 }$ . $\forall t > 0$ let $\begin{array} { r } { h _ { t } ( x ) = \frac { 1 } { t ^ { N } } h ( \frac { x } { t } ) } \end{array}$ . For every distribution $f$ , one defines

$$
f ^ {*} (x) = \sup  _ {t > 0} | (h _ {t} * f) (x) |.
$$

If $f ^ { * } \in L ^ { 1 }$ , then from the Lebesgue dominance theorem, we have $f \in L ^ { 1 }$ and $\| f \| _ { L ^ { 1 } } \leq \| f ^ { * } \| _ { L ^ { 1 } }$ . By definition $f ^ { * }$ depends on $h$ , but it is proved that $f \in H ^ { 1 }$ if and only if $f ^ { * } \in L ^ { 1 }$ for any such $h$ , and

$$
\| f \| = \left\| f ^ {*} \right\| _ {L ^ {1}}
$$

is an equivalent norm. $\mathbf { H } ^ { 1 }$ is a Banach space.

In order to study the $\mathbf { H } ^ { 1 }$ space, we need some knowledge on harmonic analysis, in particular, the notion of Hardy-Littlewood maximal functions: $\forall f \in L _ { \mathrm { l o c } } ^ { 1 }$ , the function

$$
M (f) (x) = \sup _ {r > 0} \frac {1}{| B _ {r} (x) |} \int_ {B _ {r} (x)} | f (y) | d y
$$

is called the maximal function of $f$ . It possesses the following properties:

1. $M ( c f ) = | c | M ( f ) , \forall c \in \mathbb { R } ^ { 1 } , \mathrm { a n d } M ( f _ { 1 } + f _ { 2 } ) \leq M ( f _ { 1 } ) + M ( f _ { 2 } ) , \forall f _ { 1 } , f _ { 2 } \in \mathbb { R } ^ { 1 } .$ $M ( c f ) = | c | M ( f ) , \forall c \in \mathbb { R } ^ { 1 }$ $L _ { \mathrm { l o c } } ^ { 1 }$ .   
2. $| f ( x ) | \leq M ( f ) ( x )$ a.e.   
3. If $M ( f ) \in L ^ { 1 } ( \mathbb { R } ^ { n } )$ , then $f = 0$ a.e.

Proof. If $| f | > 0$ on a positive measure set $E$ , then we may assume that $\exists R > 0 , \exists \epsilon > 0$ such that $E \subset B _ { R } ( \theta ) , | f ( x ) | \geq \epsilon , \forall x \in E$ . Thus,

$$
M (f) (x) \geq C _ {N} \epsilon | E | (| x | + R) ^ {- N},
$$

where $C _ { N }$ is a constant depending on $N$ only. It contradicts $M ( f ) \in$ $L ^ { 1 } ( \mathbb { R } ^ { N } )$ . □

4. The maximal function operator is of weak $( 1 , 1 )$ type, i.e., there exists a constant $C > 0$ such that

$$
| E _ {\lambda} | \leq \frac {C}{\lambda} \| f \| _ {L ^ {1}} \forall \lambda > 0, f \in L ^ {1} (\mathbb {R} ^ {N}),
$$

where $E _ { \lambda } = \{ x \in \mathbb { R } ^ { N } \mid M ( f ) ( x ) > \lambda \} \mathrm { { } }$ .

Proof. For $\forall x \in E _ { \lambda } , \exists r _ { x } > 0$ such that

$$
\int_ {B _ {r _ {x}} (x)} | f | d x \geq \lambda | B _ {r _ {x}} (x) |.
$$

Thus, $\{ B _ { r _ { x } } ( x ) \mid x \in E _ { \lambda } \}$ is a covering of $E _ { \lambda }$ . According to the Vitali covering theorem, there exist a family of countable disjoint balls $\{ B _ { r _ { x _ { j } } } ( x _ { j } ) \}$ and a constant $c _ { N }$ such that

$$
\left| E _ {\lambda} \right| \leq c _ {N} \sum_ {j} \left| B _ {r _ {x _ {j}}} (x _ {j}) \right| \leq c _ {N} \lambda^ {- 1} \sum_ {j} \int_ {B _ {r _ {x _ {j}}} (x _ {j})} | f | d x \leq c _ {N} \lambda^ {- 1} \| f \| _ {L ^ {1}}.
$$

5. For $p \in ( 1 , \infty )$ , if $f \in L ^ { p } ( \mathbb { R } ^ { N } )$ , then $M ( f ) \in L ^ { p } ( \mathbb { R } ^ { N } )$ and there exists a constant $C _ { p } > 0$ such that

$$
\left\| M (f) \right\| _ {L ^ {p}} \leq C _ {p} \| f \| _ {L ^ {p}}.
$$

Proof. Since $M$ is of $( \infty , \infty )$ type, in combination with property 4, the conclusion follows from the interpolation theory. □

6. (Kolmogorov’s inequality) For any $\delta \in ( 0 , 1 )$ , there is a constant $C = C _ { \delta } >$ 0 such that

$$
\int_ {E} M (f) ^ {\delta} d x \leq \frac {1}{1 - \delta} C ^ {\delta} | E | ^ {1 - \delta} \| f \| _ {L ^ {1}} ^ {\delta}, \forall \text {m e a s u r a b l e} E \subset \mathbb {R} ^ {N}.
$$

Proof. In fact from property 4,

$$
\begin{array}{l} \int_ {E} M (f) ^ {\delta} d x = \delta \int_ {0} ^ {\infty} | \{x \mid M (f) (x) > \lambda \} \cap E | \lambda^ {\delta - 1} d \lambda \\ = \delta \left[ \int_ {0} ^ {t} + \int_ {t} ^ {\infty} \right] | \{x \mid M (f) (x) > \lambda \} \cap E \mid \lambda^ {\delta - 1} d \lambda \\ \end{array}
$$

$$
\begin{array}{l} \leq | E | t ^ {\delta} + C \delta \int_ {t} ^ {\infty} \lambda^ {\delta - 2} d \lambda \| f \| _ {L ^ {1}} \\ \leq | E | t ^ {\delta} + \frac {C \delta}{1 - \delta} t ^ {\delta - 1} \| f \| _ {L ^ {1}} /;, \\ \end{array}
$$

where $C$ is the weak $( 1 , 1 )$ norm of $M ( f )$ . Setting $\begin{array} { r } { t = \frac { C \| f \| _ { L ^ { 1 } } } { | E | } } \end{array}$ , we obtain the desired inequality. □

We investigate the behavior of $\mathbf { H } ^ { 1 }$ functions.

1. There exists a constant $C _ { 1 } > 0$ such that $f ^ { * } \leq C _ { 1 } M ( f )$ . If $f \geq 0$ , then $M ( f ) \leq C _ { 2 } f ^ { * }$ for some constant $C _ { 2 }$ . Thus if $f \geq 0$ and $f \in \mathbf { H } ^ { 1 }$ , then $f = 0$ .   
2. If $f \in \mathbf { H } ^ { 1 }$ , then $\textstyle \int _ { \mathbb { R } ^ { N } } f d x = 0$ .

Proof. Let $\phi \in C ^ { \infty } (  { \mathbb { R } } ^ { N } )$ with $\operatorname { s u p p } \phi \subset B _ { 1 } ( \theta ) \operatorname { a n d } \phi ( \theta ) = 1 . \forall x \in \mathbb { R } ^ { N } , \forall s \geq | x |$ , one defines $h ( y ) = c \phi ( s ^ { - 1 } x - 2 y )$ , where $c$ is the normalized constant such that $\begin{array} { r } { \int _ { \mathbb { R } ^ { N } } h ( y ) d y = 1 } \end{array}$ . Then

$$
\begin{array}{l} f ^ {*} (x) = \sup  _ {t > 0} | (h _ {t} * f) (x) | \\ \geq (2 s) ^ {- N} \left| \int_ {\mathbb {R} ^ {N}} h \left(\frac {x - y}{2 s}\right) f (y) d y \right| \\ = (2 s) ^ {- N} c \left| \int_ {\mathbb {R} ^ {N}} \phi \left(\frac {y}{s}\right) f (y) d y \right|, \\ \end{array}
$$

since the last integral tends to $\int _ { \mathbb { R } ^ { N } } f ( y ) d y$ as $s \longrightarrow \infty$ . If $\textstyle \int _ { \mathbb { R } ^ { N } } f ( y ) d y \neq 0$ , then we would have $\begin{array} { r } { \operatorname* { l i m } \operatorname* { i n f } _ { | x | \to \infty } f ^ { * } ( x ) | x | ^ { N } > 0 } \end{array}$ , which contradicts $f ^ { * } \in L ^ { 1 } ( \mathbb { R } ^ { N } )$ .

3. If $f \in L ^ { \infty } ( \mathbb { R } ^ { N } )$ with compact suppf and $\textstyle \int _ { \mathbb { R } ^ { N } } f ( x ) d x = 0$ , then $f \in \mathbf { H } ^ { 1 }$

Proof. Assume $\operatorname { s u p p } f \subset B _ { R } ( \theta )$ . Since

$$
\begin{array}{l} \left| \left(h _ {t} * f\right) (x) \right| = \left| \int_ {\mathbb {R} ^ {N}} \left[ h _ {t} (x - y) - h _ {t} (\theta) \right] f (y) d y \right| \\ = \left| \int_ {\mathbb {R} ^ {N}} \int_ {0} ^ {1} (x - y) \cdot \nabla h _ {t} (s (x - y)) d s f (y) d y \right| \\ \leq C \int_ {| x - y | \leq t, | y | \leq R} t ^ {- (N + 1)} d y \| f \| _ {\infty}, \\ \end{array}
$$

therefore $f ^ { * } ( x ) \leq C R | x | ^ { - N - 1 } \| f \| _ { \infty }$ . The conclusion follows.

The dual space of $\mathbf { H } ^ { 1 }$ is the BMO space, which is defined as follows: $\forall f \in L _ { l o c } ^ { 1 } ( \mathbb { R } ^ { N } ) , \forall x \in \mathbb { R } ^ { N } , \forall r > 0$ , let

$$
\overline {{f}} _ {r} (x) = \frac {1}{| B _ {r} (x) |} \int_ {B _ {r} (x)} f (y) d y,
$$

and define

Definition 4.5.13 (BMO space)

$$
\| f \| _ {\mathrm {B M O}} = \sup _ {x \in R ^ {N}} \sup _ {r > 0} \frac {1}{| B _ {r} (x) |} \int_ {B _ {r} (x)} | f (y) - \overline {{f}} _ {r} (x) | d y.
$$

BMO( $\mathbb { R } ^ { N }$ ) is the space of functions such that $\| f \| _ { \mathrm { B M O } } < \infty$ .

The quantity $\| f \| _ { \mathrm { B M O } }$ is a semi-norm; in fact, $\| f \| _ { \mathrm { B M O } } = 0$ if and only if $f = \mathrm { c o n s t a n t }$ . After modulo constants, BMO is a Banach space.

There is an equivalent BMO semi-norm:

$$
\| f \| ^ {\prime} = \sup  _ {x \in \mathbb {R} ^ {N}} \sup  _ {r > 0} \inf  _ {c} \frac {1}{| B _ {r} (x) |} \int_ {B _ {r} (x)} | f (y) - c | d y.
$$

In fact, on one hand it is obvious $\| \boldsymbol { f } \| ^ { \prime } \leq \| \boldsymbol { f } \| _ { \mathrm { B M O } }$ , and on the other hand,

$$
f _ {B _ {r} (x)} | f (y) - \overline {{f}} _ {r} (x) | d y \leq f _ {B _ {r} (x)} | f (y) - c | d y + | \overline {{f}} _ {r} (x) - c | \leq 2 \| f \| ^ {\prime},
$$

where $f$ denotes the average.

The following simple properties are easily seen from the definition.

1. $L ^ { \infty } \subset \mathrm { B M O } \subset L _ { \mathrm { l o c } } ^ { 1 }$

2. The function $\log | x | \in$ BMO. In fact, the scaling transformation: $\forall \lambda \ >$ $0 , f ( x ) \to f ( \lambda x )$ maps BMO functions to BMO functions, and preserves the semi-norm. Under the scaling, $\log | x |$ is changed by adding a constant. The verification is then reduced to

$$
\int_ {B} | \log | x | | d x \leq C, \mathrm {o r} \int_ {B} | \log | x | - \log | x _ {0} | | d x \leq C,
$$

where $B$ is unit ball centered at $x _ { 0 }$ , The first inequality is true for $| x _ { 0 } | \le 1$ and the second for $| x _ { 0 } | \geq 1$ . These hold by elementary calculation.

3. If $f \in \mathrm { B M O }$ , then $| f | \in \mathrm { B M O }$ , and then $f _ { \pm } \in \mathrm { B M O }$ .

Let $\begin{array} { r } { f _ { \Omega } = \frac { 1 } { | \Omega | } \int _ { \Omega } } \end{array}$ denote the average; one has

$$
\left. f _ {B _ {r} (x)} | | f (y) | - | c | | d y \leq f _ {B _ {r} (x)} | f (y) - c | d y, \right.
$$

which implies that $\| | | f | \| ^ { \prime } \leq \| f \| ^ { \prime }$ .

In the case where $( x , r )$ is not specified, we define $Q = B _ { r } ( x )$ and $m _ { Q } ( f ) =$ ${ \overline { { f } } } _ { r } ( x )$ .

4. If $f \in L _ { l o c } ^ { 1 } (  { \mathbb { R } } ^ { N } )$ and if there exists $C > 0$ such that $m _ { Q } ( f ) - \mathrm { e s s i n f } _ { Q } f \leq$ $C \forall Q$ , then $f \in \mathrm { B M O }$ and $\| f \| _ { \mathrm { B M O } } \leq 2 C$ .

Proof. Since $\begin{array} { r } { \int _ { Q } [ f ( y ) - m _ { Q } ( f ) ] d y = 0 } \end{array}$ ,

$$
\begin{array}{l} f _ {Q} | f (y) - m _ {Q} (f) | d y = 2 f _ {Q} \max  [ m _ {Q} (f) - f (y), 0 ] d y \\ \leq 2 f _ {Q} [ m _ {Q} (f) - \mathrm {e s s i n f} _ {Q} f ] d y \leq 2 C. \\ \end{array}
$$

□

5. Let $f \in L _ { \mathrm { l o c } } ^ { 1 } ( \mathbb { R } ^ { N } )$ with $f \geq 0$ . If there exists a constant $C > 0$ such that $m _ { Q } ( f ) \leq C \mathrm { e s s i n f } _ { Q } f ,$ Q, then $\log f \in \mathrm { B M O }$ .

Proof. Taking logarithms and using Jessen’s inequality, we have

$m _ { Q } ( \log f ) = f _ { Q } \log f d y \le \log f _ { Q } f d y = \log m _ { Q } ( f ) \le \log C + \exp \mathrm { e s s i n f } _ { Q } \log f .$

The conclusion follows from property 4.

6. Assume $f \in L _ { \mathrm { l o c } } ^ { 1 } ( \mathbb { R } ^ { N } )$ and that $M ( f ) ( x ) \neq \infty$ a.e. $\forall \delta \in ( 0 , 1 )$ , set $w =$ $M ( f ) ^ { \delta }$ . Then there exists a constant $C = C _ { \delta } > 0$ independent of $Q$ , such that $m _ { Q } ( w ) \leq C \mathrm { i n f } _ { Q } w$ , $\forall \ : Q$ .

Proof. Fixing $Q$ , we may assume $Q = B _ { R } ( \theta )$ without loss of generality. Decompose $f = f _ { 1 } + f _ { 2 }$ , where $f _ { 1 } = \chi _ { Q ^ { \prime } } \cdot f$ with $Q ^ { \prime } = 2 Q$ . Let $w _ { i } =$ $M ( f _ { i } ) ^ { \partial } , i \ = \ 1 , 2$ . Since $M ( f ) \le M ( f _ { 1 } ) + M ( f _ { 2 } )$ , there is a constant $c$ depending on $\delta$ , such that $w \leq c ( w _ { 1 } + w _ { 2 } )$ . Now we apply Kolmogorov’s inequality to $w _ { 1 }$ :

$$
\begin{array}{l} m _ {Q} (w _ {1}) = f _ {Q} M (f _ {1}) ^ {\delta} (y) d y \leq \frac {C _ {\delta}}{| Q |} | Q | ^ {1 - \delta} \| f _ {1} \| _ {L ^ {1}} ^ {\delta} \\ = 2 ^ {N} C _ {\delta} m _ {Q ^ {\prime}} (f) ^ {\delta} \\ \leq C _ {1} \inf  _ {Q ^ {\prime}} w \leq C _ {1} \inf  _ {Q} w \\ \end{array}
$$

where $C _ { 1 }$ is independent of $Q$ . Next we turn to estimating $w _ { 2 }$ . It remains to verify that there exists a constant $C _ { 2 } > 0$ such that

$$
M \left(f _ {2}\right) (x) \leq C _ {2} M \left(f _ {2}\right) (y), \forall x, y \in Q.
$$

Indeed, it implies that $w _ { 2 } ( x ) \ \leq \ C _ { 2 } \mathrm { i n f } _ { Q } w _ { 2 } \ \leq \ C _ { 2 } \mathrm { i n f } _ { Q } w \ \forall x \ \in \ Q$ , then $m _ { Q } ( w _ { 2 } ) \leq C _ { 2 } \mathrm { i n f } _ { Q } w$ . Combining these two estimates together, we obtain

$$
m _ {Q} (w) \leq c (m _ {Q} (w _ {1}) + m _ {Q} (w _ {2})) \leq C \operatorname * {i n f} _ {Q} w.
$$

Now we return to estimate $M ( f _ { 2 } )$ . Notice that for any $r > 0$ , only when $B _ { r } ( x ) \cap ( Q ^ { \prime } ) ^ { c } \neq \emptyset$ , i.e., $r \geq R , \int _ { B _ { r } ( x ) } | f _ { 2 } ( z ) | d z$ can be nonzero. From

$$
f _ {B _ {r} (x)} | f _ {2} (z) | d z \leq \frac {| B _ {r + R} (y) |}{B _ {r} (x)} f _ {B _ {r + R} (y)} | f _ {2} (z) | d z \leq 2 ^ {N} M (f _ {2}) (y),
$$

it follows that $M ( f _ { 2 } ) ( x ) \leq 2 ^ { N } M ( f _ { 2 } ) ( y )$ .

Combining properties 5 and 6, we obtain:

7. If $f \in L _ { \mathrm { l o c } } ^ { 1 } ( \mathbb { R } ^ { N } )$ and $M ( f ) ( x ) \neq \infty$ a.e., then $\log M ( f ) \in$ BMO and $\| \log M ( f ) \| _ { \mathrm { B M O } } \le c _ { N }$ , a constant depending on $N$ only.

Definition 4.5.14 (VMO space) The space VMO( $\mathbb { R } ^ { N }$ ) is the closure of $C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { N } )$ in BMO.

It is proved (see Stein [Ste 2]): that $( \mathrm { V M O } ) ^ { * } = \mathbf { H } ^ { 1 } , ( \mathbf { H } ^ { 1 } ) ^ { * } = \mathrm { B M O }$ , in the following sense, f  BMO, the following integral makes sense:

$$
l (g) = \int f \cdot g d x, \forall g \in {\bf H} _ {\alpha} ^ {1} := \left\{g \in L ^ {\infty} | \mathrm {s u p p} (g) \mathrm {i s c o m p a c t , a n d} \int g = 0 \right\}
$$

(see property 3 of $\mathbf { H } ^ { 1 }$ functions), and has a unique bounded extension to $\mathbf { H } ^ { 1 }$ :

$$
| l (g) | \leq C \| f \| _ {\mathrm {B M O}} \| g \| _ {\mathbf {H} ^ {1}}
$$

for some constant $C > 0$ .

Conversely, $\forall l \in ( \mathbf { H } ^ { 1 } ) ^ { * } , \exists f \in \mathrm { B M O }$ , such that $l ( g ) = \int f \cdot g \ \forall g \in { \bf H } _ { \alpha } ^ { 1 }$ , with $\| f \| _ { \mathrm { B M O } } \leq C \| l \| _ { ( \mathbf { H } ^ { 1 } ) ^ { * } }$ .

Since $\mathbf { H } ^ { 1 }$ is the dual of a separable Banach space VMO, the weak- $^ *$ topology on $\mathbf { H } ^ { 1 }$ is well defined, and then the Banach–Alaoglu theorem is applicable. Namely:

If $\{ f _ { j } \}$ is a bounded sequence in $\mathbf { H } ^ { 1 }$ , then there is a subsequence $\left\{ f _ { j } ^ { \prime } \right\}$ , which converges in the distribution sense to $f ~ \in ~ \bf H ^ { \mathrm { I } }$ , with $\begin{array} { r l } { \| f \| _ { \mathbf { H } ^ { 1 } } } & { { } \leq } \end{array}$ $\operatorname* { l i m } \operatorname* { i n f } _ { j \to \infty } \| f _ { j } \| _ { \mathbf { H } ^ { 1 } }$ .

# 4.5.3 Compensation Compactness

If we intend to use the space $\mathbf { H } ^ { 1 }$ in variational problems, then the prices we have to pay are: (1) To verify the $\mathbf { H } ^ { 1 }$ boundedness of nonlinear quantities appearing in the integrand of the functional along a minimizing sequence, and (2) such nonlinear quantities are sequentially weakly ∗ l.s.c.

There are few examples of typical nonlinear quantities, which are in the space $L ^ { 1 }$ with some additional compensation conditions so that they fall into $\mathbf { H } ^ { 1 }$ and preserve the $w ^ { * }$ -continuity.

Example 1. (div-curl) Suppose $\{ E _ { n } \} \subset L ^ { p } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } ) , \mathrm { d i v } E _ { n } = 0$ in the distribution sense, and $\{ B _ { n } \} \subset L ^ { p ^ { \prime } } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } ) , \mathrm { c u r l } B _ { n } = 0$ in the distribution sense, where $p \in ( 1 , \infty ) , \frac { 1 } { p } + \frac { 1 } { p ^ { \prime } } = 1$ . Assume that both $\{ E _ { n } \} , \{ B _ { n } \}$ are bounded in their own spaces respectively, then $E _ { n } \cdot B _ { n }$ is bounded in $L ^ { 1 } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { 1 } )$ . The compensation conditions $\mathrm { d i v } E _ { n } = 0$ , $\mathrm { c u r l } B _ { n } = 0$ play a role in the weak*- convergence.

Example 2. (Jacobian) Suppose $u \in L _ { \mathrm { l o c } } ^ { q } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } )$ , for some $q \in ( 1 , \infty )$ , in which $\nabla u \in L ^ { N } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N \times N } )$ . Then $\operatorname* { d e t } ( \nabla u ) \in L ^ { 1 } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { 1 } )$ . The special structure of the Jacobian is also a kind of compensation condition.

First we verify that the compensation condition implises the nonlinear quantities are indeed in $H ^ { 1 }$ , and then these nonlinear quantities are $w ^ { * }$ continuous along weakly convergent sequences.

Theorem 4.5.15 Suppose that $E \ \in \ L ^ { p } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } ) , B \ \in \ L ^ { p ^ { \prime } } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } )$ , with $p \in ( 1 , \infty ) , \frac { 1 } { p } + \frac { 1 } { p ^ { \prime } } = 1$ . If $\mathrm { c u r l } B = 0$ and $\mathrm { d i v } E = 0$ , then $\boldsymbol { E } \cdot \boldsymbol { B } \in \boldsymbol { H } ^ { 1 }$ and $\exists C > 0$ a constant such that $\| E \cdot B \| _ { H ^ { 1 } } \leq C \| E \| _ { p } \| B \| _ { p ^ { \prime } }$ .

Proof. Taking $\alpha , \beta$ satisfying $\begin{array} { r } { \frac { 1 } { \alpha } + \frac { 1 } { \beta } = 1 + \frac { 1 } { N } , \alpha \in ( 1 , p ) , \beta \in ( 1 , p ^ { \prime } ) , \forall h \in C _ { 0 } ^ { \infty } } \end{array}$ nonnegative, with $\textstyle \int h = 0$ , we claim that

$$
\left| \left\{h _ {t} * (E \cdot B) \right\} (x) \right| \leq C \left(f _ {B _ {t} (x)} | E | ^ {\alpha}\right) ^ {\frac {1}{\alpha}} \left(f _ {B _ {t} (x)} | B | ^ {\beta}\right) ^ {\frac {1}{\beta}},
$$

where $\begin{array} { r } { f _ { B } = \frac { 1 } { \left| B \right| } \int _ { B } } \end{array}$ denotes the average.

Indeed, From $\mathrm { c u r l } B = 0$ , we have $\omega \in W ^ { 1 , p ^ { \prime } } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { 1 } )$ such that $\nabla \omega = B$ . $\mathrm { d i v } E = 0$ yields

$$
\operatorname {d i v} (\omega E) = \omega \operatorname {d i v} E + E \cdot \nabla \omega = E \cdot B.
$$

By suitably choosing $h$ such that $\operatorname { s u p p } ( h ) \subset B _ { 1 } ( \theta )$ , we have

$$
\begin{array}{l} \left| h _ {t} * (E \cdot B) (x) \right| = \left| \int \frac {1}{t ^ {N + 1}} \nabla h \left(\frac {x - y}{t}\right) E (y) \omega (y) d y \right| \\ = \left| \int \frac {1}{t ^ {N + 1}} \nabla h \left(\frac {x - y}{t}\right) E (y) \left\{\omega - f _ {B _ {t} (x)} \omega \right\} d y \right| \\ \leq C (f _ {B _ {t} (x)} | E | ^ {\alpha}) ^ {\frac {1}{\alpha}} \left(f _ {B _ {t} (x)} \left[ \frac {1}{t} | \omega - f _ {B _ {t} (x)} \omega | \right] ^ {\alpha^ {\prime}}\right) ^ {\frac {1}{\alpha^ {\prime}}} \\ \leq C (f _ {B _ {t} (x)} | E | ^ {\alpha}) ^ {\frac {1}{\alpha}} (f _ {B _ {t} (x)} | \nabla \omega | ^ {\beta}) ^ {\frac {1}{\beta}} \\ \end{array}
$$

from the Sobolev–Poincar´e inequality.

With the aid of the maximal function of $f \in L _ { \mathrm { l o c } } ^ { 1 } ( \mathbb { R } ^ { n } )$ :

$$
M (f) (x) = \sup  _ {r > 0} \frac {1}{m \left(B _ {r} (x)\right)} \int_ {B _ {r} (x)} | f (y) | d y.
$$

The above inequality can be rewritten as

$$
\sup  _ {t > 0} h _ {t} * (E \cdot B) (x) \leq C (M (| E | ^ {\alpha}) (x)) ^ {\frac {1}{\alpha}} (M (| B | ^ {\beta}) (x)) ^ {\frac {1}{\beta}}.
$$

Again by Holder inequality and the $L ^ { p }$ boundedness of the maximal function, we have

$$
\begin{array}{l} \| E \cdot B \| _ {\mathbf {H} ^ {1}} = \int_ {t > 0} \sup  _ {h _ {t} * (E \cdot B) (x) | d x} \\ \leq C \left(\int (M (| E | ^ {\alpha})) ^ {\frac {p}{\alpha}} d x\right) ^ {\frac {1}{p}} \left(\int (M (| B | ^ {\beta})\right) ^ {\frac {p ^ {\prime}}{\beta}} d x) ^ {\frac {1}{p ^ {\prime}}} \\ \leq C \| E \| _ {p} \| B \| _ {p ^ {\prime}}. \\ \end{array}
$$

□

The second example can be reduced to the first one, since

$$
\det (\nabla u) = \sum_ {i} \partial_ {i} u ^ {1} A _ {i} ^ {1} = \nabla u ^ {1} \cdot \boldsymbol {\sigma},
$$

where $u = ( u ^ { 1 } , u ^ { 2 } , \ldots , u ^ { N } )$ , and $\sigma = ( A _ { 1 } ^ { 1 } , A _ { 2 } ^ { 1 } , \ldots , A _ { N } ^ { 1 } )$ is the $N$ -vector consisting of all $N - 1$ minors of $\operatorname* { d e t } ( \nabla \boldsymbol { u } )$ with respect to the first row. Define $E = \sigma$ and $\boldsymbol { B } = \nabla \boldsymbol { u } ^ { 1 }$ , then $E \in L ^ { \frac { N } { N - 1 } } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } )$ , satisfying $\mathrm { d i v } E = 0$ , and $B \in L ^ { N } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } )$ , satisfying $\mathrm { c u r l } B = 0$ .

According to Theorem 4.5.15, $\operatorname* { d e t } ( \nabla u ) \in \mathbf { H } ^ { 1 }$ and there exists a constant $C > 0$ such that

$$
\left\| \det (\nabla u) \right\| _ {\mathbf {H} ^ {1}} \leq C \| \nabla u \| _ {L ^ {N}} ^ {N}.
$$

Apart from the w∗ compactness, compensation compactness theory identifies classes of such nonlinear quantities, which are sequentially weakly ∗ continuous. Generally speaking, weak convergence may not preserve nonlinear quantities, e.g., if $u _ { n }  u , v _ { n }  v$ , then we cannot conclude that $u _ { n } v _ { n }  u v$ . This can be seen from the example $u _ { n } = v _ { n } = \sin ( n \pi x )$ in $L ^ { 2 } [ 0 , 1 ]$ . Both $u _ { n }$ and $v _ { n } \to 0$ , but $\begin{array} { r } { u _ { n } v _ { n } = \frac { 1 } { 2 } ( 1 - \cos ( 2 n \pi x ) )  \frac { 1 } { 2 } } \end{array}$ . However, luckily we have:

Theorem 4.5.16 Suppose that $\{ E _ { j } \} , \{ B _ { j } \}$ are two bounded sequences in $L ^ { 2 } ( \mathbb { R } ^ { N } )$ satisfying $\mathrm { d i v } ( E _ { j } ) = 0$ and $\operatorname { c u r l } ( B _ { j } ) = 0$ in the distribution sense. If further, $E _ { j }  E , B _ { j }  B$ in $L ^ { 2 } ( \mathbb { R } ^ { N } )$ , then $E _ { j } \cdot B _ { j }  E \cdot B$ in the distribution sense, and then $E _ { j } \cdot B _ { j } \ ^ { * }  \ E \cdot B$ in $H ^ { \scriptscriptstyle \mathrm { I } }$ .

Proof. If $B$ is curl free, we claim that there exists a distribution $w$ such that $\nabla w = B$ . This can be shown by Fourier transformations. Let $\hat { B } ( \xi )$ be the Fourier transform of $B$ , since $\mathrm { c u r l } B = 0$ implies that $\xi _ { i } \hat { B } _ { k } - \xi _ { k } \hat { B } _ { i } = 0$ , $\forall \ : i , k =$ $1 , 2 , \ldots , N$ . Let $w$ be the Fourier inverse transform of $\frac { \sum _ { 1 } ^ { N } \xi _ { k } \ddot { B } _ { k } } { | { \xi } | ^ { 2 } }$ . Obviously, $\nabla w = B$ . Moreover, if $B$ has a bounded support: $\mathrm { s u p p } B \subset B _ { R } ( \theta ) ^ { \circ }$ , then we have $w \in L ^ { 2 }$ and ${ \mathrm { s u p p } } w \subset B _ { R } ( \theta ) ^ { \circ }$ .

Now let $w _ { j }$ be the distribution such that $B _ { j } ~ = ~ \nabla w _ { j }$ . For a given $\phi \in \mathbf { \Xi }$ $C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { N } )$ , we want to prove that

$$
\int_ {\mathbb {R} ^ {N}} E _ {j} \cdot B _ {j} \phi d x \rightarrow \int_ {\mathbb {R} ^ {N}} E \cdot B \phi d x \text {a s} j \rightarrow \infty .
$$

In this case, we may assume all the supports supp $w _ { j } \subset B _ { R } ( \theta ) ^ { \circ } j = 1 , 2 , . . . ,$ for some $R > 0$ . Thus $\{ w _ { j } \} \subset H ^ { 1 } ( B _ { R } ( \theta ) )$ and is bounded. Modulo a subsequence, we may assume $w _ { j }  w$ in $L ^ { 2 } ( B _ { R } ( \theta ) )$ . Thus

$$
\begin{array}{l} \langle E _ {j} \cdot B _ {j}, \phi \rangle = \langle \operatorname {d i v} (w _ {j} E _ {j}), \phi \rangle \\ = - \left\langle E _ {j}, w _ {j} (\nabla \phi) \right\rangle \\ \rightarrow - \langle E, w (\nabla \phi) \rangle \\ = \langle \operatorname {d i v} (w E), \phi \rangle = \langle E \cdot B, \phi \rangle \\ \end{array}
$$

where in taking limits, we used $E _ { j } \to E$ and $w _ { j }  w$ in $L ^ { 2 } ( B _ { R } ( \theta ) )$ .

Since $C _ { 0 } ^ { \infty }$ is dense in VMO space, $E _ { j } \cdot B _ { j } \ ^ { * }  \boldsymbol { E } \cdot \boldsymbol { B }$ in $\mathbf { H } ^ { 1 }$ .

□

Remark 4.5.17 The above theorem has the following generalization: Let $\Omega \subset$ $\mathbb { R } ^ { N }$ be an open domain. Assume that $\{ E _ { j } \} , \{ B _ { j } \}$ are two bounded sequences in $L ^ { 2 } ( \Omega , \mathbb { R } ^ { N } )$ such that

1. $\{ \mathrm { d i v } E _ { j } \}$ lies in a compact subset of $W ^ { - 1 , 2 } ( \Omega )$ ,   
2. $\{ \mathrm { c u r l } B _ { j } \}$ lies in a compact subset of $W ^ { - 1 , 2 } ( \Omega , \mathbb { M } ^ { N \times N } )$

If further, $E _ { j } \to E$ and $B _ { j }  B$ in $L ^ { 2 } ( \Omega , \mathbb { R } ^ { N } )$ , then $E _ { j } \cdot B _ { j }  E \cdot B$ in the distribution sense.

Similarly, one has:

Theorem 4.5.18 Assume that $\{ u _ { j } \}$ is bounded in $W ^ { 1 , N } ( \mathbb { R } ^ { N } , \mathbb { R } ^ { N } )$ , and $u _ { j } $ $u$ . Then $\operatorname* { d e t } ( \nabla \boldsymbol { u } _ { j } )  \operatorname* { d e t } ( \nabla \boldsymbol { u } )$ in the distribution sense, so is $\mathrm { d i v } ( \nabla u _ { j } ) ^ { * } $ $\mathrm { d i v } ( \boldsymbol { \nabla } u )$ in $H ^ { 1 } ( R ^ { n } )$ .

Proof. Again we use the previous special decomposition:

$$
\det (\nabla u) = \sum_ {k} \partial_ {k} u ^ {1} A _ {k} ^ {1} = \nabla u ^ {1} \cdot \sigma = \nabla (u ^ {1} \cdot \sigma),
$$

where $\sigma = ( A _ { 1 } ^ { 1 } , A _ { 2 } ^ { 1 } , \ldots , A _ { N } ^ { 1 } )$ . Since $\mathrm { d i v } \sigma = 0$ , for $\forall \phi \in C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { N } )$ , one has

$$
\left\langle \det (\nabla u _ {j}), \phi \right\rangle = - \left\langle u _ {j} ^ {1} \cdot \sigma_ {j}, \nabla \phi \right\rangle\rightarrow - \left\langle u ^ {1} \cdot \sigma , \nabla \phi \right\rangle = \left\langle \det (\nabla u), \phi \right\rangle .
$$

□

One can define a Hardy space on a domain $\Omega$ in $\mathbb { R } ^ { N }$ . A distribution $f$ on $\Omega$ is said to be in $\mathbf { H } _ { \mathrm { l o c } } ^ { 1 } ( \Omega )$ if for each compact set $K \subset \Omega$ , there is an $\epsilon > 0$ so that

$$
\int_ {K} (\sup _ {0 <   t <   \epsilon} | h _ {t} * f (x) |) d x <   \infty ,
$$

where $h$ is defined as before, and $h _ { t } * f ( x )$ is defined for $h$ as long as $t \ <$ $\mathrm { d i s t } ( x , \partial \Omega )$ .

An equivalent definition is as follows:

$f \in { \mathbf { H } } _ { \mathrm { l o c } } ^ { 1 } ( \Omega )$ if and only if $\forall \boldsymbol { \phi } \in C _ { 0 } ^ { \infty } ( \Omega )$ with $\textstyle { \int _ { \Omega } \phi \neq 0 }$ , there is a constant $c$ such that $\phi ( f - c ) \in { \mathbf { H } } ^ { 1 } ( \mathbb { R } ^ { N } )$ .

# 4.5.4 Applications to the Calculus of Variations

We shall now show how the Hardy space is applied to variational problems.

Recall that in a compact space $X$ with Radon measure $\mu$ , a sequence $\{ f _ { j } \} \subset L ^ { 1 } ( X , B , \mu )$ is weakly precompact, i.e., there exists a weakly convergent subsequence, if and only if $\{ f _ { j } \}$ is bounded in $L ^ { 1 }$ norm and is uniformly absolutely continuous, see for instance [DS], Theorem IV.8.9. For an $L ^ { 1 }$ -bounded sequence $\{ f _ { j } \}$ without the latter condition, we cannot say whether it is weakly convergent in $L ^ { 1 }$ ; this can be seen from the $\delta$ -type sequences. However, Chacon introduced the following:

Lemma 4.5.19 (Bitting lemma) Let $\Omega \subset \mathbb { R } ^ { N }$ be a bounded open domain, and let $\{ u _ { j } \} \subset L ^ { 1 } ( \Omega )$ be bounded. Then there exist a subsequence and decreasing Borel measurable subsets $\{ E _ { k } \} \subset \Omega$ such that $| E _ { k } |  0$ and that $\chi _ { \Omega \backslash E _ { k } } | u _ { j } |$ i s uniformly absolutely continuous for all $k \in \mathcal N$ .

Proof. Without loss of generality, we may assume that $\{ u _ { j } \}$ are nonnegative. Let $\{ \nu _ { j } \}$ be the associated Young measures in $\mathbb { R } ^ { 1 }$ , which $w ^ { * }$ -converges in $\mathbb { R } ^ { N } \times \mathbb { R } ^ { 1 }$ to $\mathcal { L } ^ { N } \lfloor \Omega \otimes \nu _ { x }$ . $\forall R > 0$ , we define $g _ { R } ( y ) = y$ if $0 \leq y \leq R , =$ $2 R - y$ if $R \leq y \leq 2 R , \mathit { \Theta } = 0$ if $y > 2 R$ or $y < 0$ . Obviously, $g _ { R } \in C _ { 0 } ( \mathbb { R } ^ { 1 } )$ , and $| y \chi _ { [ 0 , R ] } ( y ) | \leq g _ { R } ( y ) \leq | y |$ . According to Remark 4.4.10, we have

$$
\begin{array}{l} \limsup_{j\to \infty}\int_{\{u_{j}\leq R\}}u_{j}d x\leq \limsup_{j\to \infty}\int_{\Omega \times \mathbb{R}^{1}}g_{R}(y)d\nu_{j}(x,y) \\ = \int_ {\Omega} \int_ {\mathbb {R} ^ {1}} g _ {R} (y) d \nu_ {x} (y) d x \\ \leq \int_ {\Omega} \int_ {\mathbb {R} ^ {1}} | y | d \nu_ {x} (y) d x <   \infty . \\ \end{array}
$$

We choose sequences $n _ { j } \geq 2 ^ { j } , k _ { j } \geq j$ such that

$$
\lim  _ {j \rightarrow \infty} \sup  _ {\left\{v _ {j} \leq n _ {j} \right\}} v _ {j} d x \leq \int_ {\Omega} \int_ {\mathbb {R} ^ {1}} | y | d \nu_ {x} (y) d x, \tag {4.51}
$$

where ${ v } _ { j } = { u } _ { k _ { j } }$ . $\forall k \in \mathcal N$ , let $\textstyle E _ { k } = \bigcup _ { j \geq k } \{ v _ { j } > n _ { j } \}$ . We shall verify:

1. $| E _ { k } |  0$ as $k  \infty$   
2. For $\forall k \in \mathcal N$ , $\{ v _ { j } \chi _ { F _ { k } } \}$ is uniformly absolutely continuous in $j$ , where $F _ { k } =$ $\Omega \setminus E _ { k }$ .

By definition we have the estimates:

$$
\left| E _ {k} \right| \leq \sum_ {j \geq k} \left| \left\{v _ {j} > n _ {j} \right\} \right| \leq \sup  _ {j} \| v _ {j} \| _ {L ^ {1}} \sum_ {j \geq k} \frac {1}{n _ {j}} \leq \frac {1}{2 ^ {k - 1}} \sup  _ {j} \| v _ {j} \| _ {L ^ {1}}.
$$

Thus (1) is verified.

We claim that

$$
\lim  _ {R \rightarrow \infty} \sup  _ {j \rightarrow \infty} \sup  _ {F _ {k} \cap \{v _ {j} > R \}} v _ {j} d x = 0. \tag {4.52}
$$

If it is proved, then for ∀ > 0, let R0 > 0, j0 > 0 be such that F v >R vj < $\forall \epsilon > 0$ $R _ { 0 } > 0 , j _ { 0 } > 0$ $\int _ { F _ { k } \cap \{ v _ { j } > R _ { 0 } \} } v _ { j } <$ $\scriptstyle { \frac { \epsilon } { 2 } }$ as $j > j _ { 0 }$ . For any $\mathcal { L } ^ { N }$ measurable set $U$ with $\begin{array} { r } { | U | < \frac { \epsilon } { 2 R _ { 0 } } } \end{array}$ , we have

$$
\int_ {U \cap F _ {k}} v _ {j} d x \leq \int_ {F _ {k} \cap \left\{v _ {j} > R _ {0} \right\}} v _ {j} d x + \int_ {U \cap F _ {k} \cap \left\{v _ {j} \leq R _ {0} \right\}} v _ {j} d x \leq \frac {\epsilon}{2} + R _ {0} | U | <   \epsilon ,
$$

for $\forall j \geq j _ { 0 }$ , i.e., $v _ { j }$ is uniformly absolutely continuous on $F _ { k } \ \forall k$ .

Now we return to proving (4.52). Since for $\forall$ Borel sets $E \in B ( \Omega ) , \forall \phi \in$ $C _ { 0 } ( \mathbb { R } ^ { 1 } )$ ,

$$
\lim  _ {j \rightarrow \infty} \int_ {E \cap \{v _ {j} \leq n _ {j} \}} \phi (v _ {j} (x)) d x = \int_ {E} \int_ {\mathbb {R} ^ {1}} \phi (y) d \nu_ {x} (y) d x,
$$

Choosing $\phi _ { j } \uparrow | y |$ , it follows that

$$
\lim  _ {j \rightarrow \infty} \inf  _ {E \cap \left\{v _ {j} \leq n _ {j} \right\}} v _ {j} (x) d x \geq \int_ {E} \int_ {\mathbb {R} ^ {1}} | y | d \nu_ {x} (y) d x, \tag {4.53}
$$

because for $j \geq k , F _ { k } \subset \{ v _ { j } \leq n _ { j } \} , F _ { k } \cup ( \{ v _ { j } \leq n _ { j } \} \cap E _ { k } ) = \{ v _ { j } \leq n _ { j } \}$ . Combining (4.51) and (4.53) with $E = E _ { k }$ , we have

$$
\lim  _ {j \rightarrow \infty} \sup  _ {F _ {k}} v _ {j} (x) d x \leq \int_ {F _ {k}} \int_ {\mathbb {R} ^ {1}} | y | d \nu_ {x} (y) d x. \tag {4.54}
$$

Thus,

$$
\begin{array}{l} \limsup_{R\to \infty}\limsup_{j\to \infty}\int_{F_{k}\cap \{v_{j} > R\}}v_{j}(x)dx \\ = \limsup_{R\to \infty}\limsup_{j\to \infty}\left(\int_{F_{k}} - \int_{F_{k}\cap \{v_{j}\leq R\}}\right)v_{j}(x)dx \\ \leq \lim  _ {R \to \infty} \sup  _ {j \to \infty} \sup  _ {\int_ {F _ {k}} \int_ {\mathbb {R} ^ {1}} | y | d \nu_ {x} (y) d x - \int_ {F _ {k}} g _ {R} (v _ {j} (x)) d x \Bigg)} \\ = \lim  _ {R \rightarrow \infty} \sup  _ {F _ {k}} \int_ {F _ {k}} \int_ {\mathbb {R} ^ {1}} (| y | - g _ {R} (y)) d \nu_ {x} (y) d x = 0. \\ \end{array}
$$

Definition 4.5.20 Let $\{ u _ { j } \} \subset L ^ { 1 } ( \Omega )$ be bounded; we say that $u _ { j }$ converges to $u \in L ^ { 1 } ( \Omega )$ in the sense of the bitting lemma, if for all $\epsilon > 0$ there exists $a$ measurable subset $E \subset \Omega$ such that $| E | < \epsilon$ and $u _ { j } \to u$ in $L ^ { 1 } ( \Omega \backslash E )$ .

What is the relationship between $\mathbf { H } ^ { 1 } - w ^ { * }$ convergence and the convergence in the sense of the bitting lemma?

Lemma 4.5.21 Let $\{ u _ { j } \} ^ { * } \ \stackrel { } { \ } { \ } u$ in $\mathbf { H } ^ { 1 } ( \mathbb { R } ^ { N } )$ . For each $R ~ > ~ 0$ , let $v \in$ $L ^ { 1 } ( B _ { R } ( \theta ) )$ be such that there exists a subsequence $n _ { j }$ for which $u _ { n _ { j } }$ converges to v in the sense of the bitting lemma in $B _ { R } ( \theta )$ , then $u = v$ a.e. on $B _ { R } ( \theta )$ .

Proof. We want to show that for $\forall \phi \in C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { N } )$ , $\textstyle \int u \phi d x = \int v \phi d x$ . Assume $\mathrm { s u p p } \phi \subset B _ { R } ( \theta )$ for suitable $R > 0 , v _ { j } = u _ { n _ { j } } \forall j$ . By the assumption, for all $\epsilon >$ $0$ there exists $E$ such that $| E | < \epsilon$ and $v _ { j }  v$ on $L ^ { 1 } ( \Omega \backslash E )$ . For all $\lambda > 0$ define $w _ { \lambda } = ( 1 + \lambda \log M ( \chi _ { E } ) ) _ { + }$ , where $M ( f )$ is the maximal function of $f$ . Provided by the propositions (3) and (7) of BMO, $w _ { \lambda } \in \mathrm { B M O } , \mathrm { a n d } \| \log M ( \chi _ { E } ) \| \le c _ { N }$ , and then $\| w _ { \lambda } \| _ { \mathrm { B M O } } \le C \lambda$ , where $C$ is a constant independent of $\lambda$ and $\epsilon$ .

Since $\chi _ { E } \leq M ( \chi _ { E } ) \leq 1$ a.e., it follows $\chi _ { E } \leq w _ { \lambda } \leq 1$ a.e. According to the proposition (4) of the maximal function, we have

$$
\left| \left\{w _ {\lambda} > 0 \right\} \right| = \left| \left\{M \left(\chi_ {E}\right) > \exp \left(- \frac {1}{\lambda}\right) \right\} \right| \leq C \exp \left(\frac {1}{\lambda}\right) \| \chi_ {E} \| _ {L ^ {1}} \leq C \varepsilon \exp \left(\frac {1}{\lambda}\right),
$$

and then

$$
\begin{array}{l} \int_ {\mathbb {R} ^ {N}} u _ {n _ {j}} \phi d x = \int_ {\mathbb {R} ^ {N}} v \phi d x \\ + \int_ {\mathbb {R} ^ {N}} u _ {n _ {j}} \phi w _ {\lambda} d x + \int_ {\mathbb {R} ^ {N}} (u _ {n _ {j}} - v) \phi (1 - w _ {\lambda}) d x - \int_ {\mathbb {R} ^ {N}} v \phi w _ {\lambda} d x \\ = I _ {1} + I _ {2} + I _ {3} + I _ {4}, \\ \end{array}
$$

where

$$
\begin{array}{l} I _ {1} = \int_ {\mathbb {R} ^ {N}} v \phi d x, \\ I _ {2} \leq \| u _ {n j} \| _ {\mathbf {H} ^ {1}} \| w _ {\lambda} \phi \| _ {\mathrm {B M O}} \leq C \| w _ {\lambda} \phi \| _ {\mathrm {B M O}}, \\ I _ {3} = \int_ {\Omega \backslash E} (u _ {n _ {j}} - v) (1 - w _ {\lambda}) \phi d x \rightarrow 0, \text {a s} j \rightarrow \infty , \\ I _ {4} \leq \int_ {\left\{w _ {\lambda} > 0 \right\}} | v | \| \phi | d x \leq \| \phi \| _ {L ^ {\infty}} \int_ {B _ {R} (\theta) \cap \left\{w _ {\lambda} > 0 \right\}} | v |. \\ \end{array}
$$

The limit of $I _ { 3 }$ is due to the assumption that $u _ { j }$ converges weakly to $v \sin L ^ { 1 } ( \Omega \backslash E )$ . We turn to estimate $I _ { 2 } . \forall r > 0 , \forall x \in \mathbb { R } ^ { N }$ , let $Q \ = \ B _ { r } ( x )$ , we have

$$
\begin{array}{l} f _ {Q} | \phi w _ {\lambda} - f _ {Q} \phi w _ {\lambda} | d z \leq \| \phi \| _ {L ^ {\infty}} \| w _ {\lambda} \| _ {\mathrm {B M O}} + \frac {1}{| Q |} f _ {Q} d z \int_ {\{w _ {\lambda} > 0 \}} | \phi (z) - \phi (y) | d y \\ \leq \left\{ \begin{array}{l} C (\lambda + R | \{w _ {\lambda} > 0 \} |) \text {i f} r <   R  , \\ C (\lambda + \frac {1}{R ^ {N}} | \{w _ {\lambda} > 0 \} |) \text {i f} r \geq R  . \end{array} \right. \\ \end{array}
$$

$\forall \eta > 0$ fixing $\lambda > 0$ such that $C \lambda < \frac { \eta } { 2 }$ at first, and then we choose $\epsilon > 0$ so small such that the summation of $I _ { 4 }$ 2and $\begin{array} { r } { C ( R + R ^ { - N } ) | \{ w _ { \lambda } > 0 \} | < \frac { \eta } { 2 } } \end{array}$ , we obtain the desired conclusion. □

Combining Lemmas 4.5.19 and 4.5.21, we have:

Theorem 4.5.22 Assume that $u _ { j } \ ^ { * }  \ u$ in $\mathbf { H } ^ { 1 } ( \mathbb { R } ^ { N } )$ . Then for all $R > 0$ there exists a subsequence $\{ u _ { n _ { j } } \}$ converging to u on $B _ { R } ( \theta )$ in the sense of the bitting lemma.

As an application to the calculus of variations, we study the polyconvex functionals in Sect. 4.3.3. Recall that a function $f : \Omega \times M ^ { n \times n } \longrightarrow \mathbb { R } ^ { 1 }$ is called polyconvex, if $f ( x , A ) = g ( x , T ( A ) )$ , where $\boldsymbol { g } : \Omega \times \mathbb { R } ^ { \tau _ { n } } \longrightarrow \mathbb { R } ^ { 1 }$ is a Caratheodory

function, for a.e. $x \in \Omega , \xi \mapsto g ( x , \xi )$ is convex, $T = \{ \mathrm { a d j } _ { s } \} _ { 1 } ^ { n } : M ^ { n \times n }  \mathbb { R } ^ { \tau _ { n } }$ , and

$$
\tau_ {n} = \sum_ {1} ^ {n} \left(\frac {n !}{s ! (n - s) !}\right) ^ {2}.
$$

Theorem 4.5.23 Let $\Omega$ be an open bounded domain in $\mathbb { R } ^ { n }$ ; $f ( x , A ) \ =$ $g ( x , T ( A ) )$ is polyconvex, satisfying

$$
C | A | ^ {n} \leq g (x, T (A)) \forall A \in M ^ {n \times n}, C > 0.
$$

Then the functional

$$
J (u) = \int_ {\Omega} f (x, \nabla u) d x
$$

has a minimizer in $W _ { 0 } ^ { 1 , n } ( \Omega , \mathbb { R } ^ { n } )$ .

Proof. Let $c = \operatorname* { i n f } _ { u \in W _ { 0 } ^ { 1 , n } } J ( u )$ , and $\{ u _ { j } \}$ be a minimizing sequence, i.e., $J ( u _ { j } )  \ c$ 0. By the coercive condition, $\| \nabla u _ { j } \| _ { L ^ { n } ( \Omega , M ^ { n \times n } ) }$ is bounded, and then $\| u _ { j } \| _ { W ^ { 1 , n } ( \Omega , \mathbb { R } ^ { n } ) }$ is bounded. Modulo a subsequence, we have $u _ { j } \quad \to$ $u$ in $W ^ { 1 , n } ( \Omega , \mathbb { R } ^ { n } )$ , $\operatorname { a d j } _ { s } ( \nabla u _ { j } )  \operatorname { a d j } _ { s } ( \nabla u )$ , in $L ^ { \frac { n } { s } } ( \Omega , \mathbb { R } ^ { ( \frac { n ! } { s ! ( n - s ) ! } ) ^ { 2 } } ) , 1 \le s < n$ . Since we can extend to be zero outside $\Omega$ , provided by Theorem 4.5.18, we $u _ { j }$ have $\operatorname* { d e t } ( \nabla u _ { j } ) \ ^ { * }  \operatorname* { d e t } ( \nabla u )$ in $\mathbf { H } ^ { \mathrm { 1 } } ( \mathbb { R } ^ { n } )$ .

For $\forall \epsilon > 0$ , we find $\delta > 0$ such that if $| E | < \delta$ then $\begin{array} { r } { \int _ { E } g ( x , T ( u ( x ) ) ) d x < \epsilon } \end{array}$ . According to Theorem 4.5.22, there exists a measurable set $E \subset \Omega$ such that $| E | < \delta$ and $\operatorname* { d e t } ( \nabla ( u _ { j } ) )  \operatorname* { d e t } ( \nabla u )$ in $L ^ { 1 } ( \Omega \backslash E )$ . Obviously, we also have $\mathrm { a d j } _ { s } ( \nabla u _ { j } )  \mathrm { a d j } _ { s } ( \nabla u ) , 1 \le s < n$ in $L ^ { 1 } ( \Omega \backslash E , \mathbb { R } ^ { ( \frac { n ! } { s ! ( n - s ) ! } ) ^ { 2 } } )$ . Since a.e., $x \in \Omega , \xi \mapsto g ( x , \xi )$ is convex, we have

$$
\begin{array}{l} \liminf_{j\to \infty}\int_{\Omega}g(x,T(\nabla u_{j}(x)))dx\geq \liminf_{j\to \infty}\int_{\Omega \setminus E}g(x,T(\nabla u_{j}(x)))dx \\ \geq \int_ {\Omega \backslash E} g (x, T (\nabla u (x))) d x \\ \geq \int_ {\Omega} g (x, T (\nabla u (x))) d x - \epsilon . \\ \end{array}
$$

Since $\epsilon > 0$ is arbitrary, we obtain

$$
\liminf_ {j \rightarrow \infty} \int_ {\Omega} g (x, T (\nabla u _ {j} (x))) d x \geq \int_ {\Omega} g (x, T (\nabla u (x))) d x.
$$

Thus $u \in W _ { 0 } ^ { 1 , n } ( \Omega , \mathbb { R } ^ { n } )$ is a minimizer.

Comparing this result with Theorem 4.3.12, the growth condition for polyconvex functions is dropped. It is useful in the elasticity theory.

Another advantage in using Hardy space is on regularity. As we mentioned above, the Calderon–Zygmund theory can be applied to the Hardy space. Once some nonlinear expression of the minimizer is proved in $\mathbf { H } ^ { 1 }$ , the Euler

Lagrange equation is applied to obtain an a priori estimate, and gain better regularity of the solution, cf. Evans [Ev 2], Helein [Hel 1].

The compensation compactness method has been successfully applied to elasticity and hyperbolic systems by Ball [Bal 1], Di Perna [Di], Tartar [Tar 1] etc.

# 4.6 Free Discontinuous Problems

# 4.6.1 Γ-convergence

$\Gamma$ -convergence has been studied extensively by De Giorgi’s school. The idea is to use a family of functionals $J _ { \epsilon }$ , depending on a parameter $\epsilon$ and approaching the given functional $J$ in a variational problem, such that the minimizers $v _ { \epsilon }$ of $J _ { \epsilon }$ have a limit point $v$ which is a minimizer of $J$ .

Definition 4.6.1 Let $X$ be a metric space and let $F _ { n } : X \to [ 0 , \infty ]$ be a sequence of functionals. We say that $F _ { n }$ $\Gamma$ -converges to $F$ on $X$ as $n \longrightarrow \infty$ , written as $\begin{array} { r } { \Gamma - \operatorname* { l i m } _ { n \to \infty } F _ { n } = F } \end{array}$ , if

(1) $\forall u \in X , \forall \{ u _ { n } \}$ such that $u _ { n } \to u$ in $X$ one has $\operatorname* { i m } \operatorname* { i n f } _ { n \to \infty } F _ { n } ( u _ { n } ) \geq F ( u )$ ,   
(2) $\forall u \in X , \exists \{ u _ { n } \}$ such that $u _ { n } \to u$ in $X$ and $\begin{array} { r } { \operatorname* { l i m } \operatorname* { s u p } _ { n \to \infty } F _ { n } ( u _ { n } ) \leq F ( u ) } \end{array}$

The importance of the notion lies in the following theorems.

Theorem 4.6.2 If $F = \Gamma - \mathrm { l i m } _ { n  \infty } F _ { n }$ , then $F$ is l.s.c.

Proof. We prove by contradiction. If $F$ is not l.s.c., then $\exists x _ { n }  x$ , such that $F ( x ) > \operatorname* { l i m } _ { n \to \infty } F ( x _ { n } )$ . However, by definition, $\forall n , \exists \{ x _ { n m } \} \subset X , x _ { n m } \to x _ { n }$ , satisfying $F _ { n } ( x _ { n m } )  F ( x _ { n } )$ . We may assume that $\begin{array} { r } { \operatorname* { l i m } _ { n  \infty } F ( x _ { n } ) > - \infty } \end{array}$ and $F ( x ) < \infty$ . Let $\begin{array} { r } { \delta = \frac { 1 } { 4 } ( F ( x ) - \operatorname* { l i m } _ { n  \infty } F ( x _ { n } ) ) > 0 } \end{array}$ , then, $\forall n \in \mathcal { N } , \exists m _ { n } \in \mathcal { N }$ such that

$$
\left| F _ {n} \left(x _ {n m _ {n}}\right) - F \left(x _ {n}\right)\right| <   \delta , m _ {n} \rightarrow \infty , \text {a n d} x _ {n m _ {n}} \rightarrow x. \tag {4.55}
$$

On one hand, by $\Gamma$ -convergence,

$$
F (x) \leq \lim  _ {n \rightarrow \infty} \inf  _ {n \rightarrow \infty} F _ {n} \left(x _ {n m _ {n}}\right).
$$

On the other hand, by the definition of $\delta$ , $F _ { n } ( x _ { n m _ { n } } ) > F ( x ) - \delta$ for $n$ large, we have

$$
F \left(x _ {n}\right) <   F (x) - 3 \delta . \tag {4.56}
$$

It follows that $F _ { n } ( x _ { n m _ { n } } ) - F ( x _ { n } ) > 2 \delta$ , which contradicts (4.55).

Theorem 4.6.3 Suppose that $\begin{array} { r } { \Gamma - \operatorname* { l i m } _ { n \to \infty } F _ { n } = F } \end{array}$ and that $v _ { n }$ minimizes $F _ { n }$ over $X \ \forall n \in { \mathcal { N } }$ . If $v$ is a limiting point of $v _ { n }$ , then $v$ is a minimizer of $F$ and $F ( v ) = \operatorname* { l i m } \operatorname* { i n f } _ { n \to \infty } F _ { n } ( v _ { n } )$ .

Proof. By (2) of the definition of $\Gamma$ -convergence, for every $u \in X$ , there exists $\{ u _ { n } \} \subset X$ such that $u _ { n } \longrightarrow u$ , and

$$
\lim  _ {n \to \infty} F _ {n} (u _ {n}) = F (u) .
$$

Since $v _ { n }$ is a minimizer of $F _ { n }$ , we have $F _ { n } ( v _ { n } ) \leq F _ { n } ( u _ { n } )$ . Therefore (1) of the definition of $\Gamma$ -convergence yields

$$
F (u) \geq \lim  _ {n \rightarrow \infty} \sup  _ {n \rightarrow \infty} F _ {n} \left(u _ {n}\right) \geq \lim  _ {n \rightarrow \infty} \inf  _ {n \rightarrow \infty} F _ {n} \left(v _ {n}\right) \geq F (v), \tag {4.57}
$$

as $v _ { n } \longrightarrow v$ in $X$ . This proves the theorem.

We note that $\Gamma$ -convergence is different from pointwise convergence.

Example 4.6.4

$$
F _ {n} (x) = \left\{ \begin{array}{l} 1 \text {i f} x \geq \frac {1}{n}, \\ n x \text {i f} x \in \left[ - \frac {1}{n}, \frac {1}{n} \right], \\ - 1 \text {i f} x \leq - \frac {1}{n} \end{array} \right.
$$

then

$$
\Gamma - \lim  _ {n \to \infty} F _ {n} (x) = \left\{ \begin{array}{l l} 1, & \text {i f} x > 0  , \\ - 1 & \text {i f} x \leq 0  . \end{array} \right.
$$

But pointwisely,

$$
F _ {n} (x) \rightarrow \operatorname {s g n} (x).
$$

The difference is at $x = 0$ .

Example 4.6.5 $\begin{array} { r } { 1 ^ { \cdot } - \operatorname* { l i m } _ { n \to \infty } \sin n x = - 1 } \end{array}$ .

# 4.6.2 A Phase Transition Problem

We present here an example due to Cahn and Hilliard on phase transitions showing how $\Gamma$ -convergence is applied to variational problems.

In a container $\Omega \subset R ^ { 3 }$ filled with two immiscible and incompressible fluids, the two fluids arrange themselves in order to minimize the area of the interface which separates these two phases. Let $u$ be a function, which takes the value 0 on the set occupied by the first fluid, and the value 1 on the set occupied by the second. Let $V = \textstyle \int u$ be the total volume of the second fluid. Obviously one has $0 \ < \ V \ < \ m ( \Omega )$ . Let $S ( u )$ be the interface, i.e., the singular set of $u$ . The equilibrium configuration is obtained by minimizing the energy $\boldsymbol { F } ( \boldsymbol { u } ) : = \sigma \mathsf { H } ^ { 2 } ( \boldsymbol { S } ( \boldsymbol { u } ) )$ , where $\sigma$ is the surface tension between the two fluids, and $\mathsf { H } ^ { 2 }$ is the two dimensional Hausdorff measure.

On the macroscopic level, we allow for a mixture of two fluids; let $u : \Omega \to$ $[ 0 , 1 ]$ be the average density of the second fluid, and let $W$ be a double-well potential, i.e., a continuous positive function that vanishes only at 0 and 1.

Cahn and Hilliard established a model: $\forall \epsilon > 0$ , we consider a minimizer $u _ { \epsilon }$ of the following energy functional:

$$
E _ {\epsilon} (u) = \epsilon \int_ {\Omega} | \nabla u | ^ {2} + \frac {1}{\epsilon} \int_ {\Omega} W (u), \text {w i t h} V = \int_ {\Omega} u. \tag {4.58}
$$

Under the volume constraint $u _ { \epsilon }$ cannot be a constant; but as $\epsilon  0$ , for a.e. $x \in \Omega , u _ { \epsilon } ( x )$ must tend to either 0 or 1. Does $u _ { \epsilon }$ tend to a minimizer of $F$ ? The following Modica–Mortola theorem gives an answer to this question. Before going to the proof, we need a few lemmas.

Lemma 4.6.6 Suppose that $u _ { m } \to u$ in a metric space $( X , d )$ satisfying lim $\begin{array} { r } { \operatorname* { s u p } _ { m  \infty } F ( u _ { m } ) \leq F ( u ) } \end{array}$ , and that $\forall m , \exists u _ { m , n }  u _ { m }$ as $n \longrightarrow \infty$ such that lim $\begin{array} { r } { \operatorname* { s u p } _ { n \to \infty } F _ { n } ( u _ { m , n } ) \leq F ( u _ { m } ) } \end{array}$ . Then $\exists m _ { n }  \infty$ , such that for $v _ { n } = u _ { m _ { n } n }$ we have $v _ { n } \longrightarrow u$ and l ${ \mathrm { m } } \operatorname* { s u p } _ { n \to \infty } F _ { n } ( v _ { n } ) \leq F ( u )$ .

Proof. We may assume $\begin{array} { r } { d ( u , u _ { m } ) < \frac { 1 } { m } } \end{array}$ . Define

$$
m _ {n} = \max \left\{m \mid , d (u _ {m n}, u) <   \frac {1}{m}, F _ {m} (u _ {m n}) \leq F (u _ {m}) + \frac {1}{m} \right\}.
$$

It is easy to verify that $m \to \infty$ . We set $v _ { n } = u _ { m _ { n } n }$ , then $v _ { n }$ satisfies the requirements. □

The following lemma is just the one-dimensional version of the general theorem.

# Lemma 4.6.7

$$
\begin{array}{l} \min  \left\{\int_ {\mathbb {R} ^ {1}} [ | \gamma^ {\prime} (t) | ^ {2} + W (\gamma (t)) ] d t \mid \gamma \in C ^ {1} (R ^ {1}), \gamma (- \infty) = 0, \gamma (\infty) = 1 \right\} \\ = 2 \int_ {0} ^ {1} \sqrt {W (t)} d t. \\ \end{array}
$$

Proof. We write the functional to be minimized as $J ( \gamma )$ , and the right-hand integral as $\sigma$ . According to the Cauchy–Schwarz inequality,

$$
J (\gamma) \geq 2 \int_ {\mathbb {R} ^ {1}} \sqrt {W (\gamma (t))} \gamma^ {\prime} (t) d t = 2 \int_ {0} ^ {1} \sqrt {W (s)} d s = \sigma .
$$

The equality holds if and only if $\gamma ^ { \prime } = \sqrt { W ( \gamma ) }$ . Since $\sqrt { W }$ is continuous and positive between the two zeroes 0 and 1, the first-order equation has a global solution $\gamma$ on $\mathbb { R } ^ { 1 }$ , with $\gamma ( - \infty ) = 0 , \gamma ( \infty ) = 1$ . □

Let $U \subset \mathbb { R } ^ { n }$ be an open set with nonempty boundary $\partial U$ . One defines the signed distance function as follows:

$$
d (x) = \left\{ \begin{array}{l} \operatorname {d i s t} (x, \partial U), \text {i f} x \in U, \\ - \operatorname {d i s t} (x, \partial U), \text {i f} x \notin U. \end{array} \right.
$$

Thus, $d$ is Lipschitzian continuous with Lipschitz constant 1.

Lemma 4.6.8 Assume that $\partial U \in C ^ { 2 }$ is bounded. $\forall t \in \mathbb { R } ^ { 1 }$ we set $\Sigma _ { t } = \{ x \in$ $\mathbb { R } ^ { n } \left| d ( x ) = t \right\}$ . Then $\exists \epsilon > 0$ such that for $| s | < \epsilon$ , $\Sigma _ { s }$ is a hypersurface of $C ^ { 2 }$ class and $\begin{array} { r } { \operatorname* { l i m } _ { s  0 } H ^ { n - 1 } ( \Sigma _ { s } ) = H ^ { n - 1 } ( \partial U ) } \end{array}$ . Moreover,

$$
| \nabla d (x) | = 1.
$$

Proof. $\forall x _ { 0 } \in \partial U$ , let $\mathsf { n } ( x _ { 0 } )$ be the unit outer normal vector of $\partial U$ at $x _ { 0 }$ , and let $T _ { x _ { 0 } } ( \partial U )$ be the tangent hyperplane at $x _ { 0 }$ . Rotate the coordinates such that the first $n - 1$ coordinate axes lie on $T _ { x _ { 0 } } ( \partial U )$ and the $x _ { n }$ coordinate axis points in the direction $- \mathsf { n } ( x _ { 0 } )$ . In a neighborhood $V ( x _ { 0 } )$ of $x _ { 0 }$ , $\partial U$ can be represented by $x _ { n } = f ( x ^ { \prime } )$ where $x ^ { \prime } = ( x _ { 1 } , \ldots , x _ { n - 1 } )$ with $f ( x _ { 0 } ^ { \prime } ) = 0 , \nabla f ( x _ { 0 } ^ { \prime } ) = \theta$ .

There exists a tubular neighborhood $\Xi$ of $\partial U$ , such that $\forall x \in \Xi$ , there is a unique representation $\xi = \pi ( x ) \in \partial U$ such that $\boldsymbol { x } = \pi ( \boldsymbol { x } ) - \mathsf { n } ( \boldsymbol { x } ) d ( \boldsymbol { x } )$ , where ${ \mathfrak { n } } ( x ) = { \mathfrak { n } } ( \pi ( x ) )$ , and that $\xi$ is the unique point satisfying $| x - \xi | = d ( x )$ is a neighborhood of $x$ . There exists $\epsilon > 0$ such that $\Sigma _ { s } \subset \Xi , \forall | s | < \epsilon$ . We claim that:

$$
| \nabla d (x) | = 1, \text {a n d} \nabla d (x) = - \mathfrak {n} (\xi).
$$

Thus $\forall x \in \Sigma _ { s } , s = d ( x )$ , we have a local representation: $x = F ( x ^ { \prime } , s ) : =$ $( x ^ { \prime } , f ( x ^ { \prime } ) ) - \mathsf { n } ( x ^ { \prime } , f ( x ^ { \prime } ) ) s$ . By the use of the implicit function theorem, one can verify that $F$ is invertible as $\epsilon > 0$ is small. This implies that $d ( x ) =$ $d ( \xi - s \mathsf { n } ( \xi ) ) = s$ . Therefore, $\nabla d ( x ) \mathsf { n } ( \xi ) = - 1$ . Since $d$ has Lipschitz constant 1, the claim is proved.

Using the Gauss–Green formula, we have

$$
\int_ {0 <   d (x) <   s} \operatorname {d i v} (\nabla d (x)) = \int_ {\Sigma_ {0}} \nabla d (x) \mathfrak {n} (x) d \mathsf {H} ^ {n - 1} + \int_ {\Sigma_ {s}} \nabla d (x) \mathfrak {n} (x) d \mathsf {H} ^ {n - 1}.
$$

As $s \to 0$ , the left-hand side tends to zero, and we obtain

$$
\mathsf {H} ^ {n - 1} \left(\Sigma_ {s}\right)\rightarrow \mathsf {H} ^ {n - 1} \left(\Sigma_ {0}\right) = \mathsf {H} ^ {n - 1} \left(\partial U\right).
$$

□

Theorem 4.6.9 (Modica–Mortola) Assume that $W \in C ( [ 0 , 1 ] )$ is nonnegative and equals zero at 0 and 1 only. Let $X = \{ u : \Omega \stackrel { } { \to } [ 0 , 1 ] |$ u is measurable and $\textstyle { \int _ { \Omega } u = V } \}$ endowed with the $L ^ { 1 }$ norm. $\forall \epsilon > 0$ , let

$$
F _ {\epsilon} (u) := \left\{ \begin{array}{l} E _ {\epsilon} (u) = \epsilon \int_ {\Omega} | \nabla u | ^ {2} + \frac {1}{\epsilon} \int_ {\Omega} W (u) \text {i f} u \in W ^ {1, 2} \cap X, \\ + \infty , \quad o t h e r w i s e, \end{array} \right. \tag {4.59}
$$

and

$$
F (u) = \left\{ \begin{array}{l} \sigma \| D \chi_ {E} \| (\Omega) \text {i f} u = \chi_ {E}, \\ + \infty , \quad o t h e r w i s e, \end{array} \right. \tag {4.60}
$$

where $\begin{array} { r } { \sigma = 2 \int _ { 0 } ^ { 1 } \sqrt { W ( t ) } d t } \end{array}$ , and $E$ is a set of finite perimetrics. Then Γ $\operatorname* { l i m } F _ { \epsilon } = F$ in $X$ , and every sequence of minimizers of $F _ { \epsilon }$ is subconvergent in $X$ to a minimizer of $F$ .

Proof. 1. We claim that $\forall u \in X$ and for every $u _ { \epsilon _ { n } }  u$ in $X$ with $\epsilon _ { n } \to 0$ , if $\exists M > 0$ such that

$$
\epsilon_ {n} \int | \nabla u _ {\epsilon_ {n}} | ^ {2} + \frac {1}{\epsilon_ {n}} \int W (u _ {\epsilon_ {n}}) \leq M,
$$

then $\begin{array} { r } { \int W ( u ) \leq \operatorname* { l i m } \operatorname* { i n f } _ { n  \infty } \int W ( u _ { \epsilon _ { n } } ) = 0 } \end{array}$ , provided by Fatou’s lemma. From the double well assumption on $W$ $u = \chi _ { E }$ for some Borel set $E$ .

2. We begin with the verification of (1) of the definition of $\Gamma$ -convergence. We may assume without loss of generality that $\mathrm { l i m i n f } _ { \epsilon \to 0 } F _ { \epsilon } ( u _ { \epsilon } )$ is bounded. According to step 1, $u = \chi _ { E }$ for some Borel set $E$ . From the Cauchy–Schwarz inequality and the l.s.c. of the total variations, we have

$$
\begin{array}{l} \liminf_{\epsilon \to 0}F_{\epsilon}(u_{\epsilon}) = \liminf_{\epsilon \to 0}\int_{\Omega}\left(\epsilon |\nabla u_{\epsilon}|^{2} + \frac{1}{\epsilon} W(u_{\epsilon})\right)dx \\ \geq \liminf_ {\epsilon \rightarrow 0} 2 \int_ {\Omega} \sqrt {W (u _ {\epsilon})} | \nabla u _ {\epsilon} | d x \\ = \liminf_ {\epsilon \rightarrow 0} \int_ {\Omega} | \nabla (\Psi (u _ {\epsilon})) | d x \\ = \lim  _ {\epsilon \rightarrow 0} \inf  _ {e \rightarrow 0} \| D \Psi (u _ {\epsilon}) \| (\Omega) \\ \geq \| D \Psi (u) \| (\Omega), \\ \end{array}
$$

where $\Psi$ is the primitive of $2 \sqrt { W }$ satisfying $\Psi ( 0 ) = 0$ . One has $\Psi ( u ) =$ $\Psi ( \chi _ { E } ) = \Psi ( 1 ) \chi _ { E } = \sigma \chi _ { E }$ . Thus, $\| D \Psi ( u ) \| = F ( u )$ , and then (1) is verified.

3. We now verify (2) of the definition of $\Gamma$ -convergence. By the definition of $F$ , it is sufficient to verify (2) for $u = \chi _ { E }$ . Suitably modifying the approximation theorem of BV functions (Theorem 4.5.7), we find approximate functions of the form:

$$
u _ {j} = \sum_ {i} \chi_ {\Omega_ {i}} ^ {j}, \text {w i t h} \| u _ {j} - u \| _ {L ^ {1}} \rightarrow 0,
$$

where $\Omega _ { i } ^ { j } , i = 1 , 2 , . . .$ , $j = 1 , 2 , \ldots$ , are disjoint bounded open sets with smooth boundary $\partial \Omega _ { i } ^ { \ j }$ . On account of Lemma 4.6.6, the verification is reduced to verifying (2) for characteristic functions of open sets with bounded smooth boundaries.

Let $u = \chi \Omega$ , let $d$ be the signed distance function of $\partial \Omega$ , and let $\gamma$ be a minimizer obtained in Lemma 4.6.7.  > 0, we define $\begin{array} { r } { u _ { \epsilon } ( x ) = \gamma ( \frac { d ( x ) } { \epsilon } ) } \end{array}$ . From Lemma 4.6.8 and the co-area formula, we have

$$
\begin{array}{l} F _ {\epsilon} (u _ {\epsilon}) = \frac {1}{\epsilon} \int_ {\Omega} \left[ \left| \gamma^ {\prime} \left(\frac {d}{\epsilon}\right) \right| ^ {2} + W \left(\gamma \left(\frac {d}{\epsilon}\right)\right) \right] d x \\ = \frac {1}{\epsilon} \int_ {\mathbb {R} ^ {1}} g (t) \left[ \gamma^ {\prime} (t / \epsilon) ^ {2} + W (\gamma (t / \epsilon)) \right] d t \\ = \int_ {\mathbb {R} ^ {1}} g (\epsilon t) \left[ \gamma^ {\prime} (t) ^ {2} + W (\gamma (t)) \right] d t. \\ \end{array}
$$

where $g ( t ) = \mathsf { H } ^ { n - 1 } ( \Sigma _ { t } )$ . Lemma 4.6.8 yields $g ( \epsilon t )  \mathbf { H } ^ { n - 1 } ( \partial \Omega )$ as $\epsilon  0$ . Thus

$$
\lim \sup F _ {\epsilon} (u _ {\epsilon}) \leq \| D \chi_ {E} \| (\Omega) \int_ {R ^ {1}} [ | \gamma^ {\prime} (t) | ^ {2} + W (\gamma (t)) ] d t = F (u).
$$

4. Finally we verify that if $u _ { \epsilon }$ is a minimizer of $F _ { \epsilon }$ , then $\{ u _ { \epsilon } \}$ is precompact.

In fact, one may assume that $F _ { \epsilon } ( u _ { \epsilon } )$ is bounded, and then by step 1, so is $\| D \Psi ( u _ { \epsilon } ) \| ( \Omega )$ . Thus $| \Psi ( u _ { \epsilon } ) | | _ { B V } = | | \Psi ( u _ { \epsilon } ) | | _ { L ^ { 1 } } + | | D \Psi ( u _ { \epsilon } ) | | ( \Omega )$ is bounded. According to the compactness property of BV functions, it is precompact in $L ^ { 1 } ( \Omega )$ . Since $\Psi$ admits a continuous inverse, $u _ { \epsilon }$ is precompact in $L ^ { 1 }$ . □

# 4.6.3 Segmentation and Mumford–Shah Problem

In image segmentation, we want to detect the edge of an image from a picture.

Given a bounded domain $\Omega \subset \mathbb { R } ^ { 2 }$ and an image represented by a function $g \in L ^ { 2 } ( \Omega )$ , find a closed set $K \subset \Omega$ with finite one-dimensional Hausdorff measure ${ \mathsf { H } } ^ { \mathrm { 1 } } ( K )$ , and a function $u \in H ^ { 1 } ( \Omega \backslash K )$ , which minimize the following cost functional:

$$
E (K, u) = \int_ {\Omega \setminus K} | \nabla u | ^ {2} d x + \mu \int_ {\Omega \setminus K} | u - g | ^ {2} d x + \lambda \mathsf {H} ^ {1} (K),
$$

where $\lambda , ~ \mu > 0$ are parameters. This is the Mumford–Shah problem.

The original image $g$ is thereby estimated by $u$ which is smooth in $\Omega$ except for the discontinuous set $K$ (in fact, in $H ^ { 1 } ( \Omega \backslash K ) )$ , and the latter is the estimated edge.

This is a variational problem with unknown $( K , u )$ . Obviously, the difficulty lies in $K$ , which is an object we have never met.

Tentatively, suppose $K$ is given. Since $E$ is quadratic in $u$ , so is convex and coercive, we obtain a minimizer $u ( K )$ with $m _ { \Omega \backslash K } : = E ( K , u ( K ) )$ . The problem is then reduced to minimizing the functional:

$$
J (K) = m _ {\Omega \backslash K} + \lambda \mathrm {H} ^ {1} (K). \tag {4.61}
$$

Since the discontinuous set $K$ is only assumed to be Hausdorff measurable, the problem is more complicated and requires further and deeper prerequisites. For pedagogical purposes, we follow Nordstr¨om [No] and restrict ourselves to assuming that only the union of finitely many $C ^ { 1 }$ -curves is the admissible $\gamma$ candidate for $K$ .

Given $\xi \in C ^ { 1 } ( [ 0 , 1 ] , \mathbb { R } ^ { 2 } )$ the image of $\xi$ is a $C ^ { 1 }$ -curve (may be self intersection). Thus the union of $N - C ^ { 1 }$ -curves $\gamma$ is defined by ${ \boldsymbol \xi } = ( \xi _ { 1 } , \dots , \xi _ { N } ) \in$ $C ^ { 1 } ( [ 0 , 1 ] , \mathbb { R } ^ { 2 } ) ^ { N }$ , with

$$
\gamma = \xi [ 0, 1 ] := \bigcup_ {n = 1} ^ {N} \xi_ {n} [ 0, 1 ].
$$

Let us recall the notion of a domain with minimally smooth boundary (cf. Stein [Ste 1]).

An open set $\mathcal { O } \subset \mathbb { R } ^ { n }$ is said to have a minimally smooth boundary if $\exists \varepsilon > 0$ , $\Finv$ an integer $N$ , a positive $M \ > \ 0$ , and a sequence of open sets $U _ { 1 } , U _ { 2 } , \dots$ , such that

(i) If $x \in \partial \mathcal { O }$ , then $B _ { \varepsilon } ( x ) \subset U _ { i }$ for some $_ i$ ,

(ii) No point of $\mathbb { R } ^ { n }$ is contained in more than $N$ of $U _ { i }$ ’s,

(iii) $\forall i$ , $\Finv$ a special Lipschitz domain $D _ { i }$ whose bound does not exceed $M$ so that

$$
U _ {i} \cap \mathcal {O} = U _ {i} \cap D _ {i}.
$$

For example, if $\partial \mathcal { O } \in C ^ { 1 }$ , then $\partial \mathcal { O }$ is minimally smooth.

The introduction of this notion is due to the following extension theorem:

Theorem 4.6.10 Let $\mathcal { O } \subset \mathbb { R } ^ { n }$ be a domain with minimally smooth boundary. Then $\Finv$ a linear operator $T$ mapping functions on $\boldsymbol { \mathcal { O } }$ to functions on $\mathbb { R } ^ { n }$ such that

(1) $T u | _ { \mathcal { O } } = u$   
(2) $\lVert \ T u \ \rVert _ { W ^ { l , p } ( \mathbb { R } ^ { n } ) } \leqslant C _ { l , p } \ \lVert \ u \ \rVert _ { W ^ { l , p } ( \mathcal { O } ) }$ for some constant $C _ { l , p } , \forall l \in \mathcal { N } , p \in$ $[ 1 , \infty ]$ .

Definition 4.6.11 An $N - C ^ { 1 }$ -curve $\gamma$ is said to be an admissible image segmentation of $\Omega$ if $\forall$ connected components $G$ of Ω γ, ε > 0, a domain with minimally smooth boundary $\mathcal { O } _ { \varepsilon }$ such that

(1) $\overline { { \mathcal { O } } } _ { \varepsilon } \subset G$   
(2) $m ( G - { \mathcal { O } } _ { \varepsilon } ) < \varepsilon$

We denote by $E _ { N }$ the set of all $N - C ^ { 1 }$ -curve admissible image segmentations of $\Omega$ endowed with the $C ^ { 1 } ( [ 0 , 1 ] , \mathbb { R } ^ { 2 } ) ^ { N }$ topology.

Thus for $\forall \gamma \in E _ { N }$

$$
\mathsf {H} ^ {1} (\gamma) = \int_ {0} ^ {1} | \dot {\xi} (t) | d t := \sum_ {n = 1} ^ {N} \int_ {0} ^ {1} | \dot {\xi} _ {n} (t) | d t, \quad \mathrm {w h e r e} \gamma = \xi [ 0, 1 ] = \bigcup_ {n = 1} ^ {N} \xi_ {n} [ 0, 1 ],
$$

i.e., the length of the piecewise $C ^ { 1 }$ -curve $\gamma$

Now, let us make some preparations. To simplify the notations, we assume $\lambda = \mu = 1$ .

For any bounded open domain $G$ , we define

$$
I _ {G} (u) = \int_ {G} | \nabla u | ^ {2} + \int_ {G} | u - g | ^ {2} \quad \text {o n} H ^ {1} (G).
$$

It is known that $I _ { G }$ has an unique minimizer $u _ { G }$ . Let

$$
m _ {G} = I _ {G} (u _ {G}) = \min  _ {u \in H ^ {1} (G)} I _ {G}.
$$

By simple calculations,

$$
m _ {G} = \int_ {G} | g | ^ {2} - g \cdot u _ {G}.
$$

Moreover, we have the following properties:

(1) $I _ { G } ^ { \prime } ( u _ { G } ) = 0$ , i.e.,

$$
\int_ {G} \nabla u _ {G} \nabla \varphi + \int_ {G} (u _ {G} - g) \varphi = 0 \quad \forall \varphi \in H ^ {1} (G).
$$

(2) $\begin{array} { r } { \int _ { G } | \nabla u _ { G } | ^ { 2 } + | u _ { G } | ^ { 2 } \leqslant \int _ { G } | g | ^ { 2 } . } \end{array}$

Lemma 4.6.12 Let $G$ be a connected component of $\Omega \backslash \gamma$ where $\gamma$ is an admissible image segmentation of $\Omega$ . Then for $\forall \varepsilon > 0$ ,  a domain with minimally smooth boundary $\boldsymbol { \mathcal { O } }$ such that

$$
\left| m _ {\mathcal {O}} - m _ {G} \right| <   \varepsilon .
$$

Proof. By the definition of the admissibility for $\gamma , \forall \epsilon > 0 , \exists$ a domain with minimally smooth boundary $\mathcal { O } \subset \overline { { \mathcal { O } } } \subset G$ such that $m ( G \backslash { \mathcal { O } } ) < \epsilon$ . Letting $u _ { \mathcal { O } }$ be the minimizer of $I _ { \mathcal { O } }$ , we have

$$
\begin{array}{l} m _ {G} - m _ {\mathcal {O}} = \int_ {G} (| g | ^ {2} - g u _ {G}) - \int_ {\mathcal {O}} (| g | ^ {2} - g u _ {\mathcal {O}}) \\ = \int_ {G \backslash \mathcal {O}} \left(| g | ^ {2} - g u _ {G}\right) + \int_ {\mathcal {O}} g \left(u _ {\mathcal {O}} - u _ {G}\right). \\ \end{array}
$$

Noticing that

$$
\begin{array}{l} \int_ {G \backslash \mathcal {O}} \nabla u _ {G} \nabla \widetilde {u} _ {\mathcal {O}} + \int_ {G \backslash \mathcal {O}} u _ {G} \widetilde {u} _ {\mathcal {O}} \\ = \int_ {G} g \tilde {u} _ {\mathcal {O}} - \int_ {\mathcal {O}} g u _ {G} \\ = \int_ {G \backslash \mathcal {O}} g \widetilde {u} _ {\mathcal {O}} + \int_ {\mathcal {O}} g (u _ {\mathcal {O}} - u _ {G}), \\ \end{array}
$$

where $\tilde { u } _ { \mathcal { O } } = T u _ { \mathcal { O } }$ , we have

$$
\int_ {\mathcal {O}} g (u _ {\mathcal {O}} - u _ {G}) = \int_ {G \backslash \mathcal {O}} \left(\nabla u _ {G} \nabla \widetilde {u} _ {\mathcal {O}} + u _ {G} \widetilde {u} _ {\mathcal {O}}\right) - \int_ {G \backslash \mathcal {O}} g \widetilde {u} _ {\mathcal {O}}.
$$

Applying the Cauchy–Schwarz inequality, the first integral ≤ $\| u _ { G } \| _ { H ^ { 1 } ( G \backslash \mathcal { O } ) } \| \widetilde { u } _ { \mathcal { O } } \| _ { H ^ { 1 } }$ . By (2) and Theorem 4.6.10 C0 > 0 such that

$$
\left\| \tilde {u} _ {\mathcal {O}} \right\| _ {H ^ {1}} \leq C _ {0} \left\| u _ {\mathcal {O}} \right\| _ {H ^ {1} (\mathcal {O})} \leq C _ {0} \| g \| _ {L ^ {2} (\mathcal {O})} \leq C _ {0} \| g \| _ {L ^ {2} (G)}.
$$

Therefore $\exists C > 0$ a constant such that

$$
\left| m _ {G} - m _ {\mathcal {O}} \right| \leqslant C \| g \| _ {L ^ {2} (G)} \left(\int_ {G \backslash \mathcal {O}} | g | ^ {2} + \| u _ {G} \| _ {H ^ {1} (G \backslash \mathcal {O})} ^ {2}\right) ^ {\frac {1}{2}}.
$$

Since $m ( G \backslash O )$ can be sufficiently small, the conclusion follows.

Lemma 4.6.13 The function $J$ restricted on $E _ { N }$ is l.s.c.

Proof. The second term $\mathcal { H } ^ { 1 } ( \gamma )$ of $J$ is trivially continuous in $C ^ { 1 }$ -topology, so is l.s.c.

It is sufficient to verify the l.s.c. of $m _ { \Omega \setminus \gamma }$ with respect to $\gamma$ , i.e., for $\forall \gamma _ { 0 } \in$ $E _ { N }$ , $\forall \varepsilon > 0$ , $\exists \delta > 0$ such that $\mathrm { d i s t } \left( \gamma , \gamma _ { 0 } \right) < \delta$ implies that $m _ { \Omega \backslash \gamma } > m _ { \Omega \backslash \gamma _ { 0 } } - \varepsilon$ .

Let $\varepsilon$ be the collection of all connected components of $\Omega \backslash \gamma _ { 0 }$ , then $\varepsilon$ is countable, say $\mathcal { E } = \{ G _ { i } \} _ { 1 } ^ { \infty }$ . From

$$
\sum_ {i = 1} ^ {\infty} \int_ {G _ {i}} | g | ^ {2} = \int_ {\Omega \backslash \gamma} | g | ^ {2} \leqslant \| g \| _ {L ^ {2} (\Omega)} ^ {2},
$$

there exists $K > 0$ such that

$$
\sum_ {i = K + 1} ^ {\infty} \int_ {G _ {i}} | g | ^ {2} <   \frac {\varepsilon}{2}. \tag {4.62}
$$

Now we focus our study on $\{ G _ { i } \} _ { 1 \leqslant i \leqslant K }$ . According to Lemma 4.6.12, $\exists { \mathcal { O } } _ { i }$ with minimally smooth boundary satisfying. ${ \overline { { \mathcal { O } } } } _ { i } ~ \subset ~ G _ { i }$ and $| m _ { \mathcal { O } _ { i } } - m _ { G _ { i } } | \leqslant$ $\begin{array} { r } { \frac { \varepsilon } { 2 K } \forall i = 1 , 2 , \dots , K } \end{array}$ .

Let $\delta > 0$ be such that $\mathrm { d i s t } \left( \overline { { \mathcal { O } } } _ { i } , \partial G _ { i } \right) \geqslant \delta > 0$ ; we choose a $\frac { \delta } { 2 }$ neighborhood of $\gamma _ { 0 }$ , then $\bigcup _ { i = 1 } ^ { K } { \mathcal { O } } _ { i } \subset \Omega \backslash \gamma$ as $\begin{array} { r } { \mathrm { d i s t } \left( \gamma , \gamma _ { 0 } \right) < \frac { \delta } { 2 } } \end{array}$ , and we have

$$
\begin{array}{l} m _ {\Omega \backslash \gamma} = I _ {\Omega \backslash \gamma} (u _ {\Omega \backslash \gamma}) \geqslant \sum_ {i = 1} ^ {K} I _ {\mathcal {O} _ {i}} (u _ {\Omega \backslash \gamma}) \\ \geqslant \sum_ {i = 1} ^ {K} m _ {\mathcal {O} _ {i}} \\ \geqslant \frac {- \varepsilon}{2} + \sum_ {i = 1} ^ {K} m _ {G _ {i}}. \\ \end{array}
$$

Let $u ( x ) = u _ { G _ { i } }$ as $x \in G _ { i } , i = 1 , 2 , \ldots , K$ and $u ( x ) = 0$ as $x \in \mathcal { E } \backslash \bigcup _ { i = 1 } ^ { K } G _ { i }$ , then $\begin{array} { r } { \Omega \backslash ( \gamma _ { 0 } \cup \bigcup _ { i = 1 } ^ { K } G _ { i } ) = \bigcup _ { i = K + 1 } ^ { \infty } G _ { i } } \end{array}$ , we obtain from inequality (4.62)

$$
m _ {\Omega \backslash \gamma_ {0}} \leq I _ {\Omega \backslash \gamma_ {0}} (u) = \sum_ {i = 1} ^ {K} I _ {G _ {i}} \left(u _ {G _ {i}}\right) + I _ {\Omega \backslash \left(\gamma_ {0} \cup \bigcup_ {i = 1} ^ {k} G _ {i}\right)} (0)
$$

$$
\leq \sum_ {i = 1} ^ {K} m _ {G _ {i}} + \frac {\varepsilon}{2}.
$$

This proves that

$$
m _ {\Omega \backslash \gamma} > m _ {\Omega \backslash \gamma_ {0}} - \varepsilon .
$$

![](images/391db2a5629558bb50a6b3e3306684d826e196d82758968658f61a9db4a9706f.jpg)

Now for $\forall N \in \mathcal { N } , \forall \alpha \in ( 0 , 1 ) , \forall \rho > 0 , \forall \omega > 0$ , let us define

$$
\begin{array}{l} \mathcal {C} _ {N} (\alpha , \rho , \omega) = \left\{\gamma = \xi [ 0, 1 ] \in E _ {N} | \| \xi \| _ {C ^ {1}} + \sup  _ {s, t \in [ 0, 1 ], s \neq t} \frac {| \xi (t) - \dot {\xi} (s) |}{| t - s | ^ {\alpha}} \right. \\ \left. \leq \rho , \int_ {0} ^ {1} \| \dot {\xi} (t) \| d t \geq \omega \right\} \\ \end{array}
$$

endowed with the $C ^ { 1 } [ 0 , 1 ] ^ { N }$ topology, and let

$$
\mathcal {C} = \{\varnothing \} \cup \bigcup_ {N = 1} ^ {\infty} \mathcal {C} _ {N} (\alpha , \rho , \omega) .
$$

We arrive at:

Theorem 4.6.14 $\exists \gamma _ { 0 } \in { \mathcal { C } } , \exists u _ { 0 } \in H ^ { 1 } ( \Omega \backslash \gamma _ { 0 } )$ , such that

$$
E (\gamma_ {0}, u _ {0}) = \operatorname * {M i n} _ {(\gamma , u) \in (\mathcal {C} \times H ^ {1} (\Omega \backslash \gamma))} E (\gamma , u).
$$

Proof. The lower bound $\omega > 0$ forces us to minimize $E$ on $C _ { N } ( \alpha , \rho , \omega )$ for some finite $N$ , and the latter is a closed compact set in $C ^ { 1 } [ 0 , 1 ] ^ { N }$ . Our conclusion follows from the general principle of calculus of variations. □

The simplified model of Mumford–Shah problem is restricted to the space of possible discontinuous sets consisting of those corresponding to line drawings.

The Mumford–Shah functional has been studied by De Giorgi and Ambrosio [DG 2], [Amb], [DA]. In the variational problem, the working space is an SBV space, a subspace of $B V ( \Omega )$ , which consists of those BV functions in which the Cantor part of their derivatives vanishes. The distributional derivative of a BV function $u$ can be decomposed into three parts: $D u = D ^ { a } u + D ^ { j } u + D ^ { c } u$ , where $D ^ { a } , D ^ { j }$ , and $D ^ { c }$ are the absolutely continuous part, jump part, and the Cantor part, resp. The advantage of the subspace SBV ( $\Omega$ ) is in the characterization of its $w ^ { * }$ -compactness. The reader is referred to Ambrosio [Amb].

Mumford and Shah conjectured that if $( K , u )$ is an optimal essential pair of $J$ , then $K$ is locally in $\Omega$ the union of finitely many $C ^ { 1 , 1 }$ embedded curves, see Morel [Mor].

# 4.7 Concentration Compactness

The loss of compactness breaks down the standard variational techniques. However, most problems arising in geometry (prescribing scalar curvature problem, Yamabe problem, minimal surfaces, fixed points and intersections in symplectic geometry etc.) and in physics ( $N$ -body problem, Yang–Mills equation, nonlinear Schr¨odinger equations, gravity theory etc.) are in this realm. Semilinear elliptic PDEs in $\mathbb { R } ^ { n }$ and those with the critical Sobolev exponent are two typical examples on this topic. They have been studied extensively in the last two decades.

If $\Omega \subset \mathbb { R } ^ { n }$ is a bounded domain, then a bounded sequence $\{ u _ { j } \}$ in $H ^ { 1 } ( \Omega )$ has a weakly convergent subsequence, and then a strongly convergent subsequence in $L ^ { q } ( \Omega )$ as $q \in [ 2 , 2 ^ { * } )$ , according to the compactness of the embedding $H ^ { 1 } ( \Omega ) \hookrightarrow L ^ { q } ( \Omega )$ . This argument is frequently used. (See Example 3 in Sect. 4.2.) However, when $\Omega$ is unbounded, the argument does not work. This can be seen by the following example: Let $\Omega = \mathbb { R } ^ { n }$ , $\{ x _ { j } \}$ be a sequence that tends to infinity. Given a nonzero $u _ { 0 } \in C _ { 0 } ^ { \infty }$ , let $u _ { j } ( x ) = u _ { 0 } ( x - x _ { j } )$ . Then $| | u _ { j } | | _ { H ^ { 1 } } = | | u _ { 0 } | | _ { H ^ { 1 } }$ and $| { u } _ { j } | _ { q } = | { u } _ { 0 } | _ { q }$ . Therefore, $u _ { j } ( x )  0$ a.e., and then $u _ { j } \to 0$ in $H ^ { 1 } ( R ^ { n } )$ , but $u _ { j }$ cannot $L ^ { q }$ converge to 0. The reason is that the associated measures

$$
\mu_ {j} (E) = \int_ {E} | u _ {j} | ^ {2} \forall \text {L e b e s g u e m e a s u r a b l e s e t} E \subset \mathbb {R} ^ {n} \tag {4.63}
$$

leak out at infinity.

# 4.7.1 Concentration Function

Relating to a measure $\mu$ , P. Levy introduced the concentration function

$$
Q (r) = \sup  _ {x \in \mathbb {R} ^ {n}} \mu \left(B _ {r} (x)\right), r \geq 0.
$$

We have the following:

Lemma 4.7.1 Suppose that $\{ u _ { j } \} \subset H ^ { 1 } ( \mathbb { R } ^ { n } )$ is a bounded sequence. If R > 0 such that $\quad \varinjlim _ { j \to + \infty } Q _ { j } ( R ) = 0$ , where $Q _ { j }$ is the concentration functions with respect to $\mu _ { j }$ → ∞defined in equation (4.63), then there exists a subsequence $\{ u _ { j _ { k } } \}$ such that $u _ { j _ { k } } \to 0$ in $L ^ { q } ( \mathbb { R } ^ { n } )$ , $\forall \ q \in ( 2 , 2 ^ { * } )$ .

Proof. Due to the interpolation inequality, we have:

$$
\left| \left| u \right| \right| _ {L ^ {q} \left(B _ {R} (x)\right)} \leq \left| \left| u \right| \right| _ {L ^ {2} \left(B _ {R} (x)\right)} ^ {1 - \theta} \left| \left| u \right| \right| _ {L ^ {2 ^ {*}} \left(B _ {R} (x)\right)} ^ {\theta}
$$

x Rn, where θ = (q−2)n . $\forall ~ x \in \mathbb { R } ^ { n }$ $\begin{array} { r } { \theta = { \frac { ( q - 2 ) n } { 2 q } } } \end{array}$

In the case where $\theta q \geq 2$ , we have $q \geq 2 + { \frac { 4 } { n } }$ and a constant $C > 0$ such that

$$
\begin{array}{l} \int_ {B _ {R} (x)} | u | ^ {q} \leq C \| u \| _ {L ^ {2} \left(B _ {R} (x)\right)} ^ {(1 - \theta) q} \| u \| _ {H ^ {1} \left(B _ {R} (x)\right)} ^ {\theta q - 2} \| u \| _ {H ^ {1} \left(B _ {R} (x)\right)} ^ {2} \\ \leq C \left(\sup  _ {x \in \mathbb {R} ^ {n}} | | u | | _ {L ^ {2} \left(B _ {R} (x)\right)} ^ {(1 - \theta) q}\right) | | u | | _ {H ^ {1} \left(\mathbb {R} ^ {n}\right)} ^ {\theta q - 2} \int_ {B _ {R} (x)} \left(| u | ^ {2} + | \nabla u | ^ {2}\right) \\ \end{array}
$$

from the Sobolev embedding theorem. We find a locally finite covering, i.e., $\exists \ \{ x _ { k } \}$ and $\exists \ \ell \in \mathbb { N }$ such that $\forall \ : x \in \mathbb { R } ^ { n }$ , $\Finv$ at most $\ell$ balls of $B _ { R } ( x _ { k } )$ to which $x$ belongs. Then

$$
\begin{array}{l} \int_ {\mathbb {R} ^ {n}} | u | ^ {q} \leq \sum_ {k} \int_ {B _ {R} \left(x _ {k}\right)} | u | ^ {q} \\ \leq \ell C \| u \| _ {H ^ {1} (\mathbb {R} ^ {n})} ^ {\theta q} \left(\sup  _ {x \in \mathbb {R} ^ {n}} \| u \| _ {L ^ {2} (B _ {R} (x))} ^ {(1 - \theta) q}\right). \\ \end{array}
$$

By the assumptions that $\| u _ { j } \| _ { H ^ { 1 } }$ is bounded and modulo a subsequence $\begin{array} { r } { \operatorname* { s u p } _ { x \in \mathbb { R } ^ { n } } \int _ { B _ { R } ( x ) } | u _ { j } | ^ { 2 } \to 0 } \end{array}$ , it follows that $\int _ { \mathbb { R } ^ { n } } | u _ { j } | ^ { q } \to 0$ .

In the case where $\theta q < 2$ , we have $2 < q < 2 + \frac { 4 } { n } : = q _ { 0 }$ . Since $\theta q _ { 0 } = 2$ , $\lvert \lvert u _ { j _ { k } } \rvert \rvert _ { L ^ { q _ { 0 } } ( \mathbb { R } ^ { n } ) } \to 0$ . Letting $\begin{array} { r } { \frac { 1 } { q } = \frac { \lambda } { 2 } + \frac { ( 1 - \lambda ) } { q _ { 0 } } } \end{array}$ n, we obtain, by interpolation, that

$$
\left|\left| u _ {j _ {k}} \right|\right| _ {L ^ {q} (\mathbb {R} ^ {n})} \leq \left| u _ {j _ {k}} \right| _ {2} ^ {\lambda} \left| u _ {j _ {k}} \right| _ {q _ {0}} ^ {1 - \lambda} \rightarrow 0.
$$

□

The lemma shows that the concentration function plays an important role in the compactness argument. It will be seen by the following:

Corollary 4.7.2 (Strauss) Let $H _ { r } ^ { 1 } ( R ^ { n } )$ be the subspace of $H ^ { 1 } ( R ^ { n } )$ consisting of radial symmetric functions. The embedding $H _ { r } ^ { 1 } ( R ^ { n } ) \hookrightarrow L ^ { p } ( R ^ { n } ) , 2 < p <$ $2 ^ { * } , n \geq 2$ is compact.

Proof. $\forall x \in R ^ { n } , \forall R > 0$ , let $m ( x , R )$ be the largest number of disjoint balls with radius $R$ and the centers lie on the same sphere with radius $| x |$ centered at $\theta$ . It is easily seen that $m ( x , R ) \to \infty$ as $\| x \|  \infty$ . By definition $\forall u \in$ $L ^ { 2 } ( R ^ { n } ) , \forall r > 0$ ,

$$
\int_ {B _ {r} (x)} | u | ^ {2} \leq m (x, r) ^ {- 1} \| u \| _ {L ^ {2}} ^ {2}.
$$

If $\{ u _ { j } \}$ is a bounded sequence in $H _ { r } ^ { 1 } ( R ^ { n } )$ , then $\forall \epsilon > 0$ , $\exists R > 0$ such that

$$
\sup  \left\{\int_ {B _ {r} (x)} | u _ {j} | ^ {2} | | x | \geq R \right\} <   \epsilon .
$$

We may assume $u _ { j } \ \to \ 0$ in $H ^ { 1 } ( R ^ { n } )$ ; then by the Rellich theorem, after a subsequence $\begin{array} { r } { \int _ { B _ { R + r } ( \theta ) } | u _ { n _ { j } } | ^ { 2 } \to 0 } \end{array}$ . It follows that

$$
\sup \left\{\int_ {B _ {r} (x)} | u _ {n _ {j}} | ^ {2} | | x | \leq R \right\}\rightarrow 0.
$$

According to Lemma 4.7.1, $u _ { n _ { j } } \to 0$ in $L ^ { p } ( R ^ { n } )$ .

If $\mu$ is a probability measure on $\mathbb { R } ^ { n }$ , then the concentration function $Q$ with respect to $\mu$ is nonnegative, nondecreasing and $\operatorname* { l i m } _ { r \to \infty } Q ( r ) = 1$ .

→∞Now we carefully study various cases of the behavior of a sequence of concentration functions.

Let $\{ Q _ { j } \}$ be a sequence of concentration functions associated with probability measures $\{ \mu _ { j } \}$ , then they consist of a bounded set in $\mathrm { B V } [ 0 , \infty )$ . By Helly’s lemma, after a subsequence, we again denote it by $\{ Q _ { j } \}$ , there exists a nonnegative nondecreasing function $Q \in \mathrm { B V } [ 0 , \infty )$ such that $Q _ { j } ( r ) \to Q ( r )$ a.e.

One may assume that $Q ( r - 0 ) = Q ( r )$ and that $Q ( r ) \leq \underline { { \operatorname* { l i m } } } Q _ { j } ( r )$ . Let us consider the limit: $\lambda = \operatorname* { l i m } _ { r \to \infty } Q ( r ) \in [ 0 , 1 ]$ .

If $\lambda = 0$ , then $\operatorname* { l i m } _ { j  \infty } Q _ { j } ( r ) = 0$ , ∀ r.

If $\lambda = 1$ , then   > 0,  R > 0, xj Rn such that $\mu _ { j } ( B _ { R } ( x _ { j } ) ) \geq 1 - \epsilon$ . In fact, by definition, $\exists \ R _ { 0 } \ > \ 0$ s.t. $Q ( R _ { 0 } ) > \frac { 1 } { 2 }$ . Let $x _ { j } \in \mathbb { R } ^ { n }$ satisfying $Q _ { j } ( R _ { 0 } ) \leq \mu _ { j } ( B _ { R _ { 0 } } ( x _ { j } ) ) + \frac { 1 } { j }$ . For $\epsilon \in ( 0 , \frac { 1 } { 2 } )$ , we choose $R _ { 1 } ~ > ~ 0$ such that $Q ( R _ { 1 } ) > 1 - \epsilon$ , and let $y _ { j }$ satisfy $Q _ { j } ( R _ { 1 } ) \leq \mu _ { j } ( B _ { R _ { 1 } } ( y _ { j } ) ) + \frac { 1 } { j } .$ . Then

$$
\mu_ {j} \left(B _ {R _ {1}} \left(y _ {j}\right)\right) + \mu_ {j} \left(B _ {R _ {0}} \left(x _ {j}\right)\right) \geq Q _ {j} \left(R _ {1}\right) + Q _ {j} \left(R _ {0}\right) - \frac {2}{j} > 1, \text {f o r} j \text {l a r g e}.
$$

Therefore $B _ { R _ { 1 } } ( y _ { j } ) \cap B _ { R _ { 0 } } ( x _ { j } ) \neq \emptyset$ , which implies $B _ { R _ { 1 } } ( y _ { j } ) \subset B _ { 2 R _ { 1 } + R _ { 0 } } ( x _ { j } )$ and $\mu _ { j } ( B _ { 2 R _ { 1 } + R _ { 0 } } ( x _ { j } ) ) \geq \mu _ { j } ( B _ { R _ { 1 } } ( y _ { j } ) ) \geq Q _ { j } ( R _ { 1 } ) - { \frac { 1 } { j } } > 1 - \epsilon - { \frac { 1 } { j } }$ 1 for $j$ large. This is the conclusion.

For $\lambda ~ \in ~ ( 0 , 1 )$ , $\forall \ : \ : \epsilon \ : > \ : 0$ , ∃ R0 > 0, ∃ {xj} such that $Q _ { j } ( R _ { 0 } ) \ \geq$ $\mu _ { j } ( B _ { R _ { 0 } } ( x _ { j } ) ) \geq \lambda - \epsilon$ for $j$ large. Also, we may find a sequence $R _ { j } \ \to \ \infty$ such that

$$
Q _ {j} \left(R _ {0}\right) \leq Q _ {j} \left(R _ {j}\right) <   \lambda + \epsilon \text {f o r} j \text {l a r g e}.
$$

Thus $\lambda - \epsilon < \mu _ { j } ( B _ { R _ { 0 } } ( x _ { j } ) ) \le \mu _ { j } ( B _ { R _ { j } } ( x _ { j } ) ) \le \lambda + \epsilon$ . Now for any given $R >$ $R _ { 0 }$ , we may assume $R _ { j } ~ > ~ R$ for all $j$ and let $\mu _ { j } ^ { 1 } \ : = \ : \mu _ { j } | _ { B _ { R _ { 0 } } ( x _ { j } ) }$ and $\mu _ { j } ^ { 2 } \ =$ $\mu _ { j } \big | _ { \mathbb { R } ^ { n } \setminus B _ { R _ { j } } ( x _ { j } ) }$ . Then $0 \leq \mu _ { j } ^ { 1 } + \mu _ { j } ^ { 2 } \leq \mu _ { j }$ , supp $\mu _ { j } ^ { 1 } \subset B _ { R _ { 0 } } ( x _ { j } )$ , supp $\mu _ { j } ^ { 2 } ~ \subset$ $\mathbb { R } ^ { n } \setminus B _ { R _ { j } } ( x _ { j } ) \subset \mathbb { R } ^ { n } \setminus B _ { R } ( x _ { j } )$ , and so

$$
\begin{array}{l} \left| \lambda - \mu_ {j} ^ {1} (\mathbb {R} ^ {n}) \right| + \left| (1 - \lambda) - \mu_ {j} ^ {2} (\mathbb {R} ^ {n}) \right| \\ \leq | \lambda - \mu_ {j} (B _ {R _ {0}} (x _ {j})) | + | \lambda - \mu_ {j} (B _ {R _ {j}} (x _ {j}) | <   2 \epsilon . \\ \end{array}
$$

In summary, we have:

Theorem 4.7.3 (Concentration compactness principle) Suppose that $\{ \mu _ { j } \} \subset$ $M ( \mathbb { R } ^ { n } )$ is a sequence of probability measures. Then one of the following three conclusions holds:

1. (Compactness) $\exists \ \{ x _ { j } \} \subset \mathbb { R } ^ { n }$ such that $\forall \ : \epsilon > 0$ , ∃ $R > 0$ with $\mu _ { j } ( B _ { R } ( x _ { j } ) ) \geq$ $1 - \epsilon$ for all $j$ .

2. (Vanishing) $\forall \ R > 0$ , $\begin{array} { r } { \operatorname* { l i m } _ { j  \infty } ( \operatorname* { s u p } _ { x \in \mathbb { R } ^ { n } } \mu _ { j } ( B _ { R } ( x ) ) ) = 0 } \end{array}$

3. (Dichotomy) $\exists ~ \lambda \in ( 0 , 1 )$ ∈such that $\forall \ \epsilon > \ 0$ , $R > 0$ and xj with the property: Given $R ^ { \prime } > R$ , there exist positive measures $\mu _ { j } ^ { 1 }$ and $\mu _ { j } ^ { 2 }$ such that $0 \leq \mu _ { j } ^ { 1 } + \mu _ { j } ^ { 2 } \leq \mu _ { j }$ , supp $\mu _ { j } ^ { 1 } \subset B _ { R } ( x _ { j } )$ , supp $\mu _ { j } ^ { 2 } \subset \mathbb { R } ^ { n } \setminus B _ { R ^ { \prime } } ( x _ { j } )$ , and lim $\begin{array} { r } { \operatorname* { s u p } _ { j \to \infty } \{ | \lambda - \mu _ { j } ^ { 1 } ( \mathbb { R } ^ { n } ) | + | ( 1 - \lambda ) - \mu _ { j } ^ { 2 } ( \mathbb { R } ^ { n } ) | \} \le \epsilon } \end{array}$ .

We shall present an example showing how this principle is applied.

In contrast with Example 3 in Sect. 4.2, we consider the same equation but on the whole space $\mathbb { R } ^ { n }$ : Find a nontrivial solution $u \in H ^ { 1 } ( \mathbb { R } ^ { n } )$ satisfying

$$
- \triangle u + u = | u | ^ {q - 2} u, 2 <   q <   2 ^ {*}.
$$

If we follow the steps in the proof of that example, then it can be stated as a minimizing problem for the functional:

$$
J (u) = \int_ {\mathbb {R} ^ {n}} \left(| \nabla u | ^ {2} + | u | ^ {2}\right)
$$

subject to the constraint:

$$
M = \left\{u \in H ^ {1} (\mathbb {R} ^ {n}) \mid \int_ {\mathbb {R} ^ {n}} | u | ^ {q} = 1 \right\}.
$$

It is important to note that both $J$ and $M$ are translation invariant.

From the translation invariance, we intend to prove that, for any minimizing sequence $\{ u _ { j } \}$ , after suitable translation, the new minimizing sequence $\{ v _ { j } \}$ is $L ^ { q }$ subconvergent. Then the limit $v _ { 0 } \in M$ .

Let

$$
S _ {q} = \inf  _ {M} J.
$$

In light of the concentration compactness principle, let $\mu _ { j } = | u _ { j } | ^ { q } \ d$ x. If one can exclude Cases 2 and 3, then $\exists \{ x _ { j } \} \subset R ^ { n }$ , $\forall \ : \epsilon > 0$ , R > 0, such that

$$
1 = \int_ {\mathbb {R} ^ {n}} | u _ {j} (x - x _ {j}) | ^ {q} d x \geq \mu_ {j} \left(B _ {R} (x _ {j})\right) \geq 1 - \epsilon .
$$

Let $v _ { j } ( x ) = u _ { j } ( x - x _ { j } )$ . Again $\{ v _ { j } \}$ is a minimizing sequence, $\left\| v _ { j } \right\| _ { H ^ { 1 } \left( R ^ { n } \right) }$ is bounded, after a subsequence $v _ { j } ~  ~ v _ { 0 }$ , and then $v _ { j } ~  ~ v _ { 0 }$ strongly in $L ^ { q } ( B _ { R } ( \theta ) )$ .

Then we have $1 \ge \int _ { B _ { R } ( 0 ) } | v _ { 0 } ( x ) | ^ { q } \ge 1 - \epsilon$ . Therefore, $\int _ { \mathbb { R } ^ { n } } | v _ { 0 } | ^ { q } = 1$ , i.e., $v _ { 0 } \in M$ . Moreover, we have $v _ { j }  v _ { 0 }$ in $L ^ { q } ( R ^ { n } )$ strongly.

To exclude the vanishing case: Suppose we had $R > 0$ such that

$$
\liminf_{j\to \infty}\sup_{x\in \mathbb{R}^{n}}\int_{B_{R}(x)}|u_{j}|^{q} = 0.
$$

Then $\operatorname* { i m i n f } _ { j \to \infty } \operatorname* { s u p } _ { x \in \mathbb { R } ^ { n } } \int _ { B _ { R } ( x ) } | u _ { j } | ^ { 2 } = 0$ . According to the Lemma 4.7.1, $u _ { j } \to 0$ j→∞ x Rn B (x) in $L ^ { p } ( \mathbb { R } ^ { n } ) \ \forall \ p \in \ ( 2 , 2 ^ { * } )$ . In particular, when we take $p = q$ , it contradicts $| u _ { j } | _ { q } = 1$ .

$\cdot$ To exclude the dichotomy case: Suppose $\exists \lambda \in ( 0 , 1 )$ such that $\forall \ \epsilon > 0$ , $\exists R > 0 , \exists \ : \{ x _ { j } \}$ , and positive measures $\mu _ { j } ^ { \mathrm { 1 } }$ and $\mu _ { j } ^ { 2 }$ such that $0 \leq \mu _ { j } ^ { 1 } + \mu _ { j } ^ { 2 } \leq \mu _ { j }$ , supp $\mu _ { j } ^ { 1 } \subset B _ { R } ( x _ { j } )$ , supp $\mu _ { j } ^ { 2 } \subset \mathbb { R } ^ { n } \backslash B _ { 2 R } ( x _ { j } )$ , and $\begin{array} { r } { \operatorname* { l i m } _ { j  \infty } \{ | \lambda - \mu _ { j } ^ { 1 } ( \mathbb { R } ^ { n } ) | + | ( 1 - } \end{array}$ $\lambda ) - \mu _ { j } ^ { 2 } ( \mathbb { R } ^ { n } ) | \} \leq \epsilon$ .

Choosing $\epsilon _ { j }  0 , \exists R _ { j } > 0$ , after a subsequence, we have supp $\mu _ { j } ^ { 1 } \subset$ $B _ { R _ { j } } ( x _ { j } )$ , supp $\mu _ { j } ^ { 2 } \subset \mathbb { R } ^ { n } \setminus B _ { 2 R _ { j } } ( x _ { j } )$ , and $\overline { { \mathrm { l i m } } } _ { j  \infty } \{ | \lambda - \mu _ { j } ^ { 1 } ( \mathbb { R } ^ { n } ) | + | ( 1 - \lambda ) - $ $\mu _ { j } ^ { 2 } ( \mathbb { R } ^ { n } ) | \} = 0$ .

In fact, we may assume $R _ { j }  \infty$ . Let $\phi \in C _ { 0 } ^ { \infty } ( B _ { 2 } ( \theta ) )$ such that $\phi = 1$ in $B _ { 1 } ( \theta )$ and let $\phi _ { j } ( x ) = \phi ( \frac { x - x _ { j } } { R _ { j } } )$ − xj ). We write vj = uj φj + uj (1 − φj ). Then $v _ { j } = u _ { j } \phi _ { j } + u _ { j } \big ( 1 - \phi _ { j } \big )$ Rj $J ( u _ { j } ) = J ( u _ { j } \phi _ { j } ) + J ( u _ { j } ( 1 - \phi _ { j } ) ) + B _ { j }$ , where $B _ { j }$ is the interaction term:

$$
B _ {j} = 2 \int \nabla (u _ {j} \phi_ {j}) \nabla (u _ {j} (1 - \phi_ {j})) + u _ {j} ^ {2} \phi_ {j} (1 - \phi_ {j}).
$$

By simple estimation $\forall \epsilon > 0$ ,

$$
B _ {j} \geq - \epsilon - \frac {1}{\epsilon} E _ {j},
$$

where

$$
E _ {j} = 2 \int_ {B _ {2 R _ {j}} (x _ {j}) \backslash B _ {R _ {j}} (x _ {j})} | u _ {j} \nabla \phi_ {j} | ^ {2} \leq \frac {C}{R _ {j} ^ {2}} \int_ {B _ {2 R _ {j}} (x _ {j}) \backslash B _ {R _ {j}} (x _ {j})} | u _ {j} | ^ {2}.
$$

Fixing $\epsilon > 0$ arbitrarily small, since $E _ { j } \to 0$ as $j \to \infty$ , we have

$$
\begin{array}{l} J (u _ {j}) \geq S _ {q} \left(| u _ {j} \phi_ {j} | _ {q} ^ {2} + | u _ {j} (1 - \phi_ {j}) | _ {q} ^ {2}\right) - \epsilon - \frac {1}{\epsilon} E _ {j} \\ \geq S _ {q} \big (\mu_ {j} ^ {1} (B _ {R _ {j}} (x _ {j})) ^ {\frac {2}{q}} + \mu_ {j} ^ {2} (\mathbb {R} ^ {n} \setminus B _ {2 R _ {j}} (x _ {j})) ^ {\frac {2}{q}} \big) - \epsilon - \frac {1}{\epsilon} E _ {j} \\ \geq S _ {q} \left(\lambda^ {\frac {2}{q}} + (1 - \lambda) ^ {\frac {2}{q}}\right) - \epsilon + o (1). \\ \end{array}
$$

But $\lambda ^ { \frac { 2 } { q } } + ( 1 - \lambda ) ^ { \frac { 2 } { q } } > 1$ as $\lambda \in ( 0 , 1 )$ . This is a contradiction.

Namely we obtain:

Theorem 4.7.4 (P.L. Lions) For any minimizing sequence $\{ u _ { j } \} \subset H ^ { 1 } ( \mathbb { R } ^ { n } )$ of the functional $\begin{array} { r } { J ( u ) = \int _ { \mathbb { R } ^ { n } } \left( | \nabla u | ^ { 2 } + | u | ^ { 2 } \right) } \end{array}$ subject to $| u | _ { q } = 1$ for $q \in ( 2 , 2 ^ { * } )$ ,

there exists a sequence $\{ x _ { j } \} \subset \mathbb { R } ^ { n }$ such that $v _ { j } ( x ) = u _ { j } ( x - x _ { j } )$ $L ^ { q }$ -strongly converges to a minimizer of $J$ .

Example 1. Assume $a \in C ( R ^ { n } )$ satisfies $a ( x ) \to a _ { \infty }$ as $| x | \to \infty$ . One studies the nontrivial solution $u \in H ^ { 1 } ( R ^ { n } )$ of the equation:

$$
- \Delta u + a (x) u = | u | ^ {q - 2} u, 2 <   q <   2 ^ {*}.
$$

Define

$$
I (u) = \int_ {R ^ {n}} (| \nabla u | ^ {2} + a (x) | u | ^ {2}), I _ {\infty} (u) = \int_ {R ^ {n}} (| \nabla u | ^ {2} + a _ {\infty} | u | ^ {2}),
$$

and

$$
M (u) := | u | _ {q} ^ {q} = \int_ {R ^ {n}} | u | ^ {q}.
$$

We define

$$
S = \inf  \left\{I (u) \mid M (u) = 1 \right\} \text {a n d} S _ {\infty} = \inf  \left\{I _ {\infty} (u) \mid M (u) = 1 \right\},
$$

and conclude: If $S ~ < ~ S _ { \infty }$ , then there is a nontrivial solution of the above equation.

Before going to the proof, we need a lemma (see [BL]):

Lemma 4.7.5 (Brezis–Lieb) Suppose that $\Omega \subset \mathbb { R } ^ { n }$ and $\{ u _ { j } \} \subset L ^ { p } ( \Omega )$ , $p \in$ $[ 1 , \infty )$ If $\{ u _ { j } \}$ is bounded in $L ^ { p } ( \Omega )$ , and $u _ { j }  u u . e$ ., on $\Omega$ . Then

$$
\lim  _ {j \rightarrow \infty} \left(\| u _ {j} \| _ {p} ^ {p} - \| u _ {j} - u \| _ {p} ^ {p}\right) = \| u \| _ {p} ^ {p}.
$$

Proof. According to Fatou’s lemma, $\begin{array} { r } { | u | _ { p } ^ { p } \leq \operatorname* { l i m } \operatorname* { i n f } _ { j  \infty } \| u _ { j } \| _ { p } ^ { p } < \infty } \end{array}$ . We begin with an elementary inequality: $\forall \epsilon > 0$ , there exists $C _ { \epsilon } > 0$ such that $\forall x , y \in$ $\mathbb { R } ^ { 1 }$ ,

$$
\left| \left| x + y \right| ^ {p} - \left| x \right| ^ {p} \right| \leq \epsilon \left| x \right| ^ {p} + C _ {\epsilon} \left| y \right| ^ {p}.
$$

$$
v _ {j, \epsilon} = (| | u _ {j} | ^ {p} - | u _ {j} - u | ^ {p} - | u | ^ {p} | - \epsilon | u _ {j} - u | ^ {p}) _ {+},
$$

then $v _ { j , \epsilon } \le ( 1 + C _ { \epsilon } ) | u | ^ { p }$ , and $v _ { j , \epsilon } \to 0$ a.e.

Then we can use Lebesgue’s dominance theorem to conclude: $\int _ { \Omega } { v _ { j , \epsilon } } \to 0$ as $j \to \infty$ . Since

$$
\left| \left| u _ {j} \right| ^ {p} - \left| u _ {j} - u \right| ^ {p} - \left| u \right| ^ {p} \right| \leq v _ {j, \epsilon} + \epsilon \left| u _ {j} - u \right| ^ {p},
$$

we have

$$
\lim  _ {j \rightarrow \infty} \sup  _ {\Omega} | | u _ {j} | ^ {p} - | | u _ {j} - u | ^ {p} - | u | ^ {p} | \leq C \epsilon ,
$$

where $C = \operatorname* { s u p } \{ \| u _ { j } - u \| _ { p } ^ { p } < \infty$ . Letting $\epsilon  0$ , the lemma is proved.

□

Now we turn to the proof of the conclusion in Example 1. In fact, by the previous discussion, after a subsequence, any

minimizing sequence $\{ u _ { j } \} \subset H ^ { 1 } ( R ^ { n } ) \cap M ^ { - 1 } ( 1 )$ is weakly $H ^ { 1 }$ convergent to some $u _ { 0 } \in H ^ { 1 } ( R ^ { n } )$ , so it is sufficient to show that $u _ { 0 } \in M ^ { - 1 } ( 1 )$ . Let $v _ { j } ~ = ~ u _ { j } - u _ { 0 }$ , we have $v _ { j } \ \to \ 0$ , in $H ^ { 1 } ( R ^ { n } )$ and $v _ { j } \  \ 0 \mathrm { i n } \ L _ { \mathrm { l o c } } ^ { 2 }$ , and then $v _ { j }  0$ a.e. Since we have:

1. $I ( u _ { j } ) = I ( u _ { 0 } ) + I ( v _ { j } ) + o ( 1 )$ .   
2. $| u _ { j } | _ { q } ^ { q } = | u _ { 0 } | _ { q } ^ { q } + | v _ { j } | _ { q } ^ { q } + o ( 1 )$ (from Lemma (4.7.6))   
3. $\begin{array} { r } { \int { a } ( { x } ) | v _ { j } | ^ { 2 } = { a } _ { \infty } \int | v _ { j } | ^ { 2 } + o ( 1 ) } \end{array}$

(from the estimate

$$
\int | a (x) - a _ {\infty} | | v _ {j} (x) | ^ {2} \leq \int_ {| x | \geq R} | a (x) - a _ {\infty} | | v _ {j} | ^ {2} + C \int_ {| x | \leq R} | v _ {j} | ^ {2},
$$

for suitably chosen $R > 0$ , where $C = 2 \mathrm { M a x } \{ a ( x ) \} )$ .

It follows that $I ( v _ { j } ) = I _ { \infty } ( v _ { j } ) + o ( 1 )$ . Let $| u _ { 0 } | _ { q } ^ { q } = \lambda$ . Obviously $\lambda \in \left[ 0 , 1 \right]$ ; we are going to exclude the case that $\lambda \in \left[ 0 , 1 \right)$ . Suppose not, either $\lambda \in ( 0 , 1 )$ , we would have

$$
\begin{array}{l} S \geq S | u _ {0} | _ {q} ^ {2} + S _ {\infty} | v _ {j} | _ {q} ^ {2} + o (1) \\ = S \lambda^ {\frac {2}{q}} + S _ {\infty} (1 - \lambda) ^ {\frac {2}{q}} + o (1) \\ \geq S \left(\lambda^ {\frac {2}{q}} + (1 - \lambda) ^ {\frac {2}{q}}\right) + o (1) > S, \\ \end{array}
$$

or, $\lambda = 0$ , then $S > S _ { \infty }$ . These are all impossible.

# 4.7.2 The Critical Sobolev Exponent and the Best Constants

The following best constant plays an important role in many variational problems arising in geometry and analysis:

$$
S = \inf _ {u \in D ^ {1, 2} \backslash \{\theta \}} \frac {\int_ {\mathbb {R} ^ {n}} | \nabla u | ^ {2}}{\left(\int_ {\mathbb {R} ^ {n}} | u | ^ {2 ^ {*}}\right) ^ {\frac {2}{2 ^ {*}}}},
$$

where $D ^ { 1 , 2 }$ is the closure of $C _ { 0 } ^ { \infty } ( \mathbb { R } ^ { n } )$ under the norm: $\begin{array} { r } { \| u \| = ( \int _ { \mathbb { R } ^ { n } } | \nabla u | ^ { 2 } ) ^ { \frac { 1 } { 2 } } } \end{array}$

We intend to figure out the precise value of $S$ and reduce it to a variational problem. Let

$$
I (u) = \| u \| ^ {2} := \int_ {\mathbb {R} ^ {n}} | \nabla u | ^ {2},
$$

and

$$
M (u) = | u | _ {2 ^ {*}} ^ {2 ^ {*}} = \int_ {\mathbb {R} ^ {n}} | u (x) | ^ {2 ^ {*}} d x.
$$

We study the minimization problem:

$$
\operatorname {M i n} \left\{I (u) \mid M (u) = 1 \right\}.
$$

Both $I$ and $M$ are not only translation invariant, but also invariant under the scaling transformation:

$$
T _ {\theta}: u (x) \rightarrow \theta^ {- \frac {n - 2}{2}} u \left(\frac {x}{\theta}\right), \forall \theta > 0.
$$

By the same idea as in the previous subsection, one may use the concentration compactness principle to show that after translation and scaling, the minimizing sequence is again convergent in $L ^ { 2 ^ { * } } ( \mathbb { R } ^ { n } )$ , and then a minimizer does exist. Let $u _ { 0 }$ be a minimizer; obviously $| u _ { 0 } |$ is also. Therefore we may assume the minimizer $u _ { 0 }$ is nonnegative. The Euler–Lagrange equation of the variational problem reads as

$$
- \Delta u = \lambda | u | ^ {2 ^ {*} - 2} u \text {i n} R ^ {n}, \tag {4.64}
$$

where $\lambda > 0$ is the Lagrange multiplier. After suitably adjusting a factor one may assume $\lambda = 1$ . By a moving plane argument due to Gidas, Ni and Nirenberg, [GNN] (see also Yanyan Li [Li 2]), $u _ { 0 }$ is radially symmetric: $u _ { 0 } ( x ) =$ $g ( | x | )$ , for some nonnegative function on $R _ { + } ^ { 1 }$ . Moreover, $g$ is nonincreasing. Plugging $g$ into equation (4.64), it becomes an ODE:

$$
- \left(r ^ {n - 1} g ^ {\prime}\right) ^ {\prime} = S r ^ {n - 1} g ^ {2 ^ {*} - 1}.
$$

However, we shall present here a more direct proof due to Lieb.

Lemma 4.7.6 If the problem $\mathrm { M i n } \{ I ( u ) | M ( u ) = 1 \}$ possesses a minimizer, then it has a radially symmetric and nonincreasing minimizer $u _ { 0 }$ ; i.e., there is a nonincreasing function $_ { g }$ defined on $\mathbb { R } _ { + } ^ { 1 }$ such that $u _ { 0 } ( x ) = g ( | x | )$ .

Proof. We may restrict ourselves to nonnegative minimizers, because $\| | u | \| \leq$ $\lVert u \rVert$ , and $\| | u | \| _ { q } = \| u \| _ { q } .$ . Let $u ^ { * }$ be the Schwarz rearrangement of $u$ (see Lemma 4.2.5), then $u ^ { * }$ is nonincreasing, radial symmetric and satisfies $m ( u _ { t } ) \ =$ $m ( u _ { t } ^ { * } )$ , $\forall t \in \mathbb { R } ^ { 1 }$ , where $u _ { t } = \{ x \in \mathbb { R } ^ { n } | u ( x ) \geq t \}$ . Thus,

$$
| u | _ {q} ^ {q} = \int_ {- \infty} ^ {\infty} | t | ^ {q} d m (u _ {t}) = \int_ {- \infty} ^ {\infty} | t | ^ {q} d m (u _ {t} ^ {*}) = | u ^ {*} | _ {q} ^ {q}.
$$

According to the Faber–Krahn inequality, it follows that

$$
\int_ {\mathbb {R} ^ {n}} | \nabla u ^ {*} | ^ {2} \leq \int_ {\mathbb {R} ^ {n}} | \nabla u | ^ {2}.
$$

□

Combining Corollary 4.7.2 with Lemma 4.7.6, one proves that the following equation possesses a positive solution in $H ^ { 1 } ( R ^ { n } )$ directly:

$$
- \Delta u + u = | u | ^ {q - 2} u \text {f o r} 2 <   q <   2 ^ {*}.
$$

Lemma 4.7.7 The minimizer of the problem in lemma 4.7.6 is achieved.

Proof. 1. By changing variables, let $F ( \alpha t ) = e ^ { \alpha t } g ( e ^ { t } )$ , where $\begin{array} { r } { \alpha = \frac { \pi } { 2 } - 1 } \end{array}$ , and $r = e ^ { t }$ ; we have

$$
\frac {1}{\alpha} \int_ {- \infty} ^ {\infty} F ^ {2 ^ {*}} d t = \int_ {0} ^ {\infty} g (r) ^ {2 ^ {*}} r ^ {n - 1} d r, \alpha \int_ {- \infty} ^ {\infty} (F ^ {\prime} - F) ^ {2} d t = \int_ {0} ^ {\infty} g ^ {\prime} (r) ^ {2} r ^ {n - 1} d r.
$$

Setting $\begin{array} { r } { f ( g ) = \int _ { 0 } ^ { \infty } g ^ { \prime } ( r ) ^ { 2 } r ^ { n - 1 } d r } \end{array}$ , $\begin{array} { r } { \phi ( g ) = \int _ { 0 } ^ { \infty } g ( r ) ^ { 2 ^ { * } } r ^ { n - 1 } d r } \end{array}$ , and $E _ { 0 } = \{ g \in$ $H _ { \mathrm { l o c } } ^ { 1 } ( R _ { + } ^ { 1 } ) | g \geq 0 , \phi ( g ) = \omega _ { n - 1 } ^ { - 1 } \}$ , $E _ { 0 } ^ { \prime } = \{ g \in E _ { 0 } | g$ is nonincreasing}, where ωn−1 = Γ( n +1) is the area of the unit sphere Sn−1, we have: $\begin{array} { r } { \omega _ { n - 1 } = \frac { n \pi ^ { \frac { n } { 2 } } } { \Gamma ( \frac { n } { 2 } + 1 ) } } \end{array}$ nπ n2 $S ^ { n - 1 }$

$$
\begin{array}{l} \operatorname {M i n} \left\{I (u) \mid M (u) = 1 \right\} \leq \operatorname {M i n} \left\{f (u) \mid u \in E _ {0} \right\} \leq \operatorname {M i n} \left\{f (u) \mid u \in E _ {0} ^ {\prime} \right\} \\ \leq \operatorname {M i n} \left\{I (u) \mid M (u) = 1 \right\}. \\ \end{array}
$$

Our problem is reduced to find:

$$
\operatorname {M i n} \left\{f (u) \mid u \in E _ {0} ^ {\prime} \right\}. \tag {4.65}
$$

2. Let $\begin{array} { r } { f _ { 1 } ( F ) ~ = ~ \int _ { - \infty } ^ { \infty } ( F ^ { \prime 2 } + F ^ { 2 } ) d t , \phi _ { 1 } ( F ) ~ = ~ \int _ { - \infty } ^ { \infty } F ^ { 2 ^ { * } } d t } \end{array}$ , and ${ \cal E } _ { 1 } ~ = ~ \{ F ~ \in ~$ $H ^ { 1 } ( R ^ { 1 } ) \mid \phi _ { 1 } ( F ) = \alpha \omega _ { n - 1 } ^ { - 1 } \}$ . Then the problem (4.65) is equivalent to the problem:

$$
\operatorname {M i n} \left\{f _ {1} (F) \mid F \in E _ {1} \right\}.
$$

In fact, on the one hand, if $F$ solves the latter, then $F ( t ) \to 0$ as $t \to \pm \infty$ , One has

$$
\int_ {- \infty} ^ {\infty} \left(F ^ {\prime} - F\right) ^ {2} d t = \int_ {- \infty} ^ {\infty} \left(F ^ {\prime 2} + F ^ {2}\right) d t. \tag {4.66}
$$

On the other hand, functions with compact supports consist of a dense subset of $E _ { 0 }$ , and for $g$ with compact support $F ( t )$ is zero as $t$ becomes large. Moreover, $F ( t ) \leq g ( 0 ) e ^ { t }$ , and then $F ( t )  0$ as $t  \pm \infty$ . Again equation (4.66) holds.

3. Let $E _ { 1 } ^ { \prime } = \{ F \in E _ { 1 } | F$ is even and nonincreasing $\mathrm { o n } R _ { + } ^ { 1 } \}$ . Again, by symmetric rearrangement, we have

$$
\operatorname {M i n} \left\{f _ {1} (F) \mid F \in E _ {1} \right\} = \operatorname {M i n} \left\{f _ {1} (F) \mid F \in E _ {1} ^ {\prime} \right\}.
$$

Let $\{ F _ { j } \}$ be a minimizing sequence in $E _ { 1 } ^ { \prime }$ , the sequence is weakly convergent to some $F _ { 0 } \in E _ { 1 } ^ { \prime }$ . We are going to verify that $\begin{array} { r } { | F _ { 0 } | _ { 2 ^ { * } } ^ { 2 ^ { * } } = \frac { \alpha } { \omega _ { n - 1 } } } \end{array}$

Since $F _ { j } \to F _ { 0 }$ , a.e., it is sufficient to find a dominant function in $L ^ { 2 ^ { * } } ( \mathbb { R } _ { + } ^ { 1 } )$ . From $F _ { j } ( t ) \to 0$ as $t \to \infty$ , we have

$$
F _ {j} (t) ^ {2} = - 2 \int_ {t} ^ {\infty} F _ {j} ^ {\prime} F _ {j} d t \leq \int_ {0} ^ {\infty} \left(F _ {j} ^ {\prime 2} + F _ {j} ^ {2}\right) d t \leq C.
$$

Since $F _ { j }$ is nonincreasing on $[ 0 , \infty )$ , and

$$
t F _ {j} (t) ^ {2} \leq \int_ {0} ^ {t} F _ {j} ^ {2} \leq C,
$$

the function $M ( t ) \ = \ \mathrm { M i n } \{ 1 , t ^ { - \frac { 1 } { 2 } } \}$ is a dominant function for $F _ { j }$ in $L ^ { 2 ^ { * } }$ $\left( \mathbb { R } _ { + } ^ { 1 } \right)$ . □

Now we can find the minimizer explicitly. Following the above notations, we conclude that the function $F _ { 0 }$ satisfies the ordinary differential equation:

$$
\left\{ \begin{array}{l} - F ^ {\prime \prime} + F = S \alpha^ {- 2} F ^ {2 ^ {*} - 1}, \\ F ^ {\prime} (0) = 0, \\ F (\infty) = 0. \end{array} \right.
$$

where $S$ is the best constant, $\begin{array} { r } { \alpha = \frac { n } { 2 } - 1 } \end{array}$ . The unique solution (see Lieb [Lieb], Aubin [Au 1] and Talenti [Tal]) reads as

$$
F _ {0} = \left(\sqrt {\frac {n (n - 2)}{4 S}} \operatorname {s e c h} (\alpha^ {- 1} t)\right) ^ {\alpha}.
$$

or

$$
g _ {0} (r) = r ^ {- \alpha} F _ {0} (\alpha \log r) = \left(\sqrt {\frac {n (n - 2)}{S}} \frac {1}{1 + r ^ {2}}\right) ^ {\frac {n}{2} - 1}.
$$

By direct computation, we obtain

$$
S = n (n - 2) \pi \left(\frac {\Gamma (n)}{\Gamma \left(\frac {n}{2}\right)}\right) ^ {- \frac {2}{n}}.
$$

Therefore we have proved:

Theorem 4.7.8 The functions

$$
u (x) = \left[ \frac {(n (n - 2) \theta)}{(\theta^ {2} + | x - y | ^ {2}) ^ {2}} \right] ^ {\frac {n - 2}{4}}, \forall \theta > 0, \forall y \in \mathbb {R} ^ {n} \tag {4.67}
$$

are minimizers of the problem $S = \mathrm { M i n } \{ \| u \| _ { 2 } ^ { 2 } | | u | _ { 2 ^ { * } } = 1 \}$ .

In fact, all minimizers for $S$ are of the form (4.67), see Caffarelli, Gidas, and Spruck [CGS]. A simple proof via the moving plane method [GNN] was given by W. Chen and C. Li, Yanyan Li, see [ChL 2], [Li 2].

Example 2. Find a nontrivial solution $u \in H _ { 0 } ^ { 1 } ( \Omega )$ of the following equation, where $\Omega \subset \mathbb { R } ^ { n }$ is a bounded domain:

$$
- \Delta u - \lambda u = | u | ^ {2 ^ {*} - 2} u \text {i n} \Omega . \tag {4.68}
$$

where we assume $\lambda < \lambda _ { 1 }$ , the first eigenvalue of $- \varDelta$ . Again, the minimization problem $\mathrm { M i n } \{ I _ { \lambda } ( u ) | | u | _ { 2 ^ { * } } = 1 \}$ is considered, where

$$
I _ {\lambda} (u) = \| u \| ^ {2} - \lambda | u | _ {2} ^ {2}.
$$

Let

$$
S _ {\lambda} (\Omega) = \inf  \left\{I _ {\lambda} (u) \mid u \in H _ {0} ^ {1} (\Omega), | u | _ {2 ^ {*}} = 1 \right\}.
$$

# Lemma 4.7.9

$$
S _ {0} (\Omega) = S.
$$

Proof. 1. For $\Omega _ { 1 } \subset \Omega _ { 2 }$ , by definition, $S _ { 0 } ( \Omega _ { 2 } ) \leq S _ { 0 } ( \Omega _ { 1 } )$ .

2. For $\forall R > 0$ , let $\Omega _ { j } = B _ { j R } ( \theta ) , j = 1 , 2$ . We claim that $S _ { 0 } ( \Omega _ { 1 } ) = S _ { 0 } ( \Omega _ { 2 } )$ This is due to the scaling invariance of the functional $S _ { 0 } ( \Omega )$ .   
3. By the translation invariance, we conclude that for all $\Omega , S _ { 0 } ( \Omega ) \ =$ $S _ { 0 } ( B _ { 1 } ( \theta ) )$ .

This proves the lemma.

![](images/a70eba2a668e1214324c49cd04e4b4383451850068d85bb1d348613b858936c9.jpg)

Following the steps in Example 1, there exists a minimizing sequence $\{ u _ { j } \}$ with $| u _ { j } | _ { 2 ^ { * } } = 1$ and $u _ { j } \to u _ { 0 }$ in $H _ { 0 } ^ { 1 } ( \Omega )$ . Let $v _ { j } = u _ { j } - u _ { 0 }$ , we have.

1. $I _ { \lambda } ( u _ { j } ) = I _ { \lambda } ( u _ { 0 } ) + I _ { \lambda } ( v _ { j } ) + o ( 1 )$ .   
2. $| u _ { j } | _ { 2 ^ { * } } ^ { 2 ^ { * } } = | u _ { 0 } | _ { 2 ^ { * } } ^ { 2 ^ { * } } + | v _ { j } | _ { 2 ^ { * } } ^ { 2 ^ { * } } + o ( 1 )$

Let $| u _ { 0 } | _ { 2 ^ { * } } ^ { 2 ^ { * } } = \mu \in [ 0 , 1 ]$ . It remains to verify that $\mu = 1$ . We shall exclude the cases $\mu = 0$ , and $\mu \in ( 0 , 1 )$ . In fact, if $\mu \in ( 0 , 1 )$ , then

$$
\begin{array}{l} S _ {\lambda} \geq S _ {\lambda} \left(\left| u _ {0} \right| _ {2 ^ {*}} ^ {2} + \left| v _ {j} \right| _ {2 ^ {*}} ^ {2}\right) + o (1) \\ = S _ {\lambda} \left(\mu^ {\frac {2}{2 ^ {*}}} + (1 - \mu) ^ {\frac {2}{2 ^ {*}}}\right) + o (1) \\ > S _ {\lambda} + o (1). \\ \end{array}
$$

This is impossible. If $\mu = 0$ , then $u _ { 0 } = \theta$ , i.e., $u _ { j }  \theta$ in $H _ { 0 } ^ { 1 } ( \Omega )$ , and then $| u _ { j } | _ { 2 } \to 0$ . Thus

$$
\begin{array}{l} I _ {\lambda} (u _ {j}) = | \nabla u _ {j} | _ {2} ^ {2} + o (1) \\ \geq S \left| u _ {j} \right| _ {\frac {2}{2 ^ {*}}} ^ {\frac {3}{2}} + o (1) = S + o (1). \\ \end{array}
$$

Now, we need the following:

Lemma 4.7.10 (Brezis–Nirenberg [BN 1] For $n \geq 4 , \forall \lambda > 0$ ,

$$
S _ {\lambda} (\Omega) := \inf  _ {u \in H _ {0} ^ {1} (\Omega) \backslash \{\theta \}} \frac {I _ {\lambda} (u)}{| u | _ {2 ^ {*}} ^ {2}} <   S. \tag {4.69}
$$

Proof. One may assume $\theta \in \Omega$ . Let $\varphi \in C _ { 0 } ^ { \infty } ( \Omega )$ , and $\varphi ( x ) = 1$ in a small neighborhood of $\theta$ . For $\varepsilon > 0$ , we define

$$
v _ {\varepsilon} (x) = \frac {\varphi (x)}{(\varepsilon^ {2} + | x | ^ {2}) ^ {\frac {n}{2} - 1}}.
$$

Then

$$
\nabla v _ {\varepsilon} (x) = \frac {\nabla \varphi (x)}{(\varepsilon^ {2} + | x | ^ {2}) ^ {\frac {n}{2} - 1}} - \frac {(n - 2) \varphi x}{(\varepsilon^ {2} + | x | ^ {2}) ^ {\frac {n}{2}}}.
$$

We have constants $K _ { 1 } , K _ { 2 } , K _ { 3 } > 0$ such that

$$
\begin{array}{l} \left\| v _ {\varepsilon} \right\| _ {2} ^ {2} = (n - 2) ^ {2} \int_ {\Omega} \frac {| x | ^ {2} d x}{(\varepsilon^ {2} + | x | ^ {2}) ^ {n}} + O (1) = (n - 2) ^ {2} \int_ {R ^ {n}} \frac {| x | ^ {2} d x}{(\varepsilon^ {2} + | x | ^ {2}) ^ {n}} + O (1) \\ = \frac {K _ {1}}{\varepsilon^ {n - 2}} + O (1) , \\ \end{array}
$$

$$
| v | _ {2 ^ {*}} ^ {2 ^ {*}} = \int_ {\Omega} \frac {| \varphi | ^ {2 ^ {*}} d x}{\left(\varepsilon^ {2} + | x | ^ {2}\right) ^ {n}} = \int_ {R ^ {n}} \frac {d x}{\left(\varepsilon^ {2} + | x | ^ {2}\right) ^ {n}} + O (1) = \frac {K _ {2}}{\varepsilon^ {n}} + O (1),
$$

and

$$
| v _ {\varepsilon} | _ {2} ^ {2} = \int_ {\Omega} \frac {| \varphi | ^ {2} d x}{(\varepsilon^ {2} + | x | ^ {2}) ^ {n - 2}} = \int_ {R ^ {n}} \frac {d x}{(\varepsilon^ {2} + | x | ^ {2}) ^ {n - 2}} + O (1) = \frac {K _ {3}}{\varepsilon^ {n - 4}} + O (1),
$$

as $n \geq 5$ , and

$$
\left| v _ {\varepsilon} \right| _ {2} ^ {2} = K _ {3} | \log \varepsilon | + O (1),
$$

as $n = 4$ .

In summary, we have

$$
S _ {\lambda} \leq \left\{ \begin{array}{l} \frac {K _ {1} - \lambda K _ {3} \varepsilon^ {2}}{K _ {2} ^ {1 - \frac {2}{n}}} \text {i f} n \geq 5  , \\ \frac {K _ {1} - \lambda K _ {3} \varepsilon^ {2} \log | \varepsilon |}{K _ {2} ^ {\frac {1}{2}}} \text {i f} n = 4  . \end{array} \right.
$$

Since $K _ { 3 } > 0$ and $\frac { K _ { 1 } } { K _ { 2 } ^ { 1 - \frac { 2 } { n } } } = S$ for small $\varepsilon > 0 , S _ { \lambda } < S$ .

Provided by Lemma 4.7.10, the vanishing case is excluded. We have proved the existence of a nontrivial solution.

Remark 4.7.11 In fact, the restriction $\lambda < \lambda _ { 1 }$ is not necessary. Provided by minimax methods, which we shall study in the next subsection, one can remove this restriction. Also the case $n = 3$ has been discussed, see Brezis–Nirenberg [BN 1].

Remark 4.7.12 (Yamabe problem) Let $( M , g _ { 0 } )$ be a connected compact ndimensional Riemannian manifold. One asks: Does there exist a metric $g$ pointwise conformal to $g _ { 0 }$ such that the scalar curvature $R$ with respect to $g$ is a constant?

Set $g \ = \ u ^ { \frac { 4 } { n - 2 } } g _ { 0 }$ , where $u \in C ^ { \infty } ( M )$ with $u > 0$ . It is reduced to the following PDE:

$$
- \frac {4 (n - 1)}{n - 2} \Delta_ {g _ {0}} u + R _ {g _ {0}} u = R u ^ {2 ^ {*} - 1},
$$

where $\varDelta _ { g _ { 0 } }$ is the Laplacian Beltrami operator, and $R _ { g _ { 0 } }$ is the scalar curvature with respect to $g _ { 0 }$ . Then it is transferred to the variational problem:

$$
\lambda (M) = \min _ {u \in H ^ {1} (M)} \frac {\int_ {M} | \nabla_ {g _ {0}} u | ^ {2} + \frac {n - 2}{4 (n - 1)} R _ {g _ {0}} u ^ {2}}{(\int_ {M} | u | ^ {2 *}) ^ {\frac {2}{2 *}}} .
$$

This is a problem very similar to Example 2. Again, $\lambda ( M ) \leq S$ . If $\lambda ( M ) <$ $S$ , then the problem is solvable by the previous method. However, it is easily seen that if $( M , g _ { 0 } ) = ( S ^ { n } , \hat { g } _ { 0 } )$ , where $\hat { g } _ { 0 }$ is the canonical metric, then $\lambda ( S ^ { n } ) =$ $S$ . The Yamabe problem has been solved by Aubin [Au 1] and Schoen [Sco]. They proved:

Theorem 4.7.13 For $n \geq 3$ , if $( M , g _ { 0 } )$ is a compact connected Riemannian manifold, which is not conformally equivalent to $( S ^ { n } , \hat { g } _ { 0 } )$ , then $\lambda ( M ) < S$ . Then Yamabe problem is solvable.

# 4.8 Minimax Methods

# 4.8.1 Ekeland Variational Principle

It is well known that the direct method does not work in the lack of compactness (i.e., the coerciveness). Without coerciveness, only approximate minimizers can be found. Let us recall the Ekeland variational principle, which we have derived in Chap. 2 as an equivalence of Caristi fixed-point theorem. Due to the importance of this principle, we present here a direct proof.

Theorem 4.8.1 (Ekeland) Let $( X , d )$ be a complete metric space, and let $f : X \to \mathbb { R } ^ { 1 } \cup \{ + \infty \}$ , but $f \not \equiv + \infty$ . If $f$ is bounded from below and l.s.c., and if $\exists \varepsilon > 0$ , $\exists x _ { \varepsilon } \in X$ satisfying $f ( x _ { \varepsilon } ) < \operatorname* { i n f } _ { X } f + \varepsilon$ . Then $\exists y _ { \varepsilon } \in X$ such that

1. $f ( y _ { \varepsilon } ) \leqslant f ( x _ { \varepsilon } )$ ,   
2. $d ( x _ { \varepsilon } , y _ { \varepsilon } ) \leqslant 1$   
3. $f ( x ) > f ( y _ { \varepsilon } ) - \varepsilon d ( y _ { \varepsilon } , x ) , \qquad \forall x \neq y _ { \varepsilon } .$

Proof. Note that the required $y _ { \varepsilon }$ is the minimum of the function $f ( x ) +$ $\varepsilon d ( y _ { \varepsilon } , x )$ , while the new function contains $y _ { \varepsilon }$ itself. Accordingly, we define a sequence approaching $y _ { \varepsilon }$ .

Choose $\boldsymbol { u } _ { 0 } = \boldsymbol { x } _ { \varepsilon }$ . Suppose that $u _ { n }$ is already chosen. Set

$$
S _ {n} = \left\{w \in X \mid f (w) \leqslant f \left(u _ {n}\right) - \varepsilon d \left(w, u _ {n}\right) \right\}.
$$

Obviously $S _ { n } \neq \emptyset$ . We choose $u _ { n + 1 } \in S _ { n }$ satisfying

$$
f \left(u _ {n + 1}\right) - \inf  _ {S _ {n}} f \leqslant \frac {1}{2} \left[ f \left(u _ {n}\right) - \inf  _ {S _ {n}} f \right], \tag {4.70}
$$

$n = 0 , 1 , 2 , \ldots .$ We want to show that $\{ u _ { n } \}$ is a Cauchy sequence. Indeed,

$$
\varepsilon d \left(u _ {m}, u _ {n}\right) \leqslant f \left(u _ {n}\right) - f \left(u _ {m}\right), \quad \forall m > n. \tag {4.71}
$$

Since $f$ is bounded below, $f ( u _ { n } ) \ : - \ : f ( u _ { m } ) \ :  \ : 0$ as $m , n \to \infty$ , it follows $u _ { n } \to u ^ { * } \in X$ . From the l.s.c. of $f$ and (4.70), we have

$$
f \left(u ^ {*}\right) \leqslant \varliminf_ {n \rightarrow \infty} f \left(u _ {n}\right) \leqslant \varliminf_ {n \rightarrow \infty} \inf  _ {S _ {n}} f. \tag {4.72}
$$

We are going to verify that $y _ { \varepsilon } = u ^ { * }$ satisfies the conclusions 1–3. In fact, since from (4.71) $f ( u _ { n } )$ is nonincreasing, conclusion 1 is trivially true. Again by (4.71), we have

$$
\begin{array}{l} \varepsilon d \left(x _ {\varepsilon}, y _ {\varepsilon}\right) = \varepsilon d \left(u _ {0}, u ^ {*}\right) \\ \leqslant f \left(x _ {\varepsilon}\right) - f \left(u ^ {*}\right) \\ \leqslant f (x _ {\varepsilon}) - \inf  _ {S _ {n}} f \\ <   \varepsilon . \\ \end{array}
$$

Thus, conclusion 2 follows. Conclusion 3 is proved by contradiction. If $y _ { \varepsilon } = u ^ { * }$ does not satisfy conclusion 3, then $\exists w \neq u ^ { * }$ such that

$$
f (w) \leqslant f \left(u ^ {*}\right) - \varepsilon d \left(u ^ {*}, w\right). \tag {4.73}
$$

By combining (4.71) with (4.73), it follows that

$$
f (w) \leqslant f (u _ {n}) - \varepsilon d (u _ {n}, w), \quad \forall n \geqslant 0,
$$

8 i.e., $w \in \bigcap _ { n = 1 } ^ { \infty } S _ { n } .$ . Thus, by (4.72), $f ( u ^ { * } ) \leqslant f ( w )$ . It contradicts (4.73).

□

Corollary 4.8.2 Let $( X , d )$ be a complete metric space, and let $f : X \to$ $\mathbb { R } _ { + } ^ { 1 } \cup \{ + \infty \}$ be proper, bounded from below and l.s.c. Then for $\forall \varepsilon > 0 , \exists y _ { \varepsilon } \in X$ such that $f ( x ) > f ( y _ { \varepsilon } ) - \varepsilon d ( x , y _ { \varepsilon } ) , \forall x \neq y _ { \varepsilon }$ .

Let us introduce the following notion, which plays an important role in the calculus of variations in the large.

Definition 4.8.3 Let $X$ be a Banach space, and $f \in C ^ { 1 } ( X , \mathbb { R } ^ { 1 } )$ . If any sequence $\{ x _ { j } \} _ { 1 } ^ { \infty } \subset X$ along which $f ( x _ { j } )  c$ and $f ^ { \prime } ( x _ { j } )  \theta$ implies a convergent subsequence. Then $f$ is said to satisfy the $( P S ) _ { c }$ condition. If $f$ satisfies the $( P S ) _ { c }$ condition, for c, then it is said to satisfy the $( P S )$ condition.

Let us return to the minimizing problem:

Corollary 4.8.4 Let $X$ be a Banach space, and let $f \in C ^ { 1 } ( X , \mathbb { R } ^ { 1 } )$ be bounded from below. If $( P S ) _ { c }$ holds with c = inf f, then $c = \operatorname* { i n f } _ { X } f$ $f$ possesses a minimum.

Proof. According to Theorem 4.8.1, $\forall n \geqslant 0$ ∃ xn ∈ X satisfying

$$
\left\{ \begin{array}{l} f (x) > f (x _ {n}) - \frac {1}{n} \| x - x _ {n} \| \\ f (x _ {n}) <   c + \frac {1}{n} \end{array} \right.
$$

which implies that $f ( x _ { n } ) \to c$ and $f ^ { \prime } ( x _ { n } )  \theta$ . Therefore we obtain a convergent subsequence $x _ { n _ { j } } \to x ^ { * }$ , according to $( P S ) _ { c }$ . So is

$$
f (x ^ {*}) = c = \inf  _ {X} f.
$$

# 4.8.2 Minimax Principle

In this section, we are concerned with those critical points that are saddle points but not minima. The following geometric intuition suggests a minimax consideration.

A valley is surrounded by mountains. Starting from a point $x _ { 1 }$ on the ground outside these mountains, we intend to get into a place $x _ { 0 }$ in the valley. A path $\it l$ one would like to take, is a path along which the highest point is lower than those at neighboring paths.

The highest point on this path is indeed a saddle point of the height function.

Theorem 4.8.5 (Ambrosetti–Rabinowitz [AR]) Let $X$ be a Banach space, and $f \in C ^ { 1 } ( X , \mathbb { R } ^ { 1 } )$ . Suppose that $\Omega \subset X$ is an open set, $x _ { 0 } \in \Omega$ , and $x _ { 1 } \notin \Omega$ . Set

$$
\Gamma = \left\{l \in C ([ 0, 1 ], X) \mid l (i) = x _ {i}, i = 0, 1 \right\},
$$

and

$$
c = \inf_{l\in \Gamma}\max_{t\in [0,1]}f\circ l(t)  .
$$

If

1. $\alpha = \operatorname* { i n f } _ { x \in \partial \Omega } f ( x ) > \operatorname* { m a x } \{ f ( x _ { 0 } ) , f ( x _ { 1 } ) \} ,$ ,   
2. $( P S ) _ { c }$ ∈ holds for $f$ ;

then $c \geqslant \alpha$ is a critical point.

We shall not prove this theorem at this moment, but introduce a more general notion.

Definition 4.8.6 Let $X$ be a Banach space. Let $Q \subset X$ be a compact manifold with boundary ∂Q and let $S \subset X$ be a closed subset of $X$ . ∂Q is said linking with $S$ , if

1. $\partial Q \cap S = \emptyset$   
2. $\forall \varphi : Q  X$ continuous with $\varphi | _ { \partial \Omega } = \mathrm { i d } | _ { \partial \Omega }$ , we have $\varphi ( Q ) \cap S \neq \emptyset$ .

Example 4.8.7 (Mountain pass) Let $\Omega , x _ { 0 } , x _ { 1 }$ be as in Theorem 4.8.5. Set $Q =$ the segment $\{ \lambda x _ { 0 } + ( 1 - \lambda ) x _ { 1 } \mid \lambda \in [ 0 , 1 ] \}$ , and $S = \partial \Omega$ . Then, $\partial Q =$ $\{ x _ { 0 } , x _ { 1 } \}$ and $S$ link.

Example 4.8.8 Let $X _ { 1 }$ be a finite-dimensional linear subspace of the Banach space $X$ , and let $X _ { 2 }$ be its complement: $X = X _ { 1 } + X _ { 2 }$ . Let $Q = B _ { R } \cap X _ { 1 } , S =$ $X _ { 2 }$ , where $B _ { R }$ is the ball with radius $R > 0$ centered at $\theta$ . Then ∂Q and $S$ link.

Indeed, $\forall \varphi : Q  X$ continuous with $\varphi | _ { \partial Q } = \mathrm { i d } | _ { \partial \Omega }$ , we want to show: $\varphi ( Q ) \cap X _ { 2 } \neq \varnothing$ .

Let $P$ be the projection onto $X _ { 1 }$ . It is sufficient to show that $P \circ \varphi : Q \to X _ { 1 }$ has a zero.

Obviously

$$
\deg (P \circ \varphi , Q, \theta) = \deg (\operatorname {i d}, Q, \theta) = 1.
$$

The conclusion follows from the Brouwer degree theory.

Theorem 4.8.9 Let $X$ be a Banach space, and let $f \in C ^ { 1 } ( X , \mathbb { R } ^ { 1 } )$ . Assume that $Q \subset X$ is a compact manifold with boundary ∂Q which links with a closed subset $S \subset X$ . Set

$$
\Gamma = \{\varphi \in C (Q, X) \mid \varphi | _ {\partial Q} = \operatorname {i d} | _ {\partial Q} \},
$$

and

$$
c = \inf_{\varphi \in \Gamma}\max_{\xi \in Q}f\circ \varphi (\xi)  .
$$

If $\doteq \alpha < \beta$ such that

$$
\sup  _ {x \in \partial Q} f (x) \leqslant \alpha <   \beta \leqslant \inf  _ {x \in S} f (x),
$$

and if $( P S ) _ { c }$ holds, then $c \left( \geqslant { \beta } \right)$ is a critical value.

Proof. Let $d$ be the distance on $C ( Q , X )$ . Then $( \Gamma , d )$ is a metric space. Let

$$
J (\varphi) = \max _ {\xi \in Q} f \circ \varphi (\xi) .
$$

Invoke the assumption that $\partial Q$ and $S$ link, J - inf f. Moreover, $J \geqslant \operatorname* { i n f } _ { S } f$ $J$ is locally Lipschitzian, since

$$
\begin{array}{l} J (\varphi_ {1}) - J (\varphi_ {2}) \leqslant \max  _ {\xi \in Q} [ f \circ \varphi_ {1} (\xi) - f \circ \varphi_ {2} (\xi) ] \\ \leqslant \max_{\substack{\theta \in [0,1]\\ \xi \in Q}}\| f^{\prime}(\theta \varphi_{1}(\xi) + (1 - \theta)\varphi_{2}(\xi))\| d(\varphi_{1},\varphi_{2})  . \\ \end{array}
$$

It follows that

$$
\left| J \left(\varphi_ {1}\right) - J \left(\varphi_ {2}\right) \right| \leqslant C d \left(\varphi_ {1}, \varphi_ {2}\right),
$$

where $C$ is a constant depending on $\varphi _ { 1 } , \varphi _ { 2 }$ . We apply the Ekeland variational principle to $J$ , and obtain a sequence $\{ \varphi _ { n } \} \subset \Gamma$ satisfying

$$
c \leqslant J \left(\varphi_ {n}\right) <   c + \frac {1}{n}, \tag {4.74}
$$

and

$$
J (\varphi) \geqslant J \left(\varphi_ {n}\right) - \frac {1}{n} d \left(\varphi , \varphi_ {n}\right), \tag {4.75}
$$

$$
n = 1, 2, 3, \dots .
$$

Set

$$
\mathcal {M} (\varphi) = \{\xi \in Q \mid f \circ \varphi (\xi) = J (\varphi) \}.
$$

Obviously $\mathcal { M } ( \varphi )$ is compact. We claim that ${ \mathcal { M } } ( \varphi ) \subset \operatorname { i n t } ( Q )$ . Indeed, since $\partial Q$ and $S$ link, if $\exists \xi _ { 0 } \in { \mathcal { M } } ( \varphi ) \cap \partial Q$ , then

$$
f \circ \varphi (\xi_ {0}) = \max  _ {\xi \in Q} f \circ \varphi (\xi) \geqslant \inf  _ {S} f \geqslant \beta .
$$

But,

$$
f \circ \varphi (\xi_ {0}) = f (\xi_ {0}) \leqslant \sup  _ {x \in \partial Q} f (x) \leqslant \alpha .
$$

This is a contradiction.

Set

$$
\Gamma_ {0} = \left\{\psi \in C (Q, X) \mid \psi | _ {\partial Q} = \theta \right\}.
$$

It is a linear closed subspace of $X$ . Let $\| \cdot \|$ be the norm of $C ( Q , X )$ . h $\Gamma _ { 0 }$ , with $\| h \| = 1 , \forall \lambda _ { j } \downarrow 0 , \forall \xi _ { j } \in \mathcal { M } ( \varphi _ { n } + \lambda _ { j } h )$ , we have

$$
\lambda_ {j} ^ {- 1} \left[ f \circ \left(\varphi_ {n} + \lambda_ {j} h\right) \left(\xi_ {j}\right) - f \circ \varphi_ {n} \left(\xi_ {j}\right) \right] \geqslant - \frac {1}{n}, \tag {4.76}
$$

from (4.75). Since $\{ \xi _ { j } \} \subset Q$ , we obtain a convergent subsequence $\xi _ { j } \to \eta _ { n } ^ { * } \in$ $\mathcal { M } ( \varphi _ { n } )$ , which depends on $\varphi _ { n } , \lambda _ { j }$ and $h$ . After taking limits, we have

$$
\langle f ^ {\prime} \circ \varphi_ {n} \left(\eta_ {n} ^ {*}\right), h \left(\eta_ {n} ^ {*}\right) \rangle \geqslant - \frac {1}{n}. \tag {4.77}
$$

We want to show that $\exists \eta _ { n } \in { \mathcal { M } } ( \varphi _ { n } )$ such that

$$
\langle f ^ {\prime} \circ \varphi_ {n} (\eta_ {n}), u \rangle \geqslant - \frac {1}{n}, \tag {4.78}
$$

$\forall u \in X$ with $\| u \| = 1$ . If not, $\forall \eta \in \mathcal { M } ( \varphi _ { n } ) , \exists v _ { \eta } \in X$ with vη = 1, satisfying

$$
\langle f ^ {\prime} \circ \varphi_ {n} (\eta), v _ {\eta} \rangle <   - \frac {1}{n},
$$

then there exists a neighborhood of $\eta$ , $O _ { \eta } \subset \operatorname { i n t } ( Q )$ such that

$$
\langle f ^ {\prime} \circ \varphi_ {n} (\xi), v _ {\eta} \rangle <   - \frac {1}{n}, \qquad \forall \xi \in O _ {\eta}.
$$

Since $\mathcal { M } ( \varphi _ { n } )$ is compact, there is a finite covering. Let $m$ be the least number Mof covering: $\cup _ { i = 1 } ^ { r n } O _ { \eta _ { i } } \supset { \mathcal { M } } ( \varphi _ { n } )$ . We obtain the associate $\{ v _ { \eta _ { i } } \} _ { 1 } ^ { m }$ , $\| v _ { \eta _ { i } } \| = 1$ satisfying

$$
\langle f ^ {\prime} \circ \varphi_ {n} (\xi), v _ {\eta_ {i}} \rangle <   - \frac {1}{n} \quad \forall \xi \in O _ {\eta_ {i}} ,
$$

$i = 1 , 2 , \dots , m$ . Construct a partition of unity subject to $\{ O _ { \eta _ { i } } \} _ { 1 } ^ { m } : 0 \leqslant \varrho _ { i } \leqslant$ $1 , \operatorname* { s u p } \varrho _ { i } \subset O _ { \eta _ { i } } , i = 1 , . . . , m$ , and

$$
\sum_ {i = 1} ^ {m} \varrho_ {i} (\xi) \equiv 1, \quad \forall \xi \in \mathcal {M} (\varphi_ {n}) .
$$

We set

$$
v = v (\xi) = \sum_ {i = 1} ^ {m} \varrho_ {i} (\xi) v _ {\eta_ {i}}.
$$

Thus $v \in \Gamma _ { 0 }$ and $\| v \| \leqslant 1$ . But, $\exists \xi ^ { * } \in \mathcal { M } ( \varphi _ { n } )$ such that there is only one $i _ { 0 }$ satisfying $\xi ^ { \ast } \in O _ { \eta _ { i _ { 0 } } }$ . Therefore $\lVert \boldsymbol { v } \rVert = 1$ . We obtain

$$
\langle f ^ {\prime} \circ \varphi_ {n} (\xi), v (\xi) \rangle <   - \frac {1}{n}, \forall \xi \in \mathcal {M} (\varphi_ {n}).
$$

This contradicts (4.77). Thus (4.78) holds.

Setting $x _ { n } = \varphi _ { n } ( \eta _ { n } )$ , we have

$$
f ^ {\prime} \left(x _ {n}\right)\rightarrow \theta .
$$

Combining with (4.74), we have also

$$
f (x _ {n}) \rightarrow c.
$$

The $( P S ) _ { c }$ condition implies that $c$ is a critical value.

The above proof is based on Shi [Shi] and Aubin and Ekeland [AE].

The following example due to Brezis and Nirenberg [BN 3] asserts that the $( P S ) _ { c }$ condition is crucial in Theorem 4.5.5.

Example. The function $\varphi ( x , y ) = x ^ { 2 } + ( 1 - x ) ^ { 3 } y ^ { 2 }$ defined on $R ^ { 2 }$ does have a mountain surround $( 0 , 0 )$ : $\varphi ( x , y ) ~ \geq ~ c ~ > ~ 0$ on the circle $x ^ { 2 } + y ^ { 2 } = { \frac { 1 } { 4 } }$ , $\varphi ( 0 , 0 ) = 0$ , and $\varphi ( 4 , 1 ) = - 1 1$ . But by direct computation it has only one critical point: $( 0 , 0 )$ .

# 4.8.3 Applications

The mountain pass lemma and the related minimax principles are widely used in the study of differential equations. We are satisfied to give few examples.

Example 1. We study the following forced resonance problem.

Given $h \in L ^ { 1 } ( [ 0 , \pi ] )$ satisfying

$$
\int_ {0} ^ {\pi} h (t) \sin t d t = 0. \tag {4.79}
$$

Assume that $g \in C ( \mathbb { R } ^ { 1 } )$ is $T$ -periodic, $T > 0$ , and satisfies:

$$
\int_ {0} ^ {T} g (t) d t = 0. \tag {4.80}
$$

Find a solution of the nonlinear BVP:

$$
\left\{ \begin{array}{l} \ddot {x} (t) + x (t) = h (t) + g (x (t)) \quad t \in (0, \pi) \\ x (0) = x (\pi) = 0. \end{array} \right. \tag {4.81}
$$

First we introduce a generalized Riemann-Lebesgue lemma.

Lemma 4.8.10 Under the above assumptions of $g$ , if $u _ { n } \to u$ in $C ( [ 0 , \pi ] )$ , $\alpha _ { n }  \infty$ , $u \in C ^ { 1 } ( [ 0 , \pi ] )$ and $k \in L ^ { 1 } ( [ 0 , \pi ] )$ , then

$$
\lim  _ {n \rightarrow \infty} \int_ {0} ^ {\pi} g (u _ {n} (t) + \alpha_ {n} \sin t) k (t) d t \rightarrow 0.
$$

Proof. Since the sequence is bounded in $C ( [ 0 , \pi ] )$ , and since $g ( u _ { n } ( t ) \textrm { + }$ $\alpha _ { n } \sin n t ) - g ( u ( t ) + \alpha _ { n } \sin t )$ uniformly converges to $0$ , it is sufficient to prove that

$$
\lim _ {n \to \infty} \int_ {0} ^ {\pi} g (u (t) + \alpha_ {n} \sin t) \cdot \chi_ {E} (t) d t = 0,
$$

$\forall E = ( a , b ) \subset ( 0 , \pi )$ , where $\chi _ { E }$ is the characteristic function of $E$ . We may assume $\alpha _ { n } ~ \to ~ + \infty$ . We write $v _ { n } ( t ) \ : = \ : \alpha _ { n } ^ { - 1 } u ( t ) + \sin t$ , and let $G ( t )$ be a primitive of $g$ . Then $G$ is $T$ -periodic, according to (4.80).

At first, we assume ${ \frac { \pi } { 2 } } \not \in E$ . Then for $n$ large, $s = v _ { n } ( t )$ is strictly increasing (decreasing), if $b < \frac \pi 2$ (or $\begin{array} { r } { a > \frac { \pi } { 2 } } \end{array}$ resp.). We have $t = v _ { n } ^ { - 1 } ( s )$ .

Let $\delta \ = \ \operatorname { d i s t } \left( E , { \frac { \pi } { 2 } } \right)$ , $\exists$ a constant $C _ { \delta }$ depending on $\delta \ > \ 0$ , such that 1v (v−1n (s))   Cδ a.e. By changing variables, $\frac { 1 } { | v _ { n } ^ { \prime } ( v _ { n } ^ { - 1 } ( s ) ) | } \leqslant C _ { \delta }$ 1

$$
\begin{array}{l} \left| \int_ {a} ^ {b} g \left(\alpha_ {n} v _ {n} (t)\right) d t \right| = \left| \int_ {v _ {n} (a)} ^ {v _ {n} (b)} g \left(\alpha_ {n} s\right) \frac {d s}{v _ {n} ^ {\prime} \left(v _ {n} ^ {- 1} (s)\right)} \right| \\ \leqslant \frac {C _ {\delta}}{\alpha_ {n}} | G \left(\alpha_ {n} v _ {n} (b)\right) - G \left(\alpha_ {n} v _ {n} (a)\right) |. \tag {4.82} \\ \end{array}
$$

Since $G$ is continuous and periodic, the RHS of the above inequality tends to zero as $n \longrightarrow \infty$ .

Second, we consider the case $\frac { \pi } { 2 } \in E$ . Since $g$ is bounded, provided by the absolute continuity of the integral of $k$ , $\forall \varepsilon > 0$ , $\exists \delta > 0$ such that

$$
\left| \int_ {\pi / 2 - \delta} ^ {\pi / 2 + \delta} k (t) g \left(\alpha_ {n} v _ {n} (t)\right) d t \right| <   \frac {\varepsilon}{2}. \tag {4.83}
$$

Combining (4.82) with (4.83), the conclusion follows.

Let us reformulate (4.81) in a variational version. Again let $G$ be a primitive of $g$ satisfying $\begin{array} { r } { \int _ { 0 } ^ { T } G ( t ) d t = 0 } \end{array}$ . Setting $X = H _ { 0 } ^ { 1 } ( [ 0 , \pi ] )$ ,

$$
I (u) = \int_ {0} ^ {\pi} \left[ \frac {1}{2} (\dot {u} ^ {2} - u ^ {2}) + h u \right] d t, N (u) = \int_ {0} ^ {\pi} G (u (t)) d t,
$$

and

$$
J (u) = I (u) + N (u).
$$

The functionals $I$ and $J$ are not coercive. In fact, letting $u _ { n } ( t ) = n \sin t$ , we have $I ( u _ { n } ) = 0$ , and from $| G ( t ) | \leqslant C$ , a constant, we have $| N ( u _ { n } ) | \leqslant C \pi$ , and then $| J ( u _ { n } ) | \leqslant C \pi$ .

Now, let us decompose $X$ into $H _ { 1 } \oplus \operatorname { s p a n } \left\{ e _ { 1 } \right\}$ , where $e _ { 1 } ~ = ~ \sin t$ , and $H _ { 1 } = e _ { 1 } ^ { \frac { 1 } { 1 } }$ , i.e., $\forall u \in H _ { 0 } ^ { 1 } ( [ 0 , \pi ] )$ one has the orthogonal decomposition:

$$
u = u _ {1} + \alpha e _ {1},
$$

where

$$
\alpha = \int_ {0} ^ {\pi} u (t) \sin t d t.
$$

Then we have:

(1) $I$ is s.w.l.s.c. and coercive on $H _ { 1 }$ . Moreover, from (4.79), $I ( u ) = I ( u _ { 1 } )$ .   
(2) $N$ is bounded and weakly continuous on $X$ , and according to Lemma 4.8.10, $N ( u _ { 1 } + \alpha e _ { 1 } )  0$ as $\alpha \longrightarrow \infty$ , $\forall u _ { 1 } \in H _ { 1 }$ .   
(3) $J$ is bounded from below on $X$ and coercive on $H _ { 1 }$ .

$$
m ^ {*} = \inf  _ {u \in X} J (u) .
$$

Lemma 4.8.11 If $m ^ { * }$ is not a minimum of $J$ , then $\exists \{ u _ { 1 j } \} \subset H _ { 1 }$ and $\alpha _ { j }  \infty$ such that $J ( u _ { 1 j } + \alpha _ { j } e _ { 1 } )  m ^ { * }$ . Moreover, $u _ { 1 j } \to \overline { { u } } _ { 1 }$ in $H _ { 1 }$ , which is a solution of ${ \ddot { x } } + x = h$ , and $m ^ { * } = I ( \overline { { u } } _ { 1 } ) = \operatorname* { i n f } I ( u _ { 1 } ) , u _ { 1 } \in H _ { 1 }$ $u _ { 1 } \in H _ { 1 }$ .

Proof. 1. We choose a minimizing sequence $\{ u _ { j } \}$ of $J$ , and decompose it on $H _ { 1 } \oplus \operatorname { s p a n } \left\{ e _ { 1 } \right\}$ :

$$
u _ {j} = u _ {1 j} + \alpha_ {j} e _ {1}.
$$

We claim that $\alpha _ { j }  \infty$ . Indeed, invoke the coerciveness of $I$ on $H _ { 1 }$ and

$$
I \left(u _ {1 j}\right) = I \left(u _ {j}\right) = J \left(u _ {j}\right) - N \left(u _ {j}\right) <   m ^ {*} + 1 + C \pi ,
$$

$\{ u _ { 1 j } \}$ is bounded in $H _ { 1 }$ . Thus after a subsequence $u _ { 1 j }  u _ { 1 }$ in $H _ { 1 }$ , which implies $u _ { 1 j } \to \overline { { u } } _ { 1 } \mathrm { i n } C ( [ 0 , \pi ] )$ .

If $\{ \alpha _ { j } \}$ is bounded, then after a subsequence $\alpha _ { j }  \overline { { \alpha } }$ , we have

$$
N (u _ {1 j} + \alpha_ {j} e _ {1}) \rightarrow N (\overline {{u}}),
$$

where $\overline { { u } } = \overline { { u } } _ { 1 } + \overline { { \alpha } } e _ { 1 }$ . We have

$$
\liminf_{j\to \infty}I(u_{j}) = \lim_{j\to \infty}I(u_{1j})\geqslant I(\overline{u}_{1}) = I(\overline{u})  ,
$$

and

$$
m ^ {*} = \lim  _ {j \rightarrow \infty} J (u _ {j}) = \lim  _ {j \rightarrow \infty} \left(I (u _ {j}) + N (u _ {j})\right) \geqslant I (\bar {u}) + N (\bar {u}) = J (\bar {u}).
$$

Therefore, $u$ is a minimizer of $J$ . This is a contradiction. The claim is proved. 2. $\forall \alpha \in \mathbb { R } ^ { 1 } , \forall u _ { 1 } \in H _ { 1 }$ , one has

$$
m ^ {*} \leq J (u _ {1} + \alpha e _ {1}) = I (u _ {1}) + N (u _ {1} + \alpha e _ {1}).
$$

Letting $\alpha \longrightarrow \infty$ , provided by Lemma 4.8.10, $N ( u _ { 1 } + \alpha e _ { 1 } )  0$ , thus

$$
m ^ {*} \leqslant I \left(u _ {1}\right) = I \left(\bar {u} _ {1}\right), \quad \forall u _ {1} \in H _ {1}. \tag {4.84}
$$

But to the minimizing sequence $\{ u _ { j } \}$ we have

$$
\begin{array}{l} I (\overline {{u}} _ {1}) \leqslant \lim  _ {j \rightarrow \infty} I (u _ {1 j}) \\ = \lim  _ {j \rightarrow \infty} (J (u _ {j}) - N (u _ {1 j} + \alpha_ {j} e)) \\ = \lim  _ {j \rightarrow \infty} J \left(u _ {j}\right) = m ^ {*}. \tag {4.85} \\ \end{array}
$$

Combining (4.84) and (4.85), we conclude that $m ^ { * } = I ( { \overline { { u } } } _ { 1 } )$ is the minimum of $I$ on $H _ { 1 }$ , and $u _ { 1 }$ is the associated minimizer, which satisfies $I ^ { \prime } ( \overline { { u } } _ { 1 } ) = 0$ , i.e., $\overline { { u } } _ { 1 }$ is a solution of

$$
\left\{ \begin{array}{l} \ddot {x} + x = h  , \\ x (0) = x (\pi) = 0  . \end{array} \right.
$$

□

Lemma 4.8.12 $\forall c > m ^ { * }$ , $J$ satisfies $( P S ) _ { c }$ .

Proof. Assume that $\{ u _ { j } \}$ is a $( P S ) _ { c }$ sequence of $J$ , i.e.,

$$
J (u _ {j}) \rightarrow c \text {a n d} J ^ {\prime} (u _ {j}) \rightarrow \theta \text {i n} X.
$$

From the coerciveness of $I$ on $H _ { 1 }$ , and

$$
I \left(u _ {1 j}\right) = J \left(u _ {j}\right) - N \left(u _ {1 j} + \alpha_ {j} e _ {1}\right) \leqslant c + 1 + c \pi ,
$$

after a subsequence, we have $u _ { 1 j } \to \overline { { u } } _ { 1 }$ in $H _ { 1 }$ .

We are going to verify that after a subsequence $\alpha _ { j }  \overline { { \alpha } }$ . For otherwise, $\alpha _ { j }  \infty$ , we would have $g ( u _ { j } ) \mathrel { \mathop { \ast } } \longrightarrow \theta \mathrm { i n } L ^ { \infty } ( [ 0 , \pi ] )$ , according to Lemma 4.8.10. Noticing that the differential operator $\textstyle { \bigl ( } { \frac { d } { d t } } { \bigr ) } ^ { 2 } + 1$ has a compact inverse $\mathbb { K }$ on $H _ { 1 }$ , from $J ^ { \prime } ( u _ { j } )  \theta$ , we have

$$
u _ {1 j} = \mathbb {K} h + \mathbb {K} P g (u _ {j}) + o (1), \tag {4.86}
$$

where $P$ is the orthogonal projection from $X$ onto $H _ { 1 }$ . Thus $u _ { 1 j }  \mathbb { K } h = \overline { { u } } _ { 1 }$ in $H _ { 1 }$ strongly. It follows from 2 in the proof of Lemma 4.8.11,

$$
\begin{array}{l} c = \lim_{j\to \infty}J(u_{j}) \\ = \lim  _ {j \rightarrow \infty} (I (u _ {1 j}) + N (u _ {1 j} + \alpha_ {j} e _ {1})) \\ = \lim  _ {j \rightarrow \infty} I (u _ {1 j}) \\ = I \left(\bar {u} _ {1}\right) \\ = m ^ {*}. \\ \end{array}
$$

This is a contradiction. Therefore $\alpha _ { j }  \alpha$ . Substituting in (4.86), we obtain a convergent sequence:

$$
u _ {j} = u _ {1 j} + \alpha_ {j} e _ {1} \rightarrow \mathbb {K} h + \mathbb {K} P g (\bar {u} _ {1} + \bar {\alpha} e _ {1}) + \bar {\alpha} e _ {1}.
$$

□

Conclusion. The problem (4.81) has at least one solution.

Proof. If $m ^ { * }$ is attainable at $u ^ { * }$ , then $u ^ { * }$ is a minimizer, which is a solution of the Euler Lagrange equation (4.81) for $J$ .

Otherwise, let

$$
b = \inf  _ {u _ {1} \in H _ {1}} J (u _ {1}) .
$$

Note that $J | _ { H _ { 1 } }$ is s.w.l.s.c. and coercive, $\exists u _ { 1 } ^ { * } \in H _ { 1 }$ such that $b ~ = ~ J ( u _ { 1 } ^ { * } )$ . Therefore $b > m ^ { * }$ . Since

$$
\lim _ {\alpha \to \pm \infty} J (\overline {{u}} _ {1} + \alpha e _ {1}) = I (\overline {{u}} _ {1}) = m ^ {*},
$$

we may choose $a > 0$ large enough such that

$$
J \left(\bar {u} _ {1} \pm a e _ {1}\right) <   b.
$$

Define

$$
\Gamma = \left\{l \in C ([ 0, 1 ], X) | l \left(\frac {1}{2} \pm \frac {1}{2}\right) = \overline {{u}} _ {1} \pm a e _ {1} \right\},
$$

and

$$
c = \inf_{l\in \Gamma}\max_{t\in [0,1]}\max J(l(t))  .
$$

Since for $\forall l \in \Gamma$ , $l \vert 0 , 1 \vert \cap H _ { 1 } \neq \mathcal { O }$ , we have $c \geqslant b > m ^ { * }$ . Thus $( P S ) _ { c }$ holds. According to the mountain pass theorem there is a critical point $u ^ { * }$ with $J ( u ^ { * } ) = c$ . Again $u ^ { * }$ solves (4.81). □

Example 2. We present here an application to the superlinear elliptic boundary problem, see Ambrosetti and Rabinowitz [AR].

Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded domain with smooth boundary. Given a function $f \in C ( \overline { { \Omega } } \times \mathbb { R } ^ { 1 } )$ satisfying the following conditions:

$[ F _ { 1 } ]$ There exist positive constants $C _ { 1 } , C _ { 2 }$ and $\textstyle \alpha \in ( 1 , { \frac { n + 2 } { n - 2 } } )$ (when $n \geq 3$ ) such that

$$
| f (x, t) | \leq C _ {1} + C _ {2} | t | ^ {\alpha}.
$$

$[ F _ { 2 } ] \exists \theta \in ( 0 , \frac { 1 } { 2 } ) , \exists$ a constant $M > 0$ such that

$$
0 <   F (x, t) := \int_ {0} ^ {t} F (x, s) d s \leq \theta t f (x, t), \text {a s} | t | \geq M.
$$

$[ F _ { 3 } ]$

$$
\lim  _ {t \rightarrow 0} \sup  _ {\lambda_ {1}} \frac {f (x , t)}{t} <   \lambda_ {1}, \text {u n i f o r m l y i n} x \in \overline {{\Omega}}.
$$

$\lfloor F _ { 4 } \rfloor$

$$
\lim  _ {t \rightarrow \infty} \inf  _ {\frac {f (x , t)}{t}} > \lambda_ {1}, \quad \text {u n i f o r m l y i n} x \in \overline {{\Omega}}.
$$

where $\lambda _ { 1 }$ is the first eigenvalue of the Laplacian $- \varDelta$ on $\Omega$ with Dirichlet boundary conditions.

Theorem 4.8.13 Under the assumptions $F _ { 1 } , F _ { 2 } , F _ { 3 } , F _ { 4 }$ , the problem

$$
\left\{ \begin{array}{c} - \Delta u = f (x, u), x \in \Omega , \\ u = 0, \quad x \in \partial \Omega , \end{array} \right. \tag {4.87}
$$

has a nontrivial solution.

Proof. We consider the functional on $H _ { 0 } ^ { 1 } ( \Omega )$ :

$$
J (u) = \int_ {\Omega} \left[ \frac {1}{2} | \nabla u | ^ {2} - F (x, u) \right] d x. \tag {4.88}
$$

From the growth condition $F _ { 1 } , J$ is $C ^ { 1 }$ .

Noticing $J ( \theta ) = 0$ , we claim that there is a mountain surrounding $\theta$ and $\exists \alpha > 0 , \exists \rho > 0$ and $u _ { 0 } \notin B _ { \rho } ( \theta )$ with $J ( u _ { 0 } ) = 0$ . Indeed, from $F _ { 3 }$ , we have $\epsilon > 0 , \delta > 0$ such that

$$
\frac {f (x , t)}{t} \leq \lambda_ {1} - \epsilon , \text {a s} 0 <   | t | <   \delta ,
$$

which implies that

$$
F (x, t) \leq \frac {1}{2} (\lambda_ {1} - \epsilon) t ^ {2} \mathrm {a s} | t | <   \delta .
$$

Combining with $F _ { 1 }$ , we have a constant $C _ { 3 }$ such that

$$
F (x, t) \leq \frac {1}{2} (\lambda_ {1} - \epsilon) t ^ {2} + C _ {3} | t | ^ {p},
$$

where $p = \alpha + 1$ , thus

$$
\int_ {\Omega} F (x, u (x)) d x \leq \frac {1}{2} \left(1 - \frac {\epsilon}{\lambda_ {1}}\right) \| u \| _ {H ^ {1}} ^ {2} + C _ {4} \| u \| _ {L ^ {p}} ^ {p}.
$$

This proves that

$$
J (u) \geq \frac {\epsilon}{2 \lambda_ {1}} \| u \| _ {H ^ {1}} ^ {2} - C _ {4} \| u \| _ {L ^ {p}} ^ {p}.
$$

Therefore $\exists \alpha > 0 , \exists \rho > 0$ such that $J | _ { \partial B _ { \rho } ( \theta ) } \geq \alpha$ .

Now, we take $u _ { 0 } = s \phi _ { 1 }$ , where $\phi _ { 1 } > 0$ is the normalized first eigenfunction, and $s > 0$ is to be determined. Setting

$$
g (s) = J (s \phi_ {1}) = \frac {\lambda_ {1}}{2} s ^ {2} - \int_ {\Omega} F (x, s \phi_ {1} (x)) d x,
$$

From $F _ { 4 }$ , $g ( s ) \to - \infty$ as $s \to + \infty$ . Therefore there exists $s _ { 0 } > 0$ satisfying $g ( s _ { 0 } ) = 0$ , set $s = s _ { 0 } , u _ { 0 }$ is as required.

It remains to verify the Palais-Smale condition. Suppose that a sequence $\{ u _ { j } \} \subset H _ { 0 } ^ { 1 } ( \Omega )$ satisfies $J ( u _ { j } )  c , J ^ { \prime } ( u _ { j } )  \theta$ ; we shall show that it possesses a convergent subsequence. Firstly, $\{ u _ { j } \}$ is bounded. In fact, $\exists C _ { 3 } > 0$ such that

$$
- C _ {3} \leq \frac {1}{2} \| u _ {j} \| _ {H ^ {1}} ^ {2} - \int_ {\Omega} F (x, u _ {j} (x)) d x \leq C _ {3}.
$$

According to $F _ { 2 }$

$$
- \theta \int_ {\Omega} u _ {j} (x) f (x, u _ {j} (x)) d x \leq - \int_ {\Omega} F (x, u _ {j} (x)) d x + C _ {4}.
$$

it follows that

$$
\left(\frac {1}{2} - \theta\right) \| u _ {j} \| _ {H ^ {1}} ^ {2} + \theta J ^ {\prime} (u _ {j}) (u _ {j}) \leq C _ {5}.
$$

Since $J ^ { \prime } ( u _ { j } )  \theta$ , this proves that $\{ u _ { j } \}$ is bounded in $H _ { 0 } ^ { 1 } ( \Omega )$ .

Next, we show the existence of a convergent subsequence. Noticing that $\textstyle p < { \frac { 2 n } { n - 2 } }$ , from the Sobolev embedding theorem $H ^ { 1 } ( \Omega ) \hookrightarrow L ^ { p } ( \Omega )$ is bounded, −so that $\| u _ { j } \| _ { L ^ { p } }$ is bounded, and then after a subsequence, $\{ f ( x , u _ { j } ( x ) ) \}$ i s weakly convergent in $\begin{array} { r } { L ^ { p ^ { \prime } } ( \Omega ) , \frac { 1 } { p } + \frac { 1 } { p ^ { \prime } } = 1 } \end{array}$ . However, $( - \varDelta ) ^ { - 1 } : L ^ { p ^ { \prime } } ( \Omega ) \to H _ { 0 } ^ { 1 } ( \Omega )$ is compact. Therefore, $( - \varDelta ) ^ { - 1 } f ( \cdot , u _ { j } )$ is strongly convergent in $H _ { 0 } ^ { 1 } ( \Omega )$ . Since

$$
J ^ {\prime} (u _ {j}) = u _ {j} - (- \Delta) ^ {- 1} f (\cdot , u _ {j}) \rightarrow \theta ,
$$

$u _ { j }$ converges strongly.

It is interesting to compare the assumptions $F _ { 1 }$ to $F _ { 4 }$ with those in Sect. 3.6, Example 4. They are very similar. But the differences lie as follows: The partial differential operator in the cited example is not necessarily a divergent form, in which case the variational method cannot be applied; while in the above example, the solution is not necessarily positive. Moreover, one may even change the assumption $F _ { 3 }$ (see below); in these cases the a priori estimate, which the degree theoretic argument relies on, either fails or holds under other assumptions.

Example 3. (Ni [Ni]) We make the conditions

$[ F _ { 3 } ^ { \prime } ]$

$$
f (x, t) = o (t) \text {a s} t \rightarrow 0 \text {u n i f o r m l y i n} x \in \overline {{\Omega}}.
$$

$[ F _ { 4 } ^ { \prime } ]$

$$
t f (x, t) \geq 0, \forall (x, t) \in \overline {{\Omega}} \times \mathbb {R} ^ {1}.
$$

Theorem 4.8.14 Under the assumptions $F _ { 1 } , F _ { 2 } , F _ { 3 } ^ { \prime } , F _ { 4 } ^ { \prime }$ , assume $a \in C ( { \overline { { \Omega } } } )$ i s positive. Then the following equation possesses a nontrivial solution:

$$
\left\{ \begin{array}{c c} - \Delta u (x) = a (x) u (x) + f (x, u (x)), & x \in \Omega , \\ u (x) = 0. & x \in \partial \Omega . \end{array} \right. \tag {4.89}
$$

Proof. We consider the functional on $H _ { 0 } ^ { 1 } ( \Omega )$ :

$$
J (u) = \int_ {\Omega} \left\{\frac {1}{2} [ | \nabla u | ^ {2} - a u ^ {2} ] - F (x, u) \right\} d x,
$$

For which the Euler–Lagrange equation is (4.89). In the same manner we verify the Palais–Smale condition. We claim that there are linking sets, which separate values of $J$ .

Indeed, let $0 < \lambda _ { 1 } < \lambda _ { 2 } \leq \cdots \leq \lambda _ { k } \leq 1 < \lambda _ { k + 1 } \leq \cdots$ be the eigenvalues and $\{ \phi _ { 1 } , \ldots , \phi _ { k } , \phi _ { k + 1 } , \ldots \}$ be the eigenfunctions of the eigenvalue problem:

$$
- \Delta u = \lambda a \cdot u, u \in H _ {0} ^ {1} (\Omega).
$$

Taking $E _ { k } \ = \ \operatorname { s p a n } \{ \phi _ { 1 } , . . . , \phi _ { k } \}$ , we have the orthogonal decomposition: $H _ { 0 } ^ { 1 } ( \Omega ) = E _ { k } \oplus \hat { E }$ .

O n $\hat { E }$ , we have

$$
\int_ {\Omega} | \nabla u | ^ {2} d x \geq \lambda_ {k + 1} \int_ {\Omega} a u ^ {2} d x,
$$

therefore

$$
J (u) \geq \int_ {\Omega} \left\{\frac {1}{2} \left(1 - \frac {1}{\lambda_ {k + 1}}\right) | \nabla u | ^ {2} - F (x, u) \right\} d x.
$$

From $F _ { 3 } ^ { \prime }$

$$
\int_ {\Omega} F (x, u (x)) d x = o \big (\| u \| ^ {2} \big) \mathrm {a s} \| u \| \to 0.
$$

Therefore there exist $\rho > 0 , \alpha > 0$ such that

$$
J (u) \geq \alpha \text {a s} u \in \partial B _ {\rho} (\theta) \cap \hat {E}.
$$

From $F _ { 4 } ^ { \prime } , F ( x , t ) \geq 0 \ \forall ( x , t ) \in \overline { { \Omega } } \times \mathbb { R } ^ { 1 }$ , it implies that

$$
J (u) \leq \frac {1}{2} \int_ {\Omega} [ | \nabla u | ^ {2} - a (x) u ^ {2} ] d x \leq 0, \forall u \in E _ {k}.
$$

From $F _ { 2 }$ , we have

$$
F (x, t) \geq C _ {1} | t | ^ {\frac {1}{\theta}} - C _ {2},
$$

where $C _ { 1 } , C _ { 2 }$ are positive constants. Since $\theta < \textstyle { \frac { 1 } { 2 } }$ ,

$$
J (u) \leq (\lambda_ {k + 1} - 1) \int_ {\Omega} a u ^ {2} d x - C _ {1} \int_ {\Omega} | u | ^ {\frac {1}{\theta}} d x + C _ {2} | \Omega | \rightarrow - \infty ,
$$

as $u \in E _ { k + 1 } : = \operatorname { s p a n } \{ \phi _ { 1 } , \dots , \phi _ { k } , \phi _ { k + 1 } \}$ and $\| u \| _ { H ^ { 1 } } \to \infty$ . There exists $R > \rho$ such that

$$
J (u) \leq 0 \text {a s} u \in \partial B _ {R} (\theta) \cap E _ {k + 1}.
$$

Setting $S = \partial B _ { \rho } ( \theta ) \cap \hat { E }$ and $\partial Q = ( B _ { R } ( \theta ) \cap E _ { k } ) \cup ( \partial B _ { R } ( \theta ) \cap \{ ( x _ { 1 } , \dots ,$ $x _ { k + 1 } ) \in E _ { k + 1 } | x _ { k + 1 } \geq 0 \}$ ). Then $S$ and $\partial Q$ link, and they separate values of $J$ .

Theorem 4.8.9 yields the existence of a critical value $c \geq \alpha > 0$ , which implies the existence of a nontrivial solution of equation (4.89). □

# Topological and Variational Methods

This chapter is devoted to the critical point theory and Conley index theory.

A typical problem from geometry in the calculus of variations is to find geodesics between two points $q _ { 0 } , q _ { 1 }$ on a given Riemannian manifold $( M , g )$ . A path on $M$ connecting $q _ { 0 }$ and $q _ { 1 }$ is denoted by $\gamma : \lfloor 0 , 1 \rfloor \to M$ with $\gamma ( i ) =$ $q _ { i } , i = 0 , 1$ . Let

$$
N = \left\{\gamma \in C ^ {1 - 0} ([ 0, 1 ], M) \mid \gamma (i) = q _ {i}, i = 0, 1 \right\},
$$

and define the energy functional:

$$
I (\gamma) = \int_ {0} ^ {1} \left| \left| \frac {d \gamma}{d t} \right| \right| ^ {2} d t,
$$

where $\| \cdot \| ^ { 2 }$ is the scalar product induced by the metric $g$ , or in the local coordinates:

$$
\left| \left| \frac {d \gamma}{d t} \right| \right| ^ {2} = \Sigma_ {i, j} g _ {i j} (\gamma (t)) \frac {d \gamma}{d t} \frac {d \gamma^ {j}}{d t},
$$

and $\gamma = ( \gamma ^ { 1 } , \dots , \gamma ^ { p } ) , p = \dim \mathrm { { M } }$ . Then the Euler–Lagrange equation reads as the geodesic equation:

$$
\left(\frac {d}{d t}\right) ^ {2} \gamma^ {i} = \Sigma_ {j, k} \Gamma_ {j k} ^ {i} (\gamma) \frac {d \gamma^ {j}}{d t} \frac {d \gamma^ {k}}{d t}, i = 1, 2, \ldots , p.
$$

Geometrically, a curve satisfying the geodesic equation has the feature that tangents along it are parallel. Generally speaking, there are many geodesics joining $q _ { 0 }$ and $q _ { 1 }$ , which are critical points of the energy functional. However, besides those minimal in length, how do we reach the others?

Influenced by the pioneering work of Birkhoff (1917) [Bi] on closed geodesics, two global methods have appeared: The minimax method and the Morse theory. Both are based on topological arguments.

The minimax method, as an outgrowth of the max-min characterization of the eigenvalues of Laplacian with Dirichlet data, was successfully developed

by Ljusternik and Schnirelmann (1934)[LjS 1]. The method provided a proof of the existence of at least three closed geodesics on a closed surface with genus zero, see Ljusternik and Schnirelmann [Lj, LjS 1], Ballmann [Bm], Klinenberg [Kl].

In the 1950s Krasnosel’ski [Kr 1] introduced the notion of genus, which is a topological index related to the group $\mathbf { Z } _ { 2 }$ , in the study of the nonlinear eigenvalue problem for a class of integral equations.

An important step towards the recent development of critical point theory is due to Palais (1966) [Pa 1, Pa 2]. Two major contributions are: (1) The Ljusternik Schnirelmann theory was extended to infinite-dimensional manifolds. (2) Homotopically stable families were used in the minimax principle.

The reviva of the study of the minimax method began with the works of Ambrosetti and Rabinowitz (1974)[AR] and Rabinowitz [Ra 3]. The first paper deals with functionals unbounded from below, and provides many applications in the study of nonlinear elliptic equations, Hamiltonian systems, and problems from geometry and mathematical physics. The second paper provides a proof of the existence of a closed orbit of an autonomous Hamiltonian system on a star-shaped hypersurface in $R ^ { 2 n }$ (independently, see Weinstein [Wei 1]). This is the first global result on this problem and is a breakthrough to the Weinstein conjecture.

In parallell, in the 1930s, Morse developed a theory which reveals the relationship between the critical points of a nondegenerate function and the topology of the underlying compact manifold. Although the nondegeneracy condition and the compactness assumption in his theory do not meet in most variational problems, he succeeded in applying his theory in the study of closed geodesics and unstable minimal surfaces, see Morse [Mo], Milnor [Mi 1], Bott[Bo 1], Morse and Tompkins [MT], Schiffman [Sch]. Moreover, Morse theory became a basic tool in computing the homology of compact manifolds. The work of Smale on the solution of the Poincar´e conjecture for $n \geq 5$ pushes the development of the theory to a new peak, see Smale [Sm 1], Milnor [Mi 3].

In the 1950s and 1960s Rothe [Ro 1–3], Palais [Pa 3], and Smale [Sm 2] extended the Morse theory to infinite-dimensional manifolds by using the Palais–Smale condition. Later, Marino and Prodi [MP 1, MP 2] and Gromoll and Meyer [GM] endeavored in weakening the restriction of the nondegeneracy. In applying Morse theory to differential equations, most functionals are unbounded from below, and the nondegenerate assumption should be removed; to this end, critical groups for isolated critical points are introduced and the relative homology is involved, by which the interconnection with the minimax principle has been revealed, see K. C. Chang [Ch 4,5,7].

Conley extended the Morse theory to flows on a compact space without a variational structure. The Conley index for isolated invariant sets with respect to the flow is a homotopy invariant. A Morse decomposition is also extended. With the aid of Conley’s theory, Floer established his homology theory, which is an important tool in the symplectic geometry.

In this chapter we shall introduce these theories as tools in the study of multiple solutions for differential equations.

Morse theory is introduced in Sect. 5.1, we shall present it in a way comparing with the Leray–Schauder degree. After the basic theorems, we shall focus on the computation of critical groups for isolated critical points, in particular, those obtained by various minimax methods.

Section 5.2 is on the minimax principles, which include the Ljusternik– Schnirelmann category theory, cap length estimates, Krasnosel’ski’s genus and other index theories. Various extensions can be found in Rabinowitz [Ra 4], [Ra 5]. The central result is the multiplicity theorem.

Section 5.3 is deals with an application to the Weinstein conjecture, see Viterbo [Vi 1].

We return to the prescribed Gauss curvature problem on $S ^ { 2 }$ in Sect. 5.4. We shall apply the Morse theory with boundary conditions to attack the problem.

We introduce the definitions of the isolated invariant set, the index pair, and the Conley index in metric space without compactness. The fundamental properties of the Conley index, in particular, the homotopy invariance and the Morse decomposition are studied. Examples and the relationship with the Morse theory are also presented.

# 5.1 Morse Theory

# 5.1.1 Introduction

Morse revealed a deep relation between the critical points of any nondegenerate function and the topology of the underlying compact manifold $M$ .

Let $f \in C ^ { 1 } ( M )$ . $\forall a \ \in \ R ^ { 1 }$ , the set $f _ { a } = \{ x \in M | f ( x ) \leq a \}$ is called a level set. We denote the set of all critical points by $K$ . Here the terminology “critical point” is coincident with that for general differential mappings, which was introduced in Sect. 1.3.5, i.e., $K = \{ x \in M \vert f ^ { \prime } ( x ) = \theta \}$ . A real value $\{ c \}$ is called critical if $f ^ { - 1 } ( c ) \cap K \neq \emptyset$ ; otherwise, it is called regular. According to the Sard–Smale theorem, the set of critical values is of the first category.

A $C ^ { 2 }$ function $f$ is called nondegenerate if it has only nondegenerate critical points. A critical point $p$ is called nondegenerate if the Hessian $f ^ { \prime \prime } ( p )$ at this point has a bounded inverse (“boundedness” is assumed if $M$ is an infinite-dimensional Hilbert–Riemannian manifold). As an application of the Sard–Smale theorem, the set of nondegenerate functions is dense in $C ^ { 2 } ( M )$ . To a nondegenerate critical point $p$ , we call the dimension of the subspace of negative eigenvectors of $f ^ { \prime \prime } ( p )$ the Morse index, and denote it by $\operatorname { i n d } ( f ^ { \prime \prime } ( p ) )$ .

The basic idea in the Morse theory is to relate the local behavior of a nondegenerate critical point with the variations of the topological structures of the level sets $f _ { a }$ . More precisely, if $f ^ { - 1 } \cap K = \{ p \}$ and $\operatorname { i n d } ( f ^ { \prime \prime } ( p ) ) = j$ , then

$$
f _ {c + \epsilon} \sim f _ {c - \epsilon} \cup B ^ {j},
$$

where $B ^ { j }$ is the $j$ -ball, for $\epsilon > 0$ small, “ $\cup$ ” means “attached by” and $\sim$ means homotopical equivalence.

Since $M$ is compact, the nondegenerate function $f$ can only have finitely many critical points $\{ p _ { 1 } , \ldots , p _ { l } \}$ . Then the space $M$ is homotopically equivalent to the space by gluing finitely many balls with dimensions $\{ { \mathrm { i n d } } ( f ^ { \prime \prime } ( p _ { i } ) ) | i = 1 , \ldots , l \}$ according to their levels.

The above consideration is based on the following Morse lemma. We assign a Riemannian structure $M$ .

Lemma 5.1.1 (Morse lemma) Suppose that $f \in C ^ { 2 } ( M , R ^ { 1 } )$ and that $p$ is $a$ nondegenerate critical point; then there exists a neighborhood $U$ of $p$ and a local diffeomorphism $\Phi : U \to T _ { p } ( M )$ with $\Phi ( p ) = \theta$ , such that

$$
f \circ \Phi^ {- 1} (\xi) = f (p) + \frac {1}{2} (f ^ {\prime \prime} (p) \xi , \xi) _ {p} \quad \forall \xi \in \Phi (U),
$$

where $( , ) _ { p }$ is the Riemannian structure at $p$ .

From the homology point of view, for a pair of topological spaces $( A , B )$ with $B \subset A$ , let $H _ { q } ( A , B , G )$ be the qth relative homology group with coefficient group $G$ , $q = 0 , 1 , 2 , \ldots .$ They are homotopically invariant.

According to the Morse lemma, after a linear homeomorphism, roughly speaking, we may assume that $f$ is a quadratic function in a neighborhood $B _ { \epsilon } ( \theta )$ of $\theta$ in $T _ { p } ( M )$ of the form:

$$
f (x) = c + \frac {1}{2} \left(\left\| x _ {+} \right\| ^ {2} - \left\| x _ {-} \right\| ^ {2}\right),
$$

where $x = x _ { + } + x _ { - } \in H _ { \pm }$ , $\mathrm { d i m } H _ { - } = j$ , and ${ \cal R } ^ { n } = { \cal H } _ { + } \oplus { \cal H } _ { - }$ is an orthogonal decomposition, where $n = \dim M$ .

To characterize homologically the nondegenerate critical point $p$ with Morse index $j$ , we have

$$
H _ {q} (f _ {c + \epsilon}, f _ {c - \epsilon}, G) = H _ {q} (B ^ {j}, G) = \delta_ {q j} G \quad \forall q \in \mathbb {N}.
$$

Since $p$ satisfies $f ^ { \prime } ( p ) = \theta$ , it is a nondegenerate zero of the mapping $f ^ { \prime } : M \longrightarrow T ^ { * } ( M )$ , the index of the Brouwer degree at $p$ is well defined and

$$
i (f ^ {\prime}, p) = \operatorname {s g n} \det {(f ^ {\prime \prime} (p))} = (- 1) ^ {j} = \sum_ {q = 0} ^ {n} (- 1) ^ {q} \mathrm {r a n k} H _ {q} (B ^ {j}, G).
$$

If further, $p$ is the only critical point in $f ^ { - 1 } [ c - \epsilon , c + \epsilon ]$ , then as a local characterization of $f$ at $p$ , $H _ { q } ( f _ { c + \epsilon } , f _ { c - \epsilon } , G ) , \ q = 0 , 1 , \ldots , n$ , is better than the Brouwer index.

We are encouraged by using the homotopy invariance $\{ H _ { q } ( f _ { b } , f _ { a } ; G ) \} _ { q = 0 } ^ { n }$ (for $a < b$ ) as a replacement for $\deg ( f ^ { \prime } , f ^ { - 1 } ( a , b ) , \theta )$ .

Unfortunately there is no additivity (but only the subadditivity, see later). In the degree theory, the excision property and then the Kronecker existence,

which makes it useful in the study of fixed points, are based on the additivity; while for relative homology groups, the excision property is related to a deformation argument. In this context, we shall establish the counterpart of the Kronecker existence theorem by deformation as follows:

Theorem 5.1.2 (Nontrivial interval theorem) If $\exists q \in \mathbb { N }$ and $\exists a \ < \ b$ such that $H _ { q } ( f _ { b } , f _ { a } ; G )$ is nontrivial, then $K \cap f ^ { - 1 } [ a , b ] \neq \emptyset$ .

One proves the theorem by contradiction, i.e., if there is no critical value in the interval $[ a , b ]$ , then $f _ { b } \sim f _ { a }$ . In fact, a homotopy between $f _ { b }$ and $f _ { a }$ can be easily constructed by the negative gradient flow:

$$
\dot {x} (t) = - f (x (t)) \quad \forall x \in f ^ {- 1} [ a, b ].
$$

Noticing that along the flow line $f$ is strictly decreasing and that $f ^ { - 1 } [ a , b ]$ is compact, the proof is left to readers as an exercise.

We shall present Morse theory in this book as a topological tool in the study of the existence and the multiplicity of solutions of certain nonlinear differential equations with variational structures.

To our purpose the two assumptions: the compactness of manifolds and the nondegeneracy of functions, are too restricted. In dealing with infinitedimensional manifolds, a certain kind of compactness is assumed on the function $f$ , e.g., the Palais–Smale condition. Under this condition, deformation theorems are derived, which implies the noncritical interval theorem. Let us use singular relative homology groups with an Abelian coefficient group $G$ , $H _ { * } ( X , Y ; G )$ to describe the topological difference between the topological spaces $X$ and $Y$ , with $Y \subset X$ . Namely, if $H _ { * } ( f _ { b } , f _ { a } ; G )$ is not trivial, then $K \cap f ^ { - 1 } [ a , b ] \neq \emptyset$ . This is the main result in Sect. 5.1.2.

Instead of the nondegeneracy, we study a certain kind of isolatedness of critical points. A series of critical groups is introduced to replace the Morse index in describing the local behavior of isolated critical points. The basic properties and computations of critical groups are studied, e.g., the shifting theorem and that for mountain pass points. The local theory is described in Sect. 5.1.3.

The Morse relation, which is the subadditivity of topological invariances $H _ { q } ( f _ { b } , f _ { a } ; G ) , q = 0 , 1 , 2 , . . .$ , links, on one hand, the global invariants: $H _ { * } ( f _ { b } , f _ { a } ; G ) , * = 0 , 1 , . . .$ , and on the other hand, the local invariants: the critical groups for isolated critical points in $f ^ { - 1 } [ a , b ]$ . It is applied to the estimation of the critical groups for isolated critical points obtained by minimax principles. It is also used to set up the relationship between the Leray– Schauder index and the critical groups for an isolated zero of a potential compact vector field, by which, we see that for a potential compact vector field, critical groups provide more information than the Leray–Schauder index. This is the contents of Sect. 5.1.4.

# 5.1.2 Deformation Theorem

The following terminologies are used:

Let $X$ be a topological space. A deformation of $X$ is a continuous map $\eta : X \times [ 0 , 1 ] \to X$ such that $\eta ( \cdot , 0 ) = \mathrm { i d }$ .

Definition 5.1.3 (Deformation retract) Given a topological pair $Y \subset X$ . A continuous map $r : X \to Y$ is called a deformation retract, if $r \circ i = i d _ { Y }$ and $i \circ r \sim i d _ { X }$ , where $i : Y \to X$ is the injection. In this case $Y$ is called $a$ deformation retraction of $X$ .

Definition 5.1.4 (Strong deformation retract) A deformation retract $r$ is called a strong deformation retract, if there exists a deformation $\eta : X \times$ $[ 0 , 1 ] \to X$ , such that $\eta ( \cdot , t ) \vert _ { Y } = i d _ { Y } \ \forall t \in [ 0 , 1 ]$ and $\eta ( \cdot , 1 ) = i \circ r$ . Then $Y$ is called a strong deformation retraction of $X$ .

According to the homotopy invariance of the relative singular homology group $H _ { * } ( X , Y ; G )$ (where $G$ is the coefficient group), if $Y$ is a strong deformation retraction of $X$ , then $H _ { * } ( X , Y ; G ) = 0$ .

In the following, $M$ is assumed to be a Banach–Finsler manifold with a Finsler structure $\| \cdot \|$ . But for pedagogical reasons, all of the following theorems are proved only on Banach spaces. Readers who are familiar with the background material on infinite-dimensional manifolds may complete the proofs themselves or may refer to [Ch 1].

As we have seen in Sect. 5.1.1, the deformation is constructed by negative gradient flow on Riemannian manifolds. However, if the $C ^ { 1 }$ function $f$ is defined on a Banach space $M$ , $f ^ { \prime } ( x ) \in M ^ { * }$ , the dual space of $M$ , then the gradient flow does not make sense. We introduce:

Definition 5.1.5 (Pseudo-gradient vector field) Let $M$ be a Banach–Finsler manifold and $f \in C ^ { 1 } ( M , R ^ { 1 } )$ . Let $K$ be the critical set of $f$ . A vector field $X$ over $M$ is called a pseudo-gradient if

(1) $\parallel X ( p ) \parallel < 2 | f ^ { \prime } ( p ) |$ and   
(2) $\langle f ^ { \prime } ( p ) , X ( p ) \rangle > | f ^ { \prime } ( p ) | ^ { 2 }$ , ∀p ∈ M1 := M \K,

where $\langle , \rangle$ is the duality between $T ^ { * } ( M )$ and $T ( M )$ , $\| \cdot \|$ and $| \cdot |$ stand for the Finsler structures on $T ( M )$ and $T ^ { * } ( M )$ , respectively.

Our main results of this section are the following:

Theorem 5.1.6 (Noncritical interval theorem) If $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfies $( P S ) _ { c } ~ \forall c \in [ a , b ]$ and if $K \cap f ^ { - 1 } [ a , b ] = \emptyset$ , then $f _ { a }$ is a strong deformation retraction of $f _ { b }$ .

As a direct consequence, Theorem 5.1.2 is extended to Banach–Finsler manifolds for $C ^ { 1 }$ functions satisfying the $( P S ) _ { c }$ condition $\forall c \in [ a , b ]$ .

Theorem 5.1.7 (Second deformation theorem) If $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfies $( P S ) _ { c } ~ \forall c \in [ a , b ]$ and $M$ is $C ^ { 2 }$ . If $K \cap f ^ { - 1 } ( a , b ] = \emptyset$ and the connected components of $K \cap f ^ { - 1 } ( a )$ are only isolated points, then $f _ { a }$ is a strong deformation retraction of $f _ { b }$ .

We need the following lemmas:

Lemma 5.1.8 (The existence of a pseudo-gradient vector field) $\forall f \in$ $C ^ { 1 } ( M , R ^ { 1 } )$ , there exists a continuous pseudo-gradient vector field of $f$ on ${ \widetilde { M } } = M \backslash K$ .

Proof. $\forall p _ { 0 } \in \widetilde { M }$ , $\exists \xi ( p _ { 0 } ) \in T _ { p _ { 0 } } ( M )$ such that $\parallel \xi ( p _ { 0 } ) \parallel = 1$ and $\langle f ^ { \prime } ( p _ { 0 } ) , \xi ( p _ { 0 } ) \rangle >$ $\frac { 2 } { 3 } | f ^ { \prime } ( p _ { 0 } ) |$ . Setting $X _ { p _ { 0 } } = { \textstyle \frac { 3 } { 2 } } | f ^ { \prime } ( p _ { 0 } ) | \xi ( p _ { 0 } )$ , we have

$$
\| X _ {p _ {0}} \| <   2 \left| f ^ {\prime} \left(p _ {0}\right) \right|, a n d
$$

$$
\langle f ^ {\prime} (p _ {0}), X _ {p _ {0}} \rangle > | f ^ {\prime} (p) | ^ {2} \quad \forall p \in V _ {p _ {0}}.
$$

Since $\widetilde { M }$ is metrizable, it is paracompact. There is a locally finite partition of unity $\{ \eta _ { \beta } | \beta \in B \}$ , with supp $\eta _ { \beta } \subset V _ { p _ { 0 } }$ for some $p _ { 0 } = p _ { 0 } ( \beta ) \in \tilde { M }$ . Let

$$
X (p) = \sum_ {\beta \in B} \eta_ {\beta} (p) X _ {p _ {0} (\beta)}.
$$

This is a required pseudo-gradient vector field.

Lemma 5.1.9 If $f$ satisfies $( P S ) _ { c } ~ \forall c \in [ a , b ]$ , and if $K \cap f ^ { - 1 } [ a , b ] = \emptyset$ , then $\exists \epsilon , \delta > 0$ such that

$$
\left| f ^ {\prime} (x) \right| \geq \epsilon \quad \forall x \in f ^ {- 1} [ a - \delta , b + \delta ].
$$

Proof. If not, then there exists a sequence $\{ x _ { n } \} \subset f ^ { - 1 } [ a - { \textstyle { \frac { 1 } { n } } } , b + { \textstyle { \frac { 1 } { n } } } ]$ such that $\begin{array} { r } { | f ^ { \prime } ( x _ { n } ) | < \frac { 1 } { n } } \end{array}$ . We find a subsequence $\{ x _ { n } ^ { \prime } \}$ such that $f ( x _ { n } ^ { \prime } )  c \in [ a , b ]$ and $f ^ { \prime } ( x _ { n } ^ { \prime } )  \theta$ . According to (PS) $c$ , we find $x ^ { * } \in f ^ { - 1 } ( c ) \cap K$ . This is a contradiction. □

Proof. (Proof of the nontrivial interval theorem) The deformation is constructed by the flow, which deforms the level sets:

$$
\left\{ \begin{array}{l} \dot {\sigma} (t) = - X (\sigma (t)) \\ \sigma (0) = x _ {0} \in f ^ {- 1} [ a, b ]  . \end{array} \right.
$$

1. It is a decreasing flow:

$$
\frac {d}{d t} f (\sigma (t)) = \langle f ^ {\prime} (\sigma (t)), \dot {\sigma} (t) \rangle \leq - | f ^ {\prime} (\sigma (t)) | ^ {2}.
$$

2. The flow does not stop until it arrives at $f _ { a }$ . Indeed, if the maximal existence time for the initial data $x _ { 0 }$ is $T _ { x _ { 0 } }$ and $f ( \sigma ( T _ { x _ { 0 } } ) - 0 ) > a$ , from Lemma 5.1.9:

$$
a - b \leq f (\sigma (t)) - f (x _ {0}) = \int_ {0} ^ {t} \langle f ^ {\prime} (\sigma (\iota)), \dot {\sigma} (\iota) \rangle d \iota <   - \epsilon^ {2} t \forall t <   T _ {x _ {0}},
$$

then $\begin{array} { r } { T _ { x _ { 0 } } < \frac { b - a } { \epsilon ^ { 2 } } } \end{array}$

$$
\begin{array}{l} d \left(\sigma \left(t _ {2}\right), \sigma \left(t _ {1}\right)\right) \leq \int_ {t _ {1}} ^ {t _ {2}} \| \dot {\sigma} (t) \| d t \leq 2 \int_ {t _ {1}} ^ {t _ {2}} \left| f ^ {\prime} (\sigma (t)) \right| d t \\ \leq 2 \sqrt {(b - a) (t _ {2} - t _ {1})}, \forall t _ {1}, t _ {2} \in \left(0, T _ {x _ {0}}\right). \\ \end{array}
$$

It implies that $\sigma ( T _ { x _ { 0 } } )$ exists and is not a critical point. Therefore the flow is extendible beyond $T _ { x _ { 0 } }$ . It contradicts the maximality of $T _ { x _ { 0 } }$ .

3. The arrival time $T _ { x }$ is a continuous function of $x$ . Indeed, $t = T _ { x }$ solves the equation $f ( \sigma ( t ) ) = a$ . Since

$$
\frac {d}{d t} f (\sigma (t)) | _ {t = T _ {x}} = \langle f ^ {\prime} (\sigma (T _ {x})), \dot {\sigma} (T _ {x}) \rangle <   - \epsilon^ {2},
$$

the continuity follows from the implicit function theorem.

4. Define $\eta : f _ { b } \times \lfloor 0 , 1 \rfloor \to f _ { b }$ by

$$
\eta (x, t) = \left\{ \begin{array}{l l} \sigma (x, T _ {x} t) & x \in f ^ {- 1} [ a, b ] \\ x & x \in f _ {a}  , \end{array} \right.
$$

where $\sigma ( x , t )$ denotes the flow with initial data $x$ .

This is a strong deformation retract.

Proof. (Proof of the second deformation theorem) We define the pseudogradient flow as before:

$$
\left\{ \begin{array}{l} \dot {\sigma} (t, x) = - \frac {X (\sigma (t , x))}{\| X (\sigma (t , x)) \| ^ {2}} \\ \sigma (0, x) = x \in f ^ {- 1} [ a, b ] \backslash K _ {b}  . \end{array} \right.
$$

In the same manner, we show that $\forall x \in f ^ { - 1 } ( a , b ] \backslash K _ { b }$ , there exists the arrival time $T _ { x } > 0$ such that lim f  σ(t, x) = a. $\operatorname* { l i m } _ { t \to T _ { x } - 0 } f \circ \sigma ( t , x ) = a$

1. We claim that lim σ(t, x) does exist and then $\operatorname* { l i m } _ { t \to T _ { x } - 0 } \sigma ( t , x )$

$$
f \left(\sigma \left(T _ {x} - 0, x\right)\right) = a.
$$

Indeed, (PS)a implies that $K _ { a }$ is compact. Either one of the following cases holds:

$$
\begin{array}{l} \left(a\right) \inf  _ {t \in [ 0, T _ {x})} \operatorname {d i s t} (\sigma (t, x), K _ {a}) > 0  , \\ \left(\mathrm {b}\right) \inf  _ {t \in [ 0, T _ {x})} \operatorname {d i s t} (\sigma (t, x), K _ {a}) = 0. \\ \end{array}
$$

In case (a), again by $( \mathrm { P S } ) _ { c } ~ \forall c \in [ a , b ] , ~ \exists \alpha > 0$ such that

$$
\inf  _ {t \in [ 0, T _ {x})} \| f ^ {\prime} (\sigma (t, x)) \| \geq \alpha .
$$

Thus

$$
\begin{array}{l} \operatorname {d i s t} \left(\sigma \left(t _ {2}, x\right), \sigma \left(t _ {1}, x\right)\right) \leq \int_ {t _ {1}} ^ {t _ {2}} \left| \left| \frac {d \sigma}{d t} \right| \right| d t \\ \leq \int_ {t _ {1}} ^ {t _ {2}} \frac {d t}{\| X (\sigma (t , x)) \|} \leq \int_ {t _ {1}} ^ {t _ {2}} \frac {d t}{\| f ^ {\prime} (\sigma (t , x)) \|} \leq \frac {\left| t _ {2} - t _ {1} \right|}{\alpha}. \\ \end{array}
$$

Since $T _ { x }$ is finite, lim σ(t, x) = z. $\operatorname* { l i m } _ { t \to T _ { x } - 0 } \sigma ( t , x ) = z$

In case (b), we shall prove that $\exists z \in K _ { a }$ such that

$$
\lim  _ {t \to T _ {x} - 0} \sigma (t, x) = z.
$$

First, we claim that

$$
\lim  _ {t \rightarrow T _ {x} - 0} \operatorname {d i s t} (\sigma (t, x), K _ {a}) = 0. \tag {5.1}
$$

If not, $\exists \epsilon _ { 0 } > 0 , \exists t _ { i } \to T _ { x } - 0$ , such that

$$
\operatorname {d i s t} \left(\sigma \left(t _ {i}, x\right), K _ {a}\right) \geq \epsilon_ {0}.
$$

By the assumption (b), $\exists t _ { i } ^ { \prime } \to T _ { x } - 0$ such that

$$
\lim  _ {i \to \infty} \operatorname {d i s t} (\sigma (t _ {i} ^ {\prime}, x), K _ {a}) = 0.
$$

Thus we have two sequences $t _ { i } ^ { * } < t _ { i } ^ { * * }$ both converging to $T _ { x }$ such that

$$
\mathrm {d i s t} (\sigma (t _ {i} ^ {*}, x), K _ {a}) = \frac {\epsilon_ {0}}{2},
$$

$$
\operatorname {d i s t} \left(\sigma \left(t _ {i} ^ {* *}, x\right), K _ {a}\right) = \epsilon_ {0},
$$

and

$$
\sigma (t, x) \in \overline {{(K _ {a})}} _ {\epsilon_ {0}} \backslash (K _ {a}) _ {\frac {\epsilon_ {0}}{2}} ^ {\circ} \forall t \in [ t _ {i} ^ {*}, t _ {i} ^ {* *} ],
$$

where $( K _ { a } ) _ { \delta }$ denotes the $\delta$ -neighborhood of $K _ { a }$

Again by (PS)c $\forall c \in [ a , b ]$ , we have

$$
\inf  _ {t \in [ t _ {i} ^ {*}, t _ {i} ^ {* *} ]} \| f ^ {\prime} (\sigma (t, x)) \| \geq \alpha > 0.
$$

Therefore

$$
\begin{array}{l} \frac {\epsilon_ {0}}{2} \leq \operatorname {d i s t} \left(\sigma \left(t _ {i} ^ {* *}, x\right), \sigma \left(t _ {i} ^ {*}, x\right)\right) \\ \leq \int_ {t _ {t} ^ {*}} ^ {t _ {i} ^ {* *}} \| \frac {d \sigma}{d t} \| d t \\ \leq \frac {1}{\alpha} \left| t _ {i} ^ {* *} - t _ {i} ^ {*} \right|\rightarrow 0. \\ \end{array}
$$

This is a contradiction, (5.1) is proved.

It follows that

$$
\lim  _ {t \to T _ {x} - 0} f ^ {\prime} (\sigma (t, x)) = \theta ,
$$

from the compactness of $K _ { a }$ .

The (PS)a condition then implies that the limit set $A$ of the orbit $\{ \sigma ( t , x ) | t \in [ 0 , T _ { x } ) \}$ is nonempty and that, for each sequence $t _ { i } \longrightarrow t _ { x - 0 }$ , there exists a subsequence $\widetilde { t _ { i } }$ such that $\sigma ( \widetilde { t } _ { i } , x )$ is convergent.

Next we prove that $A$ is a compact connected subset of $K _ { a }$ , and then a single point, according to the assumption that the connected components of $K _ { a }$ are isolated points. The compactness of $A$ is obvious. We only want to prove the connectedness. If not, $\Finv$ open subsets $O$ and $O ^ { \prime }$ such that

$$
O \cap O ^ {\prime} = \varnothing , A = (O \cap A) \cup \left(O ^ {\prime} \cap A\right),
$$

and $O \cap A \neq \varnothing , O ^ { \prime } \cap A \neq \varnothing$ .

Choosing z ∈ O ∩ A, z- ∈ O- ∩ A, ∃ti → Tx − 0, t- → Tx − 0 satisfying $\sigma ( t _ { i } , x )  z , \ \sigma ( t _ { i } ^ { \prime } , x )  z ^ { \prime }$ . For large $_ i$ , we have

$$
\sigma (t _ {i}, x) \in O, \sigma (t _ {i} ^ {\prime}, x) \in O ^ {\prime},
$$

so $\exists t _ { i } ^ { * } \in [ t _ { i } , t _ { i } ^ { \prime } ]$ (or $[ t _ { i } ^ { \prime } , t _ { i } ]$ ) such that

$$
\sigma (t _ {i} ^ {*}, x) \notin O \cup O ^ {\prime}.
$$

Denote the limit of $\sigma ( t _ { i } ^ { * } , x )$ by $z ^ { * }$ . (It exists because of (PS) $^ { a }$ .) So $z ^ { * } \in A$ , but $z ^ { * } \not \in O \cup O ^ { \prime }$ .

This is a contradiction. We conclude that $A = \{ z \} \subset K _ { a }$ . In both cases, claim 1 is proved.

2. We shall prove the continuity of the function $T _ { x }$ .

As in the proof of Theorem 5.1.6, if $\sigma ( T _ { x _ { 0 } } - 0 , x _ { 0 } ) \not \in K _ { a }$ , then the function $T _ { x }$ is continuous at $x _ { 0 }$ . So we restrict ourselves to the case $z = \sigma ( T _ { x _ { 0 } } - 0 , x _ { 0 } ) \in$ $K _ { a }$ .

If $T _ { x }$ is not continuous at such a $x _ { 0 }$ , then $\exists \epsilon _ { 0 } > 0 , \exists x _ { n } \to x _ { 0 }$ such that

$$
\left| T _ {x _ {n}} - T _ {x _ {0}} \right| \geq \epsilon_ {0},
$$

so there exists a subsequence

$$
\text {e i t h e r} T _ {x _ {n}} \leq T _ {x _ {0}} - \epsilon_ {0}, \text {o r} T _ {x _ {n}} \geq T _ {x _ {0}} + \epsilon_ {0}.
$$

Since

$$
\begin{array}{l} f \circ \sigma (T _ {x} - \epsilon , x) - f \circ \sigma (t, x) = \int_ {t} ^ {T _ {x} - \epsilon} f ^ {\prime} (\sigma (t, x)) d t \\ \leq - \frac {1}{4} (T _ {x} - \epsilon - t), \\ \end{array}
$$

we have

$$
f \circ \sigma (t, x) \geq a + \frac {1}{4} (T _ {x} - \epsilon - t).
$$

But for any fixed $\epsilon > 0$ , according to the continuous dependence of the initial data ODE,

$$
\operatorname {d i s t} \left(\sigma \left(T _ {x _ {0}} - \epsilon , x _ {n}\right), \sigma \left(T _ {x _ {0}} - \epsilon , x _ {0}\right)\right)\rightarrow 0.
$$

If $T _ { x _ { n } } \geq T _ { x _ { 0 } } + \epsilon _ { 0 }$ , then we have

$$
\begin{array}{l} f \circ \sigma (T _ {x _ {0}} - \epsilon , x - 0) = \lim  _ {n \to \infty} f \circ \sigma (T _ {x _ {0}} - \epsilon , x _ {n}) \\ \geq \lim _ {n \to \infty} \left[ a + \frac {1}{4} (T _ {x _ {n}} - T _ {x _ {0}} + \epsilon) \right] \\ \geq a + \frac {1}{4} (\epsilon_ {0} + \epsilon). \\ \end{array}
$$

Letting $\epsilon  0$ , we obtain

$$
a \geq a + \frac {\epsilon_ {0}}{4}.
$$

This is a contradiction.

Similarly, we prove that $T _ { x _ { n } } \ \leq \ T _ { x _ { 0 } } - \epsilon _ { 0 }$ is impossible. Therefore, $T _ { x }$ i s continuous.

3. Finally, we define the deformation retract as follows:

$$
\eta (t, x) = \left\{ \begin{array}{l l} x & \text {i f} (t, x) \in [ 0, 1 ] \times f _ {a}  , \\ \sigma (T _ {x} t, x) & \text {i f} (t, x) \in [ 0, 1) \times (f _ {b} \backslash (f _ {a} \cup K _ {b}))  , \\ \sigma (T _ {x} - 0, x) & \text {i f} (t, x) \in \{1 \} \times (f _ {b} \backslash (f _ {a} \cup K _ {b}))  . \end{array} \right.
$$

Claim. Only the continuity of $\eta$ has to be verified. Four cases are distinguished:

(a) (t, x) ∈ [0, 1] × ◦f a, 0   
(b) (t, x) ∈ [0, 1) × (f −1(a, b]\Kb),   
(c) $( t , x ) \in \{ 1 \} \times ( f ^ { - 1 } ( a , b ] \backslash K _ { b } ) ,$   
(d) $( t , x ) \in [ 0 , 1 ] \times f ^ { - 1 } ( a )$ .

Only cases (c) and (d) have to be verified. Since their proofs are similar, we only give the verification for (c).

If $\eta$ is discontinuous at $( 1 , x _ { 0 } )$ , then $\exists \epsilon > 0$ , $\exists t _ { n }  T _ { x _ { 0 } } - 0$ , and $\exists x _ { n }  x _ { 0 }$ such that

$$
\operatorname {d i s t} \left(\sigma \left(t _ {n}, x _ {n}\right), \sigma \left(T _ {x _ {0}} - 0, x _ {0}\right)\right) \geq \epsilon .
$$

Let $z = \sigma ( T _ { x _ { 0 } } - 0 , x _ { 0 } ) ( \in K _ { a } )$ , and let

$$
F _ {1} = \{z \}, F _ {2} = (M \backslash \stackrel {\circ} {B} _ {\epsilon} (z)) \cap K _ {a}.
$$

Both $F _ { 1 }$ and $F _ { 2 }$ are compact subsets of $K _ { a }$ . Provided by the assumption of $K _ { a }$ , and Lemma 3.5.2, we have compact subsets $K _ { 1 }$ , $K _ { 2 } ~ \subset ~ K _ { a }$ such that

$K _ { 1 } \cap K _ { 2 } = \emptyset , \ F _ { i } \subset K _ { i } , \ i = 1 , 2$ , and $K _ { 1 } \cup K _ { 2 } = K _ { a }$ . Obviously, we may take $K _ { 1 } \subset \breve { B } _ { \epsilon } ( z )$ . Let $N = K _ { 2 } \cup ( M \backslash ^ { \circ } B _ { \epsilon } ( z ) )$ , then

$$
\alpha = \operatorname {d i s t} (N, K _ {1}) > 0.
$$

The continuity of the flow as well as of the arrival time $T _ { x }$ implies that $\exists \delta > 0$ 0 such that

$$
\operatorname {d i s t} \left(\sigma \left(t, x _ {0}\right), \sigma \left(T _ {x _ {0}} - 0, x _ {0}\right)\right) <   \frac {\alpha}{8}
$$

if $t \in [ T _ { x _ { 0 } } - \delta , T _ { x _ { 0 } } ]$ , and $\exists \delta _ { 1 } > 0$ such that

$$
T _ {x} > T _ {x _ {0}} - \delta
$$

if $x \in B _ { \delta } ( x _ { 0 } )$

For $t \in [ T _ { x _ { 0 } } - \delta , T _ { x _ { 0 } } ) \cap [ T _ { x _ { 0 } } - \delta , T _ { x } ) , \ x \in B _ { \delta _ { 1 } } ( x _ { 0 } ) , \ \exists t \in ( 0 , \delta )$ such that

$$
\operatorname {d i s t} \left(\sigma (t, x), \sigma (t, x _ {0})\right) <   \frac {\alpha}{8}.
$$

In summary, for such a $t$ , and for any $x \in B _ { \delta _ { t } } ( x _ { 0 } )$ ,

$$
\sigma (t, x) \in \left(K _ {1}\right) _ {\frac {\alpha}{4}}, \text {t h e} \frac {\alpha}{4} \text {n e i g h b o r h o o d} K _ {1}.
$$

For large n, $x _ { n } \in B _ { \delta _ { t } } ( x _ { 0 } )$ , let $t _ { n } ^ { \prime }$ be such a $t$ , satisfying

$$
\sigma (t _ {n} ^ {\prime}, x _ {n}) \in (K _ {1}) _ {\frac {\alpha}{4}}.
$$

Reducing $\delta$ , and repeating the above procedure, we obtain subsequences $\{ x _ { n } , t _ { n } ^ { \prime } , t _ { n } \}$ such that

$$
\left\{ \begin{array}{l} t _ {n} ^ {\prime}, t _ {n} \to T _ {x _ {0}} - 0 \\ \sigma (t _ {n} ^ {\prime}, x _ {n}) \in (K _ {1}) _ {\frac {\alpha}{4}} \\ \sigma (t _ {n}, x _ {n}) \notin B _ {\epsilon} (z). \end{array} \right.
$$

We may assume $t _ { n } ^ { \prime } < t _ { n }$ ; then we have $\overline { { t _ { n } ^ { \prime } } } , \overline { { t _ { n } } }$ such that

$$
t _ {n} ^ {\prime} \leq \overline {{t _ {n} ^ {\prime}}} <   \overline {{t _ {n}}} \leq t _ {n}
$$

and

$$
\sigma (\overline {{t _ {n} ^ {\prime}}}, x _ {n}) \in \partial [ (K _ {1}) _ {\frac {\alpha}{4}} ]
$$

$$
\sigma (\overline {{t _ {n}}}, x _ {n}) \in \partial [ (N) _ {\frac {\alpha}{4}} ]
$$

$$
\sigma (t, x _ {n}) \notin (K _ {1}) _ {\frac {\alpha}{4}} \cup (N) _ {\frac {\alpha}{4}} \forall t \in [ \overline {{t _ {n} ^ {\prime}}}, \overline {{t _ {n}}} ],
$$

$$
n = 1, 2, \dots .
$$

According to (PS)c, $\forall c \in [ a , b ]$

$$
\beta = \inf  \left\{\parallel f ^ {\prime} (x) \parallel | x \in f ^ {- 1} [ a, b) \backslash ((K _ {1}) _ {\frac {\alpha}{4}} \cup (N) _ {\frac {\alpha}{4}}) \right\} > 0.
$$

Therefore

$$
\frac {\alpha}{2} \leq \operatorname {d i s t} (\sigma (\overline {{t _ {n} ^ {\prime}}}, x _ {n}), \sigma (\overline {{t _ {n}}}, x _ {n})) \leq \int_ {\overline {{t _ {n} ^ {\prime}}}} ^ {\overline {{t _ {n}}}} \| \frac {d \sigma}{d t} \| d t \leq \frac {1}{\beta} | \overline {{t _ {n}}} - \overline {{t _ {n} ^ {\prime}}} | \to 0.
$$

This is a contradiction. The proof is completed.

Remark 5.1.10 A flow on a metric space $M$ is a continuous map $\eta : M \times$ $R ^ { 1 } \to M$ possessing the following properties:

(i) $\eta ( x , 0 ) = x \quad \forall x \in M$ ,   
(ii) $\eta ( \eta ( x , t _ { 1 } ) , t _ { 2 } ) = \eta ( x , t _ { 1 } + t _ { 2 } ) \quad \forall t _ { 1 } , t _ { 2 } \in R ^ { 1 } , \ \forall x \in M .$

A set $A \subset M$ is said to be (positively) invariant with respect to $\eta$ , if

$$
\eta (x, t) \in A \quad \forall x \in A, \forall t \in R ^ {1} (\forall t \in R _ {+} ^ {1}).
$$

By definition, for any subset $A$ , $\widetilde { A } = \bigcup _ { t \in R ^ { 1 } } \eta ( A , t )$ (A+ = η(A, t)) is the ∪t∈R 1+ smallest $\eta$ - (positively) invariant set containing $A$ .

With these terminologies, it is easy to extend Theorems 5.1.6 and 5.1.7 to pseudo-gradient flow invariant sets. Namely, let $\eta$ be a pseudo-gradient flow for $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfying $( \mathrm { P S } ) _ { c } \forall c \in [ a , b ]$ , and let $A$ be a $\eta$ -positively invariant set.

If $K \cap f ^ { - 1 } ( a , b ] \cap A = \emptyset$ , and the connected components of $K \cap f ^ { - 1 } ( a ) \cap A$ are only isolated points, then $f _ { a } \cap A$ is a strong deformation retraction of $f _ { b } \cap A$ .

# 5.1.3 Critical Groups

Definition 5.1.11 Let $f$ be a $C ^ { 1 }$ function defined on $M$ , let $p$ be an isolated critical point $f$ , and let $c = f ( p )$ .

$$
C _ {q} (f, p) = H _ {q} \left(f _ {c} \cap U, \left(f _ {c} \backslash \{p \}\right) \cap U; G\right)
$$

is called the qth critical group of $f$ at $p , q = 0 , 1 , 2 , \ldots$ , where $U$ is an isolated neighborhood of $p$ , i.e., $K \cap U = \{ p \}$ .

According to the excision property of the singular homology theory, the critical groups are well defined, i.e., they do not depend on the special choice of $U$ .

From the definition, we have:

Example 1. If $p$ is an isolated minimum point of $f$ , then

$$
C _ {q} (f, p) = \delta_ {q 0} \cdot G.
$$

Under some additional conditions, the converse of the above statement is true. Namely: If $f \in C ^ { 2 - 0 } ( M , R ^ { 1 } )$ satisfies (PS), and if $p$ is an isolated critical point, which is not a local minimum point, then

$$
C _ {0} (f, p) = 0.
$$

See [Ch 5].

Example 2. If $M$ is $n$ -dimensional, and $p$ is an isolated local maximum point of $f$ , then

$$
C _ {q} (f, p) = \delta_ {q n} \cdot G.
$$

In the following, we assume that $M$ is a Hilbert–Riemannian manifold. The Morse lemma, which is a special case of the following more general decomposition theorem – the splitting theorem – proved later, is a cornerstone in studying the local behavior of a nondegenerate critical point.

Definition 5.1.12 Let $p$ be a nondegenerate critical point of $f$ , we call the dimension of the negative space corresponding to the spectral decomposition of $f ^ { \prime \prime } ( p )$ , the Morse index of $p$ , and denote it by $i n d ( f , p )$ .

Example 3. Suppose that $f \in C ^ { 2 } ( M , R ^ { 1 } )$ and $p$ is a nondegenerate critical point of $f$ with Morse index $j$ ; then

$$
C _ {q} (f, p) = \delta_ {q j} \cdot G.
$$

Proof. According to the Morse lemma, after a linear homeomorphism, we may assume that $f$ is a quadratic function on a Hilbert space $H$ of the form:

$$
f (x) = \frac {1}{2} \big (\parallel x _ {+} \parallel^ {2} - \parallel x _ {-} \parallel^ {2} \big),
$$

where $x = x _ { + } + x _ { - } , \ x _ { \pm } \in H _ { \pm }$ , and ${ \cal H } = { \cal H } _ { + } \oplus { \cal H } _ { - }$ is an orthogonal decomposition.

Let $U = B _ { \epsilon }$ be the $\epsilon$ -ball centered at $\theta$ :

$$
B _ {\epsilon} \cap f _ {0} = \left\{x \in B _ {\epsilon} | \| x _ {+} \| \leq \| x _ {-} \| \right\}.
$$

Define

$$
\eta (x, t) = x _ {-} + t x _ {+} \quad \forall (x, t) \in \left(B _ {\epsilon} \cap f _ {0}\right) \times [ 0, 1 ].
$$

It is a strong deformation retract from $( B _ { \epsilon } \cap f _ { 0 } , B _ { \epsilon } \cap ( f _ { 0 } \backslash \{ \theta \} ) )$ to $( H _ { - } \cap$ $B _ { \epsilon } , ( H _ { - } \backslash \{ \theta \} ) \cap B _ { \epsilon } )$ . Thus

$$
\begin{array}{l} C _ {q} (f, q) \cong H _ {q} \left(H _ {-} \cap B _ {\epsilon}, \left(H _ {-} \backslash \{\theta \}\right) \cap B _ {\epsilon}\right) \\ \cong H _ {q} (B ^ {j}, S ^ {j - 1}) = \delta_ {q j} \cdot G, \\ \end{array}
$$

if $j < + \infty$ . Nevertheless, for $j = + \infty$ , since $S ^ { \infty }$ is contractible, again, we have

$$
C _ {q} (f, p) \cong 0.
$$

The conclusion is proved.

Theorem 5.1.13 (Splitting) Suppose that $U$ is a neighborhood of $\theta$ in a Hilbert space $H$ , and that $f \in C ^ { 2 } ( U , R ^ { 1 } )$ . Assume that $\theta$ is the only critical point of $f$ , and that $A = f ^ { \prime \prime } ( \theta )$ is a Fredholm operator. Then there exist a ball $B \subset U$ centered at $\theta$ , an origin-preserving local homeomorphism $\varphi$ defined on $B$ , and a $C ^ { 1 }$ map $h : B \cap N \to N ^ { \perp }$ , where $N = k e r ( A )$ such that

$$
f \circ \varphi (z + y) = \frac {1}{2} (A z, z) + f (y + h (y)) \quad \forall x \in B ,
$$

where $y \in N$ and $z \in N ^ { \bot }$ , with $x = y + z$ .

Proof. • Decomposing $H$ into $N \oplus N ^ { \perp }$ , let $P$ be the orthogonal projection onto $N ^ { \perp }$ . We solve the equation for fixing $y \in B \cap N$ :

$$
P f ^ {\prime} (y + z) = \theta_ {1} \quad (\theta_ {1} \text {a n d} \theta_ {2} \text {a r e t h e o r i g i n s i n} N ^ {\perp} \text {a n d} N \text {r e s p .}).
$$

Since $f ^ { \prime } ( \theta _ { 1 } + \theta _ { 2 } ) = \theta _ { 1 }$ , and $f ^ { \prime \prime } ( \theta _ { 1 } + \theta _ { 2 } ) = A$ , by the implicit function theorem, there exist a ball $B$ and $h : B \cap N \to N ^ { \bot }$ satisfying

$$
P f ^ {\prime} (y + h (y)) = \theta_ {1} \quad \forall y \in B \cap N  .
$$

• Setting $u = z - h ( y )$ , and letting

$$
\left\{ \begin{array}{l} F (u, y) = f (z + y) - f (h (y) + y), \\ F _ {2} (u) = \frac {1}{2} (A u, u), \end{array} \right.
$$

we have

$$
\begin{array}{l} F (\theta_ {1}, y) = 0, \\ F _ {u} ^ {\prime} (\theta_ {1}, y) = P f ^ {\prime} (y + h (y)) = \theta_ {1}, \\ F _ {u} ^ {\prime \prime} (\theta_ {1}, \theta_ {2}) = P f ^ {\prime \prime} (\theta) = A | _ {N ^ {\perp}}. \\ \end{array}
$$

Define $\xi : ( u , y )  u _ { 0 } \in F _ { 2 } ^ { - 1 } \circ F ( u , y ) \cap \{ \eta ( u , t ) | | t | < | | \ u \parallel \}$ , where $\eta$ is the flow defined by the following ODE:

$$
\left\{ \begin{array}{l} \dot {\eta} (s) = - A \eta (s) / \| A \eta (s) \|  , \\ \eta (0) = u  . \end{array} \right.
$$

Claim: $\eta$ is well defined for $| t | < \parallel u \parallel$ .

Indeed, $\| { \boldsymbol { \eta } } ( s ) - { \boldsymbol { u } } \| \leq | s |$ , we have $\parallel \eta ( u , t ) \parallel \geq \parallel u \parallel - | t |$

Noticing that $\eta ( u , t ) \in N ^ { \bot }$ , the denominator of the vector field does not vanish for $| t | < \parallel u \parallel$ .

If there exists a unique $t ( u , y ) \in ( 0 , \parallel u \parallel )$ such that

$$
F _ {2} (\eta (u, \bar {t} (u, y))) = F (u, y),
$$

for $u \neq \theta _ { 1 }$ , we set

$$
\xi (u, y) = \left\{ \begin{array}{l l} \theta_ {1}, & u = \theta_ {1}  , \\ \eta (u, \bar {t} (u, y)), & u \neq \theta_ {1}  , \end{array} \right.
$$

then $( \xi ( u , y ) , y ) \longmapsto ( z , y )$ defines a local homeomorphism $\varphi$

Indeed, from the implicit function theorem, $t ( u , y )$ is continuous provided that

![](images/8f641e675f84fe2a6503fffab50845ecf2163b5b49c020acda3bfa46bbee3719.jpg)  
Fig. 5.1.

$$
\frac {\partial}{\partial t} F _ {2} \circ \eta (u, \bar {t}) = - \| A \eta (u, \bar {t}) \| \neq 0,
$$

if $u \neq \theta _ { 1 }$

It is easy to verify that $\varphi$ is a local homeomorphism.

$\bullet$ We verify the existence and the uniqueness of $t ( u , y )$ in a few steps. Let $B _ { 1 } = B \cap N ^ { \bot }$ and $B _ { 2 } = B \cap N$ .

1. For any $\epsilon > 0$ , there exists suitable $B$ , such that

$$
\begin{array}{l} | F (u, y) - F _ {2} (u) | = | F (u, y) - F (\theta_ {1}, y) - \left(F _ {u} ^ {\prime} (\theta_ {1}, y), u\right) - F _ {2} (u) | \\ = \left| \int_ {0} ^ {1} (1 - t) \left(\left(F _ {u} ^ {\prime \prime} (t u, y) - F _ {u} ^ {\prime \prime} \left(\theta_ {1}, \theta_ {2}\right)\right) u, u\right) d t \right| \\ <   \epsilon \| u \| ^ {2}, \\ \end{array}
$$

for $( u , y ) \in B _ { 1 } \times B _ { 2 }$

2.

$$
\begin{array}{l} | F _ {2} (\eta (u, t)) - F _ {2} (u) | = \Big | \int_ {0} ^ {t} \frac {d}{d s} F _ {2} (\eta (u, s)) d s \Big | \\ = \left| \int_ {0} ^ {t} \left(F _ {2} ^ {\prime} (\eta), \dot {\eta}\right) d s \right| \\ \end{array}
$$

$$
\begin{array}{l} = \int_ {0} ^ {| t |} \| A \eta (u, s) \| d s \\ \geq C \int_ {0} ^ {| t |} \| \eta (u, s) \| d s \\ \geq C \left(\| u \| | t | - \frac {t ^ {2}}{2}\right), \\ \end{array}
$$

where $C > 0$ is a constant determined by the spectrum of $A$ .

$F _ { 2 } ( \eta ( u , t ) )$ , as a function of $t$ , is strictly decreasing on $( - \parallel u \parallel , \parallel u \parallel )$ , and $F _ { 2 } ( \eta ( u , - t ) ) > F ( u , y ) > F _ { 2 } ( \eta ( u , t ) )$ holds for

$$
\left(1 - \sqrt {1 - \frac {2 \epsilon}{C}}\right) \| u \| \leq t \leq \| u \| .
$$

We conclude the existence and the uniqueness of the function $t ( u , y )$ , with

$$
| \bar {t} (u, y) | \leq \left(1 - \sqrt {1 - \frac {2 \epsilon}{C}}\right) \| u \| .
$$

![](images/332c49257b3ad6c22491b5172f4c0d227017df8c010480efed682ab5b01d2e5e.jpg)

Remark 5.1.14 In the case $N = \{ 0 \}$ , the Morse lemma is a consequence of the above theorem. It remains to verify that $\varphi$ is a diffeomophism. We omit the verification, in fact, a weaker version is sufficient to prove Example 3.

Remark 5.1.15 In the study of nonlinear PDEs, sometimes we work on Banach spaces rather than Hilbert spaces. We note that the above theorem can be applied under the following assumptions:

Let $X$ be a Banach space, embedded continuously into a Hilbert space $H$ as a dense linear subspace. Assume:

(A1) There are Banach spaces as follows:

![](images/a3090662d6c19501e0c5f8d96630d6fe2704097d6de003c8e7cc0596da15977d.jpg)  
Fig. 5.2.

(A2) $f \in C ^ { 2 } ( H , \mathbb { R } ^ { 1 } )$ has a derivative of the form

$$
f ^ {\prime} (x) = x - K \cdot G (x),
$$

where $K : Y _ { i }  X _ { i + 1 }$ is linear and continuous, and $G : X _ { i }  Y _ { i }$ is continuous, $i = 0 , 1 , \ldots , n - 1$ .

Let $A = I - K \cdot G ^ { \prime } ( \theta )$ , and let $H _ { * } , * = - , 0 , +$ , be the negative, null, and positive spaces resp., according to the spectral decomposition of $A$ .

(A3) $H _ { 0 } \subset X$

A map is called regular, if it maps $X$ into $X$ .

We claim that the splitting theorem remains true for the Banach space $X$ if assumptions $( A _ { 1 } ) , ( A _ { 2 } )$ and $\left( A _ { 3 } \right)$ hold.

To this end, we continue to use the notations $\varphi , \xi , \eta , ~ h$ and $B$ of Theorem 5.1.13, and prove that $\varphi$ is regular. It is sufficient to show that the map $\psi : ( u , y ) \longmapsto ( \xi ( u , y ) , y )$ is regular, and that $\ln h \subset X$ .

Indeed, the flow $\eta$ is a reparametrization of the following flow $\zeta$ :

$$
\left\{ \begin{array}{l} \dot {\zeta} (s) = - \zeta (s) + K \cdot G ^ {\prime} (\theta) \zeta (s) \\ \zeta (0) = u  . \end{array} \right.
$$

Or equivalently,

$$
\zeta (t) = e ^ {- t} u + K \cdot G ^ {\prime} (\theta) \int_ {0} ^ {t} e ^ {t - s} \zeta (s) d s.
$$

By a bootstrap procedure, we have $\zeta \in X$ whenever $u \in X \cap B$ . This shows the regularity of $\psi$ .

Again, as an element in H ${ \bf { \dot { \theta } } } , { \bf \Phi } ^ { z } = h ( y )$ satisfies the equation:

$$
P f ^ {\prime} (y + z) = \theta ,
$$

i.e.,

$$
h (y) = - P K \cdot G (y + h (y)) + (I - P) h (y).
$$

Noticing that $( I - P ) H = H _ { 0 } \subset X$ , $P$ is regular. By the same reason, $P X _ { i } \subset$ $X _ { i } , \ i = 0 , 1 , \dotsc , n$ . After a bootstrap iteration, it follows that $\ln h \subset X$ .

Theorem 5.1.16 Under the assumptions of the splitting theorem, (A1), (A2) and (A3), we have

$$
C _ {q} (f, \theta) = C _ {q} (f | _ {X}, \theta).
$$

Proof. We use $\varphi$ and $B$ as in the splitting theorem, and choose $U = \varphi ( B )$ . Define a deformation $\eta : U \times \lfloor 0 , 1 \rfloor \to U$ as follows:

$$
\eta (x, t) = \varphi (t x _ {+} + x _ {-}),
$$

where $x _ { + } + x _ { - } = \varphi ^ { - 1 } ( x )$ , and $x _ { + } ~ \in ~ H _ { + }$ , $x _ { - } ~ \in ~ H _ { - } \oplus H _ { 0 }$ . Then $\eta$ is a −deformation retract $f _ { c } \cap U \ \to \ f _ { c } \cap U _ { - }$ −, where $U _ { - } \ = \ \varphi ( B \cap ( H _ { - } \oplus H _ { 0 } ) )$ ). Therefore

$$
C _ {*} (f, \theta) = H _ {*} \left(f _ {c} \cap U, f _ {c} \cap U \backslash \{\theta \}\right)
$$

$$
= H _ {*} \left(f _ {c} \cap U _ {-}, f _ {c} \cap U _ {-} \backslash \{\theta \}\right),
$$

where $c = f ( \theta )$

Since $\varphi$ is regular, we also have

$$
C _ {*} (\hat {f}, \theta) = H _ {*} (\hat {f} _ {c} \cap U _ {-}, \hat {f} _ {c} \cap U _ {-} \backslash \{\theta \}),
$$

where ${ \hat { f } } \ = \ f | _ { X }$ . However, $f _ { c } \cap U _ { - } = \hat { f } _ { c } \cap U _ { - }$ from (A3). This proves our conclusion. □

As a consequence of the splitting theorem, we have:

Theorem 5.1.17 (Shifting) Assume that the Morse index of $f$ at an isolated critical point $p$ is $j < + \infty$ . Under the assumptions of the splitting theorem, we have

$$
C _ {q} (f, p) = C _ {q - j} (\widetilde {f}, \theta),
$$

where $\widetilde f ( y ) = f ( y + h ( y ) )$ as in the splitting theorem.

Proof. We may assume that $f$ is of the form $f ( z + y ) = f _ { 1 } ( z ) + f ( y + h ( y ) )$ , where $f _ { 1 } ( z ) = \frac { 1 } { 2 } ( \parallel z _ { + } \parallel ^ { 2 } - \parallel z _ { - } \parallel ^ { 2 } )$ , $z = z _ { + } + z _ { - }$ , $( z _ { + } , z _ { - } ) \in H _ { + } \oplus H _ { - } = N ^ { \bot }$ , and $y \in N$ , where the notations are inherited from the splitting theorem. Since $\mathrm { d i m } N \mathrm { ~ < ~ } \infty$ , it is easy to construct a new function $f _ { 2 }$ such that $f _ { 2 }$ equals $f ( y + h ( y ) )$ in a neighborhood of $\theta$ in $N$ , and $f _ { 2 }$ satisfies the (PS) condition.

Let $U _ { i }$ be an isolated neighborhood of $\theta _ { i }$ for $f _ { i }$ in the space $Y _ { i } , i = 1 , 2$ , where $Y _ { 1 } = N ^ { \perp }$ and $Y _ { 2 } = N$ . Suppose $U _ { i } \subset f _ { i } ^ { - 1 } [ - \epsilon , \epsilon ] , ~ i = 1 , 2$ , for some $\epsilon > 0$ . Let $W _ { i } = \widetilde { U } _ { i } \cap f _ { i } ^ { - 1 } [ - \epsilon , \epsilon ]$ , then $W _ { i }$ is again an isolated neighborhood of $\theta _ { i }$ for $f _ { i }$ , where $\widetilde { U } _ { i }$ is the smallest $\eta$ -invariant set containing $U _ { i }$ , $i = 1 , 2$ . We have a strong deformation retract $\psi$ deforming $( f _ { 2 } ) _ { \varepsilon }$ to $( f _ { 2 } ) _ { 0 }$ , according to the second deformation theorem. One constructs a deformation as follows:

$$
\varphi (z, y, t) = z _ {-} + (1 - t) z _ {+} + \psi (y, t).
$$

Accordingly,

$$
\begin{array}{l} C _ {*} (f, p) \cong H _ {*} \left(f _ {0} \cap \left(W _ {1} \times W _ {2}\right), \left(f _ {0} \cap \left(W _ {1} \times W _ {2}\right) \backslash \left\{\left(\theta_ {1}, \theta_ {2}\right) \right\}\right); G\right) \\ \cong H _ {*} \left(\left(\left(f _ {1}\right) _ {0}\right) \cap W _ {1}\right) \times \left(\left(f _ {2}\right) _ {0} \cap W _ {2}\right), \\ \left(\left(f _ {1}\right) _ {0}\right) \cap W _ {1}) \times \left(\left(f _ {2}\right) _ {0} \cap W _ {2}\right) \backslash \left\{\left(\theta_ {1}, \theta_ {2}\right) \right\}; G) \\ \cong H _ {*} \left(\left(f _ {1}\right) _ {0} \cap W _ {1}, \left(f _ {1}\right) _ {0} \cap W _ {1} \backslash \left\{\theta_ {1} \right\}; G\right) \\ \otimes H _ {*} \left(\left(f _ {2}\right) _ {0} \cap W _ {2}, \left(f _ {2}\right) _ {0} \cap W _ {2} \backslash \left\{\theta_ {2} \right\}; G\right) \\ \cong C _ {*} \left(f _ {1}, \theta_ {1}\right) \otimes C _ {*} \left(f _ {2}, \theta_ {2}\right) \\ \end{array}
$$

Provided by the Kunneth formula, where we omit the verification of the pair $\langle ( ( f _ { 1 } ) _ { 0 } \cap W _ { 1 } ) \backslash \{ \theta _ { 1 } \} \rangle \times ( ( f _ { 2 } ) _ { 0 } \cap W _ { 2 } )$ , $( ( f _ { 1 } ) _ { 0 } \cap W _ { 1 } ) \times ( ( f _ { 2 } ) _ { 0 } \cap W _ { 2 } ) \backslash \{ \theta _ { 2 } \} )$ is an excision couple.

By the use of Example 3, we obtain

$$
C _ {q} (f, p) = C _ {q - j} \left(f _ {2}, \theta\right).
$$

Since $f _ { 2 } ( y ) = f ( y + h ( y ) ) = \widetilde { f } ( y )$ , this proves our conclusion.

□

Corollary 5.1.18 Under the assumption of the shifting theorem with $\mathrm { d i m } N =$ k, if p is:

(1) a local minimum point of $\hat { f }$ , then $C _ { q } ( f , p ) = \delta _ { q j } \cdot G$ ;   
(2) a local maximum point of $\hat { f }$ , then $C _ { q } ( f , p ) = \delta _ { q ( j + k ) } \cdot G$ ;   
(3) neither a local minimum point nor a local maximum point, then

$$
C _ {q} (f, p) = 0 \text {f o r} q \leq j \text {a n d} q \geq j + k.
$$

Definition 5.1.19 (Mountain pass point) An isolated critical point $p$ of $f$ is called a mountain pass point if $C _ { 1 } ( f , p ) \neq 0$ .

Theorem 5.1.20 Suppose that $f \in C ^ { 2 } ( M , R ^ { 1 } )$ has a mountain pass point p, and that $f ^ { \prime \prime } ( p )$ is a Fredholm operator satisfying the condition:

$$
(\Phi) \quad f ^ {\prime \prime} (p) \geq 0 a n d 0 \in \sigma \left(f ^ {\prime \prime} (p)\right) \Longrightarrow \dim \ker f ^ {\prime \prime} (p) = 1.
$$

Then

$$
C _ {q} (f, p) = G \cdot \delta_ {q 1}.
$$

Proof. Let $j = \operatorname { i n d } ( f ^ { \prime \prime } ( p ) )$ .

If $p$ is nondegenerate, then by Example 3: $C _ { q } ( f , p ) = \delta _ { q j } \cdot G$ ; we have $j = 1$ The conclusion follows.

Otherwise, from the shifting theorem, $C _ { q } ( f , p ) = C _ { q - j } ( \stackrel { \sim } { f } , p )$ ; we obtain $j \leq 1$ .

In the case where $j = 1$ , $C _ { 0 } ( \tilde { f } , \theta ) \neq 0 . \ 6$ θ is a local minimum of $f$ , from Example 1, $C _ { q } ( f , p ) = C _ { q - 1 } ( \widetilde { f } , \theta ) = \delta _ { q 1 } \cdot G$ .

In the case where $j = 0$ , $C _ { 1 } ( \tilde { f } , \theta ) \neq 0$ . Now, $\dim \ker { ( f ^ { \prime \prime } ( p ) ) } = 1$ , $\theta$ is a local maximum. From Example 2, $C _ { q } ( f , p ) = C _ { q } ( \widetilde { f } , \theta ) = \delta _ { q 1 } \cdot G$ .

The proof is complete.

# 5.1.4 Global Theory

For a function $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfying the $( \mathrm { P S } ) _ { c } \forall c \in [ a , b ]$ , where $a , b$ are regular values, we know from the nontrivial interval theorem that the nontriviality of $H _ { * } ( f _ { b } , f _ { a } ; G )$ implies the existence of a critical point in $f ^ { - 1 } ( a , b )$ . In this subsection we shall show that the topological invariance $H _ { * } ( f _ { b } , f _ { a } ; G )$ possesses the homotopy invariance and the subadditivity as follows.

Theorem 5.1.21 Suppose that $\{ f _ { \sigma } \in C ^ { 1 } ( M , R ^ { 1 } ) | \sigma \in [ 0 , 1 ] \}$ is a family of functions satisfying the (PS) condition. Assume that $a ( \sigma )$ and $b ( \sigma )$ are two continuous functions defined on $[ 0 , 1 ]$ with $a ( \sigma ) < b ( \sigma )$ , and that both $a ( \sigma )$ and $b ( \sigma )$ are regular values of $f _ { \sigma }$ , $\forall \sigma \in [ 0 , 1 ]$ . Assume that $\sigma \mapsto f _ { \sigma }$ is continuous in $C ^ { 1 } ( M )$ topology. Then the homology group $H _ { * } ( ( f _ { \sigma } ) _ { b ( \sigma ) } , ( f _ { \sigma } ) _ { a ( \sigma ) } ; G )$ i s independent of $\sigma$ .

Proof. For simplifying notations, $\forall \sigma _ { 0 } , \sigma _ { 1 } \in [ 0 , 1 ]$ , we write $f _ { \sigma _ { i } }$ , $a ( \sigma _ { i } )$ , b(σi), and $K ( f _ { \sigma _ { i } } ) \cap f _ { \sigma _ { i } } ^ { - 1 } ( a ( \sigma _ { i } ) , b ( \sigma _ { i } ) )$ as $f _ { i } , a _ { i } , b _ { i }$ and $K _ { i }$ , respectively, $i = 0 , 1$ .

As $| \sigma _ { 1 } - \sigma _ { 0 } |$ is small, we have $c < d$ such that

$$
f _ {0} \left(K _ {0}\right) \subset (c, d) \subset [ c, d ] \subset \left(a _ {0}, b _ {0}\right) \cap \left(a _ {1}, b _ {1}\right).
$$

The first inclusion is due to the (PS) condition for $f _ { 0 }$ .

By the continuity of $f _ { 0 }$ , we have $\delta > 0$ such that

$$
f _ {0} \left(\left(K _ {0}\right) _ {\delta}\right) \subset (c, d),
$$

where $( K _ { 0 } ) _ { \delta }$ is the $\delta$ -neighborhood of $K _ { 0 }$ . Again by the (PS) condition of $f _ { 0 }$ , $\exists \epsilon = \epsilon ( \delta ) > 0$ such that

$$
\| f _ {0} ^ {\prime} (x) \| \geq \epsilon \quad \forall x \in f _ {0} ^ {- 1} [ a _ {0}, b _ {0} ] \backslash (K _ {0}) _ {\delta}.
$$

As $\left| \sigma _ { 1 } - \sigma _ { 0 } \right|$ is small, we have $K _ { 1 } \subset ( K _ { 0 } ) _ { \delta } , f _ { 1 } ( ( K _ { 0 } ) _ { \delta } ) \subset ( c , d )$ and $f _ { 1 } ^ { - 1 } [ c , d ] \subset$ $f _ { 0 } ^ { - 1 } ( a , b )$ . Thus $f _ { i } ( K _ { j } ) \subset ( c , d )$ , $i , j = 0 , 1$ .

Again as $| \sigma _ { 1 } - \sigma _ { 0 } |$ is small, we can construct a pseudo-gradient vector field for $f _ { 1 }$ , which coincides with that of $f _ { 0 }$ in $( ( f _ { 1 } ) _ { b _ { 1 } } \cap ( f _ { 0 } ) _ { b _ { 0 } } ) \backslash ( K _ { 0 } ) _ { \delta }$ . Thus the pseudo-gradient flow defines strong deformation retracts $( ( f _ { 1 } ) _ { b _ { 1 } } , ( f _ { 0 } ) _ { c } \cap$ $( f _ { 1 } ) _ { c } )  ( ( f _ { 0 } ) _ { d } \cap ( f _ { 1 } ) _ { d } , ( f _ { 0 } ) _ { c } \cap ( f _ { 1 } ) _ { c } )$ and $( f _ { 1 } ) _ { c }  ( f _ { 0 } ) _ { c } \cap ( f _ { 1 } ) _ { c }$ , according to the nontrivial interval theorem. In the same manner we have strong deformation retracts $\left( ( f _ { 0 } ) _ { b _ { 0 } } , ( f _ { 0 } ) _ { c } \cap ( f _ { 1 } ) _ { c } \right) \to \left( ( f _ { 0 } ) _ { d } \cap ( f _ { 1 } ) _ { d } , ( f _ { 0 } ) _ { c } \cap ( f _ { 1 } ) _ { c } \right)$ and $( f _ { 0 } ) _ { c } \ $ $( f _ { 0 } ) _ { c } \cap ( f _ { 1 } ) _ { c }$ . Therefore the exactness of the homology sequence yields:

$$
H _ {*} \left(\left(f _ {0}\right) _ {b _ {0}}, \left(f _ {0}\right) _ {c}; G\right) = H _ {*} \left(\left(f _ {1}\right) _ {b _ {1}}, \left(f _ {1}\right) _ {c}; G\right).
$$

Again by the nontrivial interval theorem, there are strong deformation retracts $( f _ { 0 } ) _ { c } \longrightarrow ( f _ { 0 } ) _ { a _ { 0 } }$ , $( f _ { 1 } ) _ { c } \longrightarrow ( f _ { 1 } ) _ { a _ { 1 } }$ .

We arrive at

$$
H _ {*} \left(\left(f _ {0}\right) _ {b _ {0}}, \left(f _ {0}\right) _ {a _ {0}}; G\right) = H _ {*} \left(\left(f _ {1}\right) _ {b _ {1}}, \left(f _ {1}\right) _ {a _ {1}}; G\right).
$$

Remark 5.1.22 One may use other homology theories instead, e.g., the singular cohomology $H ^ { * } ( A , B ; F )$ or Alexander–Spanier cohomology $\bar { H } ^ { * } ( A , B ; F )$ , with a coefficient field $F$ , etc. Since they share most common properties but have only a few differences, we use special homology theory for special problems.

The homotopy invariance $H _ { * } ( f _ { b } , f _ { a } ; G )$ can be localized on a pseudogradient flow invariant set. For details see Chang and Ghoussoub [CG]. We present here a special case, which will be used later.

Let $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfy the (PS) condition, and let $a < b$ be regular values. Since $K \cap f ^ { - 1 } [ a , b ]$ is compact, it has a bounded neighborhood $U \subset$ $f ^ { - 1 } [ a , b ]$ . Let $\widetilde { U }$ be the $\eta$ -invariant set for a pseudo-gradient flow $\eta$ for $f$ .

Lemma 5.1.23 The set $W = \widetilde { U } \cap f ^ { - 1 } [ a , b ]$ is bounded.

Proof. From the (PS) condition, we have $\epsilon > 0$ such that

$$
\| f ^ {\prime} (x) \| \geq \epsilon \quad \forall x \in f ^ {- 1} [ a, b ] \backslash U.
$$

$\forall y \in W \backslash U$ , there exist $x \in \partial U$ and $t > 0$ (or $t < 0$ ) such that $y = \eta ( x , t )$ and $\eta ( x , s ) \in \widetilde { U } \backslash U \ \forall s > 0$ ( $\forall s < 0$ resp.). Let $X$ be the pseudo-gradient vector field with respect to $\eta$ . Thus

$$
\begin{array}{l} \epsilon^ {2} t \leq \int_ {0} ^ {t} \| f ^ {\prime} (\eta (x, s)) \| ^ {2} d s \\ \leq \int_ {0} ^ {t} \langle f ^ {\prime} (\eta (x, s)), X (\eta (x, s)) \rangle d s \\ = - \int_ {0} ^ {t} \frac {d}{d s} f (\eta (x, s)) d s = f (x) - f (y) \leq b - a \\ \end{array}
$$

as $t > 0$ . We obtain the estimate $\textstyle 0 < t \leq { \frac { b - a } { \epsilon ^ { 2 } } }$ ; similarly, for $t < 0$ , by which, we have the estimate

$$
\begin{array}{l} \parallel y - x \parallel = \parallel \eta (x, t) - x \parallel \leq \int_ {0} ^ {t} \parallel \dot {\eta} (x, s) \parallel d s \\ \leq 2 \left(t \int_ {0} ^ {t} \| f ^ {\prime} (\eta (x, s)) \| ^ {2} d s\right) ^ {1 / 2} \\ \leq 2 (t (b - a)) ^ {1 / 2} \leq \frac {2 (b - a)}{\epsilon}. \\ \end{array}
$$

Therefore $W$ is bounded.

Since the regular set is open, we have regular values $c , d$ such that $[ a , b ] \subset$ $( c , d )$ . Let $W = \widetilde { U } \cap f ^ { - 1 } [ c , d ]$ .

Corollary 5.1.24 With the above notations, the singular homology groups $H _ { * } ( f _ { b } \cap \widetilde { U } , f _ { a } \cap \widetilde { U } ; G )$ are invariant under small $C ^ { 1 } ( W )$ perturbation of $f$ , i.e., if $\parallel g - f \parallel _ { C ^ { 1 } ( W ) }$ is small and both $f$ and $_ { g }$ satisfy the (PS) condition, then

$$
H _ {*} (f _ {b} \cap \widetilde {U}, f _ {a} \cap \widetilde {U}; G) = H _ {*} (g _ {b} \cap \widetilde {U}, g _ {a} \cap \widetilde {U}; G).
$$

Corollary 5.1.25 With the (PS) condition, the critical groups $C _ { * } ( f , p )$ for an isolated critical point $p$ are invariant under $C ^ { 1 }$ topology on any bounded neighborhood of $p$ .

Now we turn to the subadditivity.

Let $S$ be an integer-valued function on certain pairs of spaces. $S$ is called subadditive if

$$
S (X, Z) \leq S (X, Y) + S (Y, Z), \text {w h e n e v e r} Z \subset Y \subset X.
$$

Example. $\forall q \in \mathbb { N }$ , rank $H _ { q } ( X , Y ; G )$ is subadditive.

In fact, by examining the exact sequence:

$$
\dots \to H _ {q} (Y, Z; G) \stackrel {j _ {*}} {\to} H _ {q} (X, Z; G) \stackrel {i _ {*}} {\to} H _ {q} (X, Y; G) \stackrel {\partial_ {*}} {\to} H _ {q - 1} (Y, Z; G) \to \dots
$$

we obtain

$$
\begin{array}{l} \operatorname {r a n k} H _ {q} (X, Z; G) = \operatorname {r a n k} \operatorname {I m} i _ {*} + \operatorname {r a n k} \ker i _ {*} \\ = \operatorname {r a n k} \operatorname {I m} i _ {*} + \operatorname {r a n k} \operatorname {I m} j _ {*} \\ \leq \operatorname {r a n k} H _ {q} (X, Y; G) + \operatorname {r a n k} H _ {q} (Y, Z; G). \\ \end{array}
$$

Applying this to the triple $( f _ { c } , f _ { b } , f _ { a } )$ with regular values $c > b > a$ , we have

$$
\operatorname {r a n k} H _ {q} \left(f _ {c}, f _ {a}; G\right) \leq \operatorname {r a n k} H _ {q} \left(f _ {b}, f _ {a}; G\right) + \operatorname {r a n k} H _ {q} \left(f _ {c}, f _ {b}; G\right).
$$

If further, we write $\epsilon _ { q } ( X , Y ) \ = \ \mathrm { r a n k } \mathrm { I m } i _ { * }$ , q(x, z) = rank Im j and $\epsilon _ { q - 1 } ( Y , Z ) = \operatorname { r a n k } \operatorname { I m } \partial _ { * }$ , then

$$
\begin{array}{l} \operatorname {r a n k} H _ {q} (X, Y) = \epsilon_ {q} (X, Y) + \epsilon_ {q - 1} (Y, Z), \\ \operatorname {r a n k} H _ {q} (X, Z) = \epsilon_ {q} (X, Z) + \epsilon_ {q} (X, Y), \\ \operatorname {r a n k} H _ {q} (Y, Z) = \epsilon_ {q} (Y, Z) + \epsilon_ {q} (X, Z). \\ \end{array}
$$

Summing them up, we obtain

$$
P (t; X, Y) + P (t; Y, Z) = P (t; X, Z) + (1 + t) Q (t; Y, Z),
$$

where

$$
P (t; X, Y) = \sum_ {q = 0} ^ {\infty} t ^ {q} \operatorname {r a n k} H _ {q} (X, Y; G), \text {a n d} Q (t; Y, Z) = \sum_ {q = 0} ^ {\infty} t ^ {q} \epsilon_ {q} (Y, Z)
$$

are formal series with nonnegative coefficients.

If there is a multiple $X _ { 0 } \subset X _ { 1 } \subset X _ { 2 } \subset \cdots \subset X _ { n }$ , we have

$$
\sum_ {j = 1} ^ {n} P \left(t, X _ {j}, X _ {j - 1}\right) = P \left(t, X _ {n}, X _ {0}\right) + (1 + t) Q (t). \tag {5.2}
$$

where $Q$ is a formal series with nonnegative coefficients.

Thus, $\forall t \geq 0 , P ( t ; X , Y )$ is subadditive, and $P ( - 1 , X , Y )$ is additive.

We turn to establishing the relationship between the singular relative homology groups $H _ { * } ( f _ { b } , f _ { a } , G )$ and the critical groups $C _ { * } ( f , p )$ . This is the Morse relation.

Suppose that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ has only isolated critical values, and that each of them corresponds to a finite number of critical points; say

$$
\dots <   c _ {- 2} <   c _ {- 1} <   c _ {0} <   c _ {1} <   c _ {2} <   \dots
$$

are critical values with

$$
K \cap f ^ {- 1} (c _ {i}) = \{z _ {j} ^ {i} \} _ {j = 1} ^ {m _ {i}}, i = 0, \pm 1, \pm 2, \dots .
$$

One chooses

$$
0 <   \epsilon_ {i} <   \max  \left\{c _ {i + 1} - c _ {i}, c _ {i} - c _ {i - 1} \right\}, i = 0, \pm 1, \pm 2, \dots .
$$

Definition 5.1.26 For a pair of regular values $a < b$ , we call

$$
M _ {q} (a, b) = \sum_ {a <   c _ {i} <   b} \operatorname {r a n k} H _ {q} \left(f _ {c _ {i} + \epsilon_ {i}}, f _ {c _ {i} - \epsilon_ {i}}; G\right)
$$

the qth Morse type number of the function $f$ on $( a , b ) , \ q = 0 , 1 , 2 , . . .$ .

For functions satisfying the (PS) condition, according to the nontrivial interval theorem, Morse type numbers are independent of the special choice of $\left\{ \epsilon _ { i } \right\}$ . Moreover, we have:

Theorem 5.1.27 Assume that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfies the (PS) condition, and has an isolated critical value $c$ , with $K \cap f ^ { - 1 } ( c ) = \{ z _ { j } \} _ { j = 1 } ^ { m }$ . Then for sufficiently small $\epsilon > 0$ we have

$$
H _ {*} (f _ {c + \epsilon}, f _ {c - \epsilon}; G) = \oplus_ {j = 1} ^ {m} C _ {*} (f, z _ {j}).
$$

Proof. By the second deformation theorem and the homotopy invariance of singular homology groups, we have

$$
H _ {*} \left(f _ {c + \epsilon}, f _ {c - \epsilon}; G\right) = H _ {*} \left(f _ {c}, f _ {c - \epsilon}; G\right),
$$

and

$$
H _ {*} \left(f _ {c} \backslash (K \cap f ^ {- 1} (c)), f _ {c - \epsilon}; G\right) = H _ {*} \left(f _ {c - \epsilon}, f _ {c - \epsilon}; G\right) = 0.
$$

Applying the exactness of singular homology groups to the triple $( f _ { c } , f _ { c } \backslash ( K \cap$ $f ^ { - 1 } ( c ) ) , f _ { c - \epsilon } )$ :

$$
\begin{array}{l} \dots \rightarrow H _ {q} \left(f _ {c} \backslash (K \cap f ^ {- 1} (c)), f _ {c - \epsilon}\right)\rightarrow H _ {q} \left(f _ {c}, f _ {c - \epsilon}\right) \\ \rightarrow H _ {q} \left(f _ {c}, f _ {c} \backslash (K \cap f ^ {- 1} (c))\right)\rightarrow H _ {q - 1} \left(f _ {c} \backslash (K \cap f ^ {- 1} (c)), f _ {c - \epsilon}\right)\rightarrow \dots \\ \end{array}
$$

we find

$$
0 \rightarrow H _ {q} \left(f _ {c}, f _ {c - \epsilon}\right)\rightarrow H _ {q} \left(f _ {c}, f _ {c} \backslash (K \cap f ^ {- 1} (c))\right)\rightarrow 0,
$$

i.e.,

$$
H _ {q} \left(f _ {c}, f _ {c - \epsilon}\right) = H _ {q} \left(f _ {c}, f _ {c} \backslash \left(K \cap f ^ {- 1} (c)\right)\right).
$$

By using the excision property, we may decompose the relative singular homology groups into critical groups:

$$
\begin{array}{l} H _ {*} \left(f _ {c}, f _ {c} \backslash (K \cap f ^ {- 1} (c))\right) = H _ {*} \left(f _ {c} \cap \left(\cup_ {j = 1} ^ {m} B _ {\epsilon} (z _ {j})\right), f _ {c} \cap \left(\cup_ {j = 1} ^ {m} \left(B _ {\epsilon} (z _ {j}) \backslash \{z _ {j} \}\right)\right)\right) \\ = \oplus_ {j = 1} ^ {m} C _ {*} (f, z _ {j}). \\ \end{array}
$$

□

Corollary 5.1.28 $\begin{array} { r } { M _ { * } ( a , b ) = \sum _ { a < c _ { i } < b } \sum _ { j = 1 } ^ { m _ { i } } \operatorname { r a n k } C _ { * } ( f , z _ { j } ^ { i } ) } \end{array}$

Thus, for a nondegenerate function,the $q$ th Morse type number is the number of critical points with Morse index $q$ .

Define the $q$ th Betti number:

$$
\beta_ {q} = \beta_ {q} (a, b) = \operatorname {r a n k} H _ {q} \left(f _ {b}, f _ {a}; G\right), q = 0, 1, \dots ,
$$

and the two formal series:

$$
\begin{array}{l} M (f, a, b; t) = \sum_ {q = 0} ^ {\infty} M _ {q} (a, b) t ^ {q}, \\ P (f, a, b; t) = \sum_ {q = 0} ^ {\infty} \beta_ {q} (a, b) t ^ {q}. \\ \end{array}
$$

Theorem 5.1.29 (Morse relation) Suppose that $f ~ \in ~ C ^ { 1 } ( M , R ^ { 1 } )$ satisfies $( P S ) _ { c } ~ \forall c \in [ a , b ]$ , where a and $b$ are regular values. Assume $( K \cap f ^ { - 1 } [ a , b ] )$ is finite. Moreover, if all $M _ { q } ( a , b )$ and $\beta _ { q } ( a , b )$ are finite, and only finitely many of them are nonzeroes, then

$$
\sum_ {q = 0} ^ {\infty} \left(M _ {q} (a, b) - \beta_ {q} (a, b)\right) t ^ {q} = (1 + t) Q (t), \tag {5.3}
$$

where $Q ( t )$ is a formal series with nonnegative coefficients. In particular, $\forall p =$ $0 , 1 , 2 , \ldots$ ,

$$
\sum_ {q = 0} ^ {p} (- 1) ^ {p - q} M _ {q} (a, b) \geq \sum_ {q = 0} ^ {p} (- 1) ^ {p - q} \beta_ {q} (a, b). \tag {5.4}
$$

More specifically,

$$
\sum_ {q = 0} ^ {\infty} (- 1) ^ {q} M _ {q} (a, b) = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \beta_ {q} (a, b).
$$

Proof. Let $c _ { 1 } < c _ { 2 } < \cdots < c _ { n }$ be the critical values of $f$ in the interval $[ a , b ]$ . Let us choose regular values $\{ d _ { j } \} _ { j = 0 } ^ { n }$ such that

$$
a = d _ {0} <   c _ {1} <   d _ {1} <   \dots <   d _ {j - 1} <   c _ {j} <   d _ {j} <   \dots <   c _ {n} <   d _ {n} = b.
$$

Plugging $X _ { j } = f _ { d _ { j } }$ into (5.2), $j = 0 , 1 , 2 , \ldots , n$ , on the one hand, we have

$$
\begin{array}{l} \sum_ {j = 1} ^ {n} P \left(t; f _ {d _ {j}}, f _ {d _ {j - 1}}\right) = P \left(t; f _ {b}, f _ {a}\right) + (1 + t) Q (t) \\ = P (f, a, b; t) + (1 + t) Q (t). \\ \end{array}
$$

On the other hand,

$$
\sum_ {j = 1} ^ {n} P (t, f _ {d _ {j}}, f _ {d _ {j - 1}}) = \sum_ {j = 1} ^ {n} \sum_ {q = 0} ^ {\infty} t ^ {q} \operatorname {r a n k} H _ {q} (f _ {d _ {j}}, f _ {d _ {j - 1}}; G)
$$

$$
\begin{array}{l} = \sum_ {j = 1} ^ {n} \sum_ {q = 0} ^ {\infty} t ^ {q} \sum_ {i = 1} ^ {m _ {j}} \operatorname {r a n k} C _ {q} (f, z _ {i} ^ {j}) \\ = \sum_ {q = 0} ^ {\infty} t ^ {q} \sum_ {j = 1} ^ {n} \sum_ {i = 1} ^ {m _ {j}} \operatorname {r a n k} C _ {q} (f, z _ {i} ^ {j}) \\ = M (f, a, b; t). \\ \end{array}
$$

(5.4) is proved by setting $t = - 1$ . Replacing the formal series by finite sums, equation (5.3) is proved similarly. □

Remark 5.1.30 (5.4) is a sequence of Morse inequalities, and the following equality is called the Morse equality. On the right-hand side, it is related to the Euler characteristic of the pair $( X , Y )$ :

$$
\chi (X, Y; G) = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \operatorname {r a n k} H _ {q} (X, Y; G)
$$

Remark 5.1.31 (Morse inequalities under general boundary conditions) Let $M$ be a Hilbert–Riemannian manifold modeled on a Hilbert space $H$ with boundary $\partial M \ = \ \Sigma$ , which is a smooth oriented submanifold with codimension 1. Let $n ( x )$ be the outward normal unit vector of $\Sigma$ at $x$ .

$f \in C ^ { 1 } ( M , R ^ { 1 } )$ is said to be satisfying the general boundary condition if:

(1) $K \cap \Sigma = \emptyset$ , i.e., it has no critical point on $\Sigma$   
(2) The restriction ${ \hat { f } } = f | _ { \Sigma }$ , as a function on $\Sigma$ , has only isolated critical points.

Let $\Sigma _ { - } = \{ x \in \Sigma | ( f ^ { \prime } ( x ) , n ( x ) ) \leq 0 \}$ , where $( \cdot , \cdot )$ is the inner product of the Hilbert space $H$ .

Suppose that $f$ has only isolated critical points, and let $\{ m _ { 0 } , m _ { 1 } , . . . , \}$ , and $\{ \mu _ { 0 } , \mu _ { 1 } , \ldots , \}$ be the Morse type numbers of the function $f$ and $\hat { f }$ on $M$ and $\Sigma _ { - }$ respectively.

Under the above assumptions, in addition, if we assume the (PS) condition for $f$ on $M$ and for $\hat { f }$ on $\Sigma$ respectively, then the Morse inequalities are modifid as follows:

$$
\Sigma_ {q = 0} ^ {\infty} (m _ {q} + \mu_ {q} - \beta_ {q}) t ^ {q} = (1 + t) Q (t),
$$

where $Q ( t )$ is a formal series with nonnegative coefficients.

Readers are referred to K. C. Chang [Ch 5] and Chang and Liu [CL 1].

At the end of this subsection, we study the relationship between the Morse theory and the Leray–Schauder degree theory.

Let $H$ be a Hilbert space, and let $f \in C ^ { 2 } ( H , R ^ { 1 } )$ . Assume that $f ^ { \prime } ( x ) =$ $x - T ( x )$ , where $T$ is compact, and that $p _ { 0 }$ is an isolated critical point of $f$ , i.e., an isolated zero of the compact vector field $f ^ { \prime }$ . Both the Leray–Schauder index

$i ( f ^ { \prime } , p _ { 0 } )$ and the critical groups $C _ { * } ( f , p _ { 0 } )$ are topological invariants describing the local behavior at $p _ { 0 }$ of $f ^ { \prime }$ .

Question: What is the relationship between $i ( f ^ { \prime } , p _ { 0 } )$ and $C _ { * } ( f , p _ { 0 } )$ ?

Theorem 5.1.32 Suppose that $f \in C ^ { 2 } ( H , R ^ { 1 } )$ satisfies the (PS) condition, and that $f ^ { \prime } ( x ) = x - T ( x )$ is a compact vector field. If $p _ { 0 }$ is an isolated critical point of $f$ , then

$$
i (f ^ {\prime}, p _ {0}) = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \mathrm {r a n k} C _ {q} (f, p _ {0}),
$$

wherever the RHS makes sense.

Proof. 1. If $p _ { 0 }$ is nondegenerate, then $C _ { q } ( f , p _ { 0 } ) = \delta _ { q j } \cdot G$ , where $j$ is the Morse index of $p _ { 0 }$ , i.e., the dimension of the negative space of

$$
f ^ {\prime \prime} (p _ {0}) = \mathrm {i d} - T ^ {\prime} (p _ {0}).
$$

Since $T ^ { \prime } ( p _ { 0 } )$ is compact, $j < \infty$ .

In this case, $i ( f ^ { \prime } , p _ { 0 } ) = ( - 1 ) ^ { \jmath }$ . Thus

$$
i (f ^ {\prime}, p _ {0}) = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \mathrm {r a n k} C _ {q} (f, p _ {0}).
$$

2. $p _ { 0 }$ is degenerate. Assume $p _ { 0 } = \theta$ and $f ( \theta ) = 0$ . Let $U$ be an isolated bounded neighborhood of $\theta$ , and $W = \widetilde { U } \cap f ^ { - 1 } [ - \gamma , \gamma ]$ for small $\gamma > 0$ . According to Lemma 5.1.23, it is bounded, and let $\delta > 0$ be sufficiently small such that $B _ { \delta } ( \theta ) \subset \operatorname { i n t } ( W ) \cap f ^ { - 1 } [ - \gamma / 2 , \gamma / 2 ]$ . We shall define a function $\hat { f }$ satisfying the (PS) condition such that:

(1) $| f ( x ) - \widetilde f ( x ) | < \gamma / 2 \forall x \in H .$ ,   
(2) $f ( x ) = { \tilde { f } } ( x ) \ \forall x \not \in B _ { \delta }$   
(3) $\hat { f }$ has only finitely many nondegenerate critical points $\{ p _ { j } \} _ { 1 } ^ { m } \subset B _ { \delta }$ in $W$

We assume the existence of such a $\hat { f }$ at this moment. From part 1, Corollary 5.1.24 and Theorem 5.1.29, we have

$$
\begin{array}{l} i \left(f ^ {\prime}, \theta\right) = \deg \left(f ^ {\prime}, W, \theta\right) = \deg \left(\widetilde {f} ^ {\prime}, W, \theta\right) \\ = \sum_ {j = 1} ^ {m} i \left(\widetilde {f} ^ {\prime}, p _ {j}\right) = \sum_ {j = 1} ^ {m} \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \operatorname {r a n k} C _ {q} \left(\widetilde {f}, p _ {j}\right) \\ = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \operatorname {r a n k} H _ {q} \left(\widetilde {f} _ {\gamma} \cap W, \widetilde {f} _ {- \gamma} \cap W; G\right) \\ \end{array}
$$

$$
\begin{array}{l} = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \operatorname {r a n k} H _ {q} \left(f _ {\gamma} \cap W, f _ {- \gamma} \cap W; G\right) \\ = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \operatorname {r a n k} C _ {q} (f, \theta). \\ \end{array}
$$

3. Now, we return to the construction of the function $f$ . Define

$$
\tilde {f} (x) = f (x) + p (\| x \|) \left(x _ {0}, x\right)
$$

where $p \in C ^ { 2 } ( [ 0 , \infty ) , R ^ { 1 } )$ satisfying $0 \leq p \leq 1$ , $\begin{array} { r } { | p ^ { \prime } ( t ) | \leq \frac { 4 } { \delta } } \end{array}$ , and

$$
p (t) = \left\{ \begin{array}{l l} 1 & t \in [ 0, \delta / 2 ] \\ 0 & t > \delta  , \end{array} \right.
$$

and $x _ { 0 } \in H$ will be determined later. Let

$$
\beta = \inf  \left\{\| f ^ {\prime} (x) \| x \in B _ {\delta} \backslash B _ {\delta / 2} \right\},
$$

The (PS) condition of $f$ yields $\beta > 0$ . We choose $x _ { 0 } \in H$ such that

$$
0 <   \left\| x _ {0} \right\| <   \min  \left\{\beta / 6, \gamma / 3 \right\}.
$$

Then we have

$$
\begin{array}{l} \left| f (x) - \widetilde {f} (x) \right| <   \gamma / 3, \\ \| \tilde {f} ^ {\prime} (x) \| \geq \beta / 6 \quad \forall x \in B _ {\delta} \backslash B _ {\delta / 2}, \\ \widetilde {f} (x) = f (x) \quad \forall x \notin B _ {\delta}. \\ \end{array}
$$

For small $\parallel x _ { 0 } \parallel$ , $\widetilde { f ^ { \prime } }$ is a $k$ -set contraction mapping vector field with $k < 1$ . Therefore $\deg ( \tilde { f } , W , \theta )$ is well defined. The (PS) condition for $\hat { f }$ is verified directly. On account of the Sard–Smale theorem, a suitable $x _ { 0 }$ can be chosen such that $\hat { f }$ is nondegenerate. □

Remark 5.1.33 The above theorem shows that for potential compact vector fields, the critical groups provide more information than the Leray–Schauder index for an isolated zero.

Theorem 5.1.34 Suppose that $f \in C ^ { 1 } ( M , \mathbb { R } ^ { 1 } )$ satisfies the (PS) condition with regular values $a < b$ . Let $U$ be a bounded neighborhood of $K \cap f ^ { - 1 } [ a , b ]$ , and $W = \widetilde { U } \cap f ^ { - 1 } [ a , b ]$ . Assume that $M$ is a Hilbert space and $f ^ { \prime } ( x ) = x { - } T ( x )$ is a compact vector field. Then

$$
\deg (f ^ {\prime}, W, \theta) = \sum_ {q = 0} ^ {\infty} (- 1) ^ {q} \mathrm {r a n k} H _ {q} (f _ {b} \cap W, f _ {a} \cap W, G).
$$

Proof. This is a combination of Theorems 5.1.32 and 5.1.29 with an application of the Sard–Smale theorem. □

# 5.1.5 Applications

We present a few simple examples showing how Morse theory can be applied to the study of the existence and the multiplicity of critical points. More sophisticated examples will be studied later.

# 1. A Three-Solution Theorem

Theorem 5.1.35 Let $H$ be a Hilbert space. Suppose that $f \in C ^ { 2 } ( H , R ^ { 1 } )$ satisfies the (PS) condition and is bounded from below, and that $p _ { 0 }$ is a nondegenerate nonminimum critical point of $f$ with finite index $j$ . Then $f$ has at least three distinct critical points.

Proof. In fact, from Corollary 4.8.4, there exists a minimizer $p _ { 1 }$ . One may assume that the minimizer is unique, for otherwise, the proof is done. Thus, we have already two distinct critical points: $p _ { 0 }$ and the minimizer $p _ { 1 }$ , with $C _ { q } ( f , p _ { 1 } ) = \delta _ { q 0 } G$ and $C _ { q } ( f , p _ { 0 } ) = \delta _ { q j } G , \ j \neq 0$ . Let $m = f ( p _ { 1 } )$ and $c = f ( p _ { 0 } )$ .

If there were no other critical point, then according to noncritical interval theorem, $\forall \epsilon > 0$ $f _ { c + \epsilon }$ would be a strong deformation retract of $H$ . Since $m$ i s the minimum, $f _ { m - \epsilon } = \varnothing$ , it follows that $\chi ( f _ { c + \epsilon } , f _ { m - \epsilon } ) = \chi ( H ) = 1$ . According to the Morse equality, we would have

$$
1 + (- 1) ^ {j} = 1.
$$

This is a contradiction.

![](images/79a6efb73602dae2be87d613a4b071463259f38e18a581970266e9efd3aeca60.jpg)

Example 1. Let $\Omega \subset R ^ { n }$ be a bounded domain with smooth boundary; we study the problem:

$$
\left\{ \begin{array}{l l} - \triangle u = g (u) & \text {i n} \Omega , \\ u = 0 & \text {o n} \partial \Omega . \end{array} \right. \tag {5.5}
$$

Let $\lambda _ { 1 } < \lambda _ { 2 } \leq \cdots$ be the eigenvalues of $- \triangle$ with the Dirichlet condition, and let $c _ { i } , i = 1 , 2 , 3$ be various constants. Assume

(g1) $g \in C ^ { 1 } ( R ^ { 1 } )$ with $g ( 0 ) = 0$ .

(g2)

$$
| g ^ {\prime} (t) | \leq \left\{ \begin{array}{l l} c _ {1} (1 + | t | ^ {p - 1}), p <   \frac {n + 2}{n - 2} & \text {i f} n \geq 3, \\ \text {n o r e s t r i c t i o n} & \text {i f} n \leq 2. \end{array} \right.
$$

(g3) $\begin{array} { r } { G ( t ) : = \int _ { 0 } ^ { t } g ( s ) d s \leq c _ { 2 } | t | ^ { 2 } + c _ { 3 } } \end{array}$ , where $c _ { 2 } < \lambda _ { 1 } / 2$

(g4) $\exists i \geq 1$ such that $\lambda _ { i } < g ^ { \prime } ( 0 ) \le \lambda _ { i + 1 }$

Theorem 5.1.36 The problem (5.5) under assumptions (g1)–(g4) has at least three distinct solutions.

Proof. Define

$$
J (u) = \int_ {\Omega} \frac {1}{2} | \nabla u | ^ {2} - G (u) \mathrm {o n} H _ {0} ^ {1} (\Omega).
$$

According to (g1) and (g2), $J$ is $C ^ { 2 }$ . (g3) implies that $J$ is bounded below. $g ( 0 ) = 0$ means that $\theta$ is a critical point of $J$ . The verification of the (PS) condition follows from (g2) and (g3). In fact, (g3) implies that a (PS) sequence must be bounded in $H ^ { 1 }$ , and (g2) implies that $| g ( t ) | \leq c _ { 4 } ( 1 + | t | ^ { p } )$ , the compact embedding is applied.

Since

$$
J ^ {\prime \prime} (\theta) = \operatorname {i d} - g ^ {\prime} (0) (- \Delta) ^ {- 1},
$$

(g4) implies that $\theta$ is nondegenerate with finite index $\operatorname { i n d } ( J ^ { \prime \prime } ( \theta ) ) = i \geq 1$ .

# 2. Bifurcation

We know from Theorem 3.5.1 that on a Banach space, for a compact vector field with parameter $\lambda : F ( x , \lambda ) = x - \lambda T x + N ( x , \lambda )$ , where $T$ is compact, $\parallel N ( x , \lambda ) \parallel = \circ ( \parallel x \parallel )$ uniformly in $\lambda$ , the bifurcation point occurs at eigenvalues $\lambda$ with odd multiplicity. Generally speaking, the odd multiplicity condition cannot be removed. However, if $\forall \lambda \in R ^ { 1 }$ , $F ( \cdot , \lambda )$ is a potential operator, i.e., the differential of a certain functional with a parameter $\lambda$ , we have:

Theorem 5.1.37 Suppose that $H$ is a Hilbert space, and that $f \in C ^ { 2 } ( H , \mathbb { R } ^ { 1 } )$ satisfies $f ^ { \prime } ( \theta ) = \theta$ . If $\lambda _ { 0 }$ is an isolated eigenvalue of the self-adjoint operator $f ^ { \prime \prime } ( \theta )$ with finite multiplicity, then $( \theta , \lambda _ { 0 } )$ is a bifurcation point of the equation:

$$
F (u, \lambda) = f ^ {\prime} (u) - \lambda u = \theta . \tag {5.6}
$$

Let $\begin{array} { r } { f ( u ) = \frac { 1 } { 2 } ( L u , u ) + g ( u ) } \end{array}$ , where $g ( u ) = o ( \| u \| ^ { 2 } )$ , then $L = f ^ { \prime \prime } ( \theta )$ , and $f ^ { \prime } ( u ) = L u + G ( u )$ , with $G ( u ) = g ^ { \prime } ( u ) , G ( u ) = \circ ( \parallel u \parallel )$ as $\parallel u \parallel \to 0$ .

The proof depends upon the Lyapunov–Schmidt reduction. Let $\boldsymbol { X } \ =$ $\ker \left( L - \lambda _ { 0 } I \right)$ , with $\dim X = n$ ; and let $P , P ^ { \perp }$ be the orthogonal projections onto $X$ and $X ^ { \perp }$ , respectively. Then (5.6) is equivalent to a pair of equations

$$
\lambda_ {0} x + P G \left(x + x ^ {\perp}\right) = \lambda x, \tag {5.7}
$$

$$
L x ^ {\perp} + P ^ {\perp} G \left(x + x ^ {\perp}\right) = \lambda x ^ {\perp}, \tag {5.8}
$$

where $u = x + x ^ { \perp } , \ x \in X , \ x ^ { \perp } \in X ^ { \perp }$ . According to the IFT, equation (5.8) is uniquely solvable in a small bounded neighborhood $\boldsymbol { \mathcal { O } }$ of $( \lambda _ { 0 } , \theta ) \in R ^ { 1 } \times X$ , say $x ^ { \perp } = \varphi ( \lambda , x )$ for $( \lambda , x ) \in { \mathcal { O } }$ , where $\varphi \in C ^ { 1 } ( { \mathcal { O } } , X ^ { \bot } )$ . Substituting $x ^ { \perp } = \varphi ( \lambda , x )$ into (5.7), we have

$$
\lambda_ {0} x + P G (x + \varphi (\lambda , x)) = \lambda x, \tag {5.9}
$$

which is again an Euler–Lagrange equation on the finite-dimensional space $X$ . Indeed, let

$$
\begin{array}{l} J _ {\lambda} (x) = f (x + \varphi (\lambda , x)) - \frac {\lambda}{2} (\parallel x \parallel^ {2} + \parallel \varphi (\lambda , x) \parallel^ {2}) \\ = \frac {1}{2} (\lambda_ {0} - \lambda) \| x \| ^ {2} + \frac {1}{2} (L \varphi , \varphi) - \frac {\lambda}{2} \| \varphi \| ^ {2} + g (x + \varphi). \\ \end{array}
$$

It is easy to verify that (5.9) is the Euler–Lagrange equation for $J _ { \lambda }$ , and that $\| \varphi ( \lambda , x ) \| = \circ ( \| \ x \| )$ as $x \to \theta$ .

The problem is reduced to finding the critical points of $J _ { \lambda }$ near $x = \theta$ for fixed $\lambda$ near $\lambda _ { 0 }$ , where $J _ { \lambda } \in C ^ { 1 } ( \Omega _ { 1 } , R ^ { 1 } )$ , $\Omega _ { 1 }$ is a neighborhood of $\theta$ in $X$ .

If $( \theta , \lambda _ { 0 } )$ were not a bifurcation point, then there would be a small neighborhood $\Omega _ { 1 } \times ( \lambda _ { 0 } - \delta , \lambda _ { 0 } + \delta ) , \delta \ >$ $\delta \ > \ 0$ such that $\theta$ is the unique critical point in $\Omega _ { 1 }$ for $J _ { \lambda }$ as $| \lambda - \lambda _ { 0 } | < \delta$ . However, as $\lambda < \lambda _ { 0 }$ , $\theta$ is a local minimum point, $C _ { q } ( J _ { \lambda } , \theta ) = \delta _ { q 0 } G$ , but as $\lambda > \lambda _ { 0 }$ , $\theta$ is a local maximum point, $C _ { q } ( J _ { \lambda } , \theta ) = \delta _ { q n } G$ . This contradicts the invariance of critical groups under $C ^ { 1 }$ -perturbation on bounded neighborhoods of $\theta$ .

Note: From the degree point of view, there is no difference between $\lambda > \lambda _ { 0 }$ and $\lambda < \lambda _ { 0 }$ , except $n$ is odd. Compare with Krasnoselski’s theorem (Theorem 3.5.1).

# 3. Superlinear Elliptic Problem

We turn to the problem studied previously in Sect. 3.6 and Sect. 4.8.

Example 2.

$$
\left\{ \begin{array}{l} - \triangle u = g (x, u) \text {i n} \Omega , \\ u = 0 \text {o n} \partial \Omega , \end{array} \right. \tag {5.10}
$$

where $\Omega \subset R ^ { n }$ is a bounded domain with smooth boundary. Assume:

(g1) $\begin{array} { r } { | g ( x , t ) | \leq C ( 1 + | t | ^ { \alpha } ) ~ \alpha < \frac { n + 2 } { n - 2 } } \end{array}$ if $n \geq 3$ .   
(g2) $\exists \theta > 2$ , $\exists M > 0$ such that $0 < \theta G ( x , t ) \leq t g ( x , t ) \ \forall x \in \Omega$ , for $| t | \geq M$ , where $\begin{array} { r } { G ( x , t ) = \int _ { 0 } ^ { t } g ( x , s ) d s } \end{array}$ .   
(g3) $g \in C ^ { 1 } ( \overline { { \Omega } } \times \mathbb { R } ^ { 1 } )$ with $g ( x , 0 ) = g _ { t } ( x , 0 ) = 0$

Define

$$
J (u) = \int_ {\Omega} \left[ \frac {1}{2} | \nabla u | ^ {2} - G (x, u (x)) \right] d x.
$$

Lemma 5.1.38 ( $Z . Q$ . Wang) Under the assumptions of (g1), (g2) and (g3), there exists a constant $A > 0$ , such that

$$
J _ {a} \simeq S ^ {\infty}, t h e u n i t s p h e r e i n H _ {0} ^ {1} (\Omega)
$$

for $- a > A$ .

Proof. By ( $g _ { 2 }$ ), we have a constant $C > 0$ such that

$$
G (x, t) \geq C (| t | ^ {\theta} - 1) \forall t, | t | \geq M.
$$

Thus $\forall u \in S ^ { \infty }$ ,

$$
J (t u) \rightarrow - \infty \mathrm {a s} t \rightarrow + \infty .
$$

First we want to show that $\exists A > 0$ such that $\forall a < - A$ , if $J ( t u ) \le a$ , then $\begin{array} { r } { \frac { d } { d t } J ( t u ) < 0 } \end{array}$ .

In fact, set

$$
A = 2M|\Omega | \max_{(x,t)\in \overline{\Omega}\times [-M,M]}\left|g(x,t)\right| + 1  .
$$

If $\begin{array} { r } { J ( t u ) = \frac { t ^ { 2 } } { 2 } - \int _ { \Omega } G ( x , t u ( x ) ) d x \leq a } \end{array}$ , then

$$
\begin{array}{l} \frac {d}{d t} J (t u) = \left(J ^ {\prime} (t u), u\right) \\ = t - \int_ {\Omega} u (x) \cdot g (x, t u (x)) d x \\ \leq \frac {2}{t} \left\{\int_ {\Omega} G (x, t u (x)) d x - \frac {1}{2} \int_ {\Omega} t u (x) \cdot g (x, t u (x)) d x + a \right\} \\ \leq \frac {2}{t} \left\{\left(\frac {1}{\theta} - \frac {1}{2}\right) \int_ {| t u (x) | \geq M} t u (x) g (x, t u (x)) d x + (A - 1) + a \right\} \\ \leq \frac {2}{t} \left\{\left(\frac {1}{\theta} - \frac {1}{2}\right) C \theta \int_ {| t u (x) | \geq M} | t | ^ {\theta} | u (x) | ^ {\theta} d x - 1 \right\} <   0, \\ \end{array}
$$

as $a \ : < \ : - A$ . The implicit function theorem is employed to obtain a unique $T ( u ) \in C ( S ^ { \infty } , \mathbb { R } ^ { 1 } )$ such that

$$
J (T (u) u) = a \forall u \in S ^ {\infty}.
$$

by (g3), Next, we claim that $g ( x , 0 ) = g _ { t } ^ { \prime } ( x , 0 ) = 0$ $\parallel T ( u ) \parallel$ possesses a positive lower bound , $\begin{array} { r } { J ( t u ) = \frac { t ^ { 2 } } { 2 } - \circ ( t ^ { 2 } ) ~ \forall u \in S ^ { \infty } } \end{array}$ as $\epsilon > 0$ $| t |$ is small. . In fact, The conclusion follows.

Finally, let us define a deformation retract $\eta ~ : ~ [ 0 , 1 ] \times ( H \backslash B _ { \epsilon } ( \theta ) ) ~ $ $H \backslash B _ { \epsilon } ( \theta )$ , where $H = H _ { 0 } ^ { 1 } ( \Omega )$ and $B _ { \epsilon } ( \theta )$ is the $\epsilon$ -ball with center $\theta$ , by

$$
\eta (s, u) = (1 - s) u + s T (u) u \forall u \in H \backslash B _ {\epsilon} (\theta).
$$

This proves that $H \backslash B _ { \epsilon } ( \theta ) \simeq J _ { a }$ , i.e., $J _ { a } \simeq S ^ { \infty }$ .

Indeed, $J$ satisfies the (PS) condition (see e.g., Sect. 4.8.3, Example 2). It is easy to give a different proof of the conclusion we obtained previously, i.e., there is a nontrivial solution of (5.10). Indeed, $\theta$ is a critical point of $J$ , which is an isolated minimum. Therefore $C _ { q } ( J , \theta ) = \delta _ { q 0 } G$ .

It is known that $S ^ { \infty }$ is contractible.

The pair $( H _ { 0 } ^ { 1 } ( \Omega ) , J _ { a } )$ must be trivial, i.e.,

$$
H _ {q} (H _ {0} ^ {1} (\Omega), J _ {a}; G) = 0 \forall q.
$$

If there were no other critical points, then the Morse relation would be $1 = 0$ This is impossible.

In fact, from Sect. 4.8, Example 2, we have already known that there is a critical point via the mountain pass lemma, we shall see in the next section, equation (5.10) possesses at least three nontrivial solutions.

# 5.2 Minimax Principles (Revisited)

The minimax method was initiated by G. Birkhoff in his pioneering work on closed geodesics. It was successfully developed by Ljusternik and Schnirelmann in proving that on a closed surface with genus zero there are at least three closed geodesics. The method has been well developed by M. A. Krasnoselski, P. S. Palais, A. Ambrosetti and P. Rabinowitz etc.

# 5.2.1 A Minimax Principle

We have studied saddle points by the minimax method in Sect. 4.8. In this section we shall introduce a general minimax principle based on the deformation theorem, which contains various types of this method as special cases.

Definition 5.2.1 For a given function $f \in C ( M , R ^ { 1 } )$ satisfying the $( P S )$ condition on a Banach–Finsler manifold $M$ , for a given $a \in R ^ { 1 }$ , let $\Phi _ { a } ( f ) =$ $\{ \phi = \eta ( 1 , \cdot ) | \eta \in C ( [ 0 , 1 ] \times M , M )$ satisfy $\eta ( t , x ) = x$ , $\forall ( t , x ) \in ( \{ 0 \} \times M ) \cup$ $( [ 0 , 1 ] \times f _ { a } ) \}$ . A family of subsets $\mathcal { F }$ of $M$ is called $\Phi _ { a } ( f )$ -invariant if $\forall A \in$ ${ \mathcal { F } } \phi ( A ) \in { \mathcal { F } }$ , whenever $\phi = \Phi _ { a } ( f )$ .

Example 1. (Linking) Let $X$ be a Banach space, let $Q \subset X$ be a compact manifold with boundary $\partial Q$ and let $S \subset X$ be a closed subset of $X$ . Assume $\partial Q$ and $S$ link (see Definition 4.8.6). Let $f \in C ^ { 1 } ( X , R ^ { 1 } )$ satisfy the (PS) condition. Assume $a = \operatorname* { m a x } _ { \partial Q } f < \operatorname* { i n f } _ { S } f$ . We set the family $\mathcal { F } = \{ \psi ( Q ) | \psi \in$ $C ( Q , X )$ with $\psi | _ { \partial Q } = \mathrm { i d } _ { \partial Q } \}$ . Then $\mathcal { F }$ is $\Phi _ { a } ( f )$ -invariant.

In fact, for $A = \psi ( Q )$ , one has $\phi ( A ) = \phi \circ \psi ( Q ) , \forall \phi \in \Phi _ { a } ( f )$ , and then $\phi \circ \psi | _ { \partial Q } = i d | _ { \partial Q }$ , so is $\phi ( A ) \in { \mathcal { F } }$ .

Example 2. (Homology class) Let $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfy the (PS) condition. For a pair of real numbers $a < b$ , if $[ \sigma ] \in H _ { q } ( f _ { b } , f _ { a } ; G )$ is a nontrivial $q$ -relative homology class for some $q \in \mathbb N$ . Set

$$
\mathcal {F} = \left\{\left| \sigma \right| \mid \sigma \in [ \sigma ] \text {i s a s i n g u l a r} q \text {- c l o s e d} \right. \text {c h a i n} \left(f _ {b}, f _ {a}\right)
$$

$$
\left. \text {w i t h c o e f f i c i e n t g r o u p} G \right\},
$$

where $| \sigma |$ is the support of $\sigma$ . Then $\mathcal { F }$ is $\Phi _ { a } ( f )$ -invariant.

In fact, let $\phi = \eta ( 1 , \cdot )$ where $\eta \in C ( [ 0 , 1 ] \times M , M )$ satisfies $\eta ( t , x ) ~ =$ $x \forall ( t , x ) \in ( \{ 0 \} \times M ) \cup ( [ 0 , 1 ] \times f _ { a } )$ , then $\forall \sigma \in [ \sigma ] , \phi ( | \sigma | ) = | \phi ( \sigma ) |$ , and $\phi ( \sigma ) \in [ \sigma ]$ .

Theorem 5.2.2 (Minimax principle) Suppose that $M$ is a smooth Banach– Finsler manifold and that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfies the (PS) condition. Let $\mathcal { F }$ be a $\Phi _ { a } ( f )$ -invariant family of subsets of $M$ for some $a \in R ^ { 1 }$ . Set

$$
c = \inf  _ {A \in \mathcal {F}} \sup  _ {x \in A} f (x). \tag {5.11}
$$

If (1) c is finite, (2) $a < c$ , then c is a critical value of $f$ .

Proof. It is proved by contradiction. If $c$ is not a critical value, then there exists $\epsilon > 0$ such that $a < c - \epsilon , f ^ { - 1 } [ c - \epsilon , c + \epsilon ] \cap K = \emptyset$ , provided by the (PS) condition. Taking $A _ { 0 } \in { \mathcal { F } }$ such that sup $f ( x ) < c + \epsilon$ i.e., $A _ { 0 } \subset f _ { c + \epsilon }$ . ${ \boldsymbol { x } } \in A _ { 0 }$ According to the nontrivial interval theorem, $\exists$ a strong deformation retraction $\eta : [ 0 , 1 ] \times M \to M$ such that $\eta ( 0 , \cdot ) = \mathrm { i d }$ , $\eta ( t , \cdot ) | _ { f _ { c - \epsilon } } = \mathrm { i d } _ { f _ { c - \epsilon } } \forall t \in [ 0 , 1 ]$ , and $\eta ( 1 , f _ { c + \epsilon } ) \subset f _ { c - \epsilon }$ . Thus $\eta ( t , x ) = x \forall ( t , x ) \in ( \{ 0 \} \times M ) \cup ( \lfloor 0 , 1 \rfloor \times f _ { a } )$ , and then $\phi : = \eta ( 1 , \cdot ) \in \Phi _ { a } ( f )$ . Since $\mathcal { F }$ is $\Phi _ { a } ( f )$ -invariant, it follows that

$$
\inf  _ {A \in \mathcal {F}} \sup  _ {x \in A} f (x) <   c - \epsilon .
$$

This is a contradiction.

□

Thus, in Example 1, Since $Q$ is compact, $c < \infty$ . From the $S$ and $\partial Q$ link, we have $c > a$ . Theorem 5.2.2 implies Theorem 4.8.9.

Also, applying Theorem 5.2.2 to Example 2,

$$
c = \inf  _ {\sigma \in [ \sigma ]} \sup  _ {x \in | \sigma |} f (x) \tag {5.12}
$$

is a critical value of $f$ .

In fact, obviously $c$ is finite. We verify that $c$ is a critical value, if not, $c \leq a . \forall \epsilon > 0 , \exists \sigma \in [ \sigma ]$ $c \leq a$ , with $| \sigma | \subset f _ { c + \epsilon }$ , according to the second deformation theorem, there exists $\eta$ deforming $f _ { c + \epsilon }$ to $f _ { c }$ . Since $\eta ( 1 , \sigma ) \in [ \sigma ]$ and $| \eta ( \sigma ) | \subset$ $f _ { a }$ , $[ \sigma ]$ is trivial in $( f _ { b } , f _ { a } )$ . A contradiction.

What can we say about the critical point with critical value $c$ defined by equation (5.12)?

Theorem 5.2.3 Suppose that $M$ is a smooth Banach–Finsler manifold, and that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfies $\left( P S \right) _ { d } \forall d \in \left[ a , b \right]$ , where $a , b$ are regular values. If $[ \sigma ]$ is a nontrivial class in $H _ { q } ( f _ { b } , f _ { a } ; G )$ , and c is defined by (5.12). Then $c \in ( a , b )$ and $K \cap f ^ { - 1 } ( c ) \neq \emptyset$ . Moreover, if c is isolated and $K \cap f ^ { - 1 } ( c )$ consists of isolated critical points, then $\exists p \in K \cap f ^ { - 1 } ( c )$ such that

$$
C _ {q} (f, p) \neq 0.
$$

Proof. The first part of the conclusion has been proved. Since $c$ is an isolated critical value, i.e., $\exists \epsilon > 0$ such that $f ^ { - 1 } [ c - \epsilon , c + \epsilon ] \cap K = K \cap f ^ { - 1 } ( c )$ and $a < c - \epsilon$ .

It is sufficient to show $H _ { q } ( f _ { c - \epsilon } , f _ { c + \epsilon } ; G ) \neq 0$ , for then by Theorem 5.1.27, there must be a $p \in K \cap f ^ { - 1 } ( c )$ such that $C _ { q } ( f , p ) \neq 0$ . Now we prove it by contradiction. If $H _ { q } ( f _ { c + \epsilon } , f _ { c - \epsilon } ; G ) = 0$ , then by the long exact sequence

$$
\dots \longrightarrow H _ {q} (f _ {c - \epsilon}, f _ {a}; G) \xrightarrow {i _ {*}} H _ {q} (f _ {c + \epsilon}, f _ {a}; G) \xrightarrow {j _ {*}} H _ {q} (f _ {c + \epsilon}, f _ {c - \epsilon}; G) \longrightarrow \dots ,
$$

we have $\operatorname { I m } i _ { * } = H _ { q } ( f _ { c + \epsilon } , f _ { a } ; G )$ . This shows that the class $[ \sigma ]$ ] is again nontrivial in $H _ { q } ( f _ { c - \epsilon } , f _ { a } ; G )$ . But this contradicts the definition of $c$ . □

Combining Theorem 5.2.3 with the shifting theorem, we have:

Corollary 5.2.4 Let $M$ be a smooth Hilbert–Riemannian manifold. Suppose that $f \in C ^ { 2 } ( M , R ^ { 1 } )$ satisfies the assumptions of Theorem 5.2.3, and that $f ^ { \prime \prime } ( x )$ is a Fredholm operator $\forall x \in K \cap f ^ { - 1 } ( c )$ . Then $\exists p \in K \cap f ^ { - 1 } ( c )$ such that $\operatorname { i n d } ( f ^ { \prime \prime } ( p ) ) \leq q \leq \operatorname { i n d } ( f ^ { \prime \prime } ( p ) ) + \dim \ker f ^ { \prime \prime } ( p )$ .

Return to the mountain pass point, we have:

Corollary 5.2.5 Suppose that $M$ is a smooth Hilbert–Riemannian manifold and that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ . If $\exists p _ { 0 } , p _ { 1 } \in M$ such that

$$
c = \inf _ {l \in \Gamma} \sup _ {t \in [ 0, 1 ]} f \circ l (t) > \max \left\{f (p _ {0}), f (p _ {1}) \right\},
$$

where $\Gamma = \{ l \in C ( [ 0 , 1 ] , M ) | l ( i ) = p _ { i } , \ i = 0 , 1 \}$ . Assume $( P S ) _ { c }$ . If $K \cap f _ { c }$ consists of isolated points, then there exists a mountain pass point $p \in K \cap$ $f ^ { - 1 } ( c )$ (see Definition 5.1.19) i.e.,

$$
C _ {1} (f, p) \neq 0. \tag {5.13}
$$

# 5.2.2 Category and Ljusternik–Schnirelmann Multiplicity Theorem

The notion of category was introduced by Ljusternik and Schnirelmann. It is a topological invariant used in the estimate of the lower bound of the number of critical points.

Definition 5.2.6 Let $M$ be a topological space, $A \subset M$ be a closed subset. Set

$\mathrm { c a t } _ { M } ( A ) = \operatorname* { i n f } \{ m \in \mathbb { N } \cup \{ + \infty \}$ m contractible closed subsets of $M$ :

$F _ { 1 } , F _ { 2 } , \ldots , F _ { m }$ such that $A \subset \cup _ { i = 1 } ^ { m } F _ { i } \}$ .

A set $F$ is called contractible (in $M$ ) if $\exists \eta : [ 0 , 1 ] \times M \to M$ such that $\eta ( 0 , \cdot ) = \mathrm { i d } _ { M }$ and $\eta ( 1 , F ) =$ one-point set.

Example 1. If $C$ is a closed convex set in a Banach space $X$ , then $\mathrm { c a t } _ { X } ( C ) = 1$ In fact, pick any $p \in C$ , and set

$$
\eta (t, x) = (1 - t) x + t p.
$$

Thus $C$ is contractible.

Example 2. Let $S ^ { n }$ be the unit ball in $R ^ { n + 1 }$ . Then $\cot _ { S ^ { n } } ( S ^ { n } ) = 2$ .

hemispheres It is easy to see that $S _ { \pm } ^ { n } = \{ ( x ^ { \prime } , x _ { n + 1 } ) \in R ^ { n } \times R ^ { 1 } \ | \quad \| \ x ^ { \prime } \| | ^ { 2 } \ + x _ { n + 1 } ^ { 2 } = 1 , \ \pm x _ { n - 1 }$ $\cot _ { S ^ { n } } ( S ^ { n } ) \ \leq \ 2$ , because the upper and the lower $\pm x _ { n + 1 } \geq$ $0 \}$ ±  are contractible. On the other hand, $S ^ { n }$ is not contractible in itself, i.e., $\cot _ { S ^ { n } } ( S ^ { n } ) > 1$ . This can be shown by contradiction. Suppose there were a continuous $\eta : \left\lfloor 0 , 1 \right\rfloor \times S ^ { n } \longrightarrow S ^ { n }$ such that $\eta ( 0 , \cdot ) = \operatorname { i d } _ { S ^ { n } }$ and $\eta ( 1 , S ^ { n } ) = p \in S ^ { n }$ . Let $A : B _ { 1 } ^ { n + 1 } ( \theta ) \to S ^ { n }$ be defined by:

$$
A (x) = \left\{ \begin{array}{l l} - p & \text {i f} x = \theta  , \\ - \eta (1 - \|   x   \|, \frac {x}{\| x \|}) & \text {i f} x \in B _ {1} ^ {n + 1} (\theta) \backslash \{\theta \}  , \end{array} \right.
$$

then $A$ would be continuous. According to the Brouwer fixed-point theorem, $\exists p _ { 0 } \in S ^ { n }$ such that $A ( p _ { 0 } ) = p _ { 0 }$ , But by the definition of $A , A ( p _ { 0 } ) = - p _ { 0 }$ , thus $p _ { 0 } = \theta$ . This is a contradiction.

Example 3. $S ^ { \infty }$ is contractible, so is $\cot _ { S ^ { \infty } } ( S ^ { \infty } ) = 1$ . (see Sect. 3.3).

![](images/6d481a6685fd6006ef749bad810d3ae1b6d1b2cab196c7e4d8c78cfc65dbc766.jpg)  
Fig. 5.3.

Example 4. Let $T ^ { 2 }$ be the two-dimensional torus, i.e., $S ^ { 1 } \times S ^ { 1 }$ . Then $\cot _ { T ^ { 2 } } ( T ^ { 2 } ) = 3$ . This can be shown by Fig. 5.3.

The following fundamental properties for the category hold:

(1) $\operatorname { c a t } _ { M } ( A ) = 0 \Leftrightarrow A = \varnothing$ .   
(2) (Monotonicity) $A \subset B \Rightarrow \mathrm { c a t } _ { M } ( A ) \leq \mathrm { c a t } _ { M } ( B )$   
(3) (Subadditivity) $\mathrm { c a t } _ { M } ( A \cup B ) \leq \mathrm { c a t } _ { M } ( A ) + \mathrm { c a t } _ { M } ( B )$ .   
(4) (Deformation nondecreasing) If $\eta : [ 0 , 1 ] \times M \to M$ is continuous such that $\eta ( 0 , \cdot ) = \mathrm { i d } _ { M }$ , then $\mathrm { c a t } _ { M } ( A ) \leq \mathrm { c a t } _ { M } ( \eta ( 1 , A ) )$ .   
(5) (Continuity) If $A$ is compact, then there is a closed neighborhood $N$ of $A$ such that $A \subset \operatorname { i n t } ( N )$ and $\cot _ { M } ( A ) = \cot _ { M } ( N )$ .   
(6) (Normality) $\mathrm { c a p } _ { M } ( \{ p \} ) = 1 \forall p \in M$

In fact properties (1), (2), (3) and (6) follow from the definition directly. We shall prove (4) and (5) below.

Proof. Proof of (4). Let $B = \overline { { \eta ( 1 , { \cal A } ) } }$ and let $\cot _ { M } ( B ) = m$ , i.e., $\Finv$ closed contractible sets $F _ { 1 } , \ldots , F _ { m }$ of $M$ such that $B \subset \cup _ { i = 1 } ^ { m } F _ { i }$ . Let $G _ { i } = \eta ^ { - 1 } ( 1 , F _ { i } )$ , then $G _ { i }$ is closed and contractible, because $\exists \xi : [ 0 , 1 ] \times M \to M$ such that $\xi ( 0 , \dots ) = \mathrm { i d } _ { M }$ and $\xi ( 1 , F _ { i } ) = p _ { i } \in M$ . Now let $\varphi = \xi \circ \eta : | 0 , 1 | \times M \to M$ , we have $\varphi ( 0 , \cdot ) = \mathrm { i d } _ { M }$ and $\varphi ( 1 , G _ { i } ) = p _ { i } , \ i = 1 , 2 , \dots , m$ . Since $A \subset \cup _ { i = 1 } ^ { m } G _ { i }$ , property (4) is proved. □

In order to verify (5), we need a result on continuous extensions due to Hanner $\left[ \mathrm { H a n n } \right]$ and Palais [Pa 3]:

Let $X$ , Y be metrizable Banach manifolds, and let $A \subset X$ be a closed subset. Then for any continuous map $\phi : ( \{ 0 \} \times X ) \cup ( [ 0 , 1 ] \times A )  Y$ , there is a continuous extension ${ \widetilde { \phi } } : [ 0 , 1 ] \times X \to Y$ .

Proof. Since $A$ is compact, $\cot _ { M } ( A ) < \infty$ , i.e., $\Finv$ closed contractible sets $F _ { 1 } , \ldots , F _ { m }$ such that $A \subset \cup _ { i = 1 } ^ { m } F _ { i }$ . Since $\exists \varphi _ { i } : : [ 0 , 1 ] \times M \to M$ such that $\varphi _ { i } ( 0 , \dots ) \ = \ \mathrm { i d } _ { M }$ and $\varphi _ { i } ( 1 , F _ { i } ) \ = \ p _ { i } \ \in \ M$ , we apply the above continuous extension theorem to $\phi _ { i }$ and obtain $\widetilde { \varphi } _ { i } \ : \ [ 0 , 1 ] \times { M } \to { M }$ such that $\tilde { \varphi } _ { i } \big | _ { ( \{ \theta \} \times M ) \cup ( [ 0 , 1 ] \times F _ { i } ) \ } = \varphi _ { i } \big | _ { ( \{ \theta \} \times M ) \cup ( [ 0 , 1 ] \times F _ { i } ) }$ , $i ~ = ~ 1 , 2 , \ldots , m$ . Let $V _ { i }$ be a closed contractible neighborhood of $p _ { i }$ , then $U _ { i } = \widetilde { \varphi } _ { i } ^ { - 1 } ( 1 , V _ { i } )$ is closed and contractible, because $\tilde { \varphi } _ { i } ^ { - 1 } ( 0 , V _ { i } ) = V _ { i }$ . Thus $N = \cup _ { i = 1 } ^ { m } U _ { i }$ is a closed neighborhood of $A$ , we obtain $m = \mathrm { c a t } _ { M } ( A ) \leq \mathrm { c a t } _ { M } ( N ) \leq m$ . □

As a consequence of the additivity and normality, we have

(7) If $\cot _ { M } ( A ) = m$ , then $\sharp A \geq m$ , i.e., there are at least $m$ distinct points in $A$ .

The main theorem of this subsection is the following multiplicity theorem:

Theorem 5.2.7 (Ljusternik–Schnirelman theorem) Let $M$ be a smooth Banach–Finsler manifold. Suppose that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ is a function bounded from below, satisfying the (PS) condition. Then $f$ has a least $\operatorname { c a t } _ { M } ( M )$ critical points.

This theorem is based on the following version of the deformation theorem:

Theorem 5.2.8 Let $M$ be a smooth Banach–Finsler manifold. Suppose $f \in$ $C ^ { 1 } ( M , R ^ { 1 } )$ . Let $N ^ { \prime } \subset N$ be two closed neighborhoods satisfying

$$
\operatorname {d i s t} \left(N ^ {\prime}, \partial N\right) \geq \frac {7}{8} \delta , \delta > 0.
$$

Suppose that there are positive constants b and , such that

$$
\| d f (x) \| \geq b \forall x \in f _ {c + \epsilon} \backslash \left(f _ {c - \epsilon} \cup N ^ {\prime}\right),
$$

$$
0 <   \bar {\epsilon} <   3 \min  \left\{\frac {1}{4} \delta b ^ {2}, \frac {1}{8} \delta b \right\}.
$$

Then for any $\begin{array} { r } { 0 < \epsilon < \frac { \epsilon } { 2 } } \end{array}$ , there exists $\eta \in C ( [ 0 , 1 ] \times M , M )$ satisfying

(1) $\eta ( 0 , \cdot ) = \mathrm { i d }$ ,   
(2) $\eta ( t , \cdot ) | _ { C f ^ { - 1 } [ c - \overline { { { \epsilon } } } , c + \overline { { { \epsilon } } } ] } = i d | _ { C f ^ { - 1 } [ c - \overline { { { \epsilon } } } , c + \overline { { { \epsilon } } } ] }$   
(3) $\eta ( t , \cdot ) : M \to M$ − is a homeomorphism $\forall t \in [ 0 , 1 ]$   
(4) $\eta ( 1 , f _ { c + \epsilon } \backslash N ) \subset f _ { c - \epsilon }$   
(5) $f \circ \eta ( t , x )$ is nonincreasing in $t \ \forall ( t , x ) \in [ 0 , 1 ] \times M$

Proof. Define a smooth function:

$$
p (s) = \left\{ \begin{array}{l l} 0 & \text {f o r} s \notin [ c - \overline {{\epsilon}}, c + \overline {{\epsilon}} ], \\ 1 & \text {f o r} s \in [ c - \epsilon , c + \epsilon ], \end{array} \right.
$$

with $0 \leq p ( s ) \leq 1$ . Let $A = \overline { { M \backslash ( N ^ { \prime } ) } } _ { \frac { \delta } { 8 } }$ -) δ8 , and B = N - be two closed subsets. Let $B = N ^ { \prime }$

$$
g (x) = \frac {\operatorname {d i s t} (x , B)}{\operatorname {d i s t} (x , A) + \operatorname {d i s t} (x , B)}.
$$

We see $0 \leq g \leq 1 , g = 0$ on $N ^ { \prime }$ and $g = 1$ outside $( N ^ { \prime } ) _ { \frac { \delta } { 8 } } .$ Define

$$
q (s) = \left\{ \begin{array}{l l} 1 & 0 \leq s \leq 1  , \\ \frac {1}{s} & s \geq 1  . \end{array} \right.
$$

Letting $X$ be a pseudo-gradient vector field of $f$ , we define

$$
V (x) = - g (x) p (f (x)) q (\parallel X (x) \parallel) X (x).
$$

Thus $V \in C ^ { 1 - 0 }$ , and $\parallel V ( x ) \parallel \leq 1$ . Consider the ODE:

$$
\left\{ \begin{array}{l} \dot {\sigma} (t) = V (\sigma (t)) \\ \sigma (0) = x _ {0} \forall x _ {0} \in M  . \end{array} \right.
$$

Since $V$ is bounded, the global existence and uniqueness of the flow $\sigma ( t )$ on $R ^ { 1 }$ are known. Let

$$
\eta (t, x) = \sigma (t), \text {w i t h} \sigma (0) = x.
$$

Then $\eta \in C ( [ 0 , 1 ] \times M , M )$ satisfies (1), (2), (3) and (5). It remains to verify (4). From (5), it is sufficient to verify (4) for $x \ \in \ f _ { c + \epsilon } \backslash ( f _ { c - \epsilon } \cup N )$ . More precisely, we shall prove:

$$
f \circ \eta \left(\frac {3}{4} \delta , x\right) \leq c - \epsilon \forall x \in f _ {c + \epsilon} \backslash \left(f _ {c - \epsilon} \cup N\right).
$$

Indeed, if not, $\exists x \in f ^ { - 1 } [ c - \epsilon , c + \epsilon ] \backslash N$ such that

$$
c - \epsilon <   f \circ \eta (t, x) \leq c + \epsilon \forall t \in \left(0, \frac {3}{4} \delta\right),
$$

it follows that

$$
p (f \circ \eta (t, x)) = 1.
$$

Noticing that $\parallel \eta ( t , x ) - \eta ( 0 , x ) \parallel \leq t$ , we have

$$
\begin{array}{l} \operatorname {d i s t} \left(\eta (t, x), \left(N ^ {\prime}\right) _ {\frac {s}{2 0}}\right) \geq \operatorname {d i s t} \left(\eta (0, x), \left(N ^ {\prime}\right) _ {\frac {s}{2 0}}\right) - t \\ > \left(\frac {7}{8} - \frac {1}{8} - \frac {3}{4}\right) \delta = 0, \\ \end{array}
$$

so $g \circ \eta ( t , x ) = 1$ . Now,

$$
\begin{array}{l} \frac {d}{d t} f \circ \eta (t, x) = \langle f ^ {\prime} (\eta (t, x)), \dot {\eta} (t, x) \rangle \\ = - q \left(\left\| X (\eta (t, x)) \right\|\right) \langle f ^ {\prime} (\eta (t, x)), X (\eta (t, x)) \rangle \\ \leq - q (\| X (\eta (t, x)) \|) \| f ^ {\prime} (\eta (t, x)) \| ^ {2}. \\ \end{array}
$$

If $\| ~ X ( \eta ( t , x ) ) ~ \| \leq 1$ , then we have

$$
\frac {d}{d t} f \circ \eta (t, x) \leq - \| f ^ {\prime} (\eta (t, x)) \| ^ {2} \leq - b ^ {2}.
$$

Otherwise,

$$
\begin{array}{l} \frac {d}{d t} f \circ \eta (t, x) \leq - \| f ^ {\prime} (\eta (t, x)) \| ^ {2} / \| X (\eta (t, x)) \| \\ \leq - \frac {1}{2} \| f ^ {\prime} (\eta (t, x)) \| <   - \frac {1}{2} b. \\ \end{array}
$$

In summary,

$$
\frac {d}{d t} f \circ \eta (t, x) \leq - \min  \left\{b ^ {2}, \frac {1}{2} b \right\}.
$$

Thus,

$$
\begin{array}{l} f \circ \eta (t, x) \leq f \circ \eta (0, x) - \frac {3}{4} \delta \min  \left\{b ^ {2}, \frac {1}{2} b \right\} \\ \leq c + \epsilon - 3 \delta \min  \left\{\frac {1}{4} b ^ {2}, \frac {1}{8} b \right\} <   c - \epsilon . \\ \end{array}
$$

This is contradiction.

We notice that in this theorem no (PS) condition is assumed, but the existence of $N$ is attributed to the (PS) condition.

Corollary 5.2.9 (First deformation lemma) Let $M$ be a smooth Banach– Finsler manifold. Suppose that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfies the $( P S ) _ { c }$ condition. Assume that $N$ is a closed neighborhood of $K _ { c } = K \cap f ^ { - 1 } ( c )$ . Then there exist a continuous map $\eta : [ 0 , 1 ] \times M \to M$ and constants $\overline { { \epsilon } } > \epsilon > 0$ such that (1)–(5) hold.

With the aid of category, we introduce families of subsets of $M$ . Let

$$
\mathcal {F} _ {k} = \{A | \text {c l o s e d s u b s e t o f} M \text {w i t h} \operatorname {c a t} _ {M} (A) \geq k \}, \forall k \in N.
$$

If $f \in C ^ { 1 } ( M , R ^ { 1 } )$ is bounded from below, then $\mathcal { F } _ { k }$ is $\Phi _ { a } ( f )$ -invariant with $a < \operatorname { i n f } _ { M } f$ , provided by the deformation nondecreasing property (4) of the category. According to minimax principle

$$
c _ {k} = \inf  _ {A \in \mathcal {F} _ {k}} \sup  _ {x \in A} f (x) \quad k = 1, 2, \dots , \tag {5.14}
$$

are critical values whenever $f$ satisfies the (PS) condition.

Since $\mathcal { F } _ { k + 1 } \subset \mathcal { F } _ { k }$ , we have $c _ { k } \leq c _ { k + 1 } , k = 1 , 2 , . . . .$ . Generally, we do not have $c _ { k } < c _ { k + 1 }$ . However, in order to get the multiplicity of critical points, we need:

Lemma 5.2.10 Let $M$ be a smooth Banach–Finsler manifold. Suppose that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ is bounded from below and satisfies the $( P S ) _ { c }$ condition, where $c = c _ { k + 1 } = \cdot \cdot \cdot = c _ { k + m }$ , are defined by (5.14), then $\cot _ { M } ( K _ { c } ) \ge m$ .

Proof. Since $K _ { c }$ is compact, provided by the continuity of the category, we have a closed neighborhood $N$ of $K _ { c }$ such that $\mathrm { c a t } _ { M } ( N ) = \mathrm { c a t } _ { M } ( K _ { c } )$ .

From the definition of $c , \forall \epsilon > 0 \exists$ a closed subset $A _ { \epsilon } \subset f _ { c + \epsilon }$ such that $\cot _ { M } ( A _ { \epsilon } ) \geq k + m$ . Applying first deformation lemma, $\exists \eta : [ 0 , 1 ] \times M \to M$ satisfying (1)–(5) in Theorem 5.2.8. Let $\phi = \eta ( 1 , \cdot )$ , then

$$
\phi \left(A _ {\epsilon} \backslash^ {\circ} N\right) \subset \phi \left(f _ {c + \epsilon} \backslash^ {\circ} N\right) \subset f _ {c - \epsilon}.
$$

It follows that

$$
\begin{array}{l} k + m \leq \operatorname {c a t} _ {M} (A _ {\epsilon}) \leq \operatorname {c a t} _ {M} (A \backslash \overset {\circ} {N}) + \operatorname {c a t} _ {M} (N) \\ \leq \operatorname {c a t} _ {M} (\overline {{\phi (A \backslash N)}}) + \operatorname {c a t} _ {M} (K _ {c}) \\ \leq \operatorname {c a t} _ {M} \left(f _ {c - \epsilon}\right) + \operatorname {c a t} _ {M} \left(K _ {c}\right) \\ \leq k + \operatorname {c a t} _ {M} \left(K _ {c}\right). \\ \end{array}
$$

Therefore

$$
\operatorname {c a t} _ {M} \left(K _ {c}\right) \geq m.
$$

Proof. Proof of Theorem 5.2.7: Define $c _ { k } , k = 1 , 2 , \cdot \cdot \cdot \mathrm { c a t } _ { M } ( M )$ ; by (5.14), we have $c _ { 1 } \leq c _ { 2 } \leq . . . \leq c _ { k } \leq . . . .$ . According to Lemma 5.2.10, the multiplicity of $c = c _ { k }$ is dominated by $\mathrm { c a t } _ { M } ( K _ { c } )$ , and then by the number of critical points in $K _ { c }$ . Combining with the fact that distinct critical values correspond to distinct critical points, the conclusion is proved. □

# 5.2.3 Cap Product

It is natural to ask: If we have two singular homology classes $[ \tau _ { 1 } ] , [ \tau _ { 2 } ] \ \in$ $H _ { * } ( f _ { b } , f _ { a } ; G )$ , both nontrivial, and if $c _ { 1 } , c _ { 2 }$ are defined in the same way as

in (5.12), are there two distinct critical points? Generally speaking, no, but with the aid of the notion of the cap product, the answer is yes if $\left[ \tau _ { 1 } \right]$ and $\left[ \tau _ { 2 } \right]$ are related by a cap product.

Noticing that there is a ring structure on the cohomology groups. let $X$ be a topological space and $C _ { q } ( X , G )$ be the set of all singular $q$ -chains in $X$ with coefficient-group $G$ .

Let $C ^ { q } ( X , G )$ be the set of all singular $q$ -cochains, i.e., ${ \mathrm { H o m } } ( C _ { q } ( X , G ) , G )$ . It is a module, and then the duality [,] is a bilinear form over $C _ { q } ( X , G ) \times$ $C ^ { q } ( X , G )$ .

For a topological pair $( X , Y )$ , let

$$
C ^ {q} (X, Y; G) = \{c \in C ^ {q} (X, G) | [ \sigma , c ] = 0 \forall \sigma \in C _ {q} (Y, G) \}.
$$

Under the duality, the coboundary operator $\delta _ { q } : C ^ { q - 1 } ( X , G ) \to C ^ { q } ( X , G )$ is defined to be the dual of the boundary operator $\partial _ { q } : C _ { q } ( X , G ) \to C _ { q - 1 } ( X , G )$ :

$$
[ \partial_ {q} \sigma , c ] = [ \sigma , \delta_ {q} c ] \quad \forall \sigma \in C _ {q} (X, G), \forall c \in C ^ {q - 1} (X, G).
$$

The qth singular cohomology group $H ^ { q } ( X , G ) = \ker \delta _ { q + 1 } / \mathrm { I m } \delta _ { q }$

A cup product structure on the graded singular cochains $\oplus _ { q = 0 } ^ { \infty } C ^ { q } ( X , G )$ is defined as follows:

$$
[ \sigma , c \cup d ] = [ \sigma \circ \lambda_ {q}, c ] \cdot [ \sigma \circ \mu_ {q}, d ]
$$

$\forall c \in C ^ { p } ( X , G ) , \forall d \in C ^ { q } ( X , G )$ and $\forall \sigma \in C _ { p + q } ( X , G )$ , where $\lambda _ { p } : \triangle _ { p } $ $\triangle _ { p + q }$ and $\mu _ { q } : \triangle _ { q } \  \ \triangle _ { p + q }$ ∀read as: $\begin{array} { r } { \lambda _ { p } ( \sum _ { i = 0 } ^ { p } x _ { i } e _ { i } ) ~ = ~ \sum _ { i = 0 } ^ { p } x _ { i } e _ { i } } \end{array}$  →, and $\textstyle \mu _ { q } ( \sum _ { i = 0 } ^ { q } x _ { i } e _ { i } ) = \sum _ { i = 0 } ^ { q } x _ { i } e _ { i + p }$ , and $\triangle _ { j }$ i=0 is the standard $j$ i=0 -simplex. It is well defined on $H ^ { * } ( X , G )$ , and makes the latter a graded algebra.

The cup product is bilinear, associative, and possesses the unit element, i.e., the 0-cochain 1, which is defined by $[ x , 1 ] = e$ , the unit element of $G , \forall x \in$ $X$ .

The cap product is defined to be the dual operator of the cup product. For a topological pair $( X , Y )$ , one defines $\cap : C _ { p + q } ( X , Y ; G ) \times C ^ { p } ( X , G ) $ $C _ { q } ( X , Y ; G )$ by $[ \sigma \cap c , d ] = [ \sigma , c \cup d ] \forall \sigma \in C _ { p + q } ( X , Y ; G )$ , $\forall c \in C ^ { p } ( X ; G )$ , d $C ^ { q } ( X , Y ; G )$ . Since one has

$$
\partial (\sigma \cap c) = (- 1) ^ {p} (\partial \sigma \cap c - \sigma \cap \delta c) \quad \forall \sigma \in C _ {p + q} (X, Y; G) \forall c \in C ^ {p} (X, G),
$$

the cap product is also induced to relative singular homology groups:

$$
\cap : H _ {p + q} (X, Y; G) \times H ^ {p} (X, G) \to H _ {q} (X, Y; G).
$$

Definition 5.2.11 Let $( X , Y )$ be a topological pair. $[ \tau _ { 1 } ] , [ \tau _ { 2 } ] \in H _ { * } ( X , Y ; G )$ are nontrivial classes. $\left[ \tau _ { 1 } \right]$ is called subordinate to $\left[ \tau _ { 2 } \right]$ if $\exists \omega \in H ^ { * } ( X ; G )$ , dim $\omega > 0$ such that

$$
[ \tau_ {1} ] = [ \tau_ {2} ] \cap \omega ,
$$

where ∩ is the cap product. In this case we write $[ \tau _ { 1 } ] < [ \tau _ { 2 } ]$ .

Lemma 5.2.12 Let $B \subset X$ be a contractible set, and $[ \omega ] \in H ^ { q } ( X ; G )$ with $q > 0$ . Then $\exists \hat { \omega } \in [ \omega ]$ such that $| \hat { \omega } | \cap B = \varnothing$ .

Proof. Since $B$ is contractible, there is a closed contractible neighborhood $N$ of $B$ (property (5) of the category). For all closed singular $q$ -chains $\sigma$ , by adding a $q + 1$ singular chain $\tau$ with $| \tau | \subset N$ and $\vert \sigma \vert \cap N \subset \vert \partial \tau \vert$ , after subdivision, we find a closed singular $q$ -chain $\sigma _ { N }$ satisfying:

(1) σ − σN = ∂τ, τ ∈ Cq+1(X, G) and |τ | ⊂ N ,   
(2) $| \sigma _ { N } | \cap B = \emptyset$ .

![](images/4e3571297ecf38d46f07120b5501087bd257a98c88ba245d33f38946d6f1d619.jpg)  
Fig. 5.4.

For any $\omega \in [ \omega ]$ , define $\hat { \omega }$ by

$$
[ \sigma , \hat {\omega} ] = [ \sigma_ {N}, \omega ],
$$

then we have:

(1) $\hat { \omega } \in [ \omega ]$ . Because

$$
\begin{array}{l} [ \sigma , \hat {\omega} ] = [ \sigma_ {N}, \omega ] \\ = [ \sigma , \omega ] - [ \partial \tau , \omega ] \\ = [ \sigma , \omega ] \quad \forall \sigma \in C _ {q} (X, G). \\ \end{array}
$$

(2) $| \hat { \omega } | \cap B = \varnothing$ . Because $\forall \sigma \in C _ { q } ( X , G )$ with $| \sigma | \subset B$ , we have $| \sigma _ { N } | = \emptyset$ . Therefore $[ \sigma , \hat { \omega } ] = [ \sigma _ { N } , \omega ] = 0$ . □

Theorem 5.2.13 Let $M$ be a smooth Banach–Finsler manifold. Suppose that $f \in C ^ { 1 } ( M , R ^ { 1 } )$ with regular values $a < b$ . Assume that $[ \tau _ { 1 } ] < [ \tau _ { 2 } ] < \cdot \cdot \cdot < [ \tau _ { m } ]$ are nontrivial homology classes in $H _ { * } ( f _ { b } , f _ { a } ; G )$ . Let

$$
c _ {i} = \inf  _ {\tau_ {i} \in [ \tau_ {i} ]} \sup  _ {x \in | \tau_ {i} |} f (x), i = 1, 2, \dots , m.
$$

If $c = c _ { 1 } = c _ { 2 } = \cdot \cdot \cdot = c _ { m }$ , and if $f$ satisfies the $( P S ) _ { c }$ condition, then we have

$$
\operatorname {c a t} _ {M} \left(K _ {c}\right) \geq m.
$$

Proof. Since $K _ { c }$ is compact, we may choose neighborhoods $N ^ { \prime } \subset \overline { { N } } ^ { \prime } \subset N$ of $K _ { c }$ satisfying the following conditions:

(1) $\mathrm { c a t } _ { M } ( N ) = \mathrm { c a t } _ { M } ( K _ { c } )$ ;

(2) there exist constants $1 < \epsilon < \overline { { \epsilon } } < \operatorname* { m i n } { \{ b - c , c - a \} }$ and $\eta \in C ( M , M )$ such that

$$
\begin{array}{l} \eta | _ {f _ {c - \bar {\epsilon}}} = \mathrm {i d} _ {f _ {c - \epsilon}} \\ \eta \sim \mathrm {i d} , \\ \end{array}
$$

and

$$
\eta \left(f _ {c + \epsilon} \backslash N ^ {\prime}\right) \subset f _ {c - \epsilon},
$$

according to the first deformation theorem.

We prove our theorem by contradiction. If $\cot _ { M } ( K _ { c } ) \leq m - 1$ , then there are $( m - 1 )$ contractible closed sets $\{ B _ { j } \} _ { 2 } ^ { m }$ that cover $N$ : $\cup _ { j = 2 } ^ { m } B _ { j } \supset N$ . Since $[ \tau _ { 1 } ] < [ \tau _ { 2 } ] < \cdot \cdot \cdot < [ \tau _ { m } ]$ , there exist $\omega _ { 2 } , \omega _ { 3 } , \ldots , \omega _ { m } \in H ^ { * } ( f _ { b } , G )$ , with dim $\omega _ { j } > 0$ such that

$$
[ \tau_ {j - 1} ] = [ \tau_ {j} ] \cap \omega_ {j}, \quad j = 2, 3, \dots , m.
$$

Provided by Lemma 5.2.12, we always may choose $\hat { \omega } _ { j } \in \omega _ { j }$ , with $| \hat { \omega } _ { j } | \cap B _ { j } =$ ? $, \ j = 2 , 3 , . . . , m$ .

Again we choose $\tau \in [ \tau _ { m } ]$ with $| \tau | \subset f _ { c + \epsilon }$ . Subdividing $^ { \prime }$ into $\tau = \sigma _ { 1 } +$ $\sigma _ { 2 } + \cdots + \sigma _ { m }$ such that

$$
\left| \sigma_ {1} \right| \subset f _ {c + \epsilon} \backslash N ^ {\prime}, \text {a n d} \left| \sigma_ {j} \right| \subset B _ {j}, j = 2, 3, \dots , m,
$$

one has

$$
\begin{array}{l} \tau^ {\prime} = \tau \cap (\hat {\omega} _ {2} \cup \dots \cup \hat {\omega} _ {m}) \\ = \sigma_ {1} \cap \left(\hat {\omega} _ {2} \cup \dots \cup \hat {\omega} _ {m}\right). \\ \end{array}
$$

Hence $\vert \tau ^ { \prime } \vert \subset f _ { c + \epsilon } \backslash N ^ { \prime }$ and $\eta ( | \tau ^ { \prime } | ) \subset f _ { c - \epsilon }$ . However, $\tau ^ { \prime } \in [ \tau _ { 1 } ]$ ; therefore, $\eta ( \tau ^ { \prime } ) \in$ $\left[ \tau _ { 1 } \right]$ , which implies that $c _ { 1 } \leq c - \epsilon$ . This is a contradiction. □

Definition 5.2.14 For a pair of topological spaces $( X , Y )$ , let

$$
L (X, Y; G) = \sup  \{l \in \mathbb {Z} _ {+} | \exists \text {n o n t r i v i a l c l a s s e s} [ \tau_ {1} ], [ \tau_ {2} ], \dots , [ \tau_ {l} ] \in
$$

$$
H _ {*} (X, Y; G) \text {s u c h t h a t} [ \tau_ {1} ] <   \dots <   [ \tau_ {l} ] \}.
$$

$L ( X , Y ; G )$ measures the length of the chain of subordinate nontrivial singular homology classes. It is directly related to the notion of cup length:

$$
C L (X, Y; G) = \sup  \{l \in \mathbb {Z} _ {+} | \exists c _ {0} \in H ^ {*} (X, Y; G), \exists c _ {1}, \dots , c _ {l} \in H ^ {*} (X; G)
$$

such that $\dim ( c _ { i } ) > 0 , i = 1 , 2 , . . . , l .$ , and c0 ∪ · · · ∪ cl = 0} .

It is known from algebraic topology that $\operatorname { c a t } _ { X } ( X ) \geq C L ( X , \varnothing ; G )$ , i.e., the cup length provides a lower bound estimate for the category; we have $\mathrm { c a t } _ { P ^ { n } } ( P ^ { n } ) = n + 1$ , $\mathrm { c a t } _ { T ^ { n } } ( T ^ { n } ) = n + 1$ , where $\mathrm { P } ^ { n }$ is the $n$ - dimensional real projective space, and $T ^ { n }$ is the $n$ -torus.

# 5.2.4 Index Theorem

Symmetric functions, or more generally, a function, which is invariant under some G-group action, may have more critical points. This is due to the fact that the underlying space quotient by the G-group action has more complicated topology. In this subsection we shall deal with this problem.

Let us recall the proof of the Ljusterik–Schnirelmann multiplicity theorem, in which only the fundamental properties (1)–(6) of the category were used. We are inspired to extend an abstract theory based on these properties for $G$ -group action invariant functions.

Let $M$ be a Banach–Finsler manifold with a compact group action $G$ . Let $\Sigma$ be the set of all $G$ -invariant closed subsets of $M$ , and $\mathcal { H }$ be the set of all $G$ -equivariant continuous mappings from $M$ into itself, i.e., $h \in { \mathcal { H } }$ if and only if $h \in C ( M , M )$ and $h \circ g = g \circ h \ \forall g \in G$ .

Definition 5.2.15 An index $( \Sigma , \mathcal { H } , i )$ with respect to $G$ is defined by $i : \Sigma $ $\mathbb { N } \cup \lbrace + \infty \rbrace$ , which satisfies:

(1) $i ( A ) = 0 \Leftrightarrow A = \emptyset , \forall A \in \Sigma$ .   
(2) (Monotonicity) $A \subset B \Rightarrow i ( A ) \leq i ( B )$ , $\forall A , B \in \Sigma$ .   
(3) (Subadditivity) $i ( A \cup B ) \leq i ( A ) + i ( B )$ , $\forall A , B \in \Sigma$   
(4) (Deformation nondecreasing) If $\eta : [ 0 , 1 ] \times \underline { { { M } } } \to M$ satisfies $\eta ( t , \cdot ) ~ \in$ $\mathcal { H } \forall t \in [ 0 , 1 ]$ , and $\eta ( 0 , \cdot ) = i d _ { M }$ , then $i ( A ) \leq i ( \eta ( 1 , A ) )$ , $\forall A \in \Sigma$ .   
(5) (Continuity) If $A \in \Sigma$ is compact, then there is a neighborhood $N \in \Sigma$ of $A$ such that $A \subset \operatorname { i n t } ( N )$ and $i ( A ) = i ( N )$ .   
(6) (Normality) $i ( [ p ] ) = 1 \ \forall p \not \in \operatorname { F i x } _ { G }$ where $[ p ] = \{ g \cdot p | \ g \in G \}$ and $\operatorname { F i x } _ { G } =$ $\{ x \in M \}$ $\boldsymbol { \jmath } \cdot \boldsymbol { x } = \boldsymbol { x } \forall g \in G \}$ is the fixed-point set of $G$ , if $G \neq \{ e \}$ .

Example 1. If $G = \{ e \}$ , then category $( \Sigma , { \mathcal { H } } , { \mathrm { c a t } } _ { M } )$ is an index.

Example 2. Recall the genus defined in Sect. 3.3, where $M$ is a Banach space, $G = \mathbb { Z } _ { 2 } = \{ I , - I \}$ , i.e., $I x = x , ( - I ) x = - x \forall x \in M$ ,. Thus $\Sigma =$ the set of all closed symmetric subset, $\mathcal { H }$ is the set of odd continuous mappings.

Let

$$
\gamma (A) = \left\{ \begin{array}{l} 0 \quad \text {i f} A = \emptyset  , \\ \inf  \{k \in \mathbb {N} | \exists \text {a n o d d m a p} \phi \in C (A, R ^ {k} \backslash \{\theta \}) \}, \\ + \infty \quad \text {i f n o s u c h o d d m a p}. \end{array} \right.
$$

Then genus $( \Sigma , { \mathcal { H } } , \gamma )$ is an index with respect to $\mathbb { Z } _ { 2 }$

(Verifications) By definition, (1) and (2) are trivial.

(3) (Subadditivity) Set $\gamma ( A ) = n , \ \gamma ( B ) = m$ ; we may assume that both $n , m < \infty$ . This means that $\exists \varphi : A  R ^ { n } \backslash \{ \theta \} \ \exists \psi : B  R ^ { m } \backslash \{ \theta \}$ both are odd and continuous. By Tietze’s theorem $\exists \widetilde { \varphi } \in C ( M , R ^ { n } ) , \ \exists \widetilde { \psi } \in C ( M , R ^ { m } )$ $\exists { \widetilde { \varphi } } \in C ( M , R ^ { n } )$ such that ${ \mathcal { \tilde { S } } } | _ { A } = \varphi$ , ${ \tilde { \psi } } | _ { B } = \psi$ . Without loss of generality, we may assume that $\widetilde { \varphi }$ and $\widetilde { \psi }$ are odd. Define $f ( x ) = ( \widetilde { \varphi } ( x ) , \widetilde { \psi } ( x ) )$ , then $f \in C ( M , R ^ { n + m } )$ is odd !and $f ( A \cup B ) \subset R ^ { n + m } \backslash \{ \theta \}$ !. This implies that $\gamma ( A \cup B ) \leq \gamma ( A ) + \gamma ( B )$ .

(4) (Deformation nondecreasing) Suppose $\underline { { { \eta } } } : \lfloor 0 , 1 \rfloor \times M \to M$ is continuous and odd, and satisfies $\eta ( 0 , \cdot ) = \mathrm { i d }$ with $\gamma ( \eta ( 1 , A ) ) = n$ , i.e., $\exists \varphi : { \overline { { \eta ( 1 , A ) } } } \to$ $R ^ { n } \backslash \{ \theta \}$ is continuous and odd. Let $\psi = \varphi \circ \eta ( 1 , \cdot )$ then $\psi : A  R ^ { n } \backslash \{ \theta \}$ i s continuous and odd. Therefore $\gamma ( A ) \leq n$ .

(5) (Continuity) Let $\gamma ( A ) = n$ . From (2), with no loss of generality, we may assume $n < + \infty$ . There exists an odd continuous $\varphi : A  R ^ { n } \backslash \{ \theta \}$ . By Tietze’s theorem, there exists an odd continuous mapping $\widetilde \varphi : M \to R ^ { n }$ , with ${ \widetilde { \varphi } } | _ { A } = \varphi$ .

Since $A$ is compact and $\theta \not \in \widetilde { \varphi } ( A )$ , $\exists \delta > 0$ such that $\theta \not \in \widetilde { \varphi } ( N )$ , where $N = \overline { { A _ { \delta } } }$ , the closure of the $\delta$ !-neighborhood of $A$ , i.e.,

$$
\widetilde {\varphi}: N \to R ^ {n} \backslash \{\theta \}.
$$

Combining with the monotonicity, we obtain

$$
n = \gamma (A) \leq \gamma (N) \leq n
$$

(6) (Normality) Note that $\operatorname { F i x } _ { G } = \{ \theta \}$ , if $p \neq \theta$ , then $[ p ] = \{ p , - p \}$ . We define $\phi ( \pm p ) = \pm 1$ ; it follows that $\gamma ( [ p ] ) = 1$ .

Moreover, applying the Borsuk–Ulam theorem in Sect. 3.2, we have computed $\gamma ( S ^ { n - 1 } ) = n$ .

Example 3. ( $S ^ { 1 }$ -index) Let $M$ be a Banach space, $S ^ { 1 } = \{ e ^ { i \theta } | \theta \in [ 0 , 2 \pi ] \}$ be a compact Lie group. Let $G$ be the isometry representation of $S ^ { 1 }$ , i.e., the group consists of homomorphisms $T : S ^ { 1 } \to L ( M , M )$ , i.e., $T ( e ^ { i \theta } )$ are isometric operators, satisfying $T ( e ^ { i \theta } ) \cdot T ( e ^ { i \varphi } ) = T ( e ^ { i ( \theta + \varphi ) } ) \forall \theta , \varphi \in [ 0 , 2 \pi ]$ . Set

Σ = the set of all G invariant closed subsets of $M$ .

$$
\mathcal {H} = \left\{h \in C (M, M), T \left(e ^ {i \theta}\right) \circ h = h \circ T \left(e ^ {i \theta}\right) \forall \theta \in [ 0, 2 \pi ] \right\}.
$$

$\forall A \in \Sigma$ , define

$$
\gamma (A) = \left\{ \begin{array}{l l} 0 & \text {i f} A = \emptyset , \\ \inf  \{k \in \mathbb {N} | \exists \phi \in C (A, \mathbb {C} ^ {k} \backslash \{\theta \}), \exists n \in \mathbb {N} \text {s a t i s f y i n g} \\ \phi (T (e ^ {i \theta}) x) = e ^ {i n \theta} \phi (x)   \forall (x, e ^ {i \theta}) \in A \times S ^ {1} \}, \\ + \infty & \text {i f t h e r e i s n o s u c h} \phi , \end{array} \right.
$$

where $\mathbb { C } ^ { k }$ is $k$ -dimensional unitary space (complex $k$ space).

We shall verify that $( \Sigma , { \mathcal { H } } , \gamma )$ is an index with respect to $S ^ { 1 }$ .

By definition, (1) and (2) are trivial. Before going on to verify other fundamental properties, we need:

Lemma 5.2.16 For any $A \in \Sigma$ , if $\phi \in C ( A , \mathbb { C } ^ { k } \backslash \{ \theta \} )$ satisfies

$$
\phi (T (e ^ {i \theta}) x) = e ^ {i n \theta} \phi (x) \forall x \in A f o r s o m e n \in \mathbb {N},
$$

then $\exists { \tilde { \phi } } : M \to \mathbb { C } ^ { k }$ satisfying ${ \\\tilde { \phi } } | _ { A } = \phi$ and

$$
\widetilde {\phi} (T (e ^ {i \theta}) x) = e ^ {i n \theta} \widetilde {\phi} (x) \forall (x, e ^ {i \theta}) \in M \times S ^ {1}.
$$

Proof. By Tietze’s theorem, $\exists { \hat { \phi } } \in C ( M , \mathbb { C } ^ { k } )$ satisfying ${ \hat { \phi } } | _ { A } = \phi$ . Setting

$$
\widetilde {\phi} (x) = \frac {1}{2 \pi} \int_ {0} ^ {2 \pi} e ^ {- i n \varphi} \hat {\phi} (T (e ^ {i \varphi}) x) d \varphi ,
$$

we have

$$
\widetilde {\phi} (T (e ^ {i \theta}) x) = \frac {e ^ {i n \theta}}{2 \pi} \int_ {0} ^ {2 \pi} e ^ {- i n (\theta + \varphi)} \hat {\phi} (T (e ^ {i (\theta + \varphi)}) x) d \varphi = e ^ {i n \theta} \widetilde {\phi} (x),
$$

and $\forall x \in A$ , by the assumption.

$$
\widetilde {\phi} (x) = \frac {1}{2 \pi} \int_ {0} ^ {2 \pi} \phi (x) d x = \phi (x).
$$

□

(Verifications) (3) (Subadditivity) $\forall A , B \in \Sigma$ , let $\gamma ( A ) ~ = ~ n$ , $\gamma ( B ) =$ $m$ . We may assume $n , m \ < \ \infty$ . There exist $\varphi : A  \mathbb { C } ^ { n } \backslash \{ \theta \}$ , $\psi : B $ $\mathbb { C } ^ { m } \backslash \{ \theta \}$ and $k , l ~ \in ~ \mathbb { N }$ such that $\varphi ( T ( e ^ { i \theta } ) x ) = e ^ { i k \theta } \varphi ( x )$ and $\psi ( T ( e ^ { i \theta } ) y ) = $ $e ^ { i l \theta } \psi ( y ) ~ \forall ( x , y , e ^ { i \theta } ) \in A \times B \times S ^ { 1 }$ .

Define

$$
f (x) = \left(\widetilde {\varphi} (x) ^ {l}, \widetilde {\psi} (x) ^ {k}\right),
$$

where the power map $z \mapsto z ^ { p }$ is defined to be $z = ( z _ { 1 } , \dots , z _ { k } ) \mapsto ( z _ { 1 } ^ { p } , \dots , z _ { k } ^ { p } )$ . Thus $f : A \cup B \to \mathbb { C } ^ { n + m }$ is continuous and $f ( z ) \neq \theta$ as $z \in A \cup B$ . Moreover,

$$
\begin{array}{l} f (T (e ^ {i \theta}) x) = (\widetilde {\varphi} (T (e ^ {i \theta}) x) ^ {l}, \widetilde {\psi} (T (e ^ {i \theta}) x) ^ {k}) \\ = e ^ {i k \theta} (\widetilde {\varphi} (x) ^ {l}, \widetilde {\psi} (x) ^ {k}) \\ = e ^ {i k \theta} f (x). \\ \end{array}
$$

Therefore

$$
\gamma (A \cup B) \leq \gamma (A) + \gamma (B).
$$

(4) (Deformation nondecreasing) Assume $A \in \Sigma , \ \eta \in [ 0 , 1 ] \times M \to M$ is continuous and satisfying $\eta ( 0 , \cdot ) = \mathrm { i d }$ , $\eta ( t , \cdot ) \in \mathcal { H }$ , and $\gamma ( \eta ( 1 , A ) ) = k$ . By definition, $\exists \varphi : \eta ( 1 , A ) \to \mathbb { C } ^ { k } \backslash \{ \theta \}$ continuous and satisfying $\varphi ( T ( e ^ { i \theta } ) \eta ( 1 , x ) ) =$ $e ^ { i n \theta } \varphi ( \eta ( 1 , x ) )$ for some $n \in \mathbb N$ , $\forall ( x , e ^ { i \theta } ) \in A \times S ^ { 1 }$ . Now let $\psi = \varphi \circ ( \eta ( 1 , \cdot ) )$ . Then $\psi : A \to \mathbb { C } ^ { k } \backslash \{ \theta \}$ and

$$
\begin{array}{l} \psi (T (e ^ {i \theta}) x) = \varphi (\eta (1, T (e ^ {i \theta}) x)) \\ = \varphi (T (e ^ {i \theta}) \eta (1, x)) \\ = e ^ {i n \theta} \varphi (\eta (1, x)) = e ^ {i n \theta} \psi (x). \\ \end{array}
$$

Therefore $\gamma ( A ) \leq k$ .

(5) (Continuity) Assume that $A \in \Sigma$ is compact and $\gamma ( A ) = k$ . Provided by Lemma 5.2.16, $\Finv$ a continuous ${ \widetilde { \phi } } : M \to \mathbb { C } ^ { k } \ \exists n \in \mathbb { N }$ satisfying ${ \ddot { \phi } } ( T ( e ^ { i \theta } ) x ) =$ $e ^ { i n \theta } { \tilde { \phi } } ( x )$ , ${ \\\tilde { \phi } } | _ { A } = \phi$ and $\phi : A \longrightarrow \mathbb { C } ^ { k } \backslash \{ \theta \}$ .

Setting $V = \widetilde { \phi } ^ { - 1 } ( \mathbb { C } ^ { k } \backslash \{ \theta \} )$ , then $V$ is an open neighborhood of $A$ . Since $A$ is compact, $\exists \delta > 0$ such that the closure of the $\delta$ -neighborhood of $A$ , $N \subset V$ . Therefore

$$
\widetilde {\phi}: N \to \mathbb {C} ^ {k} \backslash \{\theta \}.
$$

It follows that $\gamma ( N ) \leq k$ .

(6) (Normality) Let $p \notin \operatorname { F i x } _ { G }$ , i.e., $\exists \theta \in [ 0 , 2 \pi ]$ such that $T ( e ^ { i \theta } ) p \neq p$ . Since $[ p ] = \{ T ( e ^ { i \theta } ) p | \theta \in [ 0 , 2 \pi ] \} \in \Sigma$ and is not empty, $\gamma ( [ p ] ) \geq 1$ . There exists $\theta _ { 0 } > 0$ , which is the minimal period of $T ( e ^ { i \theta } ) p$ , i.e., $T ( e ^ { i \theta _ { 0 } } ) p = p$ but $T ( e ^ { i \theta } ) p \neq p \forall \theta \in ( 0 , \theta _ { 0 } )$ . $\begin{array} { r } { n = { \frac { 2 \pi } { \theta _ { 0 } } } } \end{array}$ must be an integer. We define

$$
\phi : [ p ] = \{T (e ^ {i \theta}) p | \theta \in [ 0, \theta_ {0} ] \} \rightarrow \mathbb {C} ^ {1} \backslash \{\theta \}
$$

as follows:

$$
\phi : T (e ^ {i \theta}) p \mapsto e ^ {i n \theta}.
$$

It is continuous, and

$$
\begin{array}{l} \phi (T (e ^ {i \varphi}) T (e ^ {i \theta}) p) = \phi (T (e ^ {i (\theta + \varphi)} p)) = e ^ {i n (\theta + \varphi)} \\ = e ^ {i n \varphi} \phi (T (e ^ {i \theta}) p). \\ \end{array}
$$

Therefore $\gamma ( [ p ] ) = 1$ .

We have proved that $\gamma$ so defined is an index.

In particular, if $M$ is a Hilbert space, and $G$ is an unitary representation of $S ^ { 1 }$ , we have the following Borsuk–Ulam-type theorem for $S ^ { 1 }$ equivariant maps.

Theorem 5.2.17 Under the above assumptions, suppose that $V ^ { 2 k }$ is a $2 k$ - dimensional $S ^ { 1 }$ -invariant subspace, which is isomorphic to $\mathbb { C } ^ { k }$ . If $V ^ { 2 k } \cap \mathrm { F i x } _ { G } =$ $\{ \theta \}$ , then $\gamma ( V ^ { 2 k } \cap \partial B _ { 1 } ( \theta ) ) = k$ .

Proof. According to Stone’s representation theorem for one-parameter unitary groups, one may diagonalize $\{ T ( e ^ { i \theta } ) | \ \theta \in [ 0 , 2 \pi ] \}$ simultaneously:

$$
T \left(e ^ {i \theta}\right) = \operatorname {d i a g} \left\{e ^ {i \lambda_ {1} \theta}, \dots , e ^ {i \lambda_ {k} \theta} \right\}.
$$

Since $T ( e ^ { i \theta } )$ is $2 \pi$ -periodic, $\lambda _ { j } \in \mathbb { Z } , ~ j = 1 , 2 , \ldots , k$ . By the assumption that there is no fixed point of $\{ T ( e ^ { i \theta } ) | \theta \in [ 0 , 2 \pi ] \}$ in $\mathbb { C } ^ { k } \backslash \{ \theta \}$ , therefore $\lambda _ { j } \neq 0 , j =$ $1 , 2 , \ldots , k$ .

φ : z = (z1, . . . , zk) → (z ∧λ11 , . .k k $\phi : z = ( z _ { 1 } , \ldots , z _ { k } ) \mapsto ( z _ { 1 } ^ { \frac { \wedge } { \lambda _ { 1 } } } , \ldots , z _ { k } ^ { \frac { \wedge } { \lambda _ { k } } } )$ Firstly, we show $\gamma ( V ^ { 2 k } \cap \partial B _ { 1 } ( \theta ) ) \leq k$ . , z λkk ), where by constructing a continuous map $\boldsymbol { \Lambda } = | \lambda _ { 1 } \dots \lambda _ { k } |$ . Obviously, $\phi :$ $\mathbb { C } ^ { k } \cap \partial B _ { 1 } ( \theta )  \mathbb { C } ^ { k } \backslash \{ \theta \}$ continuously and $\phi ( T ( e ^ { i \theta } ) z ) = e ^ { i \wedge \theta } \phi ( z )$ , $\forall z \in \mathbb { C } ^ { k } \cap$ $\partial B _ { 1 } ( \theta )$ .

Next we are going to show that $\gamma ( \mathbb { C } ^ { k } \cap \partial B _ { 1 } ( \theta ) ) \geq k$

Suppose $\gamma ( \mathbb { C } ^ { k } \cap \partial B _ { 1 } ( \theta ) ) = k _ { 1 } < k$ , i.e., $\Finv$ a continuous $\phi : \mathbb { C } ^ { k } \cap \partial B _ { 1 } ( \theta ) $ $\mathbb { C } ^ { k _ { 1 } } \backslash \{ \theta \}$ , and $\exists n \in \mathbb { N }$ such that $\phi ( T ( e ^ { i \theta } ) z ) = e ^ { i n \theta } \phi ( z )$ . From Lemma 5.2.16, there is an continuous extension $\widetilde { \phi } : \mathbb { C } ^ { k } \longrightarrow \mathbb { C } ^ { k _ { 1 } }$ with $\phi \bigr \rvert _ { \mathbb { C } ^ { k } \cap \partial B _ { 1 } ( \theta ) } = \phi$ . According to Theorem 3.3.10,

$$
\deg (\widetilde {\phi}, B _ {1} (\theta), \theta) = \deg (\phi , B _ {1} (\theta), \theta) \neq 0.
$$

Thus for $y \in \mathbb { C } ^ { k }$ with small $\parallel y \parallel$ , one has

$$
\deg (\widetilde {\phi}, B _ {1} (\theta), y) \neq 0.
$$

In particular, we take $y \in \mathbb { C } ^ { k } \backslash \mathbb { C } ^ { k _ { 1 } }$ , then $\widetilde { \phi } ^ { - 1 } ( y ) \cap B _ { 1 } ( \theta ) \neq \emptyset$ , which contradicts $\widetilde { \phi } ( \mathbb { C } ^ { k } ) \subset \mathbb { C } ^ { k _ { 1 } }$ . □

The Ljusternik–Schnirelmann multiplicity theorem is extended as follows:

Theorem 5.2.18 (Multiplicity theorem) Let $M$ be a smooth Finsher–Banach manifold with a compact Lie group action $G$ . Let $( \Sigma , \mathcal { H } , i )$ be an index with respect to $G$ . Suppose that $f \in C ^ { 1 } ( M , \mathbb { R } ^ { 1 } )$ satisfies $( P S ) _ { c }$ conditions, where $c = c _ { k + 1 } = \cdot \cdot \cdot = c _ { k + m }$ is finite, and

$$
c _ {n} = \inf  _ {i (A) \geq n _ {x \in A}} \sup  f (x) \quad n = 1, 2, \dots .
$$

Then $i ( K _ { c } ) \ge m$ .

Proof. The proof follows step by step that of Lemma 5.2.10. The only thing to be worked out is that the deformation constructed in Corollary 5.2.9 is $G$ -equivariant. In fact, let $\eta : [ 0 , 1 ] \times M \to M$ be the deformation obtained in Theorem 5.2.8. Then $\begin{array} { r } { \eta _ { g } ( t , z ) = \int _ { G } g ^ { - 1 } \eta ( t , g \cdot z ) d \mu } \end{array}$ is the $G$ -equivariant deformation, where $d \mu$ is the right invariant Haar measure on compact continuous group $G$ . We omit the verifications. □

# 5.2.5 Applications

Example 1. (An existence of three nontrivial solutions) Return to Sect. 5.1.5, Example 2, where we proved the existence of a nontrivial solution for the superlinear elliptic BVP both at infinity and at zero. In fact, it was also studied in Sect. 4.8, Example 2, where we showed the existence of a mountain pass solution $p$ . After Corollary 5.2.5, we know that $C _ { 1 } ( f , p ) \neq 0$ ; if further, the condition ( $\Phi$ ) in Theorem 5.1.20 is fulfilled, then we have $C _ { q } ( f , p ) = \delta _ { q 1 } G$ .

We assume condition ( $\Phi$ ) at this moment, and conclude:

Statement 5.2.19 Under assumptions (g1) (g2) and (g3) of Sect. 5.1.5 Example 2, equation (5.10) possesses at least three nontrivial solutions.

Proof. We continue to use the notations in Sect. 5.1.5 Example 2.

1. From (g3),

$$
J (u) = \frac {1}{2} \| u \| ^ {2} + \circ (\| u \| ^ {2}). \tag {5.15}
$$

Therefore $\theta$ is a local minimum, and $C _ { q } ( J , \theta ) = \delta _ { q 0 } G$ .

2. We claim the existence of two nontrivial solutions. Let us define

$$
g _ {+} (x, t) = \left\{ \begin{array}{l l} g (x, t) & t \geq 0 \\ 0 & t \leq 0  , \end{array} \right.
$$

and

$$
J _ {+} (u) = \int_ {\Omega} \left[ \frac {1}{2} | \nabla u | ^ {2} - G _ {+} (x, u (x)) \right] d x,
$$

where

$$
G _ {+} (x, t) = \int_ {0} ^ {t} g _ {+} (x, s) d s.
$$

Similarly, one verifies that $J _ { + } \in C ^ { 2 } ( H _ { 0 } ^ { 1 } ( \Omega ) , \mathbb { R } ^ { 1 } )$ satisfies (PS), and that

$$
J _ {+} (t \varphi_ {1}) \rightarrow - \infty \text {a s} t \rightarrow + \infty ,
$$

where $\varphi _ { 1 } > 0$ is the first eigenvector of $- \triangle$ with 0-Dirichlet data.

On the other hand, $\exists \delta > 0$ such that

$$
J _ {+} | _ {\partial B _ {\delta} (\theta)} \geq \frac {1}{4} \delta^ {2},
$$

From (5.15). The mountain pass lemma is applied to obtain a critical point $u _ { + } \in H _ { 0 } ^ { 1 } ( \Omega )$ , with critical value $c _ { + } > 0$ , which satisfies

$$
\left\{ \begin{array}{l} - \triangle u _ {+} = g _ {+} (x, u _ {+}) \\ u _ {+} | _ {\partial \Omega} = 0. \end{array} \right.
$$

By using the maximum principle, $u _ { + } \geq 0$ , so it is again a critical point of $J$ .

Analogously, we define

$$
g _ {-} (x, t) = \left\{ \begin{array}{l l} g (x, t) & t \leq 0 \\ 0 & t > 0  , \end{array} \right.
$$

and obtain a critical point $u _ { - } \leq 0$ , with critical value $c _ { - } > 0$ . Since ( $\Phi$ ) is assumed, $C _ { q } ( J _ { \pm } , u _ { \pm } ) = \delta _ { q 1 } G$ . According to Theorem 5.1.16, we have

$$
C _ {q} (J _ {\pm}, u _ {\pm}) = C _ {q} (\widetilde {J _ {\pm}}, u _ {\pm}) = C _ {q} (\widetilde {J}, u _ {\pm}) = C _ {q} (J, u _ {\pm}),
$$

where $\tilde { J } = J | _ { C _ { 0 } ^ { 1 } }$ . Therefore

$$
C _ {q} (J, u _ {\pm}) = \delta_ {q 1} G.
$$

3. Suppose that there were no other critical points of $J$ . The Morse type numbers over the pair $( H _ { 0 } ^ { 1 } ( \Omega ) , J _ { a } )$ would be

$$
M _ {0} = 1, M _ {1} = 2, M _ {q} = 0, q \geq 2,
$$

but the Betti numbers read as

$$
\beta_ {q} = 0 \forall q = 0, 1, 2, \dots .
$$

From Lemma 5.1.38: $H _ { q } ( H _ { 0 } ^ { 1 } ( \Omega ) , J _ { a } ) \simeq H _ { q } ( H _ { 0 } ^ { 1 } ( \Omega ) , S ^ { \infty } ) \simeq 0$ .

This contradicts the Morse relation.

![](images/2862716d527d52526211ef113c225484f8fb85392328b9b2f2be276dd77082a8.jpg)

Now we return to verifying the condition $( \Phi ) \colon J ^ { \prime \prime } ( u ) \geq 0$ and $0 \in \sigma ( J ^ { \prime \prime } ( u ) )$ imply $\dim \ker ( J ^ { \prime \prime } ( u ) ) = 1$ .

It is based on the following:

Theorem 5.2.20 (Manes–Micheletti) Suppose that $m \in C ( \overline { { \Omega } } )$ satisfies $\operatorname* { s u p } \{ m ( x ) | \ x \in \Omega \} > 0$ . Then the equation

$$
\left\{ \begin{array}{l l} - \triangle u = \lambda m u & i n   \Omega \\ u | _ {\partial \Omega} = 0 \end{array} \right.
$$

admits a smallest positive eigenvalue $\lambda _ { 1 }$ , associated with a positive eigenfunction. Moreover,

$$
\dim \ker (- \triangle - \lambda_ {1} m \cdot) = 1.
$$

Continuation of the proof of Statement 5.2.19 (Verification of the condition (Φ))

Now

$$
\left(J ^ {\prime \prime} (u) v, v\right) = \int_ {\Omega} | \nabla v | ^ {2} - m v ^ {2} \quad \forall v \in H _ {0} ^ {1} (\Omega), \tag {5.16}
$$

where $m ( x ) = g ^ { \prime } ( x , u ( x ) )$ .

Assume that the right-hand side of equation (5.16) is nonnegative and that $\exists \boldsymbol { \phi } \in H _ { 0 } ^ { 1 } ( \Omega ) \backslash \{ \boldsymbol { \theta } \}$ satisfies the equation

$$
- \triangle v - m \cdot v = 0. \tag {5.17}
$$

We want to show that all solutions of equation (5.17) are multiples of $\phi$ . After Theorem 5.2.20, it is sufficient to show that $\lambda = 1$ is the smallest positive eigenvalue of

$$
- \triangle v = \lambda m v \quad v \in H _ {0} ^ {1} (\Omega). \tag {5.18}
$$

On the one hand,

$$
\lambda_ {1} := \inf  _ {v \in H _ {0} ^ {1} \backslash \{\theta \}} \frac {\int | \nabla v | ^ {2}}{\int m v ^ {2}} \leq \frac {\int | \nabla \phi | ^ {2}}{\int m \phi^ {2}} = 1,
$$

from $\begin{array} { r } { \int _ { \Omega } m \phi ^ { 2 } = \int _ { \Omega } | \nabla \phi | ^ { 2 } \neq 0 } \end{array}$ . On the other hand, provided by the nonnega-Ω tiveness of $J ^ { \prime \prime } ( u )$ Ω,

$$
\lambda = \frac {\int_ {\Omega} | \nabla v | ^ {2}}{\int_ {\Omega} m v ^ {2}} \geq 1
$$

for every $\lambda \in \sigma ( J ^ { \prime \prime } ( u ) )$ . Therefore the smallest positive eigenvalue $\lambda _ { 1 }$ of equation (5.18) equals 1. Condition ( $\Phi$ ) is verified.

We turn to the proof of Theorem 5.2.20.

Proof. 1. Let $T : u \mapsto ( - \triangle ) ^ { - 1 } m \cdot u$ be the self-adjoint compact operator on $H _ { 0 } ^ { 1 } ( \Omega )$ . According to Courant’s max-min characterization,

$$
\pm \lambda_ {\pm n} ^ {- 1} = \sup  _ {V \in \mathcal {F} _ {n}} \inf  _ {u \in V} \frac {\pm \int_ {\Omega} m u ^ {2}}{\int_ {\Omega} | \nabla u | ^ {2}}, n = 0, 1, 2, \dots .
$$

are eigenvalues of $T$ , where ${ \mathcal { F } } _ { n }$ is the family of all $n$ -dimensional linear subspaces of $H _ { 0 } ^ { 1 } ( \Omega )$ . Thus, $\pm \lambda _ { \pm 1 } \leq \pm \lambda _ { \pm 2 } \leq \cdot \cdot \cdot$ , if they exist.

2. Under our assumption, $\lambda _ { 1 } > 0$ . Let $\omega$ be an eigenfunction. We claim that $\omega$ does not change sign. Otherwise, let $\omega _ { + } = \omega \vee 0$ and $\omega _ { - } = \omega \wedge 0$ ; they are not zero, then

$$
\begin{array}{l} \int_ {\Omega} m \omega^ {2} = \int_ {\Omega} m \omega_ {+} ^ {2} + \int_ {\Omega} m \omega_ {-} ^ {2} = \alpha_ {+} + \alpha_ {-}, \\ \int_ {\Omega} | \nabla \omega | ^ {2} = \int_ {\Omega} | \nabla \omega_ {+} | ^ {2} + \int_ {\Omega} | \nabla \omega_ {-} | ^ {2} = \beta_ {+} + \beta_ {-}, \\ \end{array}
$$

where $\alpha _ { \pm } = \int _ { \Omega } m \omega _ { \pm } ^ { 2 } > 0$ and $\beta _ { \pm } = \int _ { \Omega } | \nabla \omega _ { \pm } | ^ { 2 }$

Either $\begin{array} { r } { \frac { \alpha _ { + } + \alpha _ { - } } { \beta _ { + } + \beta _ { - } } = \frac { \alpha _ { + } } { \beta _ { + } } = \frac { \alpha _ { - } } { \beta _ { - } } } \end{array}$ α+ or $\begin{array} { r } { \frac { \alpha _ { + } + \alpha _ { - } } { \beta _ { + } + \beta _ { - } } < \operatorname* { m a x } \{ \frac { \alpha _ { + } } { \beta _ { + } } , \frac { \alpha _ { - } } { \beta _ { - } } \} , } \end{array}$ , α− }. However, the latter case cannot occur, because $\begin{array} { r } { \lambda _ { 1 } ^ { - 1 } = \frac { \alpha _ { + } + \alpha _ { - } } { \beta _ { + } + \beta _ { - } } = \operatorname* { s u p } _ { u } \int _ { \Omega } m u ^ { 2 } / \int _ { \Omega } | \nabla u | ^ { 2 } } \end{array}$ = . In the former case, both $\omega _ { \pm }$ are eigenfunctions with respect to $\lambda _ { 1 }$ and $\pm m \cdot \omega _ { \pm } \geq 0$ . From the maximum principle, we obtain $\pm \omega _ { \pm } ( x ) > 0$ a.e. $x \in \Omega$ . This is impossible.

3. We show that $\dim \ker ( - \triangle - \lambda _ { 1 } m \cdot ) = 1$ .

Suppose that there are $\omega _ { 1 }$ and $\omega _ { 2 } \in H _ { 0 } ^ { 1 } ( \Omega )$ satisfying

$$
- \triangle \omega_ {i} = \lambda_ {1} m \omega_ {i} \quad i = 1, 2.
$$

Then by step 2, $\forall \alpha \in R ^ { 1 } , ~ \omega _ { 1 } + \alpha \omega _ { 2 }$ $\forall \alpha \in R ^ { 1 }$ $\omega _ { 1 } + \alpha \omega _ { 2 }$ does not change sign. Let $A _ { + } = \{ \alpha \in$ $R ^ { 1 } | \omega _ { 1 } + \alpha \omega _ { 2 } \geq 0 \}$ and $A _ { - } = \left\{ \alpha \in R ^ { 1 } | \omega _ { 1 } + \alpha \omega _ { 2 } \leq 0 \right\}$ . Then both $A _ { + }$ and $A _ { - }$ are nonempty closed subsets and $A _ { + } \cup A _ { - } = R ^ { 1 }$ . Let $\alpha _ { 0 } \in A _ { + } \cap A _ { - }$ , then $\omega _ { 1 } + \alpha _ { 0 } \omega _ { 2 } = 0$ , i.e., $\omega _ { 1 }$ and $\omega _ { 2 }$ are colinear. This proves $\mathrm { d i m } \ker ( - \triangle$ $- \lambda _ { 1 } m \cdot ) = 1$ . □

Example 2. (Spectrum of the $p$ -Laplacian)

Let $\Omega \subset R ^ { n }$ be a bounded open domain. $\forall p \in ( 1 , \infty )$ we define the $p$ - Laplacian operator to be

$$
\triangle_ {p} u = \operatorname {d i v} (| \nabla u | ^ {p - 2} \nabla u) \quad \forall u \in W _ {0} ^ {1, p} (\Omega).
$$

In particular, $\triangle _ { 2 }$ is the Laplacian. $\lambda$ is called an eigenvalue of $- \triangle _ { p }$ if there exists a nontrivial weak solution $u \in W _ { 0 } ^ { 1 , p } ( \Omega )$ of the equation:

$$
- \triangle_ {p} u = \lambda | u | ^ {p - 2} u \quad \text {i n} \Omega , \tag {5.19}
$$

i.e.,

$$
\int_ {\Omega} | \nabla u | ^ {p - 2} \nabla u \nabla \varphi = \lambda \int_ {\Omega} | u | ^ {p - 2} u \cdot \varphi \quad \forall \varphi \in W _ {0} ^ {1, p} (\Omega),
$$

where $\begin{array} { r } { \frac { \perp } { p } + \frac { \perp } { p ^ { \prime } } = 1 } \end{array}$

This is reduced to a constraint variational problem. Set

$$
f (u) = \frac {1}{p} \int_ {\Omega} | u | ^ {p} \text {a n d} g (u) = \frac {1}{p} \int_ {\Omega} | \nabla u | ^ {p}.
$$

Let $M = g ^ { - 1 } ( p ^ { - 1 } )$ . It is a nonempty closed submanifold of $W _ { 0 } ^ { 1 , p } ( \Omega )$ . In fact, $\begin{array} { r } { \forall u \in M , \langle g ^ { \prime } ( u ) , u \rangle = \int _ { \Omega } | \nabla u | ^ { p } = 1 } \end{array}$ , which implies that $g ^ { \prime } ( u ) \neq \theta$ .

Let $\widetilde { f } = f | _ { M }$ . We have

$$
d \widetilde {f} (u) = | u | ^ {p - 2} u + \left(\int_ {\Omega} | u | ^ {p}\right) \left(\triangle_ {p} u\right) \in W ^ {- 1, p ^ {\prime}} (\Omega),
$$

which satisfies $\langle d { \widetilde { f } } ( u ) , u \rangle = 0 \forall u \in M$ .

Set $J _ { 0 } = ( - \triangle _ { 2 } ) ^ { - 1 / 2 }$ and $J _ { 1 } w = | w | ^ { p ^ { \prime } - 2 } w / \parallel w \parallel _ { p ^ { \prime } } ^ { p ^ { \prime } - 2 }$ , we have

$$
\| J _ {0} w \| _ {p ^ {\prime}} = \| w \| _ {- 1, p ^ {\prime}}, \| J _ {0} w \| _ {1, p} = \| w \| _ {p},
$$

and

$$
\parallel J _ {1} w \parallel_ {p} = \parallel w \parallel_ {p ^ {\prime}}.
$$

Thus $J = J _ { 0 } \circ J _ { 1 } \circ J _ { 0 } : W ^ { - 1 , p ^ { \prime } } ( \Omega ) \to W ^ { 1 , p } ( \Omega )$ is well defined and satisfies:

(1) $J$ is continuous,   
(2)   
(3)

An odd pseudo-gradient vector field for $\hat { f }$ is defined by

$$
X (u) = J \cdot d \widetilde {f} (u) - \lambda u \in W ^ {1, p} (\Omega),
$$

where $\lambda = \langle J \cdot d \widetilde { f } ( u ) , - \triangle _ { p } u \rangle$ . We claim that $X ( u ) \in T _ { u } M$

In fact, let $s : ( - 1 , + 1 ) \to M$ be a $C ^ { 1 }$ -curve with $s ( 0 ) = u$ and $\dot { s } ( 0 ) =$ $X ( u )$ , then

$$
\frac {d}{d t} g (s (t)) | _ {t = 0} = \langle X (u), g ^ {\prime} (u) \rangle = \langle X (u), - \triangle_ {p} u \rangle = 0.
$$

But we have

$$
\langle X (u), d \widetilde {f} (u) \rangle = \langle J \cdot d \widetilde {f} (u), d \widetilde {f} (u) \rangle = \| d \widetilde {f} (u) \| _ {- 1, p ^ {\prime}} ^ {2}.
$$

From

$$
\| X (u) \| _ {1, p} \leq \| d \widetilde {f} (u) \| _ {- 1, p ^ {\prime}} + | \lambda | \| u \| _ {1, p},
$$

and

$$
\begin{array}{l} | \lambda | \leq \| J \cdot d \widetilde {f} (u) \| _ {1, p} \| - \triangle_ {p} u \| _ {- 1, p ^ {\prime}} \\ \leq \parallel d \widetilde {f} (u) \parallel_ {- 1, p ^ {\prime}} \parallel u \parallel_ {1, p} = \parallel d \widetilde {f} (u) \parallel_ {- 1, p ^ {\prime}}, \\ \end{array}
$$

it follows

$$
\| X (u) \| _ {1, p} \leq 2 \| d \widetilde {f} (u) \| _ {- 1, p ^ {\prime}}.
$$

Thus $X ( u )$ is a pseudo-gradient vector field for $\hat { \boldsymbol f }$ . Now we verify the (PS)c condition for $\hat { f }$ , as $c > 0$ . Suppose that $\{ u _ { j } \}$ is a sequence on $M$ such that $d \widetilde { f } ( u _ { j } )  \theta$ and $\widetilde f ( u _ { j } )  c > 0$ . $\{ u _ { j } \}$ is bounded in $W _ { 0 } ^ { 1 , p } ( \Omega )$ , and then after a subsequence $u _ { j }  u _ { 0 } \in W _ { 0 } ^ { 1 , p } ( \Omega )$ . From $d \widetilde { f } ( u _ { j } )  \theta$ $\left( W ^ { - 1 , p ^ { \prime } } ( \Omega ) \right)$ , and $\tilde { f } ( u _ { j } )  c$ one has

$$
- \triangle_ {p} u _ {j} \rightarrow (p c) ^ {- 1} | u _ {0} | ^ {p - 2} u _ {0} \quad W ^ {- 1, p ^ {\prime}} (\Omega).
$$

From Sect. 2.6, Example 4, $- \triangle _ { p }$ is a monotone operator with a continuous inverse: $W _ { 0 } ^ { 1 , p } ( \Omega )$ $( - \triangle _ { p } ) ^ { - 1 } : W ^ { - 1 , p ^ { \prime } } ( \Omega ) \to W _ { 0 } ^ { 1 , p } ( \Omega )$ . , therefore $u _ { j }$ strongly converges in

We notice that $M$ is the unit sphere in $W _ { 0 } ^ { 1 , p } ( \Omega )$ , and that the function $\hat { f }$ is even. Quotient by a $\mathbb { Z } _ { 2 }$ -group, $M / \mathbb { Z } _ { 2 }$ is homeomorphic to the infinitedimensional projective space $P ^ { \infty }$ .

Applying the Ljusternik–Schnirelmann theorem, we have:

Statement 5.2.21 For $1 < p < \infty$ , the equation (5.19) has infinitely many pairs of solutions $( \lambda _ { m } , \pm u _ { m } ) \in R _ { + } ^ { 1 } \times ( W _ { 0 } ^ { 1 , p } ( \Omega ) \backslash \{ \theta \} )$ , $m = 1 , 2 , \ldots$ . Moreover, let be the spectrum of the -Laplacian, i.e., the set of all $\lambda$ associated with $o _ { p }$ $p$ nontrivial solutions. Then $\sigma _ { p }$ is unbounded.

Proof. Since $\hat { f }$ satisfies (PS) , $c > 0$ , we apply the Ljusternik–Schnirelmann theorem to $- \hat { f }$ , i.e., let

$$
c _ {k} = \inf  _ {\gamma (A) \geq k _ {u \in A}} \sup  - \widetilde {f} (u) \quad k = 1, 2, \dots .
$$

where $A$ is a closed symmetric (with resp. to $\theta$ ) subset of $M$ and $\gamma$ is the genus. All these $c _ { k }$ are negativet critical values with $c _ { k } \ \leq \ c _ { k + 1 }$ . We have $\begin{array} { r } { \lambda _ { k } = - \frac { 1 } { c _ { k } } , \ k = 1 , 2 , . . . } \end{array}$ .

kWe shall prove that $\sigma _ { p }$ is unbounded by contradiction. If not, then there exists $\epsilon > 0$ such that $c _ { k } < - \epsilon$ . k large.

Let us choose a sequence of linear subspaces $\{ E _ { j } \} _ { 1 } ^ { \infty }$ , such that

$$
E _ {j} \subset E _ {j + 1}, \dim E _ {j} = j, \mathrm {a n d} \overline {{\cup_ {j = 1} ^ {\infty} E _ {j}}} = W _ {0} ^ {1, p} (\Omega).
$$

Set

$$
d _ {k} = \inf  _ {\gamma (A) \geq k} \sup  _ {u \in A \cap E _ {k - 1} ^ {\perp}} - \widetilde {f} (u),
$$

where $E _ { k } ^ { \bot }$ denotes a complementary linear subspace of $E _ { k }$

We need the following intersection result:

Lemma 5.2.22 Let $X$ be a Banach space and $E \subset X$ be a $j < \infty$ dimensional linear subspace. If A is a symmetric closed set with $\gamma ( A ) > j$ then $A \cap E ^ { \bot } \neq \emptyset$ , where $E ^ { \perp }$ is any complementary subspace of $E$ .

Proof. We prove it by contradiction. If $A \cap E ^ { \bot } = \emptyset$ , one defines the projection $P : X  E$ , according to the direct sum composition: $X = E \oplus E ^ { \perp }$ . Define a function $\rho _ { \epsilon } \in C ( R _ { + } ^ { 1 } , R _ { + } ^ { 1 } )$ to be $\rho _ { \epsilon } ( t ) = \epsilon$ as $t \leq \epsilon$ , and $\rho _ { \epsilon } ( t ) = t$ as $t \geq \epsilon$ for $\epsilon > 0$ and an odd deformation:

$$
\eta (t, x) = (1 - t) x + t \frac {P x}{\rho_ {\epsilon} (\parallel P x \parallel)}.
$$

From $A \cap E ^ { \bot } = \emptyset$ , we have $\epsilon > 0$ such that $P ( A ) \cap B _ { \epsilon } ( \theta ) = \varnothing$ . Since

$$
\eta (1, x) = \frac {P x}{\rho_ {\epsilon} (\parallel P x \parallel)}, \eta (1, A) \subset E \cap \partial B _ {1} (\theta).
$$

Provided by the monotonicity and the deformation nondecreasing,

$$
\gamma (A) \leq \gamma (\overline {{\eta (1 , A)}}) \leq \gamma (E \cap \partial B _ {1} (\theta)) = j.
$$

This is a contradiction.

(Continuing the proof of Statement 5.2.21) According to Lemma 5.2.22, if $\gamma ( A ) \geq k$ , then $A \cap E _ { k - 1 } ^ { \perp } \neq \emptyset$ , therefore $d _ { k }$ is well defined, and $d _ { k } \leq c _ { k } , \ k =$ $1 , 2 , \ldots$ .

We suppose $c _ { k } < - \epsilon ~ \forall k$ large, then there exists a closed symmetric subset $A _ { k } \in \Sigma$ , with $\gamma ( A _ { k } ) \geq k$ , and $\exists u _ { k } \in A _ { k } \cap E _ { k - 1 } ^ { \perp }$ such that $- \tilde { f } ( u _ { k } ) < - \epsilon$ , i.e., $\tilde { f } ( u _ { k } ) > \epsilon \forall k$ large.

However, $u _ { k } \in M$ implies that after a subsequence $u _ { k }  v$ in $W _ { 0 } ^ { 1 , p } ( \Omega )$ , and then $u _ { k } \ \to \ v$ in $L ^ { p } ( \Omega )$ , in particular, $\tilde { f } ( u _ { k } )  f ( v )$ , as $k  \infty$ . But $u _ { k } \in E _ { k - 1 } ^ { \perp }$ , which implies $u _ { k } \to \theta$ in $W _ { 0 } ^ { 1 , p } ( \Omega )$ , and $f ( \theta ) = 0$ . This contradicts $\tilde { f } ( u _ { k } ) > \epsilon$ . □

Example 3. Let $\Omega \subset \mathbb { R } ^ { n }$ be a bounded domain with smooth boundary. We consider the problem:

$$
\left\{ \begin{array}{l l} - \triangle u (x) = \lambda g (x, u (x)) & \text {i n} \Omega \\ u (x) = 0 & \text {o n} \partial \Omega \end{array} \right. \tag {5.20}
$$

where $g \in C ( \overline { { \Omega } } \times R ^ { 1 } )$ is odd in $t$ , and satisfies:

(1) $\risingdotseq t _ { 0 } > 0$ such that $g ( x , t _ { 0 } ) \leq 0 ~ \forall x \in \overline { { \Omega } }$

(2) $\exists a \in C ( { \overline { { \Omega } } } ) , a > 0$ , such that $g ( x , t ) - a ( x ) t = \circ ( t )$ as $t  0$ uniformly in $x \in \Omega$ .

One has the following abstract result:

Theorem 5.2.23 Let $X$ be a Banach space, and $f \in C ^ { 1 } ( X , R ^ { 1 } )$ be an even function satisfying the (PS) condition. Assume $a < b$ and either $f ( \theta ) < a$ or $f ( \theta ) > b$ . If further,

(1) there are an m-dimensional linear subspace $E$ and $\rho > 0$ such that

$$
\sup  _ {x \in E \cap \partial B _ {\rho} (\theta)} f (x) \leq b,
$$

(2) there is a $j$ -dimensional linear subspace $F$ such that

$$
\inf  _ {x \in F ^ {\perp}} f (x) > a,
$$

where $F ^ { \perp }$ is a complementary space of $F$ , (3) $m > j$ ,

then $f$ has at least $m - j$ pairs of distinct critical points.

Proof. Let

$$
c_{n} = \inf_{\gamma (A)\geq n_{x}\in A}\sup f(x) \quad n = 1,2,\ldots .
$$

1. From condition (1) $E \cap \partial B _ { \rho } ( \theta ) \subset f _ { b }$ , by the monotonicity of genus $\gamma ( f _ { b } ) \geq \gamma ( E \cap \partial B _ { \rho } ( \theta ) ) = m$ . We obtain $c _ { m } \leq b$ .

2. We verify that $c _ { n } \geq a$ as $n > j$ . If it is not true, then $\exists n _ { 0 } > j$ such that $c _ { n _ { 0 } } ~ < ~ a$ , i.e., $\exists$ a closed symmetric set $A$ with $\gamma ( A ) ~ \geq ~ n _ { 0 }$ such that

$f ( x ) < a \forall x \in A$ . Condition (2) implies that $f ( x ) > a \forall x \in F ^ { \bot }$ . But according to Lemma 5.2.22, $A \cap F ^ { \bot } \neq \emptyset$ . This is a contradiction.

Combining steps 1 and 2, we obtain critical values in $[ a , b ]$

$$
c _ {j + 1} \leq \dots \leq c _ {m}.
$$

From the multiplicity theorem, there are at least $m \textrm { - } j$ pairs of critical points. □

Remark 5.2.24 Another proof of Theorem 5.2.23 based on the estimation of cup length can be found in [Ch 5].

Now we return to equation (5.20). First, we replace $g$ by the function

$$
\hat {g} (x, t) = \left\{ \begin{array}{l l} g (x, t) & t \leq t _ {0} \\ g (x, t _ {0}) & t > t _ {0}  . \end{array} \right.
$$

From the maximum principle, we know that if $u \in H _ { 0 } ^ { 1 } ( \Omega )$ is a solution of the equation:

$$
\left\{ \begin{array}{c} - \triangle u (x) = \lambda \hat {g} (x, u (x)) \text {i n} \Omega \\ u (x) = 0 \quad \text {o n} \partial \Omega , \end{array} \right. \tag {5.21}
$$

then $u ( x ) < t _ { 0 } \forall x \in \Omega$ , and then it is a solution of equation (5.20).

Let us write

$$
p (x, t) = \hat {g} (x, t) - a (x) t
$$

and $\textstyle P ( x , t ) = \int _ { 0 } ^ { t } p ( x , s ) d s$ . Consider the functional with a parameter $\lambda$ :

$$
J _ {\lambda} (u) = \int_ {\Omega} \left[ \frac {1}{2} | \nabla u | ^ {2} - \lambda \left(\frac {a u ^ {2}}{2} + P (x, u (x))\right) \right] d x.
$$

Obviously, $\forall \lambda \in R ^ { 1 }$ , $J _ { \lambda }$ is bounded from below.

Since $a > 0$ , the eigenvalue problem:

$$
- \triangle v = \mu a \cdot v \quad v \in H _ {0} ^ {1} (\Omega)
$$

has eigenvalues $\lambda _ { 1 } < \lambda _ { 2 } \leq \cdot \cdot \cdot \leq \lambda _ { k } \leq \cdot \cdot \cdot$ .

If $\lambda > \lambda _ { k }$ then there exists $\rho > 0$ such that $J _ { \lambda } | _ { E _ { k } \cap \partial B _ { \rho } ( \theta ) } < 0$ , where $E _ { k }$ i s ∩the subspace spanned by eigenfunctions with eigenvalues $\mu \leq \lambda _ { k }$ .

Omitting the verification of the (PS) condition, we apply Theorem 5.2.23 and obtain:

Theorem 5.2.25 Under assumptions (1) and (2) of Example (3), (5.20) possesses at least $k$ distinct pairs of solutions for $\lambda > \lambda _ { k }$ .

# 5.3 Periodic Orbits for Hamiltonian System and Weinstein Conjecture

For $H \in C ^ { 1 } ( R ^ { n } \times R ^ { n } , R ^ { 1 } )$ , the following ODE system:

$$
\left\{ \begin{array}{l} \dot {x} = - H _ {p} (x, p) \\ \dot {p} = H _ {x} (x, p) \end{array} \right. \tag {5.22}
$$

$\forall ( x , p ) \in R ^ { n } \times R ^ { n }$ , is called a Hamiltonian system, and $H$ is called a Hamiltonian function. Sometimes we use the notations:

$$
z = (x, p),
$$

and

$$
J = \left( \begin{array}{c c} 0 & - I \\ I & 0 \end{array} \right)  ,
$$

where $I$ is the $n \times n$ unit matrix. then (5.22) has a simple form:

$$
- J \dot {z} = H ^ {\prime} (z),
$$

or

$$
\dot {z} = J H ^ {\prime} (z). \tag {5.23}
$$

$J$ grad is called a symplectic gradient.

For the Hamiltonian system two types of periodic solution problems are often examined.

1. For a given period $T$ , find a solution $z$ satisfying (5.23) and $z ( t ) = z ( t { + } T )$ .   
2. For a given energy surface, i.e., a real number $c$ , find a periodic solution $z$ of (5.23) lying on the given energy surface, i.e., $H ( z ( t ) ) = c$ .

The reason why we can set up the type 2 problem is that Hamiltonian systems are conservative:

$$
\begin{array}{l} \frac {d}{d t} H (z (t)) = \left(H ^ {\prime} (z (t)), \dot {z} (t)\right) \\ = \left(H ^ {\prime} (z (t)), J H ^ {\prime} (z (t))\right) = 0, \tag {5.24} \\ \end{array}
$$

i.e., along a trajectory $t \mapsto z ( t )$ , the Hamiltonian function is a constant.

Note: the periodic trajectory $z$ on $\Sigma = H ^ { - 1 } ( c )$ is virtually independent of the behavior of the function $H$ outside $\Sigma$ , that is, we have:

Lemma 5.3.1 Let $H$ , $H _ { 1 } \in C ^ { 1 } ( R ^ { 2 n } , R ^ { 1 } ) , c \in R ^ { 1 }$ . Assume that $\Sigma = H ^ { - 1 } ( c )$ is a compact manifold, If $H | _ { \Sigma } = H _ { 1 } | _ { \Sigma }$ , then

$$
\dot {z} = J H ^ {\prime} (z) a n d \dot {z} = J H _ {1} ^ {\prime} (z)
$$

has the same periodic trajectories on $\Sigma$ .

Proof. By the hypotheses, $\exists \mu \in C ( \Sigma , R ^ { \bot } )$ satisfying

$$
H ^ {\prime} (z) = \mu (z) H _ {1} ^ {\prime} (z), \quad \forall z \in \Sigma .
$$

Since $\mu ( z ) \neq 0$ and $\Sigma$ is compact, there are constants $m , M > 0$ ,

$$
m \leqslant \mu (z) \leqslant M, \quad \forall z \in \Sigma . \tag {5.25}
$$

Let $z$ be an integral curve of $H$ on $\Sigma$ , define

$$
z _ {1} (t) = z \circ s (t),
$$

where $s : R ^ { 1 } \to R ^ { 1 }$ will be determined later. First we solve the equation:

$$
\left\{ \begin{array}{l} \dot {\sigma} (t) = \frac {1}{(\mu \circ z) (\sigma (t))} \\ \sigma (0) = 0. \end{array} \right. \tag {5.26}
$$

Since the functions $\mu , z$ are all bounded and continuous, (5.26) has a solution on every open interval in $R ^ { 1 }$ . Let $T$ be the period of $z$ , since $\sigma ( t ) \to + \infty$ as $t \to + \infty$ , there exists $\tau > 0$ such that $\sigma ( \tau ) = T$ . Let

$$
s (t) = \left[ \frac {t}{\tau} \right] T + \sigma \left(t - \left[ \frac {t}{\tau} \right] \tau\right),
$$

where $[ t ]$ denotes the largest integer less than $t$ , then $s \in C ^ { 1 } ( R ^ { 1 } , R ^ { 1 } ) , s ( 0 ) = 0$

It remains to verify the differentiability at $t = j \tau , j = 0 , \pm 1 , \pm 2 , . . . .$ In fact, for $j = 1$ ,

$$
\begin{array}{l} s ^ {\prime} (\tau + 0) = \dot {\sigma} (0) = \frac {1}{(\mu \circ z) (\sigma (0))} = \frac {1}{(\mu \circ z) (0)} = \frac {1}{(\mu \circ z) (T)} \\ = \dot {\sigma} (\tau - 0) = s ^ {\prime} (\tau - 0) \\ \end{array}
$$

Similarly, we prove this for $j \neq 1$ . Hence, $s : R ^ { 1 } \to R ^ { 1 }$ is a diffeomorphism and satisfies

$$
\left\{ \begin{array}{l} \dot {s} (t) = \frac {1}{(\mu \circ z) (s (t))}  , \\ s (0) = 0  . \end{array} \right.
$$

It follows that

$$
\dot {z} _ {1} (t) = \dot {s} (t) \dot {z} (t) \circ s (t) = J H _ {1} ^ {\prime} (z _ {1} (t)),
$$

and $z _ { 1 } ( 0 ) = z ( 0 ) \in \Sigma$ , hence $z _ { 1 }$ is an integral curve of $H _ { 1 }$ on $\Sigma$ . The converse can be proved in the same way. □

Thus a given hypersurface $\Sigma$ determines the orbits of every Hamiltonian vector field $J$ grad $H$ with $\Sigma$ as regular energy surface $H ^ { - 1 } ( c )$ for some constant $c$ .

In this sense, one can ask the question: Which hypersurfaces carry a periodic orbit?

# 5.3.1 Hamiltonian Operator

Let $S ^ { 1 }$ be the unit circle $\{ e ^ { i t } | t \in [ 0 , 2 \pi ] \}$ and $V = R ^ { n } \times R ^ { n }$ . Define the real Hilbert space $L ^ { 2 } ( S ^ { 1 } , V )$ as follows: $z \in L ^ { 2 } ( S ^ { 1 } , V )$ means that $z$ is defined on $S ^ { 1 }$ with range in $V$ and $| z | ^ { 2 }$ is integrable, the inner product is defined by

$$
\langle z, w \rangle = \frac {1}{2 \pi} \int_ {S ^ {1}} \sum_ {j = 1} ^ {2 n} z _ {j} (t) w _ {j} (t) d t, \quad \forall z, w \in L ^ {2} \left(S ^ {1}, V\right).
$$

Similarly define the Sobolev space $H ^ { 1 } ( S ^ { 1 } , V )$ with the inner product

$$
\langle z, w \rangle = \frac {1}{2 \pi} \int_ {S ^ {1}} \sum_ {j = 1} ^ {2 n} \left[ z _ {j} (t) w _ {j} (t) + \dot {z} _ {j} (t) \dot {w} _ {j} (t) \right] d t.
$$

Let

$$
J = \left( \begin{array}{c c} 0 & - I _ {n} \\ I _ {n} & 0 \end{array} \right)  ,
$$

where $I _ { n }$ is the $n \times n$ identity matrix. Consider the linear operator $A : z \mapsto$ $\textstyle - J { \frac { d } { d t } } z$ in $L ^ { 2 } ( S ^ { 1 } , V )$ with domain $D ( A ) = H ^ { 1 } ( S ^ { 1 } , V )$ .

The real space $V$ is isomorphic to the complex linear space $\mathbb { C } ^ { n } = R ^ { n } + i R ^ { n }$ . The isomorphism is defined as follows: Let $e _ { 1 } , \ldots , e _ { 2 n }$ be an orthonormal basis in $V$ , and let

$$
\phi_ {j} = e _ {j} + i e _ {j + n}, \quad j = 1, 2, \dots , n.
$$

They form a basis in $\mathbb { C } ^ { n }$ . Let $\begin{array} { r } { z ~ = ~ \sum _ { j = 1 } ^ { 2 n } z _ { j } e _ { j } } \end{array}$ be corresponding to $\begin{array} { r l } { \hat { z } } & { { } = } \end{array}$ $\sum _ { j = 1 } ^ { n } ( z _ { j } - i z _ { j + n } ) \phi _ { j }$ . Define the inner product

$$
\begin{array}{l} [ \hat {z}, \hat {w} ] = \operatorname {R e} \sum_ {j = 1} ^ {n} (z _ {j} - i z _ {j + n}) \overline {{(w _ {j} - i w _ {j + n})}} \\ = \sum_ {j = 1} ^ {n} \left(z _ {j} w _ {j} + z _ {j + n} w _ {j + n}\right) = \sum_ {j = 1} ^ {2 n} z _ {j} w _ {j}. \\ \end{array}
$$

Then the correspondence $z \mapsto { \hat { z } }$ is linear and inner product preserving, and then $\{ \phi _ { j } \vert j = 1 , \ldots , n \}$ is an orthogonal basis in $\mathbb { C } ^ { n }$ .

The real Hilbert space $L ^ { 2 } ( S ^ { 1 } , V )$ is isomorphic and homeomorphic to the complex Hilbert space $L ^ { 2 } ( S ^ { 1 } , \mathbb { C } ^ { n } )$ with inner product defined by

$$
\frac {1}{2 \pi} \int_ {S ^ {1}} [ \hat {z} (t), \hat {w} (t) ] d t = \langle z, w \rangle ,
$$

then $z \mapsto { \hat { z } }$ is an inner product preserving linear isomorphism of $L ^ { 2 } ( S ^ { 1 } , V ) \to$ $L ^ { 2 } ( S ^ { 1 } , \mathbb { C } ^ { n } )$ .

The space $L ^ { 2 } ( S ^ { 1 } , \mathbb { C } ^ { n } )$ has an orthonormal basis:

$$
\left\{e ^ {- i m t} \phi_ {j} \mid j = 1, 2, \dots , n; m = 0, \pm 1, \pm 2, \dots \right\}.
$$

Every $\hat { z } \in L ^ { 2 } ( S ^ { 1 } , \mathbb { C } ^ { n } )$ has the Fourier expansion

$$
\hat {z} = \sum_ {j = 1} ^ {n} \left(\sum_ {m = - \infty} ^ {+ \infty} c _ {j m} e ^ {- i m t}\right) \phi_ {j}.
$$

One has

$$
\sum_ {j = 1} ^ {n} \sum_ {m = - \infty} ^ {+ \infty} | c _ {j m} | ^ {2} = \| z \| ^ {2} = \frac {1}{2 \pi} \int_ {S ^ {1}} [ \hat {z}, \hat {z} ] d t.
$$

Hence

$$
\hat {z} \leftrightarrow c = \{c _ {j m} \}
$$

is an isometric isomorphism of $L ^ { 2 } ( S ^ { 1 } , \mathbb { C } ^ { n } ) \to ( l ^ { 2 } ) ^ { n } = \prod _ { j = 1 } ^ { n } l ^ { 2 }$

According to the Fourier expansion, $\hat { z } \in H ^ { 1 } ( S ^ { 1 } , \mathbb { C } ^ { n } )$ if and only if

$$
\sum_ {j = 1} ^ {n} \sum_ {m = - \infty} ^ {+ \infty} (1 + | m |) ^ {2} | c _ {j m} | ^ {2} <   + \infty .
$$

Note that

$$
- J \frac {d}{d t} (e ^ {- i m t} \phi_ {j}) = m e ^ {- i m t} \phi_ {j};
$$

it is easy to verify that $A$ is a self-adjoint operator with domain $D ( A ) =$ $H ^ { 1 } ( S ^ { 1 } , \mathbb { C } ^ { n } )$ , and then $\{ e ^ { - \imath m t } \phi _ { j } \}$ is just the diagonalized orthonormal basis. Let

$$
M (m) = \operatorname {s p a n} \left\{e ^ {- i m t} \phi_ {1}, \dots , e ^ {- i m t} \phi_ {n} \right\}, m \in \mathbb {Z},
$$

then

$$
L ^ {2} (S ^ {1}, \mathbb {C} ^ {n}) = \oplus_ {m \in \mathbb {Z}} M (m);
$$

this is the spectral decomposition or $A$ .

# 5.3.2 Periodic Solutions

Theorem 5.3.2 Suppose that $H \in C ^ { 2 } ( R ^ { n } \times R ^ { n } , R ^ { 1 } )$ satisfies:

(1) $H ( \theta ) = 0 , H ^ { \prime } ( \theta ) = \theta , H ^ { \prime \prime } ( \theta ) = 0 , H ( z ) \geqslant 0$ .   
(2) $\begin{array} { r } { H ( z ) = \frac { \gamma } { 2 } | z | ^ { 2 } } \end{array}$ , for $| z | \geqslant R , \ R > 0 , \gamma \in ( 1 , 2 ) .$

Then the Hamiltonian system (5.22) possesses a nontrivial $2 \pi$ -periodic solution.

Proof. Define $\begin{array} { r } { G ( z ) = H ( z ) - \frac { \gamma } { 2 } | z | ^ { 2 } } \end{array}$ . Set $\begin{array} { r } { A = - J { \frac { d } { d t } } } \end{array}$

$$
\psi (z) = \int_ {S ^ {1}} H (z (t)) d t,
$$

and

$$
g (z) = \int_ {S ^ {1}} G (z (t)) d t;
$$

then $\psi \in C ^ { 2 } ( \mathcal { H } , R ^ { \mathrm { i } } )$ , ( ${ \mathcal { H } } = L ^ { 2 } ( S ^ { 1 } , \mathbb { C } ^ { n } ) )$ possesses the following properties:

$$
\begin{array}{l} \left\| g ^ {\prime} (x) \right\| _ {L (\mathcal {H})} \leqslant M, \\ \psi (x) \geqslant \frac {\gamma}{2} \| x \| _ {\mathcal {H}} ^ {2} - b, \\ \psi^ {\prime} (x) = \gamma x + g ^ {\prime} (x). \\ \end{array}
$$

Denote the Sobolev space $H ^ { \frac { 1 } { 2 } } ( S ^ { 1 } , \mathbb { C } ^ { n } ) = D ( | A | ^ { \frac { 1 } { 2 } } )$ by $E$ , and let $E ^ { * }$ denote the subspace associated with the $^ *$ eigenvalues of $E$ , where $\ast = + , 0 , -$ . Let $P ^ { * }$ be the orthogonal projections onto $E ^ { * }$ , $* = + , 0 , -$ . We define a bilinear form on $E$ :

$$
\left(\left(z, w\right)\right) = \left(\left| A \right| ^ {\frac {1}{2}} z, \left| A \right| ^ {\frac {1}{2}} w\right),
$$

where $( , )$ is the inner product on $\mathcal { H }$ . The system (5.22) is rewritten as

$$
A z = \psi^ {\prime} (z), \quad z \in D (A) ， \tag {5.27}
$$

and equivalently,

$$
(A - \gamma) z = g ^ {\prime} (z).
$$

We introduce a functional on $E$ :

$$
\Phi (x) = \frac {1}{2} \left(\left(\left(P ^ {+} - P ^ {-}\right) x, x\right)\right) - \psi (x).
$$

Obviously, if $z \in D ( A )$ is a critical point of $\Phi$ , then $z$ solves (5.27).

We choose an arbitrary element $e$ in $M ( 1 )$ with norm one, and set

$$
Q = \left(\left(E ^ {-} \oplus E ^ {0}\right) \cap B _ {R} (\theta)\right) \times [ 0, R ] e \quad \text {f o r} R > 0 \text {l a r g e , a n d}
$$

$$
S = E ^ {+} \cap \partial B _ {\varepsilon} (\theta) \qquad \text {f o r} \varepsilon > 0 \text {s m a l l}.
$$

Since $\psi ( x ) = o ( \| x \| _ { \mathcal { H } } ^ { 2 } )$ as $\| \boldsymbol { x } \| _ { \mathcal { H } } \to 0$ , we have $\beta > 0$ such that

$$
\inf  _ {S} \Phi \geqslant \frac {\varepsilon^ {2}}{2} + o (\varepsilon^ {2}) \geqslant \beta .
$$

We turn out to estimate the values of $\Phi$ on $\partial Q$ :

$$
\Phi (x) \leqslant - \psi (x) \leqslant 0 \quad \forall x = x ^ {-} + x ^ {0} \in E ^ {-} \oplus E ^ {0},
$$

$$
\Phi \left(x ^ {-} + x ^ {0} + \mathrm {s e}\right) \leqslant - \frac {1}{2} \| x ^ {-} \| ^ {2} + \frac {1}{2} (1 - \gamma) s ^ {2} + b - \frac {\gamma}{2} \| x ^ {0} \| ^ {2} \leqslant 0,
$$

as $x ^ { - } + x ^ { 0 } + \sec \partial Q , s > 0$ and $R$ large enough. In summary $\Phi | _ { \partial Q } \leqslant 0$

Since both $S$ and $Q$ are infinite dimensional, and the intersection of $\partial Q$ and $S$ has not been discussed previously, we shall use the Galerkin method instead. Let $E _ { k } = \oplus _ { | j | \leqslant k } M ( j )$ , and $\Phi _ { k } = \Phi | _ { E _ { k } } .$ Then $S _ { k }$ and $\partial Q _ { k }$ link, where $S _ { k } = S \cap E _ { k } , Q _ { k } = Q \cap E _ { k } , \forall k$ . We verify the $( P S )$ condition for $\Phi _ { k }$ . Denote by $\Pi _ { k }$ the orthogonal projection onto $E _ { k }$ .

Let $\{ x _ { j } \} \subset E _ { k }$ be a sequence along which $\Phi _ { k } ^ { \prime } ( x _ { j } ) \to \theta$ . Then

$$
(A - \gamma) x _ {j} = \Pi_ {k} g ^ {\prime} (x _ {j}) + o (1),
$$

i.e.,

$$
x _ {j} = (A - \gamma) ^ {- 1} \left(\Pi_ {k} g ^ {\prime} (x _ {j}) + o (1)\right).
$$

Since $g ^ { \prime } ( x _ { j } )$ is bounded, $\{ x _ { j } \}$ is bounded, and then $\{ x _ { j } \}$ is subconvergent. Applying the minimax theorem for linkings, we have a critical point $z _ { k } \in E _ { k }$ satisfying

$$
\left\{ \begin{array}{l} (A - \gamma) z _ {j} = \Pi_ {k} g ^ {\prime} (z _ {k})  , \\ \Phi (z _ {k}) \geqslant \beta > 0  . \end{array} \right.
$$

It remains to prove that $\left\{ \boldsymbol { z } _ { k } \right\}$ is subconvergent in $E$

Since we have

$$
z _ {k} = (A - \gamma) ^ {- 1} \Pi_ {k} g ^ {\prime} (z _ {k}),
$$

and that the linear map $( A - \gamma ) ^ { - 1 } : L ^ { 2 }  H ^ { 1 }  E$ is compact, $\left\{ { z } _ { k } \right\}$ has a convergent subsequence with limit $\hat { z }$ . Thus $\Phi ( \hat { z } ) \geqslant \beta > 0$ and $\hat { z } = ( A -$ $\gamma ) ^ { - 1 } g ^ { \prime } ( \hat { z } )$ , which implies $\hat { z } ~ \in ~ D ( A )$ and that $\hat { z }$ is a nontrivial solution of (5.27). The proof is complete. □

# 5.3.3 Weinstein Conjecture

As to the given energy question, the breakthrough is due to Rabinowitz and Weinstein. In 1978, Rabinowitz [Ra 3] proved that a strongly star-shaped hypersurface (Weinstein [Wei 1] proved for a convex hypersurface) carries at least one periodic orbit. Nevertheless, the star-shapedness, and then the convexity, is not invariant under canonical diffeomorphism. Weinstein [Wei 2] called for a symplectic invariant version. He made the following conjecture:

Every compact smooth hypersurface of contact type carries at least a periodic orbit.

A hypersurface $\Sigma$ is said to be of contact type if there exists a vector field $X$ in a neighborhood of $\Sigma$ , such that

(1) $X \pitchfork \Sigma$ .   
(2) The flow $\phi ^ { t }$ , generated by $X$ (i.e., $\textstyle { \frac { d } { d t } } \phi ^ { t } = X \circ \phi ^ { t }$ and $\phi ^ { 0 } = \mathrm { i d }$ ) satisfies $( d \phi ^ { t } ) J ( d \phi ^ { t } ) ^ { T } = e ^ { t } J , \quad \forall t$ .

A matrix $A$ satisfying $A J A ^ { T } = \mu J$ , where $\mu$ is a nonzero constant, is called a canonical matrix with multiple $\mu$ . A diffeomorphism $\phi : R ^ { 2 n } \to R ^ { 2 n }$ satisfying $( d \phi ) J ( d \phi ) ^ { T } = \mu J$ , is called a canonical diffeomorphism with multiple $\mu$ .

Remark 5.3.3 In the definition of contact type, the condition (2) is equivalent to $L _ { X } \omega ~ = ~ \omega$ , where $\omega$ is the sympectic form $\omega ( u , v ) ~ = ~ \langle J u , v \rangle$ , $\forall u , v \in R ^ { 2 n }$ , $\langle \cdot \rangle$ is the inner product in $R ^ { 2 n }$ , and $L _ { X }$ is the Lie-derivative of the 2-form.

Lemma 5.3.4 Suppose that $\phi$ is a canonical diffeomorphism with multiple $\mu$ , and that $\widehat { z }$ is a solution of the Hamiltonian system

$$
\dot {z} = J H ^ {\prime} (z).
$$

Let $\widehat { H } ( u ) = \mu H ( z )$ , where $u = \phi ( z )$ . Then $\hat { u } = \phi ( \hat { z } )$ is a solution of

$$
\dot {u} = J \widehat {H} ^ {\prime} (u).
$$

Proof. By changing variables:

$$
\begin{array}{l} \dot {u} = d \phi (z) \cdot \dot {z} = (d \phi (z)) J (\nabla_ {z} H) \\ = \frac {1}{\mu} (d \phi (z)) J (d \phi (z)) ^ {T} \nabla_ {u} \widehat {H} \\ = \frac {1}{\mu} (d \phi (z)) \cdot J \cdot (d \phi (z)) ^ {T} \cdot (\nabla_ {u} \widehat {H}) \\ = J \widehat {H} ^ {\prime} (u). \\ \end{array}
$$

Example 5.3.5 A hypersurface $\Sigma$ is called strongly star-shaped, if there exists a point $x _ { 0 } \in R ^ { 2 n }$ , such that

$$
\langle x - x _ {0}, n (x) \rangle > 0 \quad \forall x \in \Sigma ,
$$

where $n ( x )$ is the unit outward normal vector on Σ. A strongly star-shaped hypersurface is of contact type.

Indeed, without loss of generality, we may assume $x _ { 0 } = \theta$ . Let the vector field

$$
X (x) = \frac {1}{2} x.
$$

The strong star-shapedness implies that $X \ \overline { { \pitchfork } } \ \Sigma$ ; the flow $\phi ^ { t }$ generated by $X$ is defined to be

$$
\phi^ {t} (x) = e ^ {t / 2} x,
$$

i.e., $\phi ^ { t } ( x ) = e ^ { t / 2 } \mathrm { i } \mathrm { c }$ d. Therefore,

$$
\left(d \phi^ {t}\right) J \left(d \phi^ {t}\right) ^ {T} = e ^ {t} J.
$$

Now we turn to the Weinstein conjecture. By definition, for a contact-type hypersurface $\Sigma$ , we have a family of hypersurfaces $\Sigma _ { \varepsilon } = \phi ^ { \varepsilon } ( \Sigma ) , \varepsilon \in ( - 1 , 1 )$ . Let $0 < \delta < 1$ , then $R ^ { 2 n } \backslash \bigcup \left\{ \Sigma _ { \varepsilon } \big | | \varepsilon | < \delta \right\}$ has two components: The interior $B$ , which with no with loss of generality contains the origin, and the exterior $A$ , which is unbounded. Also one may assume $\bigcup \left\{ \Sigma _ { \varepsilon } \big \vert \varepsilon \in ( - 1 , - \delta ) \right\} \subset B$ .

Let $d = \dim \bigcup \{ \Sigma _ { \varepsilon } | \varepsilon \in ( - 1 , 1 ) \}$ . We define d < r < 2d, $\begin{array} { r } { \frac { \gamma } { 2 } r ^ { 2 } < b < r ^ { 2 } , \gamma = } \end{array}$ $\frac { 3 } { 2 }$ , and functions $f \in C ^ { \infty } ( - 1 , 1 )$ , $g \in C ^ { \infty } ( 0 , \infty )$ satisfying

$$
f (s) = \left\{ \begin{array}{l l} 0, & s \in \big (- 1, - \delta \big ]  , \\ b, & s \in \big [ \delta , 1 \big)  , \end{array} \right.
$$

$$
g (s) = \left\{ \begin{array}{l} b, s \leq r, \\ \frac {\gamma}{2} s ^ {2}, s \geq R, \text {w h e r e} R > r  , \end{array} \right.
$$

$$
g (s) \geq \frac {\gamma}{2} s ^ {2} \text {f o r} s > r,
$$

$$
f ^ {\prime} (s) > 0 \quad \text {f o r} \quad s \in (- \delta , \delta) \qquad 0 <   g ^ {\prime} (s) <   \gamma s \qquad s \geqslant r.
$$

Then we define a Hamiltonian function:

$$
H (z) = \left\{ \begin{array}{l l} 0 & z \in B, \\ f (\varepsilon) & z \in \Sigma_ {\varepsilon} \quad | \varepsilon | \leqslant \delta , \\ b & z \in A \quad \text {a n d} \quad | z | \leqslant r, \\ g (| z |) & | z | > r. \end{array} \right.
$$

The function $H$ satisfies assumptions (1) and (2) of Theorem 5.3.2.

According to Theorem 5.3.2, there is a nontrivial $2 \pi$ -periodic solution $\hat { z }$ for this Hamiltonian system, with

$$
\Phi (\hat {z}) = \frac {1}{2} \left(- J \frac {d}{d t} \hat {z}, \hat {z}\right) - \int_ {S ^ {1}} H (\hat {z} (t)) d t \geqslant \beta \mathrm {f o r s o m e} \beta > 0.
$$

Lemma 5.3.6 The solution $\hat { z }$ lies on $\Sigma _ { \varepsilon }$ for some $\varepsilon \in ( - \delta , \delta )$ .

Proof. 1. If $| \hat { z } ( 0 ) | > r$ , then $| \hat { z } ( t ) | = | \hat { z } ( 0 ) |$ , because the energy surface of $H$ at $\hat { z } ( 0 )$ is a sphere. Thus

$$
\begin{array}{l} \Phi (\hat {z}) = \frac {1}{2} g ^ {\prime} (| \hat {z} (0) |) | \hat {z} (0) | ^ {2} \cdot 2 \pi - \int_ {0} ^ {2 \pi} g (| \hat {z} (t) |) d t \\ = \gamma \pi | \hat {z} (0) | ^ {2} - \gamma \pi | \hat {z} (0) | ^ {2} = 0. \\ \end{array}
$$

This is impossible.

2. If $| \hat { z } ( 0 ) | \leqslant r$ but $\hat { z } ( 0 ) \notin \bigcup _ { | \varepsilon | < \delta } \Sigma _ { \varepsilon }$ , then $H ( \hat { z } ( 0 ) ) = b$ or 0. In either case, $\hat { z } ( t ) = \mathrm { c o n s t }$ . Thus

$$
\Phi (\hat {z}) = - \int_ {0} ^ {2 \pi} H (| \hat {z} (t) |) d t \leqslant 0.
$$

Again, it is impossible.

In the remaining case, $\hat { z } ( 0 ) \in \Sigma _ { \varepsilon }$ for some $\varepsilon \in ( - \delta , \delta )$ , then the whole orbit $\hat { z } ( t )$ lies on $\Sigma _ { \varepsilon }$ , because $\Sigma _ { \varepsilon }$ is an energy surface of $H$ .

The lemma is proved.

![](images/ca0f49fda5325e22a5ada536230a07bd5d54fd5f9576691f756654af23187575.jpg)  
Fig. 5.5.

Theorem 5.3.7 (Viterbo) Every compact smooth hypersurface of contact type carries at least one periodic orbit.

Proof. Since $\phi ^ { - \varepsilon }$ is a canonical diffeomorphism, which maps $\Sigma _ { \varepsilon }$ onto $\Sigma$ , the Hamiltonian ${ \overline { { H } } } = H \circ \phi ^ { \varepsilon }$ , i.e.,

$$
\overline {{H}} (u) = H (z)
$$

with $u = \phi ^ { - \varepsilon } ( z )$ , has $\Sigma$ as an energy surface. According to Lemma 5.3.6, the periodic solution $\hat { z }$ on $\Sigma _ { \varepsilon }$ reduces to a periodic solution $\hat { u } = \phi ^ { - \varepsilon } ( \hat { z } )$ on $\Sigma$ of the Hamiltonian system (5.22) with Hamiltonian function $\overline { H }$ . However, $\overline { H }$ and $H$ both have $\Sigma$ as an energy surface; by Lemma 5.3.1, $\hat { u }$ is also a periodic solution of the Hamiltonian system (5.22) with Hamiltonian function $H$ . The proof is complete.

The above proof is due to Hofer and Zehnder (see [HZ 1]).

# 5.4 Prescribing Gaussian Curvature Problem on $S ^ { 2 }$

We continue the study of the prescribing Gaussian curvature problem in Sect. 4.2 Example 4. Let $g _ { 0 }$ be the canonical metric on $S ^ { 2 }$ , where $k = 1$ , and the problem is reduced to the following equations:

$$
\triangle u = 1 - K e ^ {2 u} \text {o n} S ^ {2}, \tag {5.28}
$$

where $K \in C ^ { \infty } ( S ^ { 2 } )$ is given.

Again, $\Lambda = \operatorname* { m a x } K > 0$ is a necessary condition for the existence of a solution. However, in this section we always assume that $K ( x ) > 0 \forall x \in S ^ { 2 }$ .

We introduce

$$
S (u) = \frac {1}{4 \pi} \int_ {S ^ {2}} [ | \nabla u | ^ {2} + 2 u ],
$$

and

$$
J (u) = S (u) - \log \frac {1}{4 \pi} \int_ {S ^ {2}} K e ^ {2 u}.
$$

The Euler–Lagrange equation for $J$ reads as

$$
\Delta u = 1 - \frac {4 \pi K e ^ {2 u}}{\int_ {S ^ {2}} K e ^ {2 u}} \text {o n} S ^ {2}. \tag {5.29}
$$

Noticing that $J ( u + c ) = J ( u ) \forall c \in \mathbb { R } ^ { 1 }$ , $u _ { 0 }$ is a critical point of $J$ if and only if $u$ solves (5.28), where $u = u _ { 0 } - c$ , and $\begin{array} { r } { c = \frac { 1 } { 2 } \log \frac { 1 } { 4 \pi } \int _ { S ^ { 2 } } K e ^ { 2 u _ { 0 } } , } \end{array}$ . Once we obtain a critical point of $J$ , after adjusting a constant, we solve equation (5.29). In order to avoid the indetermination caused by the translation invariance of $J$ , we add a constraint. Let

$$
X = \left\{u \in H ^ {1} (S ^ {2}) | \int_ {S ^ {2}} e ^ {2 u} = 4 \pi \right\},
$$

and consider the functional $J$ on $X$ . The Euler–Lagrange equation is again (5.29).

# 5.4.1 The Conformal Group and the Best Constant

In this section, we first analyze the special case $K \equiv 1$ , and find out all the minimizers.

Now, we introduce the following stereographic projection:

$$
\pi : S ^ {2} \to \hat {\mathbb {C}} = \mathbb {C} \cup \{\infty \}
$$

$$
\pi (x _ {1}, x _ {2}, x _ {3}) = z = \frac {x _ {1} + i x _ {2}}{1 - x _ {3}} \forall (x _ {1}, x _ {2}, x _ {3}) \in S ^ {2} \subset R ^ {3}.
$$

This is a conformal diffeomorphism with

$$
(\pi^ {- 1}) ^ {*} g _ {0} = \frac {4}{(1 + | z | ^ {2}) ^ {2}} d z d \bar {z}.
$$

Let $\mathrm { C o n f } ( S ^ { 2 } )$ be the conformal group consisting of all conformal diffeomorphisms from $S ^ { 2 }$ into itself. $\mathrm { C o n f } ( S ^ { 2 } )$ is isomorphic to the M¨obius group $S L ( 2 , \mathbb { C } )$ . A subgroup $\mathcal { D }$ of $\mathrm { C o n f } ( S ^ { 2 } )$ is defined as follows: for any $Q \in S ^ { 2 }$ , we choose an orthonormal frame $\{ e _ { 1 } , e _ { 2 } , e _ { 3 } \}$ , such that $Q$ is the north pole, i.e., $Q = e _ { 3 }$ , by which we define a stereographic projection $\pi _ { Q }$ , $\forall t \in [ 1 , \infty )$ , let $\tau _ { t } z = t z \ \forall z \in \hat { \mathbb { C } } , \phi _ { Q , t } = \pi _ { Q } ^ { - 1 } \circ \tau _ { t } \circ \pi _ { Q }$ , and let

$$
\mathcal {D} = \{\phi_ {Q, t} | (Q, t) \in S ^ {2} \times [ 1, \infty) \}.
$$

Since

$$
\phi_ {Q, 1} = \operatorname {i d}, \text {a n d} \phi_ {- Q, t} = \phi_ {Q, t ^ {- 1}} ,
$$

$\mathcal { D }$ is a submanifold and is parameterized to be $B ^ { 3 } \simeq ( S ^ { 2 } \times [ 1 , \infty ) ) / ( S ^ { 2 } \times \{ 1 \} )$ under the correspondence: $\begin{array} { r } { \phi _ { Q , t } \longrightarrow \frac { t - 1 } { t } Q } \end{array}$ .

For any Riemannian metric $g$ t on $S ^ { 2 }$ , $\forall \phi \in \operatorname { C o n f } ( S ^ { 2 } )$ , the function

$$
\psi = \frac {1}{2} \log \det  (d \phi) \tag {5.30}
$$

satisfies $\phi ^ { * } g _ { 0 } = e ^ { 2 \psi } g _ { 0 }$ . The conformal group action on the function $v$ reads as:

$$
\phi^ {*} v := v _ {\phi} = v \circ \phi + \psi . \tag {5.31}
$$

We define the conformal Laplacian as follows:

$$
L _ {g} \varphi = - \triangle_ {g} \varphi + K _ {g} \quad \forall \varphi \in C ^ {\infty} \left(S ^ {2}\right), \tag {5.32}
$$

where $\triangle _ { g }$ is the Laplacian–Beltrami operator and $K _ { g }$ is the Gaussian curvature with respect to $g$ , respectively. Thus, for $g = e ^ { 2 u } g _ { 0 }$ , we have

$$
\begin{array}{l} L _ {g} \varphi = - \triangle_ {g} \varphi + K _ {g} \\ = e ^ {2 \varphi} K _ {e ^ {2 \varphi} g} \tag {5.33} \\ = e ^ {- 2 u} \cdot e ^ {2 (u + \varphi)} K _ {e ^ {2 (u + \varphi)} g _ {0}} \\ = e ^ {- 2 u} L _ {g _ {0}} (u + \varphi). \\ \end{array}
$$

# Lemma 5.4.1

$$
L _ {g _ {0}} v _ {\phi} = \det  (d \phi) \left(L _ {g _ {0}} v\right) \circ \phi \tag {5.34}
$$

Proof.

$$
\begin{array}{l} \left(L _ {g _ {0}} v\right) \circ \phi = L _ {e ^ {2 \psi} g _ {0}} \left(v \circ \phi\right) \\ = e ^ {- 2 \psi} L _ {g _ {0}} (v \circ \phi + \psi) \\ = \det  (d \phi) ^ {- 1} L _ {g _ {0}} v _ {\phi}, \\ \end{array}
$$

provided by (5.33) and (5.31).

□

Lemma 5.4.2 All solutions of the equation:

$$
- \triangle \psi + 1 = e ^ {2 \psi} \quad \text {o n} S ^ {2} \tag {5.35}
$$

are of the form (5.30).

Proof. Since $L _ { g _ { 0 } } \psi = - \triangle \psi + 1$ , plugging $\psi = o _ { \phi }$ into (5.34), (5.35) follows from Lemma (5.4.1). Conversely, if $\psi$ is a solution of (5.35), then $K _ { g } = 1$ for $g = e ^ { 2 \psi } g _ { 0 }$ . According to the isometric theorem in Riemannian geometry, there exists an isometry $\phi : ( S ^ { 2 } , g )  ( S ^ { 2 } , g _ { 0 } )$ satisfying $\phi ^ { * } g _ { 0 } = g$ , i.e., $\phi \in$ $\mathrm { C o n f } ( S ^ { 2 } )$ . By (5.30), $\psi$ has the form ${ \frac { 1 } { 2 } } \log \operatorname* { d e t } ( d \phi )$ . □

Lemma 5.4.3 Both the functionals $S$ and $\begin{array} { r } { T ( u ) = \frac { 1 } { 4 \pi } \int _ { S ^ { 2 } } e ^ { 2 u } } \end{array}$ are conformal invariant. Furthermore, $S ( \psi ) = 0 \ \forall \psi$ of form (5.30).

Proof. 1. Obviously, $\forall \phi \in \operatorname { C o n f } ( S ^ { 2 } ) , \forall v \in H ^ { 1 } ( S ^ { 2 } )$ ,

$$
T (v _ {\phi}) = \frac {1}{4 \pi} \int_ {S ^ {2}} e ^ {2 (v \circ \phi)} \det (d \phi) = \frac {1}{4 \pi} \int_ {S ^ {2}} e ^ {2 v} = T (v).
$$

2. We prove that $S ( \psi ) = 0 \ \forall \psi$ of the form (5.30).

For any $\phi _ { 0 } \in \mathrm { C o n f } ( S ^ { 2 } )$ , we introduce a differentiable path $\phi _ { t } \in \operatorname { C o n f } ( S ^ { 2 } )$ , with $\phi _ { t } | _ { t = 0 } = \phi _ { 0 }$ . Let $\begin{array} { r } { \psi _ { t } = \frac 1 2 \log \operatorname* { d e t } ( d \phi _ { t } ) } \end{array}$ , and $\begin{array} { r } { w = \frac { d } { d t } \psi _ { t } | _ { t = 0 } } \end{array}$ . Then by step 1,

$$
0 = \frac {d}{d t} T \left(o _ {\phi_ {t}}\right) | _ {t = 0} = \frac {1}{2 \pi} \int_ {S ^ {2}} e ^ {2 \psi_ {0}} w, \tag {5.36}
$$

and

$$
\begin{array}{l} \frac {d}{d t} S (\psi_ {t}) | _ {t = 0} = \frac {1}{2 \pi} \int_ {S ^ {2}} [ \nabla \psi_ {0} \cdot \nabla w + w ] \\ = \frac {1}{2 \pi} \int_ {S ^ {2}} [ - \triangle \psi_ {0} + 1 ] w \\ = \frac {1}{2 \pi} \int_ {S ^ {2}} e ^ {2 \psi_ {0}} w = 0. \\ \end{array}
$$

Since the path $\phi _ { t }$ is arbitrary, we have $S ( \psi ) = \mathrm { c o n s t } .$ . In particular, $\mathrm { l e t } \phi _ { 0 } = \mathrm { i d }$ , then $\psi _ { 0 } = 0$ and $S ( \psi ) = 0$ .

3. Finally, we turn to proving the conformal invariant of $S$ . $\forall \phi \in \operatorname { C o n f } ( S ^ { 2 } )$ ,

$$
\begin{array}{l} S (v _ {\phi}) = \frac {1}{4 \pi} \left(\int_ {S ^ {2}} | \nabla (v \circ \phi) | ^ {2} + 2 \int_ {S ^ {2}} [ \nabla (v \circ \phi) \nabla \psi + v \circ \phi ]\right) + S (\psi) \\ = \frac {1}{4 \pi} \left(\int_ {S ^ {2}} | \nabla v | ^ {2} + 2 \int_ {S ^ {2}} \left(L _ {g _ {0}} \psi\right) v \circ \phi\right) \\ = \frac {1}{4 \pi} \left(\int_ {S ^ {2}} | \nabla v | ^ {2} + 2 \int_ {S ^ {2}} v \circ \phi e ^ {2 \psi}\right) \\ = S (v). \\ \end{array}
$$

□

In the sequel of this subsection we are going to prove that $S ( u ) \geq 0 \forall u \in X$ , and then, all the minimizers of $S$ on $X$ are of the form (5.30).

Firstly, we give a necessary condition for the solvability of (5.28). Namely:

Theorem 5.4.4 (Kazdan–Warner) If u is solution of (5.28) then

$$
\int_ {S ^ {2}} \left\langle \nabla K, \nabla x _ {i} \right\rangle e ^ {2 u} = 0 i = 1, 2, 3. \tag {5.37}
$$

Proof. By Lemma 5.4.3, $\forall \phi _ { Q , t } \in \mathcal { D }$ ,

$$
J \left(u _ {\phi_ {Q}, t}\right) = S (u) - \log \frac {1}{4 \pi} \int_ {S ^ {2}} K \circ \phi_ {Q, t} ^ {- 1} e ^ {2 u}. \tag {5.38}
$$

If $u$ is a solution of (5.28), then $u$ is a critical point of $J$ , therefore

$$
\frac {d}{d t} J (u _ {\phi_ {Q, t}}) | _ {t = 1} = \left\langle J ^ {\prime} (u), \frac {d}{d t} u _ {\phi_ {Q, t}} | _ {t = 1} \right\rangle = 0.
$$

By (5.38),

$$
\frac {d}{d t} J (u _ {\phi_ {Q, t}}) | _ {t = 1} = \frac {1}{\int_ {S ^ {2}} K \circ \phi_ {Q , t} ^ {- 1} e ^ {2 u}} \int_ {S ^ {2}} \langle \nabla K, \nabla (x, Q) \rangle e ^ {2 u}.
$$

We choose $Q = e _ { 1 } , e _ { 2 }$ , and $e _ { 3 }$ , respectively; (5.37) follows.

![](images/d47dc6ab21bb04d7a2046d74a47b2622ea0b1b208fcd15c70043726f9c3c9cd5.jpg)

For any $u \in H ^ { 1 } ( S ^ { 2 } )$ , we define the mass center of $u$ to be

$$
P (u) = \left(\int_ {S ^ {2}} e ^ {2 u}\right) ^ {- 1} \cdot \left(\int_ {S ^ {2}} x _ {1} e ^ {2 u}, \int_ {S ^ {2}} x _ {2} e ^ {2 u}, \int_ {S ^ {2}} x _ {3} e ^ {2 u}\right).
$$

Obviously, $P ( u ) \in B ^ { 3 }$ . We want to show that $\forall u \in H ^ { 1 } ( S ^ { 2 } )$ , after a conformal transform, the mass center can be moved to the origin, i.e., there exists $( Q , t ) \in$ $S ^ { 2 } \times [ 1 , \infty )$ such that $P ( u \circ \phi _ { Q , t } ) = \theta$ , or equivalently, the nonlinear system:

$$
\int_ {S ^ {2}} x _ {i} e ^ {2 u _ {\phi_ {\theta , t}}} = 0, \quad i = 1, 2, 3, \tag {5.39}
$$

is solvable.

We solve the nonlinear system by IFT. Let $F : H ^ { 1 } ( S ^ { 2 } ) \times B ^ { 3 }  B ^ { 3 }$ be the map

$$
F (u, \xi) = \int_ {S ^ {2}} x \circ \phi_ {- Q, t} e ^ {2 u}, \tag {5.40}
$$

where $\xi = s ( t ) Q$ , and

$$
s (t) = \left\{ \begin{array}{l l} 1 - t ^ {- 1} & t \in [ 1, 2 ], \\ 1 - t ^ {- 2} \ln t & \mathrm {f o r} t \mathrm {l a r g e}, \end{array} \right.
$$

is a diffeomorphism: $[ 1 , \infty ) \mapsto [ 0 , 1 )$ .

We need some estimate for $\partial _ { \xi } F$ .

Lemma 5.4.5 Assume $\{ Q ( t ) \} \subset S ^ { 2 } , Q ( t )  Q _ { 0 }$ as $t \to + \infty$ . Then

$$
\int_ {S ^ {2}} x \circ \phi_ {Q (t), t} e ^ {2 u} \rightarrow Q _ {0} \int_ {S ^ {2}} e ^ {2 u} a s t \rightarrow + \infty \tag {5.41}
$$

uniformly in $\{ u \in H ^ { 1 } ( S ^ { 2 } ) | \quad \| u \| \leq c \} \ \forall c > 0$ . Moreover,

$$
e ^ {2 \psi_ {Q, t}} \rightarrow 4 \pi \delta (- Q) a s t \rightarrow + \infty . \tag {5.42}
$$

Proof. Since the mapping $H ^ { 1 } ( S ^ { 2 } ) ~  ~ L ^ { 1 } ( S ^ { 2 } ) : w \mapsto e ^ { 2 w }$ is compact, it is sufficient to prove (5.41) for fixed $u$ . Noticing that

$$
x \circ \phi_ {Q, t} = D ^ {- 1} \left\{2 t (x - (Q \cdot x) Q) + \left[ t ^ {2} (1 + Q \cdot x) - (1 - Q \cdot x) \right] Q \right\},
$$

where

$$
D = t ^ {2} (1 + Q \cdot x) + (1 - Q \cdot x),
$$

we have

$$
x \circ \phi_ {Q (t), t} \to Q _ {0} \text {e x c e p t} x = - Q _ {0}.
$$

Our conclusion follows from the Lebesgue dominance theorem.

![](images/2a1d22c97255134ef10401643a358f580177f284e15fe78e489b9e62ee733d7e.jpg)

Lemma 5.4.6 For any compact subset $C \subset H ^ { 1 } ( S ^ { 2 } )$ and any constants $b > 1$ , the matrix $\partial _ { \xi } F ( u , \xi )$ has a uniformly bounded inverse $\forall u \in C , \ t \leq b$ , where $\xi = s ( t ) Q$ solves the equation $F ( u , \xi ) = \theta$ .

Proof. We write $\widetilde { \boldsymbol { x } } = \boldsymbol { x } \circ \boldsymbol { \phi } _ { Q , t }$ , and compute

$$
\partial_ {t} \widetilde {x} = \frac {1}{t} (Q - (\widetilde {x} \cdot Q) \widetilde {x}),
$$

$$
\partial_ {e} \widetilde {x} = \frac {t ^ {2} - 1}{2 t} (e - (\widetilde {x} \cdot e) \widetilde {x}) + \frac {(t ^ {2} - 1) ^ {2}}{2 t} [ (\widetilde {x} \cdot e) Q - (\widetilde {x} \cdot Q) e ],
$$

$\forall e \in T _ { Q } ( S ^ { 2 } )$ . One chooses an orthonormal frame $\{ e _ { 1 } , e _ { 2 } \}$ in $T _ { Q } ( S ^ { 2 } )$ ; the matrix $\partial _ { \xi } F ( u , \xi )$ is then an array of the three vectors $\begin{array} { r l } {  { \big ( \frac { t ^ { 2 } - 1 } { 2 t } g _ { 1 } , \frac { t ^ { 2 } - 1 } { 2 t } g _ { 2 } , \frac { 1 } { t s ^ { \prime } ( t ) } g _ { 3 } \big ) } \quad } & { { } } \end{array}$ , where

$$
g _ {i} = \frac {1}{4 \pi} \int_ {S ^ {2}} (e _ {j} - (\hat {x} \cdot e _ {j}) \hat {x}) e ^ {2 u} j = 1, 2, 3,
$$

$\hat { x } = x \circ \phi _ { - Q , t }$ and $e _ { 3 } = Q$ , because $\xi$ is a solution.

definite. Indeed, Let $A$ be the matrix consisting of $\begin{array} { r } { \forall \lambda = \sum _ { i = 1 } ^ { 3 } \lambda _ { i } e _ { i } } \end{array}$ with $( g _ { 1 } , g _ { 2 } , g _ { 3 } )$ $| \lambda | = 1$ 3, . It is symmetric and positive

$$
\begin{array}{l} (A \lambda) \cdot \lambda = \left(\sum_ {i = 1} ^ {3} \lambda_ {i} g _ {i}\right) \cdot \lambda \\ = 1 - \frac {1}{4 \pi} \int_ {S ^ {2}} (\hat {x} \cdot \lambda) ^ {2} e ^ {2 u}. \\ \end{array}
$$

Fixing $u$ , $\exists \delta = \delta ( u ) > 0$ such that

$$
\frac {1}{4 \pi} \int_ {S ^ {2}} (\hat {x} \cdot \lambda) ^ {2} e ^ {2 u} \leq 1 - \delta
$$

uniformly in $( Q , t ) \in S ^ { 2 } \times [ 1 , b ]$ . Since $C$ is compact, $\exists \delta _ { 0 } > 0$ such that

$$
\frac {1}{4 \pi} \int_ {S ^ {2}} (\hat {x} \cdot \lambda) ^ {2} e ^ {2 u} \leq 1 - \delta_ {0}, \forall (u, \xi) \in C \times (S ^ {2} \times [ 1, b ]).
$$

i.e.,

$$
(A \lambda) \cdot \lambda \geq \delta_ {0}.
$$

Thus $\exists \delta _ { 1 } > 0$ such that $| \operatorname* { d e t } \left( A \right) | \geq \delta _ { 1 }$ , and then

$$
| \det  (\partial_ {\xi} F (u, \xi)) | \geq \frac {(t ^ {2} - 1) ^ {2} \delta_ {1}}{4 t ^ {3} s ^ {\prime} (t)}.
$$

The lemma is proved.

We are ready to solve (5.40) by the continuity method. According to Lemma 5.4.6, equation (5.39) is locally solvable for any solution $( u _ { 0 } , \xi _ { 0 } )$ of (5.39) from the implicit function theorem. One chooses a starting point $( u _ { 0 } , \xi _ { 0 } )$ , satisfying $F ( u _ { 0 } , \xi _ { 0 } ) = \theta$ . For any $u \in H ^ { 1 } ( S ^ { 2 } )$ , we consider a path $\begin{array} { r } { u ( \tau ) = \frac { 1 } { 2 } \mathrm { { l o g } ( \tau e ^ { 2 u } + ( 1 - \tau ) e ^ { 2 u _ { 0 } } ) } } \end{array}$ connecting $u _ { 0 }$ and $u$ , and study the equation $F ( u ( \tau ) , \xi ( \tau ) ) = \theta$ for $\tau \in [ 0 , 1 ]$ .

Let $[ 0 , \tau _ { 0 } )$ be the maximal existence interval. If $\tau _ { 0 } = 1$ , then (5.39) is solvable for such a $u$ . Otherwise, there must be a sequence $\tau _ { j } \  \ \tau _ { 0 }$ such that $t _ { j } = t ( \tau _ { j } ) \to + \infty$ . Because, if not, $t ( \tau )$ is bounded on $\lfloor 0 , \tau _ { 0 } )$ , and then by Lemma 5.4.6, $( \partial _ { \xi } F ( u ( \tau ) , \xi ( \tau ) ) ) ^ { - 1 }$ is uniformly bounded. The solution is extendible beyond $\tau _ { 0 }$ . This contradicts the maximality of $\tau _ { 0 }$ .

After subtracting a subsequence from $\{ \tau _ { j } \}$ , again denoted by $\{ \tau _ { j } \}$ , we have $Q _ { j } = Q ( \tau _ { j } )  Q _ { 0 } \in S ^ { 2 }$ . By Lemma 5.4.5,

$$
\begin{array}{l} \theta = F (u (\tau_ {j}), \xi (\tau_ {j})) \\ = \int_ {S ^ {2}} x \circ \phi_ {- Q _ {j}, t _ {j}} e ^ {2 u (\tau_ {j})} \rightarrow - Q _ {0} \int_ {S ^ {2}} e ^ {2 u (\tau_ {0})}. \\ \end{array}
$$

But this is impossible. Thus we have completed the proof.

Theorem 5.4.7 $\forall u \in H ^ { 1 } ( S ^ { 2 } )$ , there exists $( Q , t ) \in S ^ { 2 } \times [ 1 , \infty )$ satisfying (5.39).

Now let us define

$$
I (u) = S (u) - \log \frac {1}{4 \pi} \int_ {S ^ {2}} e ^ {2 u}.
$$

Obviously, $I ( u ) = S ( u ) \ \forall u \in X$ . We shall prove:

Theorem 5.4.8 $I ( u ) \geq 0 \ \forall u \in H ^ { 1 } ( S ^ { 2 } )$ , and the equality holds if and only if $\begin{array} { r } { u = \frac { 1 } { 2 } \log \operatorname* { d e t } ( d \phi ) + c , \ \forall \phi \in \operatorname { C o n f } ( S ^ { 2 } ) \ \forall c \in R ^ { 1 } } \end{array}$ . Thus all the minimizers of $S$ on $X$ are of the form (5.30).

Proof. First we prove the existence of a minimizer of $I$ on $Y ~ = ~ \{ u ~ \in$ $H ^ { 1 } ( S ^ { 2 } ) \vert ~ P ( u ) = \theta , ~ \int _ { S ^ { 2 } } u = 0 \}$ $H ^ { 1 } ( S ^ { 2 } ) |$ . According to Aubin’s inequality Sect. 4.2, $\forall u \in Y$ .

$$
\begin{array}{l} I (u) = \frac {1}{4 \pi} \int_ {S ^ {2}} [ | \nabla u | ^ {2} + 2 u ] - \log \frac {1}{4 \pi} \int_ {S ^ {2}} e ^ {2 u} \\ \geq \left(\frac {1}{4 \pi} - \frac {1}{8 \pi - \epsilon}\right) \| u \| ^ {2} - \log C _ {\epsilon}, \\ \end{array}
$$

i.e., $I$ is coercive on $Y$ . Since $Y$ is weakly closed in $H ^ { 1 } ( S ^ { 2 } )$ , we conclude that there exists $u _ { 0 } \in Y$ satisfying $I ( u _ { 0 } ) = \operatorname* { m i n } \{ I ( u ) | u \in Y \}$ . The Euler–Lagrange equation reads as

$$
- \triangle u _ {0} + 1 - \left(\frac {1}{4 \pi} \int_ {S ^ {2}} e ^ {2 u _ {0}}\right) ^ {- 1} e ^ {2 u _ {0}} = \mu \cdot x e ^ {2 u _ {0}}, \mathrm {f o r s o m e} \mu \in R ^ {3}.
$$

Let $\begin{array} { r } { c = \frac { 1 } { 2 } \log \frac { 1 } { 4 \pi } \int e ^ { 2 u _ { 0 } } } \end{array}$ and $v = u _ { 0 } - c$ , then

$$
- \triangle v + 1 = (1 + \mu \cdot x e ^ {2 c}) e ^ {2 v}.
$$

Applying the necessary condition in Theorem 5.4.4, we have

$$
\int_ {S ^ {2}} \mu e ^ {2 u _ {0}} = \theta ,
$$

i.e., $\mu = \theta$ . Hence $v$ is a solution of (5.35), it must be of form (5.30). However, $P ( v ) = \theta$ ; it follows that $v = \theta$ , or $u _ { 0 } = \mathrm { c o n s t }$ . Therefore $I ( u _ { 0 } ) = 0$ and then $I ( u ) \geq 0 \ \forall u \in Y$ .

Since $I$ is translation invariant: $I ( u + c ) = I ( u ) \forall c \in R ^ { 1 }$ , and also conformal invariant: $I ( u _ { \phi } ) = I ( u ) \ \forall \phi \in \mathrm { C o n f } ( S ^ { 2 } )$ , our conclusion follows from Theorem 5.4.7. □

Returning to equation (5.28), in which $K \equiv 1$ , all solutions are also the minimizers of the functional $I$ . We may even write down these solutions explicitly:

$$
\psi = \frac {1}{2} \log \frac {1 - \alpha^ {2}}{(1 + \alpha Q \cdot x) ^ {2}} \tag {5.43}
$$

where $\begin{array} { r } { \alpha = \frac { 1 - t ^ { 2 } } { 1 + t ^ { 2 } } } \end{array}$ , for $( Q , t ) \in S ^ { 2 } \times [ 1 , \infty )$ . Although $\mathrm { C o n f } ( S ^ { 2 } )$ is a group with six parameters, the representation $\begin{array} { r } { \psi _ { Q , t } = \frac { 1 } { 2 } \log \operatorname* { d e t } ( d \phi _ { Q , t } ) } \end{array}$ only has three, i.e., it is totally determined by the subgroup $\mathcal { D }$ .

Corollary 5.4.9 (Onofri) $\forall u \in H ^ { 1 } ( S ^ { 2 } )$ , we have

$$
\frac {1}{4 \pi} \int_ {S ^ {2}} e ^ {2 u} \leq \exp \left[ \frac {1}{4 \pi} \int_ {S ^ {2}} (| \nabla u | ^ {2} + 2 u) \right].
$$

see also Hong [Hon 1].

# 5.4.2 The Palais–Smale Sequence

From Theorem 5.4.8, we see that $J$ is bounded below. We claim that if $K \neq$ const, then the infimum of $J$ is not achieved. Indeed, let $P _ { 0 } ~ \in ~ S ^ { 2 }$ be the maximum of $K$ . On one hand, we have $J ( u ) \ge - \log K ( P _ { 0 } )$ , provided by Corollary 5.4.9. On the other hand,

$$
J (\psi_ {- P _ {0}, t}) = S (\psi_ {- P _ {0}, t}) - \log \frac {1}{4 \pi} \int_ {S ^ {2}} K e ^ {2 \psi_ {- P _ {0}, t}} = - \log \frac {1}{4 \pi} \int_ {S ^ {2}} K e ^ {2 \psi_ {- P _ {0}, t}}.
$$

Applying (5.42), we obtain $J ( \psi _ { - P _ { 0 } , t } )  - \log K ( P _ { 0 } )$ , i.e., $\operatorname* { i n f } J = - \log K ( P _ { 0 } )$

If $u _ { 0 }$ were a minimum of $J$ , then we would have $J ( u _ { 0 } ) = - \log K ( P _ { 0 } )$ Define

$$
\hat {J} (u) = S (u) - \log \frac {1}{4 \pi} \int_ {S ^ {2}} K (P _ {0}) e ^ {2 u},
$$

then $\operatorname* { m i n } \hat { J } = - \log K ( P _ { 0 } )$ , which is achieved by $\boldsymbol { u } = \psi _ { Q , t } + c \forall c \in R ^ { 1 }$ , $\forall ( Q , t ) \in$ $S ^ { 2 } \times [ 1 , \infty )$ . Then

$$
- \log K (P _ {0}) \leq \hat {J} (u _ {0}) \leq J (u _ {0}) = - \log K (P _ {0}),
$$

which implies that $u _ { 0 }$ is a minimum of $\hat { J }$ . Therefore, $u _ { 0 } = \psi _ { Q , t } + c$ for some $( Q , t ) \in S ^ { 2 } \times [ 1 , \infty )$ and $c \in R ^ { 1 }$ . But this is impossible.

The above argument proved indirectly that the PS condition does not hold for $J$ (Ekeland variational principle). Now, we shall analyze the (PS) sequence carefully, and then reveal the reason why the (PS) condition breaks down.

Firstly, we continue the proof in Theorem 5.4.7, and obtain:

Theorem 5.4.10 Let $P ( u )$ be the mass center of u, and let

$$
X _ {0} = \left\{u \in X \mid P (u) = \theta \right\},
$$

then

$$
X \cong X _ {0} \times \stackrel {\circ} {B ^ {3}},
$$

where $\cong$ means diffeomorphism.

Proof. After Theorem 5.4.7, it remains to prove that the solution of equation (5.39) is unique.

Suppose not, then we may assume that $\xi _ { i } = s ( t _ { i } ) Q _ { i }$ satisfies (5.39), $i =$ $0 , 1$ . Define $w _ { i } = u _ { \phi _ { Q _ { i } , t _ { i } } }$ , $i = 0 , 1$ ; we pick a path $( Q ( \tau ) , t ( \tau ) )$ connecting $( Q _ { 0 } , t _ { 0 } )$ and $( Q _ { 1 } , t _ { 1 } )$ , and let $\begin{array} { r } { w ( \tau ) = \frac { 1 } { 2 } \log ( \tau e ^ { 2 w _ { 1 } } + ( 1 - \tau ) e ^ { 2 w _ { 0 } } ) \forall \tau \in [ 0 , 1 ] } \end{array}$ . Set

$$
u (\tau) = w (\tau) _ {\phi_ {Q (\tau), t (\tau)} ^ {- 1}},
$$

then

$$
F (u (\tau), \xi (\tau)) = \int_ {S ^ {2}} x \circ \phi_ {- Q (t), t (\tau)} e ^ {2 u (\tau)} = \theta .
$$

We introduce a new parameter $\sigma$ , and define for $( \sigma , \tau ) \in [ 0 , 1 ] ^ { 2 }$ ,

$$
u (\sigma , \tau) = \frac {1}{2} \log [ (1 - \sigma) e ^ {2 u (\tau)} + \sigma e ^ {2 u} ]
$$

and we solve the equation:

$$
F (u (\sigma , \tau), \xi (\sigma , \tau)) = \theta .
$$

By the same argument as used in Theorem 5.4.8, we have solutions $( Q ( \sigma , \tau )$ , $t ( \sigma , \tau )$ ) depending on $\sigma , \tau$ continuously, and satisfying

$$
\begin{array}{l} Q (0, \tau) = Q (\tau), t (0, \tau) = t (\tau). \\ Q (\sigma , i) = Q _ {i}, t (\sigma , i) = t _ {i}, i = 0, 1, \text {a n d} \\ F (u, \xi (1, \tau)) = \theta . \\ \end{array}
$$

This contradicts the local uniqueness of the solution of (5.39), which follows from the implicit function theorem. The smoothness of $( Q , t )$ depending on $u$ follows directly. □

Theorem 5.4.10 provides a parameterization of $X : X  X _ { 0 } \times ( S ^ { 2 } \times X )$ $[ 1 , \infty ) / S ^ { 2 } \times \{ 1 \} )$ , $u \mapsto ( w , Q , t )$ .

Theorem 5.4.11 Suppose that $\{ u _ { j } ~ = ~ ( w _ { j } , Q _ { j } , t _ { j } ) \}$ is a Palais–Smale sequence of $J$ on $X$ , then either it is subconvergent or $t _ { j } \to + \infty$ .

Proof. Since

$$
J (u _ {j}) = S (w _ {j}) - \log \int K \circ \phi_ {Q _ {j}, t _ {j}} e ^ {2 w _ {j}} \rightarrow c,
$$

and

$$
- \triangle w _ {j} + 1 = \frac {4 \pi K (\widetilde {x} _ {j}) e ^ {2 w _ {j}}}{\int K (\widetilde {x} _ {j}) e ^ {2 w _ {j}}} + \lambda_ {j} e ^ {2 w _ {j}} + \left(\mu_ {j} \cdot x\right) e ^ {2 w _ {j}} + o (1), \tag {5.44}
$$

where $( \lambda _ { j } , \mu _ { j } ) \in \mathbb { R } ^ { 1 } \times \mathbb { R } ^ { 3 }$ , if $\{ t _ { j } \}$ is bounded, then after a subsequence, $\widetilde { x } _ { j } =$ $x \circ \phi _ { Q _ { j } , t _ { j } }  \widetilde { x } = x \circ \phi _ { Q , t }$ for some $( Q , t ) \in S ^ { 2 } \times [ 1 , \infty )$ .

!It follows that $S ( w _ { j } )$ and $\lambda _ { j }$ are both bounded. Since

$$
\wedge (w _ {j}) \mu_ {j} = \frac {- \int K (\widetilde {x} _ {j}) x e ^ {2 w _ {j}}}{\int K (\widetilde {x} _ {j}) e ^ {2 w _ {j}}} + \circ (1),
$$

where

$$
\wedge (w _ {j}) = \left(\frac {1}{4 \pi} \int_ {S ^ {2}} x _ {i} x _ {k} e ^ {2 w _ {j}}\right) _ {3 \times 3},
$$

and $\wedge ( w _ { j } )$ is uniformly positive definite, $\{ \mu _ { j } \}$ is bounded. Moreover, the embedding $w \mapsto e ^ { 2 w }$ from $H ^ { 1 }$ t o $L ^ { p }$ , p > 1, is compact; we may use (5.44) to conclude that $\{ w _ { j } \}$ is subconvergent. □

# 5.4.3 Morse Theory for the Prescribing Gaussian Curvature Equation on $\mathbf { S } ^ { 2 }$

From Theorem 5.4.11, we know that the Palais–Smale Condition for $J$ breaks down at $t = + \infty$ . We are inspired with courage to compactify the open ball $\circ$ $B ^ { 3 }$ . By simple estimates, we obtain the limit of the integral:

$$
\frac {1}{4 \pi} \int_ {\mathcal {S} ^ {2}} K \circ \phi_ {Q, t} e ^ {2 w} = K (Q) + \circ (1) \text {a s} t \rightarrow \infty .
$$

Let us extend $J$ to be $\scriptstyle { \hat { J } }$ defined on the manifold with boundary $M = X _ { 0 } \times \overline { { B } } ^ { 3 }$ where $\partial M = X _ { 0 } \times S ^ { 2 }$ :

$$
\widetilde {J} (u) = \left\{ \begin{array}{l l} J (u) & u \in X \\ S (w) - \log K (Q) & (w, Q) \in X _ {0} \times S ^ {2}  . \end{array} \right.
$$

If $\hat { J }$ were $C ^ { 1 }$ on $M$ , then Remark 5.1.31 would be applicable directly to this problem. Unfortunately, this is not true. We are forced to figure out the asymptotic expansions for the partial derivatives for $J$ . Namely we have

Lemma 5.4.12 $\exists b > 0 , c _ { b } > 0$ and $\exists \widetilde { v } \in T _ { w } ( X _ { 0 } )$ such that

$$
\langle S ^ {\prime} (w), \widetilde {v} \rangle \geq c _ {b} S ^ {\frac {1}{2}} (w) \| \widetilde {v} \|,
$$

if $S ( w ) \leq b$ . Furthermore,  µ, δ > 0, N = N (δ, b, µ), T = T (δ, b, µ) such that

$$
\begin{array}{l} \left| \partial_ {e} J + \frac {\partial_ {e} K (Q)}{K (Q)} \right| \leq N t ^ {- 1 + \mu} \log t \forall e \in T _ {Q} \left(S ^ {2}\right), \\ \left| \partial_ {s} J - \frac {2 \triangle K (Q)}{K (Q)} \right| \leq N \left(\left| \nabla K (Q) \right| t \log^ {- \frac {1}{2}} t S ^ {\frac {1}{2}} (w) \right. \\ + (\log t) ^ {- 1} + t ^ {2} (\log t) ^ {- 1} S ^ {2} (w)). \\ \end{array}
$$

if $K ( Q ) \geq \delta$ and $t \geq T$ .

The proof is referred to Chang and Liu [CL 2].

Combining Lemma 5.4.12 with the partition of unity, we obtain:

Theorem 5.4.13 Assume $K > 0$ . If $\triangle K \ne 0$ whenever $\nabla K ( x ) = \theta$ , then 3 there exists a function $f$ defined on $X _ { 0 } \times \overline { { B } } ^ { 3 }$ possessing the same critical set as $J$ , and a neighborhood $U$ of $X _ { 0 } \times S ^ { 2 }$ , such that on $U$

$$
f (w, s Q) = S (w) - \log K (Q) - \frac {2 \triangle K (Q)}{K (Q)} (1 - s),
$$

where $s : [ 1 , \infty ) \to [ 0 , 1 )$ is a diffeomorphism with $s ( t ) = 1 - t ^ { - 2 } \log t$ for $t$ large.

Furthermore, $f$ satisfies the (PS) condition.

Now we are ready to apply the Morse theory under general boundary conditions to $f$ on $X _ { 0 } \times \overline { { B } } ^ { 3 }$ . For given $a < b$ such that $e ^ { - a } , e ^ { - b }$ are regular values of $K$ , we assume:

(I) $K$ has only a finite number of critical points with values in the interval $[ e ^ { - b } , e ^ { - a } ]$ .

Set

$$
\Omega = \{x \in S ^ {2} | \triangle K (x) <   0 \},
$$

CR0(a, b) = {x ∈ Ω| $K ( x ) \in ( e ^ { - b } , e ^ { - a } )$ , $x$ is a local maximum of $\mathrm { K } \}$

$C R _ { 1 } ( a , b ) = \{ x \in \Omega | K ( x ) \in ( e ^ { - b } , e ^ { - a } )$ , x is a saddle point of K} .

Theorem 5.4.14 Under assumption (I), assume (II)

$$
\triangle K (x) \neq 0 \text {w h e n e v e r} \nabla K (x) = \theta .
$$

If $J$ has only isolated critical points, then the following Morse inequalities hold:

$$
\sum (m _ {q} + \mu_ {q} - \beta_ {q}) t ^ {q} = (1 + t) Q (t),
$$

where $Q$ is a formal series with nonnegative coefficients, and

mq = qth Morse type number of $J$ in $J ^ { - 1 } [ a , b ]$ ,

βq = rank Hq(Jb, Ja) ,

$$
\mu_ {g} = \left\{ \begin{array}{l l} 0 & q \geq 2 \\ \sharp C R _ {q} (a, b) & q = 0, 1  . \end{array} \right.
$$

Proof. It is equivalent to study the Morse inequalities for $f$ .We notice that $f$ i s of separate variables, and that $S ( w )$ has no critical point except the minimum $\theta$ on $X _ { 0 }$ , from Lemma 5.4.12. Moreover, $f$ possesses the same critical set as $J$ and equals $J$ in a neighborhood of the critical set.

Now the restriction $\hat { f }$ on the boundary $X _ { 0 } \times S ^ { 2 }$ reads as $S ( w ) - \log K ( Q )$ , which has only isolated critical points. Since

$$
f (w, s Q) = S (w) - \log K (Q) - 2 \frac {\triangle K (Q)}{K (Q)} (1 - s)
$$

near the boundary, and the normal

$$
n (w, Q) = (\theta , Q) \in T _ {w} (X _ {0}) \times R ^ {3},
$$

we see that

$$
\begin{array}{l} \Sigma_ {-} = \{(w, Q) \in X _ {0} \times S ^ {2} | \langle f ^ {\prime} (u), n (u) \rangle \leq 0 \} \\ = \left\{\left(w, Q\right) \in X _ {0} \times S ^ {2} \mid Q \in \Omega \cap K ^ {- 1} \left[ e ^ {- b}, e ^ {- a} \right] \right\}. \\ \end{array}
$$

It remains to compute the critical groups of the critical points of $- \log K$ on $\Omega$ .

Now $\forall Q \in C R _ { 0 } ( a , b )$

$$
\operatorname {r a n k} C _ {q} (- \log K, Q) = \delta_ {q _ {0}},
$$

and $\forall Q \in C R _ { 1 } ( a , b )$

$$
\operatorname {r a n k} C _ {q} (- \log K, Q) = \delta_ {q _ {1}},
$$

from (II).

We claim that $\mathrm { r a n k } C _ { 2 } ( - \log K , Q ) = 0 \forall Q \in \Omega$ . If not, $Q$ must be a local minimum of $K$ , and then $\triangle K ( Q ) \geq 0$ . This is a contradiction. Obviously we have

$$
\operatorname {r a n k} C _ {q} (- \log K, Q) = 0 \forall q > 2.
$$

On the other hand, $\theta$ is the minimum of $S$ on $X _ { 0 }$ ; it follows that

$$
\operatorname {r a n k} C _ {q} (S, \theta) = \delta_ {q _ {0}} \forall q  .
$$

Thus the Morse type numbers of $\hat { f }$ read as

$$
\mu_ {q} = \left\{ \begin{array}{l l} 0 & \forall q \geq 2 \\ \sharp C R _ {q} (a, b) & q = 0, 1  . \end{array} \right.
$$

The proof is complete.

□

Corollary 5.4.15 Under assumptions (I) and (II). Let

$$
p = \sharp \text {l o c a l} K \text {i n} \Omega ,
$$

$$
q = \sharp s a d d l e \text {p o i n t s} K \text {i n} \Omega .
$$

If $p \neq q + 1$ , then (5.28) admits a solution.

Proof. By taking $e ^ { - b } < \operatorname* { m i n } K$ and $e ^ { - a } > \operatorname* { m a x } K$ . Since $X _ { 0 } \times B ^ { 3 }$ is contractible, the conclusion follows from the Morse inequalities. □

Even assumption (I) can be dropped. Namely,

Theorem 5.4.16 Assume (II) and if $\mathrm { d e g } ( \Omega , \nabla K , \theta ) \neq 1$ , then (5.28) admits a solution.

Proof. First, we perturb $K$ to $\tilde { K }$ satisfying:

(1) $\widetilde K > 0$ possesses only nondegenerate critical points.   
(2) For a given neighborhood $U$ of the critical set $V$ of $K$ , $K ( x ) = \tilde { K } ( x ) \forall x \notin$ $U$ .   
(3) $\| \ K - \widetilde K \ \| _ { C ^ { 2 } }$ is small.

$\tilde { K }$ is constructed as follows: Find a cover $\cup _ { i = 1 } ^ { l } B _ { \epsilon } ( x _ { i } ) \subset U$ of $V , \ \forall i$ , we pull back the linear function on $B _ { \epsilon } ( x _ { i } )$ , and obtain a function $\phi _ { a , i } ( x )$ such that $\nabla \phi _ { a , i } ( x ) = a \ \forall x \in B _ { \epsilon } ( x _ { i } )$ , where $a$ is any given vector in $R ^ { 2 }$ . Let $\{ \rho _ { i } \} _ { 1 } ^ { l }$ be the partition of unity of $V$ with $\mathrm { s u p p } \rho _ { i } \subset B _ { \epsilon } ( x _ { i } )$ , and let

$$
\widetilde {K} (x) = K (x) - \sum \phi_ {a, i} (x) \rho_ {i} (x).
$$

By Sard’s theorem, $\parallel a \parallel$ can be chosen as small as we wish such that (1) and (3) hold.

Second, as in Theorem 5.4.13, we can construct a functional $\hat { f }$ similar to $f$ satisfying:

(1) In a neighborhood $O$ of $X _ { 0 } \times S ^ { 2 }$ , $\hat { f }$ is similar to $f$ with respect to $\widetilde { K }$ , and there are no critical points there.   
(2) Outside $O$ , ${ \widetilde { f } } = f = J$ .

Now, let $p , q$ be the numbers of local maxima and saddle points of $\widetilde { K }$ in $\widetilde \Omega = \{ x \in S ^ { 2 } | \triangle \widetilde K ( x ) < 0 \}$ , respectively. By homotopy invariance of the Brouwer degree:

$$
1 \neq \det (\Omega , \nabla K, \theta) = \deg (\widetilde {\Omega}, \nabla \widetilde {K}, \theta) = p - q.
$$

This proves the existence of a critical point of $\hat { f }$ so does for $J$ .

Remark 5.4.17 Other sufficient conditions were obtained by Chen and Ding [CD]. Chang and Yang [CY 1, 2], Han [Han], and Hong [Hon 2]. All these results are contained in Theorem 5.4.14, see [CL 2].

Remark 5.4.18 The prescribing scalar curvature problem on $S ^ { n }$ is a similar problem. For $n = 3$ , it was due to Bahri and Coron [BC] and later, Schoen and Zhang [SZ]. For general n, see Li [Li], and Chen and Lin [ChL].

# 5.5 Conley Index Theory

The Conley index is another topological tool in nonlinear analysis. It can be seen as an extension of both the Leray–Schauder degree and the critical groups. In fact, the Conley index theory is an extension of the Morse theory. It is based on flows without a variational structure.

In contrast with the degree theory and the Morse theory, Conley’s theory studies the invariant sets of a flow rather than either the fixed points for a compact vector field or the critical points for differentiable functions.

Let $( X , d )$ be a metric space, and let $\varphi : R ^ { 1 } \times X \to X$ be a flow, i.e., it is continuous and satisfies:

(1)   
(2)

Definition 5.5.1 A set $S \subset X$ is called invariant with respect to the flow $\varphi$ , if $\varphi ( R ^ { 1 } , S ) = S$ , i.e., x S, t R1, $\varphi ( t , x ) \in S$ .

Example 1. Let $X$ be a Banach space, and let $K : X  X$ be compact. Let $\varphi$ be the flow derived by the compact vector field $f = \operatorname { i d } - K$ :

$$
\left\{ \begin{array}{l} \dot {\varphi} (t, x) = f (\varphi (t, x)) \quad \forall (t, x) \in R ^ {1} \times X  , \\ \varphi (0, x) = x  , \end{array} \right.
$$

then any subset of the fixed-point set of $K$ is an invariant set for the flow $\varphi$ .

Example 2. Let $M$ be a smooth Banach–Finsler manifold and let $f \in$ $C ^ { 1 } ( M , R ^ { 1 } )$ satisfy the (PS) condition. If $V$ is a pseudo-gradient vector field of $f$ and $\varphi$ is the associated flow:

$$
\dot {\varphi} (t, x) = - g (\varphi (t, x)) \frac {V (\varphi (t , x))}{\| V (\varphi (t , x)) \|} \text {w i t h} \varphi (0, x) = x, \tag {5.45}
$$

where $g ( x ) = \mathrm { M i n } \{ \mathrm { d } ( \mathrm { x } , \mathrm { K } ) , 1 \}$ , and $K$ is the critical set of $f$ .

Then any subset of the critical set of $f$ is an invariant set of the flow $\varphi$ .

Definition 5.5.2 We call the triple $( M , f , \varphi )$ a pseudo-gradient flow if $M$ is a smooth Banach–Finsler manifold, $f \in C ^ { 1 } ( M , R ^ { 1 } )$ satisfies the (PS) condition, and $\varphi$ is defined by equation (5.45.)

# 5.5.1 Isolated Invariant Set

Invariant sets can be extremely complicated. It is known from dynamical systems that there are chaotic dynamics and fractal structure. As we have learned in Chap. 3 and in Sects. 5.1 and 5.2, we single out isolated invariant sets as the object of our study. Before going on, we introduce some notions from dynamical systems.

$\forall T \in R _ { + } ^ { 1 } \cup \{ + \infty \}$ , for any subset $A$ of $X$ , we write

$$
G ^ {T} (A) = \bigcap_ {| t | \leq T} \varphi (t, \overline {{A}}), \text {a n d} \Gamma^ {T} (A) = \{x \in G ^ {T} (A) | \varphi ([ 0, T ], x) \cap \partial A \neq \emptyset \}.
$$

In particular, we write $I ( A ) = G ^ { \infty } ( A )$ . It is the maximal invariant subset of $\overline { { A } }$ .

By definition, we have the following properties:

(1) $G ^ { T } ( A ) = G ^ { T } ( { \overline { { A } } } )$

$G ^ { { \cal I } _ { 1 } } ( A ) \subset G ^ { { \cal I } _ { 2 } } ( A )$ , if T1 ≥ T2,

$A _ { 1 } \subset A _ { 2 } \Rightarrow G ^ { T } ( A _ { 1 } ) \subset G ^ { T } ( A _ { 2 } )$

$G ^ { T } ( A )$ is closed,

$G ^ { T _ { 1 } + T _ { 2 } } ( A ) = G ^ { T _ { 2 } } ( G ^ { T _ { 1 } } ( A ) )$

(2) If $G ^ { T } ( A ) \subset \operatorname { i n t } ( A )$ , then $G ^ { 2 T } ( A ) \subset \operatorname { i n t } ( G ^ { T } ( A ) )$

Proof. Suppose not, then $\exists y ~ \in ~ G ^ { 2 ^ { \prime } I ^ { \prime } } ( A ) \cap \partial G ^ { I ^ { \prime } } ( A ) $ , i.e., $\exists y _ { n } \ \to \ y$ with $\varphi ( [ - T , T ] , y _ { n } ) \ \subset \ { \overline { { A } } }$ , or $\exists t _ { n } \ \in \ [ - T , T ]$ , $\varphi ( t _ { n } , y _ { n } ) \notin \overline { { A } }$ . There exists a subsequence $\{ t _ { n } ^ { \prime } \}$ of $\left\{ t _ { n } \right\}$ such that $t _ { n } ^ { \prime }  \bar { t } \in [ - T , T ]$ and $\varphi ( { \bar { t } } , y ) \in \partial A$ . But $y \in G ^ { 2 T } ( A )$ , i.e., $\varphi ( [ - 2 T , 2 T ] , y ) \in A$ , which implies that $\varphi ( { \bar { t } } , y ) \in G ^ { T } ( { \overline { { A } } } ) =$ $G ^ { T } ( A ) \subset \operatorname { i n t } ( A )$ , a contradiction. □

Now, we introduce a family of closed sets:

$\Sigma = \Sigma ( \varphi ) = \{ A \subset X |$ $A$ is closed, and $\exists T > 0$ such that $G ^ { T } ( A ) \subset \operatorname { i n t } ( A ) \}$

Definition 5.5.3 A neighborhood $U$ of an invariant set $S$ for the flow $\varphi$ is called isolating if $U \in \Sigma$ and $I ( U ) = S$ . An invariant set $S$ is called an isolated invariant set if there exists an isolating neighborhood.

Remark 5.5.4 If $U$ is compact and if $I ( U ) = S \subset \operatorname { i n t } ( U )$ , then $U \in \Sigma$ , i.e., $\exists T > 0$ such that $G ^ { T } ( U ) \subset \operatorname { i n t } ( U )$ .

In fact, if not, i.e., $G ^ { n } ( U ) \not \subset \mathrm { i n t } ( U ) \ \forall n \in \mathbb { N }$ , we have $y _ { n } \in G ^ { n } ( U ) \backslash \operatorname { i n t } ( U )$ , i.e., $\varphi ( [ - n , n ] , y _ { n } ) \subset U$ but $y _ { n } \not \in \operatorname { i n t } ( U )$ . Since $U$ is compact, there is a subsequence $y _ { n _ { i } } \to y _ { 0 } \in I ( U )$ , but $y _ { 0 } \not \in \operatorname { i n t } ( U )$ . A contradiction.

Moreover, we have:

(3) If $U \in \Sigma$ , then $\exists T _ { 1 } > 0$ such that $G ^ { T 1 } ( U ) \in \Sigma$ , and $\forall t \in R ^ { 1 }$ , $\varphi ( t , U ) \in \Sigma$ .

Proof. It is known that both $G ^ { T _ { 1 } } ( U )$ and $\varphi ( t , U )$ are closed. Since $\exists T > 0$ such that $G ^ { T } ( U ) \subset \operatorname { i n t } ( U )$ . Taking $T _ { 1 } = T$ , from (1) and (2), we have $G ^ { T } ( G ^ { T _ { 1 } } ( U ) ) = G ^ { 2 T ^ { \prime } } ( U ) \subset \mathrm { i n t } ( G ^ { T ^ { \prime } } ( U ) ) = \mathrm { i n t } ( G ^ { T _ { 1 } } ( U ) )$ . Also

$$
G ^ {T} (\varphi (t, U)) = \bigcap_ {| s | \leq T} \varphi (t + s, U) = \varphi (t, G ^ {T} (U)) \subset \varphi (t, \operatorname {i n t} (U)) = \operatorname {i n t} (\varphi (t, U)).
$$

(4) If $U \in \Sigma$ , then $\Gamma ^ { T } ( U ) \subset \partial G ^ { T } ( U )$ is closed.

Proof. If $\{ x _ { n } \} \in \Gamma ^ { T } ( U )$ and $x _ { n } \to x$ , then $\exists t _ { n } \in [ 0 , T ]$ such that $\varphi ( t _ { n } , x _ { n } ) \in$ $\partial U$ , after a subsequence, we have $t _ { n } \implies \bar { t } \in [ 0 , T ]$ and $\varphi ( \bar { t } , x ) \in \partial U$ . Thus $x \in \Gamma ^ { T } ( U )$ . This shows that $\Gamma ^ { T } ( U )$ is closed.

Next, $\forall x \in \Gamma ^ { T } ( U )$ , we verify $x \in \partial G ^ { T } ( U )$ . By definition, $\exists { \bar { t } } \in [ 0 , T ] , \exists y _ { n } \notin$ $U$ such that $y _ { n } ~ \to ~ \varphi ( \bar { t } , x )$ . Let $x _ { n } ~ = ~ \varphi ( - \bar { t } , y _ { n } )$ , we have $x _ { n } \to x$ and $x _ { n } \notin G ^ { T } ( U )$ . Since $G ^ { T } ( U )$ is closed, we have $x \not \in \ i n t ( G ^ { \mathrm { T } } ( \mathrm { U } ) )$ , therefore $x \in \partial G ^ { T } ( U )$ . □

The following terminologies are taken from dynamical systems:

Definition 5.5.5 $\forall x \in X$ , the set

$$
\omega (x) = \bigcap_ {t > 0} \overline {{\varphi ([ t , \infty) , x)}}
$$

is called the $\omega$ -limit set of $x$ , and the set

$$
\omega^ {*} (x) = \underset {t > 0} {\cap} \overline {{\varphi ((- \infty , - t ] , x)}}
$$

is called the $\omega ^ { * }$ -limit set of $x$ . Given a subset $S \subset X$ , the set

$$
[ S ] = \{x \in X | \omega (x) \cup \omega^ {*} (x) \subset S \}
$$

is called the invariant hull of $S$ .

By definition, $\omega ( x ) = \omega ( \varphi ( t , x ) ) , \ \omega ^ { * } = \omega ^ { * } ( \varphi ( t , x ) ) \ \forall t \in R ^ { 1 }$ $\omega ( x ) = \omega ( \varphi ( t , x ) )$ , and then $[ S ]$ is invariant. Moreover, if $S$ is invariant, then $S \subset [ S ]$ .

Lemma 5.5.6 Let $( M , f , \varphi )$ be a pseudo-gradient flow. Then for any $x \in M$ the set $\omega ( x )$ is compact and is a subset of $K _ { c }$ for some critical value $c$ , where $K _ { c } = K \cap f ^ { - 1 } ( c )$ . The same holds true for the set $\omega ^ { * } ( x )$ .

Proof. Since the function is nonincreasing along the pseudo-gradient flow, first we show that $\omega ( x )$ is on one level, say $\omega ( x ) \subset f ^ { - 1 } ( c )$ for some $c \in R ^ { 1 }$ . Indeed, if not, then there exist $t _ { n }$ , $t _ { n } ^ { \prime } \uparrow \infty$ such that $\varphi ( t _ { n } , x )  y$ and $\varphi ( t _ { n } ^ { \prime } , x ) \to y ^ { \prime }$ with $f ( y ) < f ( y ^ { \prime } )$ . We may always assume that $t _ { n } ^ { \prime } > t _ { n }$ , which means that

$$
f \left(y ^ {\prime}\right) = \lim  _ {n} f \left(\varphi \left(t _ {n} ^ {\prime}, x\right)\right) \leq \lim  _ {n} f \left(\varphi \left(t _ {n}, x\right)\right) = f (y).
$$

This is a contradiction.

Next, we prove that $\omega ( x ) \subset K$ . Indeed, if $\exists y \in \omega ( x ) \backslash K$ , then we choose regular values $a < b$ such that both $f ( x )$ and $f ( y )$ are in $( a , b )$ . Since $K _ { a } ^ { b } =$ $K \cap f ^ { - 1 } [ a , b ]$ is compact, there exists $r > 0$ such that $B _ { r } ( y ) \cap ( K _ { a } ^ { b } ) _ { r } = \emptyset$ . By definition, there exists $t _ { n } \to + \infty$ such that $x _ { n } = \varphi ( t _ { n } , x ) \to y$ . From the (PS) condition, there exists $\delta > 0$ such that $\parallel f ^ { \prime } ( x ) \parallel \geq \delta$ for all $x \in f ^ { - 1 } [ a , b ] \backslash ( K _ { a } ^ { b } ) _ { r }$ ,

We claim that there exists $t _ { n } ^ { \prime } \to + \infty$ such that $x _ { n } ^ { \prime } = \varphi ( t _ { n } ^ { \prime } , x ) \in \partial ( K _ { a } ^ { b } ) _ { r }$ . If not, $\exists T > 0$ such that $\varphi ( [ T , \infty ) , x ) \cap ( K _ { a } ^ { b } ) _ { r } = \emptyset$ , then we would have

$$
f (y) = \lim  _ {n} f (x _ {n}) = \lim  _ {t \rightarrow + \infty} \inf  _ {0} f (\varphi (t, x)) \leq a,
$$

This is impossible. Now we choose $t _ { n } ^ { \prime \prime } \to + \infty$ with $t _ { n } ^ { \prime } < t _ { n } ^ { \prime \prime }$ such that

$$
x _ {n} ^ {\prime \prime} = \varphi (t _ {n} ^ {\prime \prime}, x) \in B _ {r} (y), \varphi ([ t _ {n} ^ {\prime}, t _ {n} ^ {\prime \prime} ], x) \cap (K _ {a} ^ {b}) _ {r} = \emptyset .
$$

It follows that

$$
f \left(x _ {n} ^ {\prime}\right) - f \left(x _ {n} ^ {\prime \prime}\right) \geq \delta \left\| x _ {n} ^ {\prime \prime} - x _ {n} ^ {\prime} \right\| \geq \delta \operatorname {d i s t} \left(B _ {r} (y), \left(K _ {a} ^ {b}\right) _ {r}\right).
$$

Again, this is impossible. The lemma is proved.

Definition 5.5.7 A subset $W$ of $X$ is said to have a mean value property (MVP) with respect to the flow $\varphi$ , if $\forall x \in X$ , $\forall t _ { 1 } < t _ { 2 }$ , ϕ(ti, x)  W, i = 1, 2 implies that $\varphi ( [ t _ { 1 } , t _ { 2 } ] , x ) \subset W$ .

We now introduce the following key concept:

Definition 5.5.8 (Dynamically isolated critical set) Let $( M , f , \varphi )$ be a pseudo-gradient flow. A subset $S$ of the critical set $K$ is said to be a dynamically isolated set if there exist a closed neighborhood $\boldsymbol { \mathcal { O } }$ of $S$ and regular values $\alpha < \beta$ of $f$ such that

$$
\mathcal {O} \subset f ^ {- 1} [ \alpha , \beta ],
$$

and

$$
\operatorname {c l} (\widetilde {\mathcal {O}}) \cap K \cap f ^ {- 1} [ \alpha , \beta ] = S,
$$

where $\widetilde { \mathcal { O } } = \bigcup _ { t \in R ^ { 1 } } \varphi ( t , \mathcal { O } )$ . We shall then say that $( \mathcal { O } , \alpha , \beta )$ is an isolating triplet for $S$ .

Lemma 5.5.9 Let $( M , f , \varphi )$ be a pseudo-gradient flow and $K$ be the critical set of $f$ . If $U$ is a closed (MVP) neighborhood of $S$ satisfying $U \cap K = S$ , then $[ S ] = I ( U )$ . If further, there exist real numbers $\alpha < \beta$ and a (MVP) closed set $W$ , satisfying $U \subset W \subset f ^ { - 1 } [ \alpha , \beta ]$ and $W \cap K = S$ , then $\exists T > 0$ such that $G ^ { T } ( W ) \subset \operatorname { i n t } ( \operatorname { U } )$ .

Proof. 1. $[ S ] = I ( U )$

$\mathbf { \tilde { \Sigma } } ^ { \ast } ( \Sigma ^ { \mathfrak { N } } \ \forall x \in [ S ]$ , by definition $\omega ( x ) \cup \omega ^ { * } ( x ) \subset S$ , then $\exists t _ { n } ^ { \pm } \to \pm \infty$ such that $\varphi ( t _ { n } ^ { \pm } , x ) \in U$ , and then $\varphi ( [ t _ { n } ^ { - } , t _ { n } ^ { + } ] , x ) \subset U$ . Since $n$ is arbitrary, it follows that $\varphi ( t , x ) \in U \ \forall t \in R ^ { 1 }$ , i.e., $x \in I ( U )$ .

“ ” $\forall x \in I ( U )$ , $\varphi ( t , x ) \in U \forall t \in R ^ { 1 }$ . Since $U$ is closed, $\omega ( x ) \cup \omega ^ { * } ( x ) \subset U$ . From Lemma 5.5.6, $\omega ( x ) \cup \omega ^ { * } ( x ) \subset K$ , therefore $\omega ( x ) \cup \omega ^ { * } ( x ) \subset U \cap K = S$ , i.e., $x \in [ S ]$ .

2. $G ^ { T } ( W ) \subset \operatorname { i n t } ( U )$

From the (PS) condition, $\exists \delta \in \mathsf { \Gamma } ( 0 , 1 )$ such that $\mathrm { d i s t } ( x , K ) \ \geq \ \delta$ and $\parallel$ $f ^ { \prime } ( x ) \parallel \geq \delta \forall x \in W \backslash \operatorname { i n t } ( U )$ . Set $T > \delta ^ { - 2 } ( \beta - \alpha )$ . We shall prove that $\forall x \notin$ $\operatorname { i n t } ( U ) \ \exists t \in [ - T , T ]$ such that $\varphi ( t , x ) \not \in W$ .

It is divided into three cases:

(a) $x \notin W$ . By taking $t = 0$ , it is done.   
(b) $x \in W \backslash \operatorname { i n t } ( U )$ . If $\varphi ( [ - T , T ] , x ) \subset W$ , then

$$
\begin{array}{l} f (\varphi (- T, x)) - f (\varphi (T, x)) = \int_ {- T} ^ {T} \langle f ^ {\prime} (\varphi (s, x)), \dot {\varphi} (s, x) \rangle d s \\ \geq 2 T \delta^ {2} > 2 (\beta - \alpha). \\ \end{array}
$$

The contradiction shows $\varphi ( [ - T , T ] , x ) \not \subset W$ .

(c) $x \ \in \ ( \operatorname { i n t } ( U ) \backslash \operatorname { i n t } ( U ) ) \cap W \ = \ \operatorname { i n t } ( U ) \cap W \backslash \operatorname { i n t } ( U )$ . Since either $x \in$ $\bigcup _ { t > 0 } \varphi ( t , \operatorname { i n t } ( U ) )$ or $x \in \cup _ { t < 0 } \varphi ( t , \operatorname { i n t } ( U ) )$ .

In the first case, by the use of (MVP) of $U$ , we have $t _ { 1 } \le 0 \le t _ { 2 }$ such that

$$
\varphi ([ t _ {1}, t _ {2} ], x) \subset (\widetilde {\operatorname {i n t} (U)} \cap W) \backslash \operatorname {i n t} (U) \text {a n d} \varphi (t _ {1} - \epsilon , x) \in U, \varphi (t _ {2} + \epsilon , x) \notin W
$$

for all $\epsilon > 0$ small. Again, we would have $\beta - \alpha \geq \delta ^ { 2 } ( t _ { 2 } - t _ { 1 } )$ so that $t _ { 2 } < T$ . Therefore $\varphi ( [ - T , T ] , x ) \not \subset W$ .

Similarly for the second case.

Theorem 5.5.10 Let $( M , f , \varphi )$ be a pseudo-gradient flow. If $( \mathcal { O } , \alpha , \beta )$ is an isolating triplet for a dynamically isolated critical set $S$ for $f$ , then $[ S ]$ is an isolated invariant set.

Moreover, any closed MVP neighborhood $U$ of $[ S ]$ , satisfying $U \subset c l ( { \widetilde { \mathcal { O } } } ) \cap$ $f ^ { - 1 } [ \alpha , \beta ]$ , is an isolating neighborhood for $[ S ]$ , and $U \in \Sigma$ .

Proof. 1. Applying Lemma 5.5.9, $W = \mathrm { c l } ( \tilde { \mathcal { O } } ) \cap f ^ { - 1 } [ \alpha , \beta ] \in \Sigma$ , is an isolated invariant neighborhood of $[ S ]$ .

2. To prove that $U \in \Sigma$ is an isolating neighborhood for $[ S ]$ , it is sufficient to show that $[ S ] = I ( U ) \subset G ^ { T } ( U ) \subset \operatorname { i n t } ( U )$ for some $T > 0$ . Since

$$
[ S ] = I ([ S ]) \subset I (U) \subset I (W) = [ S ],
$$

where $W = \mathrm { c l } ( \tilde { \mathcal { O } } ) \cap f ^ { - 1 } [ \alpha , \beta ]$ , we obtain $[ S ] = I ( U )$ .

Again by Lemma 5.5.9, we have $G ^ { T } ( U ) \subset G ^ { T } ( W ) \subset \operatorname { i n t } ( U )$

Example 1. If $c$ is an isolated critical value, i.e., $K _ { c } = K \cap f ^ { - 1 } ( c ) \neq \emptyset$ , and there is no critical point on the levels in $[ c - \epsilon , c + \epsilon ] \backslash \{ c \}$ for some $\epsilon > 0$ , then the set $K _ { c }$ is a dynamically isolated critical set.

Example 2. If $x _ { 0 }$ is an isolated critical point of $f$ , then $S = \{ x _ { 0 } \}$ is a dynamically isolated critical set.

# 5.5.2 Index Pair and Conley Index

As we have seen in the Morse theory, isolated neighborhoods only are not enough in characterizing the isolated critical points, the local dynamic behavior provides the necessary information. This leads us to:

Definition 5.5.11 Let $( N , L )$ be a pair of subspaces of $X$ . A subset $L$ of $N$ is called positively invariant in $N$ with respect to the flow $\varphi$ , if $x \in L$ and $\varphi ( [ 0 , t ] , x ) \subset N$ imply $\varphi ( [ 0 , t ] , x ) \subset L$ . It is called an exit of $N$ , if $\forall x \in$ $N , \exists t _ { 1 } > 0$ such that $\varphi ( t _ { 1 } , x ) \not \in N$ , implies $\exists t _ { 0 } \in [ 0 , t _ { 1 } )$ such that $\varphi ( [ 0 , t _ { 0 } ] , x ) \subset$ $N$ and $\varphi ( t _ { 0 } , x ) \in L$ .

Example. Let $( M , f , \phi )$ be a pseudo-gradient flow, and let $\alpha < \beta < \gamma$ . Let $N = f ^ { - 1 } [ \alpha , \gamma ]$ and $L = f ^ { - 1 } [ \alpha , \beta ]$ . $L$ is positively invariant in $N$ , and also an exit set of $N$ .

To an isolated invariant set $S$ , we introduce:

Definition 5.5.12 For $U \in \Sigma$ , let $( N , L )$ be a pair of closed subsets of $U$ with $L \subset N$ . It is called an index pair relative to $U$ if:

(1) $\overline { { N \backslash L } } \in \Sigma$ ,

(2) $L$ is positively invariant in $N$ ,   
(3) $L$ is an exit set of $N$ ,   
(4) ${ \overline { { N \backslash L } } } \subset U$ and $\exists T > 0$ such that $G ^ { T } ( U ) \subset { \overline { { N \backslash L } } }$ .

According to the definition $S : = I ( U ) = I ( N \backslash L )$ is an isolated invariant set, and both $U$ and $\overline { { N \backslash L } }$ are isolating neighborhoods of $S$ .

Example. Let $( M , f , \varphi )$ be a pseudo-gradient flow. If $( \mathcal { O } , \alpha , \beta )$ and $( \mathcal { O } ^ { \prime } , \alpha ^ { \prime } , \beta ^ { \prime } )$ are two isolating triplets for a dynamically isolated critical set $S$ for $f$ , with $\mathcal { O } ^ { \prime } \subset \mathcal { O }$ , $[ \alpha ^ { \prime } , \beta ^ { \prime } ] \subset [ \alpha , \beta ]$ , then $\exists T > 0$ such that $N = G ^ { I ^ { \prime } } ( W ) , L = \varphi ( - T , W _ { - } )$ is an index pair relative to $U = \mathrm { c l } ( \widetilde { \mathcal { O } ^ { \prime } } ) \cap f ^ { - 1 } [ \alpha ^ { \prime } , \beta ^ { \prime } ]$ , where $W = \mathrm { c l } ( \tilde { \mathcal { O } } ) \cap$ $f ^ { - 1 } [ \alpha , \beta ]$ and $W _ { - } = \operatorname { c l } ( { \widetilde { \mathcal { O } } } ) \cap f ^ { - 1 } ( \alpha )$ . Moreover, both $U$ and $N$ are isolating neighborhoods of $[ S ]$ .

Note that $N \backslash L = N$ and $\operatorname { i n t } ( N \backslash L ) = \operatorname { i n t } ( N )$ . In order to verify the conclusion, we need:

1. $\forall T > 0 , N = G ^ { \prime I ^ { \prime } } ( W )$ is a closed MVP neighborhood of $[ S ]$ .

It is sufficient to verify that $[ S ] \subset \operatorname { i n t } ( N )$ . From Lemma 5.5.9, we have $[ S ] \subset N$ . If the conclusion is not true, then $\exists x \in | S | \cap \partial N$ , i.e., $\exists x _ { n } \not \in G ^ { T } ( W )$ with $x _ { n } \longrightarrow x$ . This means that there are $t _ { n } \in [ - T , T ]$ , such that $\varphi ( t _ { n } , x _ { n } ) \not \in W$ . After a subsequence, we have $t _ { n } ^ { \prime }  t \in [ - T , T ]$ and then $\varphi ( t , x ) \not \in \operatorname { i n t } ( W )$ . But $x \in [ S ]$ implies that $\varphi ( t , x ) \in [ S ] \subset \operatorname { i n t } ( W )$ , provided by Lemma 5.5.9. This is a contradiction.

Now, we are going to verify conditions (1)–(4).

2. Applying Lemma 5.5.9 to $U = N$ , $\exists T _ { 0 } > 0$ such that

$$
G ^ {T + T _ {0}} (W) \subset G ^ {T _ {0}} (W) \subset \operatorname {i n t} (N).
$$

Since $G ^ { \prime } { } ^ { T + } { } ^ { T _ { 0 } } ( W ) = G ^ { \prime } { } ^ { T _ { 0 } } ( N )$ . This shows that $N \backslash L \in \Sigma$ , (1) is verified.

3. Again applying Lemma 5.5.9, $\exists T > 0$ such that $N = G ^ { T } ( W ) \subset \operatorname { i n t } ( U ) \subset$ $U$ . Moreover,

$$
G ^ {T} (U) \subset G ^ {T} (W) = N = \overline {{N \backslash L}}.
$$

Poperty (4) is verified.

4. Since $W _ { - }$ is an exit set of $W$ , $L$ is an exit set of $N$ . Obviously, $L$ is positively invariant in $N$ .

This completes the verification. From Lemma 5.5.9, both $U$ and $N$ are isolating neighborhoods of $[ S ]$ .

For a system without variational structure, does there exist an index pair relative to any set $U \in \Sigma$ ? We have:

Theorem 5.5.13 (Existence of an index pair) Let $\varphi$ be a flow on a metric space $X$ . $\forall U \in \Sigma$ , $( G ^ { T } ( U ) , \Gamma ^ { T } ( U ) )$ is an index pair relative to $U$ , where $T > 0$ is assumed such that $G ^ { T } ( U ) \subset \operatorname { i n t } ( U )$ .

Proof. From the properties (1) and (4), both $G ^ { T } ( U )$ and $\Gamma ^ { T } ( U )$ are closed. We shall verify the four conditions in Definition 5.5.11 successively.

(1) By property (4), $\operatorname { i n t } ( G ^ { T } ( U ) \backslash \Gamma ^ { T } ( U ) ) = \operatorname { i n t } ( G ^ { T } ( U ) )$ . Applying property (2),

$$
G ^ {T} \left(G ^ {T} (U) \backslash \Gamma^ {T} (U)\right) \subset G ^ {2 T} (U) \subset \operatorname {i n t} \left(G ^ {T} (U)\right).
$$

Thus $\overline { { G ^ { T } ( U ) \backslash \Gamma ^ { T } ( U ) } } \in \Sigma$

(2) $\Gamma ^ { T } ( U )$ is positively invariant in $G ^ { T } ( U )$ , i.e., if $x \in \ \Gamma ^ { T } ( U )$ and $\varphi ( [ 0 , T _ { 1 } ] , x ) \in G ^ { T } ( U )$ , then $\varphi ( [ 0 , T _ { 1 } ] , x ) \subset \Gamma ^ { T } ( U )$ .

Suppose not, then $\exists t \in [ 0 , T _ { 1 } ]$ such that $\varphi ( t , x ) \not \in \Gamma ^ { T } ( U )$ . Let $t ^ { * } = \operatorname* { i n f } \{ s \in$ $[ 0 , T _ { 1 } ] | \ \varphi ( s , x ) \notin \Gamma ^ { T } ( U ) \}$ . Since $\Gamma ^ { T } ( U )$ is closed, $y = \varphi ( t ^ { \ast } , x ) \in \Gamma ^ { T } ( U )$ and $\exists \epsilon _ { n }  + 0$ such that $\varphi ( t ^ { * } + \epsilon _ { n } , x ) \not \in \Gamma ^ { T } ( U )$ . Thus $\varphi ( [ 0 , T ] , y ) \cap \partial U \neq \emptyset$ and $\varphi ( [ \epsilon _ { n } , T ] , y ) \cap \partial U = \emptyset$ ; it follows that $y \in \partial U$ . But $y \in G ^ { T } ( U ) \subset \operatorname { i n t } ( U )$ . This is a contradiction.

(3) $\Gamma ^ { T } ( U )$ is an exit set of $G ^ { T } ( U )$ , i.e., if $x \in G ^ { T } ( U )$ and if $\exists t _ { 1 } > 0$ such that $\varphi ( t _ { 1 } , x ) \not \in G ^ { T } ( U )$ , then $\exists t _ { 0 } \in [ 0 , t _ { 1 } )$ such that $\varphi ( [ 0 , t _ { 0 } ] , x ) \subset G ^ { T } ( U )$ and $y = \varphi ( t _ { 0 } , x ) \in \Gamma ^ { T } ( U )$ .

Let us define

$$
\begin{array}{l} t _ {0} = \inf  \{s > 0 | \varphi ([ s - T, s + T ], x) \not \subset U \} \\ = \inf  \left\{s > 0 \mid \varphi (s, x) \notin G ^ {T} (U) \right\}; \\ \end{array}
$$

we have $t _ { 0 } \in [ 0 , t _ { 1 } ]$ . Since $G ^ { T } ( U )$ is closed, $\varphi ( [ 0 , t _ { 0 } ] , x ) \subset G ^ { T } ( U )$ , therefore $t _ { 0 } < t _ { 1 }$ . Defining $y = \varphi ( t _ { 0 } , x )$ , we have

$$
\varphi (T, y) = \varphi (t _ {0} + T, x) \in \partial U,
$$

therefore $y \in \Gamma ^ { T } ( U )$ .

(4) From property (4), ${ \overline { { G ^ { T } ( U ) \backslash \Gamma ^ { T } ( U ) } } } = G ^ { T } ( U ) \subset U$ . For $T _ { 1 } ~ > ~ T$ , we obtain

$$
G ^ {T _ {1}} (U) \subset G ^ {T} (U) = \overline {{G ^ {T} (U) \backslash \Gamma^ {T} (u)}}.
$$

A topological invariant is introduced to describe the index pair $( N , L )$ relative to an isolating neighborhood $U$ . Conley called the homotopy type $h ( U ) = [ N / L ]$ the invariant. In comparing with that in the Morse theory, we prefer to replace it by the relative homology groups. However, in order to match Conley’s definition, Alexander–Spanier cohomology is more suitable, because it possesses a special excision property not shared by singular cohomology theory.

For a topological pair $( X , A )$ and a coeficient field $F$ , $\overline { { H } } ^ { * } ( X , A ; F )$ stands for Alexander–Spanier cohomology. The following excision property holds.

Suppose that $X$ and $Y$ are paracompact Hausdorff spaces, and that $A$ and $B$ are closed in $X$ and $Y$ respectively. If $X \backslash A$ is homeomorphic to $Y \backslash B$ . Then $\overline { { { H } } } ^ { * } ( X , A ; F ) \cong \overline { { { H } } } ^ { * } ( Y , B ; F )$ .

Definition 5.5.14 (Conley index) For a given flow $\varphi$ and a given $U \in \Sigma$ , let $( N , L )$ be an index pair with respect to $( U , \varphi )$ . We call

$$
h (U) = h (U, \varphi) = \overline {{H}} ^ {*} (N, L; F)
$$

the Conley index for $U$ (or for the isolated invariant set $S \ = \ I ( U )$ ) with respect to the flow $\varphi$ .

In contrast with equation (5.3), we also write

$$
h (t, U) = \sum_ {q = 0} ^ {\infty} t ^ {q} \bar {H} ^ {q} (N, L; F).
$$

In the following we omit $\varphi$ in the notation $h$ , if there is no ambiguity.

In order to show that the Conley index is well-defined, one has to verify that if $( N _ { 1 } , L _ { 1 } )$ and $( N _ { 2 } , L _ { 2 } )$ are two index pairs relative to $U$ , then $\overline { { H } } ^ { \ast } ( N _ { 1 } , L _ { 1 } ; F ) \cong \overline { { H } } ^ { \ast } ( N _ { 2 } , L _ { 2 } ; F )$ .

In fact by definition, $\exists T > 0$ such that

$$
G ^ {T} \left(N _ {1} \backslash L _ {1}\right) \subset G ^ {T} (U) \subset \overline {{N _ {2} \backslash L _ {2}}} \text {a n d} G ^ {T} \left(N _ {2} \backslash L _ {2}\right) \subset G ^ {T} (U) \subset \overline {{N _ {1} \backslash L _ {1}}}.
$$

Because $N _ { i } \backslash L _ { i } \in \Sigma , ~ i = 1 , 2$ , for sufficient large $T > 0$ , we have

$$
G ^ {T} \left(N _ {1} \backslash L _ {1}\right) \subset \operatorname {i n t} \left(\overline {{N _ {2} \backslash L _ {2}}}\right) \text {a n d} G ^ {T} \left(N _ {2} \backslash L _ {2}\right) \subset \operatorname {i n t} \left(\overline {{N _ {1} \backslash L _ {1}}}\right). \tag {5.46}
$$

Lemma 5.5.15 If $( N _ { 1 } , L _ { 1 } )$ and $( N _ { 2 } , L _ { 2 } )$ are two index pairs relative to $U \in$ $\Sigma$ , and if $T > 0$ such that (5.46) holds, then the map $f : [ T , \infty ) \times N _ { 1 } / L _ { 1 } $ $N _ { 2 } / L _ { 2 }$ defined by

$$
f (t, [ x ]) = \left\{ \begin{array}{l l} \varphi (3 t, x) & i f   \varphi ([ 0, 2 t ], x) \subset N _ {1} \backslash L _ {1} a n d   \varphi ([ t, 3 t ], x) \subset N _ {2} \backslash L _ {2}  , \\ [ L _ {2} ] & o t h e r w i s e \end{array} \right.
$$

is continuous.

Proof. There are three cases in logic: (1) $\varphi ( [ t , 3 t ] , x ) \subset \overline { { \mathbb { N } _ { 2 } \backslash L _ { 2 } } }$ , (2) $\varphi ( [ 0 , 2 t ]$ , $x ) \not \subset \overline { { N _ { 1 } \backslash L _ { 1 } } }$ , (3) $\varphi ( [ t , 3 t ] , x ) \subset \overline { { { N _ { 2 } } \backslash { L _ { 2 } } } }$ and $\varphi ( [ 0 , 2 t ] , x ) \subset N _ { 1 } \backslash L _ { 1 }$ .

In case (1), by the continuity of the flow, there exists a neighborhood $W$ of $( t , x )$ such that $\varphi ( [ s , 3 s ] , y ) \not \subset N _ { 2 } \backslash L _ { 2 } \ \forall ( s , y ) \in W$ then $f ( s , [ y ] ) = [ L _ { 2 } ] =$ $f ( t , [ x ] )$ , i.e., $f$ is continuous at $( t , [ x ] )$ .

Similarly for case (2).

In case (3), we consider two possibilities:

(a) $\varphi ( [ t , 3 t ] , x ) \cap L _ { 2 } \ = \ \varnothing$ . Since $\varphi ( [ t , 3 t ] , x ) \ \subset \ N _ { 2 } \backslash L _ { 2 }$ , it follows that $\varphi ( 2 t , x ) \in G ^ { t } ( N _ { 2 } \backslash L _ { 2 } ) \subset \mathrm { i n t } ( N _ { 1 } \backslash L _ { 1 } ) \subset N _ { 1 } \backslash L _ { 1 }$ . Again from $\varphi ( [ 0 , 2 t ] , x ) \subset$ $N _ { 1 } \backslash L _ { 1 }$ and the positive invariance of $L _ { 1 }$ in $N _ { 1 }$ , it follows that $\varphi ( [ 0 , 2 t ] , x ) \subset$ $N _ { 1 } \backslash L _ { 1 }$ . Therefore $f ( t , [ x ] ) = \varphi ( 3 t , x ) \in N _ { 2 } \backslash L _ { 2 }$ . For any open neighborhood $U$ of $\varphi ( 3 t , x )$ in $N _ { 2 } \backslash L _ { 2 }$ , by the continuity of the flow $\varphi$ , $\Finv$ a neighborhood $W$ in $N _ { 1 } / L _ { 1 }$ of $( t , x )$ such that $\varphi ( [ 0 , 2 s ] , y ) \subset N _ { 1 } \backslash L _ { 1 } , \varphi ( [ s , 3 s ] , y ) \subset N _ { 2 } \backslash L _ { 2 }$ and $\varphi ( 3 s , y ) \in U \forall ( s , y ) \in W$ . Thus $f ( s , [ y ] ) = \varphi ( 3 s , y ) \in U$ as $( s , y ) \in W$ with $y \in N _ { 1 }$ , i.e., $f$ is continuous at this point $( t , x )$ .

(b) $\varphi ( [ t , 3 t ] , x ) \cap L _ { 2 } \neq \varnothing$ . Since $L _ { 2 }$ is an exit set of $N _ { 2 }$ , $\varphi ( 3 t , x ) \in L _ { 2 }$ . Let $[ U ]$ be any neighborhood of $\left[ L _ { 2 } \right]$ in $N _ { 2 } \backslash L _ { 2 }$ , define $V = \{ x \in N _ { 2 } \backslash L _ { 2 } | x \in$ $[ U ] \} \cup ( X \backslash N _ { 2 } ) \cup L _ { 2 }$ , then $V$ is a neighborhood of $L _ { 2 }$ in $X$ , and $[ U ] = ( V \cap$ $( N _ { 2 } \backslash L _ { 2 } ) ) \cup [ L _ { 2 } ]$ .

By the continuity of the flow $\varphi$ , $\exists$ a neighborhood $W$ of $( t , x )$ such that $\varphi ( 3 s , y ) \in V \forall ( s , y ) \in W$ . Thus

$$
f (s, [ y ]) \in \left\{\left[ \varphi (3 s, y) \right], [ L _ {2} ] \right\} \subset \left(V \cap \left(N _ {2} \backslash L _ {2}\right)\right) \cup \left[ L _ {2} \right] \subset [ U ],
$$

$\forall ( s , y ) \in W$ with $y \in N _ { 1 }$ . Again $f$ is continuous at this point. The proof is complete. □

Theorem 5.5.16 If $( N _ { 1 } , L _ { 1 } )$ and $( N _ { 2 } , L _ { 2 } )$ are two index pairs relative to $U \in \Sigma$ , then $\overline { { H } } ^ { \ast } ( N _ { 1 } , L _ { 1 } ; F ) \cong \overline { { H } } ^ { \ast } ( N _ { 2 } , L _ { 2 } ; F )$ .

Proof. According to Lemma 5.5.15, there are continuous functions: $f : \lfloor T , \infty \rfloor \times$ $N _ { 1 } / L _ { 1 } \to N _ { 2 } / L _ { 2 }$ and $g : [ T , \infty ) \times N _ { 2 } / L _ { 2 } \to N _ { 1 } / L _ { 1 }$ defined by

$$
f (t, [ x ]) = \left\{ \begin{array}{l l} \varphi (3 t, x) & \text {i f} \varphi ([ 0, 2 t ], x) \subset N _ {1} \backslash L _ {1} \text {a n d} \varphi ([ t, 3 t ], x) \subset N _ {2} \backslash L _ {2}  , \\ [ L _ {2} ] & \text {o t h e r w i s e}  , \end{array} \right.
$$

and

$$
g (t, [ x ]) = \left\{ \begin{array}{l l} \varphi (3 t, x) & \text {i f} \varphi ([ 0, 2 t ], x) \subset N _ {2} \backslash L _ {2} \text {a n d} \varphi ([ t, 3 t ], x) \subset N _ {1} \backslash L _ {1}  , \\ [ L _ {1} ] & \text {o t h e r w i s e}  , \end{array} \right.
$$

respectively.

One defines $\eta _ { i } : [ 0 , T ] \times N _ { i } / L _ { i } \to N _ { i } / L _ { i }$

$$
\eta_ {i} (t, [ x ]) = \left\{ \begin{array}{l l} \varphi (6 t, x) & \text {i f} \varphi ([ 0, 6 t ], x) \subset N _ {i} \backslash L _ {i}  , \\ [ L _ {i} ] & \text {o t h e r w i s e}  , \end{array} \right.
$$

$i = 1 , 2$ . Again they are continuous and satisfy $\eta _ { 1 } ( T , [ x ] ) = g ( T , f ( T , [ x ] ) )$ , $\eta _ { 2 } ( T , [ x ] ) = f ( T , g ( T , [ x ] ) )$ and $\eta _ { i } ( 0 , [ x ] ) = \mathrm { i d } _ { N _ { i } / L _ { i } } \ i = 1 , 2$ . This shows that $N _ { 1 } / L _ { 1 }$ and $N _ { 2 } / L _ { 2 }$ have the same homotopy type. It follows that

$$
\overline {{H}} ^ {*} (N _ {1} / L _ {1}, [ L _ {1} ]; F) \cong \overline {{H}} ^ {*} (N _ {2} / L _ {2}, [ L _ {2} ]; F).
$$

By the special excision property, we have

$$
\overline {{H}} ^ {*} (N _ {i}, L _ {i}; F) \cong \overline {{H}} ^ {*} (N _ {i} / L _ {i}, [ L _ {i} ]; F), i = 1, 2.
$$

Combining them together, we obtain

$$
\overline {{H}} ^ {*} (N _ {1}, L _ {1}; F) \cong \overline {{H}} ^ {*} (N _ {2}, L _ {2}; F).
$$

□

Example 1. (Hyperbolic fixed point) Recall the hyperbolic system and the Hartman–Grobman theorem in Chap. 1. Let $f : R ^ { n } \to R ^ { n }$ be $C ^ { 1 }$ . Assume that $\theta$ is a hyperbolic fixed point with $k$ -dimensional unstable manifold, i.e., $A = e ^ { L }$ with $L = f ^ { \prime } ( \theta )$ satisfying $\sigma ( A ) \cap S ^ { 1 } = \varnothing$ . Then we have the decomposition ${ \cal R } ^ { n } = E ^ { u } \oplus E ^ { s }$ where $E ^ { u }$ and $E ^ { s }$ are invariant subspaces of $A$ , on which the eigenvalues of $A ^ { u } = A | _ { E ^ { u } }$ lie outside the unit circle, and those of $A ^ { s } = A | _ { E ^ { s } }$ lie inside the unit circle with $\mathrm { d i m } E ^ { u } = k$ . Thus $L = L ^ { u } \oplus L ^ { s }$ , where $L ^ { u } = L | _ { E ^ { u } }$ and $L ^ { s } = L | _ { E ^ { s } } $ . The real parts of all eigenvalue of $L ^ { u }$ are greater than zero, while those of $L ^ { s }$ are less than zero. According to the Hartman–Grobman theorem the flow in a neighborhood of $\theta$ for the ODE ${ \dot { x } } = f ( x )$ is topologically equivalent to the flow in a neighborhood of $\dot { x } = L x$ , i.e., $\varphi ( t , x ) = e ^ { L t } x$ . An isolating neighborhood of the origin is given by $B ^ { k } \times B ^ { n - k }$ and the exit set by $S ^ { k - 1 } \times B ^ { n - k }$ . Let $U = N = B ^ { k } \times B ^ { n - k }$ and $L = S ^ { k - 1 } \times B ^ { n - k }$ . It follows that

$$
h (t, U) = t ^ {k}.
$$

Example 2. An invariant manifold $S$ is an invariant set with manifold structure. It is called normally hyperbolic, if the tangent bundle over $R ^ { n }$ restricted on $S$ has the bundle decomposition: $( \mathrm { T R } ^ { n } ) _ { S } = E ^ { u } \oplus E ^ { s }$ with

$$
E _ {\varphi (t, x)} ^ {u} = T \varphi (t, \cdot) E _ {x} ^ {u}, E _ {\varphi (t, x)} ^ {s} = T \varphi (t, \cdot) E _ {x} ^ {s} \forall x \in S, \forall t \in R ^ {1}.
$$

According to Thom’s isomorphism theorem, see [BT],[Mi 3],[Sp], we have the following result: Let $S$ be a normally hyperbolic invariant manifold. Let $E ^ { u }$ be the vector bundle over $S$ defined by the local unstable manifold of $S$ . If $E ^ { \pi }$ is a rank $k$ orientable bundle, then

$$
h (U) = \overline {{H}} ^ {* + k} (S; F),
$$

for $U \in \Sigma$ with $I ( U ) = S$ .

Thus, if $S$ is a hyperbolic periodic orbit with an oriented unstable manifold of dimension $k + 1$ , then

$$
h (t, U) = t ^ {k} + t ^ {k + 1}.
$$

Example 3. (Critical groups) Let $( M , f , \phi )$ be a pseudo-gradient flow, and let $\{ \mathcal { O } , \alpha , \beta \}$ be an isolating triplet for a dynamically isolated critical set $S$ of $f$ . Then

$$
h (\mathcal {O}) = \overline {{H}} ^ {*} \left(\tilde {\mathcal {O}} \cap f _ {\beta}, \tilde {\mathcal {O}} \cap f _ {\alpha}; F\right).
$$

In particular, if $S$ is an isolated critical point $p$ , then

$$
h (\mathcal {O}) = C _ {*} (f, p),
$$

where $C _ { * } ( f , p )$ is the critical groups of $f$ at $S$ .

The Conley index possesses many important properties: Wazewski’s principle, the continuation and the Morse–Smale decomposition, which are the

counterparts of the Kronecker existence, the homotopy invariance and the (sub) additivity of the degree theory.

First, we introduce a type of compactness condition on $U \in \Sigma$ .

Definition 5.5.17 (Condition(B)) $U \in \Sigma$ is said to satisfy Condition (B) if $\forall$ closed neighborhoods $W$ of $I ( U )$ there exists $T > 0$ such that $G ^ { \prime } ( U ) \subset W$ .

In particular, if $U \in \Sigma$ satisfies Condition (B) and $I ( U ) = \emptyset$ , then $\exists T > 0$ such that $G ^ { T } ( U ) = \emptyset$ .

It is known that if $U \in \Sigma$ is compact, then $U$ satisfies Condition (B) (see Remark 5.5.4).

We investigate other sufficient conditions of Condition (B): Rubakowski [Ry] introduced:

Condition (A) $U \in \Sigma$ is said to satisfy Condition (A) if $\forall x _ { n } \in U , \ \forall t _ { n } \to$ $+ \infty$ , $\varphi ( [ 0 , t _ { n } ] , x _ { n } ) \subset U$ implies that $\varphi ( t _ { n } , x _ { n } )$ is subconvergent.

Condition (A) ⇒ Condition (B).

Indeed, if not, $\exists W$ , a closed neighborhood of $I ( U )$ such that $G ^ { n } ( U ) ~ \mathcal { L }$ $W \forall n$ , i.e., $\exists y _ { n } \in G ^ { n } ( U ) \backslash W$ . Let $x _ { n } = \varphi ( - n , y _ { n } )$ , then $\varphi ( [ 0 , n ] , x _ { n } ) \subset U$ . By Condition (A), $y _ { n } = \varphi ( n , x _ { n } )$ subconvergent to $y _ { 0 } \in U$ , and then $y _ { 0 } \in I ( U )$ . But $W$ is closed, therefore $y _ { 0 } \not \in \operatorname { i n t } ( W )$ . This is a contradiction.

The isolating neighborhoods of the examples of dynamically isolated critical sets after Theorem 5.5.10 satisfy Condition (B). More precisely, let $( M , f , \varphi )$ be a pseudo-gradient flow, and let $c$ be an isolated critical value. If $K _ { c } ~ = ~ K \cap f ^ { - 1 } ( c )$ is isolated (or, $p ~ \in ~ K _ { c }$ is isolated), then the set $W = c l ( \bar { \mathcal { O } } ) \cap f ^ { - 1 } [ c - \epsilon , c + \epsilon ]$ is an isolating neighborhood of $K _ { c }$ (or $p$ resp.) satisfying Condition (B), if $\epsilon > 0$ is small so that $f ^ { - 1 } [ c - \epsilon , c + \epsilon ] \cap K = K _ { c }$ , and $\boldsymbol { \mathcal { O } }$ is a neighborhood of $K _ { c }$ (or $p$ resp.) containing only critical points in $K _ { c }$ (or $p$ resp.).

From Lemma 5.5.9, it is sufficient to show that any neighborhood $V$ of $p$ , containing $p$ as the only critical point, contains a (MVP) neighborhood $U$ of $p$ .

In fact, let $B _ { R } ( p ) \subset V$ for some $R > 0$ , we choose $\begin{array} { r } { r = \frac { R } { 2 } , \mathcal { O } = B _ { r } ( p ) } \end{array}$ and $U = c l ( \tilde { \mathcal { O } } ) \cap f ^ { - 1 } ( c - \delta , c + \delta )$ for small $\delta > 0$ . By standard estimates, $U \subset B _ { R } ( p ) \subset V$ .

The following Wazewski principle holds:

Theorem 5.5.18 Let $U ~ \in ~ \Sigma$ satisfy Condition (B). If $h ( U ) ~ \neq ~ 0$ then $I ( U ) \neq \emptyset$ .

Proof. We prove that $I ( U ) = \emptyset$ implies $\overline { { H } } ^ { * } ( N , L ; F ) = 0$ for all index pairs $( N , L )$ relative to $U$ . According to Theorem 5.5.13, it is sufficient to verify that for a special index pair $( N , L ) = ( G ^ { T } ( U ) , \Gamma ^ { T } ( U ) )$ . Condition (B) yields the existence of large $T > 0$ such that $G ^ { T } ( U ) = \emptyset$ . The conclusion follows trivially. □

Next, we turn to the continuation. A special form of the homotopy invariance of the topological degree asserts that the degree $\deg ( f , \Omega , p )$ is continuous with respect to the vector field $f$ . In Conley index theory, the vector field $f$ is replaced by the flow $\varphi$ , so we should study: Under what condition on $U \in \Sigma$ , and in which sense of the variance between flows $\varphi$ and $\psi$ , does one have

$$
h (U, \varphi) = h (U, \psi)?
$$

First we notice:

Lemma 5.5.19 Assume that $U \in \Sigma ( \varphi )$ satisfies Condition (B) and that $I ( U )$ is compact. Then U satisfies the condition:

$$
\Sigma_ {0} (\varphi): \exists T, \delta > 0 \text {s u c h t h a t} N _ {\delta} \left(G ^ {T} (U)\right) \subset \operatorname {i n t} (U),
$$

where $N _ { \delta } ( A )$ is the $\delta$ -neighborhood of the set A. Moreover, $\exists T , \delta > 0$ such that

$$
N _ {\delta} \left(G ^ {T} (U)\right) \subset G ^ {T / 2} (U), N _ {\delta} \left(G ^ {T / 2} (U)\right) \subset \operatorname {i n t} (U). \tag {5.47}
$$

Proof. 1. Let $\boldsymbol { \epsilon } = d ( \partial \boldsymbol { U } , I ( \boldsymbol { U } ) )$ where $d ( \cdot , \cdot )$ is the distance between two sets induced by the metric $d$ . Since $I ( U )$ is compact, $\epsilon > 0$ . Let $V = N _ { \epsilon / 2 } ( I ( U ) )$ ; by Condition (B), one has $T > 0$ such that $G ^ { T } ( U ) \subset V$ . Let $\delta = \epsilon / 2$ ; the first conclusion follows.

2. To prove the second conclusion, we claim that $\partial G ^ { T } ( U ) \cap I ( U ) = \emptyset$ . Indeed, if not, $\exists x _ { 0 } \in \partial G ^ { \prime } ( U ) \cap I ( U )$ . From $x _ { 0 } \in \partial G ^ { T } ( U )$ , it implies that $\exists t _ { 0 } \in [ - T , T ]$ such that $\varphi ( t _ { 0 } , x _ { 0 } ) \in \partial U$ . But from $x _ { 0 } \in I ( U )$ , we have $\varphi ( [ t _ { 0 } -$ $T , t _ { 0 } + T ] , x _ { 0 } ) \subset U$ , then $\varphi ( t _ { 0 } , x _ { 0 } ) \in G ^ { T } ( U ) \subset \operatorname { i n t } ( \operatorname { U } )$ . This is a contradiction.

Then $\epsilon _ { 1 } : = d ( \partial G ^ { T } ( U ) , I ( U ) ) > 0$ ; we repeat the proof in step 1, and obtain $T _ { 1 } > 0$ such that $G ^ { T _ { 1 } } ( U ) \subset V : = N _ { \frac { \epsilon _ { 1 } } { 2 } } ( I ( U ) )$ . Thus, $N _ { \frac { \epsilon _ { 1 } } { 2 } } ( G ^ { T _ { 1 } } ) \subset G ^ { T } ( U )$ .

One defines the Hausdorff metric between two sets $U$ and $V$ as follows:

$$
d _ {H} (U, V) = \sup  _ {x \in U} d (x, V) + \sup  _ {x \in V} d (U, x).
$$

Lemma 5.5.20 For $U \in \Sigma _ { 0 } ( \varphi )$ , there exist $T , \epsilon > 0$ such that if a flow $\psi$ satisfies

$$
d (\varphi (t, x), \psi (t, x)) \leq \epsilon \forall (t, x) \in [ - T, T ] \times N _ {\delta} (U) \tag {5.48}
$$

and a closed set V satisfies

$$
d _ {H} (U, V) \leq \epsilon , \tag {5.49}
$$

then $U , V \in \Sigma _ { 0 } ( \psi )$ and $V \in \Sigma _ { 0 } ( \varphi )$ .

Proof. By definition, one has $T , \delta > 0$ such that

$$
N _ {\delta} (G _ {\varphi} ^ {T} (U)) \subset \operatorname {i n t} (U).
$$

Letting $\epsilon \leq \delta / 4$ , we have

$$
N _ {\epsilon} (G _ {\psi} ^ {T} (U)) \subset N _ {2 \epsilon} (G _ {\varphi} ^ {T} (U)) \subset \operatorname {i n t} (U),
$$

i.e., $U \in \Sigma _ { 0 } ( \psi )$ , and

$$
N _ {2 \epsilon} (G _ {\psi} ^ {T} (V)) \subset N _ {4 \epsilon} (G _ {\varphi} ^ {T} (U)) \subset \mathrm {i n t} (U).
$$

Thus $d _ { H } ( N _ { \epsilon } ( G _ { \psi } ^ { T } ( V ) ) , \partial V ) \geq \epsilon$ ; it follows that $N _ { \epsilon } ( G _ { \psi } ^ { T } ( V ) ) \subset \mathrm { i n t } ( V )$ i.e., $V \in$ $\Sigma _ { 0 } ( \psi )$ . Similarly, we prove that $V \in \Sigma _ { 0 } ( \varphi )$ . □

Theorem 5.5.21 (Continuation) For $U \in \Sigma _ { 0 } ( \varphi )$ , T,  > 0 such that

$$
h (U, \varphi) = h (V, \psi),
$$

whenever $( \psi , V )$ satisfies (5.48) and (5.49), and that both $\varphi$ and $\psi$ are uniformly continuous on $[ - T , T ] \times N _ { 2 \delta } ( U )$ , namely,

$$
d (\phi (t, x), \phi (t, y)) <   \frac {\delta}{3}, d (\psi (t, x), \psi (t, y)) <   \frac {\delta}{3}, a s d (x, y) <   \epsilon .
$$

Proof. The proof is fairly long, we divide it into several steps.

First we assume $U = V$ , and use the following simplified notations:

$$
\begin{array}{l} \widetilde {\varphi} = \psi , G ^ {T} = G _ {\varphi} ^ {T} (U), \Gamma^ {T} = \Gamma_ {\varphi} ^ {T} (U), \\ \widetilde {G} ^ {T} = G _ {\widetilde {\varphi}} ^ {T} (V), \widetilde {\Gamma} ^ {T} = \Gamma_ {\widetilde {\varphi}} ^ {T} (V). \\ \end{array}
$$

According to (5.47), we may assume

$$
N _ {\delta} \left(G ^ {T}\right) \subset G ^ {T / 2}, N _ {\delta} \left(G ^ {T / 2}\right) \subset \operatorname {i n t} (U).
$$

Setting $\epsilon \in ( 0 , \delta / 3 )$ in (5.48), we have

$$
N _ {\frac {2 \delta}{3}} (\widetilde {G} ^ {T}) \subset G ^ {T / 2}, N _ {\frac {2 \delta}{3}} (\widetilde {G} ^ {T / 2}) \subset \mathrm {i n t} (U), N _ {\frac {2 \delta}{3}} (G ^ {T}) \subset \widetilde {G} ^ {T / 2},
$$

and

$$
N _ {\frac {\delta}{3}} \left(\widetilde {G} ^ {T}\right) \subset \widetilde {G} ^ {T / 2}.
$$

1. $\forall \lambda \in [ - 1 , + 1 ]$ , let $\xi _ { \lambda } ( T , x ) = \widetilde { \varphi } ( \lambda T , \varphi ( - \lambda T , x ) )$ , then $\xi _ { \lambda } ( T , \cdot ) : G ^ { T } \to$ $\operatorname { i n t } ( G ^ { T / 2 } ) \cap \operatorname { i n t } ( \widetilde { G } ^ { T / 2 } )$ and $\Gamma ^ { T } \to N _ { \epsilon } ( \Gamma ^ { T } )$ is continuous.

In fact, $\forall x \in G ^ { I ^ { \prime } } , y = \varphi ( - \lambda T , x ) \in U$ , we have

$$
d \left(\xi_ {\lambda} (T, x), x\right) = d \left(\widetilde {\varphi} (\lambda T, y), \varphi (\lambda T, y)\right) <   \epsilon ,
$$

and $N _ { \epsilon } ( G ^ { T } ) \subset G ^ { T / 2 }$ , $N _ { \epsilon } ( G ^ { T } ) \subset \widetilde { G } ^ { T / 2 } .$ .

2. The map ${ h _ { 1 } } : \mathrm { i n t } ( G ^ { T / 2 } )  G ^ { T } / \Gamma ^ { T }$ defined by

$$
h _ {1} (x) = \left\{ \begin{array}{l l} [ \varphi (T, x) ] & \varphi (T, x) \in G ^ {T} \\ [ \Gamma^ {T} ] & \text {o t h e r w i s e} \end{array} \right.
$$

is continuous.

In fact, it is sufficient to verify that $\varphi ( T , x ) \in \partial G ^ { \prime } { } ^ { T } \Rightarrow \varphi ( T , x ) \in \Gamma ^ { \prime }$ . Since

$$
\varphi (T, x) \in G ^ {T} \Leftrightarrow \varphi \left(\left[ \frac {T}{2}, \frac {3}{2} T \right], x\right) \subset G ^ {T / 2} \subset \operatorname {i n t} (U),
$$

in combination with $x \in \operatorname { i n t } ( G ^ { T / 2 } )$ , i.e., $\varphi ( [ - { \frac { T } { 2 } } , { \frac { T } { 2 } } ] , x ) \subset \operatorname { i n t } ( U )$ , we have $\varphi ( [ - { \frac { \pi } { 2 } } , { \frac { 3 T } { 2 } } ] , x ) \subset \operatorname { i n t } ( \mathrm { U } )$ , i.e., $\varphi ( [ - \frac { T } { 2 } , \frac { 3 } { 2 } T ] , x ) \cap \partial U = \emptyset$ . Since $\varphi ( T , x ) \in \partial G ^ { T }$ implies that $\varphi ( [ 0 , 2 T ] , x ) \cap \partial U \neq \emptyset$ , so $\exists \bar { t } \in [ \frac { 3 } { 2 } T , 2 T ]$ such that $\varphi ( t , x ) \in \partial U$ , and then $\varphi ( T , x ) \in \Gamma ^ { T }$ . Also the map $\tilde { h _ { 1 } } : \mathrm { i n t } ( \tilde { G } ^ { T / 2 } )  \tilde { G } ^ { T } / \tilde { \Gamma } ^ { T }$ defined by

$$
\widetilde {h _ {1}} (x) = \left\{ \begin{array}{l l} [ \widetilde {\varphi} (T, x) ] & \widetilde {\varphi} (T, x) \in \widetilde {G} ^ {T} \\ [ \widetilde {\Gamma} ^ {T} ] & \text {o t h e r w i s e} \end{array} \right.
$$

is continuous.

Note: On the right-hand side of the definitions of $h _ { 1 }$ and $\widetilde { h } _ { 1 }$ , it is equivalent to write $\varphi ( [ \frac { T } { 2 } , T ] , x ) \subset G ^ { T }$ and $\widetilde { \varphi } ( [ \frac { T } { 2 } , T ] , x ) \subset \widetilde { \cal G } ^ { T }$ instead, respectively.

3. The composition map $g _ { 1 } = \widetilde { h } _ { 1 } \circ \xi _ { 1 } ( T , \cdot ) : G ^ { T } / \Gamma ^ { T } \to \widetilde { G } ^ { T } / \widetilde { \Gamma } ^ { T }$ is well defined and continuous:

$$
g _ {1} (x) = \left\{ \begin{array}{l l} [ \widetilde {\varphi} (2 T, \varphi (- T, x)) ] & \widetilde {\varphi} (2 T, \varphi (- T, x)) \in \widetilde {G} ^ {T} \\ [ \widetilde {\Gamma} ^ {T} ] & \text {o t h e r w i s e}. \end{array} \right.
$$

It is sufficient to verify that $x \in \Gamma ^ { T } \Rightarrow g _ { 1 } ( x ) = [ \widetilde { \Gamma } ^ { T } ]$ .

In fact, $x \in G ^ { T }$ implies that $\varphi ( { \frac { T } { 2 } } , x ) \in G ^ { \frac { T } { 2 } } \subset \operatorname { i n t } ( U )$ . Therefore $x \in \Gamma ^ { T } \Rightarrow$ $\exists \bar { t } \in ( \frac { T } { 2 } , T ]$ such that $\varphi ( t , x ) \in \partial U$ . From step 1, the assumptions on the uniform continuity of $\widetilde { \varphi }$ and (5.48), we have

$$
\begin{array}{l} d (\widetilde {\varphi} (\bar {t}, \xi_ {1} (T, x)), \partial U) \leq d (\widetilde {\varphi} (\bar {t}, \xi_ {1} (T, x)), \varphi (\bar {t}, x)) \\ \leq d (\widetilde {\varphi} (\bar {t}, \xi_ {1} (T, x)), \widetilde {\varphi} (\bar {t}, x)) + d (\widetilde {\varphi} (\bar {t}, x), \varphi (\bar {t}, x)) \\ \leq \delta / 3 + \epsilon . \\ \end{array}
$$

It follows that $\widetilde { \varphi } ( \bar { t } , \xi _ { 1 } ( T , x ) ) \notin \widetilde { { \cal G } } ^ { T }$ , and then $\widetilde { \varphi } ( [ \frac { T } { 2 } , T ] , \xi _ { 1 } ( T , x ) ) \ \mathscr { C } \ \widetilde { G } ^ { T }$ . But $\widetilde { \varphi } ( 2 T , \varphi ( - T , x ) ) \in \widetilde { \cal G } ^ { T }$ is equivalent to $\widetilde { \varphi } ( [ 0 , T ] , \xi _ { 1 } ( T , x ) ) \subset \widetilde { \cal G } ^ { T } .$ . The conclusion !follows.

4. Similarly, we define $\widetilde { h } _ { 2 } : \mathrm { i n t } ( \widetilde { G } ^ { T / 2 } )  G ^ { T } / \Gamma ^ { T }$ by

$$
\widetilde {h} _ {2} (x) = \left\{ \begin{array}{l l} [ \varphi (T, x) ] & \varphi (T, x) \in \widetilde {G} ^ {T} \\ [ \Gamma^ {T} ] & \text {o t h e r w i s e ,} \end{array} \right.
$$

and $\bar { \xi _ { \lambda } } ( T , x ) = \varphi ( \lambda T , \widetilde { \varphi } ( - \lambda T , x ) ) : \tilde { G } ^ { T } \to \mathrm { i n t } ( \tilde { G } ^ { T / 2 } )$ . Also

$$
g _ {2} = \widetilde {h} _ {2} \circ \widetilde {\xi_ {1}} (T, \cdot): \widetilde {G} ^ {T} / \widetilde {\Gamma} ^ {T} \rightarrow G ^ {T} / \Gamma^ {T}
$$

is well defined and continuous.

5. In order to verify that $h ( U , \varphi ) ~ = ~ h ( U , \widetilde { \varphi } )$ , it is sufficiently to take $( N , L ) = ( G ^ { T } , \Gamma ^ { T } )$ , $( \widetilde { N } , \widetilde { L } ) = ( \widetilde { G } ^ { T } , \widetilde { \Gamma } ^ { T } )$ ! for sufficiently large $T > 0$ , and show that $g _ { 2 } \cup g _ { 1 }$ and $g _ { 1 } \cup g _ { 2 }$ are homotopic to identity maps on $G ^ { T } / \Gamma ^ { T }$ and $\tilde { G } ^ { T } / \tilde { \Gamma } ^ { T }$ , respectively.

Let us define for $\lambda \in [ 0 , 1 ]$ the map

$$
H (\lambda , [ x ]) = \left\{ \begin{array}{l l} {[ \varphi (2 T, \xi_ {- \lambda} (T, x)) ]} & {\varphi ([ 0, 2 T ], \xi_ {- \lambda} (T, x)) \subset G ^ {T}} \\ {[ \Gamma^ {T} ]} & \text {o t h e r w i s e}. \end{array} \right.
$$

Note: It is equivalent to replace $\varphi ( [ 0 , 2 T ] , \xi _ { - \lambda } ( T , x ) ) \subset G ^ { T }$ by $\varphi ( 2 T , \xi _ { - \lambda } ( T ,$ , $x ) ) \in G ^ { T }$ on the right-hand side of the definition.

Since $H ( 1 , [ x ] ) = g _ { 2 } \circ g _ { 1 }$ and

$$
H (0, [ x ]) = \left\{ \begin{array}{l l} {[ \varphi (2 T, x) ]} & {\varphi ([ 0, 2 T ], x) \subset G ^ {T}} \\ {[ \Gamma^ {T} ]} & \text {o t h e r w i s e ,} \end{array} \right.
$$

the latter is homotopic to $\mathrm { i d } | _ { G ^ { T } / \Gamma ^ { T } }$ as shown in Theorem 5.5.16.

It remains to verify that the definition is well-defined and that $H ( \lambda , [ x ] )$ is continuous.

We shall show that $x \in \Gamma ^ { T } \Rightarrow H ( \lambda , x ) = [ \Gamma ^ { T } ]$ i.e., $\varphi ( [ 0 , 2 T ] , \xi _ { - \lambda } ( T , x ) ) \ \varphi$ $G ^ { T }$ .

From $x \in \Gamma ^ { T }$ , by step 1, $\xi _ { - \lambda } ( T , x ) \in N _ { \epsilon } ( \Gamma ^ { T } )$ , i.e., $\exists \overline { { x } } \in \Gamma ^ { T }$ such that $d ( \xi _ { - \lambda } ( T , x ) , \overline { { { x } } } ) < \epsilon \qquad $ . Following the argument in 3, $\varphi ( [ \frac { 1 } { 2 } T , \frac { 3 } { 2 } T ] , \overline { { x } } ) \cap \partial U \neq \emptyset$ , we have $\varphi ( [ 0 , 2 T ] , \xi _ { - \lambda } ( T , x ) ) \not \subset G ^ { T }$ .

− ⊂ As to the continuity, we define $h _ { 2 } : G ^ { T } / \Gamma ^ { T } \to G ^ { T } / \Gamma ^ { T }$ by

$$
h _ {2} ([ x ]) = \left\{ \begin{array}{l l} [ \varphi (T, x) ] & \varphi (T, x) \in G ^ {T} \\ [ \Gamma^ {T} ] & \text {o t h e r w i s e}. \end{array} \right.
$$

Since $\varphi ( T , x ) \in G ^ { I ^ { \prime } }$ can be replaced by $\varphi ( [ 0 , T ] , x ) \subset G ^ { T }$ , the continuity of $h _ { 2 }$ has been proved in Theorem 5.5.16, and from $\varphi ( [ 0 , T ] , x ) \subset G ^ { T } \subset \operatorname { i n t } ( U )$ , it implies $x \notin \Gamma ^ { T }$ , $h _ { 2 }$ is well defined.

By definition

$$
H (\lambda , [ x ]) = h _ {2} \circ h _ {1} \circ \xi_ {- \lambda} (T, x),
$$

therefore $H$ is continuous on $[ 0 , T ] \times G ^ { T } / \Gamma ^ { T }$

6. Finally, we consider the case $U \neq V$ , but with $\widetilde { \varphi } = \varphi$ , and write $G ^ { T } ( U ) =$ $G _ { \varphi } ^ { T } ( U )$ , $G ^ { T } ( V ) = G _ { \varphi } ^ { T } ( V )$ . Since $N _ { \delta } ( G ^ { T } ( U ) ) \subset \operatorname { i n t } ( U )$ , $N _ { \delta } ( G ^ { T } ( V ) ) \subset \operatorname { i n t } ( V )$ , if one takes $0 < \epsilon < \delta / 3$ and $d _ { H } ( U , V ) < \epsilon$ , we have $N _ { \epsilon } ( G ^ { T } ( U ) ) \subset V$ and $N _ { \epsilon } ( G ^ { T } ( V ) ) \subset U$ .

Let $( N , L )$ and $( \widetilde { N } , \widetilde { L } )$ be index pairs related to $U$ and $V$ resp., then $\exists T _ { 1 } , T _ { 2 } ~ > ~ 0$ such that $G ^ { T _ { 1 } } ( N \backslash L ) \subset G ^ { T _ { 1 } } ( U ) \subset V$ , and $G ^ { T _ { 2 } } ( V ) \subset \overline { { \widetilde { N } \backslash \widetilde { L } } }$ . Thus $G ^ { T _ { 1 } + T _ { 2 } } ( N \backslash L ) \subset \widetilde { N } \backslash \widetilde { L }$ . Similarly one has $G ^ { T _ { 1 } + T _ { 2 } } ( \mathcal { \tilde { N } } \backslash \mathcal { \tilde { L } } ) \ \subset \ \overline { { N \backslash L } }$ . By the proof of Theorem 5.5.16, we obtain $\overline { { H } } ^ { * } ( N , L ; F ) \cong \overline { { H } } ^ { * } ( \tilde { N } , \tilde { L } ; F )$ , i.e., $h ( U , \varphi ) = h ( V , \varphi )$ . Combining steps 5 and 6, we have

$$
h (U, \varphi) = h (V, \varphi) = h (V, \psi).
$$

The proof is complete.

# 5.5.3 Morse Decomposition on Compact Invariant Sets and Its Extension

In the study of the dynamics on invariant sets the Morse–Smale decomposition is an extension of the Morse relation. Before going on, let us introduce the notion of attractor–repeller pairs.

On a metric space $( X , d )$ with flow $\varphi$ , $\forall Y \subset X$ , define

$$
\omega (Y) = \underset {t \geq 0} {\cap} \overline {{\varphi ([ t , \infty) , Y)}} \mathrm {a n d} \omega^ {*} (Y) = \underset {t > 0} {\cap} \overline {{\varphi ((- \infty , - t ] , Y)}}.
$$

By definition, $\omega ( Y )$ is an invariant closed set.

Definition 5.5.22 Let $S$ be a compact invariant set for $\varphi$ ; a subset $A \subset S$ is called an attractor in $S$ , if there exists a neighborhood $U$ of $A$ such that $\omega ( U \cap S ) = A$ . The dual repeller of $A$ in $S$ is defined by

$$
A ^ {*} = \{x \in S | \omega (x) \cap A = \emptyset \}.
$$

The pair $( A , A ^ { * } )$ is called an attractor–repeller pair.

The set

$$
C \left(A ^ {*}, A, S\right) = \{x \in S | \omega (x) \subset A, \omega^ {*} (x) \subset A ^ {*} \}
$$

is called the set of connecting orbits from $A ^ { * }$ to $A$ in $S$ .

The following properties hold.

(1) $A$ and $A ^ { * }$ are disjoint compact invariant sets.

Proof. In the following, we use the notation $U$ as in Definition 5.5.22. In fact, $A = \omega ( U \cap S )$ is invariant. Both $\omega ( x )$ and $A$ are invariant, so is $A ^ { * }$ . If $\exists x \in A \cap A ^ { * }$ , then $\omega ( x ) \subset A$ . But by definition of $A ^ { * }$ , $\omega ( x ) \cap A = \emptyset$ . Since $S$ is compact, $\omega ( x ) \neq \emptyset$ . This is a contradiction. Therefore $A \cap A ^ { * } = \varnothing$ .

By definition, $A$ is closed.

It remains to verify the closedness of $A ^ { * }$

If $\{ x _ { n } \} \subset A ^ { * }$ with $x _ { n } \to x \in S$ , and if $\omega ( x ) \cap A \neq \emptyset$ , then $\varphi ( t , x ) \in U$ for some $t > 0$ . Consequently $\varphi ( t , x _ { n } ) \in U$ for $n$ large. Therefore $\omega ( x _ { n } ) ~ \subset$ $\omega ( U \cap S ) = A$ . This is impossible. Therefore $\omega ( x ) \cap A = \emptyset$ , i.e., $x \in A ^ { * }$

(2) For any open neighborhood $V$ of $A$ , $\exists T = T ( V )$ such that ∪ ϕ(t, U ∩ $\underset { t \geq T } { \bigcup } \varphi ( t , U \cap$ $S ) \subset V$ .

Proof. If not, $\exists V \supset A$ open, $\exists x _ { n } \in U \cap S \exists t _ { n } \to + \infty$ such that $\varphi ( t _ { n } , x _ { n } ) \not \in V$ . Since $S$ is a compact invariant set, $\varphi ( t _ { n } , x _ { n } ) \to y \in S \backslash V$ . But $y \in \omega ( U \cap S ) =$ $A$ . This is a contradiction. □

(3) If $B$ is a closed set disjoint from $A$ , then $\forall \epsilon > 0 \exists T = T _ { \epsilon } > 0$ such that $d ( x , A ^ { * } ) < \epsilon$ , whenever $x \in S$ and $t \geq T$ such that $\varphi ( t , x ) \in B$ .

Proof. If not, $\exists \epsilon \ > \ 0 \ \exists t _ { n } \  \ \infty \ \exists x _ { n } \ \in \ S$ such that $\varphi ( t _ { n } , x _ { n } ) ~ \in ~ B$ but $d ( x _ { n } , A ^ { * } ) \geq \epsilon$ .

Since $S$ is compact and invariant, $\exists x \in S$ such that $x _ { n } \to x$ . Then $d ( x , A ^ { * } ) \geq \epsilon$ , which implies that $x \notin A ^ { * }$ , i.e., $\omega ( x ) \cap A \neq \emptyset$ . Thus for the neighborhood $U$ of $A$ ; we have $t _ { 1 } \in R ^ { 1 }$ such that $\varphi ( t _ { 1 } , x ) \in U \cap S$ and then $\varphi ( t _ { 1 } , x _ { n } ) \in U \cap S$ for $n$ large. Let $V = X \backslash B$ , which is an open neighborhood of $A$ , from (2), $\exists T > 0$ such that $\varphi ( t , x _ { n } ) \in V \forall t \geq T$ . This contradicts $\varphi ( t _ { n } , x _ { n } ) \in B \ \forall n$ . □

For any $x \in X$ , we denote the orbit passing through $x$ by $o ( x ) \ =$ $\{ \varphi ( t , x ) | t \in R ^ { 1 } \}$ .

(4) $\omega ( y ) \cap A ^ { * } \neq \varnothing \Rightarrow o ( y ) \subset A ^ { * } .$

Proof. Let $B$ be a closed neighborhood of $A ^ { * }$ such that $A \cap B = \emptyset$ . Since $\omega ( y ) \cap A ^ { * } \neq \varnothing$ , $\exists t _ { n } \to + \infty$ such that $\varphi ( t _ { n } , y ) \in B$ . $\forall t \in R ^ { 1 }$ , let $z = \varphi ( t , y )$ ; we have $\varphi ( t _ { n } - t , z ) = \varphi ( t _ { n } , y ) \in B$ , thus from (3), $\forall \epsilon > 0$ , we have $d ( z , A ^ { * } ) < \epsilon$ . Since $\epsilon > 0$ is arbitrarily small, $o ( y ) \subset A ^ { * }$ . □

(5) $\omega ^ { * } ( y ) \cap A \neq \emptyset \Rightarrow o ( y ) \subset A .$

Proof. By the assumption that $\exists t _ { n } \to + \infty$ such that $\varphi ( - t _ { n } , y ) \in U \cap S$ , then $\forall t \in R ^ { 1 }$ , $t + t _ { n } \geq 0$ for $n$ large, from $\varphi ( t , y ) = \varphi ( t _ { n } + t , \varphi ( - t _ { n } , y ) )$ we have $\varphi ( t , y ) \in \omega ( U \cap S )$ , i.e., $o ( y ) \subset A$ . □

Lemma 5.5.23 Let $( A , A ^ { * } )$ be an attractor–repeller pair of a compact invariant set $S$ , then

$$
S = A \cup A ^ {*} \cup C (A ^ {*}, A, S).
$$

Proof. It is sufficient to prove that $\forall x \in S \backslash ( A \cup A ^ { * } ) , \omega ( x ) \subset A$ , and $\omega ^ { \ast } ( x ) \subset$ $A ^ { * }$ .

1. $\omega ( x ) \subset A : \forall y \in \omega ( x )$ , i.e., $\exists t _ { n } \to + \infty$ such that $\varphi ( t _ { n } , x )  y$ . Let $B = S \backslash U$ , where $U$ is as in Definition 5.5.22, then either $\exists n _ { 0 } \in \mathbb { N }$ such that $z = \varphi ( t _ { n _ { 0 } } , x ) \in U$ or $\varphi ( t _ { n } , x ) \in B$ , $\forall n$ . But the latter case is impossible, because from (3) we would have $x \in A ^ { * }$ . In the former case, $y \in \omega ( S \cap U ) = A$ .

2. $\omega ^ { * } ( x ) \subset A ^ { * } : \forall y \in \omega ^ { * } ( x )$ , $\exists t _ { n } \to + \infty$ such that $z _ { n } = \varphi ( - t _ { n } , x ) \to y$ . Let $B = \{ x \}$ . Since $\{ x \} \cap A = \varnothing$ , we have $d ( z _ { n } , A ^ { * } ) \to 0$ , provided by (3). Thus $y \in A ^ { * }$ . □

Combining the conclusions of Lemma 5.5.23 and properties (4) and (5), we see that $\forall x \in C ( A ^ { * } , A , S )$ , $\omega ( x ) \subset A$ and $\omega ^ { * } ( x ) \subset A ^ { * }$ , and there are connecting orbits from $A$ to $A ^ { * }$ but none from $A ^ { * }$ to $A$ .

Let us define the Morse decomposition for a compact invariant set.

Definition 5.5.24 Let $S$ be a compact invariant set of $X$ with respect to the flow $\varphi$ . An ordered collection $( M _ { 1 } , \ldots , M _ { n } )$ of invariant subsets $M _ { j } \subset S$ i s called a Morse decomposition of $S$ , if there exists an increasing sequence of attractors in $S$ :

$$
\varnothing = A _ {0} \subset A _ {1} \subset \dots \subset A _ {n} = S
$$

such that $M _ { j } = A _ { j } \cap A _ { j - 1 } ^ { * }$ , 1 ≤ j ≤ n.

Example 1. An attractor–repeller pair $( A , A ^ { * } )$ of a compact invariant set $S$ is a Morse decomposition, where $A _ { 0 } = \emptyset , \ A _ { 1 } = A , \ A _ { 2 } = S$ .

Example 2. Suppose $f \in C ^ { 1 } ( M , \mathbb { R } ^ { 1 } )$ , where $M$ is a compact manifold. Assume that $f ^ { - 1 } [ a , b ] \cap K = \{ p _ { 1 } , . . . , p _ { n } \}$ , where $a , ~ b$ are regular values of $f$ with $f ( p _ { i } ) \leq f ( p _ { i + 1 } ) , i = 1 , \ldots , n - 1 $ $i = 1 , \ldots , n - 1$ . Then $( \{ p _ { 1 } \} , \ldots , \{ p _ { n } \} )$ is a Morse decomposition of $S = I ( f ^ { - 1 } ( [ a , b ] ) )$ .

In fact, by setting $A _ { 0 } = \emptyset$ , and $A _ { i } = \{ x \in S | \omega ^ { * } ( x ) \subset \{ p _ { 1 } , . . . , p _ { i } \} \} , i =$ $1 , 2 , \ldots , n$ . We shall verify that this is an increasing sequence of attractors in $S$ with $A _ { i } ^ { * } = \{ x \in S | \ \omega ( x ) \subset \{ p _ { i + 1 } , \dots , p _ { n } \} \}$ $A _ { i } ^ { * } = \{ x \in S |$ , and then $A _ { i } \cap A _ { i - 1 } ^ { * } = \{ p _ { i } \}$ .

It is proved by induction. Let $S _ { k } \ : = \ : I ( f ^ { - 1 } [ a , a _ { k } ] )$ , where $a _ { k } \in \mathsf { \Gamma } ( f ( p _ { k } )$ , $f ( p _ { k + 1 } ) ) , k = 1 , \dots , n - 1$ , and $a _ { n } = b$ . Thus $S _ { n } = S$ . We verify that $A _ { i } =$ x  Sk ω∗(x) p1, p2, . . . , pi , i = 1, 2, . . . , k, is an increasing sequence of attractors in $S _ { k }$ , and $A _ { i } ^ { * } = \left\{ x \in S _ { k } | \omega ( x ) \subset \left\{ p _ { i + 1 } , \dots , p _ { k } \right\} \right\}$ .

For $n = 1$ . Obviously, $A _ { 1 } = \{ p _ { 1 } \}$ is an attractor in $S _ { 1 } = \{ p _ { 1 } \}$ , with $A _ { 1 } ^ { * } = \varnothing$ and $A _ { 0 } ^ { * } = S _ { 1 }$ . Thus $A _ { 1 } \cap A _ { 0 } ^ { * } = \{ p _ { 1 } \}$ .

If the conclusion holds for $ { \boldsymbol { n } } \ =  { \boldsymbol { k } }$ , i.e., $A _ { i } ~ = ~ \{ x ~ \in ~ S _ { k } | ~ \omega ^ { * } ( x ) ~ \subset$ $\{ p _ { 1 } , \dotsc , p _ { i } \} \} , i = 1 , \dotsc , k$ , is an increasing sequence of attractors in $S _ { k }$ .

For $n = k + 1$ . It is easily seen that $S _ { k }$ is an attractor in $S _ { k + 1 } .$ . Indeed, let $U _ { k } = f ^ { - 1 } ( a , a _ { k } )$ , then $\omega ( S _ { k + 1 } \cap U _ { k } ) = S _ { k }$ with $S _ { k } ^ { * } = \{ p _ { k + 1 } \}$ . Then

$$
A _ {k} = \{x \in S _ {k + 1} | \omega^ {*} (x) \subset \left\{p _ {1}, \dots , p _ {k} \right\} \} = S _ {k}
$$

is an attractor in $S _ { k + 1 }$ with $A _ { k } ^ { * } = S _ { k } ^ { * } = \{ p _ { k + 1 } \}$ . In this case

$$
\emptyset = A _ {0} \subset A _ {1} \subset \dots \subset A _ {k} = S _ {k} \subset A _ {k + 1} = S _ {k + 1}
$$

is an increasing sequence of attractors in $S = S _ { k + 1 }$ . By easy computation,

$$
A _ {i} ^ {*} = \left\{x \in S _ {k + 1} \mid \omega (x) \subset \left\{p _ {i + 1}, \dots , p _ {k + 1} \right\} \right\} \text {a n d t h e n} A _ {i} \cap A _ {i - 1} ^ {*} = \left\{p _ {i} \right\}.
$$

Our conclusion follows from the mathematical induction.

By the definition of the Morse decomposition $( M _ { 1 } , \ldots , M _ { n } )$ of $S$ , we have the following conclusions:

(1) $\{ M _ { i } | \ 1 \le i \le n \}$ are pairwise disjoint.

Indeed if $i < j$ , then $M _ { i } \cap M _ { j } = A _ { i } \cap A _ { i - 1 } ^ { * } \cap A _ { j } \cap A _ { j - 1 } ^ { * } \subset A _ { i } \cap A _ { j - 1 } ^ { * } \subset$ $A _ { j - 1 } \cap A _ { j - 1 } ^ { * } = \varnothing$ , from property (1).

− − (2) ∀x ∈ S\( ∪ Mi), $\forall x \in S \backslash ( \bigcup _ { 1 \leq i \leq n } M _ { i } )$ $\exists i < j + 1$ such that $\omega ( x ) \subset M _ { i }$ and $\omega ^ { * } ( x ) \subset M _ { j + 1 }$ . Indeed, set

$$
i = \min  \left\{k \in \mathbb {N} \mid \omega (x) \subset A _ {k} \right\} \text {a n d} j = \max  \left\{k \in \mathbb {N} \mid \omega^ {*} (x) \subset A _ {k} ^ {*} \right\}.
$$

From $A _ { 0 } = \emptyset$ and $A _ { n } \ = \ S$ , we see $0 \ < \ i$ and $j < n$ . Since $\omega ( x ) \not \subset A _ { i - 1 }$ provided by the decomposition lemma, $x \in A _ { i - 1 } ^ { * }$ , and then by property (4), $o ( x ) \subset A _ { i - 1 } ^ { * }$ , in particular, $\omega ( x ) \subset A _ { i - 1 } ^ { * }$ . By the same reason, $\omega ^ { * } ( x ) \not \subset A _ { j + 1 }$ , we have $o ( x ) \subset A _ { j + 1 }$ and $\omega ^ { \ast } ( x ) \subset A _ { j + 1 }$ .

We claim that $i \ \leq \ j + 1$ . For otherwise, $j + 1 \le i - 1$ , then $o ( x ) ~ \subset$ $A _ { j + 1 } \cap A _ { i - 1 } ^ { * } \subset A _ { i - 1 } \cap A _ { i - 1 } ^ { * } = \varnothing$ , a contradiction. For $i = j + 1$ , we have $o ( x ) \subset A _ { i } \cap A _ { i - 1 } ^ { * } = M _ { i }$ , it contradicts $x \notin M _ { i }$ . Therefore the only possibility is $i < j + 1$ . In this case we have $\omega ( x ) \subset A _ { i } \cap A _ { i - 1 } ^ { * } = M _ { i }$ and $\omega ^ { \ast } ( x ) \in$ $A _ { j + 1 } \cap A _ { j } ^ { * } = M _ { j + 1 }$ .

Comparing with Example 2, the following conclusion is easily verified:

(3) $A _ { j } = \{ x \in S | \omega ( x ) \cup \omega ^ { * } ( x ) \subset M _ { 1 } \cup . . . \cup M _ { j } \}$ and $A _ { j } ^ { \ast } = \{ x \in S | \omega ( x ) \cup$ $\omega ^ { * } ( x ) \subset M _ { j + 1 } \cup \ldots \cup M _ { n } \} , \ j = 1 , 2 , \ldots , n .$   
(4) If $( M _ { 1 } , \ldots , M _ { n } )$ is a Morse decomposition of $S$ , and if $\varnothing \ = \ A _ { 0 } \ \subset$ $A _ { 1 } \subset . . . \subset A _ { n } = S$ is the associated increasing sequence of attractors, then for $0 ~ < ~ i ~ \le ~ j ~ < ~ n$ , $( M _ { 1 } , \ldots , M _ { i - 1 } , M _ { j i } , M _ { j + 1 } , \ldots , M _ { n } )$ is again a Morse decomposition of $S$ , where $M _ { j i } = A _ { j } \cap A _ { i - 1 } ^ { * } = \{ x \in S |$ $\omega ( x ) \cup \omega ^ { * } ( x ) \subset$ $M _ { i } \cup \dots \cup M _ { j } \}$ .   
(5) If $S$ is an isolating invariant compact set, and if $\{ M _ { 1 } , \ldots , M _ { n } \}$ is a Morse decomposition of $S$ , then $M _ { i } , \ 1 \leq i \leq n$ are isolating.

Proof. We assume that there exists a compact neighborhood $U$ of $S$ such that $I ( U ) = S \subset \operatorname { i n t } ( U )$ . According to (1), $M _ { i }$ and $M _ { j }$ are disjoint, $\forall i \ne j$ , we have a neighborhood $U _ { i } \subset U$ of $M _ { i }$ such that $U _ { i } \cap M _ { j } = \emptyset \ \forall j \neq i$ . We claim that $I ( U _ { i } ) = M _ { i }$ .

“⊂” $\forall x \in I ( U _ { i } )$ , $o ( x ) \subset I ( U _ { i } ) \subset U _ { i } \subset U$ . Therefore $\omega ( x ) \cup \omega ^ { * } ( x ) \subset U _ { i } \cap S$ . According to (2), $\omega ( x ) \cup \omega ^ { * } ( x ) \subset M _ { i }$ and then $x \in M _ { i }$ .

“ $\supset$ ” Since $M _ { i }$ is an invariant set in $U _ { i } , ~ M _ { i } \subset I ( U _ { i } )$ .

For a compact isolating invariant set $S$ , let $U \in \Sigma$ be its compact isolated neighborhood, i.e., $I ( U ) = S \subset \operatorname { i n t } ( U )$ . We define

$$
I _ {j} ^ {+} = \{x \in U | \varphi (R _ {+} ^ {1}, x) \subset U, \omega (x) \subset M _ {j} \cup \ldots \cup M _ {n} \} \mathrm {a n d}
$$

$$
I _ {j} ^ {-} = \{x \in U | \varphi (R _ {-} ^ {1}, x) \subset U, \omega^ {*} (x) \subset M _ {1} \cup \dots \cup M _ {j} \}, 1 \leq j \leq n.
$$

(6)

$$
I _ {1} ^ {+} \supset I _ {2} ^ {+} \supset \dots \supset I _ {n} ^ {+}, \varphi \left(R _ {+} ^ {1}, I _ {j} ^ {+}\right) = I _ {j} ^ {+}, 1 \leq j \leq n,
$$

$$
I _ {1} ^ {-} \supset I _ {2} ^ {-} \supset \dots \supset I _ {n} ^ {-}, \varphi (R _ {-} ^ {1}, I _ {j} ^ {-}) = I _ {j} ^ {-}, 1 \leq j \leq n.
$$

$$
A _ {j} \subset I _ {j} ^ {-}, A _ {j - 1} ^ {*} \subset I _ {j} ^ {+},
$$

and then

$$
M _ {j i} = A _ {j} \cap A _ {i - 1} ^ {*} = I _ {i} ^ {+} \cap I _ {j} ^ {-},
$$

for $i \leq j$

(7) $I _ { j } ^ { \pm }$ , $1 \leq j \leq n$ are compact.

Proof. It is sufficient to show that they are closed. For $I _ { 1 } ^ { + }$ and $I _ { n } ^ { - }$ it follows from the continuity of the flow and the closedness of $U$ . In the case where $n = 2$ , it suffices to verify the closedness of $I _ { 2 } ^ { + }$ and $I _ { 1 } ^ { - }$ . Let $( M _ { 1 } , M _ { 2 } )$ be the Morse decomposition, and let $\{ x _ { n } \} \subset I _ { 2 } ^ { + }$ and $x _ { n } \to x$ ; we want to show that $x \in I _ { 2 } ^ { + }$ . Since $x \in I _ { 1 } ^ { + }$ , we have $\omega ( x ) \subset M _ { 1 } \cup M _ { 2 }$ . It is sufficient to show that $\omega ( x ) \not \subset M _ { 1 }$ .

Suppose not, i.e., $\omega ( x ) \subset M _ { 1 }$ . We choose open neighborhoods $V _ { i }$ of $M _ { i } , \thinspace i =$ $1 , 2$ such that ${ \overline { { V } } } _ { 1 } \cap { \overline { { V } } } _ { 2 } = \emptyset$ . Since $\omega ( x _ { n } ) \subset M _ { 2 }$ and $x _ { n } \to x$ , we have $t _ { n } ^ { \prime } \geq 0$ and $t _ { n } ^ { \prime \prime } \geq 0$ such that $\varphi ( [ t _ { n } ^ { \prime } , \infty ) , x _ { n } ) \subset V _ { 2 }$ and $\varphi ( t _ { n } ^ { \prime \prime } , x _ { n } ) \in V _ { 1 }$ . Thus $\exists t _ { n } \in$ $[ t _ { n } ^ { \prime \prime } , t _ { n } ^ { \prime } ]$ such that $\varphi ( [ t _ { n } , \infty ) , x _ { n } ) \subset U \backslash V _ { 1 }$ and $\varphi ( t _ { n } , x _ { n } ) \in U \backslash ( V _ { 1 } \cup V _ { 2 } )$ . From the compactness of $U$ , after a subsequence $\varphi ( t _ { n } , x _ { n } ) \to y \not \in M _ { 1 } \cup M _ { 2 }$ and $\varphi ( [ 0 , + \infty ) , y ) \subset U \backslash V _ { 1 }$ . From $x _ { n } \in I _ { 1 } ^ { + }$ , we have $\varphi ( t _ { n } , x _ { n } ) \in I _ { 1 } ^ { + }$ ; it follows that $y \in I _ { 1 } ^ { + }$ , and then $\omega ( y ) \subset M _ { 2 }$ . If $\left\{ t _ { n } \right\}$ is bounded, then after a subsequence, $t _ { n } \to s$ ; we have $\omega ( \varphi ( - s , y ) ) = \omega ( y ) \subset M _ { 2 }$ . But $\varphi ( - s , y ) = x$ . This contradicts $\omega ( x ) \subset M _ { 1 }$ . If $\left\{ t _ { n } \right\}$ is unbounded, without loss of generality may assume, $t _ { n }  + \infty . \forall t > 0 \exists n$ large such that $\varphi ( [ - t , 0 ] , \varphi ( t _ { n } , x _ { n } ) ) = \varphi ( [ t _ { n } - t , t _ { n } ] , x _ { n } ) \subset$ $\varphi ( [ 0 , \infty ) , x ) \subset U$ ; it follows that $\varphi ( [ - t , 0 ] , y ) \subset U$ . Since $t > 0$ is arbitrary, we have $\varphi ( R _ { - } ^ { 1 } , y ) \subset U$ . Combining with $\varphi ( R _ { + } ^ { 1 } , y ) \subset U \backslash V _ { 1 }$ , it follows that $o ( y ) \subset U$ , i.e., $y \in I ( U ) = S$ . Since $( M _ { 1 } , M _ { 2 } )$ is a Morse decomposition of $S$ , and $\omega ( y ) \subset M _ { 2 }$ , it follows that $y \in M _ { 2 }$ , again a contradiction. Similarly, $I _ { 1 } ^ { - }$ is closed.

For $n > 2$ , it can be reduced to the case where $n = 2$ by defining $M _ { 1 } ^ { \prime } =$ $M _ { j - 1 , 1 }$ and $M _ { 2 } ^ { \prime } = M _ { n , j }$ . Thus $( M _ { 1 } ^ { \prime } , M _ { 2 } ^ { \prime } )$ is again a Morse decomposition of $S$ from (4), since $I _ { j } ^ { + } = I _ { 2 } ^ { + ^ { \prime } }$ , where $I _ { 2 } ^ { + ^ { \prime } }$ is that in $( M _ { 1 } ^ { \prime } , M _ { 2 } ^ { \prime } )$ . Thus $I _ { j } ^ { + }$ is closed. Similarly $I _ { j } ^ { - } = I _ { j } ^ { - ^ { \prime \prime } }$ , where $M _ { 1 } ^ { \prime \prime } = M _ { j 1 }$ and $M _ { 2 } ^ { \prime \prime } \ = \ M _ { n , j + 1 }$ . The proof is complete. □

(8) Let $U \in \Sigma$ be compact. If $Z \subset U$ is closed, and if $\forall x \in Z \exists t > 0$ such that $\varphi ( t , x ) \not \in U$ , then the minimal positively invariant set in $U$ which contains $Z$ , defined by

$$
P (Z, U) = \{x \in U | \exists t \geq 0 \text {s u c h t h a t} \varphi ([ - t, 0 ], x) \subset U \text {a n d} \varphi (- t, x) \in Z \}
$$

is closed.

It follows directly from the continuity of the flow and the closedness of $U$

Lemma 5.5.25 Suppose that $U \in \Sigma$ is a compact isolating neighborhood with $S = I ( U )$ , and that $( M _ { 1 } , \ldots , M _ { n } )$ is a Morse decomposition of $S$ . Then for every open neighborhood $V$ of $I _ { j } ^ { - }$ , there exists a compact neighborhood $N _ { j }$ such that $N _ { j } \subset V$ and $N _ { j }$ is positively invariant in $U$ .

Proof. Since $I _ { j + 1 } ^ { + } \cap I _ { j } ^ { - } = \varnothing$ , we choose a compact neighborhood $W \subset U$ of $I _ { j + 1 } ^ { + } \cap I _ { n } ^ { - }$ such that $W \cap I _ { j } ^ { - } = \varnothing$ .

1. $\exists t ^ { * } > 0$ such that $\varphi ( [ - t ^ { * } , 0 ] , x ) \subset U \backslash W \Rightarrow x \in V \cap ( U \backslash W )$ . If not, $\exists \{ x _ { n } \} \subset U$ and $\{ t _ { n } \} \subset R _ { + } ^ { 1 }$ with $t _ { n } \to + \infty$ such that $\varphi ( [ - t _ { n } , 0 ] , x _ { n } ) \subset U \backslash W$ , but $x _ { n } \not \in V \cap ( U \backslash W )$ . Let $x$ be a limit point of $x _ { n }$ , then $x \not \in V \cap ( U \backslash W )$ and $\varphi ( R _ { - } ^ { 1 } , x ) \subset U \backslash W$ . Thus $\omega ^ { * } ( x ) \subset M _ { 1 } \cup . . . \cup M _ { j }$ . Therefore $x \in I _ { j } ^ { - } \subset$ $V \cap ( U \backslash W )$ . A contradiction.

2. Define two disjoint subsets of $I _ { j } ^ { - }$

$$
A = \{x \in I _ {j} ^ {-} | \varphi ([ 0, t ^ {*} ], x) \subset U \} \mathrm {a n d} B = \{x \in I _ {j} ^ {-} | \varphi ([ 0, t ^ {*} ], x) \not \subset U \},
$$

then $\forall x \in A \exists \delta = \delta ( x ) > 0$ such that $\varphi ( [ 0 , t ^ { * } ] , B _ { \delta } ( x ) ) \subset V \cap ( X \backslash W )$ , from step 1. $\forall x \in B \ \exists t = t ( x ) > 0$ such that $\varphi ( [ 0 , t ] , x ) \subset V \cap ( X \backslash W )$ and $\varphi ( t , x ) \not \in U$ . This enables us to choose $\delta = \delta ( x ) > 0$ such that $\varphi ( [ 0 , t ( x ) ] , B _ { \delta } ( x ) ) \subset V \cap$ $( X \backslash W )$ and $\varphi ( t ( x ) , B _ { \delta ( x ) } ( x ) ) \cap U = \varnothing$ . Since $I _ { j } ^ { - }$ is compact, $\exists x _ { 1 } , \dotsc , x _ { k } \in I _ { j } ^ { - }$ such that $I _ { j } ^ { - } \subset \bigcup _ { i = 1 } ^ { k } B _ { \delta ( x _ { i } ) } ( x _ { i } )$ , and then we choose a compact neighborhood of $I _ { j } ^ { - }$ , $Z \subset \bigcup _ { i = 1 } ^ { k } B _ { \delta ( x _ { i } ) } ( x _ { i } )$ .

3. We claim that $P ( Z , U ) \subset V \cap ( U \backslash W )$ .

$\forall x \in P ( Z , U )$ , by definition $\exists t \geq 0$ such that $\varphi ( [ - t , 0 ] , x ) \subset U$ , and $\varphi ( - t , x ) \in Z$ . Then by the definition of $Z$ , $\exists i \in [ 1 , k ]$ , such that $\varphi ( - t , x ) \in$ $B _ { \delta ( x _ { i } ) } ( x _ { i } )$ . If $x \not \in V \cap ( U \backslash W )$ , there are two cases:

Either $x _ { i } \in A$ , then $\varphi ( [ - t , t ^ { * } - t ] , x ) \subset V \cap ( X \backslash W )$ which implies $t \ >$ $t ^ { * }$ , and then $\exists t _ { 1 } ~ \in ~ [ 0 , t ^ { * } - t ]$ such that $\varphi ( [ - t , - t _ { 1 } ] , x ) \subset V \cap ( \underline { { X } } \backslash W )$ and $y \ = \ \varphi ( - t _ { 1 } , x ) \ \not \in \ V \cap ( X \backslash W )$ . It implies that $\varphi ( [ - t ^ { * } , 0 ] , y ) \subset \overline { { U \backslash W } }$ , but $y \not \in V \cap ( X \backslash W )$ . This contradicts step 1.

Or $x _ { i } \in B$ , then $\varphi ( [ - t , t ( x _ { i } ) - t ] , x ) \subset V \cap ( X \backslash W )$ and $\varphi ( t ( x _ { i } ) - t , x ) \not \in U$ . From $\varphi ( [ - t , 0 ] , x ) \subset U$ , we see that $t ( x _ { i } ) > t$ . But if $x \not \in V \cap ( X \backslash W )$ , we would have $t ( x _ { i } ) ~ < ~ t$ . Again this is a contradiction. We have proved that $x \in V \cap ( X \backslash W ) \cap U = V \cap ( U \backslash W )$ .

4. $P ( Z , U )$ is compact.

In fact, if $\{ x _ { n } \} \subset P ( Z , U )$ with $x _ { n } \to x$ , i.e., $\exists t _ { n } ~ \geq ~ 0$ such that $\varphi ( [ - t _ { n } , 0 ] , x _ { n } ) \subset U$ and $\varphi ( - t _ { n } , x _ { n } ) \in Z$ , then $\varphi ( [ - t _ { n } , 0 ] , x _ { n } ) \subset P ( Z , U ) \subset$ $V \cap ( U \backslash W )$ . If $\left\{ t _ { n } \right\}$ is unbounded, then $\varphi ( R _ { - } ^ { 1 } , x ) \subset V \cap ( U \backslash W )$ . It follows that $\omega ^ { * } ( x ) \in M _ { i }$ for some $i \le j$ , and then $x \in I _ { j } ^ { - } \subset P ( Z , U ) .$ . Otherwise, $\{ t _ { n } \}$ is bounded; after a subsequence, $t _ { n } \to s \geq 0$ , then $\varphi ( [ - s , 0 ] , x ) \subset U$ and $\varphi ( - s , x ) \in Z$ ; again $x \in P ( Z , U )$ .

Now we set $N _ { j } = P ( Z , U )$ . Combining steps 1 and 4 and (8), the conclusion follows. □

Inductively applying Lemma 5.5.25, we have the following:

Theorem 5.5.26 Let $S$ be an isolating invariant set with a compact isolated neighborhood $U _ { 0 }$ . If $( M _ { 1 } , \ldots , M _ { n } )$ is a Morse decomposition of $S$ , then there exists an increasing sequence of compact sets: $N _ { 0 } \subset N _ { 1 } \subset . . . \subset N _ { n }$ such that

$( N _ { j } , N _ { i - 1 } )$ is an index pair for ${ M } _ { j i } \ \forall i \leq j$ . In particular, $( N _ { n } , N _ { 0 } )$ is an index pair for $S$ and $( N _ { j } , N _ { j - 1 } )$ is an index pair for $M _ { j } , \forall j$ .

Proof. According to Theorem 5.5.13, in combination with Remark 5.5.4, there exists an index pair $( N _ { n } , N _ { 0 } )$ for $S$ . Setting $U = \overline { { N _ { n } \backslash N _ { 0 } } }$ , we define compact sets $I _ { j } ^ { \pm } \ j = 1 , 2 , \ldots , n$ see (7), and $U _ { n } = U \cap N _ { n }$ . Applying Lemma 5.5.25, we have a compact neighborhood $U _ { n - 1 } \subset U$ of $I _ { n - 1 } ^ { - }$ , which is positively invariant in $N _ { n }$ , and $U _ { n - 1 } \cap I _ { n } ^ { + } = \emptyset$ . Define $N _ { n - 1 } = U _ { n - 1 } \cup N _ { 0 }$ . Inductively, one defines − a compact neighborhood $U _ { j } \subset U$ of $I _ { j } ^ { - }$ −, which is positively invariant in $N _ { j + 1 }$ and $U _ { j } \cap I _ { j + 1 } ^ { + } = \emptyset$ , and $N _ { j } = U _ { j } \cup N _ { 0 } , ~ j = n - 1 , \ldots , 1$ . □

Recalling (5.2) and (5.3), as a consequence of Theorem 5.5.26, we have the Morse relations for the Morse decomposition $( M _ { 1 } , \ldots , M _ { n } )$ , i.e.,

$$
\sum_ {j = 1} ^ {n} P (t; N _ {j}, N _ {j - 1}) = P (t; N _ {n}, N _ {0}) + (1 + t) Q (t),
$$

where $Q$ is a formal series with nonnegative coefficients, and

$$
P (t; N _ {j}, N _ {j - 1}) = \sum_ {q = 0} ^ {\infty} t ^ {q} \quad \mathrm {r a n k} \overline {{H}} ^ {q} (N _ {j}, N _ {j - 1}; F),
$$

or, in the spirit of Theorem 5.1.29,

$$
\sum_ {j = 1} ^ {n} h (t; M _ {j}) = h (t; U _ {0}) + (1 + t) Q (t).
$$

Let us return to Example 2, and assume that $f$ is a Morse function, i.e., $\forall p \in K$ , $f ^ { \prime \prime } ( p )$ is invertible, or in other words, the gradient system ${ \dot { x } } = - f ^ { \prime } ( x )$ is hyperbolic.

$\forall p \in K$ , let

$$
W ^ {s} (p) = \{x \in M | \omega (x) = p \} \text {a n d} W ^ {u} (p) = \{x \in M | \omega^ {*} (x) = p \}
$$

be the stable and unstable manifolds at $p$ , respectively. One has

$$
\dim W ^ {s} (p) = \operatorname {i n d} (f, p) = \operatorname {c o d i m} W ^ {u} (p).
$$

According to the Sard–Smale theorem, one may choose a generic Riemannian metric on $M$ such that $W ^ { s } ( p ) \pitchfork W ^ { u } ( q ) \ \forall p , q \in K$ . Such Morse functions are said to be of Morse–Smale type.

Thus, if $\operatorname { i n d } ( f , q ) - \operatorname { i n d } ( f , p ) = k$ , and let

$$
M (q, p) = W ^ {u} (q) \cap W ^ {s} (p), \dim M (q, p) = k.
$$

We have seen that if $K = \{ p _ { 1 } , \ldots , p _ { n } \}$ , then $( \{ p _ { 1 } \} , \ldots , \{ p _ { n } \} )$ is a Morse decomposition of $M$ , where $p _ { 1 } , \ldots , p _ { n }$ are ordered by their values $\{ f ( p _ { i } ) | i \leq i \leq$

$n \}$ . However, there is another way to make a Morse decomposition according to their Morse indices. Setting

$$
S _ {k} = \{p \in K | \operatorname {i n d} (f, p) = k \}, 0 \leq k \leq m = \dim M,
$$

and letting

$$
A _ {j} = \{x \in M | \omega (x) \cup \omega^ {*} (x) \subset S _ {0} \cup \dots \cup S _ {j} \} 0 \leq j \leq m,
$$

then $( S _ { 0 } , S _ { 1 } , \ldots , S _ { m } )$ is a Morse decomposition of $M$ .

According to Theorem 5.5.26, there exists an increasing sequence of compact sets: $\emptyset = N _ { - 1 } \subset N _ { 0 } \subset . . . \subset N _ { m }$ such that $( N _ { j } , N _ { j - 1 } )$ is an index pair of $S _ { j } , 0 \le j \le m$ .

Noticing the following exact sequences, we have: (1) $j$ is injective, (2) $\partial _ { k - 1 } ^ { \prime } \circ j = 0$ , and (3) $\partial _ { k } ^ { \prime } = j \circ i$ . Thus $\partial _ { k - 1 } ^ { \prime } \circ \partial _ { k } ^ { \prime } = 0$ .

![](images/7a8ae38e4b95fcb1ff2059e6afb194f84bb3f994daa17432831d5ae596b87399.jpg)  
Fig. 5.6.

With suitable geometric representation of $\left\{ H _ { k } ( N _ { k } , N _ { k - 1 } ; G ) \vert k = 0 \right.$ , $1 , \ldots , m \}$ , Floer used $\partial ^ { \prime }$ to establish the Floer homology.

In order to extend the notion of Morse decomposition to noncompact isolated invariant sets, let us keep in mind the Morse relations for (PS) functions on Banach–Finsler manifolds, and introduce the following:

Definition 5.5.27 Let $U _ { 1 } , U _ { 2 } \in \Sigma$ with $\operatorname { i n t } ( U _ { 1 } ) \cap \operatorname { i n t } ( U _ { 2 } ) = \emptyset$ . One defines an order $U _ { 2 } > U _ { 1 }$ , if $\exists T > 0$ such that $U _ { 1 } \cap G ^ { T } ( U _ { 1 } \cup U _ { 2 } )$ is positively invariant with respect to $G ^ { T } ( U _ { 1 } \cup U _ { 2 } )$ .

If $U _ { 2 } > U _ { 1 }$ and $U _ { 1 } > U _ { 2 }$ , then we say that $U _ { 1 }$ and $U _ { 2 }$ are $\varphi$ -disconnected. Otherwise, we say that they are $\varphi$ -connected.

Lemma 5.5.28 Let $U _ { 1 } , U _ { 2 } \in \Sigma$ with $U _ { 2 } > U _ { 1 }$ , and let $U = U _ { 1 } \cup U _ { 2 }$ . Then there exists closed subsets $N _ { 0 } \subset N _ { 1 } \subset N _ { 2 }$ such that $( N _ { 2 } , N _ { 0 } )$ , $( N _ { 2 } , N _ { 1 } )$ and $( N _ { 1 } , N _ { 0 } )$ are index pairs for $U$ , $U _ { 2 }$ and $U _ { 1 }$ , respectively.

Proof. Taking $T > 0$ large enough such that $( G ^ { T } ( U ) , \Gamma ^ { T } ( U ) )$ is an index pair for $U$ , $G ^ { T } ( U _ { 1 } ) \subset \operatorname { i n t } ( U _ { 1 } )$ and $U _ { 1 } \cap G ^ { T } ( U )$ is positively invariant with respect to $G ^ { T } ( U )$ . Setting $N _ { 0 } = \Gamma ^ { T } ( U ) , \ N _ { 1 } = ( U _ { 1 } \cap G ^ { T ^ { \prime } } ( U ) ) \cup \Gamma ^ { T ^ { \prime } } ( U )$ $N _ { 0 } = \Gamma ^ { T } ( U )$ and $N _ { 2 } = G ^ { T } ( U )$ . Obviously $( N _ { 2 } , N _ { 0 } )$ is an index pair for $U$ (Theorem 5.5.13).

(1) In order to verify that $( N _ { 1 } , N _ { 0 } )$ is an index pair for $U _ { 1 }$ , it is sufficient to verify the four conditions in Definition 5.5.12 as follows:

1.

$$
\begin{array}{l} G ^ {T} \left(N _ {1} \backslash N _ {0}\right) = G ^ {T} \left(U _ {1} \cap G ^ {T} (U)\right) \subset G ^ {T} \left(U _ {1}\right) \cap G ^ {2 T} (U) \subset \operatorname {i n t} \left(U _ {1} \cap G ^ {T} (U)\right) \\ = \operatorname {i n t} \left(N _ {1} \backslash N _ {0}\right). \\ \end{array}
$$

2. Since $\Gamma ^ { T } ( U )$ is positively invariant in $G ^ { T } ( U )$ , and $U _ { 1 } \cap G ^ { I ^ { \prime } } ( U )$ is positively invariant in $G ^ { T } ( U )$ , $\Gamma ^ { T } ( U )$ is positively invariant in $N _ { 1 }$ .   
3. By the same reasoning, $\Gamma ^ { T } ( U )$ is the exit set of $N _ { 1 }$   
4. $\overline { { N _ { 1 } \backslash N _ { 0 } } } = ( U _ { 1 } \cap G ^ { T } ( U ) ) \subset U _ { 1 }$

$$
G ^ {T} (U _ {1}) \subset U _ {1} \cap G ^ {T} (U) = N _ {1} \backslash N _ {0} \subset \overline {{N _ {1} \backslash N _ {0}}}.
$$

(2) Similarly, we verify that $( N _ { 2 } , N _ { 1 } )$ is an index pair for $U _ { 2 }$ . Since $\overline { { { N _ { 2 } \backslash N _ { 1 } } } } ~ = ~ \overline { { { G ^ { T } ( U ) \backslash U _ { 1 } } } } ~ = ~ G ^ { T } ( U ) \cap U _ { 2 }$ , the verification of $G ^ { \prime } ( N _ { 2 } \backslash N _ { 1 } ) ~ \subset$ $\operatorname { i n t } ( N _ { 2 } \backslash N _ { 1 } )$ follows similarly to that of $( N _ { 1 } , N _ { 0 } )$ . Moreover

$$
G ^ {T} (U _ {2}) \subset G ^ {T} (U) \cap U _ {2} = \overline {{N _ {2} \backslash N _ {1}}}.
$$

Other verifications are omitted.

Definition 5.5.29 Let $U \in \Sigma$ , a family of subsets of $U$ , $\{ U _ { 1 } , \ldots , U _ { n } \}$ is called a Morse decomposition of $U$ , if

(1) $U _ { i } \in \Sigma , ~ i = 1 , \dots , n .$   
(2) $U = \bigcup _ { 1 < i < \infty } U _ { i }$ ∪1 i n   
(3) $\operatorname { i n t } ( U _ { i } ) \cap \operatorname { i n t } ( U _ { j } ) = \emptyset$ as $i \neq j$ ,   
(4) $U _ { i + 1 } > \bigcup _ { 1 \leq j \leq i } U _ { j }$ , for $i = 1 , 2 , \ldots , n - 1$

By observing the exactness of sequences

$$
\rightarrow \overline {{H}} ^ {q} (N _ {2}, N _ {1}; G) \rightarrow \overline {{H}} ^ {q} (N _ {2}, N _ {0}; G) \rightarrow \overline {{H}} ^ {q} (N _ {1}, N _ {0}; G) \rightarrow \ldots .
$$

and the proof of (5.3), we arrive at:

Theorem 5.5.30 If $\{ U _ { 1 } , \ldots , U _ { n } \}$ is a Morse decomposition of $U \in \Sigma$ , then there is a formal series $Q$ with nonnegative coefficients such that

$$
\sum_ {j = 1} ^ {n} h (t; U _ {j}) = h (t; U) + (1 + t) Q (t).
$$

# Notes

# Chapter 1

Section 1.1 contains the basic differential calculus on Banach spaces; the material can be found in any nonlinear functional analysis book, for instance, Schwartz [Scw], Nirenberg [Ni 1], Deimling [De], Zeidler [Zei], Berger [Ber 1] etc. The discussion on Nemytscki operator can be found in Vainberg [Va], but the proof is much simpler than that in the reference.

The presentations of the implicit function theorem and the inverse function theorem are standard. Interesting applications are scattered in the literature, for instance, Nirenberg [Ni 4], Kazdan [Ka], Chow, Hale [CH], Mawhin [Maw 3] etc. The continuity method is extensively used in the existence proof of differential equations. The global implicit function theorem is due to Hadamard [Ha], Caccioppoli [Cac 1]; extensions can be found in Browder [Bd 1] and Plastock [Pl]. However, the proof presented here is very different. The application of the continuity method to an a priori bound as a simpler proof for semi-linear elliptic equations with quadratic growth was given by Amann and Crandall [AC]; the result for quasi-linear elliptic equations can be found in Ladyzenskaya and Ural’zeva [LU].

The Lyapunov–Schmidt reduction is extensively used in nonlinear problems. The application to the study of bifurcation problems introduced here is due to Crandall and Rabinowitz [CR 1], [CR 2]. The references on bifurcation theory are recommended to Chow and Hale [CH].

Sections 1.3.3–1.3.4 provide examples on gluing, which is an important technique in symplectic geometry. The material here is taken from Floer and Weinstein [FW] and Oh [Oh]. Further references are Floer [Fl 1] [Fl 2], Hofer and Zehnder [HZ 2]. Parallel to the implicit function theorem method, a gluing technique via the variational method was developed by Sere [Se]; it has been applied to the homoclinic orbits in Hamiltonian systems, multi-bump solutions, and multi-peak solutions for elliptic differential equations, see also Coti Zelati and Rabinowitz [CR], Li [Li 1], Gui [Gu] etc. Section 1.3.5 is another important technique in applying the implicit function theorem. The

notion of transversality is taken from differential geometry. Combining with the Sard theorem, it provides a method for proving various generic type results. The finite-dimensional form of the transversality theorem can be found in Guillemin and Podollak [GP]. The Sard–Smale theorem is taken from Smale [Sm 3]. The simplicity of eigenvalues of the Laplacian on generic domains is due to Uhlenbeck [Uh].

Section 1.4 is on the Nash–Moser technique. See Nash [Na 2] and Moser [Mos 1] [Mos 2]. KAM theory is due to Kolmogorov [Ko], Arnold [Ar 2] and Moser [Mo 3]. The presentation here can be found in Hormander [Hor 2]. Applications in differential geometry can be found in Hamilton [Ham]; a version setting up on Frechet spaces is given therein. Other versions can be found in Nirenberg [Ni 1], see also Zehnder [Ze]. For further reading on recent developments of KAM theory to partial differential equations, Bourgain [Bou], Kuksin [Kuk], Wayne [Way], and Poschel [Po] are recommended.

# Chapter 2

The order method is very different from other methods in this book. In concrete problems, once the assumptions are met, the method is simple and powerful. Our discussions start with the Bourbaki–Kneser principle [Bo], [Kn], see also Tarski [Ta]. The Amann theorem [Am 1] is a version that is easy to apply. The sub- and super-solutions method is extensively applied in ODE and PDE whenever the maximum principle is applicable. However, the constructions of sub- and super-solutions require special knowledge and techniques. We are satisfied introducing the method by an example. The Caristi fixed-point theorem [Ca] is among a few fixed-point theorems without assuming the continuity of the nonlinear mappings, applications can be found in [Lie]. Also, an equivalent version of this theorem is the very important Ekeland variational principle [Ek 1]; various applications in different branches of analysis can be found in [Ek 1] and de Figueiredo [dF].

There are lots of books on convex analysis; in Sect. 2.2, we only present very briefly the necessary material for the sequel discussions. References can be found in Ekeland and Temam [ET], Aubin and Ekeland [AE] etc.

Fixed points for nonexpansive mappings have been studied by Browder [Bd 2], Goebel [GK], etc. The importance of this class of mappings is that one of the fixed points can be figured out by iteration methods. Many algorithms for finding feasible solutions for convex programming have been studied in recent years, see [B], [BB].

There are many ways to introduce the Schauder fixed-point theorem and related topics. We use the KKM theorem [KKM] and Ky Fan’s inequality [FK 3] as the starting point. The Nash equilibrium [Na 3], the Von Neumann– Sion minimax theorem [VN] [Si], the Schauder fixed-point theorem [Sc], the Schauder–Techonoff theorem, the Ky Fan–Glicksberg theorem [FK 1], [FK 2], and the existence result of Hartman and Stampacchia on variational inequalitiy [HS] are direct consequences. The variational inequality [LS], [Bd 3], [Stm]

is another direction in convex analysis with many applications in free boundary problems from mathematical physics, see Duvaut and Lions [DL], Kinderlehrer and Stampacchia [KS], Friedman [Fr] etc. However, the approach based on the fixed points of set-valued mappings is due to the author, see [Ch 1], [Ch 2].

The theory of monotone operators and pseudo-monotone operators attracted much attention in the 1960s and 70s. The works of Minty [Min], Browder [Bd 4], Hartman and Stampacchia [HS], H. Brezis [Br 1], etc. constitute the basic content of the theory. Again we use the version of the Ky Fan inequality due to Brezis, Nirenberg and Stampacchia [BNS] to derive the most important results on this topics. For applications to quasi-linear elliptic equations see Leray and Lions [LL], to boundary value problems in nonlinear partial differential equations see Lions [LJ 1], and to nonlinear semigroups of operators see Brezis [Br 1], Crandall and Ligget [CLi].

# Chapter 3

The Brouwer degree is a topological invariant; roughly speaking, there are two approaches: algebraic (see for instance Spanier [Sp], Greenberg [GH]) and differential (Milnor [Mi 2]). The Leray–Schauder degree is its extension to compact vector fields [LS] on Banach space.

The analytic presentation can be found in many books and lecture notes, for instance: J. Schwartz [Scw], Nirenberg [Ni 1], Rabinowitz [Ra 2], Guillemin and Podollak [GP], Zeidler [Zei] etc. The materials of Sects. 3.1–3.4 are taken from these references. An application of the transversality theorem to the proof for the Borsuk–Ulam theorem and to the computation of $S ^ { 1 }$ -invariant degree are due to Nirenberg [Ni 3].

Sections 3.5 and 3.6.3 are adapted from Rabinowitz [Ra 1], [Ra 2]. Section 3.6.2 is based on Dancer [Dan 1]. The material of Sects. 3.6.4 and 3.6.5 is taken from Amann [Am 2], Krasnosel’ski [Kr 2], de Figueiredo, Lions and Nussbaum [FLN], Chang [Ch 3], and Dancer [Dan 2]. The blowing up method in a priori estimates is a useful technique, see for instance, Gidas and Spruck [GS].

Section 7 contains various extensions of the Leray–Schauder degree; for $\alpha$ -set contraction mappings, see Darbo [Dar], Stuart and Toland [ST]; for condensing mappings, see Nussbaum [Nu], Sadovskii [Sa], set-valued mappings see Browder [Bd 5], Cellina [Ce], Ma [Ma], Chang [Ch 2]. There are other directions: Fredholm operators, see Elworthy and Tromba [ElT 1] [ElT 2], Nirenberg [Ni 4], Llyord [Ll], Fitzpatrick, Pejsachowicz and Rabier [FP], [FPR], [PR 1, PR 2]; coincidence degree theory, Mawhin [Maw 1], etc.

# Chapter 4

Section 4.1 is an introduction to the calculus of variations. The derivations of the Euler–Lagrange equation, the Legendre–Hadamard condition and the Ljusternik theorem on constraint variational problems can be found in any

standard textbook. For the dual variational principle, i.e., the Legendre– Fenchel transformation, see for instance, Arnold [Ar 1], Ekeland and Temam [ET] or Aubin and Ekeland [AE]. For the Hamiltonian systems, the second version of the dual variational principle, i.e., the Legendre transform to the Hamiltonian function of all variables if the latter is strictly convex, see Clarke and Ekeland [CE].

In Sect. 4.2, the direct method is a general principle in the calculus of variations. We introduce a few interesting examples showing how the principle works.

Harmonic maps were introduced by Eells and Sampson [ES] in representing homotopy classes of mappings between two manifolds. Here we only touch on the existence of a weak solution. For $m = 2$ the harmonic map with minimal energy is smooth, see Morrey [Mo 2]; for $m > 2$ , Schoen and Uhlenbeck [ScU 1], [ScU 2] proved that the singularity has at most a codimension 3 finite Hausforff measure. A nonsmooth minimal energy harmonic map was given by Lin [Lin]. As to nonminimal energy harmonic maps, the smoothness for $m = 2$ was proved by Helein [Hel 1], and the existence of a nowhere continuous harmonic for $m = 3$ constructed by Riviere [Ri].

The example on constant mean curvature surface is taken from Hildebrandt [Hi 1]; a systematic introduction can be found in Struwe [St 2].

The prescribing scalar curvature problem can be found in Kazdan [Ka], Kazdan and Warner [KW 1], [KW 2]. The result for $\chi ( M ) = 0$ is due to Berger [Ber 2], that for real projective space $P ^ { 2 }$ is due to Moser [Mos 4]; however, we present an easy proof with the aid of Aubin’s inequality [Au 3].

Section 4.3 is from Morrey [Mo 1] [Mo 2], Dacorogna [Dac], Acerbi and Fusco [AF], Marcellini [Mar], Muller [Mul 1] [Mul 2] Sverak [Sv 1] [Sv 2], Zhang [Zh 1] and Ball [Bal 1].

The relaxation method is from Buttazzo [Bu] and Dacorogna [Dac]. For the two-well problem, Zhang [Zh 2] has obtained an explicit expression of the quasi-convex envelope for the square distance function.

The Young measure was introduced by Young [Yo]; the presentation here is due to Ball [Bal 2], see also [Mul 1], Kinderlehrer and Pedregal [KP 1, KP 2]. In Chipot [Chi], there are examples on the computation of Young measures.

There are few books in dealing with the BV space, see for instance, Giusti [Gi], Evans and Gariepy [EG], Zimer [Zi], Ambrosio [Amb] etc. Section 4.5.1 is adapted from these books.

The Hardy space and BMO space are important parts in harmonic analysis, see Stein [Ste 1, Ste 2] and Stein–Weiss [SW]. They are applied to PDEs in compensated compactness (as seen in Sect. 4.5.2) and in regularity (for harmonic maps see Helein [Hel 2] Evans [Ev 2]. For the background material we recommend Brezis and Nirenberg [BN 3], and Semmes [Se] as references. The connection between Hardy space and the compensated compactness is revealed by Coifman, Lions, Meyer and Semmes [CLMS]. In the applications to the calculus of variations, the bitting theorem due to Chacon is required, see Brooks and Chacon [BC]. Compensated compactness is also an important

tool in nonlinear analysis; we refer the reader to Tartar [Tar 1, Tar 2], DiPerna [Di] and Evans [Ev 2].

We briefly introduce the $\Gamma$ -convergence in Sect. 4.6 by a result of Modica and Mortola [MM], see also Alberti [Al]. Readers who want to know more can read Dal Maso [D].

The Munford–Shah model [MS] in the segmentation of the image processing is an interesting subject; the existence proof was given by Ambrisio [Amb], De Giorgi and [DG 2], and De Giorgi and Ambrosio [GA]. More details can be found in Ambrosio Fusco and Pallara [AFP], Dal Maso, Morel and Solimini [DMS] and Morel and Solimini [MS]. As to the regularity of the edge curve, see Morel [Mor]. However, all of these results require deep knowledge on BV functions. A much simplified model presented here is from Nordstr¨om [No].

Concentration compactness [LP] in combination with blowing up analysis [SU] is one of the most important techniques in nonlinear problems without compactness. Since interesting concrete problems require more background knowledge, they are out of the scope of this book. We present in Sect. 4.7 just an introduction of the idea: The concentration phenomenon and the role of the bubble (or best constant). Two examples on semilinear elliptic equations are studied: The subcritical exponent on $R ^ { n }$ , see Coti Zelati [CZ], and the critical exponent on bounded domain, see Brezis and Nirenberg [BN 1].

Section 4.8 is the preliminary of the minimax method. The Palais–Smale condition [PS] and the mountain pass lemma due to Ambrosetti and Rabinowitz [AR] are introduced in accordance with the Ekeland minimizing principle. The proof was given by Shi [Shi] and Aubin and Ekeland [AE]. Examples are taken from Mawhin [Maw 2] and Ambrosetti and Rabinowitz [AR].

# Chapter 5

Section 5.1: The most recommended book in introducing Morse theory on compact manifolds is Milnor [Mi 1]. The classics are Morse [Mo] and Morse and Cairns [MC]. The extension to infinite-dimensional space can be found in Chang [Ch 4], [Ch 5]. See also Mawhin and Willem [MW].

The background material in this section can be found in Palais [Pa 1], Palais and Smale [PS], Rothe [Ro 1], [Ro 2], [Ro 3], Marino and Prodi [MP 1], [MP 2], Chang [Ch 7], [Ch 8], Wang [Wa 1], [Wa 2], Castro and Lazer [CaL].

The critical point theory has been extended to non-differentiable functionals, see Chang [Ch 6] for locally Lipschitzian functionals on Banach space, and Kartiel [Ka], Ioffe, Schwartzmann [IS], and Corvellec, De Giovanni, Marzocchi [CGM] for continuous functions on metric spaces.

Section 5.2: There are several books on minimax methods, see for instance Rabinowitz [Ra 4], Struwe [St 1], Ghoussoub [GH], and Willem [Wi] etc.

The background material can be found in Rabinowitz [Ra 4], [Ra 5], Palais [Pa 2], [Pa 3], Ni [Ni], Chang [Ch 5], Tian [Ti], Liu [Liu 1], [Liu 2], Viterbo [Vi 2], Solimini [So], Chang, Long and Zehnder [CYZ], Fournier and Willem [FoW].

The notion of category was discovered by Ljusternik and Schnirelmenn [LjS 1]. The genus was introduced by Krasnosel’ski in [Kr 1], while for general index theory see Fadell and Rabinowitz [Fr]. The geometric index for $S ^ { 1 }$ is taken from Benci [Be 1] and Nirenberg [Ni 3]; other extensions of the index theory can be found in Benci and Rabinowitz [BR 2].

Section 5.3: There are many papers on periodic solutions for Hamiltonian systems via variational methods. The results related to this section are Rabinowitz [Ra 3], Weinstein [We 1], [We 2], Viterbo [Vi 1], Hofer and Zehnder [HZ 2], Struwe [St 1]. Further development on the Weinstein conjecture can be found in Hofer and Viterbo [HV], Hofer [Ho 2] and Liu and Tian [LT 1].

The following books are recommended: Hofer and Zehnder [HZ 1], Ekeland [Ek 2], Mawhin and Willem [MW].

Other important topics on the periodic solutions include:

For Arnold conjecture on the symplectic fixed points and Lagrangian intersections, see Conley and Zehnder [CZ 1], Floer [Fl 1], [Fl 2], [Fl 3], Liu and Tian, [LT 2], Fukaya Ono [FO].   
• The number of the periodic orbits on compact convex hypersurface has been estimated by Long and Zhu [LZ].   
• For the $N .$ -body problem, see Bahri and Rabinowitz [Br].

Section 5.4: The main result of this section is taken from Chang and Yang [CY 1], [CY 2], and Chang and Liu [CL 2]. See also Han [Han], Chen and Li [ChL 1], [ChL 2]. The related prescribing scalar curvature problem on $S ^ { n }$ has received extensive attention. For $n = 3$ see Bahri and Coron [BC], and for high dimension, see Schoen and Zhang [SZ], Bahri [Bar 1], [Bar 2], Li [Li 1], and Chen and Lin [ChL].

Section 5.5: Conley’s index theory was introduced by Conley [Co], in which isolating neighborhoods for isolated invariant sets are compact. The theory is completed by Conley and Zehnder [CZ 2], Salamon [Sal], Salamon and Zehnder [SZ]. There are many ways to extend the theory to infinite-dimensional spaces, see Rybakowski [Ry], Rybakowski and Zehnder [RZ], Benci [Be 2] etc. The relationship between the Conley theory and the Morse theory can be found in Chang and Ghoussoub [CG]. For further reading see Michaikov [Mic] and Smoller [Smo].

# References

[AF] Acerbi E., Fusco N., Semicontinuity problems in the calculus of variations, Arch. Rat. Mech. Anal. 86 (1984), 125–145.   
[Ad] Adams R. A., Sobolev spaces, Acad. Press (1975).   
[Al] Alberti G., Variational models for phase transition, an approach via $\Gamma$ - convergence calculus of variations and partial differential equations. Topics on geometrical evolution problems and degree theory (ed. by Ambrossio, Dancer), Springer-Verlag, (2000) 95–114.   
[Am 1] Amann H., Order structures and fixed points, Ruhr-Universit¨at, Bochum (1977).   
[Am 2] Amann H., Fixed point equations and elliptic eigenvalue problems in ordered Banach spaces, SIAM Rev. 18 (1976), 620–709.   
[Am 3] Amann H., On the number of solutions of nonlinear equations in ordered Banach spaces, J. Funct. Anal. 14 (1973), 346–384.   
[Am 4] Amann H., Saddle points and multiple solutions of differential equations, Math. Zeit. 169 (1979), 122–166.   
[AC] Amann H., Crandall M., On some existence theorems for semi-linear elliptic equations, Indiana Univ. Math. J. 27 (1978), 779–790.   
[AR] Ambrosetti A., Rabinowitz P., Dual variational methods in the critical point theory and applications, J. Funct. Anal. 14 (1973), 349–381.   
[Amb] Ambrosio L., Variational problems in SBV and image segmentation, Acta Appl. Math. 17 (1989), 1–40.   
[AFP] Ambrosio L., Fusco N., Pallara D., Functions of bounded variation and free discontinuity problem, Oxford Sci Publ. (2000).   
[Ar 1] Arnold V. I., Mathematical methods of classical mechanics, Springer-Verlag (1978).   
[Ar 2] Arnold V. I., Proof of a theorem of A. N. Kolmogorov on the conservation of quasi-periodic motions under a small change of the Hamiltonian function, Uspekhi Mat. Nauk 18:5 (1963), 9–36.   
[AE] Aubin J. P., Ekeland I., Applied nonlinear analysis, John Wiley and Sons (1984).   
[Au 1] Aubin Th., Nonlinear analysis on manifolds, Monge Ampere equations, Grundlehren 252 (1982), Springer-Verlag.   
[Au 2] Aubin Th., Equations differentielles nonlin´eaires et probl´eme de Yamabe concernant la courbure scalaire, J. Math. Pure Appl. 55 (1976), 269–293.

[Au 3] Aubin Th., Meilleures constants dans le th´eor`eme d’inclusion de Sobolev et un th´eor`eme de Fredholm nonlin´eaire pour la transformation conforme de la courbure scalaire, J. Funct. Anal. 32 (1979), 148–174.   
[Ba 1] Bahri A., Critical points at infinity in some variational problems, Pitman Research Notes in Math. V. 182, Longman (1989).   
[Ba 2] Bahri A., The scalar curvature problem on sphere of dimension larger or equal to 7, Preprint (1994).   
[BC] Bahri A., Coron J. M., The scalar curvature problem on the standard three dimensional sphere, J. Funct. Anal. 95 (1991), 106–172.   
[Bal 1] Ball J., Convexity conditions and existence theorems in nonlinear elasticity, Arch. Rat. Mech. Anal. 63 (1977), 337–403.   
[Bal 2] Ball J., A version of fundamental theorem for Young measures, PDE’s and continuum models of phase transitions (Rascle M. Serre D. Slemrod M. eds.), Lecture Notes in Physics, 344, Springer-Verlag (1989), 207–215.   
[Bm] Ballmann W., Der Satz von Lyusternik und Schnirelmann, Bonn. Math. Schr. Nr. 102 (1978).   
[B] Bauschke H. H., The approximation of fixed points of compositions of nonexpansive mappings in Hilbert space, JMAA 202 (1996) 150–159.   
[BB] Bauschke H. H., Borwein J. M., On projection algorithms for solving convex feasible problems, SIAM Revi. 38 (1996), 367–426.   
[Be 1] Benci V., A geometric index for the group $S ^ { 1 }$ and some applications to the study of periodic solutions of ordinary differential equations. Comm. Pure Appl. Math. 274 (1981) 393–432.   
[Be 2] Benci V., A new approach to the Morse Conley theory and some applications. Ann. Mat. Pura Appl. (IV) 158 (1991), 231–305.   
[BR] Benci V., Rabinowitz P., Critical point theorems for indefinite functionals, Invent. Math. 52 (1979), 241–273.   
[Ber 1] Berger M. S., Nonlinearity and functional analysis, Acad. Press (1977).   
[Ber 2] Berger M. S., On Riemannian structures of prescribing Gaussian curvature for compact 2-manifolds, J. Diff. Geom. 5 (1971), 325–332.   
[Bi] Birkhoff G. D., Dynamical systems with two degrees of freedom, Trans. AMS 18 (1917), 199–300.   
[BZS] Borisovich Y. G., Zvyagin V. G., Sapronov Y. I., Nonlinear Fredholm maps and the Leray Schauder degree theory, Russian Math. Surveys 324 (1977) 1–54.   
[Bo 1] Bott R., Nondegenerate critical manifolds, Ann. Math. 60 (1954), 248–261.   
[Bo 2] Bott R., Lectures on Morse theory, old and new, Bull. AMS 7 (1982), 331–358.   
[BT] Bott R., Tu L. W., Differential forms in algebraic topology, Springer-Verlag (1982).   
[Bo] Bourbaki N., Topologie g´en´erale, Hermann, Paris (1940).   
[Bou] Bourgain J., Global solutions of nonlinear Schrodinger equations, AMS Colloquium Publ. Vol. 46 (1999).   
[Br 1] Brezis H., Op´erateurs maximaux monotones, Lecture Notes, Vol. 5, North Holland Amsterdam (1973).   
[BrC] Brezis H., Coron J. M., Multiple solutions of H-systems and Rellich’s conjecture, Comm. Pure Appl. Math. 37 (1984), 149–187.   
[BL] Brezis H., Lieb E. H., A relation between pointwise convergence of functions and convergence of functionals, Proc. AMS 88 (1983), 486–490.

[BN 1] Brezis H., Nirenberg L., Positive solutions of nonlinear elliptic equations involving critical Sobolev exponents, Comm. Pure Appl. Math. 36 (1983), 437–477.   
[BN 2] Brezis H., Nirenberg L., Remarks on finding critical points, Comm. Pure Appl. Math. 64 (1991), 939–963.   
[BN 3] Brezis H., Nirenberg L., Degree theory and BMO, Selecta Math. 1 (1995), 197–263.   
[BNS] Brezis H., Nirenberg L., Stampacchia G., A remark on Ky Fan’s minimax principle, Boll. Un. Mat. Ital. (4) 6, (1973), 293–300.   
[BC] Brooks J. K., Chacon R. V., Continuity and compactness of measures, Adv. Math. 37 (1980), 16–26.   
[Br] Brouwer L. E. J., Uber Abbildung von Mannigfaltikeiten, Math. Ann. 71¨ (1912), 97–115.   
[Bd 1] Browder F., Covering spaces, fibre spaces and local homeomorphisms, Duke Math. J. 21 (1974), 329–336.   
[Bd 2] Browder F., Non-expansive nonlinear operators in a Banach space, Proc. Nat. Acad. Sci. USA 54 (1965), 1041–1044.   
[Bd 3] Browder F., On the unification of the calculus of variations and the theory of monotone nonlinear operators in Banach space, Proc. Nat. Acad. USA 56 (1966), 419–425.   
[Bd 4] Browder F., Nonlinear elliptic boundary value problems, Bull. AMS 69 (1963), 862–874.   
[Bd 5] Browder F., The fixed point theory of multi-valued mappings in topological vector spaces, Math. Ann. 117 (1968), 283–301.   
[Bu] Buttazzo G., Semicontinuity, relaxation and integral representation in the calculus of variations, Pitman Res. Notes Math. Ser., 207, Longman (1989).   
[Cac 1] Caccioppoli R., Sugli elementi uniti delle transformazioni funzionali, Rend. Sem. Mat. Padova 3 (1932), 1–15.   
[Cac 2] Caccioppoli R., Sulle corrispondenze funzionali inverse diramate, teoria generale e applicazioni ad alcune equazioni funzionali nonlineari al problema di Plateau, I, II, Rend. Acad. Naz. Lincei (6) 24 (1936), 258–263, 416–421.   
[CGS] Caffarelli L., Gidas B., Spruck J., Asymptotic symmetry and local behavior of semilinear elliptic equations with critical Sobolev growth, Comm. Pure Appl. Math. 42 (1989), 271–297.   
[Ca] Caristi J., Fixed point theorems for mappings satisfying inwardness conditions, Tran. AMS 215 (1976), 241–251.   
[CC] Carleson L., Chang A. S. Y., On the existence of an extremal function for an inequality of J. Moser, Bull. Sc. Math. 2 110 (1986), 113–127.   
[CaL] Castro A., Lazer A. C., Critical point theory and the number of solutions of a nonlinear Dirichlet problem, Ann. Pura Appl. 70 (1979), 113–137.   
[Ce] Cellina A., Approximation of set-valued functions and fixed point theorems, Ann. Pura Appl. Mat. 4 (1969), 17–24.   
[CY 1] Chang A. S. Y., Yang P., Prescribing Gaussian curvature on $S ^ { 2 }$ , Acta Math. 159 (1987), 215–259.   
[CY 2] Chang A. S. Y., Yang P., Conformal deformation of metric on $S ^ { 2 }$ , J. Diff. Geom. 23 (1988), 259–296.   
[Ch 1] Chang K. C., The obstacle problem and partial differential equations with discontinuous nonlinear terms, Comm. Pure Appl. Math., 33 (1980), 117– 146.

[Ch 2] Chang K. C., Free boundary problems and set valued mappings, J. Diff. Eq. 49 (1981), 1–28.   
[Ch 3] Chang K. C., Remarks on some free boundary problems for equilibrium equation of plasmas, Comm. PDE 5 (1980), 741–751   
[Ch 4] Chang K. C., Infinite dimensional Morse theory and its applications, les presses de l’ univ. de Montreal SMS 97 (1985).   
[Ch 5] Chang K. C., Infinite dimensional Morse theory and multiple solution problems, Birkhauser (1993).   
[Ch 6] Chang K. C., Variational methods for non-differentiable functionals and their applications to PDE, J. Math. Anal. Appl. 80 (1981), 102–129.   
[Ch 7] Chang K. C., Solutions of asymptotic linear operator equations via Morse theory, Comm. Pure Appl. Math. 34 (1981), 693–712.   
[Ch 8] Chang K. C., $H ^ { 1 }$ versus $C ^ { 1 }$ isolated critical points, C. R. Acad. Sci. Paris 319 (1994), 441–446.   
[CG] Chang K. C., Ghoussoub N., The Conley index and the critical groups via an extension of Gromoll-Meyer theory, Topol. Methods in Nonlinear Analysis 7 (1996), 77–93.   
[CL 1] Chang K. C., Liu J. Q., Morse theory under general boundary conditions, J. System Sci. and Math. Sci. 4 (1991), 78–83.   
[CL 2] Chang K. C., Liu J. Q., On Nirenberg’s problem, International J. Math. 4 (1993) 35–58.   
[CYZ] Chang K. C., Long Y., Zehnder E., Forced oscillations for the triple pendulum, Analysis et cetera (P. Rabinowitz, E. Zehnder eds.) Academic Press, (1990).   
[ChL] Chen C. C., Lin C. S., Blowing up with infinite energy of conformal metric on $S ^ { n }$ , Comm. PDE 24 (1999), 785–799.   
[CD] Chen W. X., Ding W. Y., Scalar curvatures on $S ^ { 2 }$ . Tran. Amer. Math. Soc. 303 (1987), 365–382.   
[ChL 1] Chen W., Li C., A priori estimates for prescribing scalar curvature equations, Ann. Math. 145 (1997), 547–564.   
[ChL 2] Chen W., Li C., A necessary and sufficient condition for the Nirenberg problem, Comm. Pure Appl. Math. 48 (1995), 657–667.   
[Chi] Chipot M., Elements in nonlinear analysis, Birkhauser, (2000).   
[CH] Chow S. N., Hale J., Methods of bifurcation theory, Springer-Verlag, (1982).   
[Cl] Clarke F. H., Optimization and nonsmooth analysis, Wiley Interscience (1983).   
[CE] Clarke F. H., Ekeland I., Hamiltonian trajectories having prescribed minimal period, Comm. Pure Appl. Math. 33 (1980), 103–116.   
[CLMS] Coifman R., Lions, P. L., Meyer Y., Semmes S., Compensated compactness and Hardy spaces, J. Math. Pure Appl. (9), 72 (1993), 247–286.   
[Co] Conley, C., Isolated invariant sets and the Morse index. CBMS 38 (1978), AMS.   
[CZ 1] Conley C., Zehnder E., The Birkhoff-Lewis fixed point theorem and a conjecture of V. I. Arnold, Inv. Math. 73 (1983) 33–49.   
[CZ 2] Conley C., Zehnder E., A Morse type index theory for flows and periodic solutions to Hamiltonian systems, Comm. Pure Appl. Math. 37 (1984), 207–253.

[CGM] Corvellec J. N., De Giovanni M., Marzocchi M., Deformation properties for continuous functions and critical point theory, preprint Univ. di Pisa (1992).   
[CZ] Coti Zelati V., Critical point theory and applications to elliptic equations in $R ^ { n }$ , Nonlinear Functional Analysis and Appl. to Diff. Eqs. (A. Ambrosetti, K. C. Chang, I. Ekeland eds.) World Sci. (1998), 102–121.   
[CR] Coti Zelati V., Rabinowitz P., Homoclinic type solutions for a semilinear elliptic partial differential equation on $R ^ { n }$ , Comm. Pure Appl. Math. 45, (1992), 1217–1269.   
[CLi] Crandall M., Ligget T., Generations of semigroups of nonlinear transformations on general Banach spaces, Amer. J. Math. 93 (1971), 265–298.   
[CR 1] Crandall M., Rabinowitz P., Bifurcation, perturbations of simple eigenvalue and linearized stability, Arch. Rat. Mech. Anal. 52 (1971), 161–180.   
[CR 2] Crandall M., Rabinowitz P., The Hopf bifurcation theorem, Arch. Rat. Math. Anal. 67 (1977), 53–72.   
[Dac] Dacorogna B., Direct methods in the calculus of variations, Springer-Verlag (1989).   
[DM] Dacorogna B., Marcellini, P. A counter example in the vectorial calculus of variations, Material Instabilities in Continuum Mechanics, Proc. (ed. J. Ball), Oxford Sci. Publ. (1988), 77–83.   
[D] Dal Maso G., An introduction to $\Gamma$ -convergence, Birkhauser (1993).   
[DMS] Dal Maso G., Morel J. M., Solimini S., A variational method in image segmentation: existence and approximation results, Acta Math. 168 (1992), 89–151.   
[Dan 1] Dancer N., Global solution branches for positive maps, Arch. Rat. Math. Anal. 55 (1974), 207–213.   
[Dan 2] Dancer N., The effect of domain shape on the number of positive solutions of certain nonlinear equations, J. Diff. Equs. 87 (1990), 316–339.   
[Dar] Darbo G., Punti uniti in transformazioni a codominio non compatto, Rend. Sem. Univ. Padua 24 (1955), 84–92.   
[dF] de Figueiredo D. G., Lectures on Ekeland variational principle with applications and detours, Lect. Notes, College on variational problems in analysis, ICTP Trieste (1988).   
[FLN] de Figueiredo D. G., Lions P. L., Nussbaum R., J. A priori estimates and existence of positive solutions of semilinear elliptic equations, Math. Pures et Appl. 61 (1982), 41–63.   
[DG 1] De Giorgi E., Frontiere orientate di misura minima, Sem. Mat. Scuola Norm. Sup. Pisa (1961).   
[DG 2] De Giorgi E., Free discontinuity problems in the calculus of variations, Frontiers in pure and applied Mathematics, a collection of papers dedicated to J. L. Lions (Dautray R., ed.) North Holland (1991), 55–62.   
[DA] De Giorgi E., Ambrosio L., Un nuovo funzionale del calcolo delle variazioni, Atti Accad. Naz. Lincei Cl. Sci. Fis. Mat. Natur. Rend. Lincei (8), Mat. Appl. 82 (1988), 199–210.   
[De] Deimling K., Nonlinear functional analysis, Springer-Verlag (1985).   
[DT] DeTurck D., Existence of metrics with prescribing Ricci curvature: local theory, Invent. Math. 65 (1981), 179–208.   
[DiT] Ding W. Y., Tian G., Energy identity for a class of approximate harmonic maps from surfaces, Comm Anal. Geom. 3 (1995), 543–544.

[Di] DiPerna R. J., Compensated compactness and general systems of conservation laws, Trans. AMS 292 (1985), 383–420.   
[DM] DiPerna R. J., Majda A. J. Oscillations and concentration in weak solutions of the incompressible fluid equations, Comm. Math. Phys. 108, (1987), 667–689.   
[Du 1] Dugundji J., An extension of Tietze’s theorem, Pac. J. Math. 1 (1951), 353–367.   
[Du 2] Dugundji J., Topology, Allyn and Bacon (1966).   
[DG] Dugundji J., Granas A., Fixed point theory, Monografie Matematyczne 16, Polish Sci. Publ. (1982).   
[DS] Dunford N., Schwartz, J. T., Linear operators, Vol. I., Interscience (1958).   
[DL] Duvaut G., Lions J. L., Les in´equations en m´ecanique et en physique, Dunod (1972).   
[ES] Eells J., Sampson J. H., Harmonic mappings of Riemannian manifolds, J. AMS 86 (1964), 109–160.   
[Ek 1] Ekeland I., Nonconvex minimization problems, Bull. AMS 1 (1979), 443– 474.   
[Ek 2] Ekeland I., Convexity methods in Hamiltonian mechanics, Ergebnisse d. Math. (Ser. III) 19, Springer-Verlag (1990).   
[ET] Ekeland I., Temam R., Convex analysis and variational problems, North Holland (1976).   
[ElT 1] Elworthy K. D., Tromba A. J., Degree theory on Banach manifolds, Nonlinear Functional Analysis (F. Browder ed.) Proc. Symp. Pure Math. Part I 18 (1970), 86–94.   
[ElT 2] Elworthy K. D., Tromba A. J., Differential structures and Fredholm maps on Banach manifolds, Global analysis (S. S. Chern and S. Smale eds.) Proc. Symp. Pure Math. 15 (1970), 45–94.   
[Ev 1] Evans L. C., Weak convergence methods for nonlinear partial differential equations, AMS, Providence (1990).   
[Ev 2] Evans L. C., Quasi-convexity and partial regularity in the calculus of variations, Arch. Rat. Mech. Anal. 95 (1986), 227–252.   
[EG] Evans L. C., Gariepy R., Measure theory and fine properties of functions, CRC Press (1992).   
[EM] Evans L. C., Muller S., Hardy spaces and the two dimensional Euler equations with nonnegative vorticity, J. AMS (1994), 199–219.   
[Fa] Faber C., Beweiss, dass unter allen homogenen Membrane von gleicher Flache und gleicher Spannung die Kriesformige die tiefsten Grundton gibt, Sitzungsber-Bayer Acad. Wiss. Math. Phys. Munich (1923), 169–172.   
[FR] Fadell E. R., Rabinowitz P., Generalized cohomological index theories for Lie group actions with applications to bifurcation questions for Hamiltonian systems, Invent. Math. 45 (1978), 139–174.   
[FK 1] Fan K., Fixed point and minimax theorems in locally convex topological spaces, Proc. Nat. Acad. Sci. USA 38 (1952), 121–126.   
[FK 2] Fan K,. A generalization of Tychnoff’s fixed point theorem, J. Math. Anal. Appl. 142 (1961), 305–310.   
[FK 3] Fan K., A minimax inequality and applications, Inequalities-III (Shisha O. ed.) Acad. Press (1972).   
[Fe] Federer H., Geometric measure theory, Springer-verlag (1969).   
[FP] Fitzpatrick P. M., Pejsachowicz J., Parity and generalized multiplicity, TAMS 326 (1991), 281–305.

[FPR] Fitzpatrick P. M., Pejsachowicz J., Rabier P. J., Orientability of Fredholm families and topological degree for orientable Fredholm mappings, J. Funct. Anal. 124 (1994), 1–39.   
[Fl 1] Floer A., Morse theory for fixed points of symplectic diffeomorphisms, Bull. AMS 16 (1987), 279–281.   
[Fl 2] Floer A., Witten complex and infinite dimensional Morse theory, J. Diff. Geom. 18 (1988), 207–221.   
[Fl 3] Floer A., Symplectic fixed points and holomorphic spheres, Comm. Math. Phys. 120 (1989), 576–611.   
[FW] Floer A., Weinstein A., Nonspreading wave packets for cubic Schrodinger equations with a bounded potential, J. Funct. Anal. 69 (1986), 397–408.   
[FoW] Fournier G., Willem M., Relative category and the calculus of variations, Preprint.   
[Fr] Friedman A., Variational principle and free boundary problems, John Wiley and Sons (1982).   
[FO] Fukaya K., Ono K., Arnold conjecture and Gromov-Witten invariants for general symplectic manifolds, The Arnoldfest, Fields Inst. Commun. 24, (173–190).   
[Gh] Ghoussoub N., Duality and perturbation method in critical point theory, Cambridge Univ. Press (1993).   
[GMS] Giaquinta M., Modica G., Soucek J., Cartesian currents in the calculus of variations, Part I, Cartesian currents, Part II, Variational integrals, Springer-Verlag (1998).   
[GNN] Gidas B., Ni W. M., Nirenberg L., Comm. Math. Phys. 68 (1979), 209.   
[GS] Gidas B., Spruck J., A priori bounds for positive solutions of nonlinear elliptic equations, Comm. PDE 6 (1981), 883–901.   
[GT] Gilbarg D., Trudinger N. S., Elliptic partial differential equations of second order, 2nd edition, Grundlehren der Mathematik, 224, Springer-Verlag (1983).   
[Gi] Giusti E., Minimal surfaces and functions of bounded variations, Monographs in Mathematics 80, Birkhauser (1984).   
[GK] Goebel K., Kirk W., Iteration process for nonexpansive mappings, Comtemp. Math. 21 (1983), 115–123.   
[GH] Greenberg M. J., Harper J. R., Algebraic topology, A first course, The Benjamin/Cummings Publ. Co. (1981).   
[GM] Gromoll D., Meyer W., On differential functions with isolated critical points, Topology 8 (1969), 361–369.   
[Gu] Gui C., Existence of multi-bump solutions for nonlinear Schrodinger equations via variational method, Comm. Part. Diff. Eq. 21 (1996), 787–820.   
[GP] Guillemin V., Pollack A., Differential topology, Prentice-Hall (1974).   
[Ha] Hadamard J., Sur les transformations ponctuelles, Bull. Soc. Math. France, 34, (1906), 7–84.   
[Ham] Hamilton, R., The inverse function theorem of Nash and Moser, Bull. AMS 7 (1982), 65–222.   
[Han] Han Z. C. Prescribing Gaussian curvature on $S ^ { 2 }$ , Duke Math. J. 61 (1990), 679–703.   
[Hann] Hanner, Some theorems on absolute neighborhood retracts, Arkiv Math. 1 (1951), 389–408.   
[HS] Hartman P., Stampacchia G., On some nonlinear elliptic differential equations, Acta Math. 115 (1966), 271–310.

[Hel 1] Helein F., R´egularit´e des applications faiblement harmoniques entre une surface et ue vari´et´e riemannienne, CR Acad. Sci. Paris 312 (1991), 591– 596.   
[Hel 2] Helein F., Constant mean curvature surfaces, harmonic maps and integrable systems, Birkhauser (2001).   
[Hi 1] Hildebrandt S., Nonlinear elliptic systems and harmonic mappings, Proc. Beijing Symp. Diff Geom. and Diff Eqs., Gordon and Breach (1983), 481– 615.   
[Hi 2] Hildebrandt S., Calculus of variations, I, II. Grudlehrer Mathematik Wiss. 310 311, Springer-Verlag (1996).   
[Ho 1] Hofer H., A note on the topological degree at a critical point of mountain pass type, Proc. AMS 90 (1984), 309–315.   
[Ho 2] Hofer H., Pseudo holomorphic curves in symplectisations with applications to the Weinstein conjecture in dimension three, Invent. Math. 114 (1993), 515–563.   
[HV] Hofer H., Viterbo C., The Weinstein conjecture in the presence of holomorphic curves, Comm. Pure Appl. Math. 45 (1992), 583–622.   
[HZ 1] Hofer H., Zehnder E., Periodic solutions on hyper-surfaces and a result by C. Viterbo, Invent. Math. 90 (1987), 1–9.   
[HZ 2] Hofer H., Zehnder E., Symplectic invariants and Hamiltonian dynamics, Birkhauser (1994).   
[Hon 1] Hong C. W., A best Constant and the Gaussian curvature, Proc. AMS 97 (1986), 737–747.   
[Hon 2] Hong C. W., A note on prescribed Gaussian curvature on $S ^ { n }$ , Partial Diff. Eqs, (in Chinese), 1 (1987), 13–20.   
[Hor 1] Hormander L., The analysis of linear partial differential operators, Vol. III, Grundlehren der Math. Wiss. 274, Springer-Verlag (1984).   
[Hor 2] Hormander L., The boundary value problems of physical geodesy, Arch. Rat. Math. Anal. 62 (1976), 1–52.   
[IS] Ioffe A., Schwartzmann E., Metric critical point theory 1, Morse regularity and homology stability of a minimum, J. Math. Pure Appl. 75 (1996), 125–153.   
[Iz] Ize J., Bifurcation theory for Fredholm operators, Memoirs, AMS, 174, Providence (1976).   
[JLS] Jeanjean H., Lucia M., Stuart C. A., Branches of solutions to semilinear elliptic equations on $R ^ { n }$ , Math. Z. 230 (1999), 79–105.   
[Kak] Kakutani S., A generalization of Brouwer’s fixed point theorem, Duke Math. J. 8 (1941), 457–459.   
[Kar] Kartiel G., Mountain pass theorems and global homeomorphism theorems, Analyse Nonlineaire 11 (1994), 189–209.   
[Ka] Kazdan J. L., Prescribing the curvature of a Riemannian manifold, CBMS no. 57 (1987).   
[KW 1] Kazdan J., Warner F., Curvature functions for compact 2-manifolds, Ann. of Math. 99 (1974), 14–47.   
[KW 2] Kazdan J., Warner F., Existence and conformal deformation of metrics with prescribing Gaussian and scalar curvatures, Ann. of Math. 101 (1975), 317–331.   
[KP 1] Kinderlehrer D., Pedregal P., Characterization of Young measure generated by gradients, Arch. Rat. Mech. Anal. 115 (1991), 329–365.

[KP 2] Kinderlehrer D., Pedregal P., Gradient Young measure generated by sequences in Sobolev spaces, J. Geometric Analysis 4 (1994), 59–90.   
[KS] Kinderlehrer D., Stampacchia G., An introduction of variational inequalities and their applications, Acad. Press (1980).   
[Kl] Klingenberg W., Lecture on closed geodesics, Grundlehren der Math. 230, Springer-Verlag (1978).   
[Ko] Kolmogorov A. N., On the conservation of conditionally periodic motions for a small change in Hamilton’s function (Russian), Dokl. Acad. Nauk SSSR 98 (1954), 525–530.   
[KKM] Knaster B., Kuratowski K., Mazurkiewicz S., Ein Beweis des Fixpunktsatze f¨ur n-dimensionale Simplexe, Fundamenta Mathematica 14 (1929), 132– 137.   
[Kn] Kneser H., Eine direkte Ableitung des Zornsche Lemmas aus dem Auswahlaxiom, Math. Zeit. 53 (1950), 110–113.   
[Krh] Krahn E., Uber eine von Rayleigh formulierte Minimaleigenschaft des¨ Kreises, Math. Ann. 94 (1925), 97–100.   
[Kr 1] Krasnosel ’ski M. A., Topological methods in the theory of nonlinear integral equations, MacMillan (1964).   
[Kr 2] Krasnosel ’ski M. A., Positive solutions of operator equations, Groningen, Noordhoff (1964).   
[Kui] Kuiper N. H., The homotopy type of the unitary group of Hilbert space, Topology 3 (1965) 19–30.   
[Kuk] Kuksin S. B., Nearly integrable infinite dimensional Hamiltonian systems, Lecture Notes in Math. 1556, Springer-Verlag (1993).   
[LU] Ladyzhenskaya O. A., Ural’ceva N. N., Linear and quasilinear elliptic equations, Acad. Press (1968).   
[La] Larsen R., Functional analysis, Marcel Dekker (1973).   
[Le] Leray J., La th´eorie des points fixes et ses applications en analyse, Proc. ICM, Cambridge 2 (1950), 202–208.   
[LL] Leray J., Lions J. L., Quelques r´esultats de Vishik sur les probl´emes elliptiques nonlin´eaires par les m´ethodes de Minty Browder, Bull. Soc. Math. France 93 (1965), 97–107.   
[LS] Leray J., Schauder J., Topologie et ´equations fonctionelles, Ann. Sci. Ecole Norm. Sup. 51 (1934), 45–78.   
[Li 1] Li Y. Y. Liouville-type theorems and Harnack-type inequalities for semilinear elliptic equations, J. d’analyse math´ematique 90 (2003), 27–87.   
[Li 2] Li Y. Y. Prescribing scalar curvature on $S ^ { n }$ and related problems, Part I, J. Diff. Eqs. 120, (1995), 319–410, Part II, Comm. Pure Appl. Math. 49 (1996), 541–597.   
[Lieb] Lieb E. H. Sharp constants in the Hardy-Littlewood-Sobolev and related inequalities, Ann. of Math. 118 (1983), 349–374.   
[Lie] Lieberman G. M., 2nd order parabolic differential equations, World Sci. (1996).   
[Lin] Lin F. H., A remark on the map $\frac { x } { | x | }$ , C. R. Sci. Paris 305 (1987), 529–531.   
[Lid] Lindqvist P., On the equation $d i v ( | \nabla u | ^ { p - 2 } \nabla u ) + \lambda | u | ^ { p - 2 } u = 0$ , Proc. AMS 109 (1992), 157–164.   
[LJ 1] Lions J. L., Quelques m´ethodes de r´esolution des probl´emes aux limites non lin´eaires, Dunod (1969).   
[LS] Lions J. L., Stampacchia G., Variational inequality, Comm. Pure Appl. Math. 20 (1967), 493–519.

[LP 1] Lions P. L., The concentration compactness principle in the calculus of variations, Part I and II, Analyse Nonlin´eaire 1 (1984), 109–145, 223–283.   
[LT 1] Liu G., Tian G., Weinstein conjecture and GW-invariants,   
[LT 2] Liu G., Tian G., Floer homology and the Arnold conjecture, J. Diff. Geom. 49 (1998), 1–74.   
[Liu 1] Liu J. Q., A Morse index of saddle points, System Sci. and Math. Sci. 2 (1989), 32–39.   
[Liu 2] Liu J. Q., A generalized saddle point theorem, J. Diff. Eqs. (1989) 372–385.   
[Lj] Ljusternik L., The topology of function spaces and the calculus of variations in the large, Trudy Mat. Inst. Steklov 19 (1947).   
[LjS 1] Ljusternik L., Schnirelmann L., M´ethodes topologiques dans les probl´emes variationnelles, Actualit´es Sci. Industr. 188 (1934).   
[LjS 2] Ljusternik L., Schnirelmann L., Sur le probl´eme de trois g´eodesiques ferm´ees sur les surfaces de genre 0, C. R. Acad. Sci Paris 189 (1929), 269–271.   
[Ll] Lloyd N. G., Degree theory, Cambridge Univ. Press (1977).   
[Lo] Long Y. M., Index theory for symplectic paths with applications, Progress in Math. 207, Birkhauser (2002).   
[LZ] Long Y. M., Zhu C. Closed characteristics on compact hypersurfaces in $R ^ { 2 n }$ , Ann. of Math. 155 (2002), 317–368.   
[Ma] Ma T. W., Topological degrees of set-valued compact fields in locally convex spaces, Dissertationes Mathematicae, 92 Warzawa (1972).   
[Man] Mann W. R., Mean value methods in iterations, Proc. AMS 4 (1953), 506– 510.   
[Mar] Marcellini P., Approximation of quasiconvex functions and lower semicontinuity of multiple integrals, Manuscripta Math. 51 (1985), 1–28.   
[MP 1] Marino A., Prodi G., La teoria di Morse per spazi di Hilbert, Rend. Sem. Mat. Univ. Padova, 41 (1968), 43–68.   
[MP 2] Marino A., Prodi G., Metodi perturbativi nella teoria di Morse, Boll. Un. Mat. Ital. Suppl. 3 (1975), 1–32.   
[Maw 1] Mawhin J., Topological degree methods in nonlinear boundary value problems, AMS (1979).   
[Maw 2] Mawhin J., Probl´emes de Dirichlet, variationnels nonlin´eaires, Univ. de Montr´eal 104 (1987)   
[Maw 3] Mawhin J. Functional analysis and BVP, Studies in Math. 14 (J. Hale ed.) (1977), 128–168.   
[MW] Mawhin J., Willem M., Critical point theory and Hamiltonian systems, Appl. Math. Sci. 74, Springer-Verlag (1989).   
[Mic] Michaikov K., Conley index theory, Lect. Notes, 1609 Springer-Verlag (1996), 119–207.   
[Mi 1] Milnor J., Morse theory, Princeton Univ. Press (1963).   
[Mi 2] Milnor J., Topology from the differential viewpoint, Univ. Press of Virginia, Charlottesville (1969).   
[Mi 3] Milnor J., Lecture on the h-cobordism theorem, Princeton Univ. Press (1965).   
[Min] Minty G., On a monotonicity method for the solutions of nonlinear equations in Banach spaces, Proc. Nat. Acad. Sci. USA, 50 (1963), 1038–1041.   
[MM] Modica L., Mortola S., Un esempio di Γ-convergenza, Boll. U. M. I. 14-B (1977), 285–299.

[Mor] Morel J. M., The Mumford Shah conjecture in image processing, Asterisque, 241, Expose 813 (1997), 221–242.   
[MS] Morel J. M., Solimini S., Variational models in image segmentation, Birkhauser (1994).   
[Mo 1] Morrey C. B., Quasi-convexity and the lower semicontinuity of multiple integrals, Pacific J. Math. 2 (1952), 25–53.   
[Mo 2] Morrey C. B., Multiple integrals in the calculus of variations, Springer (1966).   
[Mo] Morse M., The calculus of variations in the large, AMS Coll. Publ. 18 (1934).   
[MC] Morse M., Cairns S. S., Critical point theory in global analysis and differential topology, Acad. Press (1969).   
[MT] Morse M., Tompkins C. B., The existence of minimal surfaces of general critical types, Ann. of Math. 40 (1939), 443–472.   
[Mos 1] Moser J., A new technique for the construction of solutions of nonlinear differential equations, Proc. Nat. Acad. Sci. USA 47 (1961), 1824–1831.   
[Mos 2] Moser J., A rapidly convergent iteration method and nonlinear partial differential equations, I, II, Ann. Scuola Norm. Sup. Pisa 20 (1966), 226– 315, 449–535.   
[Mos 3] Moser J., Convergent series expansions for quasi-periodic motions, Math. Ann. 169 (1967), 136–176.   
[Mos 4] Moser J., On a nonlinear problem in differential geometry, Dynamical systems, Acad. Press (1973).   
[Mos 5] Moser J., A sharp form of an inequality by N. Trudinger, Indiana U. Math. J. 20 (1971), 1077–1092.   
[Mul 1] Muller S., Variational models for microstructure and phase transitions, Lecture Notes in Math, 1713 Calculus of variations and geometric evolution problems (Hildebrandt S., Struwe M., eds.) Springer-Verlag (1999), 85–210.   
[Mul 2] Muller S., A sharp version of Zhang’s theorem on truncating sequences of gradients, Trans. AMS,   
[MS] Mumford D., Shah J., Boundary detection by minimizing functionals, IEEE Conference on computer vision and pattern recognition, San Francisco (1985).   
[Na 1] Nash J., The embedding problem for Riemann manifolds, Ann. of Math. 63 (1956), 20–63.   
[Na 2] Nash J., Continuity of solutions of parabolic and elliptic equations, Amer. J. Math. 80 (1958), 931–954.   
[Na 3] Nash J., Non-cooperative games, Ann. of Math. 54 (1951), 286–295.   
[NN] Newlander A., Nirenberg L., Complex coordinates in almost complex manifolds, Ann. Math. 65 (1957), 391–404.   
[Ni] Ni W. M., Some minimax principles and their applications in nonlinear elliptic equations, J. d’analyse Math. 37 (1980), 248–275.   
[NT] Ni W. M., Takagi I., On the shape of least energy solutions to a semilinear Neumann problem, Comm. Pure Appl. Math. 45 (1991), 819–851.   
[Ni 1] Nirenberg L., Topics in nonlinear functional analysis, Courant Institute Lecture Notes, New York (1974).   
[Ni 2] Nirenberg L., Linear partial differential equations, CBMS, no.17 (19720.   
[Ni 3] Nirenberg L., Comments on nonlinear problems, Le Mathematische 16 (1981).

[Ni 4] Nirenberg L., Variational and topological methods in nonlinear problems, Bull. AMS 3 (1981), 267–302.   
[No] Nordstrom N., Minimization of energy functional with curve-represented edges (J. J. Koenderink ed.), 155–168.   
[Nu] Nussbaum R. D., The fixed point index for local condensing maps, Ann. Mat. Pura Appl. 37 (1972), 741–766.   
[Ob] Obata M., The conjectures on conformal transformations of Riemannian manifolds, J. Diff. Geom. 6 (1971), 247–258.   
[Oh] Oh Y. G., On positive multi-lump bound states of nonlinear Schrodinger equations under multiple well potential, Comm Math. Phys. 131 (1990), 223–253.   
[Pa 1] Palais R. S., Morse theory on Hilbert manifolds, Topology (1963), 299–340.   
[Pa 2] Palais R. S., Ljusternik Schnirelmann theory on Banach manifolds, Topology 5 (1966), 115–132.   
[Pa 3] Palais R. S., Homotopy theory of infinite dimensional manifolds, Topology 5 (1966), 1–16.   
[PS] Palais R. S., Smale S., A generalized Morse theory, Bull. AMS 70 (1964), 165–171.   
[PR 1] Pejsachowicz J., Rabier P. J., Degree theory for $C ^ { 1 }$ Fredholm mappings of index 0, J’Analyse mathematique 76 (1998), 289–319.   
[PR 2] Pejsachowicz J., Rabier P. J., A substitute for the Sard-Smale theorem in the $C ^ { 1 }$ case, ibid., 76 (1998), 265–288.   
[Pl] Plastock R., Homeomorphisms between Banach spaces, TAMS 200 (1974), 169–183.   
[Po] Poschel J., Small divisors with spatial structure in infinite dimensional Hamiltonian systems, Comm. Math. Phys. 127 (1990), 351–393.   
[Pw] Protter M. H., Weinbergeer H. F, Maximum principles in differential equations, Prentice–Hall, (1967).   
[QS] Quinn F., Sard A., Hausdorff conullity of critical images of Fredholm maps, Amer. J. Math. 94 (1972), 1101–1110.   
[Ra 1] Rabinowitz P., A global theorem for nonlinear eigenvalue problems and applications, Contributions to Nonlinear Functional Analysis, Academic Press (1971), 11–30.   
[Ra 2] Rabinowitz P., Th´eorie du degr´e topologique et applications ´a des probl´emes aux limites non lin´eaires, Univ. laborat. anal. num. Paris (1975).   
[Ra 3] Rabinowitz P., Periodic solutions for Hamiltonian systems, Comm. Pure Appl. Math. 31 (1978), 157–184.   
[Ra 4] Rabinowitz P., Minimax methods in critical point theory with applications to differential equations, CBMS Regional Conference Series Math. 65 (1986), AMS Providence.   
[Ra 5] Rabinowitz P., Variational method for nonlinear eienvalue problems (ed. LG. Prodi), Cremonese, Roma (1974), 141–195.   
[Re 1] Reshetnyak Yu. G., On the stability of conformal mappings in multidimensional spaces, Sib. Math. J. 8 (1967), 69–85.   
[Re 2] Reshetnyak Yu. G., Space mapping with bounded distortion, AMS (1989).   
[Ri] Riviere T., Everywhere discontinuous harmonic maps into spheres, Acta Math. 175 (1995), 197–226.   
[Ro 1] Rothe E., Critical points theory in Hilbert space under regular boundary conditions, J. Math. Anal. Appl. 36 (1971), 377–431.

[Ro 2] Rothe E., Morse theory in Hilbert space, Rocky Mountain J. Math. 3 (1973), 251–274.   
[Ro 3] Rothe E., On the connection between critical point theory and Leray Schauder degree, J. Math. Anal. Appl. 88 (1982), 265–269.   
[Ry] Rybakowski K. P., The homotopy index and partial differential equations, Springer-Verlag (1987).   
[RZ] Rybakowski K. P., Zehnder E., On the Morse equation in Conley’s index theory for semiflows on metric spaces, Ergodic Theory Dyn, Syst. 5 (1985), 123–143.   
[SU] Sacks J., Uhlenbeck K., The existence of minimal immersions of 2 spheres, Ann. Math. 113 (1981), 1–24.   
[Sa] Sadovskii B., Limit compact and condensing operators (Russian), Uspehi Mat. Nauk 27(1) (1972), 81–146.   
[Sal] Salamon D., Connected simple systems and the Conley index of isolated invariant sets, Trans. Amer. Math. Soc. 291 (1985), 1–41.   
[SZ] Salamon D., Zehnder, E., Morse theory for periodic solutions of Hamiltonian systems and the Morse index, Comm. Pure Appl. Math. 45 (1992), 1303–1360.   
[Sca] Schaefer H., Uber die Methode der a-priori Schranken, Math. Ann. 129 ¨ (1955), 415–416.   
[Sc] Schauder J., Der Fixpunktsatz in Funktionalr¨aumen, Studia Math. 2 (1930), 171–180.   
[Sch] Schiffman M., The Plateau problem for non-relative minima, Ann. of Math. 40 (1939), 834–854.   
[Sco] Schoen R., Conformal deformation of a Riemannian metric to a constant scalar curvature, J. Diff. Geom. 20 (19084), 479–495.   
[ScU 1] Schoen R., Uhlenbeck K., A regularity for harmonic maps, J. Diff. Geom. 17 (1982), 307–335.   
[ScU 2] Schoen R, Uhlenbeck K., Boundary regularity and the Dirichlet problem for harmonic maps, J. Diff. Geom. 18 (1983), 253–268.   
[SZ] Schoen R., Zhang D., Prescribing scalar curvature on the n-sphere, Calc. Var. PDE 4 (1996), 1–25.   
[Scw] Schwartz J. T., Nonlinear functional analysis, Golden Beach Publ. (1969).   
[Se] Semmes S., A primer on Hardy spaces and some remarks on a theorem of Evans and Muller, Comm. PDE, 19 (1994), 277–319.   
[Se] S´er´e E., Existence of infinitely many homoclinic orbits in Hamiltonian systems, Math. Zeit. 209 (1992), 27–42.   
[Shi] Shi S.Z., Ekeland’s variational principle and the mountain pass lemma, Acta Math. Sinica (NS) 1 (1985) 348–355.   
[Si] Sion M., On general minimax theorems, Pacific J. Math. 8 (1958), 171–176.   
[Sm 1] Smale S., Generalized Poincar´e’s conjecture in dimension great than four, Ann. of Math. 74 (1961), 391–406.   
[Sm 2] Smale S., Morse theory and a nonlinear generalization of the Dirichlet problem, Ann. of Math. 80 (1964), 382–396.   
[Sm 3] Smale S., An infinite dimensional version of Sard’s theorem, Amer. J. Math. 87 (1965), 861–867.   
[Sma] Smart D. R., Fixed point theorems, Cambridge Univ. Press (1974).   
[Smo] Smoller J., Shock waves and reaction diffusion equations, Grundlehren der Mathematischen Wissenschaften 258, Springer-Verlag (1982).

[So] Solimini S., Morse index estimates in Min-Max theorems, Manusc. Math. 32 (1989), 421–454.   
[Sp] Spanier E. H., Algebraic topology, Springer-Verlag (1966).   
[Ste 1] Stein E., Singular integrals and differentiability properties of functions, Princeton Univ. Press (1970).   
[Ste 2] Stein E., Harmonic analysis: real variable methods, orthogonality and oscillatory integrals, Princeton Univ. Press (1993).   
[SW] Stein E., Weiss G., Introduction to Fourier analysis on Euclidean spaces, Princeton Univ. Press (1971).   
[St 1] Struwe M., Variational Methods, Applications to nonlinear partial differential equations and Hamiltonian systems, 2nd ed., Springer-Verlag (1996).   
[St 2] Struwe M., Plateau problem and the calculus of variations, Princeton Univ. Press (1988).   
[ST] Stuart C. A., Toland J. F., The fixed point index of a linear k-set contraction, J. London Math. Soc. 6 (1973), 317–320.   
[Sv 1] Sver´ak V., Rank one convexity does not imply quasiconvexity, Pr ˇ oc. Royal Soc. Edinburgh 120 (1992), 185–189.   
[Sv 2] Sver´ak V., Lower semicontinuity of variational integrals and com ˇ pensated compactness, Proc. ICM 1994, Vol 2, Birkhauser (1995), 1153–1158.   
[Tal] Talenti G., Best constant in Sobolev inequality, Ann. Mat. Pure Appl. 110 (1976), 353–372.   
[Tar 1] Tartar L., The compensated compactness method applied to systems of conservations laws, Systems of Nonlinear Partial Differential Equations(Ball J., ed.), NATO ASI series, Vol CIII, Reidel (1983), 263–285.   
[Tar 2] Tartar L., H-measures, a new approach for studying homogenization, oscillations and concentation effects in partial differential equations, Proc. Roy. Soc. Edinburgh A 115 (1990), 193–230,   
[Ta] Tarski A., A lattice theoretical fixed point theorem and its applications, Pac. J. Math. 5 (1955), 285–309.   
[Tau] Taubes C. H. Self-dual connections on non-self-dual 4-manifolds, JDG 17 (1982), 139–170.   
[Ti] Tian G., On the mountain pass lemma, Kexue Tongbao 14 (1983), 833–835.   
[Tr] Trudinger N. S., On imbeddings into Orlicz spaces and some applicatons, J. Math Mech. 17 (1967), 473–483.   
[Tu] Turner R., Positive solutions of nonlinear eigenvalue problems (ed. LG. Prodi) Eigenvalues of Nonlinear Problems, Crmenese, Roma (1974), 213– 239.   
[Uh] Uhlenbeck K., Generic properties of eigenfunctions, Amer. J. Math. 98 (1976), 1059–1078.   
[Va] Vainberg M., Variational method for the study of nonlinear operators (Russian), Gostehizdt (1956).   
[Vi 1] Viterbo C., A proof of the Weinstein conjecture on $R ^ { 2 n }$ , Ann. Inst. H. Poincar´e, Annlyse non lineaire 4 (1987), 337–356.   
[Vi 2] Viterbo C., Indices de Morse des points critiques obtenus par minimax, Ann. Inst. H. Poincar´e, Analyse nonlin´eaire 5 (1988), 221–225.   
[VN] Von Neumann J., Zur Theorie der Gesellschaftsspiele, Math. Ann. 100 (1927), 295–320.   
[Wa 1] Wang Z. Q., A note on the deformation theorem, Acta Math. Sinica 30 (1987), 106–110.

[Wa 2] Wang Z. Q., On a superlinear elliptic equation, Analyse Nonlineaire 8 (1991), 43–58.   
[Way] Wayne C. E., Periodic and quasi-periodic solutions of nonlinear wave equations via KAM theory, Comm. Math. Phys. 127 (1990), 479–528.   
[Wei 1] Weinstein A., Periodic orbits for convex Hamiltonian systems, Ann Math. 108 (1978), 507–518.   
[Wei 2] Weinstein A., On the hypotheses of Rabinowitz’s periodic orbit theorem, J. Diff. Eq. 33 (1979), 353–358.   
[Wi] Willem M., Minimax methods, Birkhauser (1994).   
[Ya] Yamabe H., On the deformation of Riemannian structures on compact manifolds, Osaka Math. J. 12 (1960), 21–37.   
[Yo] Young L. C., Lectures on the calculus of variations and optimal control theory, Saunders (1969).   
[Ze] Zehnder E., Generalized implicit function theorem with applications to some small divisor problems, I, II, Comm. Pure Appl. Math. 28 (1975), 91–140 (1976), 49–111.   
[Zei] Zeidler, E., Nonlinear functional analysis and its applications I-IV, Springer-Verlag, New York (1988).   
[Zh 1] Zhang K. W., A construction of quasiconvex function with linear growth at infinity, Ann. S. N. S. Pisa 19 (1992), 313–326.   
[Zh 2] Zhang K. W., Two well structure and intrinsic mountain pass points, preprint.   
[Zi] Ziemer W. P., Weakly differentiable functions, Springer-Verlag (1989).

# Springer Monographs in Mathematics

This series publishes advanced monographs giving well-written presentations of the “state-of-the-art” in fields of mathematical research that have acquired the maturity needed for such a treatment. They are sufficiently self-contained to be accessible to more than just the intimate specialists of the subject, and sufficiently comprehensive to remain valuable references for many years. Besides the current state of knowledge in its field, an SMM volume should also describe its relevance to and interaction with neighbouring fields of mathematics, and give pointers to future directions of research.

Abhyankar, S.S. Resolution of Singularities of Embedded Algebraic Surfaces 2nd enlarged ed. 1998

Alexandrov, A.D. Convex Polyhedra 2005

Andrievskii, V.V.; Blatt, H.-P. Discrepancy of Signed Measures and Polynomial Approximation 2002

Angell, T. S.; Kirsch, A. Optimization Methods in Electromagnetic Radiation 2004

Ara, P.; Mathieu, M. Local Multipliers of $\mathbf { C } ^ { * }$ -Algebras 2003

Armitage, D.H.; Gardiner, S.J. Classical Potential Theory 2001

Arnold, L. Random Dynamical Systems corr. 2nd printing 2003 (1st ed. 1998)

Arveson, W. Noncommutative Dynamics and E-Semigroups 2003

Aubin, T. Some Nonlinear Problems in Riemannian Geometry 1998

Auslender, A.; Teboulle M. Asymptotic Cones and Functions in Optimization and Variational Inequalities 2003

Bang-Jensen, J.; Gutin, G. Digraphs 2001

Baues, H.-J. Combinatorial Foundation of Homology and Homotopy 1999

Brown, K.S. Buildings 3rd printing 2000 (1st ed. 1998)

Chang, K.-C. Methods in Nonlinear Analysis 2005

Cherry, W.; Ye, Z. Nevanlinna’s Theory of Value Distribution 2001

Ching, W.K. Iterative Methods for Queuing and Manufacturing Systems 2001

Crabb, M.C.; James, I.M. Fibrewise Homotopy Theory 1998

Chudinovich, I. Variational and Potential Methods for a Class of Linear Hyperbolic Evolutionary Processes 2005

Dineen, S. Complex Analysis on Infinite Dimensional Spaces 1999

Dugundji, J.; Granas, A. Fixed Point Theory 2003

Elstrodt, J.; Grunewald, F. Mennicke, J. Groups Acting on Hyperbolic Space 1998

Edmunds, D.E.; Evans, W.D. Hardy Operators, Function Spaces and Embeddings 2004

Engler, A.J.; Prestel, A. Valued Fields 2005

Fadell, E.R.; Husseini, S.Y. Geometry and Topology of Configuration Spaces 2001

Fedorov, Y.N.; Kozlov, V.V. A Memoir on Integrable Systems 2001

Flenner, H.; O’Carroll, L. Vogel, W. Joins and Intersections 1999

Gelfand, S.I.; Manin, Y.I. Methods of Homological Algebra 2nd ed. 2003

Griess, R.L. Jr. Twelve Sporadic Groups 1998

Gras, G. Class Field Theory corr. 2nd printing 2005

Hida, H. $\pmb { p }$ -Adic Automorphic Forms on Shimura Varieties 2004

Ischebeck, F.; Rao, R.A. Ideals and Reality 2005

Ivrii, V. Microlocal Analysis and Precise Spectral Asymptotics 1998

Jech, T. Set Theory (3rd revised edition 2002)

Jorgenson, J.; Lang, S. Spherical Inversion on SLn (R) 2001

Kanamori, A. The Higher Infinite corr. 2nd printing 2005 (2nd ed. 2003)

Kanovei, V. Nonstandard Analysis, Axiomatically 2005

Khoshnevisan, D. Multiparameter Processes 2002

Koch, H. Galois Theory of $\pmb { p }$ -Extensions 2002

Komornik, V. Fourier Series in Control Theory 2005   
Kozlov, V.; Maz’ya, V. Differential Equations with Operator Coefficients 1999   
Landsman, N.P. Mathematical Topics between Classical & Quantum Mechanics 1998   
Leach, J.A.; Needham, D.J. Matched Asymptotic Expansions in Reaction-Diffusion Theory 2004   
Lebedev, L.P.; Vorovich, I.I. Functional Analysis in Mechanics 2002   
Lemmermeyer, F. Reciprocity Laws: From Euler to Eisenstein 2000   
Malle, G.; Matzat, B.H. Inverse Galois Theory 1999   
Mardesic, S. Strong Shape and Homology 2000   
Margulis, G.A. On Some Aspects of the Theory of Anosov Systems 2004   
Murdock, J. Normal Forms and Unfoldings for Local Dynamical Systems 2002   
Narkiewicz, W. Elementary and Analytic Theory of Algebraic Numbers 3rd ed. 2004   
Narkiewicz, W. The Development of Prime Number Theory 2000   
Parker, C.; Rowley, P. Symplectic Amalgams 2002   
Peller, V. (Ed.) Hankel Operators and Their Applications 2003   
Prestel, A.; Delzell, C.N. Positive Polynomials 2001   
Puig, L. Blocks of Finite Groups 2002   
Ranicki, A. High-dimensional Knot Theory 1998   
Ribenboim, P. The Theory of Classical Valuations 1999   
Rowe, E.G.P. Geometrical Physics in Minkowski Spacetime 2001   
Rudyak, Y.B. On Thom Spectra, Orientability and Cobordism 1998   
Ryan, R.A. Introduction to Tensor Products of Banach Spaces 2002   
Saranen, J.; Vainikko, G. Periodic Integral and Pseudodifferential Equations with   
Numerical Approximation 2002   
Schneider, P. Nonarchimedean Functional Analysis 2002   
Serre, J-P. Complex Semisimple Lie Algebras 2001 (reprint of first ed. 1987)   
Serre, J-P. Galois Cohomology corr. 2nd printing 2002 (1st ed. 1997)   
Serre, J-P. Local Algebra 2000   
Serre, J-P. Trees corr. 2nd printing 2003 (1st ed. 1980)   
Smirnov, E. Hausdorff Spectra in Functional Analysis 2002   
Springer, T.A. Veldkamp, F.D. Octonions, Jordan Algebras, and Exceptional Groups 2000   
Sznitman, A.-S. Brownian Motion, Obstacles and Random Media 1998   
Taira, K. Semigroups, Boundary Value Problems and Markov Processes 2003   
Talagrand, M. The Generic Chaining 2005   
Tauvel, P.; Yu, R.W.T. Lie Algebras and Algebraic Groups 2005   
Tits, J.; Weiss, R.M. Moufang Polygons 2002   
Uchiyama, A. Hardy Spaces on the Euclidean Space 2001   
Üstünel, A.-S.; Zakai, M. Transformation of Measure on Wiener Space 2000   
Vasconcelos, W. Integral Closure. Rees Algebras, Multiplicities, Algorithms 2005   
Yang, Y. Solitons in Field Theory and Nonlinear Analysis 2001   
Zieschang P.-H. Theory of Association Schemes 2005

# A new U-shapelet clustering algorithm by capturing morphological trends

Mei Chena,∗ iD, Yu Wanga, Mingwei Leng b, Boya Liua, Yuanyuxiu Youa, Yiying Yaoa, Song Wanga

a School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, 730070, China   
b College of Educational Science and Technology, Northwest Minzu University, Lanzhou, 730030, China

# H I G H L I G H T S

∙ A representation method precisely captures time series morphology trends.   
∙ A similarity ensures that the captured trend information remains intact.   
∙ A U-shapelet clustering algorithm raises accuracy while reducing time cost.

# A R T I C L E I N F O

Communicated by: Y. Bao

Keywords:

Time series clustering

U-shapelet

Feature representation

Similarity measurement

# A B S T R A C T

U-shapelet-based time series clustering methods utilize discriminative subsequences for clustering, ensuring strong interpretability and high accuracy. While many methods focus on improving e˥ciency by shrinking the U-shapelet candidate set, they often sacriˡce clustering accuracy due to inadequate attention to U-shapelet quality. To address this issue, this paper presents Morph, a novel U-shapelet-based clustering algorithm that captures -morphological trends in time series. Firstly, the Discrete Feature Representation (DFR) is introduced to e˫ec tively capture key information and morphological trends. Secondly, the Extended Euclidean Distance (EED) is developed as a robust similarity measure that enhances the selection of high-quality U-shapelets by integrating seamlessly with DFR. Lastly, random masking is incorporated into Morph to expedite the selection process of the U-shapelet set. Experiments on 14 diverse datasets show that Morph outperforms 15 baselines, demonstrating its e˫ectiveness in achieving high accuracy and interpretability.

# 1. Introduction

Time series clustering is a data mining method based on time series representation and similarity measurement that can mine the inherent patterns and trends in time series without any prior knowledge about the data. In numerous domains such as ˡnance [1], biomedicine [2], weather forecasting [3], engineering [4], and others, time series clustering has become an important topic.

Traditional time series clustering methods [5–8] usually focus on processing the whole time series data, which cannot solve the time series clustering problem well. Firstly, traditional clustering algorithms usually use Euclidean Distance (ED) to calculate the similarity between time series. However, ED is very sensitive to noise in the data. This means that even though two time series are not similar in shape and trend,

they may still be incorrectly grouped together because they have similar noise characteristics, thereby a˫ecting the ˡnal clustering results. Secondly, most traditional time series clustering methods are completed under the assumption that the length of the time series is the same, which requires a lot of time and energy to preprocess the data. Although some researchers have used Dynamic Time Warping (DTW) [9–12] to deal with unequal-length time series, the time consumption of this method is unacceptable. Finally, the results obtained are poorly interpretable, as they are only reˣected in the data and not in the actual di˫erences between classes, such as shape and morphological trends.

In 2012, Keogh et al. [13] proposed the concept of unsupervised shapelets (U-shapelets) in the BruteForce method, which aim to use subsequences as objects for time series clustering analysis. A U-shapelet,

exempliˡed by the red-marked segment in Fig. 1, is a discriminative subsequence in a time series. When similarity is calculated directly on the entire time series, it becomes di˥cult to accurately distinguish between the two time series in Fig. 1. However, precisely distinguishing between the two time series becomes possible when we use the red-marked U-shapelet. The key lies in the fact that each extracted -U-shapelet serves as a core feature for a particular class. These dis criminative local subsequences provide a basis for clustering analysis by capturing the most signiˡcant morphological di˫erences between classes. Their interpretability is manifested in two aspects: time series of the same class necessarily contain subsequences with similar U-shapelet -morphology, and each U-shapelet directly represents the essential char acteristics of that class. Thanks to this direct correlation, the U-shapelets -method signiˡcantly improves the interpretability while achieving ef fective partitioning. Furthermore, utilizing di˫erences in local features -between di˫erent time series not only enhances the accuracy of clus tering but also helps mitigate the e˫ect of noisy data. Moreover, this approach is particularly versatile as it is applicable to time series with varying sequence lengths. While U-shapelets o˫er signiˡcant advantages in time series clustering, the BruteForce method su˫ers from excessive time complexity. Speciˡcally, if there are $n$ time series in a dataset, the length of each time series is $m$ , then the time complexity of BruteForce is $O ( n ^ { 2 } m ^ { 4 } )$ .

To reduce the time cost of the algorithm, many researchers are devoted to accelerating the selection process of U-shapelets. Ulanova -et al. [14] proposed a Scalable U-SHapelet (SUSH) method, which trans forms the time series using Symbolic Aggregate Approximation (SAX) [15] and then utilizes random masking to select $1 ~ \%$ -from a large num ber of subsequences as the U-shapelet candidate set. However, since SAX -only uses the mean value of time series segments for local feature ex traction during the symbolization process [16,17], it cannot objectively capture the intra-segment information of each segment. As a result, it -primarily reˣects the overall trends of the original time series while eas ily overlooking important details such as morphological changes and -feature points of the time series. Keogh et al. [18] accelerated the dis covery of U-shapelets by employing pruning strategies and optimizing -the order of choosing the candidate U-shapelets. Luo et al. [19] intro duced the locally sensitive hash algorithm to retrieve similar sequences of the query subsequence, and then determine the likelihood of it being a U-shapelet based on the number of similar sequences. Yu et al. [20] used a multivariate top-k query to remove redundant subsequences to get the U-shapelet candidate set. Although these methods improve the e˥ciency of U-shapelet discovery to some extent, their only focus on the size of

![](images/e6f497091d48259fd70d138a58678dbe70af743458c5c050e6d82782f0b339f8.jpg)  
Fig. 1. Two time series from the trace dataset. (https://www.cs.ucr.edu/ ~eamonn/time_series_data_2018/.)

the candidate set may cause the highly discriminative subsequences to be ˡltered out. The discrimination of a U-shapelet is reˣected by its quality [21]. The higher the quality of the U-shapelet, the stronger its discrimination. For example, as shown in Fig. 1, the U-shapelet marked in red can be regarded as a high-quality subsequence because it can distinguish two classes of time series better compared to the other subsequences. High-quality U-shapelets can accurately capture and reˣect -the key features and patterns of time series, thus e˫ectively distin guishing di˫erent time series categories. Therefore, selecting a set of high-quality U-shapelets is essential for improving the accuracy of time series clustering.

To achieve a high quality U-shapelet set and e˫ective clustering, this paper introduces Morph, a novel U-shapelet-based clustering algorithm. -Morph utilizes a Discrete Feature Representation (DFR) to capture mor phological trends and employs an Extended Euclidean Distance (EED) for similarity measurement. Additionally, the algorithm accelerates the U-shapelet selection through random masking [14]. Experiments demonstrate its superior clustering performance. The main contribution of this study can be summarized as follows,

-(1) To meticulously capture the morphological trends and key infor -mation inherent in time series data, a new time series representa tion, Discrete Feature Representation (DFR), is proposed.   
(2) To ensure that the trend information encapsulated by DFR remains intact, we derive an Extended Euclidean Distance (EED) from the length-normalized Euclidean Distance.   
(3) Leveraging the strengths of DFR and EED, we propose Morph, a novel U-shapelet-based clustering algorithm. Morph extracts a high-quality set of U-shapelets, enabling it to cluster time series accurately.

Organization. The rest of this paper is organized as follows: Section 2 -brieˣy reviews existing time series representation methods and similar ity measurements. In Section 3, we deˡne the necessary notations. The details of our proposed method are presented in Section 4. In Section 5, experimental results are reported, which validate the feasibility of the proposed method. Finally, conclusions are given in the last section.

# 2. Related work

The representation and similarity measurement of time series are pivotal tasks that underpin the e˫ectiveness of time series clustering [5].

# 2.1. Time series representation methods

Time series representation serves as a fundamental step in time series analysis, transforming raw data into a form more suitable for subsequent analysis, thereby enhancing e˥ciency, accuracy, and reliability. Existing time series representation methods can be divided into three types: nondata adaptive [22–25], data adaptive [15–17,26], and model-based [27– 29], as shown in Table 1.

Non-data adaptive methods enable the transformation of highdimensional time series data into lower-dimensional data spaces, where the data itself remains relatively independent and decoupled from the -transformation process and the choice of feature coe˥cients. Data adap tive methods operate on all time series within datasets, aiming to -minimize the global reconstruction error by using segments of arbi trary length [33]. Unlike non-data adaptive methods, data-adaptive -methods permit inconsistent transformation parameters across time se ries. These methods not only rely on individual time series but are also -inˣuenced by the overall dataset, while maintaining relative indepen dence. Model-based methods assume in advance that time series data are generated by a certain model and then set appropriate parameters or coe˥cients for the model to achieve a characteristic representation -of the time series. Such methods require the selection of an appro priate model and a thorough understanding of the time series data

Table 1 Time series representation methods.   

<table><tr><td>Method Type</td><td>Method</td><td>Advantage</td><td>Disadvantage</td></tr><tr><td rowspan="5">Data adaptive</td><td>SAX [15]</td><td>Reduce dimensions and remove the noise effectively, which can reflect the overall trend of the time series.</td><td rowspan="5">Segment the time series first, and then represent each segment with a single character, making it impossible to distinguish differences within segments.</td></tr><tr><td>ESAX [16]</td><td>Reflect the shape of the time series relatively efficiently by simultaneously considering the maximum, minimum, and average values of time series segments.</td></tr><tr><td>TSAX [17]</td><td>Consider the trend characteristics of each segment.</td></tr><tr><td>SAX-TD [26]</td><td>Express time series with different trend characteristics efficiently.</td></tr><tr><td>fABBA [30]</td><td>Effective capture of morphological changes in time series by adaptive segmentation.</td></tr><tr><td rowspan="4">Model-based</td><td>HMM [27]</td><td>Capture dependencies between time series variables effectively.</td><td rowspan="4">Due to global modelling properties, it is difficult to capture the local trends of time series.</td></tr><tr><td>ARIMA [28]</td><td>Express the feature information of the time series intuitively.</td></tr><tr><td>MC [29]</td><td>Represent dynamic features of time series efficiently.</td></tr><tr><td>aLLM4TS [31]</td><td>Effective enhancement of the quality of time series representation by combining the sequence modelling capabilities of LLM.</td></tr><tr><td rowspan="4">Non-data adaptive</td><td>DFT [22]</td><td>Realize frequency-domain conversion.</td><td rowspan="4">Focus mainly on global information when processing data, and easily ignore or weaken the local features of time series.</td></tr><tr><td>PAA [23,24]</td><td>Realize fast indexing and reduce dimension.</td></tr><tr><td>DWT [25]</td><td>Express the time-domain information and frequency-domain information of time series.</td></tr><tr><td>RFB [32]</td><td>Feature diversity can be enhanced by combining the original signal with randomly generated low-pass, high-pass, and band-pass filters.</td></tr></table>

generation process in advance. The existing methods cannot describe the key information (e.g., local extremes, trend turning points, and amplitude changes, etc.) and morphological trends of time series in detail, leading to the fact that highly discriminative subsequences may lose their important features during the feature representation process. Thus, it is not possible to ensure that they will be added to the Ushapelet candidate set. This issue signiˡcantly impacts the accuracy of clustering.

# 2.2. Similarity measure methods

As the most commonly used distance, Euclidean Distance (ED) can be applied to time series, but it primarily focuses on measuring numerical distances, overlooking the shape-changing characteristics of time series [34]. Keogh et al. [13] utilize the length-normalized Euclidean Distance (lnED) as a means of quantifying the similarity between time series. -However, the z-normalization required before using this distance signif icantly changes the intrinsic properties of the time series. To accurately capture the shape di˫erences in time series, Dynamic Time Warping (DTW) measures distances between them by disregarding both global and local time o˫sets. However, the method is sensitive to noise, exhibits high time complexity, and may fail to capture the overall shape of time series e˫ectively. Although many improved methods [35–37] enhance the operational e˥ciency of DTW, the distance they obtain is merely -an approximation of the classical DTW, whose accuracy is not consis tently stable and relies heavily on predeˡned parameters. In addition, the Shape-Based Distance (SBD) [6] focuses on the shape characteristics of time series for similarity measurement, which is less sensitive to noise but has higher computational complexity.

-In conclusion, similarity selection is crucial for time series cluster ing as it determines how the algorithm captures and utilizes the key information and morphological trends of time series data.

# 2.3. Random masking

Random masking is a feature selection method, and its core idea is to quickly screen out the sub-patterns with distinguishing ability by means of random projection.

1. Generating SAX words

SAX is used to convert continuous time series into discrete symbolic representations, where continuous time series refers to all the subsequences extracted from the dataset, and each subsequence generates a corresponding SAX word.

2. Random projection

-Even small di˫erences between time series may generate dif ferent SAX words during the discrete representation. To solve this problem, the random masking generates a new set of masked symbol strings by randomly projecting each SAX word and randomly masking some of the symbol positions to improve the collision probability of similar subsequences. This process increases the probability that similar subsequences will share identical masked strings despite having di˫erent original SAX representations.

3. Counting the number of collisions

-For each masked symbol string generated by random projec tion, we count its occurrence frequency across all time series (i.e., the number of collisions). The process works as follows: in each -iteration, di˫erent masking patterns are applied, and the colli sion count for all SAX words containing each resulting substring is recorded. The collision results generated by each projection are accumulated and eventually form a collision table for subsequent screening of candidate subsequences.

4. Screening candidates

After 𝑟 iterations of random projection, ˡrstly, according to the statistical results of the number of collisions, the SAX words with stable number of collisions and strong discriminatory ability are screened out, and then these selected SAX words are mapped back to the original time series.

# 3. Preliminaries

Consider a set of time series $\textit { D } = \ \{ T _ { 1 } , T _ { 2 } , \ldots , T _ { n } \}$ . Each example $T _ { i }$ $\mathbf { \varepsilon } ( 1 ~ \leq ~ i ~ \leq ~ n )$ contains an ordered set     of real values denoted as $T _ { i } = \{ t _ { 1 } , t _ { 2 } , \dots , t _ { m } \}$ , where $m$ is the length   of $T _ { i }$ .

(1) Subsequences: A subsequence $S$ is a contiguous sequence of a time series. Subsequence $S$ of length $l$ of a time series $T _ { i }$ starting    at position $j$ can be written as $S = T _ { j , l } = \{ t _ { j } , t _ { j + 1 } , \dots , t _ { j + l - 1 } \}$ .

There are $n \times \frac { m ( m { + } 1 ) } { 2 }$ subsequences of all possible lengths in total. We focus on selecting high-quality subsequences in time series for clustering.

(2) Subsequence distance: The subsequence distance $s d i s t ( S , T _ { i } )$ be - tween a subsequence $S$ and a time series $T _ { i }$ is deˡned as the minimum distance between the subsequence $S$ and all possible subsequences of $T _ { i }$ of length equal to the length of $S$ .   
(3) $U$ -shapelet candidate set: The U-shapelet candidate set contains all the candidate subsequences obtained by ˡltering.   
(4) U-shapelet: U-shapelet is deˡned as the most discriminative time series segment. A U-shapelet $\hat { S }$ can divide the dataset $D$ into subsets $D _ { A }$ and $D _ { B }$ , where the division is based on the criterion that $s d i s t ( \hat { S } , D _ { A } ) < < s d i s t ( \hat { S } , D _ { B } )$ .   
(5) Gap: Gap is used to evaluate the quality of candidate U-shapelet $S ^ { \prime }$

$$
g a p = \mu_ {B} - \sigma_ {B} - (\mu_ {A} + \sigma_ {A}) \tag {1}
$$

where $\mu _ { A }$ and $\mu _ { B }$ denote the mean of $s d i s t ( S ^ { \prime } , D _ { A } )$ and the mean of $s d i s t ( S ^ { \prime } , D _ { B } )$ , respectively, while $\sigma _ { A }$ and $\sigma _ { B }$ denote     the standard deviation of $s d i s t ( S ^ { \prime } , D _ { A } )$ and $s d i s t ( S ^ { \prime } , D _ { B } ) _ { : }$ ,  respectively. $D _ { A }$ and $D _ { B }$ are obtained by dividing dataset $D$ using $S ^ { \prime }$ .

# 4. Methodology

This section describes the Morph algorithm. The complete ˣow is shown in Fig. 2. First, the extracted subsequences are represented by Discrete Feature Representation (DFR), proposed in Section 4.1, to capture their morphological trends and key information. Meanwhile,

random masking [14] is introduced to ˡlter redundant subsequences to accelerate the extraction e˥ciency of U-shapelets (steps $\textcircled{2}$ and $\textcircled{3}$ ). Next, the similarity between sequences is measured by Extended Euclidean Distance (EED), presented in Section 4.2, to fully exploit the trend information captured by DFR, and the characteristic of U-shapelet is used to obtain the clustering results (steps $\textcircled{4}$ and $\textcircled{5}$ ). Lastly, the clustering process of the algorithm is detailed in Section 4.3.

# 4.1. Time series representation: DFR

Local shapes can reveal the local characteristics and behaviors of the time series. Extracting local shapes from the time series can make clustering easier. For example, in Fig. 1, two time series are highly similar overall, except for the local shape within the black box, which is signiˡcantly di˫erent. By focusing on local shapes, we can ensure that -these local di˫erences are not overshadowed by the overall similari ties, as is often the case with traditional whole time series clustering methods [5]. To represent the shape features of the time series as well as the shapes themselves, we propose a new method, Discrete Feature -Representation (DFR), as a proxy representation for real-valued time se ries data. The method can capture the local shape trends of time series in a concise and e˫ective way to identify and obtain a high-quality set of U-shapelets.

-DFR uses a sliding window to discretize the time series. Here, a slid ing window refers to a window with size $w$ that is used to slide from the starting point of a time series to the end point with a step length of $s l$ -, and the values in the window are recorded for subsequent re search when a window moves forward. The following illustrates the speciˡc processing steps of DFR, using a sequence fragment $S = T _ { j - 2 , l } =$ $\{ t _ { j - 2 } , t _ { j - 1 } , t _ { j } , \dots , t _ { j + l - 3 } \}$ as an where example, the s time serie     within the sliding window is denoted by $Q = \{ t _ { x } , t _ { x + 1 } , \ldots , t _ { x + w - 1 } \}$ . Note that here we ˡx $s l$ to 1.

![](images/97e045d4ea7d770cb24e663229b01830582385202986f46241ff61bfd7efec54.jpg)  
Fig. 2. The framework of the proposed Morph algorithm. From the original time series $\textcircled{1}$ , we extract all subsequences with the length of 𝑠𝐿𝑒𝑛 $\textcircled{2}$ ; use DFR to process the extracted subsequences and adopt random masking to obtain the U-shapelet candidate set $\textcircled{3}$ ; get a $\hat { S }$ , calculate its similarity with the time series using EED and clustering by $\hat { S }$ , then repeat this process until convergence $\textcircled{4} )$ . Finally, get the clustering result $\textcircled{5} )$ .

![](images/ebe34e2cadec42ea558a8c82037a293af5c211835cd3245a7f0499b24d72ce99.jpg)  
Fig. 3. Example of the DFR process.

(1) Initialize two lists: $l i s t _ { t m p } = \{ 0 \} _ { i = 1 } ^ { w } = \{ 0 , 0 , \ldots , 0 \}$ and $l i s t _ { d i s } =$ $\{ 0 \} _ { i = 1 } ^ { l } = \{ 0 , 0 , \dots , 0 \}$ , and calculate the global mean (𝑔𝑚𝑒𝑎𝑛) of $S$ .

$$
g m e a n = \frac {1}{l} \sum_ {i = j - 2} ^ {j + l - 3} t _ {i} \tag {2}
$$

where $l i s t _ { t m p }$ is used to store intermediate variables obtained during the window sliding, $l i s t _ { d i s }$ is used to store the ˡnal discrete result, and 𝑔𝑚𝑒𝑎𝑛 is used to capture the overall trends of the time series.

Next, as the window slides over the time series, Eqs. (3)–(5) are iteratively executed, and the data is processed multiple times until the sliding window reaches the end point.

(2) Calculate the local mean (𝑙𝑚𝑒𝑎𝑛) of $S$ .

$$
l m e a n = \frac {1}{w} \sum_ {i = x} ^ {x + w - 1} t _ {i} \tag {3}
$$

where 𝑙𝑚𝑒𝑎𝑛 provides a micro-view of the time series analysis. By using 𝑔𝑚𝑒𝑎𝑛 and 𝑙𝑚𝑒𝑎𝑛, we can reveal the intrinsic characteristics of the time series at di˫erent levels, and comprehensively describe the morphological trends and key information of the time series.

(3) Update $l i s t _ { t m p }$ based on the sorted index of $Q$

$$
l i s t _ {t m p} = \left\{ \begin{array}{c} \arg \operatorname {s o r t} (Q) + 1, \quad g \text {m e a n} \leq l \text {m e a n} \\ - (\arg \operatorname {s o r t} (- Q) + 1), g \text {m e a n} > l \text {m e a n} \end{array} \right\} \tag {4}
$$

where arg 𝑠𝑜𝑟𝑡(𝑄) is used to return the index of $Q$ in ascending order when 𝑔𝑚𝑒𝑎𝑛 ≤ 𝑙𝑚𝑒𝑎𝑛, and arg 𝑠𝑜𝑟𝑡 $( - Q )$ is used to return the index of $Q$ in descending order when 𝑔𝑚𝑒𝑎𝑛 $>$ 𝑙𝑚𝑒𝑎𝑛. Then, $l i s t _ { t m p }$ is updated based on the sorting result.

(4) Calculate $l i s t _ { d i s }$ by $l i s t _ { t m p }$ and 𝑔𝑚𝑒𝑎𝑛.

$$
l i s t _ {d i s} [ i ] = l i s t _ {d i s} [ i ] + \left[ l i s t _ {t m p} [ \text {i n d e x o f} (Q, S [ i ]) ] \cdot g m e a n \right] \tag {5}
$$

where $l i s t _ { d i s } [ i ]$ denotes the value of the $i ^ { t h }$ element in $l i s t _ { d i s }$ 𝑖𝑛𝑑𝑒𝑥𝑜𝑓 $( Q , S [ i ] )$ is used to obtain the index of the position of $S [ i ]$ in $Q$ . By using $l i s t _ { t m p }$ and 𝑔𝑚𝑒𝑎𝑛 to process 𝑄, $l i s t _ { d i s }$ is updated con     - tinuously. The combination of 𝑔𝑚𝑒𝑎𝑛 with the sorting result allows us to highlight the local trends of time series more pronouncedly while keeping their global trends intact.

Since the $l i s t _ { d i s }$ obtained during the above iteration process is a cumulative value, Eq. (6) is required to obtain the ˡnal $l i s t _ { d i s }$ .

(5) Get the DFR representation.

$$
l i s t _ {d i s} [ i ] = \frac {l i s t _ {d i s} [ i ]}{q}, \quad q = \left\{ \begin{array}{l l} 2 & \text {i f} i = j - 1 \text {o r} i = j + l - 4 \\ 3 & \text {i f} j - 1 <   i <   j + l - 4 \\ 1 & \text {o t h e r w i s e} \end{array} \right. \tag {6}
$$

where $q$ denotes the number of times each data point of $S$ appears within the sliding window.

Note that in DFR, the choice of the window size 𝑤 needs to strike a balance between feature capture and noise robustness: too large a value of $w$ will smooth out local features, while too small a value of $w$ will be sensitive to noise. In this paper, we set the window size to $w = 3$ , a choice based on the following considerations: (1) integrity: three temporally adjacent data points are the smallest unit that can uniquely identify -the underlying pattern of a local trend (up, down, or turn); (2) sensitiv ity: the three-point window is able to capture short-term ˣuctuations in the time series in a timely manner, avoiding the omission of important -changes due to too large a window; (3) robustness: the relative relation ship between the three points can e˫ectively suppress the interference of noise. This setup avoids the oversensitivity of the two-point window to noise and overcomes the problem of local feature blurring that may be caused by a larger window.

-To clearly illustrate the process of DFR, Fig. 3 demonstrates the com plete procedure using an example sequence. First, the $g m e a n = 6 4 2 . 4 2$ is obtained and $l i s t _ { t m p }$ and $l i s t _ { d i s }$ -are initialized. During the sliding win dow phase, taking the ˡrst window $Q _ { 1 } = \{ 5 0 1 , 1 9 5 , 2 1 8 \}$ as an example, the 𝑙𝑚𝑒𝑎𝑛 is obtained using Eq. (3). Since 𝑔𝑚𝑒𝑎𝑛 $>$ 𝑙𝑚𝑒𝑎𝑛, $Q _ { 1 }$ is sorted in descending order, yielding $a r g s o r t ( - Q ) = [ 0 , 2 , 1 ]$ . $l i s t _ { t m p }$ -is then up dated according to Eq. (4), followed by the update of $l i s t _ { d i s }$ based on Eq. (5). This process iterates until the entire sequence is traversed. Finally, the ˡnal $l i s t _ { d i s }$ is calculated using Eq. (6), which is the ˡnal DFR representation.

DFR achieves multilevel discretization of time series by a sliding window mechanism, and its core theoretical contributions are reˣected in two key designs: (1) Global-local synergistic analysis maintains the global consistency of data while signiˡcantly enhancing the recognition capability for local key patterns (e.g., trend turning points and extreme points). It captures the overall trend of the time series through the global -mean (𝑔𝑚𝑒𝑎𝑛), while simultaneously analyzing micro-ˣuctuations us ing the local mean (𝑙𝑚𝑒𝑎𝑛) within sliding windows. (2) A discretization -strategy based on sorting is used, where the sorting direction (ascend ing/descending) is adaptively determined by comparing the relative magnitude of the 𝑔𝑚𝑒𝑎𝑛 and the 𝑙𝑚𝑒𝑎𝑛. Ascending order is used to strengthen the upward trend when the lmean $>$ -gmean, and descend ing order is used to highlight the downward trend. This nonlinear mapping transforms temporal variations into semantically meaningful -symbolic sequences, which can signiˡcantly improve the di˫erentia tion of features while ensuring the consistency of the discrete results with the original trend. These two designs make DFR both robust and discriminative for time series representation, e˫ectively smoothing the -data and reducing noise interference. When random masking is subse quently used, this advantage increases the likelihood of mapping similar subsequences to similar discrete sequences, thus improving the quality of the U-shapelet candidate set.

![](images/70dbac670f47f970613a8e8bf7703f0c596d7593128ecebd008bd1dac2d240ec.jpg)  
Fig. 4. Visual comparison of DFR and SAX on a time series sample.

To highlight the advantages of DFR, we visually compare its performance with SAX, a classical and widely-used time series representation, on a time series, as illustrated in Fig. 4, where DFR (depicted in red) and SAX (depicted in blue) display contrasting abilities in capturing the trends and characteristics of the original time series (depicted in black).

DFR (red) accurately captures the peaks and troughs of the original time series (black) by identifying and amplifying its local trends. This demonstrates its superiority in capturing the morphological changes and detailed features of the time series, attributed to Eq. (5) which plays a crucial role in maintaining and amplifying these trends. Furthermore, the overall trends of DFR (red) closely align with those of the original time series (black), indicating that DFR not only clariˡes the local trends but also faithfully maintains the global trends. In contrast, SAX (blue) falls short in e˫ectively mirroring both the overall and local trends. While it captures some local trends around positions 20 to 25 on the $\mathbf { X }$ -axis, its representation diverges signiˡcantly from the original trends at other points, revealing its limitations.

# 4.2. Similarity measure: EED

Most U-shapelet-based time series clustering methods utilize the length-normalized Euclidean Distance (lnED) [13,18] to measure the similarity between two time series $_ { T X }$ and 𝑇 𝑌 , as shown in Eq. (7).

$$
\operatorname {d i s t} (T X, T Y) = \sqrt {2 (1 - C (T X , T Y))}, \tag {7}
$$

$$
C = \frac {\sum_ {j = 1} ^ {n} \left(T X _ {j} \times T Y _ {j}\right) - n \mu_ {T X} \mu_ {T Y}}{n \sigma_ {T X} \sigma_ {T Y}}. \tag {8}
$$

where $T X _ { j }$ and $T Y _ { j }$ denote the $j ^ { t h }$ data points of the time se - ries $_ { T X }$ and $T Y$ , respectively, and $C$ is their positive correlation coe˥cient.

-The data processed by Eq. (7) ideally should conform to a stan dard normal distribution for optimal analysis. To facilitate this, znormalization is applied, e˫ectively aligning the mean to 0 and the standard deviation to 1. However, it should be noted that this process -does not guarantee that the resulting data strictly follows a standard nor mal distribution, but rather aligns its mean and standard deviation with those of a standard normal distribution. Additionally, it’s important to -note that the z-normalization presupposes an approximately normal dis tribution in the original data, which is often not the case with real-world time series data. When the data deviates signiˡcantly from a normal distribution, applying z-normalization can severely distort the original shape and obscure signiˡcant trend changes and intrinsic information within the original time series. This can diminish the discriminative power of U-shapelets, making it more challenging to identify and extract meaningful patterns.

To overcome this limitation, we derive an Extended Euclidean Distance (EED) from Eq. (7), and the derivation is shown in Eq. (9).

$$
\begin{array}{l} E E D (T X, T Y) = \sqrt {\frac {1}{n} \sum_ {j = 1} ^ {n} \left(T X _ {j} - T Y _ {j}\right) ^ {2}} \tag {9} \\ = \sqrt {\frac {1}{n} \sum_ {j = 1} ^ {n} T X _ {j} ^ {2} - \frac {2}{n} \sum_ {j = 1} ^ {n} T X _ {j} \cdot T Y _ {j} + \frac {1}{n} \sum_ {j = 1} ^ {n} T Y _ {j} ^ {2}} \\ = \sqrt {(\sigma_ {T X} ^ {2} + \mu_ {T X} ^ {2}) - 2 (C (T X , T Y) \cdot \sigma_ {T X} \sigma_ {T Y} + \mu_ {T X} \mu_ {T Y}) + (\sigma_ {T Y} ^ {2} + \mu_ {T Y} ^ {2})} \\ = \sqrt {\sigma_ {T X} ^ {2} + \sigma_ {T Y} ^ {2} - 2 C (T X , T Y) \cdot \sigma_ {T X} \sigma_ {T Y} + (\mu_ {T X} ^ {2} + \mu_ {T Y} ^ {2} - 2 \mu_ {T X} \mu_ {T Y})} \\ = \sqrt {(\sigma_ {T X} - \sigma_ {T Y}) ^ {2} + (\mu_ {T X} - \mu_ {T Y}) ^ {2} + 2 (1 - C (T X , T Y)) \cdot \sigma_ {T X} \sigma_ {T Y}} \\ = \sqrt {2 \sigma_ {T X} \sigma_ {T Y} (1 - C (T X , T Y)) + (\sigma_ {T X} - \sigma_ {T Y}) ^ {2} + (\mu_ {T X} - \mu_ {T Y}) ^ {2}} \\ \end{array}
$$

To simplify the expression, we let $a = \sigma _ { T X } \sigma _ { T Y }$ , $b = ( \sigma _ { T X } - \sigma _ { T Y } ) ^ { 2 } +$ $( \mu _ { T X } - \mu _ { T Y } ) ^ { 2 }$ ,     and then the ˡnal EED distance can be written as,

$$
E E D (T X, T Y) = \sqrt {2 a (1 - C (T X , T Y)) + b} \tag {10}
$$

When evaluating the quality of a subsequence $\begin{array} { r l } { S } & { { } = } \end{array}$ $\{ t _ { j } , t _ { j + 1 } , \dotsc , t _ { j + l - 1 } \}$ , we need to calculate the distance between $S$ and each time series $T = \{ t _ { 1 } , t _ { 2 } , \dots , t _ { m } \}$ in the time    series   dataset. The distance between them is deˡned as the minimum among the distances between the subsequence $S$ and all possible subsequences of $T$ that are of the same length as $S$ . We denote this distance as $s d i s t ( S , T )$ , as shown in Eq. (11).

$$
s d i s t (S, T) = \min  _ {1 \leq i \leq m - l} E E D (S, T _ {i, l}) \tag {11}
$$

EED does not necessitate that the data follow a standard normal distribution, thus eliminating the need for z-normalization. This ensures -that the key information and trends captured by DFR are preserved in tact, enabling more e˫ective mining of the time series data to identify and select the optimal set of U-shapelets.

# 4.3. Morph clustering process

Morph can be divided into three steps as follows: (1) using DFR to represent subsequences; (2) obtaining the U-shapelet candidate set; and (3) clustering. The detailed process of Morph is outlined in Algorithm 1.

Morph takes one input parameter, which is the length of $\hat { S }$ , denoted as 𝑠𝐿𝑒𝑛.

In Step 1, it slides a window of the same length as $\hat { S }$ to extract all subsequences of length 𝑠𝐿𝑒𝑛 from the dataset D (line 4). In lines 5–11, -DFR is used to convert the extracted subsequences into a discrete repre sentation, which helps capture the key information and morphological trends of the subsequences while reducing the e˫ect of noise. This step -converts the original subsequences into a more informative representa tion, which is crucial for obtaining a high-quality U-shapelet set in the subsequent process.

In Step 2, the algorithm ˡlters the discrete subsequences by using random masking and selects $1 ~ \%$ of the subsequences as the U-shapelet -candidate set (lines 4–6). The purpose of introducing random mask ing here is to ˡlter the discrete subsequences, thereby accelerating the extraction e˥ciency of U-shapelets. Here, roundNum represents the number of masking rounds, and the cCount matrix is used to store the collision counts for each masking round. Note that the values of roundNum $\ l = 1 0$ and the $1 ~ \%$ candidate subsequence ˡltering ratio are both consistent with the literature [14,38].

-In Step 3, the gap values of all subsequences in the U-shapelet candi date set are ˡrst calculated (lines 5–7), and then the subsequence with the largest gap value is selected as the current $\hat { S }$ (line 8). Next, the two subsets $D _ { A }$ and $D _ { B }$ are obtained from the $\hat { S }$ -partition, where the par tition is based on the criterion that $s d i s t ( \hat { S } , D _ { A } ) \ll s d i s t ( \hat { S } , D _ { B } )$ . $D _ { A }$ is

Algorithm 1: Morph (𝐷, 𝑠𝐿𝑒𝑛)   
Input: $D$ : dataset, sLen: the length of $\hat{S}$ Output: $C$ : a set of clusters   
1 $C\gets \emptyset$ Step 1: Discrete subsequences by DFR   
1 allUsh $\leftarrow$ []   
2 $w\gets$ window size   
3 Initialize two lists: listtemp and listdis   
4 Extract all sLen length subsequences from dataset $D$ and put them into allUsh.   
5 for each ush $\in$ allUsh do   
6 gmean $\leftarrow$ calculate gmean of ush using Eq. (2)   
7 for $j\gets 1$ to sLen- $w + 1$ do   
8 lmean $\leftarrow$ get lmean of ush $j,j + w$ by Eq. (3)   
9 listtemp $\leftarrow$ update listtemp through Eq. (4)   
10 listdis $\leftarrow$ update listdis by Eq. (5)   
11 ush $\leftarrow$ use Eq. (6) to process listdis   
Step 2: Get candidate subsequences by random masking   
1 seqnum $\leftarrow$ size(allUsh)   
2 roundNum $\leftarrow$ the number of masking rounds   
3 collisionCount $\leftarrow$ an (seqnum $\times$ roundNum) empty matrix   
4 for $r\gets 1$ to roundNum do   
5 collisionCount $(:,r)\gets$ the number of generated collisions //store in the $r^{th}$ column of the matrix   
6 allUsh $\leftarrow$ filter allUsh by collisionCount   
Step 3: Get the U-shapelet set and perform clustering   
1 ushNum $\leftarrow$ size(allUsh)   
2 repeat   
3 $\hat{S}\gets \emptyset$ 4 gap $\leftarrow 0$ Gap $\leftarrow$ []   
5 for each $S^{\prime}\in$ allUsh do   
6 gap $\leftarrow$ Calculate the gap value of $S^{\prime}$ by Eq. (1)   
7 Put gap into Gap   
8 $\hat{S}\gets$ allUsh(index(max(Gap)))   
9 Obtain the $D_{A}$ and $D_B$ of $\hat{S}$ partition, where $D_A\cup D_B = D$ 10 Put $D_{A}$ into C   
11 $D\gets D_B$ until the new $\hat{S}$ 's gap value is less than half of the first $\hat{S}$ 's gap value;   
12 Output C

treated as a separate cluster, and all the time series in $D _ { A }$ are removed from 𝐷. Continue searching for new $\hat { S }$ from the remaining time series until the iteration stop condition is met. In this step, employing EED to compute the similarity between time series is crucial for achieving a high-quality clustering result. EED accurately assesses the similarity between time series, forming the foundation for precise clustering.

# 4.4. Complexity analysis

Let $n$ be the number of time series and $m$ be the length of the time–series in a time series dataset $D$ .

In Step 1, the time complexity is $O ( n \cdot ( m - s L e n + 1 ) )$ , where $s L e n$ is the length of the subsequence. In Step 2, the time complexity is $O ( p r o j e c t i o n N u m \cdot s L e n ^ { 2 } )$ , where 𝑝𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛𝑁𝑢𝑚 is the number of rounds of random masking. The number of candidate subsequences remaining after ˡltering by random masking is $z = 0 . 0 1 \cdot n \cdot ( m - s L e n + 1 )$ .

In Step 3, the time complexity of calculating the distances of all candidate subsequences from $n$ time series is $O ( z \cdot s L e n \cdot m \cdot n )$ , and the time complexity of calculating the gap values of all candidate subsequences is $O ( z \cdot n )$ . Therefore, the time complexity of this step is $O ( z \cdot s L e n \cdot m \cdot n + z \cdot n )$ .

The total time complexity of Morph is $O ( n \cdot ( m - s L e n + 1 ) +$ 𝑝𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛𝑁𝑢𝑚 $\cdot s L e n ^ { 2 } + z \cdot s L e n \cdot m \cdot n + z \cdot n )$ ), where $s L e n \ \ll \ m , n$

and 𝑝𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛𝑁𝑢𝑚 $\ll m , n$ , thus the time complexity ˡnally reduces to $O ( n ^ { 2 } \cdot m ^ { 2 } )$ .

# 5. Experiments

We validated the performance of Morph with a variety of experiments conducted on a Windows 10 machine with 20 GB memory and 2.30 GHz CPU.

# 5.1. Experimental settings

# 5.1.1. Benchmarks

We evaluate Morph on 14 publicly available real-world datasets from the UCR time-series repository.1 The detailed information is summarized in Table 2. In the following experiments, the parameter 𝑠𝐿𝑒𝑛 of Morph ranges from $[ 5 , m / 2 ]$ , the window size of DFR is set to $w = 3$ , and the number of masking rounds is set to roundNum $= 1 0$ , where $m$ is the length of the time series.

# 5.1.2. Baselines

-We compare Morph with the 16 time series clustering algorithms be longing to four di˫erent types, respectively: (a) algorithms based on Ushapelet, including BruteForce [13], SUSH [14], USSL [39], FOTS [40], and USLM [41]; (b) algorithms based on feature selection, including UDFS [42], NDFS [43], RUFS [44], and RSFS [45]; (c) algorithms based on deep learning, including DEC [46], DSC [47], SDCN [48] and DTC [49]; and (d) traditional algorithms, including $k$ -shape [6], $D D _ { D T W }$ [50], and R-Clustering [7].

-Note that in our experiments, since the source codes are not pub licly available, the experimental results of the four algorithms based on feature selection are provided by [39], and the experimental result of $D D _ { D T W }$ [50] is taken from its original paper. $k$ -shape is implemented with the “scikit-learn” library of Python, while the remaining algorithm codes are provided by their authors. To achieve the best performance -of each comparison algorithm, this paper follows the parameter set tings speciˡed in the corresponding literature, and the algorithms with unstable results are tested 30 times to obtain their best results.

# 5.2. Comparison

We evaluate Morph against the baselines using the Rand Index (RI), and the experimental results on the 14 datasets are summarized in Table 3. Best results are highlighted in bold.

Morph consistently achieved the highest RI values on all datasets. -Compared to the 16 baseline methods, Morph shows an average im provement of $2 8 . 9 0 ~ \%$ in RI, with the highest improvement being $5 2 . 6 9 \ \%$ and the lowest $9 . 4 9 ~ \%$ . This demonstrates the signiˡcant advantages of Morph in time series clustering.

The baselines based on U-shapelet [13,14,39–41] outperform the other three types of baselines on all 14 datasets. For instance, on the COF dataset, the BruteForce and USSL achieved an RI of 1.0000, tying for ˡrst place with Morph. On the Beef, CHL, and MPA datasets, FOTS is ranked second, following Morph. This highlights the e˫ectiveness of -U-shapelet-based methods in using local time series features for clus tering. However, despite their advantages in time series analysis, these algorithms generally exhibit inferior clustering e˫ectiveness compared to Morph. Table 3 shows Morph’s average RI improvements of $2 6 . 7 7 \%$ , $2 1 . 2 6 \%$ , $1 7 . 0 3 \%$ , $1 4 . 2 8 \%$ , and $2 4 . 9 4 \%$ over the baselines BruteForce, SUSH, USSL, FOTS, and USLM, respectively, on the 14 datasets. This is due to Morph’s ability to accurately capture key information in time series through DFR and EED.

-The baselines relying on feature selection [42–45] exhibit supe rior performance on most datasets. For instance, both NDFS and RSFS achieve an RI of 1.0000 on the COF dataset, sharing the top spot with Morph. On the TLE dataset, RUFS attains an RI of 0.8246, securing the

Table 2 Description of the benchmarks.   

<table><tr><td>No.</td><td>Dataset</td><td>#CL</td><td>#EL</td><td>LEN</td><td>#TYPE</td><td>#NATURE</td></tr><tr><td>1</td><td>Beef (BEE)</td><td>5</td><td>60</td><td>470</td><td>Spectro</td><td>Each beef class represents a differing degree of contamination with offal.</td></tr><tr><td>2</td><td>Coffee (COF)</td><td>2</td><td>56</td><td>286</td><td>Spectro</td><td>The two classes of coffee are readings from Arabica and Robusta coffee variants.</td></tr><tr><td>3</td><td>Meat (MEA)</td><td>3</td><td>120</td><td>488</td><td>Spectro</td><td>FTIR spectra of chicken/turkey/pork (fresh/frozen).</td></tr><tr><td>4</td><td>Wine (WIN)</td><td>2</td><td>111</td><td>234</td><td>Spectro</td><td>FTIR spectra of two wine varieties.</td></tr><tr><td>5</td><td>Car (CAR)</td><td>4</td><td>120</td><td>577</td><td>Sensor</td><td>Multivariate sensor readings from 4 vehicle operation modes.</td></tr><tr><td>6</td><td>ChlorineConcentration (CHL)</td><td>3</td><td>4307</td><td>166</td><td>Sensor</td><td>Chlorine concentration measurements at 166 pipe junctions (5-min intervals, 15 days).</td></tr><tr><td>7</td><td>TwoLeadECG (TLE)</td><td>2</td><td>1162</td><td>82</td><td>ECG</td><td>MIT-BIH derived two-lead ECG segments (82 samples/sequence).</td></tr><tr><td>8</td><td>FaceAll (FAL)</td><td>14</td><td>2250</td><td>131</td><td>Image</td><td>Face outline sequences from 14 individuals.</td></tr><tr><td>9</td><td>OSULEaf (OSU)</td><td>6</td><td>442</td><td>427</td><td>Image</td><td>Leaf contours from digitized Acer/Quercus images (anti-clockwise boundary extraction).</td></tr><tr><td>10</td><td>ProximalPhalanxTW (PPT)</td><td>6</td><td>605</td><td>80</td><td>Image</td><td>The outlines of three bones of the middle finger (proximal, middle and distal phalanges) were extracted from images.</td></tr><tr><td>11</td><td>MiddlePhalanxOutlineAgeGroup (MPA)</td><td>3</td><td>544</td><td>80</td><td>Image</td><td>Middle phalanx contours for 3 age groups.</td></tr><tr><td>12</td><td>Haptics (HAP)</td><td>5</td><td>463</td><td>1092</td><td>Motion</td><td>Touchscreen passgraph trajectories from 5 subjects.</td></tr><tr><td>13</td><td>ToeSegmentation2 (TS2)</td><td>2</td><td>166</td><td>343</td><td>Motion</td><td>Data are taken from labeling the movement trajectories of toes.</td></tr><tr><td>14</td><td>GunPoint (GUN)</td><td>2</td><td>200</td><td>150</td><td>Motion</td><td>X-axis trajectories of hand motions (gun draw vs. finger pointing).</td></tr></table>

#CL: Number of clusters. #EL: Number of elements. #LEN: Time series length. #TYPE: Dataset type.

third position and trailing second place by only 0.0028. However, on the HAP dataset, NDFS and RUFS exhibit RI values of 0.3521 and 0.2007, respectively. This may be attributed to their struggle in capturing the complex structure of time series, resulting in inadequate feature selection for clustering. In contrast, Morph outperforms these algorithms on all the 14 datasets, with mean RI values improved by $3 3 . 6 4 \%$ , $2 6 . 0 1 \%$ $3 0 . 9 7 \ \%$ , and $2 8 . 2 5 ~ \%$ , respectively. This advantage can be ascribed to -Morph’s comprehensive consideration of both local and global informa tion in time series, enabling it to derive high-quality U-shapelets that e˫ectively represent the time series.

The deep learning-based baselines [46–49] generally show poorer performance compared to other methods, with DTC ranking second on the PPT dataset, just behind Morph. Speciˡcally, DSC and SDCN achieve mean RI values of 0.5195 and 0.5121, respectively, notably lower than the other baselines. This can be attributed to the inherent complexity of the deep learning models employed by DSC and SDCN. These deep learning models often involve intricate parameter conˡgurations and -lack robust interpretability. Without su˥cient data and training, accu rately capturing time series features and performing e˫ective clustering becomes challenging. In contrast, Morph features simpler parameter settings, strong interpretability, and e˫ective time series clustering.

The traditional methods [6,7,50], particularly $D D _ { D T W }$ , perform less e˫ectively compared to Morph, achieving a mean RI value of 0.5725 -on the 14 datasets, which places it third from last among the 15 base lines. In contrast, Morph improves mean RI values on these datasets by $2 7 . 0 8 ~ \%$ , $3 6 . 5 8 ~ \%$ , and $9 . 4 9 ~ \%$ compared to $k$ -shape, $D D _ { D T W }$ , and R-Clustering, respectively. This is because traditional methods are unable to comprehensively capture the global and local features of time series with complex structures, while Morph is able to more accurately capture the key information and morphological trends of time series through DFR.

Overall, Morph demonstrates strong e˫ectiveness compared to other methods by e˫ectively capturing key information and morphological -trends of time series through DFR. Additionally, EED ensures a more ac curate measurement of similarity between time series. Moreover, Morph excels at e˫ectively extracting high-quality U-shapelets.

# 5.3. Evaluation of EED e˫ectiveness

To validate the e˫ectiveness of EED, we conduct a classiˡcation performance test using the nearest neighbor classiˡer (1NN) on the 14 datasets. The experiments compare EED against four other widely

used distances, including Euclidean Distance (ED), length-normalized Euclidean Distance (lnED), Dynamic Time Warping (DTW), and Shape-Based Distance (SBD) [6]. To fairly compare the ˡve distances, we use identical train/test splits for all the datasets. The classiˡcation accuracy results are presented and compared in Fig. 5.

EED consistently achieves the highest accuracy on all the 14 datasets, establishing itself as the top performer. Notably, lnED tied with EED for ˡrst place on the COF, MEA, and WIN datasets, possibly due to their normal-like data distribution. When using lnED, the data shape is not a˫ected by z-normalization, thus the classiˡcation accuracy is higher. On datasets CHL and FAL, the minimal di˫erence in classiˡcation accuracies between the four distances and EED can be attributed primarily to the low levels of noise present in these datasets. When noise levels are low, the underlying patterns and structures in the data are clearer and more distinguishable.

In conclusion, EED demonstrates outstanding performance under various data distributions and noise conditions, making it suitable for a wide range of data analysis and classiˡcation tasks.

# 5.4. DFR vs SAX

In this experiment, we delve into the validity of DFR in capturing the key information and morphological trends of time series, as well as reducing noise, by comparing the e˫ectiveness of DFR and SAX in representing time series data.

# 5.4.1. Comparison between DFR and SAX

-We take the Beef dataset as an example to visually display and ana lyze the original dataset and the datasets represented by DFR and SAX. Fig. 6(a)–(c) shows each cluster individually in the form of a subgraph, -where di˫erent clusters are distinguished by colors. Meanwhile, to ob serve the local features more clearly, the key interval of [300–370] is magniˡed in Fig. 7. The ˡgure follows a $3 \times 5$ subˡgure layout, where the ˡrst column displays the original data visualization, and the second and third columns show the results of the SAX and DFR representations, respectively. Each subˡgure adopts a dual-view design, with the upper part showing the trend of the complete series and the lower part focusing on the details of the zoomed-in intervals.

From Fig. 6(a), it is evident that the topmost red cluster between -horizontal coordinates 300–400 exhibits both peaks and troughs, distin guishing it distinctly from the other four clusters. As demonstrated in Figs. 6(b), (c) and 7, the SAX representation fails to e˫ectively capture

Table 3 RI comparisons on the 14 benchmarks.   

<table><tr><td rowspan="3">Datasets</td><td colspan="15">Rand index</td><td></td><td></td></tr><tr><td colspan="6">U-shapelet method</td><td colspan="4">Feature selection method</td><td colspan="4">Deep learning method</td><td colspan="3">Traditional method</td></tr><tr><td>Morph</td><td>BruteForce</td><td>SUSH</td><td>USSL</td><td>FOTS</td><td>USLM</td><td>UDFS</td><td>NDFS</td><td>RUFS</td><td>RSFS</td><td>DEC</td><td>DSC</td><td>SDCN</td><td>DTC</td><td>k-shape</td><td>DDdtw</td><td>R-clustering</td></tr><tr><td>BEE</td><td>0.8073</td><td>0.6870</td><td>0.7446</td><td>0.6401</td><td>0.7514</td><td>0.6718</td><td>0.6759</td><td>0.7034</td><td>0.7149</td><td>0.6975</td><td>0.5954</td><td>0.5220</td><td>0.5731</td><td>0.6345</td><td>0.5402</td><td>0.5825</td><td>0.6774</td></tr><tr><td>COF</td><td>1.0000</td><td>1.0000</td><td>0.8968</td><td>1.0000</td><td>0.9000</td><td>0.7916</td><td>0.8624</td><td>1.0000</td><td>0.5476</td><td>1.0000</td><td>0.4921</td><td>0.5167</td><td>0.6130</td><td>0.4841</td><td>0.7460</td><td>0.4914</td><td>0.7737</td></tr><tr><td>MEA</td><td>0.8315</td><td>0.6742</td><td>0.5359</td><td>0.7440</td><td>0.6500</td><td>0.6277</td><td>0.6483</td><td>0.6635</td><td>0.6578</td><td>0.6657</td><td>0.6475</td><td>0.3029</td><td>0.3015</td><td>0.3220</td><td>0.6575</td><td>0.7702</td><td>0.8271</td></tr><tr><td>WIN</td><td>0.6164</td><td>0.4975</td><td>0.4975</td><td>0.4958</td><td>0.5200</td><td>0.4988</td><td>0.4987</td><td>0.5123</td><td>0.5021</td><td>0.5033</td><td>0.4913</td><td>0.4836</td><td>0.4905</td><td>0.4906</td><td>0.5011</td><td>0.4992</td><td>0.5073</td></tr><tr><td>CAR</td><td>0.7469</td><td>0.4136</td><td>0.4136</td><td>0.7345</td><td>0.7171</td><td>0.5520</td><td>0.6757</td><td>0.6260</td><td>0.6667</td><td>0.6708</td><td>0.6859</td><td>0.5020</td><td>0.5739</td><td>0.6695</td><td>0.7028</td><td>0.4982</td><td>0.7318</td></tr><tr><td>CHL</td><td>0.5818</td><td>0.5298</td><td>0.4953</td><td>0.4997</td><td>0.5687</td><td>0.5353</td><td>0.5282</td><td>0.5225</td><td>0.5330</td><td>0.5316</td><td>0.5348</td><td>0.5225</td><td>0.4210</td><td>0.5353</td><td>0.4111</td><td>0.4034</td><td>0.5080</td></tr><tr><td>TLE</td><td>0.8343</td><td>0.5404</td><td>0.5010</td><td>0.8271</td><td>0.5500</td><td>0.7427</td><td>0.5495</td><td>0.6328</td><td>0.8246</td><td>0.5635</td><td>0.5007</td><td>0.5152</td><td>0.4977</td><td>0.5116</td><td>0.8246</td><td>0.5008</td><td>0.8090</td></tr><tr><td>FAL</td><td>0.9029</td><td>0.9011</td><td>0.8861</td><td>0.7206</td><td>0.8401</td><td>0.7023</td><td>0.5671</td><td>0.8652</td><td>0.8247</td><td>0.4982</td><td>0.8214</td><td>0.5826</td><td>0.5789</td><td>0.6658</td><td>0.7825</td><td>0.6048</td><td>0.8968</td></tr><tr><td>OSU</td><td>0.7818</td><td>0.5525</td><td>0.5239</td><td>0.6551</td><td>0.7548</td><td>0.5872</td><td>0.5372</td><td>0.5622</td><td>0.5497</td><td>0.5665</td><td>0.7484</td><td>0.6465</td><td>0.6021</td><td>0.7329</td><td>0.5538</td><td>0.6197</td><td>0.7782</td></tr><tr><td>PPT</td><td>0.8609</td><td>0.7812</td><td>0.8232</td><td>0.7282</td><td>0.7612</td><td>0.7736</td><td>0.4947</td><td>0.6053</td><td>0.5579</td><td>0.5211</td><td>0.8189</td><td>0.6071</td><td>0.4997</td><td>0.8380</td><td>0.5211</td><td>0.7682</td><td>0.7849</td></tr><tr><td>MPA</td><td>0.7424</td><td>0.5396</td><td>0.7005</td><td>0.5807</td><td>0.7382</td><td>0.5748</td><td>0.5269</td><td>0.5350</td><td>0.5315</td><td>0.5473</td><td>0.7059</td><td>0.4935</td><td>0.5006</td><td>0.5757</td><td>0.5105</td><td>0.7290</td><td>0.7327</td></tr><tr><td>HAP</td><td>0.7474</td><td>0.3888</td><td>0.6340</td><td>0.5134</td><td>0.7088</td><td>0.5458</td><td>0.5982</td><td>0.3521</td><td>0.2007</td><td>0.7011</td><td>0.5124</td><td>0.5584</td><td>0.5223</td><td>0.5710</td><td>0.7088</td><td>0.3891</td><td>0.6701</td></tr><tr><td>TS2</td><td>0.8197</td><td>0.5020</td><td>0.7981</td><td>0.6778</td><td>0.4800</td><td>0.6508</td><td>0.5257</td><td>0.5968</td><td>0.5968</td><td>0.5826</td><td>0.4991</td><td>0.5207</td><td>0.5049</td><td>0.5348</td><td>0.5257</td><td>0.6605</td><td>0.6780</td></tr><tr><td>GUN</td><td>0.6730</td><td>0.6278</td><td>0.5773</td><td>0.5363</td><td>0.6382</td><td>0.5066</td><td>0.5029</td><td>0.5102</td><td>0.6498</td><td>0.4994</td><td>0.4981</td><td>0.4991</td><td>0.4908</td><td>0.5400</td><td>0.6278</td><td>0.4980</td><td>0.6218</td></tr><tr><td>Average</td><td>0.7819</td><td>0.6168</td><td>0.6448</td><td>0.6681</td><td>0.6842</td><td>0.6258</td><td>0.5851</td><td>0.6205</td><td>0.5970</td><td>0.6106</td><td>0.6109</td><td>0.5195</td><td>0.5121</td><td>0.5790</td><td>0.6153</td><td>0.5725</td><td>0.7141</td></tr><tr><td>(improvement)</td><td></td><td>(26.77 %)</td><td>(21.26 %)</td><td>(17.03 %)</td><td>(14.28 %)</td><td>(24.94 %)</td><td>(33.64 %)</td><td>(26.01 %)</td><td>(30.97 %)</td><td>(28.05 %)</td><td>(27.99 %)</td><td>(50.51 %)</td><td>(52.69 %)</td><td>(35.04 %)</td><td>(27.08 %)</td><td>(36.58 %)</td><td>(9.49 %)</td></tr></table>

![](images/f10d095f1d73006804844f511a46ec73967cd67c6de6438e01ac3705fcdc43d2.jpg)  
Fig. 5. Classiˡcation results of 1NN under di˫erent distances.

![](images/aed1fad7b21e0893416fa8ad5b7ad09e330cce454a37b1db063849a933f59dcd.jpg)  
(a) Original dataset (subplot)

![](images/8abf72fe955c5f0626f7a83ea043916913a4162a3383a03ee136dc999e1ec76b.jpg)  
(b) SAX representation (subplot)

![](images/8549c5f67f47177395978dc8d2103b2f80fb15941d7b9e209d75d82297eb9dfd.jpg)  
(c)DFR representation (subplot)   
Fig. 6. Class-wise comparison of DFR and SAX representations on the Beef dataset.

these local variation trends, whereas the DFR representation successfully preserves them. Speciˡcally, around horizontal coordinate 390, the SAX representation fails to accurately reˣect the local ˣuctuations of each cluster, while the DFR representation accurately depicts the trend in this critical region. Similarly, beyond the horizontal coordinate 400, we are unable to observe the local variations of the original data from the SAX representation. In contrast, the DFR retains important variation trends, allowing subtle di˫erences between di˫erent clusters to be preserved and displayed e˫ectively.

Thus, DFR has a signiˡcant advantage in capturing the key information and morphological trends in the time series.

# 5.4.2. Noise reduction e˫ect veriˡcation

To demonstrate the superior noise reduction capabilities of DFR -compared to SAX, this experiment systematically investigates the rela tionships between masking rounds and collision counts under the same 𝑠𝐿𝑒𝑛 after applying DFR and SAX representations to the BEE, WIN, and MEA datasets. As noise increases, random masking typically leads to more collisions.

In this experiment, the 𝑠𝐿𝑒𝑛 ranges from 10 to 23. The results for SAX and DFR are presented in Fig. 8(a)–(c) and (d)–(f), respectively. -It is evident from these ˡgures that SAX consistently generates signiˡ cantly higher collision counts compared to DFR, despite using the same 𝑠𝐿𝑒𝑛 and number of masking rounds on these three datasets. Moreover, as the number of masking rounds increases, both SAX and DFR exhibit a gradual decline in newly added collisions. However, DFR demonstrates

a more pronounced reduction e˫ect, often achieving 0 newly added collisions by the ˡnal masking round.

Overall, this experiment substantiates DFR’s superior ability over SAX in mitigating noise within time series, highlighting its potential for enhancing data quality in various applications.

# 5.5. Sensitivity to the choice of the sLen

-In Morph, the only parameter is 𝑠𝐿𝑒𝑛. This experiment aims to ex plore the RI values of Morph while varying 𝑠𝐿𝑒𝑛 on the datasets BEE and MEA, and compare them with those obtained using the BruteForce and SUSH algorithms. BruteForce is chosen for comparison because it is the original U-shapelet algorithm, and SUSH is included because it also employs random masking. 𝑠𝐿𝑒𝑛 ranges from 10 to $1 0 ~ \%$ of the length of the time series.

As depicted in Fig. 9(a) and (b), the RI values of BruteForce and SUSH exhibit substantial ˣuctuations with changes in 𝑠𝐿𝑒𝑛. Morph shows much less ˣuctuation than BruteForce and SUSH, demonstrating a more -stable pattern. This shows that Morph is able to maintain a more con sistent performance with di˫erent $s L e n$ parameters. In contrast, SUSH ˣuctuates more than BruteForce, showing the worst stability. This is -mainly because SUSH employs random masking in the process of ob taining the U-shapelet candidate set, which introduces uncertainty and thus leads to greater ˣuctuations. Although Morph also employs the same random masking as SUSH, it exhibits much higher stability. This is mainly because DFR e˫ectively reduces the e˫ect of noise, thereby

![](images/dce5fe4ced82361bcbfdf0abb5935b0bcc8f7f8388f21cff949c2a89b80bcbe7.jpg)

![](images/938dd371284704d383ec5d7de0b9f16ae1ccf2819f87c42a27d4ba3c453141b9.jpg)

![](images/b8c3d126cf0e6f7274dadcb3ec4681af6fe3b4f250178b1da0097d9b74bed9b8.jpg)

![](images/6a10ce9c40b41f6bc82c7df3b62ae5c999ea96a375f185c938849b927ae950f8.jpg)

![](images/7eca3e772b7c5bad08c5bc6ea1d8aa85b6118bf3b14c865850516951cd7a72d9.jpg)

![](images/28a8b7632c8014e9078041efd62bb05dbf15b9ed67f6b6c4cb3985438501390d.jpg)

![](images/6499fdb762ea69ed7eb5245da4b37d9ea0edc2a7a322f17c7f1ea4feb770919b.jpg)

![](images/e5f604734b71384bb8d666a3000efdfa34010c7ca82d4b88ca4775973044b61b.jpg)

![](images/8761a80614e4c996599236e166db5064208c70d809d52cc90cfd575ff8bfcd26.jpg)

![](images/767ccb00a95703403742879827bf9dfcc9257f2bbd11c2d938c793403b8d2a72.jpg)

![](images/3d2585c66513c51df589b85e35b96722dd07243aa0fa78ede0faeafbe26af191.jpg)

![](images/2c3dbfba9836bd0ac3ebab283818633f9fc023eb8fbca580719268cb24668491.jpg)

![](images/871338832110bfa490762555977b2ca9dcd2d4c9a7cba35f9b0fd6c4cac15a58.jpg)

![](images/d47c7beb2629d42b2f78d2d3b8df8852c86254d97d3f00b6268e9a0edf18a38e.jpg)

![](images/f4cc1d038936c1da945c68c5154510a65a8627f88e03919e511cf512c34b39df.jpg)  
(a) Original dataset   
(b) SAX representation   
(c) DFR representation   
Fig. 7. Class-wise comparison of DFR and SAX representations in the intervals [300, 370] on the Beef dataset.

decreasing the number of conˣicts that must be counted generated by random masking, resulting in a more stable overall operation of Morph.

# 5.6. Ablation experiment

In this subsection, to empirically investigate the e˫ectiveness of DFR and EED, we conduct a thorough ablation study on the 14 datasets. There are four groups in our experiment: (a) $\mathrm { D F R + E E D }$ , (b) $\mathrm { D F R + l n E D }$ , (c) SAX+EED, and (d) $\mathbf { S } \mathbf { A } \mathbf { X } + \mathbf { l n } \mathbf { E } \mathbf { D }$ , all of them running the Morph algorithm. Experimental results are shown in Table 4, with the best results highlighted in bold.

Group (a) has the best performance among all the groups, with the highest RI values on all the 14 datasets, further verifying the positive impact of the combination of DFR and EED on the overall performance. -First, DFR can e˫ectively present the key information and morpholog ical trends of time series, and reduce the inˣuence of noise. Second,

z-normalization is not required when using EED, which ensures that the key information and trends captured by DFR are retained intact, -enabling more e˫ective mining of time series data. Therefore, the com bination of DFR and EED presents a synergistic e˫ect and signiˡcantly improves the performance of the clustering.

When using the same distance, the results of group (b) are all better than those of group (d). This is mainly because DFR better preserves the key information of time series compared to SAX. On 13 of the 14 datasets, the RI values of group (b) are signiˡcantly higher than those of group (d). The only exception is on the FAL dataset, where the di˫erence between the results of the two groups is small. This could be attributed to the FAL dataset having less noise and a simpler structure, which allows SAX to e˫ectively capture the key information of the time series.

Group (c) consistently outperforms group (d), indicating that EED is -more e˫ective than lnED in measuring the similarity of time series rep resented by SAX. The RI values on the datasets CHL, FAL, and PPT show

![](images/12c637820d278a5d75ed518a50160a55bf8e9f658e72b9fd2d79f2b13fe82339.jpg)  
(a) SAX on dataset BEE

![](images/ea0dee39399ed25657851edbe6419943435a0ddec4eb7ff9353f888d4aa54b7f.jpg)  
(b) SAX on dataset WIN

![](images/61b6d9efd83803b2ed8fdf107153e21c2ce7b2332e11464b6786f5112b492dd5.jpg)  
(c) SAX on dataset MEA

![](images/14f5cc7320ff8223c4c0e54d95ce71989df5512c2d2de847375049dd0cdd8ac2.jpg)  
(d) DFR on dataset BEE

![](images/aa8e51588775db0dc522393fb38fdcc13f4e5af239c2f72eef461931b2a6921e.jpg)  
(e) DFR on dataset WIN

![](images/a9d9c174aecb0eb5c88f6aae33394da317355cb3b1f77d2cd9bc26be34d72807.jpg)  
(f) DFR on dataset MEA

![](images/f73abc0c0a8bc0402b586801d8b4160d2f0ebbda52cde27bd8bc1cb25bc650ce.jpg)  
Fig. 8. Collision numbers in DFR and SAX methods.   
(a) BEE dataset

![](images/abc4cbcc5e26b513db37a4b33f3eceadcbfb4d294ed9544f3f3a2df9cf78f804.jpg)  
(b) MEA dataset   
Fig. 9. RI changes of Morph, BruteForce and SUSH with varying 𝑠𝐿𝑒𝑛 on BEE and MEA datasets.

minor di˫erences, which could be attributed to two main factors: (1) these three datasets have less noise, and (2) these three datasets approximate a normal distribution, reducing the impact of z-normalization. On the other 11 datasets, the results of group (c) are signiˡcantly superior -to those of group (d). This is mainly because EED does not neces sitate z-normalization, thereby preserving the integrity of the feature information captured from the time series by SAX.

-In summary, the combination of DFR and EED has signiˡcant advan tages in processing time series data.

# 5.7. Speedup evaluation

In this experiment, we compare the running time of the proposed Morph algorithm with that of the original U-shapelet-based algorithm BruteForce and the USLM algorithm [41] on the 14 datasets. Here, USLM is a new unsupervised-shapelet learning model that can automatically

learn the most discriminative features from unlabeled time-series data by combining the strengths of pseudo-class labeling, spectral analysis, shapelet regularization, and regularized least squares miniaturization. The results are presented in Table 5.

We can observe that Morph signiˡcantly reduces the computation time compared to both baseline algorithms on all datasets. Compared -with BruteForce, Morph’s computational speed increases by an aver age of $4 1 . 0 4 ~ \%$ . The maximum and minimum speedups are $9 0 . 5 9 ~ \%$ and $4 . 3 6 ~ \%$ , respectively. Meanwhile, compared with USLM, Morph’s computational speed ranging from $1 . 3 3 ~ \%$ to $7 . 2 4 ~ \%$ , suggests that the Morph’s advantage over USLM is also signiˡcant.

The performance advantage varies depending on the noise level of the dataset. On datasets with high noise levels, such as TLE, COF, -and MEA, the speedup over both baselines is relatively lower. This oc curs because although DFR helps reduce noise impact, random masking still generates some conˣicts that require computation. Conversely, on

Table 4 Ablation experiment.   

<table><tr><td rowspan="2">Datasets</td><td colspan="4">Rand index</td></tr><tr><td>(a) DFR + EED</td><td>(b) DFR + InED</td><td>(c) SAX + EED</td><td>(d) SAX + InED</td></tr><tr><td>BEE</td><td>0.8073</td><td>0.7708</td><td>0.7729</td><td>0.7446</td></tr><tr><td>COF</td><td>1.0000</td><td>0.9643</td><td>0.9643</td><td>0.8968</td></tr><tr><td>MEA</td><td>0.8315</td><td>0.7653</td><td>0.7993</td><td>0.5359</td></tr><tr><td>WIN</td><td>0.6164</td><td>0.5427</td><td>0.5091</td><td>0.4980</td></tr><tr><td>CAR</td><td>0.7469</td><td>0.7058</td><td>0.6954</td><td>0.4136</td></tr><tr><td>CHL</td><td>0.5818</td><td>0.5346</td><td>0.5117</td><td>0.4953</td></tr><tr><td>TLE</td><td>0.8343</td><td>0.7588</td><td>0.5373</td><td>0.5010</td></tr><tr><td>FAL</td><td>0.9029</td><td>0.8869</td><td>0.8988</td><td>0.8861</td></tr><tr><td>OSU</td><td>0.7818</td><td>0.5413</td><td>0.7599</td><td>0.5239</td></tr><tr><td>PPT</td><td>0.8609</td><td>0.8466</td><td>0.8254</td><td>0.8232</td></tr><tr><td>MPA</td><td>0.7331</td><td>0.7331</td><td>0.7292</td><td>0.7005</td></tr><tr><td>HAP</td><td>0.7474</td><td>0.7158</td><td>0.6447</td><td>0.6340</td></tr><tr><td>TS2</td><td>0.8197</td><td>0.8167</td><td>0.8167</td><td>0.7981</td></tr><tr><td>GUN</td><td>0.6730</td><td>0.5809</td><td>0.6328</td><td>0.5773</td></tr></table>

Table 5 Running time and speedup.   

<table><tr><td rowspan="2">Datasets</td><td>Morph (min)</td><td colspan="2">BruteForce (min)</td><td colspan="2">USLM (min)</td></tr><tr><td>Time</td><td>Time</td><td>Speedup</td><td>Time</td><td>Speedup</td></tr><tr><td>BEE</td><td>0.23</td><td>11.57</td><td>50.30</td><td>1.36</td><td>5.91</td></tr><tr><td>COF</td><td>0.18</td><td>3.04</td><td>16.89</td><td>0.24</td><td>1.33</td></tr><tr><td>MEA</td><td>1.21</td><td>22.68</td><td>18.74</td><td>4.28</td><td>3.54</td></tr><tr><td>WIN</td><td>0.34</td><td>12.57</td><td>36.97</td><td>1.65</td><td>4.86</td></tr><tr><td>CAR</td><td>1.91</td><td>59.01</td><td>30.90</td><td>3.27</td><td>1.71</td></tr><tr><td>CHL</td><td>269.55</td><td>16,896.67</td><td>62.68</td><td>1950.59</td><td>7.24</td></tr><tr><td>TLE</td><td>1.73</td><td>7.55</td><td>4.36</td><td>6.95</td><td>4.01</td></tr><tr><td>FAL</td><td>48.66</td><td>3534.14</td><td>72.63</td><td>157.97</td><td>3.25</td></tr><tr><td>OSU</td><td>7.54</td><td>435.86</td><td>57.81</td><td>37.93</td><td>5.03</td></tr><tr><td>PPT</td><td>0.85</td><td>32.55</td><td>38.29</td><td>1.63</td><td>1.92</td></tr><tr><td>MPA</td><td>1.09</td><td>27.50</td><td>25.23</td><td>3.49</td><td>3.20</td></tr><tr><td>HAP</td><td>48.63</td><td>4405.57</td><td>90.59</td><td>74.02</td><td>1.52</td></tr><tr><td>TS2</td><td>0.25</td><td>10.80</td><td>43.20</td><td>0.68</td><td>2.72</td></tr><tr><td>GUN</td><td>0.47</td><td>12.20</td><td>25.96</td><td>1.97</td><td>4.19</td></tr></table>

datasets with lower noise levels, similar subsequences are more likely to share the same DFR representation, resulting in fewer new collisions during random masking and consequently higher speedups. Notably, Morph demonstrates particularly strong performance on large datasets (e.g., CHL), where it maintains signiˡcant speed advantages over both BruteForce and USLM.

# 6. Conclusion

In this paper, we propose Morph for accurate clustering with low time complexity. By introducing a novel representation method, DFR, and a new distance measure, EED, tailored for time series data, Morph is able to e˫ectively capture key information and morphological trends, as well as accurately measure the similarity between the time series, thereby obtaining a high-quality U-shapelet set. In addition, Morph accelerates U-shapelet selection by using random masking. The experiments show that Morph surpasses the 15 baselines in clustering performance and is user-friendly due to parameter insensitivity. They -also conˡrm the e˫ectiveness of DFR and EED, which signiˡcantly en hance clustering performance when used together. Additionally, Morph -demonstrates superior computational e˥ciency compared to the tradi tional BruteForce algorithm. In our future work, we will explore more e˫ective ways to obtain a high-quality U-shapelet set.

# CRediT authorship contribution statement

Mei Chen: Writing – review & editing, Supervision, Methodology, Investigation. Yu Wang: Writing – review & editing, Writing – original -draft, Methodology, Formal analysis. Mingwei Leng: Writing – re view & editing, Methodology. Boya Liu: Validation, Conceptualization.

Yuanyuxiu You: Writing – review & editing, Software. Yiying Yao: Writing – review & editing, Software. Song Wang: Writing – review & editing, Software.

# Declaration of competing interest

The authors declare that they have no known competing ˡnancial interests or personal relationships that could have appeared to inˣuence the work reported in this paper.

# Acknowledgements

We are hugely grateful to the authors who provide their source codes for us. This research is supported by National Natural Science Foundation of China (No.62266029), Gansu Higher Education Industry Support Program, China (No.2022CYZC-36) and Gansu Key Research and Development Program, China (No.24YFGA036).

# Data availability

Data will be made available upon request.

# References

[1] H. Zheng, J. Wu, R. Song, L. Guo, Z. Xu, Predicting ˡnancial enterprise stocks and economic data trends using machine learning time series analysis, Appl. Comput. Eng. 87 (1) (2024) 26–32.   
[2] G. Sunil, S. Gowtham, A. Bose, S. Harish, G. Srinivasa, Graph neural network and machine learning analysis of functional neuroimaging for understanding schizophrenia, BMC Neurosci. 25 (1) (2024) 2.   
[3] Z. Hu, Y. Gao, S. Ji, M. Mae, T. Imaizumi, Improved multistep ahead photovoltaic power prediction model based on LSTM and self-attention with weather forecast data, Appl. Energy 359 (2024) 122709.   
-[4] M. Dissem, M. Amayri, N. Bouguila, Neural architecture search for anomaly detec tion in time-series data of smart buildings: a reinforcement learning approach for optimal autoencoder design, IEEE Internet. Things J. 11 (10) (2024) 18059–18073.   
[5] S. Aghabozorgi, A.S. Shirkhorshidi, T.Y. Wah, Time-series clustering–a decade review, Inf. Syst. 53 (2015) 16–38.   
[6] J. Paparrizos, L. Gravano, K-shape: e˥cient and accurate clustering of time series, in: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, 2015, pp. 1855–1870.   
[7] M.-B. Jorge, C. Rubén, Time series clustering with random convolutional kernels, Data Min. Knowl. Discov. (2024) 1–27.   
[8] X. Wang, R. Song, J. Xiao, T. Li, X. Li, Accelerating k-shape time series clustering algorithm using GPU, IEEE Trans. Parallel Distrib. Syst. 34 (10) (2023) 2718–2734, https://doi.org/10.1109/TPDS.2023.3298148.   
[9] C. Holder, M. Middlehurst, A. Bagnall, A review and evaluation of elastic distance functions for time series clustering, Knowl. Inf. Syst. 66 (2) (2024) 765–809.   
-[10] Y. Pan, M. Wu, L. Zhang, J. Chen, Time series clustering-enabled geological con dition perception in tunnel boring machine excavation, Autom. Constr. 153 (2023) 104954.   
[11] J. Li, R. Ma, M. Deng, X. Cao, X. Wang, X. Wang, A comparative study of clustering algorithms for intermittent heating demand considering time series, Appl. Energy 353 (2024) 122046.   
-[12] J.F. Vera, J.M. Angulo, An mds-based unifying approach to time series k-means clus tering: application in the dynamic time warping framework, Stoch. Environ. Res. Risk Assess. 37 (12) (2023) 4555–4566.   
[13] J. Zakaria, A. Mueen, E. Keogh, Clustering time series using unsupervised-shapelets, in: 2012 IEEE 12th International Conference on Data Mining, IEEE, 2012, pp. 785–794.   
[14] L. Ulanova, N. Begum, E. Keogh, Scalable clustering of time series with U-shapelets, in: Proceedings of the 2015 SIAM International Conference on Data Mining, SIAM, 2015, pp. 900–908.   
[15] Q. Wang, V. Megalooikonomou, G. Li, A symbolic representation of time series, in: Proceedings of the Eighth International Symposium on Signal Processing and Its Applications, 2005., vol. 2, Citeseer, 2005, pp. 655–658.   
-[16] J. Lin, E. Keogh, L. Wei, S. Lonardi, Experiencing sax: a novel symbolic representa tion of time series, Data Min. Knowl. Discov. 15 (2) (2007) 107–144.   
[17] K. Zhang, Y. Li, Y. Chai, L. Huang, Trend-based symbolic aggregate approximation for time series representation, in: 2018 Chinese Control and Decision Conference (CCDC), IEEE, 2018, pp. 2234–2240.   
[18] J. Zakaria, A. Mueen, E. Keogh, N. Young, Accelerating the discovery of unsupervised-shapelets, Data Min. Knowl. Discov. 30 (1) (2016) 243–281.   
[19] L. Luo, S. Lv, An accelerated U-shapelet time series clustering method with LSH, in: Journal of Physics: Conference Series, vol. 1631, IOP Publishing, 2020, pp. 012077.   
[20] S. Yu, Q. Yan, X. Yan, Improving U-shapelets clustering performance: an shapelets quality optimizing method, Int. J. Hybrid Inf. Technol. 10 (4) (2017) 27–40.   
-[21] B. Cai, G. Huang, S. Yang, Y. Xiang, C.-H. Chi, Se-shapelets: semi-supervised clus tering of time series using representative shapelets, Expert Syst. Appl. 240 (2024) 122584.

[22] R. Agrawal, C. Faloutsos, A. Swami, E˥cient similarity search in sequence databases, in: Foundations of Data Organization and Algorithms: 4th International Conference, FODO’93 Chicago, Illinois, USA, October 13–15, 1993 Proceedings 4, Springer, 1993, pp. 69–84.   
[23] E. Keogh, K. Chakrabarti, M. Pazzani, S. Mehrotra, Dimensionality reduction for fast similarity search in large time series databases, Knowl. Inf. Syst 3 (3) (2001) 263–286.   
[24] C. Guo, H. Li, D. Pan, An improved piecewise aggregate approximation based on statistical features for time series mining, in: Knowledge Science, Engineering and Management: 4th International Conference, KSEM 2010, Belfast, Northern Ireland, UK, September 1-3, 2010. Proceedings 4, Springer, 2010, pp. 234–244.   
[25] K.-P. Chan, A.W.-C. Fu, E˥cient time series matching by wavelets, in: Proceedings 15th International Conference on Data Engineering (Cat. No. 99CB36337), IEEE, 1999, pp. 126–133.   
[26] Y. Sun, J. Li, J. Liu, B. Sun, C. Chow, An improvement of symbolic aggregate approximation distance measure for time series, Neurocomputing 138 (2014) 189–198.   
[27] M. Azzouzi, I.T. Nabney, Analysing time series structure with hidden Markov models, in: Neural Networks for Signal Processing VIII. Proceedings of the 1998 IEEE Signal Processing Society Workshop (Cat. No. 98TH8378), IEEE, 1998, pp. 402–408.   
[28] K. Kalpakis, D. Gada, V. Puttagunta, Distance measures for e˫ective clustering of ARIMA time-series, in: Proceedings 2001 IEEE International Conference on Data Mining, IEEE, 2001, pp. 273–280.   
-[29] P. Sebastiani, M. Ramoni, P. Cohen, J. Warwick, J. Davis, Discovering dynamics us ing bayesian clustering, in: Advances in Intelligent Data Analysis: Third International Symposium, IDA-99 Amsterdam, The Netherlands, August 9–11, 1999 Proceedings 3, Springer, 1999, pp. 199–209.   
[30] X. Chen, S. Güttel, An e˥cient aggregation method for the symbolic representation of temporal data, ACM Trans. Knowl. Discov. Data. 17 (1) (2023) 1–22.   
[31] Y. Bian, X. Ju, J. Li, Z. Xu, D. Cheng, Q. Xu, Multi-patch prediction: adapting llms for time series representation learning, arXiv preprint arXiv:2402.04852, (2024).   
[32] A. Keshavarzian, H. Salehinejad, S. Valaee, Representation learning of clinical multivariate time series with random ˡlter banks, in: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp. 1–5.   
[33] X. Wang, A. Mueen, H. Ding, G. Trajcevski, P. Scheuermann, E. Keogh, Experimental comparison of representation methods and distance measures for time series data, Data Min. Knowl. Discov. 26 (2) (2013) 275–309.   
-[34] L. Sun, H. Mao, C. Zheng, C. Zhang, T. Balezentis, Interval-valued functional cluster ing based on the improved euclidean distance with application to air quality index, Appl. Math. Model. 123 (2023) 627–643.   
[35] D.S. Shen, M. Chi, TC-DTW: accelerating multivariate dynamic time warping through triangle inequality and point clustering, Inf. Sci. 621 (2023) 611–626.   
-[36] H. El Amouri, T. Lampert, P. Gançarski, C. Mallet, Constrained DTW preserv ing shapelets for explainable time-series clustering, Pattern Recognit. 143 (2023) 109804.   
[37] Q. Zhang, C. Zhang, L. Cui, X. Han, Y. Jin, G. Xiang, Y. Shi, A method for measuring similarity of time series based on series decomposition and dynamic time warping, Appl. Intell. 53 (6) (2023) 6448–6463.   
[38] T. Rakthanmanon, E. Keogh, Fast shapelets: a scalable algorithm for discovering time series shapelets, in: Proceedings of the 2013 SIAM International Conference on Data Mining, SIAM, 2013, pp. 668–676.   
[39] Q. Zhang, J. Wu, P. Zhang, G. Long, C. Zhang, Salient subsequence learning for time series clustering, IEEE Trans. Pattern Anal. Mach. Intell. 41 (9) (2018) 2193–2207.   
-[40] V.S.S. Fotso, E.M. Nguifo, P. Vaslin, Frobenius correlation based U-shapelets discov ery for time series clustering, Pattern Recognit. 103 (2020) 107301.   
[41] Q. Zhang, J. Wu, H. Yang, Y. Tian, C. Zhang, Unsupervised feature learning from time series., in: IJCAI, New York, USA, 2016, pp. 2322–2328.   
[42] Y. Yang, H.T. Shen, Z. Ma, Z. Huang, X. Zhou, L 2, 1-norm regularized discriminative feature selection for unsupervised learning, in: IJCAI International Joint Conference on Artiˡcial Intelligence, 2011.   
-[43] Z. Li, Y. Yang, J. Liu, X. Zhou, H. Lu, Unsupervised feature selection using non negative spectral analysis, in: Proceedings of the AAAI Conference on Artiˡcial Intelligence, vol. 26, 2012, pp. 1026–1032.   
[44] M. Qian, C. Zhai, Robust unsupervised feature selection, in: Twenty-third International Joint Conference on Artiˡcial Intelligence, Citeseer, 2013.   
[45] L. Shi, L. Du, Y.-D. Shen, Robust spectral learning for unsupervised feature selection, in: 2014 IEEE International Conference on Data Mining, IEEE, 2014, pp. 977–982.   
[46] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clustering analysis, in: International Conference on Machine Learning, PMLR, 2016, pp. 478–487.   
-[47] X. Yang, C. Deng, F. Zheng, J. Yan, W. Liu, Deep spectral clustering using dual au toencoder network, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4066–4075.   
[48] D. Bo, X. Wang, C. Shi, M. Zhu, E. Lu, P. Cui, Structural deep clustering network, in: Proceedings of the Web Conference 2020, 2020, pp. 1400–1410.   
[49] N.S. Madiraju, Deep temporal clustering: fully unsupervised learning of time-domain features (Ph.D. thesis), Arizona State University, 2018.   
[50] M. Łuczak, Hierarchical clustering of time series data with parametric derivative dynamic time warping, Expert Syst. Appl. 62 (2016) 116–130.

![](images/99151e1e85e2fb9fff3ad49e2567a52ed384e6241dcd329480236133d3b625de.jpg)  
Author biography

Mei Chen received the Ph.D. degree in Computer Application Technology from Lanzhou University, in 2016. She is currently a Professor and Ph.D. supervisor with the School of Electronic and Information Engineering, Lanzhou Jiaotong University. She has published over 20 research papers in many conferences and journals, such as, Pattern Recognition, Atmospheric Environment, Frontiers of Computer Science, and WAIM. Her research interests include artiˡcial intelligence and data mining. She is a member of CCF.

![](images/2b1401dc10ab5cf3b223743bbec511249b390f3927e413e500fa217927f1bbb4.jpg)

Yu Wang, was born in 2000. She received the B.S. degree in Computer Science and Technology from School of Electronic and Information Engineering, Lanzhou Jiaotong University, in -June 2022, where she is currently pursuing the Graduate de gree. Her research interests include complex data mining and time series clustering.

![](images/a50fe4fe659be9ace3683693eab861ba875e3b152338c80d7ded016ade1af3a5.jpg)

Mingwei Leng received the Ph.D. degree in Computer Application Technology from Lanzhou University, in 2014. He is currently a Professor and Ph.D. supervisor with the School of Educational Science and Technology, Northwest Minzu University, Lanzhou. His research interests include data mining and complex network.

![](images/325efa955510efed0f9df129daa86b60f29bdb582966a2429c773a8964d839d8.jpg)

-Boya Liu, was born in 2000. She received the B.S. de gree in Network Engineering from the College of Artiˡcial Intelligence, Tianjin University of Science and Technology, in -June 2022, where she is currently pursuing the Graduate de gree. Her research interests include complex data mining and time series clustering.

![](images/a3eadb1600685108d81d505bdd3c1c9cb2189fe60823d94fb99b33a34430e372.jpg)

-Yuanyuxiu You, was born in 2000. She received the B.S. de gree in Data Science and Big Data Technology from College of Engineering, Shantou University, in June 2022, where she is currently pursuing the Graduate degree. Her research interests include complex data mining and pattern recognition.

![](images/61484d84ddd91a2f7bb7299a480245202277afa8a026a6228050d4de33bbfc23.jpg)

Yiying Yao, was born in 2000. She received the B.S. degree in Computer Science and Technology from School of Computer Science and Engineering, Xi’an Technological University, in -June 2022, where she is currently pursuing the Graduate de gree. Her research interests include complex data mining and graph learning clustering.

![](images/757a8ddfbf7d220b82b86497ae195c479e307bd65e6d9071aea32f0f93e77442.jpg)

-Song Wang, was born in 2001. He received the B.S. de gree in Computer Science and Technology from the School of Information and Engineering, Xi’an University, in June 2022, -where he is currently pursuing the Graduate degree. His re search interests include complex data mining and complex network.

# Python Cookbook.

中文版

[美]David BeazleyG Brian K.Jones著

陈译

O'REILLY

# Python Cookbook

# （第 3 版）中文版

[美] David Beazley Brian K.Jones 著

陈 舸 译

人 民 邮 电 出 版 社

北 京

# 异步社区电子书

感谢您购买异步社区电子书！异步社区已上架电子书 500 余种，社区还会经常发布福利信息，对社区有贡献的读者赠送免费样书券、优惠码、积分等等，希望您在阅读过程中，把您的阅读体验传递给我们，让我们了解读者心声，有问题我们会及时修正。

社区网址：http://www.epubit.com.cn/

反馈邮箱：contact@epubit.com.cn

异步社区里有什么？

图书、电子书（半价电子书）、优秀作译者、访谈、技术会议播报、赠书活动、下载资源。

异步社区特色：

纸书、电子书同步上架、纸电捆绑超值优惠购买。

最新精品技术图书全网首发预售。

晒单有意外惊喜！

异步社区里可以做什么？

博客式写作发表文章，提交勘误赚取积分，积分兑换样书，写书评赢样书券等。

# 联系我们：

微博：

@ 人邮异步社区 $\textcircled{2}$ 人民邮电出版社- 信息技术分社

微信公众号：

人邮IT 书坊 异步社区

QQ 群：368449889

# 图书在版编目（CIP）数据

Python Cookbook：中文版：第3版／（美）比斯利(Beazley,D.），（美）琼斯（Jones,B.K.）著；陈译--北京：人民邮电出版社，2015.5ISBN 978-7-115-37959-7

Ⅰ．①P…ⅡI．①比…②琼…③陈…IⅢI. $\textcircled{1}$ 软件工具一程序设计－英文 IV．①TP311.56

中国版本图书馆CIP数据核字(2015)第004926号

# 版权声明

Copyright $©$ 2013 by O'Reilly Media.Inc.

Simplified Chinese Edition, jointly published by O'Reilly Media, Inc.and Posts & Telecom Press, 2015.Authorized translation of the English edition, 201l O'Reilly Media, Inc.,the owner of all rights to publish and sell the same.

All rights reserved including the rights of reproduction in whole or in part in any form.

本书中文简体字版由O'Reilly Media,Inc.授权人民邮电出版社出版。未经出版者书面许可，对本书的任何部分不得以任何方式复制或抄袭。

版权所有，侵权必究。

←著 [美] David BeazleyBrian K.Jones

译 陈

责任编辑傅道坤

责任印制 张佳莹彭志环

$\bullet$ 人民邮电出版社出版发行 北京市丰台区成寿寺路11号

邮编100164 电子邮件315@ptpress.com.cn

网址http://www.ptpress.com.cn

三河市中晟雅豪印务有限公司印刷

$\spadesuit$ 开本： $7 8 7 { \times } 1 ~ 0 0 0$ 1/16

印张：43.75

字数：914千字 2015年5月第1版

印数：1-3000册 2015年5月河北第1次印刷

著作权合同登记号图字：01-2013-7656号

# 内容提要

本书介绍了 Python 应用在各个领域中的一些使用技巧和方法，其主题涵盖了数据结构和算法，字符串和文本，数字、日期和时间，迭代器和生成器，文件和 I/O，数据编码与处理，函数，类与对象，元编程，模块和包，网络和 Web 编程，并发，实用脚本和系统管理，测试、调试以及异常，C语言扩展等。

本书覆盖了 Python 应用中的很多常见问题，并提出了通用的解决方案。书中包含了大量实用的编程技巧和示例代码，并在 Python 3.3 环境下进行了测试，可以很方便地应用到实际项目中去。此外，本书还详细讲解了解决方案是如何工作的，以及为什么能够工作。

本书非常适合具有一定编程基础的 Python 程序员阅读参考。

# OíReilly Media, Inc. 介绍

O’Reilly Media 通过图书、杂志、在线服务、调查研究和会议等方式传播创新知识。自 1978 年开始，O’Reilly 一直都是前沿发展的见证者和推动者。超级极客们正在开创着未来，而我们关注真正重要的技术趋势—通过放大那些“细微的信号”来刺激社会对新科技的应用。作为技术社区中活跃的参与者，O’Reilly 的发展充满了对创新的倡导、创造和发扬光大。

O’Reilly 为软件开发人员带来革命性的“动物书”；创建第一个商业网站（GNN）；组织了影响深远的开放源代码峰会，以至于开源软件运动以此命名；创立了 Make 杂志，从而成为 DIY 革命的主要先锋；公司一如既往地通过多种形式缔结信息与人的纽带。O’Reilly 的会议和峰会集聚了众多超级极客和高瞻远瞩的商业领袖，共同描绘出开创新产业的革命性思想。作为技术人士获取信息的选择，O’Reilly 现在还将先锋专家的知识传递给普通的计算机用户。无论是通过书籍出版，在线服务或者面授课程，每一项 O’Reilly 的产品都反映了公司不可动摇的理念——信息是激发创新的力量。

# 业界评论

“O’Reilly Radar 博客有口皆碑。”

Wired

“O’Reilly 凭借一系列（真希望当初我也想到了）非凡想法建立了数百万美元的业务。”

Business 2.0

“O’Reilly Conference 是聚集关键思想领袖的绝对典范。”

CRN

“一本 O’Reilly 的书就代表一个有用、有前途、需要学习的主题。”

Irish Times

“Tim 是位特立独行的商人，他不光放眼于最长远、最广阔的视野并且切实地按照Yogi Berra的建议去做了：‘如果你在路上遇到岔路口，走小路（岔路）。’回顾过去Tim似乎每一次都选择了小路，而且有几次都是一闪即逝的机会，尽管大路也不错。”

Linux Journal

# 前言

自 2008 年以来，我们已经目睹了整个 Python 世界正缓慢向着 Python 3 进化的事实。众所周知，完全接纳 Python 3 要花很长的时间。事实上，就在写作本书时（2013 年），大多数 Python 程序员仍然坚持在生产环境中使用 Python 2。关于 Python 3 不能向后兼容的事实也已经做了许多努力来补救。的确，向后兼容性对于任何已经存在的代码库来说是个问题。但是，如果你着眼于未来，你会发现 Python 3 带来的好处绝非那么简单。

正因为 Python 3 是着眼于未来的，本书在之前的版本上做了很大程度的修改。首先也是最重要的一点，这是一本积极拥抱 Python 3 的书。所有的章节都采用 Python 3.3 来编写并进行了验证，没有考虑老的 Python 版本或者“老式”的实现方式。事实上，许多章节都只适用于 Python 3.3 甚至更高的版本。这么做可能会有风险，但是最终的目的是要编写一本 Python 3 的秘籍，尽可能基于最先进的工具和惯用法。我们希望本书可以指导人们用 Python 3 编写新的代码，或者帮助开发人员将已有的代码升级到Python 3。

无需赘言，以这种风格来编写本书给编辑工作带来了一定的挑战。只要在网络上搜索一下 Python 秘籍，立刻就能在 ActiveState 的 Python 版块或者 Stack Overflow 这样的站点上找到数以千计的使用心得和秘籍。但是，大部分这类资源已经沉浸在历史和过去中了。由于这些心得和秘籍几乎完全是针对 Python 2 所写的，其中常常包含有各种针对 Python 不同版本（例如 2.3 版对比 2.4 版）之间差异的变通方法和技巧。此外，这些网上资源常常使用过时的技术，而这些技术现在成了 Python 3.3 的内建功能。想寻找专门针对 Python 3 的资源会比较困难。

本书并非搜寻特定于 Python 3 方面的秘籍将其汇集而成，本书的主题都是在创作中由现有的代码和技术而产生出的灵感。我们将这些思想作为跳板，尽可能采用最现代化的 Python 编程技术来写作，因此本书的内容完全是原创性的。对于任何希望以现代化的风格来编写代码的人，本书都可以作为参考手册。

在选择应该包含哪些章节时，我们有一个共识。那就是根本不可能编写一本涵盖了每种 Python 用途的书。因此，我们在主题上优先考虑 Python 语言核心方面的内容，以及能够广泛适用于各种应用领域的常见任务。此外，有许多秘籍是用来说明在 Python3 中新增的功能，这对许多人来说比较陌生，甚至对于那些使用老版 Python 经验丰富的程序员也是如此。我们也会优先选择普遍适用的编程技术（即，编程模式）作为主

题，而不会选择那些试图解决一个非常具体的实际问题但适用范围太窄的内容。尽管在部分章节中也提到了特定的第三方软件包，但本书绝大多数章节都只关注语言核心和标准库。

# 本书适合谁

本书的目标读者是希望加深对 Python 语言的理解以及学习现代化编程惯用法的有经验的程序员。本书许多内容把重点放在库、框架和应用中使用的高级技术上。本书假设读者已经有了理解本书主题的必要背景知识（例如对计算机科学的一般性知识、数据结构、复杂度计算、系统编程、并发、C语言编程等）。此外，本书中提到的秘籍往往只是一个框架，意在提供必要的信息让读者可以起步，但是需要读者自己做更多的研究来填补其中的细节。因此，我们假设读者知道如何使用搜索引擎以及优秀的 Python在线文档。

有一些更加高级的章节将作为读者耐心阅读的奖励。这些章节对于理解 Python 底层的工作原理提供了深刻的见解。你将学到新的技巧和技术，可以将这些知识运用到自己的代码中去。

# 本书不适合谁

这不是一本用来给初学者首次学习 Python 编程而使用的书。事实上，本书已经假设读者通过Python 教程或者入门书籍了解了基本知识。本书同样不能用来作为快速参考手册（即，快速查询特定模块中的某个函数）。相反，本书的目标是把重点放在特定的编程主题上，展示可能的解决方案并以此作为跳板引导读者学习更加高级的内容。这些内容你可能会在网上或者参考书中遇到过。

# 本书中的约定

![](images/811f059a7c7034ba2d549262bb0ecdb903836884705cae34da5d9a8871c57da8.jpg)

# 提示

这个图标用来强调一个提示、建议或一般说明。

![](images/5cf1bd59b5ca8d7d3fe1d1620e7ec62edcb756b17256215d44cb3a3dcebb1020.jpg)

# 警告

这个图标用来说明一个警告或注意事项。

# 在线代码示例

本书中几乎所有的代码示例都可以在 http://github.com/dabeaz/python-cookbook 上找到。作者欢迎读者针对代码示例提供 bug 修正、改进以及评论。

# 使用代码示例

本书的目的是为了帮助读者完成工作。一般而言，你可以在你的程序和文档中使用本书中的代码，而且也没有必要取得我们的许可。但是，如果你要复制的是核心代码，则需要和我们打个招呼。例如，你可以在无需获取我们许可的情况下，在程序中使用本书中的多个代码块。但是，销售或分发 O’Reilly图书中的代码光盘则需要取得我们的许可。通过引用本书中的示例代码来回答问题时，不需要事先获得我们的许可。但是，如果你的产品文档中融合了本书中的大量示例代码，则需要取得我们的许可。

在引用本书中的代码示例时，如果能列出本书的属性信息是最好不过。一个属性信息通常包括书名、作者、出版社和 ISBN。例如：Python Cookbook, 3rd edtion, by David Beazley and BrainK.Jones(O’Reilly)。Copyright 2013 David Beazley and Brain Jones, 978-1-449-34037-7。

在使用书中的代码时，如果不确定是否属于正常使用，或是否超出了我们的许可，请通过 permissions@oreilly.com 与我们联系。

# 联系方式

如果你想就本书发表评论或有任何疑问，敬请联系出版社。

美国：

O’Reilly Media Inc.

1005 Gravenstein Highway North

Sebastopol, CA 95472

中国：

北京市西城区西直门南大街 2 号成铭大厦 C 座 807 室（100035）

奥莱利技术咨询（北京）有限公司

我们还为本书建立了一个网页，其中包含了勘误表、示例和其他额外的信息。你可以通过链接 http://oreil.ly/python_cookbook_3e 来访问页面。

关于本书的技术性问题或建议，请发邮件到：

bookquestions@oreilly.com

欢迎登录我们的网站（http://www.oreilly.com），查看更多我们的书籍、课程、会议和最新动态等信息。

Facebook: http://facebook.com/oreilly

Twitter: http://twitter.com/oreillymedia

YouTube: http://www.youtube.com/oreillymedia

# 致谢

我们要感谢本书的技术校审人员，他们是 Jake Vanderplas、Robert Kern 以及 AndreaCrotti。感谢他们非常有用的评价，也要感谢整个 Python 社区的支持和鼓励。我们也要感谢本书第 2 版的编辑 Alex Martelli、Anna Ravenscroft 以及 David Ascher。尽管本书的第 3 版是新创作的，但之前的版本为本书提供了挑选主题以及所感兴趣的秘籍的初始框架。昀后也是昀重要的是，我们要感谢本书早期版本的读者，感谢你们为本书的改进做出的评价和建议。

# David Beazley 的致谢

写一本书绝非易事。因此，我要感谢我的妻子 Paula 以及我的两个儿子，感谢你们的耐心以及支持。本书中的许多素材都来自于我过去 6 年里所教的与 Python 相关的训练课程。因此，我要感谢所有参加了我的课程的学生，正是你们昀终促成了本书的问世。我也要感谢 Ned Batchelder、Travis Oliphant、Peter Wang、Brain Van deVen、Hugo Shi、Raymond Hettinger、Michael Foord 以及 Daniel Klein，感谢他们飞到世界各地去教学，而让我可以留在芝加哥的家中完成本书的写作。感谢来自O’Reilly 的 Meghan Blanchette 以及 Rachel Roumeliotis，你们见证了本书的创作过程，当然也经历了那些无法预料到的延期。昀后也是昀重要的是，我要感谢 Python社区不间断的支持，以及容忍我那不着调的胡思乱想。

David M.Beazley

http://www.dabeaz.com

https://twitter.com/dabeaz

# Brain Jones 的致谢

我要感谢我的合著者 David Beazley 以及 O’Reilly 的 Meghan Blanchette 和 RachelRoumeliotis，感谢你们和我一起完成了本书的创作。我也要感谢我的妻子Natasha，感谢你在我写作本书时给予的耐心和鼓励，也要谢谢你对于我所有追求的支持。我尤其要感谢 Python 社区。虽然我已经在多个开源项目和编程语言中有所贡献，但与 Python社区长久以来所做的如此令人欣慰和富有意义的工作相比，我做的算不上什么。

Brain K.Jones

http://www.protocolostomy.com

https://twitter.com/bkjones

# 目录

# 第 1 章 数据结构和算法.

1.1 将序列分解为单独的变量  
1.2 从任意长度的可迭代对象中分解元素  
1.3 保存昀后 N 个元素.. 5  
1.4 找到昀大或昀小的 N 个元素.  
1.5 实现优先级队列 .   
1.6 在字典中将键映射到多个值上. .11  
1.7 让字典保持有序. ....13   
1.8 与字典有关的计算问题.. ....14  
1.9 在两个字典中寻找相同点 . ......15  
1.10 从序列中移除重复项且保持元素间顺序不变.... ......17  
1.11 对切片命名 . ......18  
1.12 找出序列中出现次数昀多的元素. .....20   
1.13 通过公共键对字典列表排序 ...22  
1.14 对不原生支持比较操作的对象排序 ...23  
1.15 根据字段将记录分组. ...25  
1.16 筛选序列中的元素. ...26  
1.17 从字典中提取子集. ..29  
1.18 将名称映射到序列的元素中 ......30  
1.19 同时对数据做转换和换算. ...33  
1.20 将多个映射合并为单个映射 ....34

# 第 2 章 字符串和文本. ...37

2.1 针对任意多的分隔符拆分字符串. ...37  
2.2 在字符串的开头或结尾处做文本匹配. .....38  
2.3 利用 Shell 通配符做字符串匹配 ....40  
2.4 文本模式的匹配和查找.. ...42  
2.5 查找和替换文本 ... ....45  
2.6 以不区分大小写的方式对文本做查找和替换. ....47  
2.7 定义实现昀短匹配的正则表达式.. ...48  
2.8 编写多行模式的正则表达式 ....49  
2.9 将 Unicode 文本统一表示为规范形式. ......50  
2.10 用正则表达式处理 Unicode 字符 . ....52

2.11 从字符串中去掉不需要的字符. . 53  
2.12 文本过滤和清理....... ....... 54  
2.13 对齐文本字符串. ..... 57  
2.14 字符串连接及合并. . 59   
2.15 给字符串中的变量名做插值处理...... 62  
2.16 以固定的列数重新格式化文本. .. 64  
2.17 在文本中处理 HTML 和 XML 实体. . 66  
2.18 文本分词 .. ..... 67  
2.19 编写一个简单的递归下降解析器. ..... 70  
2.20 在字节串上执行文本操作 .... . 80

# 第 3 章 数字、日期和时间 83

3.1 对数值进行取整 .... . 83  
3.2 执行精确的小数计算. 85  
3.3 对数值做格式化输出. 87  
3.4 同二进制、八进制和十六进制数打交道 89  
3.5 从字节串中打包和解包大整数...... .... 90  
3.6 复数运算. . 92   
3.7 处理无穷大和 NaN ..... .... 94  
3.8 分数的计算 .... .... 96  
3.9 处理大型数组的计算. . 97  
3.10 矩阵和线性代数的计算. ....... 101  
3.11 随机选择 ....... ......... 103   
3.12 时间换算 . .... 105  
3.13 计算上周 5 的日期. .... 107  
3.14 找出当月的日期范围. 108  
3.15 将字符串转换为日期. .....110  
3.16 处理涉及到时区的日期问题. ......112

# 第 4 章 迭代器和生成器 .....114

4.1 手动访问迭代器中的元素. .....114  
4.2 委托迭代. ......115   
4.3 用生成器创建新的迭代模式 ......116  
4.4 实现迭代协议 .... ......118  
4.5 反向迭代. .... 121  
4.6 定义带有额外状态的生成器函数. 122  
4.7 对迭代器做切片操作. .... 123  
4.8 跳过可迭代对象中的前一部分元素.. .. . 124  
4.9 迭代所有可能的组合或排列 ... 127

4.10 以索引-值对的形式迭代序列 ......129  
4.11 同时迭代多个序列. .....131  
4.12 在不同的容器中进行迭代..... .....133  
4.13 创建处理数据的管道. .....134  
4.14 扁平化处理嵌套型的序列.... .....137  
4.15 合并多个有序序列，再对整个有序序列进行迭代 ..... ......139  
4.16 用迭代器取代 while 循环 ...... .....140

# 第 5 章 文件和 I/O . .......142

5.1 读写文本数据 ......142  
5.2 将输出重定向到文件中. ......145  
5.3 以不同的分隔符或行结尾符完成打印....... .....145  
5.4 读写二进制数据 ......146  
5.5 对已不存在的文件执行写入操作 .....149  
5.6 在字符串上执行 I/O操作. .....150  
5.7 读写压缩的数据文件. .....151  
5.8 对固定大小的记录进行迭代 .....152  
5.9 将二进制数据读取到可变缓冲区中. .....153  
5.10 对二进制文件做内存映射 ... .....155  
5.11 处理路径名 ... .....157  
5.12 检测文件是否存在. .....158  
5.13 获取目录内容的列表. .....159  
5.14 绕过文件名编码 . .....161  
5.15 打印无法解码的文件名.. .....162  
5.16 为已经打开的文件添加或修改编码方式 ....164  
5.17 将字节数据写入文本文件 .....166  
5.18 将已有的文件描述符包装为文件对象 .....167  
5.19 创建临时文件和目录. ....169  
5.20 同串口进行通信 ......171  
5.21 序列化 Python 对象 ......172

# 第 6 章 数据编码与处理. .....177

6.1 读写 CSV 数据 ......177  
6.2 读写 JSON 数据. ......181  
6.3 解析简单的 XML 文档 .....186  
6.4 以增量方式解析大型 XML 文件. .....188  
6.5 将字典转换为 XML. ......192  
6.6 解析、修改和重写 XML.. .....194   
6.7 用命名空间来解析 XML 文档. .....196

6.8 同关系型数据库进行交互. ....... 198  
6.9 编码和解码十六进制数字. . 201  
6.10 Base64 编码和解码 . ........ 202  
6.11 读写二进制结构的数组 ....... 203  
6.12 读取嵌套型和大小可变的二进制结构 ........ 207  
6.13 数据汇总和统计..... ....... 218

# 第 7 章 函数.. . 221

7.1 编写可接受任意数量参数的函数...... . 221  
7.2 编写只接受关键字参数的函数. ..... 223  
7.3 将元数据信息附加到函数参数上.. ...... 224  
7.4 从函数中返回多个值. ..... 225  
7.5 定义带有默认参数的函数 ....... ........ 226  
7.6 定义匿名或内联函数.. ...... 229  
7.7 在匿名函数中绑定变量的值 ...... 230  
7.8 让带有N个参数的可调用对象以较少的参数形式调用. ........ 232  
7.9 用函数替代只有单个方法的类. ..... 235  
7.10 在回调函数中携带额外的状态. ..... 236  
7.11 内联回调函数....... ........ 240  
7.12 访问定义在闭包内的变量 ..... 242

# 第 8 章 类与对象...... . 246

8.1 修改实例的字符串表示.. ....... 246  
8.2 自定义字符串的输出格式 . ..... 248  
8.3 让对象支持上下文管理协议 . ....... 249  
8.4 当创建大量实例时如何节省内存 ...... 251  
8.5 将名称封装到类中. ..... 252  
8.6 创建可管理的属性. .... 254  
8.7 调用父类中的方法. ..... 259  
8.8 在子类中扩展属性. ........ 263  
8.9 创建一种新形式的类属性或实例属性. ..... 267  
8.10 让属性具有惰性求值的能力..... ....... 271  
8.11 简化数据结构的初始化过程. ..... 274  
8.12 定义一个接口或抽象基类 ....... 278  
8.13 实现一种数据模型或类型系统. ...... 281  
8.14 实现自定义的容器 ...... 287   
8.15 委托属性的访问. ....... 291  
8.16 在类中定义多个构造函数 ..... 296  
8.17 不通过调用 init 来创建实例 . ..... 298

8.18 用 Mixin 技术来扩展类定义.. .....299  
8.19 实现带有状态的对象或状态机..... ......305  
8.20 调用对象上的方法，方法名以字符串形式给出 ......311  
8.21 实现访问者模式 . ......312  
8.22 实现非递归的访问者模式. ......317   
8.23 在环状数据结构中管理内存 ......324  
8.24 让类支持比较操作. ......327  
8.25 创建缓存实例 ....... ......330

# 第 9 章 元编程. ...335

9.1 给函数添加一个包装. .....335  
9.2 编写装饰器时如何保存函数的元数据 .....337  
9.3 对装饰器进行解包装. ......339   
9.4 定义一个可接受参数的装饰器. .....341  
9.5 定义一个属性可由用户修改的装饰器 .....342  
9.6 定义一个能接收可选参数的装饰器. .....346  
9.7 利用装饰器对函数参数强制执行类型检查... .....348  
9.8 在类中定义装饰器. .....352  
9.9 把装饰器定义成类..... ....354   
9.10 把装饰器作用到类和静态方法上.... .....357  
9.11 编写装饰器为被包装的函数添加参数....... ....359  
9.12 利用装饰器给类定义打补丁 ..... ....362  
9.13 利用元类来控制实例的创建 ....364  
9.14 获取类属性的定义顺序.. ....367   
9.15 定义一个能接受可选参数的元类. .....370  
9.16 在*args 和**kwargs 上强制规定一种参数签名.. .......372  
9.17 在类中强制规定编码约定 .....375  
9.18 通过编程的方式来定义类 .....378  
9.19 在定义的时候初始化类成员 .....382  
9.20 通过函数注解来实现方法重载.... ....384  
9.21 避免出现重复的属性方法 .....391  
9.22 以简单的方式定义上下文管理器. .....393  
9.23 执行带有局部副作用的代码 ....395  
9.24 解析并分析 Python 源代码 . .....398  
9.25 将 Python 源码分解为字节码 . .....402

# 第 10 章 模块和包 ...406

10.1 把模块按层次结构组织成包 .....406  
10.2 对所有符号的导入进行精确控制 .....407

10.3 用相对名称来导入包中的子模块.. . 408  
10.4 将模块分解成多个文件... ..... 410  
10.5 让各个目录下的代码在统一的命名空间下导入.......... ...... 413  
10.6 重新加载模块 . ....... 415  
10.7 让目录或 zip 文件成为可运行的脚本 ..... 416  
10.8 读取包中的数据文件. ...... 417  
10.9 添加目录到 sys.path 中 . ..... 418  
10.10 使用字符串中给定的名称来导入模块.... ..... 420  
10.11 利用 import 钩子从远端机器上加载模块 .... ..... 421  
10.12 在模块加载时为其打补丁 . ..... 439  
10.13 安装只为自己所用的包 . ...... 441  
10.14 创建新的 Python 环境. ..... 442  
10.15 发布自定义的包. ..... 444

# 第 11 章 网络和 Web 编程 446

11.1 以客户端的形式同 HTTP服务交互. ..... 446  
11.2 创建一个 TCP 服务器. .... 450  
11.3 创建一个 UDP 服务器 . ..... 454  
11.4 从 CIDR 地址中生成 IP 地址的范围 . ..... 456  
11.5 创建基于 REST 风格的简单接口 ..... ... 458  
11.6 利用 XML-RPC 实现简单的远端过程调用 .... 463  
11.7 在不同的解释器间进行通信. . 466  
11.8 实现远端过程调用. ... 468   
11.9 以简单的方式验证客户端身份...... ..... 472  
11.10 为网络服务增加 SSL 支持. ..... 474  
11.11 在进程间传递 socket 文件描述符 . ..... 481  
11.12 理解事件驱动型 I/O.. . 486  
11.13 发送和接收大型数组 ..... 493

# 第 12 章 并发.. . 496

12.1 启动和停止线程. ... 496   
12.2 判断线程是否已经启动 ..... 499  
12.3 线程间通信 . ..... 503  
12.4 对临界区加锁 ...... 508  
12.5 避免死锁 ...... .......511   
12.6 保存线程专有状态. ....... 515  
12.7 创建线程池 . ....... 517  
12.8 实现简单的并行编程. ...... 521   
12.9 如何规避 GIL 带来的限制. ..... 525

12.10 定义一个 Actor 任务... ......528  
12.11 实现发布者/订阅者消息模式.. ......532   
12.12 使用生成器作为线程的替代方案..... .....536  
12.13 轮询多个线程队列.. .....544  
12.14 在 UNIX 上加载守护进程 . .....547

# 第 13 章 实用脚本和系统管理. ..552

13.1 通过重定向、管道或输入文件来作为脚本的输入....... ......552  
13.2 终止程序并显示错误信息. .....553  
13.3 解析命令行选项 . ......554   
13.4 在运行时提供密码输入提示 ......557  
13.5 获取终端大小 .... ......558   
13.6 执行外部命令并获取输出 .....558  
13.7 拷贝或移动文件和目录. ......560  
13.8 创建和解包归档文件. ......562  
13.9 通过名称来查找文件. .....563  
13.10 读取配置文件 . .....565  
13.11 给脚本添加日志记录. .....568  
13.12 给库添加日志记录. .......571   
13.13 创建一个秒表计时器. .....573  
13.14 给内存和 CPU 使用量设定限制.... .....575  
13.15 加载 Web 浏览器 . .......576

# 第 14 章 测试、调试以及异常. .....578

14.1 测试发送到 stdout 上的输出. .......578  
14.2 在单元测试中为对象打补丁 ......579  
14.3 在单元测试中检测异常情况 .....583  
14.4 将测试结果作为日志记录到文件中. .....585  
14.5 跳过测试，或者预计测试结果为失败 ......586   
14.6 处理多个异常 . .....587  
14.7 捕获所有的异常 . .....589  
14.8 创建自定义的异常. ......591  
14.9 通过引发异常来响应另一个异常. ......593  
14.10 重新抛出上一个异常. ......595  
14.11 发出告警信息..... ........596  
14.12 对基本的程序崩溃问题进行调试.. ......598  
14.13 对程序做性能分析以及计时统计. .....600  
14.14 让你的程序运行得更快 .....603

# 第 15 章 C 语言扩展.. ...... 610

15.1 利用 ctypes 来访问 C 代码........ ....... 612  
15.2 编写简单的 C 语言扩展模块. . 618  
15.3 编写一个可操作数组的扩展函数. . 622  
15.4 在 C 扩展模块中管理不透明指针 . 625  
15.5 在扩展模块中定义并导出 C API. . 628  
15.6 从 C 中调用 Python...... ... 633  
15.7 在 C 扩展模块中释放 GIL ..... . 639  
15.8 混合使用 C 和 Python 环境中的线程 . 639  
15.9 用 Swig 来包装 C 代码 . ..... 640  
15.10 用 Cython 来包装 C 代码. . 646  
15.11 用 Cython 来高效操作数组. . 652  
15.12 把函数指针转换为可调用对象. . 657  
15.13 把以 NULL 结尾的字符串传给 C 库. . 659   
15.14 把 Unicode 字符串传递给 C 库. . 663   
15.15 把 C 字符串转换到 Python 中. .. 667  
15.16 同编码方式不确定的 C 字符串打交道 669  
15.17 把文件名传给 C 扩展模块. ..... 672  
15.18 把打开的文件传给 C 扩展模块 . 673  
15.19 在 C中读取文件型对象. .... 674  
15.20 从 C 中访问可迭代对象. ... 677  
15.21 排查段错误 . ..... 678

# 附录 A 补充阅读 . . 680

# 数据结构和算法

Python 内置了许多非常有用的数据结构，比如列表（list）、集合（set）以及字典（dictionary）。就绝大部分情况而言，我们可以直接使用这些数据结构。但是，通常我们还需要考虑比如搜索、排序、排列以及筛选等这一类常见的问题。因此，本章的目的就是来讨论常见的数据结构和同数据有关的算法。此外，在 collections 模块中也包含了针对各种数据结构的解决方案。

# 1.1 将序列分解为单独的变量

# 1.1.1 问题

我们有一个包含 N 个元素的元组或序列，现在想将它分解为 N 个单独的变量。

# 1.1.2 解决方案

任何序列（或可迭代的对象）都可以通过一个简单的赋值操作来分解为单独的变量。唯一的要求是变量的总数和结构要与序列相吻合。例如：

```txt
>>> p = (4, 5)  
>>> x, y = p  
>>> x  
4  
>>> y  
5  
>>> data = ['ACME', 50, 91.1, (2012, 12, 21)]  
>>> name, shares, price, date = data  
>>> name 
```

'ACME'   
>>>date   
(2012，12，21)   
>>>name，shares，price，（year，mon，day） $=$ data   
>> name   
'ACME'   
>> year   
2012   
>> mon   
12   
>> day   
21   
>>

如果元素的数量不匹配，将得到一个错误提示。例如：

```txt
>>> p = (4, 5)  
>>> x, y, z = p  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
ValueError: need more than 2 values to unpack  
>>> 
```

# 1.1.3 讨论

实际上不仅仅只是元组或列表，只要对象恰好是可迭代的，那么就可以执行分解操作。这包括字符串、文件、迭代器以及生成器。比如：

>>>s $=$ 'Hello'   
>>>a,b,c,d,e $=$ s   
>>>a   
'H   
>>>b   
'e'   
>>>e   
'o'   
>>>

当做分解操作时，有时候可能想丢弃某些特定的值。Python 并没有提供特殊的语法来实现这一点，但是通常可以选一个用不到的变量名，以此来作为要丢弃的值的名称。例如：

```txt
>>> data = ['ACME', 50, 91.1, (2012, 12, 21)]  
>>> _, shares, price, _ = data  
>>> shares 
```

```txt
50  
>>> price  
91.1  
>>> 
```

但是请确保选择的变量名没有在其他地方用到过。

# 1.2 从任意长度的可迭代对象中分解元素

# 1.2.1 问题

需要从某个可迭代对象中分解出 N 个元素，但是这个可迭代对象的长度可能超过 N，这会导致出现“分解的值过多（too many values to unpack）”的异常。

# 1.2.2 解决方案

Python 的“*表达式”可以用来解决这个问题。例如，假设开设了一门课程，并决定在期末的作业成绩中去掉第一个和最后一个，只对中间剩下的成绩做平均分统计。如果只有 4 个成绩，也许可以简单地将 4 个都分解出来，但是如果有 24 个呢？*表达式使这一切都变得简单：

```python
def drop_first_last (grades):
    first, *middle, last = grades
    return avg(middle) 
```

另一个用例是假设有一些用户记录，记录由姓名和电子邮件地址组成，后面跟着任意数量的电话号码。则可以像这样分解记录：

```txt
>>> record = ('Dave', 'dave@example.com', '773-555-1212', '847-555-1212')
>>> name, email, *phoneNumbers = user_record
>>> name
'Dave'
>>> email
'dave@example.com'
>>> phoneNumbers
['773-555-1212', '847-555-1212']
>>> 
```

不管需要分解出多少个电话号码（甚至没有电话号码），变量 phone_numbers 都一直是列表，而这是毫无意义的。如此一来，对于任何用到了变量phone_numbers 的代码都不必对它可能不是一个列表的情况负责，或者额外做任何形式的类型检查。

由*修饰的变量也可以位于列表的第一个位置。例如，比方说用一系列的值来代表公司过去 8 个季度的销售额。如果想对最近一个季度的销售额同前 7 个季度的平均值做比

较，可以这么做：

\*trailing_qtrs, current_qtr = sales_record   
trailing_avg $=$ sum(trailing_qtrs) / len(trailing_qtrs)   
return avg_comparison(trailing_avg, current_qtr)

从 Python 解释器的角度来看，这个操作是这样的：

>>> \*trailing, current $=$ [10, 8, 7, 1, 9, 5, 10, 3]   
>>> trailing   
[10, 8, 7, 1, 9, 5, 10]   
>>> current   
3

# 1.2.3 讨论

对于分解未知或任意长度的可迭代对象，这种扩展的分解操作可谓是量身定做的工具。通常，这类可迭代对象中会有一些已知的组件或模式（例如，元素 1 之后的所有内容都是电话号码），利用*表达式分解可迭代对象使得开发者能够轻松利用这些模式，而不必在可迭代对象中做复杂花哨的操作才能得到相关的元素。

*式的语法在迭代一个变长的元组序列时尤其有用。例如，假设有一个带标记的元组序列：

records $=$ [ ('foo',1,2), ('bar', 'hello'), ('foo',3,4),   
]   
def do_foo(x,y): print('foo',x,y)   
def do_bar(s): print('bar',s)   
for tag,\*args in records: if tag $= =$ foo': do_foo(\*args)   
elif tag $= =$ bar': do_bar(\*args)

当和某些特定的字符串处理操作相结合，比如做拆分（splitting）操作时，这种*式的语法所支持的分解操作也非常有用。例如：

```txt
>>> line = 'nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false'  
>>> uname, *fields, homedir, sh = line.split(":")  
>>> uname  
'nobody'  
>>> homedir 
```

```txt
'/var/empty'  
>>> sh  
'/usr/bin/false'  
>>> 
```

有时候可能想分解出某些值然后丢弃它们。在分解的时候，不能只是指定一个单独的*，但是可以使用几个常用来表示待丢弃值的变量名，比如_或者 ign（ignored）。例如：

```txt
>>> record = ('ACME', 50, 123.45, (12, 18, 2012))  
>>> name, *, (*_, year) = record  
>>> name  
'ACME'  
>>> year  
2012  
>>> 
```

*分解操作和各种函数式语言中的列表处理功能有着一定的相似性。例如，如果有一个列表，可以像下面这样轻松将其分解为头部和尾部：

```txt
>>> items = [1, 10, 7, 4, 5, 9]  
>>> head, *tail = items  
>>> head  
1  
>>> tail  
[10, 7, 4, 5, 9]  
>>> 
```

在编写执行这类拆分功能的函数时，人们可以假设这是为了实现某种精巧的递归算法。例如：

```txt
>>> def sum(items):
...
...
...
...
...
>>> sum(items)
36
>>> 
```

但是请注意，递归真的不算是 Python 的强项，这是因为其内在的递归限制所致。因此，最后一个例子在实践中没太大的意义，只不过是一点学术上的好奇罢了。

# 1.3 保存昀后 N 个元素

# 1.3.1 问题

我们希望在迭代或是其他形式的处理过程中对最后几项记录做一个有限的历史记

录统计。

# 1.3.2 解决方案

保存有限的历史记录可算是 collections.deque的完美应用场景了。例如，下面的代码对一系列文本行做简单的文本匹配操作，当发现有匹配时就输出当前的匹配行以及最后检查过的 N 行文本。

from collections import deque   
def search lines, pattern, history $= 5$ .. previouslines $\equiv$ deque(maxlen $\equiv$ history) for line in lines: if pattern in line: yield line, previouslines previouslines.append(line)   
#Example use on a file   
if_name $\equiv =$ 'main': with open('somefile.txt') as f: for line,prevlines in search(f,'python',5): for pline in prevlines: print(pline,end $= ^{\prime \prime}$ print(line,end $= ^{\prime \prime}$ ) print(-\*\*20)

# 1.3.3 讨论

如同上面的代码片段中所做的一样，当编写搜索某项记录的代码时，通常会用到含有yield 关键字的生成器函数。这将处理搜索过程的代码和使用搜索结果的代码成功解耦开来。如果对生成器还不熟悉，请参见 4.3 节。

deque(maxlen $\mathsf { \Pi } = \mathbf { N }$ )创建了一个固定长度的队列。当有新记录加入而队列已满时会自动移除最老的那条记录。例如：

```txt
>>> q = deque(maxlen=3)  
>>> q.append(1)  
>>> q.append(2)  
>>> q.append(3)  
>>> q  
deque([1, 2, 3], maxlen=3)  
>>> q.append(4)  
>>> q  
deque([2, 3, 4], maxlen=3)  
>>> q.append(5)  
>>> q  
deque([3, 4, 5], maxlen=3) 
```

尽管可以在列表上手动完成这样的操作（append、del），但队列这种解决方案要优雅得多，运行速度也快得多。

更普遍的是，当需要一个简单的队列结构时，deque 可祝你一臂之力。如果不指定队列的大小，也就得到了一个无界限的队列，可以在两端执行添加和弹出操作，例如：

>>>q $\equiv$ deque()   
>>>q.append(1)   
>>>q.append(2)   
>>>q.append(3)   
>>>q   
deque([1,2,3])   
>>>q.appendleft(4)   
>>>q   
deque([4，1，2，3])   
>>>q.pop()   
3   
>>>q   
deque([4，1，2])   
>>>q.popleft()   
4

从队列两端添加或弹出元素的复杂度都是 O(1)。这和列表不同，当从列表的头部插入或移除元素时，列表的复杂度为 O(N)。

# 1.4 找到昀大或昀小的 N 个元素

# 1.4.1 问题

我们想在某个集合中找出最大或最小的 N 个元素。

# 1.4.2 解决方案

heapq 模块中有两个函数—nlargest()和 nsmallest()—它们正是我们所需要的。例如：

```python
import heapq  
nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]  
print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]  
print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2] 
```

这两个函数都可以接受一个参数 key，从而允许它们工作在更加复杂的数据结构之上。例如：

```python
portfolio = [  
{ 'name': 'IBM', 'shares': 100, 'price': 91.1}, 
```

{'name': 'AAPL', 'shares': 50, 'price': 543.22}, {'name': 'FB', 'shares': 200, 'price': 21.09}, {'name': 'HPQ', 'shares': 35, 'price': 31.75}, {'name': 'YHOO', 'shares': 45, 'price': 16.35}, {'name': 'ACME', 'shares': 75, 'price': 115.65}   
]   
cheap $=$ heapq.nsmallest(3, portfolio, key $\equiv$ lambda s: s['price']) expensive $=$ heapq.nlargest(3, portfolio, key $\equiv$ lambda s: s['price'])

# 1.4.3 讨论

如果正在寻找最大或最小的 N 个元素，且同集合中元素的总数目相比，N 很小，那么下面这些函数可以提供更好的性能。这些函数首先会在底层将数据转化成列表，且元素会以堆的顺序排列。例如：

```txt
>>> nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]  
>>> import heapq  
>>> heap = list(nums)  
>>> heapq.heapify(heap)  
>>> heap  
[-4, 2, 1, 23, 7, 2, 18, 23, 42, 37, 8]  
>>> 
```

堆最重要的特性就是 heap[0]总是最小那个的元素。此外，接下来的元素可依次通过heapq.heappop()方法轻松找到。该方法会将第一个元素（最小的）弹出，然后以第二小的元素取而代之（这个操作的复杂度是 O(logN)，N 代表堆的大小）。例如，要找到第 3小的元素，可以这样做：

```txt
>>> heapq. heappop (heap)  
-4  
>>> heapq. heappop (heap)  
1  
>>> heapq. heappop (heap)  
2 
```

当所要找的元素数量相对较小时，函数nlargest()和 nsmallest()才是最适用的。如果只是简单地想找到最小或最大的元素（ $\mathrm { N } { = } 1$ 时），那么用 min()和 max()会更加快。同样，如果 N 和集合本身的大小差不多大，通常更快的方法是先对集合排序，然后做切片操作（例如，使用 sorted(items)[:N]或者 sorted(items)[-N:]）。应该要注意的是，nlargest()和nsmallest()的实际实现会根据使用它们的方式而有所不同，可能会相应作出一些优化措施（比如，当 N 的大小同输入大小很接近时，就会采用排序的方法）。

使用本节的代码片段并不需要知道如何实现堆数据结构，但这仍然是一个有趣也是值

得去学习的主题。通常在优秀的算法和数据结构相关的书籍里都能找到堆数据结构的实现方法。在 heapq 模块的文档中也讨论了底层实现的细节。

# 1.5 实现优先级队列

# 1.5.1 问题

我们想要实现一个队列，它能够以给定的优先级来对元素排序，且每次 pop 操作时都会返回优先级最高的那个元素。

# 1.5.2 解决方案

下面的类利用 heapq 模块实现了一个简单的优先级队列：

import heapq   
class PriorityQueue: def__init__(self): self._queue $= []$ self._index $= 0$ def push(self, item, priority): heapq.heappush(self._queue, (-priority, self._index, item)) self._index $+ = 1$ def pop(self): return heapq.heappop(self._queue)[-1]

下面是如何使用这个类的例子：

>>>class Item:   
... def __init__(self，name):   
... self.name $=$ name   
... def __repr__(self):   
... return 'Item({!r})'.format(self.name)   
...   
>>>q $=$ PriorityQueue()   
>>>q.push(Item('foo'),1)   
>>>q.push(Item('bar'),5)   
>>>q.push(Item('spam'),4)   
>>>q.push(Item('grok'),1)   
>>>q.pop()   
Item('bar')   
>>>q.pop()

```txt
Item('spam')  
>>> q.pop()  
Item('foo')  
>>> q.pop()  
Item('grok')  
>>>
```

请注意观察，第一次执行 pop()操作时返回的元素具有最高的优先级。我们也观察到拥有相同优先级的两个元素（foo和grok）返回的顺序同它们插入到队列时的顺序相同。

# 1.5.3 讨论

上 面 的 代 码 片 段 的 核 心 在 于 heapq 模 块 的 使 用 。 函 数 heapq.heappush() 以 及heapq.heappop()分别实现将元素从列表_queue 中插入和移除，且保证列表中第一个元素的优先级最低（如 1.4 节所述）。heappop()方法总是返回“最小”的元素，因此这就是让队列能弹出正确元素的关键。此外，由于 push 和 pop 操作的复杂度都是 O(logN)，其中 N 代表堆中元素的数量，因此就算 N 的值很大，这些操作的效率也非常高。

在这段代码中，队列以元组(-priority, index, item)的形式组成。把 priority 取负值是为了让队列能够按元素的优先级从高到低的顺序排列。这和正常的堆排列顺序相反，一般情况下堆是按从小到大的顺序排序的。

变量 index 的作用是为了将具有相同优先级的元素以适当的顺序排列。通过维护一个不断递增的索引，元素将以它们入队列时的顺序来排列。但是，index在对具有相同优先级的元素间做比较操作时同样扮演了重要的角色。

为了说明 Item 实例是没法进行次序比较的，我们来看下面这个例子：

```txt
>>> a = Item('foo')
>>> b = Item('bar')
>>> a < b
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: unorderable types: Item() < Item()
>>> 
```

如果以元组(priority, item)的形式来表示元素，那么只要优先级不同，它们就可以进行比较。但是，如果两个元组的优先级值相同，做比较操作时还是会像之前那样失败。例如：

```txt
>>> a = (1, Item('foo'))
>>> b = (5, Item('bar'))
>>> a < b
True
>>> c = (1, Item('grok')) 
```

```txt
>>> a < c  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
TypeError: unordered types: Item() < Item()  
>>> 
```

通过引入额外的索引值，以(prioroty, index, item)的方式建立元组，就可以完全避免这个问题。因为没有哪两个元组会有相同的 index 值（一旦比较操作的结果可以确定，Python就不会再去比较剩下的元组元素了）：

```txt
>>> a = (1, 0, Item('foo'))
>>> b = (5, 1, Item('bar'))
>>> c = (1, 2, Item('grok'))
>>> a < b
True
>>> a < c
True
>>> 
```

如果想将这个队列用于线程间通信，还需要增加适当的锁和信号机制。请参见 12.3 节的示例学习如何去做。

关于堆的理论和实现在 heapq 模块的文档中有着详细的示例和相关讨论。

# 1.6 在字典中将键映射到多个值上

# 1.6.1 问题

我们想要一个能将键（key）映射到多个值的字典（即所谓的一键多值字典[multidict]）。

# 1.6.2 解决方案

字典是一种关联容器，每个键都映射到一个单独的值上。如果想让键映射到多个值，需要将这多个值保存到另一个容器如列表或集合中。例如，可能会像这样创建字典：

$\begin{array}{rl} & {\mathrm{d} = \{\mathrm{a}}\\ & {\mathrm{e} = \{\mathrm{b}}\\ & {\mathrm{e} = \{\mathrm{b}}\\ & {\mathrm{e} = \{\mathrm{b}}\\ & {\mathrm{e} = \{\mathrm{b}}\\ & {\mathrm{e} = \{\mathrm{b}}\\ & {\mathrm{e} = \{\mathrm{b}}\\ & {\mathrm{e} = \{\mathrm{b}}\\ & {\mathrm{e} = \{\mathrm{b}}\end{array}$

要使用列表还是集合完全取决于应用的意图。如果希望保留元素插入的顺序，就用列

表。如果希望消除重复元素（且不在意它们的顺序），就用集合。

为了能方便地创建这样的字典，可以利用 collections 模块中的 defaultdict 类。defaultdict的一个特点就是它会自动初始化第一个值，这样只需关注添加元素即可。例如：

from collections import defaultdict

```javascript
d = defaultdict(list)  
d['a'].append(1)  
d['a'].append(2)  
d['b'].append(4) 
```

$\mathrm{d} =$ defaultdict(set) $\mathrm{d}[a].add(1)$ $\mathrm{d}[a].add(2)$ $\mathrm{d}[b].add(4)$

于defaultdict，需要注意的一个地方是，它会自动创建字典表项以待稍后的访问（即这些表项当前在字典中还没有找到）。如果不想要这个功能，可以在普通的字典上调setdefault()方法来取代。例如：

```txt
d = {} # A regular dictionary  
d.setdefault('a', [], append(1))  
d.setdefault('a', [], append(2))  
d.setdefault('b', [], append(4))  
... 
```

然而，许多程序员觉得使用 setdefault()有点不自然—更别提每次调用它时都会创建一个初始值的新实例了（例子中的空列表[]）。

# 1.6.3 讨论

原则上，构建一个一键多值字典是很容易的。但是如果试着自己对第一个值做初始化操作，这就会变得很杂乱。例如，可能会写下这样的代码：

$\mathrm{d} = \{\}$ for key, value in pairs: if key not in d: $\mathrm{d[key] = [ ]}$ d[key].append(value)

使用 defaultdict 后代码会清晰得多：

```python
d = defaultdict(list)  
for key, value in pairs:  
    d[key].append(value) 
```

这一节的内容同数据处理中的记录归组问题有很强的关联。请参见 1.15 节的示例。

# 1.7 让字典保持有序

# 1.7.1 问题

我们想创建一个字典，同时当对字典做迭代或序列化操作时，也能控制其中元素的顺序。

# 1.7.2 解决方案

要控制字典中元素的顺序，可以使用 collections 模块中的 OrderedDict 类。当对字典做迭代时，它会严格按照元素初始添加的顺序进行。例如：

```python
from collections import OrderedDict  
d = OrderedDict()  
d['foo'] = 1  
d['bar'] = 2  
d['spam'] = 3  
d['grok'] = 4  
# Outputs "foo 1", "bar 2", "spam 3", "grok 4"  
for key in d:  
    print(key, d[key]) 
```

当想构建一个映射结构以便稍后对其做序列化或编码成另一种格式时，OrderedDict 就显得特别有用。例如，如果想在进行 JSON 编码时精确控制各字段的顺序，那么只要首先在 OrderedDict中构建数据就可以了。

```txt
>>> import json
>>> json.dumps(d)
'{"foo": 1, "bar": 2, "spam": 3, "grok": 4}'
>>> 
```

# 1.7.3 讨论

OrderedDict 内部维护了一个双向链表，它会根据元素加入的顺序来排列键的位置。第一个新加入的元素被放置在链表的末尾。接下来对已存在的键做重新赋值不会改变键的顺序。

请注意 OrderedDict的大小是普通字典的 2 倍多，这是由于它额外创建的链表所致。因此，如果打算构建一个涉及大量 OrderedDict 实例的数据结构（例如从 CSV 文件中读取 100000 行内容到OrderedDict列表中），那么需要认真对应用做需求分析，从而判断

使用 OrderedDict 所带来的好处是否能超越因额外的内存开销所带来的缺点。

# 1.8 与字典有关的计算问题

# 1.8.1 问题

我们想在字典上对数据执行各式各样的计算（比如求最小值、最大值、排序等）。

# 1.8.2 解决方案

假设有一个字典在股票名称和对应的价格间做了映射：

prices $=$ { 'ACME':45.23, 'AAPL':612.78, 'IBM':205.55, 'HPQ':37.20, 'FB':10.75 }

为了能对字典内容做些有用的计算，通常会利用 zip()将字典的键和值反转过来。例如，下面的代码会告诉我们如何找出价格最低和最高的股票。

```txt
min_price = min(zip(prices.values(), prices.keys())
# min_price is (10.75, 'FB')
max_price = max(zip(prices.values(), prices.keys())
# max_price is (612.78, 'AAPL') 
```

同样，要对数据排序只要使用 zip()再配合 sorted()就可以了，比如：

```python
prices Sorted = sorted(zip(prices.values(), prices.keys())
# prices Sorted is [(10.75, 'FB'), (37.2, 'HPQ'),
# (45.23, 'ACME'), (205.55, 'IBM'),
# (612.78, 'AAPL'])] 
```

当进行这些计算时，请注意 zip()创建了一个迭代器，它的内容只能被消费一次。例如下面的代码就是错误的：

```python
prices_and_names = zip(pieces.values(), prices.keys())
print(min(pieces_and_names)) # OK
print(max(pieces_and_names)) # ValueError: max() arg is an empty sequence 
```

# 1.8.3 讨论

如果尝试在字典上执行常见的数据操作，将会发现它们只会处理键，而不是值。例如：

min(prices) # Returns 'AAPL'

max(prices) # Returns 'IBM'

这很可能不是我们所期望的，因为实际上我们是尝试对字典的值做计算。可以利用字典的 values()方法来解决这个问题：

min(prices.values()) # Returns 10.75

max(prices.values()) # Returns 612.78

不幸的是，通常这也不是我们所期望的。比如，我们可能想知道相应的键所关联的信息是什么（例如哪支股票的价格最低？）

如果提供一个 key 参数传递给 min()和 max()，就能得到最大值和最小值所对应的键是什么。例如：

```python
min(pieces, key= lambda k: prices[k]) # Returns 'FB'  
max(pieces, key= lambda k: prices[k]) # Returns 'AAPL' 
```

但是，要得到最小值的话，还需要额外执行一次查找。例如：

```python
min_value = prices[min(prices, key= lambda k: prices[k])] 
```

利用了 zip()的解决方案是通过将字典的键-值对“反转”为值-键对序列来解决这个问题的。

当在这样的元组上执行比较操作时，值会先进行比较，然后才是键。这完全符合我们的期望，允许我们用一条单独的语句轻松的对字典里的内容做整理和排序。

应该要注意的是，当涉及（value, key）对的比较时，如果碰巧有多个条目拥有相同的value 值，那么此时 key 将用来作为判定结果的依据。例如，在计算 min()和 max()时，如果碰巧 value 的值相同，则将返回拥有最小或最大 key值的那个条目。示例如下：

>>>prices $=$ {'AAA':45.23，'ZZZ':45.23}   
>>>min(zip(prices.values(),prices.keys()) (45.23，'AAA')   
>>>max(zip(prices.values)，prices.keys()） (45.23，'ZZZ')   
>>>

# 1.9 在两个字典中寻找相同点

# 1.9.1 问题

有两个字典，我们想找出它们中间可能相同的地方（相同的键、相同的值等）。

# 1.9.2 解决方案

考虑如下两个字典：

```python
a = {  
'x': 1,  
'y': 2,  
'z': 3  
}  
b = {  
'w': 10,  
'x': 11,  
'y': 2  
} 
```

要找出这两个字典中的相同之处，只需通过 keys()或者 items()方法执行常见的集合操作即可。例如：

```txt
Find keys in common  
a.keys() & b.keys() # {'x', 'y'}  
# Find keys in a that are not in b  
a.keys() - b.keys() # {'z'}  
# Find (key, value) pairs in common  
a.items() & b.items() # {'y', 2} 
```

这些类型的操作也可用来修改或过滤掉字典中的内容。例如，假设想创建一个新的字典，其中会去掉某些键。下面是使用了字典推导式的代码示例：

```txt
Make a new dictionary with certain keys removed  
c = {key: a[key] for key in a.keys() - {'z', 'w'}}  
# c is {'x': 1, 'y': 2} 
```

# 1.9.3 讨论

字典就是一系列键和值之间的映射集合。字典的 keys()方法会返回 keys-view 对象，其中暴露了所有的键。关于字典的键有一个很少有人知道的特性，那就是它们也支持常见的集合操作，比如求并集、交集和差集。因此，如果需要对字典的键做常见的集合操作，那么就能直接使用 keys-view 对象而不必先将它们转化为集合。

字典的 items()方法返回由(key,value)对组成的 items-view 对象。这个对象支持类似的集合操作，可用来完成找出两个字典间有哪些键值对有相同之处的操作。

尽管类似，但字典的 values()方法并不支持集合操作。部分原因是因为在字典中键和值是不同的，从值的角度来看并不能保证所有的值都是唯一的。单这一条原因就使得某

些特定的集合操作是有问题的。但是，如果必须执行这样的操作，还是可以先将值转化为集合来实现。

# 1.10 从序列中移除重复项且保持元素间顺序不变

# 1.10.1 问题

我们想去除序列中出现的重复元素，但仍然保持剩下的元素顺序不变。

# 1.10.2 解决方案

如果序列中的值是可哈希（hashable）的，那么这个问题可以通过使用集合和生成器轻松解决。示例如下①：

```python
def dedupe(items):
    seen = set()
    for item in items:
        if item not in seen:
            yield item
        seen.add(item) 
```

这里是如何使用这个函数的例子：

```txt
>>> a = [1, 5, 2, 1, 9, 1, 5, 10]  
>>> list(dedupe(a))  
[1, 5, 2, 9, 10]  
>>> 
```

只有当序列中的元素是可哈希的时候才能这么做。如果想在不可哈希的对象（比如列表）序列中去除重复项，需要对上述代码稍作修改：

```python
def dedupe(items, key=None): seen = set() for item in items: val = item if key is None else key(item) if val not in seen: yield item seen.add(val) 
```

这里参数 key 的作用是指定一个函数用来将序列中的元素转换为可哈希的类型，这么做的目的是为了检测重复项。它可以像这样工作：

>>a=[{'x':1,'y':2},{'x':1,'y':3},{'x':1,'y':2},{'x':2,'y':4}]  
>>>list(dedupe(a,key $\equiv$ lambda d:(d['x'],d['y]))）  
{['x':1，'y':2},{'x':1，'y':3},{'x':2，'y':4}]  
>>>list(dedupe(a,key $\equiv$ lambda d:d['x'])）  
{['x':1，'y':2},{'x':2，'y':4}]  
>>>

如果希望在一个较复杂的数据结构中，只根据对象的某个字段或属性来去除重复项，那么后一种解决方案同样能完美工作。

# 1.10.3 讨论

如果想要做的只是去除重复项，那么通常足够简单的办法就是构建一个集合。例如：

```txt
>>>a   
[1，5，2，1，9，1，5，10]   
>>>set(a)   
{1，2，10，5，9}   
>>>
```

但是这种方法不能保证元素间的顺序不变①，因此得到的结果会被打乱。前面展示的解决方案可避免出现这个问题。

本节中对生成器的使用反映出一个事实，那就是我们可能会希望这个函数尽可能的通用—不必绑定在只能对列表进行处理。比如，如果想读一个文件，去除其中重复的文本行，可以只需这样处理：

```txt
with open(somefile,'r') as f: for line in dedupe(f): 
```

我们的 dedupe()函数也模仿了内置函数 sorted()、min()以及 max()对 key 函数的使用方式。例子可参考 1.8 节和 1.13 节。

# 1.11 对切片命名

# 1.11.1 问题

我们的代码已经变得无法阅读，到处都是硬编码的切片索引，我们想将它们清理干净。

# 1.11.2 解决方案

假设有一些代码用来从字符串的固定位置中取出具体的数据（比如从一个平面文件或

类似的格式）①：

012345678901234567890123456789012345678901234567890' record $=$ .100.513.25 cost $\equiv$ int (record[20:32]）\* float (record[40:48])

与其这样做，为什么不对切片命名呢？

SHARES $=$ slice(20,32)   
PRICE $=$ slice(40,48)   
cost $=$ int(record[SHARES]) \* float(record[PRICE])

在后一种版本中，由于避免了使用许多神秘难懂的硬编码索引，我们的代码就变得清晰了许多。

# 1.11.3 讨论

作为一条基本准则，代码中如果有很多硬编码的索引值，将导致可读性和可维护性都不佳。例如，如果一年以后再回过头来看代码，你会发现自己很想知道当初编写这些代码时自己在想些什么。前面展示的方法可以让我们对代码的功能有着更加清晰的认识。

一般来说，内置的 slice()函数会创建一个切片对象，可以用在任何允许进行切片操作的地方。例如：

```txt
>>> items = [0, 1, 2, 3, 4, 5, 6]  
>>> a = slice(2, 4)  
>>> items[2:4]  
[2, 3]  
>>> items[a]  
[2, 3]  
>>> items[a] = [10, 11]  
>>> items  
[0, 1, 10, 11, 4, 5, 6]  
>>> del items[a]  
>>> items  
[0, 1, 4, 5, 6] 
```

如果有一个 slice 对象的实例 s，可以分别通过 s.start、s.stop 以及 s.step 属性来得到关于该对象的信息。例如：

```txt
>>> a = slice()
>>> a.start
10
>>> a.stop 
```

```txt
50  
>>> a_STEP  
2  
>>> 
```

此外，可以通过使用 indices(size)方法将切片映射到特定大小的序列上。这会返回一个(start, stop, step)元组，所有的值都已经恰当地限制在边界以内（当做索引操作时可避免出现 IndexError 异常）。例如：

>>>s $=$ 'HelloWorld'   
>>>aindices(len(s))   
(5,10,2)   
>>>for i in range(\*a indices(len(s)): .print(s[i])   
W   
r   
d   
>>>

# 1.12 找出序列中出现次数昀多的元素

# 1.12.1 问题

我们有一个元素序列，想知道在序列中出现次数最多的元素是什么。

# 1.12.2 解决方案

collections 模块中的 Counter 类正是为此类问题所设计的。它甚至有一个非常方便的most_common()方法可以直接告诉我们答案。

为了说明用法，假设有一个列表，列表中是一系列的单词，我们想找出哪些单词出现的最为频繁。下面是我们的做法：

words $=$ [ look', 'into', my', eyes', look', into', my', eyes', the', eyes', the', eyes', the', eyes', 'not', 'around', 'the', eyes", "don't", look', around', the', eyes', look', into', my', eyes', "you're", under'   
from collections import Counter word_counts $=$ Counter words) top_three $=$ word_counts.best_common(3)   
print(top_three) #Outputs [(eyes, 8), (the, 5), (look, 4)]

# 1.12.3 讨论

可以给 Counter 对象提供任何可哈希的对象序列作为输入。在底层实现中，Counter 是一个字典，在元素和它们出现的次数间做了映射。例如：

```txt
>>> word_counts['not']  
1  
>>> word_counts['eyes']  
8  
>>> 
```

如果想手动增加计数，只需简单地自增即可：

>>>morewords $=$ ['why','are'，'you'，'not'，'looking'，'in'，'my'，'eyes']   
>>>for word in morewords:   
...word_counts[word] $+ = 1$ >>>word_counts['eyes']   
9   
>>>

另一种方式是使用 update()方法。

```html
>>> word_counts.update(morewords)  
>>> 
```

关于 Counter 对象有一个不为人知的特性，那就是它们可以轻松地同各种数学运算操作结合起来使用。例如：

```txt
>>> a = Counter words)  
>>> b = Counter (morewords)  
>>> a  
Counter({'eyes': 8, 'the': 5, 'look': 4, 'into': 3, 'my': 3, 'around': 2, "you're": 1, "don't": 1, 'under': 1, 'not': 1})  
>>> b  
Counter({'eyes': 1, 'looking': 1, 'are': 1, 'in': 1, 'not': 1, 'you': 1, 'my': 1, 'why': 1})  
>>> # Combine counts  
>>> c = a + b  
>>> c  
Counter({'eyes': 9, 'the': 5, 'look': 4, 'my': 4, 'into': 3, 'not': 2, 'around': 2, "you're": 1, "don't": 1, 'in': 1, 'why': 1, 'looking': 1, 'are': 1, 'under': 1, 'you': 1})  
>>> # Subtract counts  
>>> d = a - b 
```

```javascript
>>>d Counter({'eyes':7,'the':5,'look':4,'into':3,'my':2,'around':2, "you're":1,"don't":1，'under':1}) >>> 
```

不用说，当面对任何需要对数据制表或计数的问题时，Counter对象都是你手边的得力工具。比起利用字典自己手写算法，更应该采用这种方式完成任务。

# 1.13 通过公共键对字典列表排序

# 1.13.1 问题

我们有一个字典列表，想根据一个或多个字典中的值来对列表排序。

# 1.13.2 解决方案

利用 operator 模块中的 itemgetter 函数对这类结构进行排序是非常简单的。假设通过查询数据库表项获取网站上的成员列表，我们得到了如下的数据结构：

```txt
rows = [  
{ 'fname': 'Brian', 'lname': 'Jones', 'uid': 1003},  
{ 'fname': 'David', 'lname': 'Beazley', 'uid': 1002},  
{ 'fname': 'John', 'lname': 'Cleese', 'uid': 1001},  
{ 'fname': 'Big', 'lname': 'Jones', 'uid': 1004}  
] 
```

根据所有的字典中共有的字段来对这些记录排序是非常简单的，示例如下：

from operator import itemgetter   
rows_by_fname $=$ sortedrows,key $\equiv$ itemgetter('fname'))   
rows_by_uid $=$ sortedrows,key $\equiv$ itemgetter('uid'))   
printrows_by_fname)   
printrows_by_uid)

以上代码的输出为：

```javascript
{fname:'Big'，'uid':1004，'lname':'Jones'}, {fname:'Brian'，'uid':1003，'lname':'Jones'}, {fname:'David'，'uid':1002，'lname':'Beazley'}, {fname:'John'，'uid':1001，'lname':'Cleese'}]   
{fname:'John'，'uid':1001，'lname':'Cleese'}, {fname:'David'，'uid':1002，'lname':'Beazley'}, {fname:'Brian'，'uid':1003，'lname':'Jones'}, {fname:'Big'，'uid':1004，'lname':'Jones'}) 
```

itemgetter()函数还可以接受多个键。例如下面这段代码：

rows_by_lfname = sorted(rows, key $\equiv$ itemgetter('lname', 'fname'))   
print (rows_by_lfname)

这会产生如下的输出：

```txt
[ \{ \text{'fname': 'David', 'uid': 1002, 'lname': 'Beazley'}, \} ]  
[ \{ \text{'fname': 'John', 'uid': 1001, 'lname': 'Cleese'}, \} ]  
[ \{ \text{'fname': 'Big', 'uid': 1004, 'lname': 'Jones'}, \} ]  
[ \{ \text{'fname': 'Brian', 'uid': 1003, 'lname': 'Jones'}} \} ] 
```

# 1.13.3 讨论

在这个例子中，rows被传递给内建的sorted()函数，该函数接受一个关键字参数key。这个参数应该代表一个可调用对象（callable），该对象从 rows 中接受一个单独的元素作为输入并返回一个用来做排序依据的值。itemgetter()函数创建的就是这样一个可调用对象。

函数 operator.itemgetter()接受的参数可作为查询的标记，用来从 rows 的记录中提取出所需要的值。它可以是字典的键名称、用数字表示的列表元素或是任何可以传给对象的__getitem__()方法的值。如果传多个标记给 itemgetter()，那么它产生的可调用对象将返回一个包含所有元素在内的元组，然后 sorted()将根据对元组的排序结果来排列输出结果。如果想同时针对多个字段做排序（比如例子中的姓和名），那么这是非常有用的。

有时候会用 lambda 表达式来取代 itemgetter()的功能。例如：

```python
rows_by_fname = sorted(rows, key=lambda r: r['fname'])  
rows_by_lfname = sorted(rows, key=lambda r: (r['lname'], r['fname'])) 
```

这种解决方案通常也能正常工作。但是用 itemgetter()通常会运行得更快一些。因此如果需要考虑性能问题的话，应该使用 itemgetter()。

最后不要忘了本节中所展示的技术同样适用于 min()和 max()这样的函数。例如：

>>> min(rows, key $\coloneqq$ itemgetter('uid'))
{'fname': 'John', 'lname': 'Cleese', 'uid': 1001}
>>> max(rows, key $\coloneqq$ itemgetter('uid'))
{'fname': 'Big', 'lname': 'Jones', 'uid': 1004}
>>>

# 1.14 对不原生支持比较操作的对象排序

# 1.14.1 问题

我们想在同一个类的实例之间做排序，但是它们并不原生支持比较操作。

# 1.14.2 解决方案

内建的 sorted()函数可接受一个用来传递可调用对象（callable）的参数 key，而该可调用对象会返回待排序对象中的某些值，sorted 则利用这些值来比较对象。例如，如果应用中有一系列的 User 对象实例，而我们想通过 user_id 属性来对它们排序，则可以提供一个可调用对象将 User 实例作为输入然后返回 user_id。示例如下：

>>>classUser:   
... def__init__(self, user_id):   
... self.user_id $=$ user_id   
... def repr_self):   
... return'User({})'.format(self.user_id)   
>>>users $=$ [User(23)，User(3)，User(99)]   
>>>users   
[User(23)，User(3)，User(99)]   
>>>sorted/users,key $\equiv$ lambdau:u.user_id)   
[User(3)，User(23)，User(99)]   
>>>

除了可以用 lambda 表达式外，另一种方式是使用 operator.attrgetter()。

```txt
>>> from operator import attrgetter
>>> sortedUsers, key=attrgetter('user_id'))
[User(3), User(23), User(99)]
>>> 
```

# 1.14.3 讨论

要使用 lambda 表达式还是 attrgetter()或许只是一种个人喜好。但是通常来说，attrgetter()要更快一些，而且具有允许同时提取多个字段值的能力。这和针对字典的 operator.itemgetter()的使用很类似（参见 1.13 节）。例如，如果 User 实例还有一个 first_name 和 last_name属性的话，可以执行如下的排序操作：

```python
by_name = sorted/users, key=attrgetter('last_name', 'first_name')) 
```

同样值得一提的是，本节所用到的技术也适用于像 min()和 max()这样的函数。例如：

```txt
>>> min/users, key=attrgetter('user_id')
User(3)
>>> max/users, key=attrgetter('user_id')
User(99)
>>> 
```

# 1.15 根据字段将记录分组

# 1.15.1 问题

有一系列的字典或对象实例，我们想根据某个特定的字段（比如说日期）来分组迭代数据。

# 1.15.2 解决方案

itertools.groupby()函数在对数据进行分组时特别有用。为了说明其用途，假设有如下的字典列表：

```python
rows = [  
{'address': '5412 N CLARK', 'date': '07/01/2012'},  
{'address': '5148 N CLARK', 'date': '07/04/2012'},  
{'address': '5800 E 58TH', 'date': '07/02/2012'},  
{'address': '2122 N CLARK', 'date': '07/03/2012'},  
{'address': '5645 N RAVENSWOOD', 'date': '07/02/2012'},  
{'address': '1060 W ADDISON', 'date': '07/02/2012'},  
{'address': '4801 N BROADWAY', 'date': '07/01/2012'},  
{'address': '1039 W GRANVILLE', 'date': '07/04/2012'},  
] 
```

现在假设想根据日期以分组的方式迭代数据。要做到这些，首先以目标字段（在这个例子中是 date）来对序列排序，然后再使用 itertools.groupby()。

from operator import itemgetter   
from itertools import groupby   
#Sort by the desired field first   
rows.sort(key $\equiv$ itemgetter('date'))   
# Iterate in groups   
for date, items in groupbyrows, key $\equiv$ itemgetter('date)): print(date) for i in items: print(''，i)

这会产生如下的输出：

```txt
07/01/2012
{'date': '07/01/2012', 'address': '5412 N CLARK'} 
```

```txt
{'date':07/02/2012'，'address':5645NRAVENSWOOD'}   
{'date':07/02/2012'，'address':1060WADDISON'}   
07/03/2012   
{'date':07/03/2012'，'address':2122NCLARK'}   
07/04/2012   
{'date':07/04/2012'，'address':5148NCLARK'}   
{'date':07/04/2012'，'address':1039WGRANVILLE'} 
```

# 1.15.3 讨论

函数 groupby()通过扫描序列找出拥有相同值（或是由参数 key指定的函数所返回的值）的序列项，并将它们分组。groupby()创建了一个迭代器，而在每次迭代时都会返回一个值（value）和一个子迭代器（sub_iterator），这个子迭代器可以产生所有在该分组内具有该值的项。

在这里重要的是首先要根据感兴趣的字段对数据进行排序。因为 groupby()只能检查连续的项，不首先排序的话，将无法按所想的方式来对记录分组。

如果只是简单地根据日期将数据分组到一起，放进一个大的数据结构中以允许进行随机访问，那么利用 defaultdict()构建一个一键多值字典（multidict，见 1.6 节）可能会更好。例如：

```python
from collections import defaultdict  
rows_by_date = defaultdict(list)  
for row in rows:  
    rows_by_date[row['date'].append(row) 
```

这使得我们可以方便地访问每个日期的记录，如下所示：

```python
>>> for r in rows_by_date['07/01/2012']:  
... print(r)  
...  
{'date': '07/01/2012', 'address': '5412 N CLARK'}  
{'date': '07/01/2012', 'address': '4801 N BROADWAY'}  
>>> 
```

对于后面这个例子，我们并不需要先对记录做排序。因此，如果不考虑内存方面的因素，这种方式会比先排序再用 groupby()迭代要来的更快。

# 1.16 筛选序列中的元素

# 1.16.1 问题

序列中含有一些数据，我们需要提取出其中的值或根据某些标准对序列做删减。

# 1.16.2 解决方案

要筛选序列中的数据，通常最简单的方法是使用列表推导式（list comprehension）。例如：

```txt
>>> mylist = [1, 4, -5, 10, -7, 2, 3, -1]  
>>> [n for n in mylist if n > 0]  
[1, 4, 10, 2, 3]  
>>> [n for n in mylist if n < 0]  
[-5, -7, -1]  
>>> 
```

使用列表推导式的一个潜在缺点是如果原始输入非常大的话，这么做可能会产生一个庞大的结果。如果这是你需要考虑的问题，那么可以使用生成器表达式通过迭代的方式产生筛选的结果。例如：

>>>pos $=$ (n for n in mylist if $\mathrm{n} > 0$ >>pos   
<generator object <genexpr> at 0x1006a0eb0>   
>>for x in pos:   
... print(x)   
1   
4   
10   
2   
3   
>>>

有时候筛选的标准没法简单地表示在列表推导式或生成器表达式中。比如，假设筛选过程涉及异常处理或者其他一些复杂的细节。基于此，可以将处理筛选逻辑的代码放到单独的函数中，然后使用内建的 filter()函数处理。示例如下：

values $=$ ['1'，2'，-3'，'-'，4'，N/A'，5']   
def is_int(val): try: $\mathrm{x} =$ int(val) return True except ValueError: return False   
ivals $=$ list.filter(is_int, values))   
print(ivals) #Outputs['1'，'2'，'-3'，'4'，'5']

filter()创建了一个迭代器，因此如果我们想要的是列表形式的结果，请确保加上了 list()，就像示例中那样。

# 1.16.3 讨论

列表推导式和生成器表达式通常是用来筛选数据的最简单和最直接的方式。此外，它们也具有同时对数据做转换的能力。例如：

```txt
>>> mylist = [1, 4, -5, 10, -7, 2, 3, -1]  
>>> import math  
>>> [math.sqrt(n) for n in mylist if n > 0]  
[1.0, 2.0, 3.1622776601683795, 1.4142135623730951, 1.7320508075688772]  
>>> 
```

关于筛选数据，有一种情况是用新值替换掉不满足标准的值，而不是丢弃它们。例如，除了要找到正整数之外，我们也许还希望在指定的范围内将不满足要求的值替换掉。通常，这可以通过将筛选条件移到一个条件表达式中来轻松实现。就像下面这样：

>>>clip_neg $=$ [n if n $>0$ else 0 for n in mylist]   
>>>clip_neg   
[1,4,0,10,0,2,3,0]   
>>>clip_pos $=$ [n if n $<  0$ else 0 for n in mylist]   
>>>clip_pos   
[0,0,-5,0,-7,0,0,-1]   
>>>

另一个值得一提的筛选工具是 itertools.compress()，它接受一个可迭代对象以及一个布尔选择器序列作为输入。输出时，它会给出所有在相应的布尔选择器中为 True 的可迭代对象元素。如果想把对一个序列的筛选结果施加到另一个相关的序列上时，这就会非常有用。例如，假设有以下两列数据：

```python
addresses = [ '5412 N CLARK', '5148 N CLARK', '5800 E 58TH', '2122 N CLARK' '5645 N RAVENSWOOD', '1060 W ADDISON', '4801 N BROADWAY', '1039 W GRANVILLE', ] counts = [ 0, 3, 10, 4, 1, 7, 6, 1] 
```

现在我们想构建一个地址列表，其中相应的 count值要大于5。下面是我们可以尝试的

方法：

```haskell
>>> from itertools import compress
>>> more5 = [n > 5 for n in counts]
>>> more5
False, False, True, False, False, True, True, False)
>>> list(compress addresses, more5))
['5800 E 58TH', '4801 N BROADWAY', '1039 W GRANVILLE']
>>> 
```

这里的关键在于首先创建一个布尔序列，用来表示哪个元素可满足我们的条件。然后compress()函数挑选出满足布尔值为 True 的相应元素。

同 filter()函数一样，正常情况下 compress()会返回一个迭代器。因此，如果需要的话，得使用 list()将结果转为列表。

# 1.17 从字典中提取子集

# 1.17.1 问题

我们想创建一个字典，其本身是另一个字典的子集。

# 1.17.2 解决方案

利用字典推导式（dictionary comprehension）可轻松解决。例如：

prices $=$ { 'ACME':45.23, 'AAPL':612.78, 'IBM':205.55, 'HPQ':37.20, 'FB':10.75 } #Makeadictionaryofall pricesover 200 p1 $=$ {key:valueforkey，valueinprices.items(）ifvalue $>200$ } #Makeadictionaryoftechstockstech_names $\equiv$ {'AAPL'，'IBM'，'HPQ'，'MSFT'} p2 $=$ {key:valueforkey, value inprices.items(）ifkeyintech_names}

# 1.17.3 讨论

大部分可以用字典推导式解决的问题也可以通过创建元组序列然后将它们传给 dict()函

数来完成。例如：

```txt
p1 = dict(key, value) for key, value in prices.items() if value > 200 
```

但是字典推导式的方案更加清晰，而且实际运行起来也要快很多（以本例中的字典prices 来测试，效率要高 2 倍多）。

有时候会有多种方法来完成同一件事情。例如，第二个例子还可以重写成：

```txt
Make a dictionary of tech stocks  
tech_names = {'AAPL', 'IBM', 'HPQ', 'MSFT'}  
p2 = {key:prices[key] for key in prices.keys() & tech_names} 
```

但是，计时测试表明这种解决方案几乎要比第一种慢上 1.6 倍。如果需要考虑性能因素，那么通常都需要花一点时间来研究它。有关计时和性能分析方面的信息，请参见14.13 节。

# 1.18 将名称映射到序列的元素中

# 1.18.1 问题

我们的代码是通过位置（即索引，或下标）来访问列表或元组的，但有时候这会使代码变得有些难以阅读。我们希望可以通过名称来访问元素，以此减少结构中对位置的依赖性。

# 1.18.2 解决方案

相比普通的元组，collections.namedtuple()（命名元组）只增加了极小的开销就提供了这些便利。实际上 collections.namedtuple()是一个工厂方法，它返回的是 Python 中标准元组类型的子类。我们提供给它一个类型名称以及相应的字段，它就返回一个可实例化的类、为你已经定义好的字段传入值等。例如：

```python
>>> from collections import namedtuple
>>> Subscriber = namedtuple('Subscriber', ['addr', 'joined}]
>>> sub = Subscriber('jonesy@example.com', '2012-10-19')
>>> sub
Subscriber(addr='jonesy@example.com', joined='2012-10-19')
>>> sub(addr
'jonesy@example.com'
>>> sub.jonesy@example.com
>>> sub.jonesy@example.com
>>> 2012-10-19 
```

尽管 namedtuple 的实例看起来就像一个普通的类实例，但它的实例与普通的元组是可互换

的，而且支持所有普通元组所支持的操作，例如索引（indexing）和分解（unpacking）。比如：

>>>len(sub)   
2   
>>>addr，joined $=$ sub   
>>>addr   
'Jonesy@example.com'   
>>>joined   
'2012-10-19'   
>>>

命名元组的主要作用在于将代码同它所控制的元素位置间解耦。所以，如果从数据库调用中得到一个大型的元组列表，而且通过元素的位置来访问数据，那么假如在表单中新增了一列数据，那么代码就会崩溃。但如果首先将返回的元组转型为命名元组，就不会出现问题。

为了说明这个问题，下面有一些使用普通元组的代码：

```python
def compute_cost(records):
    total = 0.0
    for rec in records:
        total += rec[1] * rec[2]
    return total 
```

通过位置来引用元素常常使得代码的表达力不够强，而且也很依赖于记录的具体结构。下面是使用命名元组的版本：

from collections import namedtuple   
Stock $=$ namedtuple('Stock'，['name'，'shares'，'price']) def compute_cost(records): total $= 0.0$ for rec in records: s $=$ Stock(\*rec) total $+ =$ s.share \*s.price return total

当然，如果示例中的 records 序列已经包含了这样的实例，那么可以避免显式地将记录转换为 Stock 命名元组①。

# 1.18.3 讨论

namedtuple 的一种可能用法是作为字典的替代，后者需要更多的空间来存储。因此，如

果要构建涉及字典的大型数据结构，使用 namedtuple 会更加高效。但是请注意，与字典不同的是，namedtuple 是不可变的（immutable）。例如：

```txt
>>> s = Stock('ACME', 100, 123.45)
>>> s
Stock(name='ACME', shares=100, price=123.45)
>>> sshares = 75
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
AttributeError: can't set attribute
>>> 
```

如果需要修改任何属性，可以通过使用 namedtuple 实例的_replace()方法来实现。该方法会创建一个全新的命名元组，并对相应的值做替换。示例如下：

```txt
>>> s = s._replace(shares=75)
>>> s
Stock(name='ACME', shares=75, price=123.45)
>>> 
```

_replace()方法有一个微妙的用途，那就是它可以作为一种简便的方法填充具有可选或缺失字段的命名元组。要做到这点，首先创建一个包含默认值的“原型”元组，然后使用_replace()方法创建一个新的实例，把相应的值替换掉。示例如下：

from collections import namedtuple   
Stock $=$ namedtuple('Stock'，['name'，'shares'，'price'，'date'，'time']) #Create a prototype instance stock prototypes $\equiv$ Stock(''，0，0.0，None，None) #Function to convert a dictionary to a Stock def dict_to_stock(s): return stockprototype._replace(\*\*s)

让我们演示一下上面的代码是如何工作的：

```python
>>> a = {'name': 'ACME', 'shares': 100, 'price': 123.45}
>>> dict_to_stock(a)
Stock(name='ACME', shares=100, price=123.45, date=None, time=None)
>>> b = {'name': 'ACME', 'shares': 100, 'price': 123.45, 'date': '12/17/2012'}
>>> dict_to_stock(b)
Stock(name='ACME', shares=100, price=123.45, date='12/17/2012', time=None)
>>> 
```

最后，也是相当重要的是，应该要注意如果我们的目标是定义一个高效的数据结构，

而且将来会修改各种实例属性，那么使用 namedtuple 并不是最佳选择。相反，可以考虑定义一个使用__slots__属性的类（参见 8.4 节）。

# 1.19 同时对数据做转换和换算

# 1.19.1 问题

我们需要调用一个换算（reduction）函数（例如 sum()、min()、max()），但首先得对数据做转换或筛选。

# 1.19.2 解决方案

有一种非常优雅的方式能将数据换算和转换结合在一起—在函数参数中使用生成器表达式。例如，如果想计算平方和，可以像下面这样做：

$\mathrm{nums} = [1,2,3,4,5]$ $\mathbf{s} = \mathrm{sum}(\mathbf{x}^{\star}\mathbf{x}$ for $\mathbf{x}$ in $\mathrm{nums})$

这里还有一些其他的例子：

```python
Determine if any.py files exist in a directory import os  
files = os.listdir('dirname')  
if any(name.endsWith(.py) for name in files):  
    print('There be python!')  
else:  
    print('Sorry, no python.')  
# Output a tuple as CSV  
s = ('ACME', 50, 123.45)  
print('./join(str(x) for x in s))  
# Data reduction across fields of a data structure  
portfolio = [  
    {'name':'GOOG','shares':50},  
    {'name':'YHOO','shares':75},  
    {'name':'AOL','shares':20},  
    {'name':'SCOX','shares':65}  
]  
min_shares = min(s['shares'] for s in portfolio) 
```

# 1.19.3 讨论

这种解决方案展示了当把生成器表达式作为函数的单独参数时在语法上的一些微妙之

处（即，不必重复使用括号）。比如，下面这两行代码表示的是同一个意思：

```python
s = sum((x * x for x in nums)) # Pass generator-expr as argument
s = sum(x * x for x in nums) # More elegant syntax 
```

比起首先创建一个临时的列表，使用生成器做参数通常是更为高效和优雅的方式。例如，如果不使用生成器表达式，可能会考虑下面这种实现：

$\mathrm{nums} = [1,2,3,4,5]$ $\mathbf{s} = \mathbf{\sum}([\mathbf{x}^{*}\mathbf{x}$ for $\mathbf{x}$ in $\mathrm{nums}])$

这也能工作，但这引入了一个额外的步骤而且创建了额外的列表。对于这么小的一个列表，这根本就无关紧要，但是如果 nums 非常巨大，那么就会创建一个庞大的临时数据结构，而且只用一次就要丢弃。基于生成器的解决方案可以以迭代的方式转换数据，因此在内存使用上要高效得多。

某些特定的换算函数比如 min()和 max()都可接受一个 key 参数，当可能倾向于使用生成器时会很有帮助。例如在 portfolio 的例子中，也许会考虑下面这种替代方案：

```python
Original: Returns 20  
min_shares = min(s['shares'] for s in portfolio)  
# Alternative: Returns {'name': 'AOL', 'shares': 20}  
min_shares = min.portfolio, key= lambda s: s['shares']) 
```

# 1.20 将多个映射合并为单个映射

# 1.20.1 问题

我们有多个字典或映射，想在逻辑上将它们合并为一个单独的映射结构，以此执行某些特定的操作，比如查找值或检查键是否存在。

# 1.20.2 解决方案

假设有两个字典：

```python
a = {''': 1, 'z': 3}  
b = {''': 2, 'z': 4} 
```

现在假设想执行查找操作，我们必须得检查这两个字典（例如，先在 a 中查找，如果没找到再去 b 中查找）。一种简单的方法是利用 collections 模块中的 ChainMap 类来解决这个问题。例如：

```python
from collections import ChainMap  
c = ChainMap(a, b) 
```

```python
print(c['x']) # Outputs 1 (from a)  
print(c['y']) # Outputs 2 (from b)  
print(c['z']) # Outputs 3 (from a) 
```

# 1.20.3 讨论

ChainMap 可接受多个映射然后在逻辑上使它们表现为一个单独的映射结构。但是，这些映射在字面上并不会合并在一起。相反，ChainMap 只是简单地维护一个记录底层映射关系的列表，然后重定义常见的字典操作来扫描这个列表。大部分的操作都能正常工作。例如：

```txt
>>>len(c)   
3   
>>>list(c.keys())   
['x'，'y'，'z']   
>>>list(c.values())   
[1,2,3]   
>>> 
```

如果有重复的键，那么这里会采用第一个映射中所对应的值。因此，例子中的 c[‘z’]总是引用字典 a 中的值，而不是字典 b 中的值。

修改映射的操作总是会作用在列出的第一个映射结构上。例如：

```python
>>> c['z'] = 10  
>>> c['w'] = 40  
>>> del c['x']  
>>> a  
{'w': 40, 'z': 10}  
>>> del c['y']  
Traceback (most recent call last):  
...  
KeyError: "Key not found in the first mapping: 'y'"  
>>> 
```

ChainMap 与带有作用域的值，比如编程语言中的变量（即全局变量、局部变量等）一起工作时特别有用。实际上这里有一些方法使这个过程变得简单：

```python
>>> values = ChainMap()
>>> values['x'] = 1
>>> # Add a new mapping
>>> values = values.new_child()
>>> values['x'] = 2
>>> # Add a new mapping
>>> values = values.new_child()
>>> values['x'] = 3 
```

>>> values  
ChainMap{'x':3},{'x':2},{'x':1})  
>>>values['x']  
3  
>>>#Discard last mapping  
>>>values $=$ values.parents  
>>>values['x']  
2  
>>>#Discard last mapping  
>>>values $=$ values.parents  
>>>values['x']  
1  
>>>values  
ChainMap({'x':1})  
>>>

作为 ChainMap 的替代方案，我们可能会考虑利用字典的 update()方法将多个字典合并在一起。例如：

```txt
>>> a = ['x': 1, 'z': 3]  
>>> b = ['y': 2, 'z': 4]  
>>> merged = dict(b)  
>>> merged.update(a)  
>>> merged['x']  
1  
>>> merged['y']  
2  
>>> merged['z']  
3  
>>> 
```

这么做行得通，但这需要单独构建一个完整的字典对象（或者修改其中现有的一个字典，这就破坏了原始数据）。此外，如果其中任何一个原始字典做了修改，这个改变都不会反应到合并后的字典中。例如：

```python
>>> a['x'] = 13  
>>> merged['x']  
1 
```

而 ChainMap 使用的就是原始的字典，因此它不会产生这种令人不悦的行为。示例如下：

```txt
>>> a = {'x': 1, 'z': 3}  
>>> b = {'y': 2, 'z': 4}  
>>> merged = ChainMap(a, b)  
>>> merged['x']  
1  
>>> a['x'] = 42  
>>> merged['x'] # Notice change to merged dicts  
42  
>>> 
```

# 字符串和文本

无论是解析数据还是产生输出，几乎每一个有实用价值的程序都会涉及某种形式的文本处理。本章的重点放在有关文本操作的常见问题上，例如拆分字符串、搜索、替换、词法分析以及解析。许多任务都可以通过内建的字符串方法轻松解决。但是，更复杂的操作可能会需要用到正则表达式或者创建完整的解析器才能得到解决。以上所有主题本章都有涵盖。此外，本章还提到了一些同 Unicode 打交道时用到的技巧。

# 2.1 针对任意多的分隔符拆分字符串

# 2.1. 1 问题

我们需要将字符串拆分为不同的字段，但是分隔符（以及分隔符之间的空格）在整个字符串中并不一致。

# 2.1.2 解决方案

字符串对象的 split()方法只能处理非常简单的情况，而且不支持多个分隔符，对分隔符周围可能存在的空格也无能为力。当需要一些更为灵活的功能时，应该使用 re.split()方法：

```python
>>> line = 'asdf fjdk; afed, fjek, asdf, foo'  
>>> import re  
>>> re.split(r'[;, \s] \s*', line)  
['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo'] 
```

# 2.1.3 讨论

re.split()是很有用的，因为可以为分隔符指定多个模式。例如，在上面的解决方案中，

分隔符可以是逗号、分号或者是空格符（后面可跟着任意数量的额外空格）。只要找到了对应的模式，无论匹配点的两端是什么字段，整个匹配的结果就成为那个分隔符。最终得到的结果是字段列表，同 str.split()得到的结果一样。

当使用 re.split()时，需要小心正则表达式模式中的捕获组（capture group）是否包含在了括号中。如果用到了捕获组，那么匹配的文本也会包含在最终结果中。比如，看看下面的结果：

```txt
>>> fields = re.split(r'(|,|\\s)\s*', line)  
>>> fields  
['asdf', ' ', 'fjdk', ';' 'afed', ';' 'fjek', ';' 'asdf', ';' 'foo']  
>>>
```

在特定的上下文中获取到分隔字符也可能是有用的。例如，也许稍后要用到分隔字符来改进字符串的输出：

```prolog
>>> values = fields[:2]  
>>> delimiters = fields[1::2] + ['']  
>>> values  
['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']  
>>> delimiters  
['', ', ', ', ', ', ', ', ']  
>>> # Reform the line using the same delimiters  
>>> '.join(v+d for v,d in zip(values, delimiters))'  
'asdf fjdk;afed,fjek,asdf,foo'  
>>> 
```

如果不想在结果中看到分隔字符，但仍然想用括号来对正则表达式模式进行分组，请确保用的是非捕获组，以(?:...)的形式指定。示例如下：

```txt
>>>re.split(r'(:;|;|\s)\\s*',line) ['asdf','fjdk','afed','fjek','asdf','foo'] >>> 
```

# 2.2 在字符串的开头或结尾处做文本匹配

# 2.2.1 问题

我们需要在字符串的开头或结尾处按照指定的文本模式做检查，例如检查文件的扩展名、URL 协议类型等。

# 2.2.2 解决方案

有一种简单的方法可用来检查字符串的开头或结尾，只要使用 str.startswith()和

str.endswith()方法就可以了。示例如下：

>>>filename $=$ 'spam.txt'   
>>>filename.endsWith(.txt')   
True   
>>>filename.startswith('file:')   
False   
>>>url $=$ 'http://www.python.org'   
>>>url.startswith('http:')   
True   
>>>

如果需要同时针对多个选项做检查，只需给 startswith()和 endswith()提供包含可能选项的元组即可：

```txt
>>> import os
>>> filenames = os.listdir('./')
>>> filenames
['Makefile', 'foo.c', 'bar.py', 'spam.c', 'spam.h']
>>> [name for name in filenames if name.endsWith(['.c', '.h'])]
['foo.c', 'spam.c', 'spam.h']
>>> any(name.endsWith(['.py']) for name in filenames)
True
>>> 
```

这里有另一个例子：

```python
from urllib.request import urlopen   
def read_data(name): if name.startswith('http','https:'，'ftp:')) return urlopen(name).read() else: with open(name) as f: return f.read() 
```

奇怪的是，这是 Python 中需要把元组当成输入的一个地方。如果我们刚好把选项指定在了列表或集合中，请确保首先用 tuple()将它们转换成元组。示例如下：

```txt
>>> choices = ['http:','ftp:']
>>> url = 'http://www.python.org'
>>> url.startswith(choices)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: startswith first arg must be str or a tuple of str, not list
>>> url.startswith(tuple(choices))
True
>>> 
```

# 2.2.3 讨论

startswith()和 endswith()方法提供了一种非常方便的方式来对字符串的前缀和后缀做基本的检查。类似的操作也可以用切片来完成，但是那种方案不够优雅。例如：

>>>filename $=$ 'spam.txt'   
>>>filename[-4:] $= =$ '.txt'   
True   
>>>url $=$ 'http://www.python.org'   
>>>url[:5] $= =$ 'http:' or url[:6] $= =$ 'https:' or url[:4] $= =$ 'ftp:'   
True   
>>>

可能我们也比较倾向于使用正则表达式作为替代方案。例如：

```txt
>>> import re
>>> url = 'http://www.python.org'
>>> re.match('http://https://ftp:','url')
<_sre.SRE_Match object at 0x101253098>
>>> 
```

这也行得通，但是通常对于简单的匹配来说有些过于重量级了。使用本节提到的技术会更简单，运行得也更快。

最后但同样重要的是，当 startswith()和 endswith()方法和其他操作（比如常见的数据整理操作）结合起来时效果也很好。例如，下面的语句检查目录中有无出现特定的文件：

```txt
if any(name.endsWith(['.c', '.h']) for name in listdir(dirname)): 
```

# 2.3 利用 Shell 通配符做字符串匹配

# 2.3.1 问题

当工作在 UNIX Shell 下时，我们想使用常见的通配符模式（即，*.py、Dat[0-9]*.csv等）来对文本做匹配。

# 2.3.2 解决方案

fnmatch 模块提供了两个函数—fnmatch()和 fnmatchcase()—可用来执行这样的匹配。使用起来很简单：

```python
>>> from fnmatch import fnmatch, fnmatchcase  
>>> fnmatch('foo.txt', '*.txt')  
True 
```

```txt
>>> fnmatch('foo.txt', '?oo.txt')  
True  
>>> fnmatch('Dat45.csv', 'Dat[0-9]*')  
True  
>>> names = ['Dat1.csv', 'Dat2.csv', 'config.ini', 'foo.py']  
>>> [name for name in names if fnmatch(name, 'Dat*.csv')]  
['Dat1.csv', 'Dat2.csv']  
>>> 
```

一般来说，fnmatch()的匹配模式所采用的大小写区分规则和底层文件系统相同（根据操作系统的不同而有所不同）。例如：

```txt
>>> # On OS X (Mac)  
>>> fnmatch('foo.txt', '*TXT')  
False  
>>> # On Windows  
>>> fnmatch('foo.txt', '*TXT')  
True  
>>> 
```

如果这个区别对我们而言很重要，就应该使用 fnmatchcase()。它完全根据我们提供的大小写方式来匹配：

```txt
>>>fnmatchcase('foo.txt'，'\*.TXT') False   
>>>
```

关于这些函数，一个常被忽略的特性是它们在处理非文件名式的字符串时的潜在用途。例如，假设有一组街道地址，就像这样：

```python
addresses = [ '5412 N CLARK ST', '1060 W ADDISON ST', '1039 W GRANVILLE AVE', '2122 N CLARK ST', '4802 N BROADWAY', ] 
```

可以像下面这样写列表推导式：

```txt
>>> from fnmatch import fnmatchcase
>>> [addr for addr in addresses if fnmatchcase(addr, '*ST')] ['5412 N CLARK ST', '1060 W ADDISON ST', '2122 N CLARK ST']
>>> [addr for addr in addresses if fnmatchcase(addr, '54[0-9][0-9] *CLARK*)]
['5412 N CLARK ST']
>>> 
```

# 2.3.3 讨论

fnmatch 所完成的匹配操作有点介乎于简单的字符串方法和全功能的正则表达式之间。如果只是试着在处理数据时提供一种简单的机制以允许使用通配符，那么通常这都是个合理的解决方案。

如果实际上是想编写匹配文件名的代码，那应该使用 glob 模块来完成，请参见5.13 节。

# 2.4 文本模式的匹配和查找

# 2.4.1 问题

我们想要按照特定的文本模式进行匹配或查找。

# 2.4.2 解决方案

如果想要匹配的只是简单的文字，那么通常只需要用基本的字符串方法就可以了，比如 str.find()、str.endswith()、str.startswith()或类似的函数。示例如下：

```txt
>>> text = 'yeah, but no, but yeah, but no, but yeah'  
>>> # Exact match  
>>> text == 'yeah'  
False  
>>> # Match at start or end  
>>> text.startswith('yeah')  
True  
>>> text.endsWith('no')  
False  
>>> # Search for the location of the first occurrence  
>>> text.find('no')  
10  
>>> 
```

对于更为复杂的匹配则需要使用正则表达式以及 re 模块。为了说明使用正则表达式的基本流程，假设我们想匹配以数字形式构成的日期，比如“11/27/2012”。示例如下：

```txt
>>> text1 = '11/27/2012'
>>> text2 = 'Nov 27, 2012'
>>> import re 
```

>>> # Simple matching: $\backslash d+$ means match one or more digits  
>>> if re.match(r'\d+/\\d+/\\d+', text1):  
... print('yes')  
... else:  
... print('no')  
...  
yes  
>>> if re.match(r'\d+/\\d+/\\d+', text2):  
... print('yes')  
... else:  
... print('no')  
...  
no  
>>>

如果打算针对同一种模式做多次匹配，那么通常会先将正则表达式模式预编译成一个模式对象。例如：

>>>datepat $=$ re.compile(r'\d+/\\d+/\\d+)   
>>if datepat.match(text1): . print('yes') ... else: . print('no') .. yes   
>>if datepat.match(text2): . print('yes') ... else: . print('no') .. no   
>>>

match()方法总是尝试在字符串的开头找到匹配项。如果想针对整个文本搜索出所有的匹配项，那么就应该使用 findall()方法。例如：

```txt
>>> text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'
>>> datepat.findall(text)
['11/27/2012', '3/13/2013']
>>> 
```

当定义正则表达式时，我们常会将部分模式用括号包起来的方式引入捕获组。例如：

```txt
>>> datepat = re.compile(r'(\d+)/(\d+)/(\d+)/')
>>> 
```

捕获组通常能简化后续对匹配文本的处理，因为每个组的内容都可以单独提取出来。

例如：

```txt
>>> m = datepat.match('11/27/2012')
>>> m
<_sre.SRE_Match object at 0x1005d2750>
>>> # Extract the contents of each group
>>> m.group(0)
'11/27/2012'
>>> m.group(1)
'11'
>>> m.group(2)
'27'
>>> m.group(3)
'2012'
>>> m_groups()
('11', '27', '2012')
>>> month, day, year = m_groups()
>>> # Find all matches (notice splitting into tuples)
>>> text
'Today is 11/27/2012. PyCon starts 3/13/2013.'
>>> datepat.findall(text)
[( '11', '27', '2012'), ('3', '13', '2013') ]
>>> for month, day, year in datepat.findall(text):
...     print '{ }-{ }-{ }'.format(year, month, day))
...
2012-11-27
2013-3-13
>>> 
```

findall()方法搜索整个文本并找出所有的匹配项然后将它们以列表的形式返回。如果想以迭代的方式找出匹配项，可以使用 finditer()方法。示例如下：

```txt
>>> for m in datetime.finditer(text):
...
...
print(m_groups())
...
(11', 27', 2012')
(3', 13', 2013')
>>> 
```

# 2.4.3 讨论

有关正则表达式的基本理论教学超出了本书的范围。但是，本节向您展示了利用 re 模

块来对文本做匹配和搜索的基础。基本功能是首先用 re.compile()对模式进行编译，然后使用像 match()、findall()或 finditer()这样的方法做匹配和搜索。

当指定模式时我们通常会使用原始字符串，比如 $\mathrm { r ^ { \prime } ( \lambda d + ) / ( \lambda d + ) / ( \lambda d + ) ^ { \prime } }$ 。这样的字符串不会对反斜线字符转义，这在正则表达式上下文中会很有用。否则，我们需要用双反斜线来表示一个单独的'\'，例如 $\langle { \langle \lfloor d + } \rangle / ( \backslash { \mathsf { d } } + ) / ( \backslash { \mathsf { d } } + ) ^ { \prime } ,$ 。

请注意 match()方法只会检查字符串的开头。有可能出现匹配的结果并不是你想要的情况。例如：

```txt
>>> m = datepat.match('11/27/2012abcdef')
>>> m
<_sre.SRE_Match object at 0x1005d27e8>
>>> m.group()
'11/27/2012'
>>> 
```

如果想要精确匹配，请确保在模式中包含一个结束标记（$），示例如下：

```txt
>>>datepat \(=\) re.compile(r'(\d+)/(\d+)/(\d+)\\( '   
>>>datepat.match('11/27/2012abcdef')   
>>>datepat.match('11/27/2012')   
\(<  _{\mathrm{~s~r~e~.~SRE\_Match}}\) object at Ox1005d2750>   
>>> 
```

最后，如果只是想执行简单的文本匹配和搜索操作，通常可以省略编译步骤，直接使用 re 模块中的函数即可。例如：

```prolog
>>> re.findall(r'\d+)/(\d+)/(\d+)', text)  
[(11', 27', 2012'), ('3', '13', '2013')]  
>>> 
```

请注意，如果打算执行很多匹配或查找操作的话，通常需要先将模式编译然后再重复使用。模块级的函数会对最近编译过的模式做缓存处理，因此这里并不会有巨大的性能差异。但是使用自己编译过的模式会省下一些查找步骤和额外的处理。

# 2.5 查找和替换文本

# 2.5.1 问题

我们想对字符串中的文本做查找和替换。

# 2.5.2 解决方案

对于简单的文本模式，使用 str.replace()即可。例如：

```txt
>>> text = 'yeah, but no, but yeah, but no, but yeah'  
>>> text.replace('yeah', 'yep')  
'Yep, but no, but yep, but no, but yep'  
>>> 
```

针对更为复杂的模式，可以使用re 模块中的 sub()函数/方法。为了说明如何使用，假设我们想把日期格式从“11/27/2012”改写为“2012-11-27”。示例如下：

```txt
>>> text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'  
>>> import re  
>>> re.sub(r'^(\d+) / (\d+)/(\d+)', r'^3-\1-\2', text)  
'Today is 2012-11-27. PyCon starts 2013-3-13.'  
>>> 
```

sub()的第1个参数是要匹配的模式，第2个参数是要替换上的模式。类似ì\3î这样的反斜线加数字的符号代表着模式中捕获组的数量。

如果打算用相同的模式执行重复替换，可以考虑先将模式编译以获得更好的性能。示例如下：

```python
>>> import re
>>> datepat = re.compile(r'^(\d+)/(\\d+)/(\d+)/')
>>> datepat.sub(r'^3-\1-\2', text)
'Today is 2012-11-27. PyCon starts 2013-3-13.' 
```

对于更加复杂的情况，可以指定一个替换回调函数。示例如下：

```python
>>> from calendar import month_abbr
>>> def change_date(m):
...
    mon_name = month_abbr[int(m.group(1))]
    return '{} {} '.format(m.group(2), mon_name, m.group(3))
...
>>> datepat.sub(change_date, text)
'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.' 
```

替换回调函数的输入参数是一个匹配对象，由 match()或 find()返回。用.group()方法来提取匹配中特定的部分。这个函数应该返回替换后的文本。

除了得到替换后的文本外，如果还想知道一共完成了多少次替换，可以使用 re.subn()。例如：

```html
>>> newtext, n = datepat.subn(r'\3-\1-\2', text)
>>> newtext
'Today is 2012-11-27. PyCon starts 2013-3-13.' 
```

```txt
>>>n   
2   
>>> 
```

# 2.5.3 讨论

除了以上展示的 sub()调用之外，关于正则表达式的查找和替换并没有什么更多可说的了。最有技巧性的地方在于指定正则表达式模式—这个最好还是留给读者自己去练习吧。

# 2.6 以不区分大小写的方式对文本做查找和替换

# 2.6.1 问题

我们需要以不区分大小写的方式在文本中进行查找，可能还需要做替换。

# 2.6.2 解决方案

要进行不区分大小写的文本操作，我们需要使用 re 模块并且对各种操作都要加上re.IGNORECASE 标记。例如：

```python
>>> text = 'UPPER PYTHON, lower python, Mixed Python'
>>> re.findall('python', text, flags=re.IGNORECASE)
['PYTHON', 'python', 'Python']
>>> re.sub('python', 'snake', text, flags=re.IGNORECASE)
'UPPER snake, lower snake, Mixed snake'
>>> 
```

上面这个例子揭示出了一种局限，那就是待替换的文本与匹配的文本大小写并不吻合。如果想修正这个问题，需要用到一个支撑函数（support function），示例如下：

```python
def matchcase(word):
    def replace(m):
        text = m.group()
        if text.isupper():
            return word upper()
        elif text.islower():
            return word lower()
        elif text[0].isupper():
            return word capitalized()
        else:
            return word
    return replace 
```

下面是使用这个函数的例子：

```prolog
>>> re.sub('python', matchcase('snake'), text, flags=re.IGNORECASE)  
'UPPER SNAKE, lower snake, Mixed Snake'  
>>> 
```

# 2.6.3 讨论

对于简单的情况，只需加上re.IGNORECASE标记就足以进行不区分大小写的匹配操作了。但请注意的是这对于某些涉及大写转换（case folding）的 Unicode 匹配来说可能是不够的。具体细节请参见 2.10 节。

# 2.7 定义实现昀短匹配的正则表达式

# 2.7.1 问题

我们正在尝试用正则表达式对文本模式做匹配，但识别出来的是最长的可能匹配。相反，我们想将其修改为找出最短的可能匹配。

# 2.7.2 解决方案

这个问题通常会在匹配的文本被一对开始和结束分隔符包起来的时候出现（例如带引号的字符串）。为了说明这个问题，请看下面的例子：

```toml
>>> strPAT = re.compile(r'\"(.*)\"')
>>> text1 = 'Computer says "no."
>>> strPAT.findall(text1)
['no']
>>> text2 = 'Computer says "no." Phone says "yes."
>>> strPAT.findall(text2)
['no." Phone says "yes.'] 
```

在这个例子中，模式 $\mathrm { r } ^ { \prime } | ^ { \gamma } ( \xrightarrow { * } ) |$ î'尝试去匹配包含在引号中的文本。但是，*操作符在正则表达式中采用的是贪心策略，所以匹配过程是基于找出最长的可能匹配来进行的。因此，在 text2 的例子中，它错误地匹配成 2 个被引号包围的字符串。

要解决这个问题，只要在模式中的*操作符后加上？修饰符就可以了。示例如下：

```coffeescript
>>> strPAT = re.compile(r'\"(.?)\\"')
>>> strPAT.findall(text2)
['no.', 'yes']
>>> 
```

这么做使得匹配过程不会以贪心方式进行，也就会产生出最短的匹配了。

# 2.7.3 讨论

本节提到了一个当编写含有句点（.）字符的正则表达式时常会遇到的问题。在模式中，句点除了换行符之外可匹配任意字符。但是，如果以开始和结束文本（比如说引号）将句点括起来的话，在匹配过程中将尝试找出最长的可能匹配结果。这会导致匹配时跳过多个开始或结束文本，而将它们都包含在最长的匹配中。在*或 $+$ 后添加一个？，会强制将匹配算法调整为寻找最短的可能匹配。

# 2.8 编写多行模式的正则表达式

# 2.8.1 问题

我们打算用正则表达式对一段文本块做匹配，但是希望在进行匹配时能够跨越多行。

# 2.8.2 解决方案

这个问题一般出现在希望使用句点（.）来匹配任意字符，但是忘记了句点并不能匹配换行符时。例如，假设想匹配 C 语言风格的注释：

```txt
>>>comment \(\equiv\) re.compile(r'/\*(.?\)\*/'   
>>>text1 \(=\) /\* this is a comment \*/   
>>>text2 \(=\) \*\*/\* this is a   
... multiline comment \*/   
...   
>>>   
>>>comment.findall(text1)   
['this is a comment']   
>>>comment.findall(text2)   
[]   
>>> 
```

要解决这个问题，可以添加对换行符的支持。示例如下：

>>>comment $\equiv$ re.compile(r'/\*((?:.|\n)\*?)\*/')   
>>>comment.findall(text2)   
['this is a\n multiline comment']   
>>>

在这个模式中， $( ? . . | \mathrm { u } )$ 指定了一个非捕获组（即，这个组只做匹配但不捕获结果，也不会分配组号）。

# 2.8.3 讨论

re.compile()函数可接受一个有用的标记—re.DOTALL。这使得正则表达式中的句点(.)

可以匹配所有的字符，也包括换行符。例如：

>>>comment $\equiv$ re.compile(r'/\*.(.?)\*\*/,re.DOTALL)   
>>>comment.findall(text2)   
['this is a\n multiline comment']

对于简单的情况，使用 re.DOTALL 标记就可以很好地完成工作。但是如果要处理极其复杂的模式，或者面对的是如 2.18 节中所描述的为了做分词（tokenizing）而将单独的正则表达式合并在一起的情况，如果可以选择的话，通常更好的方法是定义自己的正则表达式模式，这样它无需额外的标记也能正确工作。

# 2.9 将 Unicode 文本统一表示为规范形式

# 2.9.1 问题

我们正在同 Unicode 字符串打交道，但需要确保所有的字符串都拥有相同的底层表示。

# 2.9.2 解决方案

在 Unicode中，有些特定的字符可以被表示成多种合法的代码点序列。为了说明这个问题，请看下面的示例：

>>s1 $=$ 'Spicy Jalapeu00f1o   
>>s2 $=$ 'Spicy Jalapenu0303o   
>>s1   
'Spicy Jalapeno'   
>>s2   
'Spicy Jalapeno'   
>>s1 $= =$ s2   
False   
>>>len(s1)   
14   
>>>len(s2)   
15   
>>>

这里的文本“Spicy JalapeÒo”以两种形式呈现。第一种使用的是字符ìÒî的全组成（fullycomposed）形式（ $\mathrm { U } { + } 0 0 \mathrm { F } 1$ ）。第二种使用的是拉丁字母ìnî紧跟着一个ì~î组合而成的字符（ $\mathrm { U } { + } 0 3 0 3$ ）。

对于一个比较字符串的程序来说，同一个文本拥有多种不同的表示形式是个大问题。为了解决这个问题，应该先将文本统一表示为规范形式，这可以通过 unicodedata 模块

# 来完成：

```python
>>> import unicodedata
>>> t1 = unicodedata normalize('NFC', s1)
>>> t2 = unicodedata normalize('NFC', s2)
>>> t1 == t2
True
>>> print(ascii(t1))
'Spicy Jalape\xf10'
>>> t3 = unicodedata normalize('NFD', s1)
>>> t4 = unicodedata normalize('NFD', s2)
>>> t3 == t4
True
>>> print(ascii(t3))
'Spicy Jalapen\u0303o'
>>> 
```

normalize()的第一个参数指定了字符串应该如何完成规范表示。NFC 表示字符应该是全组成的（即，如果可能的话就使用单个代码点）。NFD表示应该使用组合字符，每个字符应该是能完全分解开的。

Python 还支持 NFKC和NFKD 的规范表示形式，它们为处理特定类型的字符增加了额外的兼容功能。例如：

>>>s $=$ '\ufb01' # A single character   
>>>s   
'fi'   
>>>unicodedata normalize('NFD',s)   
'fi'   
# Notice how the combined letters are broken apart here.   
>>>unicodedata normalize('NFKD',s)   
'fi'   
>>>unicodedata normalize('NFKC',s)   
'fi'   
>>>

# 2.9.3 讨论

对于任何需要确保以规范和一致性的方式处理Unicode文本的程序来说，规范化都是重要的一部分。尤其是在处理用户输入时接收到的字符串时，此时你无法控制字符串的编码形式，那么规范化文本的表示就显得更为重要了。

在对文本进行过滤和净化时，规范化同样也占据了重要的部分。例如，假设想从某些

文本中去除所有的音符标记（可能是为了进行搜索或匹配）：

```txt
>>> t1 = unicodedata很正常('NFD', s1)  
>>> '.join(c for c in t1 if not unicodedatacombineng(c))  
'Spicy Jalapeno'  
>>> 
```

最后一个例子展示了 unicodedata 模块的另一个重要功能—用来检测字符是否属于某个字符类别。使用工具 combining()函数可对字符做检查，判断它是否为一个组合型字符。这个模块中还有一些函数可用来查找字符类别、检测数字字符等。

很显然，Unicode 是一个庞大的主题。要获得更多有关规范化文本方面的参考信息，可访问http://www.unicode.org/faq/normalization.html。Ned Batchelder 也在他的网站 http://nedbatchelder.com/text/unipain.html 上对 Python 中的 Unicode 处理给出了优秀的示例说明。

# 2.10 用正则表达式处理 Unicode 字符

# 2.10.1 问题

我们正在用正则表达式处理文本，但是需要考虑处理 Unicode 字符。

# 2.10.2 解决方案

默认情况下 re 模块已经对某些 Unicode 字符类型有了基本的认识。例如，\d 已经可以匹配任意 Unicode 数字字符了：

```diff
>>> import re
>>> num = re.compile("\\d+")
>>> # ASCII digits
>>> num.match('123')
<_sre.SRE^KMatch object at 0x1007d9ed0>
>>> # Arabic digits
>>> num.match("\\u0661\u0662\u0663")
<_sre.SRE^KMatch object at 0x101234030>
>>> 
```

如果需要在模式字符串中包含指定的 Unicode 字符，可以针对Unicode 字符使用转义序列（例如\uFFFF 或\UFFFFFFF）。比如，这里有一个正则表达式能在多个不同的阿拉伯代码页中匹配所有的字符：

```txt
>>> arabic = re.compile(['[\u0600-\u06ff\u0750-\u077f\u08a0-\u08ff]+'])  
>>> 
```

当执行匹配和搜索操作时，一个好主意是首先将所有的文本都统一表示为标准形式（见2.9 节）。但是，同样重要的是需要注意一些特殊情况。例如，当不区分大小写的匹配

和大写转换（case folding）匹配联合起来时，考虑会出现什么行为：

```python
>>> pat = re.compile('stra\b00dfe', re.IGNORECASE)  
>>> s = 'stra 还'  
>>> pat.match(s) # Matches  
<_sre.SRE_MATCH object at 0x10069d370>  
>>> pat.match(s_upper()) # Doesn't match  
>>> s_upper() # Case folds  
'STRASSE'  
>>> 
```

# 2.10.3 讨论

把Unicode 和正则表达式混在一起使用绝对是个能让人头痛欲裂的办法。如果真的要这么做，应该考虑安装第三方的正则表达式库（http://pypi.python.org/pypi/regex），这些第三方库针对 Unicode 大写转换提供了完整的支持，还包含其他各种有趣的特性，包括近似匹配。

# 2.11 从字符串中去掉不需要的字符

# 2.11.1 问题

我们想在字符串的开始、结尾或中间去掉不需要的字符，比如说空格符。

# 2.11.2 解决方案

strip()方法可用来从字符串的开始和结尾处去掉字符。lstrip()和 rstrip()可分别从左或从右侧开始执行去除字符的操作。默认情况下这些方法去除的是空格符，但也可以指定其他的字符。例如：

```python
>>> # Whitespace stripping  
>>> s = ' hello world\n'  
>>> s.strip()  
'hello world'  
>>> s.lstrip()  
'hello world\n'  
>>> s.rstrip()  
' hello world'  
>>>  
>>> # Character stripping  
>>> t = '-----hello==='  
>>> t.lstrip('-')  
'hello==='  
>>> t.strip('-')  
'hello'  
>>> 
```

# 2.11.3 讨论

当我们读取并整理数据以待稍后的处理时常常会用到这类 strip()方法。例如，可以用它们来去掉空格、移除引号等。

需要注意的是，去除字符的操作并不会对位于字符串中间的任何文本起作用。例如：

>>>s $=$ ' hello world n'   
>>>s $=$ s.strip()   
>>>s   
'hello world'   
>>>

如果要对里面的空格执行某些操作，应该使用其他技巧，比如使用 replace()方法或正则表达式替换。例如：

```prolog
>>>s.replace('', '   
'helloworld'   
>>>import re   
>>>re.sub('\\s+','',s)   
'hello world'   
>>> 
```

我们通常会遇到的情况是将去除字符的操作同某些迭代操作结合起来，比如说从文件中读取文本行。如果是这样的话，那就到了生成器表达式大显身手的时候了。例如：

with openfilename) as f: lines $=$ (line.strip() for line in f) for line in lines:

这里，表达式 lines $=$ (line.strip() for line in f)的作用是完成数据的转换①。它很高效，因为这里并没有先将数据读取到任何形式的临时列表中。它只是创建一个迭代器，在所有产生出的文本行上都会执行 strip 操作。

对于更高级的 strip 操作，应该转而使用 translate()方法。请参见下一节以获得进一步的细节。

# 2.12 文本过滤和清理

# 2.12.1 问题

某些无聊的脚本小子在 Web 页面表单中填入了ìp˝tĥˆÒî这样的文本，我们想以某种方式将其清理掉。

# 2.12.2 解决方案

文本过滤和清理所涵盖的范围非常广泛，涉及文本解析和数据处理方面的问题。在非常简单的层次上，我们可能会用基本的字符串函数（例如 str.upper()和 str.lower()）将文本转换为标准形式。简单的替换操作可通过 str.replace()或 re.sub()来完成，它们把重点放在移除或修改特定的字符序列上。也可以利用 unicodedata.normalize()来规范化文本，如 2.9 节所示。

然而我们可能想更进一步。比方说也许想清除整个范围内的字符，或者去掉音符标志。要完成这些任务，可以使用常被忽视的 str.translate()方法。为了说明其用法，假设有如下这段混乱的字符串：

>>>s $=$ 'python\fis\tawesome\r\n'   
>>>s   
'python\x0cis\tawesome\r\n'   
>>>

第一步是清理空格。要做到这步，先建立一个小型的转换表，然后使用 translate()方法：

>>>remap $=$ {  
... ord('t'): ' ',  
... ord('f'): ' ',  
... ord('r'): None #Deleted  
...}  
>>>a $\equiv$ stranslate(remap)  
>>>a  
'python is awesome\n'  
>>>

可以看到，类似\t 和\f 这样的空格符已经被重新映射成一个单独的空格。回车符\r 已经完全被删除掉了。

可以利用这种重新映射的思想进一步构建出更加庞大的转换表。例如，我们把所有的Unicode 组合字符都去掉：

```txt
>>> import unicodedata
>>> import sys
>>> cmb_chrs = dict.fromkeys(c for c in range(sys.maxunicode)
...
...
>>> if unicodedata.combining(chr(c)))
...
>>> b = unicodedata normalize('NFD', a)
>>> b
'python is awesome\n'
>>> b.translate(cmb_chrs)
'python is awesome\n'
>>> 
```

在这个例子中，我们使用 dict.fromkeys()方法构建了一个将每个 Unicode 组合字符都映射为 None 的字典。

原始输入会通过 unicodedata.normalize()方法转换为分离形式，然后再通过 translate()方法删除所有的重音符号。我们也可以利用相似的技术来去掉其他类型的字符（例如控制字符）。

下面来看另一个例子。这里有一张转换表将所有的 Unicode 十进制数字字符映射为它们对应的 ASCII 版本：

```txt
>>> digitmap = { c: ord('0') + unicodedata-digit(chr(c))  
... for c in range(sys.maxunicode)  
... if unicodedata(category(chr(c)) == 'Nd')  
...  
>>> len(digitmap)  
460  
>>> # Arabic digits  
>>> x = '\u0661\u0662\u0663'  
>>> xtranslate(digitmap)  
'123'  
>>> 
```

另一种用来清理文本的技术涉及I/O解码和编码函数。大致思路是首先对文本做初步的清理，然后通过结合 encode()和 decode()操作来修改或清理文本。示例如下：

```prolog
>>> a  
'python is awesome\n'  
>>> b = unicodedata很正常('NFD', a)  
>>> b.encode('ascii', 'ignore').decode('ascii')  
'python is awesome\n'  
>>> 
```

这里的 normalize()方法先对原始文本做分解操作。后续的 ASCII 编码/解码只是简单地一次性丢弃所有不需要的字符。很显然，这种方法只有当我们的最终目标就是 ASCII形式的文本时才有用。

# 2.12.3 讨论

文本过滤和清理的一个主要问题就是运行时的性能。一般来说操作越简单，运行得就越快。对于简单的替换操作，用 str.replace()通常是最快的方式—即使必须多次调用它也是如此。比方说如果要清理掉空格符，可以编写如下的代码：

```python
def clean Spaces(s):
    s = s.replace('\\r', '')
    s = s.replace('\\t', '') 
```

```python
s = s.replace('\\f', ' ' )
return s 
```

如果试着调用它，就会发现这比使用 translate()或者正则表达式的方法要快得多。

另一方面，如果需要做任何高级的操作，比如字符到字符的重映射或删除，那么 translate()方法还是非常快的。

从整体来看，我们应该在具体的应用中去进一步揣摩性能方面的问题。不幸的是，想在技术上给出一条“放之四海而皆准”的建议是不可能的，所以应该尝试多种不同的方法，然后做性能统计分析。

尽管本节的内容主要关注的是文本，但类似的技术也同样适用于字节对象（byte），这包括简单的替换、翻译和正则表达式。

# 2.13 对齐文本字符串

# 2.13.1 问题

我们需要以某种对齐方式将文本做格式化处理。

# 2.13.2 解决方案

对于基本的字符串对齐要求，可以使用字符串的 ljust()、rjust()和 center()方法。示例如下：

```txt
>>> text = 'Hello World'
>>> text.ljust(20)
'Hello World'
>>> text.rjust(20)
'Hello World'
>>> text.center(20)
'Hello World'
>>> 
```

所有这些方法都可接受一个可选的填充字符。例如：

```txt
>>> text.rjust(20, '=' )
'======Hello World'
>>> text.center(20, '*')
'****Hello World*****'
>>> 
```

format()函数也可以用来轻松完成对齐的任务。需要做的就是合理利用'<'、'>'，或'^'字

符以及一个期望的宽度值①。例如：

>>>format(text， $\prime >20^{\prime}$ 1 'HelloWorld'   
>>>format(text， $<  20^{*}$ 1 'HelloWorld'   
>>>format(text， $\hat{.} 20^{\prime}$ 1 'HelloWorld'   
>>>

如果想包含空格之外的填充字符，可以在对齐字符之前指定：

```txt
>>> format(text, '=>20s')
'===Hello World'
>>> format(text, '*^20s')
'****Hello World*****
>>> 
```

当格式化多个值时，这些格式化代码也可以用在 format()方法中。例如：

```txt
>>> {':>10s} {':>10s}.format('Hello', 'World')  
' Hello World'  
>>> 
```

format()的好处之一是它并不是特定于字符串的。它能作用于任何值，这使得它更加通用。例如，可以对数字做格式化处理：

```txt
>> x = 1.2345  
>>> format(x, '>10')  
' 1.2345'  
>>> format(x, '^10.2f')  
' 1.23'  
>>> 
```

# 2.13.3 讨论

在比较老的代码中，通常会发现%操作符用来格式化文本。例如：

```txt
>>> '-%20s' % text
'Hello World'
>>> '-%20s' % text
' Hello World'
>>>
```

但是在新的代码中，我们应该会更钟情于使用 format()函数或方法。format()比%操作符提供的功能要强大多了。此外，format()可作用于任意类型的对象，比字符串的 ljust()、rjust()以及 center()方法要更加通用。

想了解 format()函数的所有功能，请参考 Python 的在线手册 http://docs.python.org/3/ library/ string. html#formatspec。

# 2.14 字符串连接及合并

# 2.14.1 问题

我们想将许多小字符串合并成一个大的字符串。

# 2.14.2 解决方案

如果想要合并的字符串在一个序列或可迭代对象中，那么将它们合并起来的最快方法就是使用 join()方法。示例如下：

>>>parts $=$ ['Is'，'Chicago'，'Not'，'Chicago?']   
>>>'.join-parts)   
'Is Chicago Not Chicago?'   
>>>','.join-parts)   
'Is,Chicago,Not,Chicago?'   
>>>''.join-parts)   
'IsChicagoNotChicago?'   
>>>

初看上去语法可能显得有些怪异，但是 join()操作其实是字符串对象的一个方法。这么设计的部分原因是因为想要合并在一起的对象可能来自于各种不同的数据序列，比如列表、元组、字典、文件、集合或生成器，如果单独在每一种序列对象中实现一个 join()方法就显得太冗余了。因此只需要指定想要的分隔字符串，然后在字符串对象上使用join()方法将文本片段粘合在一起就可以了。

如果只是想连接一些字符串，一般使用 $^ +$ 操作符就足够完成任务了：

```txt
>>> a = 'Is Chicago'  
>>> b = 'Not Chicago?'  
>>> a + ' ' + b  
'Is Chicago Not Chicago?'  
>>> 
```

针对更加复杂的字符串格式化操作，+操作符同样可以作为 format()的替代，很好地完成任务：

```txt
>>> print('{} {}'.format(a,b))  
Is Chicago Not Chicago?  
>>> print(a + ' ' + b)  
Is Chicago Not Chicago?  
>>> 
```

如果打算在源代码中将字符串字面值合并在一起，可以简单地将它们排列在一起，中间不加 $^ +$ 操作符。示例如下：

```coffeescript
>>> a = 'Hello' 'World'
>>> a
'HelloWorld'
>>> 
```

# 2.14.3 讨论

字符串连接这个主题可能看起来还没有高级到要用一整节的篇幅来讲解，但是程序员常常会在这个问题上做出错误的编程选择，使得他们的代码性能受到影响。

最重要的一点是要意识到使用 $^ +$ 操作符做大量的字符串连接是非常低效的，原因是由于内存拷贝和垃圾收集产生的影响。特别是你绝不会想写出这样的字符串连接代码：

```python
s =''
for p in parts:
    s += p 
```

这种做法比使用 join()方法要慢上许多。主要是因为每个 $+ { = }$ 操作都会创建一个新的字符串对象。我们最好先收集所有要连接的部分，最后再一次将它们连接起来。

一个相关的技巧（很漂亮的技巧）是利用生成器表达式（见 1.19 节）在将数据转换为字符串的同时完成连接操作。示例如下：

```matlab
>>> data = ['ACME', 50, 91.1]  
>>> ', '.join(str(d) for d in data)  
'ACME,50,91.1'  
>>> 
```

对于不必要的字符串连接操作也要引起重视。有时候在技术上并非必需的时候，程序员们也会忘乎所以地使用字符串连接操作。例如在打印的时候：

```python
print(a + ': ' + b + ': ' + c) # Ugly  
print(['':].join([a, b, c])) # Still ugly  
print(a, b, c, sep=':') # Better 
```

将字符串连接同 I/O 操作混合起来的时候需要对应用做仔细的分析。例如，考虑如下两段代码：

```julia
# Version 1 (string concatenation)
f.write(chunk1 + chunk2) 
```

```txt
# Version 2 (separate I/O operations)
f.write(chunk1)
f.write(chunk2) 
```

如果这两个字符串都很小，那么第一个版本的代码能带来更好的性能，这是因为执行一次I/O系统调用的固有开销就很高。另一方面，如果这两个字符串都很大，那么第二个版本的代码会更加高效。因为这里避免了创建大的临时结果，也没有对大块的内存进行拷贝。这里必须再次强调，你需要对自己的数据做分析，以此才能判定哪一种方法可以获得最好的性能。

最后但也是最重要的是，如果我们编写的代码要从许多短字符串中构建输出，则应该考虑编写生成器函数，通过 yield 关键字生成字符串片段。示例如下：

```python
def sample():
    yield 'Is'
    yield 'Chicago'
    yield 'Not'
    yield 'Chicago?' 
```

关于这种方法有一个有趣的事实，那就是它不会假设产生的片段要如何组合在一起。比如说可以用 join()将它们简单的连接起来：

text $=$ '.join(sample())

或者，也可以将这些片段重定向到 I/O：

```python
for part in sample(): f.write(part) 
```

又或者我们能以混合的方式将 I/O 操作智能化地结合在一起：

```python
def combine(sample, maxsize):
    parts = []
    size = 0
    for part in source:
        parts.append(part)
        size += len(part)
        if size > maxsize:
            yield '.join(sample)
            parts = []
            size = 0
    yield '.join(parts)
for part in combine(sample(), 32768):
    f.write(part) 
```

关键在于这里的生成器函数并不需要知道精确的细节，它只是产生片段而已。

# 2.15 给字符串中的变量名做插值处理

# 2.15.1 问题

我们想创建一个字符串，其中嵌入的变量名称会以变量的字符串值形式替换掉。

# 2.15.2 解决方案

Python 并不直接支持在字符串中对变量做简单的值替换。但是，这个功能可以通过字符串的 format()方法近似模拟出来。示例如下：

>>>s $=$ {'name}has{n}messages.'   
>>s.format(name $\equiv$ 'Guido'，n=37) 'Guido has 37 messages.'   
>>>

另一种方式是，如果要被替换的值确实能在变量中找到，则可以将 format_map()和vars()联合起来使用，示例如下：

>>>name $=$ 'Guido'   
>>>n $= 37$ >>>s.format_map(args()) 'Guido has 37 messages.'   
>>>

有关 vars()的一个微妙的特性是它也能作用于类实例上。比如：

>>>class Info:   
... def __init__(self，name，n):   
... self.name $=$ name   
... self.n $\equiv$ n   
>>>a $\equiv$ Info('Guido',37)   
>>>s.format_map(args(a))   
'Guido has 37 messages.'   
>>>

而 format()和 format_map()的一个缺点则是没法优雅地处理缺少某个值的情况。例如：

```txt
>>>s.format(name='Guido')   
Traceback (most recent call last): File "<stdin>",line 1,in <module>   
KeyError:'n'   
>>> 
```

避免出现这种情况的一种方法就是单独定义一个带有__missing__()方法的字典类，示例如下：

```python
class safesub(dist): def __missing__(self, key): return '(' + key + ')' 
```

现在用这个类来包装传给 format_map()的输入参数：

```txt
>>> del n # Make sure n is undefined  
>>> s.format_map(safesub(vars()))  
'Guido has {n} messages.'  
>>> 
```

如果发现自己在代码中常常需要执行这些步骤，则可以将替换变量的过程隐藏在一个小型的功能函数内，这里要采用一种称之为“frame hack”的技巧①。示例如下：

```python
import sys   
def sub(text): return text.format_map(safesub(sys._getframe(1).f_locals)) 
```

现在，我们就可以像这样编写代码了：

```txt
>>> name = 'Guido'  
>>> n = 37  
>>> print(sub('Hello {name}'))  
Hello Guido  
>>> print(sub('You have {n} messages.'))  
You have 37 messages.  
>>> print(sub('Your favorite color is {color}'))  
Your favorite color is {color}  
>>> 
```

# 2.15.3 讨论

多年来，由于 Python 缺乏真正的变量插值功能，由此产生了各种解决方案。作为本节中已给出的解决方案的替代，有时候我们会看到类似下面代码中的字符串格式化操作：

>>>name $=$ 'Guido'   
>>>n $= 37$ >>>'(name）has $\% (\mathrm{n})$ messages.'%vars()

```txt
'Guido has 37 messages.'  
>>> 
```

我们可能还会看到模板字符串（template string）的使用：

>>> import string
>>> s = string.Template('\ $name has \$ n messages.")
>>> s substitute(vars())
'Guido has 37 messages.'
>>>

但是，format()和 format_map()方法比上面这些替代方案都要更加现代化，我们应该将其作为首选。使用 format()的一个好处是可以同时得到所有关于字符串格式化方面的功能（对齐、填充、数值格式化等），而这些功能在字符串模板对象上是不可能做到的。

在本节的部分内容中还提到了一些有趣的高级特性。字典类中鲜为人知的__missing__()方法可用来处理缺少值时的行为。在 safesub 类中，我们将该方法定义为将缺失的值以占位符的形式返回，因此这里不会抛出 KeyError 异常，缺少的那个值会出现在最后生成的字符串中（可能对调试有些帮助）。

sub()函数使用了 sys._getframe(1)来返回调用方的栈帧。通过访问属性 f_locals 来得到局部变量。无需赘言，在大部分的代码中都应该避免去和栈帧打交道，但是对于类似完成字符串替换功能的函数来说，这会是有用的。插一句题外话，值得指出的是 f_locals 是一个字典，它完成对调用函数中局部变量的拷贝。尽管可以修改 f_locals 的内容，可是修改后并不会产生任何持续性的效果。因此，尽管访问不同的栈帧可能看起来是很邪恶的，但是想意外地覆盖或修改调用方的本地环境也是不可能的。

# 2.16 以固定的列数重新格式化文本

# 2.16.1 问题

我们有一些很长的字符串，想将它们重新格式化，使得它们能按照用户指定的列数来显示。

# 2.16.2 解决方案

可以使用 textwrap 模块来重新格式化文本的输出。例如，假设有如下这段长字符串：

```python
s = "Look into my eyes, look into my eyes, the eyes, the eyes, \ the eyes, not around the eyes, don't look around the eyes, \ 
```

look into my eyes, you're under."

这里可以用 textwrap 模块以多种方式来重新格式化字符串：

```txt
>>> import textwrap
>>> print(textwrap fill(s, 70))
Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under.
>>> print(textwrap fill(s, 40))
Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under.
>>> print(textwrap fill(s, 40, initial_indent=''))
Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under.
>>> print(textwrap fill(s, 40, subsequent_indent=''))
Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under. 
```

# 2.16.3 讨论

textwrap 模块能够以简单直接的方式对文本格式做整理使其适合于打印—尤其是当希望输出结果能很好地显示在终端上时。关于终端的尺寸大小，可以通过 os.get_terminal_size()来获取。例如：

```txt
>>> import os
>>> os.get-terminal_size().columns
80
>>> 
```

fill()方法还有一些额外的选项可以用来控制如何处理制表符、句号等。请参阅textwrap.TextWrapper 类的文档（http://docs.python.org/3.3/library/ textwrap. html# text w rap.TextWrapper）以获得进一步的细节。

# 2.17 在文本中处理 HTML 和 XML 实体

# 2.17.1 问题

我们想将&entity 或&#code 这样的 HTML 或 XML 实体替换为它们相对应的文本。或者，我们需要生成文本，但是要对特定的字符（比如 $< , >$ 或&）做转义处理。

# 2.17.2 解决方案

如果要生成文本，使用 html.escape()函数来完成替换<or>这样的特殊字符相对来说是比较容易的。例如：

```txt
>>> s = 'Elements are written as "<tag>text</tag)".'
>>> import html
>>> print(s)
Elements are written as "<tag>text</tag>";
>>> printHTML.escape(s))
Elements are written as &quot;&lt;tag&gt;text&gt;/tag&gt;&quot;
>>> # Disable escaping of quotes
>>> printHTML.escape(s, quote=False))
Elements are written as "&lt;tag&gt;text&gt;/tag&gt;";
>>> 
```

如果要生成 ASCII 文本，并且想针对非 ASCII 字符将它们对应的字符编码实体嵌入到文本中，可以在各种同 I/O 相关的函数中使用 errors $\mathbf { \alpha } = \mathbf { \ " }$ xmlcharrefreplace'参数来实现。示例如下：

>>>s $=$ 'Spicy JalapeNo'   
>>>s.encode('ascii',errors $\equiv$ 'xmlcharrefreplace')   
b'Spicy Jalape#241;o'   
>>>

要替换文本中的实体，那就需要不同的方法。如果实际上是在处理 HTML 或 XML，首先应该尝试使用一个合适的 HTML 或 XML 解析器。一般来说，这些工具在解析的过程中会自动处理相关值的替换，而我们完全无需为此操心。

如果由于某种原因在得到的文本中带有一些实体，而我们想手工将它们替换掉，通常可以利用各种 HTML 或 XML 解析器自带的功能函数和方法来完成。示例如下：

>>>s $=$ 'Spicy &quot;Jalape&#241;o&quot.'   
>>>fromhtmlparserimportHTMLParser   
>>>p $\equiv$ HTMLParser()   
>>>p.unescape(s)

```txt
'Spicy "Jalapeno"."  
>>>  
>>> t = 'The prompt is &gt;&gt;&gt;'  
>>> from xml.sax.saxutils import unescape  
>>> unescape(t)  
'The prompt is >>>'  
>>> 
```

# 2.17.3 讨论

在生成 HTML 或 XML 文档时，适当地对特殊字符做转义处理常常是个容易被忽视的细节。尤其是当自己用 print()或其他一些基本的字符串格式化函数来产生这类输出时更是如此。简单的解决方案是使用像 html.escape()这样的工具函数。

如果需要反过来处理文本（即，将 HTML 或 XML 实体转换成对应的字符），有许多像 xml.sax.saxutils.unescape()这样的工具函数能帮上忙。但是，我们需要仔细考察一个合适的解析器应该如何使用。例如，如果是处理 HTML 或 XML，像 html.parser或 xml.etree.ElementTree 这样的解析模块应该已经解决了有关替换文本中实体的细节问题。

# 2.18 文本分词

# 2.18.1 问题

我们有一个字符串，想从左到右将它解析为标记流（stream of tokens）。

# 2.18.2 解决方案

假设有如下的字符串文本：

```python
text = 'foo = 23 + 42 * 10' 
```

要对字符串做分词处理，需要做的不仅仅只是匹配模式。我们还需要有某种方法来识别出模式的类型。例如，我们可能想将字符串转换为如下的序列对：

```python
tokens = ['NAME', 'foo'), ('EQ', ':='), ('NUM', '23'), ('PLUS', ':+'), ('NUM', '42'), ('TIMES', '*'), ('NUM', 10)] 
```

要完成这样的分词处理，第一步是定义出所有可能的标记，包括空格。这可以通过正则表达式中的命名捕获组来实现，示例如下：

```python
import re
NAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)' 
```

NUM $=$ r'(?P<NUM>\d+)   
PLUS $=$ r'(?P<PLUS>\+)'   
TIMES $=$ r'(?P<TIMES>\*)   
EQ $=$ r'(?P<EQ>=)'   
WS $=$ r'(?P<WS>\s+)   
masterpat $\equiv$ re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))

在这些正则表达式模式中，形如?P<TOKENNAME>这样的约定是用来将名称分配给该模式的。这个我们稍后会用到。

接下来我们使用模式对象的 scanner()方法来完成分词操作。该方法会创建一个扫描对象，在给定的文本中重复调用 match()，一次匹配一个模式。下面这个交互式示例展示了扫描对象是如何工作的：

```coffeescript
>>> scanner = masterPAT.scanner('foo = 42')
>>> scanner.match()
<_sre.SRE_MATCH object at 0x100677738>
>>> _lastgroup, _.group()
('NAME', 'foo')
>>> scanner.match()
<_sre.SRE_MATCH object at 0x100677738>
>>> _lastgroup, _.group()
('WS', '')
>>> scanner.match()
<_sre.SRE_MATCH object at 0x100677738>
>>> _lastgroup, _.group()
('EQ', ':')
>>> scanner.match()
<_sre.SRE_MATCH object at 0x100677738>
>>> _lastgroup, _.group()
('WS', '')
>>> scanner.match()
<_sre.SRE_MATCH object at 0x100677738>
>>> _lastgroup, _.group()
('NUM', '42')
>>> scanner.match()
>>> 
```

要利用这项技术并将其转化为代码，我们可以做些清理工作然后轻松地将其包含在一个生成器函数中，示例如下：

```python
from collections import namedtuple Token =namedtuple('Token',['type','value']) 
```

```python
def generate_tokens(pat, text):
    scanner = pat.scanner(text)
    for m in iter(scanner.match, None):
        yield Token(m.lastgroup, m.group())
# Example use
for tok in generate_tokens(masterPAT, 'foo = 42'): print(tok)
# Produces output
# Token(type='NAME', value='foo')
# Token(type='WS', value='')
# Token(type='EQ', value='')
# Token(type='WS', value='')
# Token(type='NUM', value='42') 
```

如果想以某种方式对标记流做过滤处理，要么定义更多的生成器函数，要么就用生成器表达式。例如，下面的代码告诉我们如何过滤掉所有的空格标记。

```python
tokens = (tok for tok in generate_tokens(masterPAT, text)  
    if tok.type != 'WS')  
for tok in tokens:  
    print(tok) 
```

# 2.18.3 讨论

对于更加高级的文本解析，第一步往往是分词处理。要使用上面展示的扫描技术，有几个重要的细节需要牢记于心。第一，对于每个可能出现在输入文本中的文本序列，都要确保有一个对应的正则表达式模式可以将其识别出来。如果发现有任何不能匹配的文本，扫描过程就会停止。这就是为什么有必要在上面的示例中指定空格标记（WS）。

这些标记在正则表达式（即 re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))）中的顺序同样也很重要。当进行匹配时，re 模块会按照指定的顺序来对模式做匹配。因此，如果碰巧某个模式是另一个较长模式的子串时，就必须确保较长的那个模式要先做匹配。示例如下：

$\mathrm{LT} = \mathrm{r}'(\mathrm{P} <   \mathrm{LT} > <   )'$ $\mathrm{LE} = \mathrm{r}'(\mathrm{P} <   \mathrm{LE} > <   =)$ $\mathrm{EQ} = \mathrm{r}'(\mathrm{P} <   \mathrm{EQ} > =)$ master_pat $=$ re.compile('|'.join([LE, LT, EQ])) # Correct  
# master_pat $=$ re.compile('|'.join([LT, LE, EQ])) # Incorrect

第 2 个模式是错误的（注释掉的那一行），因为这样会把文本 $! { \mathop { < } } = !$ 匹配为 LT（'<'）紧跟

着 EQ（'='），而没有匹配为单独的标记 LE（ $! < = "$ ），这与我们的本意不符。

最后也最重要的是，对于有可能形成子串的模式要多加小心。例如，假设有如下两种模式：

PRINT $=$ r'(P<PRINT>print)'   
NAME $=$ r'(P<NAME>[a-zA-Z_][a-zA-Z_0-9]\*)'   
masterpat $\equiv$ re.compile('|.join([PRINT，NAME]))   
for tok in generate_tokens(masterpat,'printer'): print(tok)   
#Outputs:   
#Token(type $\coloneqq$ 'PRINT'，value $\coloneqq$ 'print')   
#Token(type $\coloneqq$ 'NAME'，value $\coloneqq$ 'er')

对于更加高级的分词处理，我们应该去看看像 PyParsing 或 PLY 这样的包。有关 PLY的例子将在下一节中讲解。

# 2.19 编写一个简单的递归下降解析器

# 2.19.1 问题

我们需要根据一组语法规则来解析文本，以此执行相应的操作或构建一个抽象语法树来表示输入。语法规则很简单，因此我们倾向于自己编写解析器而不是使用某种解析器框架。

# 2.19.2 解决方案

在这个问题中，我们把重点放在根据特定的语法来解析文本上。要做到这些，应该以BNF 或 EBNF 的形式定义出语法的正式规格。比如，对于简单的算术运算表达式，语法看起来是这样的：

```autohotkey
expr ::= expr + term
| expr - term
| term
term ::= term * factor
| term / factor
| factor
factor ::= ( expr )
| NUM 
```

又或者以 EBNF 的形式定义为如下形式：

```autohotkey
expr ::= term { (+|-) term }*  
term ::= factor { (*|/) factor }*  
factor ::= ( expr )  
| NUM 
```

在 EBNF 中，部分包括在{ ... }*中的规则是可选的。*意味着零个或更多重复项（和在正则表达式中的意义相同）。

现在，如果我们对 BNF 还不熟悉的话，可以把它看做是规则替换或取代的一种规范形式，左侧的符号可以被右侧的符号所取代（反之亦然）。一般来说，在解析的过程中我们会尝试将输入的文本同语法做匹配，通过 BNF 来完成各种替换和扩展。为了说明，假设正在解析一个类似于 $3 + 4 * 5$ 这样的表达式。这个表达式首先应该被分解为标记流，这可以使用 2.18 节中描述的技术来实现。得到的结果可能是下面这样的标记序列：

```txt
NUM + NUM * NUM 
```

从这里开始，解析过程就涉及通过替换的方式将语法匹配到输入标记上：

```txt
expr  
expr ::= term { (+|-) term }*  
expr ::= factor { (*|/) factor }* { (+|-) term }*  
expr ::= NUM { (*|/) factor }* { (+|-) term }*  
expr ::= NUM { (+|-) term }*  
expr ::= NUM + term { (+|-) term }*  
expr ::= NUM + factor { (*|/) factor }* { (+|-) term }*  
expr ::= NUM + NUM { (*|/) factor }* { (+|-) term }*  
expr ::= NUM + NUM * factor { (*|/) factor }* { (+|-) term }*  
expr ::= NUM + NUM * NUM { (+|-) term }*  
expr ::= NUM + NUM * NUM { (+|-) term }* 
```

完成所有的替换需要花上一段时间，这是由输入的规模和尝试去匹配的语法规则所决定的。第一个输入标记是一个 NUM，因此替换操作首先会把重点放在匹配这一部分上。一旦匹配上了，重点就转移到下一个标记 $+$ 上，如此往复。当发现无法匹配下一个标记时，右手侧的特定部分（ $\{ \ ( ^ { * } / )$ factor $\} ^ { * }$ ）就会消失。在一个成功的解析过程中，整个右手侧部分会完全根据匹配到的输入标记流来相应地扩展。

有了前面这些基础，下面就向各位展示如何构建一个递归下降的表达式计算器：

import re   
import collections   
#Token specification   
NUM $=$ r'(?P<NUM>\\d+) '   
PLUS $=$ r'(?P<PLUS>\+)

MINUS $=$ r'(?PMINUS>-)'   
TIMES $=$ r'(?P<TIMES>\*)'   
DIVIDE $=$ r'(?P<DIVIDE>/)'   
LPAREN $=$ r'(?P<LPAREN>\{)'   
RPAREN $=$ r'(?P<RPAREN>\{)'   
WS $=$ r'(?P<WS>\s+)   
masterpat $\equiv$ re.compile('|.join([NUM,PLUS,MINUS,TIMES, DIVIDE, LPAREN, RPAREN, WS])   
# Tokenizer   
Token $\equiv$ collections.namedtuple('Token',['type','value'])   
def generate_tokens(text): scanner $=$ masterpat.scanner(text) for m in iter(scanner.match, None): tok $=$ Token(m.lastgroup,m.group()) if tok.type != 'WS': yield tok   
# Parser   
class ExpressionEvaluator:   
```
Implementation of a recursive descent parser. Each method implements a single grammar rule.Use the .accept() method to test and accept the current lookahead token.Use the .expect method to exactly match and discard the next token on on the in (or raise a SyntaxError if it doesn't match).   
```
def parse(self,text): self(tokens $\equiv$ generate_tokens(text) selftok $\equiv$ None # Last symbol consumed self.nexttok $\equiv$ None # Next symbol tokenized self._advance() # Load first lookahead token return self.expr()   
def _advance(self): 'Advance one token ahead' self tok,self.nexttok $\equiv$ self.nexttok,next(self_tokens,No   
def _accept(self,toktype): 'Test and consume the next token if it matches toktype' if self.nexttok and self.nexttok.type $\equiv$ toktype: self._advance() return True

else: return False   
def expect(self,toktype): 'Consume next token if it matches toktype or raise SyntaxError' if not self._accept(toktype): raise SyntaxError('Expected' + toktype)   
# Grammar rules follow   
def expr(self): "expression ::= term { ('+'|'-') term }*" exprval $=$ self.term() while self._accept('PLUS') or self._accept('MINUS'): op $=$ self.?ok.type right $=$ self.?term() if op $= =$ 'PLUS': exprval $+ =$ right elif op $= =$ MINUS': exprval $- =$ right return exprval   
def term(self): "term ::= factor { ('*'|'/') factor }*" termval $=$ self.factor() while self._accept('TIMES') or self._accept('DIVIDE'): op $=$ self.?ok.type right $=$ self.?factor() if op $= =$ 'TIMES': termval $^{\ast} =$ right elif op $= =$ 'DIVIDE': termval / $=$ right return termval   
def factor(self): "factor ::= NUM | ( expr )" if self._accept('NUM'): return int(self.?ok.value) elif self._accept('LPAREN'): exprval $=$ self.?exp() self._expect('RPAREN') return exprval

```autoit
else: raise SyntaxError('Expected NUMBER or LPAREN') 
```

下面是以交互式的方式使用 ExpressionEvaluator 类的示例：

```python
>>> e = ExpressionEvaluator()
>>> e.parse('2')
2
>>> e.parse('2 + 3')
5
>>> e.parse('2 + 3 * 4')
14
>>> e.parse('2 + (3 + 4) * 5')
37
>>> e.parse('2 + (3 + * 4)')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "exprparse.py", line 40, in parse
        return self_expr()
    File "exprparse.py", line 67, in expr
        right = self.term()
    File "exprparse.py", line 77, in term
        termval = self.factor()
    File "exprparse.py", line 93, in factor
        exprval = self_expr()
    File "exprparse.py", line 67, in expr
        right = self.term()
    File "exprparse.py", line 77, in term
        termval = self.factor()
    File "exprparse.py", line 97, in factor
        raise SyntaxError("Expected NUMBER or LPAREN")
SyntaxError: Expected NUMBER or LPAREN 
```

如果我们想做的不只是纯粹的计算，那就需要修改 ExpressionEvaluator 类来实现。比如，下面的实现构建了一棵简单的解析树：

```python
class ExpressionTreeBuilder(ExpressionEvaluator):
    def expr(self):
        "expression ::= term { ('+'| '-') term }"
        exprval = self.term()
        while self._accept('PLUS') or self._accept('MINUS'): 
            op = self tok.type
            right = self.term()
            if op == 'PLUS': 
```

exprval $=$ (' $+^{\prime \prime}$ ,exprval,right) elif op $= =$ MINUS': exprval $= ($ '-,',exprval,right) return exprval   
def term(self): "term ::= factor { ('\*'|'/') factor}" termval $=$ self.factor() while self._accept('TIMES') or self._accept('DIVIDE'): op $=$ self.tk.type right $=$ self.factor() if op $= =$ 'TIMES': termval $= ($ '\*',termval,right) elif op $= =$ 'DIVIDE': termval $= ($ '/,',termval,right) return termval   
def factor(self): 'factor ::= NUM | ( expr )' if self._accept('NUM'): return int(self.tk.value) elif self._accept('LPAREN'): exprval $=$ self_expr() self._expect('RPAREN') return exprval else: raise SyntaxError('Expected NUMBER or LPAREN')

下面的示例展示了它是如何工作的：

```coffeescript
>>> e = ExpressionTreeBuilder()
>>> e.parse('2 + 3')
('+', 2, 3)
>>> e.parse('2 + 3 * 4')
('+', 2, ('*', 3, 4))
>>> e.parse('2 + (3 + 4) * 5')
('+', 2, ('*', ('+', 3, 4), 5))
>>> e.parse('2 + 3 + 4')
('+', ('+', 2, 3), 4)
>>> 
```

# 2.19.3 讨论

文本解析是一个庞大的主题，一般会占用学生们编译原理课程的前三周时间。如果你

正在寻找有关语法、解析算法和其他相关信息的背景知识，那么应该去找一本编译器方面的图书来读。无需赘言，本书是不会重复那些内容的。

然而，要编写一个递归下降的解析器，总体思路还是比较简单的。我们要将每一条语法规则转变为一个函数或方法。因此，如果我们的语法看起来是这样的：

```autohotkey
expr ::= term { ('+'| '-') term }*  
term ::= factor { ('*'| '/') factor }*  
factor ::= '(' expr ')'  
| NUM 
```

就可以像下面这样将它们转换为对应的方法：

```txt
class ExpressionEvaluator: def expr(self): def term(self): def factor(self): 
```

每个方法的任务很简单—必须针对语法规则的每个部分从左到右扫描，在扫描过程中处理符号标记。从某种意义上说，这些方法的目的就是顺利地将规则消化掉，如果卡住了就产生一个语法错误。要做到这点，需要应用下面这些实现技术。

如果规则中的下一个符号标记是另一个语法规则的名称（例如，term或者factor），就需要调用同名的方法。这就是算法中的“下降”部分—控制其下降到另一个语法规则中。有时候规则中会涉及调用已经在执行的方法（例如，在规则factor ::='(' expr ')'中对 expr 的调用）。这就是算法中的“递归”部分。  
• 如果规则中的下一个符号标记是一个特殊的符号（例如'('），需要检查下一个标记，看它们是否能完全匹配。如果不能匹配，这就是语法错误。本节给出的_expect()方法就是用来处理这些步骤的。  
如果规则中的下一个符号标记存在多种可能的选择（例如 $^ +$ 或-），则必须针对每种可能性对下一个标记做检查，只有在有匹配满足时才前进到下一步。这就是本节给出的_accept()方法的目的所在。这有点像_except()的弱化版，在_accept()中如果有匹配满足，就前进到下一步，但如果没有匹配，它只是简单的回退而不会引发一个错误（这样检查才可以继续进行下去）。

• 对于语法规则中出现的重复部分（例如 expr ::=term $\{ \{ + ^ { \prime } \mid ^ { \prime } - ^ { \prime } )$ term $\} ^ { * }$ ）,这是通过while循环来实现的。一般在循环体中收集或处理所有的重复项，直到无法找到更多的重复项为止。  
• 一旦整个语法规则都已经处理完，每个方法就返回一些结果给调用者。这就是在解析过程中将值进行传递的方法。比如，在计算器表达式中，表达式解析的部分结果会作为值来返回。最终它们会结合在一起，在最顶层的语法规则方法中得到执行。

尽管本节给出的例子很简单，但递归下降解析器可以用来实现相当复杂的解析器。例如，Python 代码本身也是通过一个递归下降解析器来解释的。如果对此很感兴趣，可以通过检查 Python 源代码中的Grammar/Grammar 文件来一探究竟。即便如此，要自己手写一个解析器时仍然需要面对各种陷阱和局限。

局限之一就是对于任何涉及左递归形式的语法规则，都没法用递归下降解析器来解决。例如，假设需要解释如下的规则：

```txt
items ::= items ', ' item | item 
```

要完成这样的解析，我们可能会试着这样来定义 items()方法：

```python
def items(self):
    itemsval = self.items()
    if itemsval and self._accept('',):
        itemsval.append(self.item())
    else:
        itemsval = [self.item() ] 
```

唯一的问题就是这么做行不通。实际上这会产生一个无穷递归的错误。

我们也可能会陷入到语法规则自身的麻烦中。例如，我们可能想知道表达式是否能以这种加简单的语法形式来描述：

```txt
expr ::= factor { ('+'|'-'|'*'|'/') factor }*  
factor ::= '(' (expression ')'  
| NUM 
```

这个语法从技术上说是能实现的，但是它却并没有遵守标准算术中关于计算顺序的约定。比如说，表达式“ $" 3 + 4 * 5 "$ 会被计算为 35，而不是我们预期的 23。因此这里需要单独的“expr”和“term”规则来确保计算结果的正确性。

对于真正复杂的语法解析，最好还是使用像 PyParsing 或 PLY 这样的解析工具。如果使用 PLY 的话，解析计算器表达式的代码看起来是这样的：

from ply.lex import lex   
from ply.yacc import yacc   
# Token list   
tokens $=$ ['NUM', 'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'LPAREN', 'RPAREN']   
# Ignored characters   
tignore $= '\backslash t\backslash n'$ # Token specifications (as regexes)   
t_plus $= r' \backslash +$ t_MINUS $= r' -$ t-times $= r' \backslash *$ t_DIVIDE $= r' /$ t_LPAREN $= r' \backslash ($ t_RPAREN $= r' \backslash)$ # Token processing functions   
def t_NUM(t):   
    r'\d+'   
    t.value $= \text{int}(t.\text{value})$ return t   
# Error handler   
def t_error(t):   
    print('Bad character: {!r}.format(t.value[0]))   
    t skip(1)   
# Build thelexer   
lexer $= \text{lex}$ # Grammar rules and handler functions   
def p_expr(p):   
    ''   
    expr : expr PLUS term   
| expr MINUS term   
    ''   
if p[2] == "+" : p[0] = p[1] + p[3]   
elif p[2] == '-': p[0] = p[1] - p[3]   
def p_expr_term(p):

```python
expr : term
```
p[0] = p[1]
def p_term(p):
    ...
    term : term TIMES factor
        | term DIVIDE factor
        ...
    if p[2] == '*':
        p[0] = p[1] * p[3]
    elif p[2] == '/':
        p[0] = p[1] / p[3]
def p_term_factor(p):
    ...
    term : factor
    ...
    p[0] = p[1]
def p_factor(p):
    ...
    factor : NUM
    ...
    p[0] = p[1]
def p_factor_group(p):
    ...
    factor : LPAREN expr RAREN
    ...
    p[0] = p[2]
def p_error(p):
    print('Syntax error')
parser = yacc() 
```

在这份代码中会发现所有的东西都是以一种更高层的方式来定义的。我们只需编写匹配标记符号的正则表达式，以及当匹配各种语法规则时所需要的高层处理函数就行了。而实际运行解析器、接收符号标记等都完全由库来实现。

下面是如何使用解析器对象的示例：

```python
>>> parser.parse('2')
2
>>> parser.parse('2+3') 
```

```txt
5   
>>> parser.parse('2+(3+4)*5')   
37   
>>> 
```

如果想在编程中增加一点激动兴奋的感觉，编写解析器和编译器会是非常有趣的课题。再次说明，一本编译器方面的教科书会涵盖许多理论之下的底层细节。但是，在网上同样也能找到许多优秀的在线资源。Python 自带的 ast模块也同样值得去看看。

# 2.20 在字节串上执行文本操作

# 2.20.1 问题

我们想在字节串（Byte String）上执行常见的文本操作（例如，拆分、搜索和替换）。

# 2.20.2 解决方案

字节串已经支持大多数和文本字符串一样的内建操作。例如：

```txt
>>> data = b'Hello World'
>>> data[0:5]
b'Hello'
>>> data.startswith(b'Hello')
True
>>> data.split()
[b'Hello', b'World']
>>> data.replace(b'Hello', b'Hello Cruel')
b'Hello Cruel World'
>>> 
```

类似这样的操作在字节数组上也能完成。例如：

```prolog
>>> data = ndarray(b'Hello World')  
>>> data[0:5]  
bytearray(b'Hello')  
>>> data.startswith(b'Hello')  
True  
>>> data.split()  
[bytearray(b'Hello'), ndarray(b'World')]  
>>> data.replace(b'Hello', b'Hello Cruel')  
bytearray(b'Hello Cruel World')  
>>> 
```

我们可以在字节串上执行正则表达式的模式匹配操作，但是模式本身需要以字节串的

形式来指定。示例如下：

```python
>>> data = b'FOO:BAR,SPAM'
>>> import re
>>> re.split(['(:,)',data)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "/usr/local/lib/python3.3/re.py", line 191, in split
        return _compile(pattern, flags).split(string, maxsplit)
TypeError: can't use a string pattern on a bytes-like object
>>> re.split(b'[(:,)',data) # Notice: pattern as bytes
[b'FOO', b'BAR', b'SPAM'] 
```

# 2.20.3 讨论

就绝大部分情况而言，几乎所有能在文本字符串上执行的操作同样也可以在字节串上进行。但是，还是有几个显著的区别值得大家注意。例如：

```txt
>>> a = 'Hello World' # Text string  
>>> a[0]  
'H'  
>>> a[1]  
'e'  
>>> b = b'Hello World' # Byte string  
>>> b[0]  
72  
>>> b[1]  
101  
>>> 
```

这种语义上的差异会对试图按照字符的方式处理面向字节流数据的程序带来影响。

其次，字节串并没有提供一个漂亮的字符串表示，因此打印结果并不干净利落，除非首先将其解码为文本字符串。示例如下：

```txt
>>>s=b'Hello World'   
>>>print(s)   
b'Hello World' #Observe b'...   
>>>print(sdecode('ascii'))   
HelloWorld   
>>> 
```

同样道理，在字节串上是没有普通字符串那样的格式化操作的。

```txt
>>> b'%10s %10d %10.2f' % (b'ACME', 100, 490.1)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for %: 'bytes' and 'tuple'
>>> b{}\{}{}\'.format(b'ACME', 100, 490.1)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
AttributeError: 'bytes' object has no attribute 'format'
>>> 
```

如果想在字节串上做任何形式的格式化操作，应该使用普通的文本字符串然后再做编码。示例如下：

```txt
>>>‘{10s}\{:10d}\{:10.2f}\'.format('ACME',100,490.1).encode('ascii')  
b'ACME 100 490.10'  
>>> 
```

最后，需要注意的是使用字节串会改变某些特定操作的语义—尤其是那些与文件系统相关的操作。例如，如果提供一个以字节而不是文本字符串来编码的文件名，文件系统通常都会禁止对文件名的编码/解码。示例如下：

```txt
>>> # Write a UTF-8 filename  
>>> with open('jalape\xf1o.txt', 'w') as f:  
... f.write('spicy')  
...  
>>> # Get a directory listing  
>>> import os  
>>> os.listdir('./.')  # Text string (names are decoded)  
['Jalapeño.txt']  
>>> os.listdir(b '.')  # Byte string (names left as bytes)  
[b'jalapen\xcc\x83o.txt']  
>>> 
```

请注意这个例子中的最后部分，本例中以字节串作为目录名从而导致产生的名称以未经编码的原始字节形式返回。在显示目录内容时，文件名包含了原始的 UTF-8 编码。有关文件名的处理请参阅 5.15 节。

最后要说的是，有些程序员可能会因为性能上有可能得到提升而倾向于将字节串作为文本字符串的替代来使用。尽管操纵字节确实要比文本来的略微高效一些（由于同Unicode 相关的固有开销较高），但这么做通常会导致非常混乱和不符合语言习惯的代码。我们常会发现字节串和 Python 中许多其他部分并不能很好地相容，这样为了保证结果的正确性，我们只能手动去执行各种各样的编码/解码操作。坦白地说，如果要同文本打交道，在程序中使用普通的文本字符串就好，不要用字节串。

# 数字、 日期和时间

在 Python 中对整数和浮点数进行数学计算是非常容易的。但是，如果需要对分数、数组或者日期和时间进行计算，就需要完成更多的工作。本章的重点正是应对这些主题。

# 3.1 对数值进行取整

# 3.1.1 问题

我们想将一个浮点数取整到固定的小数位。

# 3.1.2 解决方案

对于简单的取整操作，使用内建的 round(value, ndigits)函数即可。示例如下：

```diff
>>> round(1.23, 1)  
1.2  
>>> round(1.27, 1)  
1.3  
>>> round(-1.27, 1)  
-1.3  
>>> round(1.25361, 3)  
1.254  
>>> 
```

当某个值恰好等于两个整数间的一半时，取整操作会取到离该值最接近的那个偶数上。也就是说，像 1.5 或 2.5 这样的值都会取整到 2。

传递给 round()的参数 ndigits 可以是负数，在这种情况下会相应地取整到十位、百位、

千位等。示例如下：

```txt
>> a = 1627731  
>> round(a, -1)  
1627730  
>> round(a, -2)  
1627700  
>> round(a, -3)  
1628000  
>>> 
```

# 3.1.3 讨论

在对值进行输出时别把取整和格式化操作混为一谈。如果只是将数值以固定的位数输出，一般来说是用不着 round()的。相反，只要在格式化时指定所需要的精度就可以了。示例如下：

>>>x=1.23456   
>>>format(x,'0.2f')   
'1.23'   
>>>format(x,'0.3f')   
'1.235'   
>>>valueis $\{:0.3\mathrm{f}\} ^{\prime}.$ .format(x)   
'value is 1.235'   
>>>

此外，不要采用对浮点数取整的方式来“修正”精度上的问题。比如，我们可能会倾向于这样做：

```txt
>>> a = 2.1  
>>> b = 4.2  
>>> c = a + b  
>>> c  
6.300000000000001  
>>> c = round(c, 2) # "Fix" result (??)  
>>> c  
6.3  
>>> 
```

对于大部分涉及浮点数的应用程序来说，一般来讲都不必（或者说不推荐）这么做。尽管这样会引入一些小误差，但这些误差是可理解的，也是可容忍的。如果说避免出现误差的行为非常重要（例如在金融类应用中），那么可以考虑使用 decimal 模块，这也正是下一节的主题。

# 3.2 执行精确的小数计算

# 3.2.1 问题

我们需要对小数进行精确计算，不希望因为浮点数天生的误差而带来影响。

# 3.2.2 解决方案

关于浮点数，一个尽人皆知的问题就是它们无法精确表达出所有的十进制小数位。此外，甚至连简单的数学计算也会引入微小的误差。例如：

>>a $= 4,2$ >>b $= 2,1$ >>a+b  
6.300000000000001  
>> $(\mathrm{a} + \mathrm{b}) = = 6,3$ False  
>>>

这些误差实际上是底层 CPU 的浮点运算单元和 IEEE 754 浮点数算术标准的一种“特性”。由于 Python 的浮点数类型保存的数据采用的是原始表示形式，因此如果编写的代码用到了 float实例，那就无法避免这样的误差。

如果期望得到更高的精度（并愿意为此牺牲掉一些性能），可以使用 decimal 模块：

```txt
>>> from decimal import Decimal
>>> a = Decimal('4.2')
>>> b = Decimal('2.1')
>>> a + b
Decimal('6.3')
>>> print(a + b)
6.3
>>> (a + b) == Decimal('6.3')
True
>>> 
```

这么做初看起来似乎有点怪异（将数字以字符串的形式来指定）。但是，Decimal 对象能以任何期望的方式来工作（支持所有常见的数学操作）。如果要将它们打印出来或是在字符串格式化函数中使用，它们看起来就和普通的数字一样。

decimal模块的主要功能是允许控制计算过程中的各个方面，这包括数字的位数和四舍五入。要做到这些，需要创建一个本地的上下文环境然后修改其设定。示例如下：

```python
>>> from decimal import localcontext  
>>> a = Decimal('1.3') 
```

```diff
>>> b = Decimal('1.7')
>>> print(a / b)
0.7647058823529411764705882353
>>> with localcontext() as ctx:
...
...
...
...
...
0.765
>>> with localcontext() as ctx:
...
...
...
...
...
0.7647058823529411764705882352941176
>>> 
```

# 3.2.3 讨论

decimal 模 块 实 现 了 IBM 的 通 用 十 进 制 算 术 规 范 （ General Decimal ArithmeticSpecification）。不用说，这里面有着数量庞大的配置选项，这些都超出了本书的范围。

Python 新手可能会倾向于利用 decimal 模块来规避处理 float 数据类型所固有的精度问题。但是，正确理解你的应用领域是至关重要的。如果我们处理的是科学或工程类的问题，像计算机图形学或者大部分带有科学性质的问题，那么更常见的做法是直接使用普通的浮点类型。首先，在真实世界中极少有什么东西需要计算到小数点后 17 位（float提供 17位的精度）。因此，在计算中引入的微小误差根本就不足挂齿。其次，原生的浮点数运算性能要快上许多—如果要执行大量的计算，那性能问题就显得很重要了。

也就是说我们无法完全忽略误差。数学家花费了大量的时间来研究各种算法，其中一些算法的误差处理能力优于其他的算法。我们同样还需要对类似相减抵消（subtractivecancellation）以及把大数和小数加在一起时的情况多加小心。示例如下：

```txt
>>> nums = [1.23e+18, 1, -1.23e+18]  
>>> sum(numbers) # Notice how 1 disappears  
0.0  
>>> 
```

上面这个例子可以通过使用 math.fsum()以更加精确的实现来解决：

```txt
>>> import math
>>> math.fsum(numbers)
1.0
>>> 
```

但是对于其他的算法，需要研究算法本身，并理解其误差传播（error propagation）的

性质。

综上所述，decimal模块主要用在涉及像金融这一类业务的程序中。在这样的程序里，计算中如果出现微小的误差是相当令人生厌的。因此，decimal 模块提供了一种规避误差的方式。当用 Python 作数据库的接口时也会常常会遇到 Decimal 对象—尤其是当访问金融数据时更是如此。

# 3.3 对数值做格式化输出

# 3.3.1 问题

我们需要对数值做格式化输出，包括控制位数、对齐、包含千位分隔符以及其他一些细节。

# 3.3.2 解决方案

要对一个单独的数值做格式化输出，使用内建的 format()函数即可。示例如下：

```txt
>>> x = 1234.56789
>>> # Two decimal places of accuracy
>>> format(x, '0.2f')
'1234.57'
>>> # Right justified in 10 chars, one-digit accuracy
>>> format(x, '>10.1f')
' 1234.6'
>>> # Left justified
>>> format(x, '<10.1f')
'1234.6'
>>> # Centered
>>> format(x, '^10.1f')
' 1234.6'
>>> # Inclusion of thousands separator
>>> format(x, ',')
'1,234.56789'
>>> format(x, '0,.1f')
'1,234.6'
>>> 
```

如果想采用科学计数法，只要把 f 改为 e 或者 E 即可，根据希望采用的指数规格来指定。

示例如下：

```txt
>>> format(x, 'e')
'1.234568e+03'
>>> format(x, '0.2E')
'1.23E+03'
>>> 
```

以上两种情况中，指定宽度和精度的一般格式为'[<>^]?width[,]?(.digits)?'，这里 width和 digits 为整数，而?代表可选的部分。同样的格式也可用于字符串的.format()方法中。示例如下：

>>> 'The value is $\{0, .2f\}$ .format(x)  
'The value is 1,234.57'  
>>>

# 3.3.3 讨论

对数值做格式化输出通常都是很直接的。本节展示的技术既能用于浮点型数，也能适用于 decimal 模块中的 Decimal 对象。

当需要限制数值的位数时，数值会根据 round()函数的规则来进行取整。示例如下：

```txt
>>>x   
1234.56789   
>>>format(x,'0.1f')   
'1234.6'   
>>>format(-x,'0.1f')   
'-1234.6'   
>>> 
```

对数值加上千位分隔符的格式化操作并不是特定于本地环境的。如果需要将这个需求纳入考虑，应该考察一下 local模块中的函数。还可以利用字符串的 translate()方法交换不同的分隔字符。示例如下：

```txt
>>> swap separators = {ord(''.'): ',', ord('','): '.']
>>> format(x, ':').translate(swap separators)
'1.234,56789'
>>>
```

在很多 Python 代码中，常用 $\%$ 操作符来对数值做格式化处理。示例如下：

```txt
>>> '0.2f' % x
'1234.57'
>>> '10.1f' % x
' 1234.6'
>>> '8-10.1f' % x 
```

```txt
'1234.6  
>>> 
```

这种格式化操作仍然是可接受的，但是比起更加现代化的 format()方法，这种方法就显得不是那么强大了。比如说，当使用 $\%$ 操作符来格式化数值时，有些功能就没法得到支持了（例如添加千位分隔符）。

# 3.4 同二进制、八进制和十六进制数打交道

# 3.4.1 问题

我们需要对以二进制、八进制或十六进制表示的数值做转换或输出。

# 3.4.2 解决方案

要将一个整数转换为二进制、八进制或十六进制的文本字符串形式，只要分别使用内建的 bin()、oct()和 hex()函数即可，示例如下：

```txt
>>> x = 1234  
>>> bin(x)  
'0b10011010010'  
>>> oct(x)  
'0o2322'  
>>> hex(x)  
'0x4d2'  
>>> 
```

此外，如果不希望出现 0b、0o 或者 0x 这样的前缀，可以使用 format()函数。示例如下：

```txt
>>> format(x, 'b')
'10011010010'
>>> format(x, 'o')
'2322'
>>> format(x, 'x')
'4d2'
>>> 
```

整数是有符号的，因此如果要处理负数的话，输出中也会带上一个符号。示例如下：

```txt
>> x = -1234  
>> format(x, 'b')  
' -10011010010'  
>> format(x, 'x')  
' -4d2'  
>> 
```

相反，如果需要产生一个无符号的数值，需要加上最大值来设置比特位的长度。比如，要展示一个 32 位数，可以像这样实现：

```txt
>> x = -1234  
>> format(2**32 + x, 'b')  
'111111111111111111101100101110'  
>> format(2**32 + x, 'x')  
'FFFFFFb2e'  
>> 
```

要将字符串形式的整数转换为不同的进制，只需要使用 int()函数再配合适当的进制即可。示例如下：

```diff
>>> int('4d2', 16)
1234
>>> int('10011010010', 2)
1234
>>> 
```

# 3.4.3 讨论

对于大部分的情况，处理二进制、八进制和十六进制数都是非常直接的。只是需要记住，这些转换只适用于转换整数的文本表示形式，实际在底层只有一种整数类型。

最后，对于那些用到了八进制数的程序员来说还有一个地方需要注意。在 Python 中指定八进制数的语法和许多其他编程语言稍有不同。比方说，如果试着做如下的操作，则会得到一个语法错误：

```html
>>> import os
>>> os chmod('script.py', 0755)
File "<stdin>", line 1
os chmod('script.py', 0755)
SyntaxError: invalid token
>>> 
```

请确保在八进制数前添加 0o 前缀，就像这样：

```txt
>>> os chmod('script.py', 0o755)  
>>> 
```

# 3.5 从字节串中打包和解包大整数

# 3.5.1 问题

我们有一个字节串，需要将其解包为一个整型数值。此外，还需要将一个大整数重新

转换为一个字节串。

# 3.5.2 解决方案

假设程序需要处理一个有着 16个元素的字节串，其中保存着一个 128 位的整数。示例如下：

```javascript
data = b'\x00\x124V\x00x\x90\xab\x00\xcd\xef\x01\x00#\x004' 
```

要将字节解释为整数，可以使用 int.from_bytes()，然后像这样指定字节序即可：

```txt
>>> len(data)
16
>>> int.from_bytes(data, 'little')
69120565665751139577663547927094891008
>>> int.from_bytes(data, 'big')
94522842520747284487117727783387188
>>> 
```

要将一个大整数重新转换为字节串，可以使用 int.to_bytes()方法，只要指定字节数和字节序即可。示例如下：

```txt
>>> x = 94522842520747284487117727783387188  
>>> x.to_bytes(16, 'big')  
b'\x00\x124V\x00x\x90\xab\x00\xcd\xef\x01\x00#\x004'  
>>> x.to_bytes(16, 'little')  
b'\x00#\x00\x01\xef\xcd\x00\xab\x90x\x00V4\x12\x00'  
>>> 
```

# 3.5.3 讨论

在大整数和字节串之间互相转换并不算是常见的操作。但是，有时候在特定的应用领域中却有这样的需求，例如加密技术或网络应用中。比方说 IPv6 网络地址就是由一个128位的整数来表示的。如果正在编写的代码需要将这样的值从数据记录中提取出来，就得面对这个问题。

作为本节中技术的替代方案，我们可能会倾向于使用 struct 模块来完成解包，具体可参见 6.11 节。这行得通，但是 struct 模块可解包的整数大小是有限制的。因此，需要解包多个值，然后再将它们合并起来以得到最终的结果。示例如下：

```txt
>>> data  
b'\x00\x124V\x00x\x90\xab\x00\xcd\xef\x01\x00#\x004'  
>>> import struct  
>>> hi, lo = struct unpack('>QQ', data)  
>>> (hi << 64) + lo  
94522842520747284487117727783387188  
>>> 
```

字节序的规范（大端或小端）指定了组成整数的字节是从低位到高位排列还是从高位到低位排列。只要我们精心构造一个十六进制数，就能很容易看出这其中的意义：

>>x $=$ 0x01020304   
>>x.to_bytes(4,'big')   
b'\x01\x02\x03\x04'   
>>x.to_bytes(4,'little')   
b'\x04\x03\x02\x01'   
>>

如果尝试将一个整数打包成字节串，但字节大小不合适的话就会得到一个错误信息。如果需要的话，可以使用 int.bit_length()方法来确定需要用到多少位才能保存这个值：

```python
>>> x = 523 ** 23
>>> x
335381300113661875107536852714019056160355655333978849017944067
>>> x.to_bytes(16, 'little')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
OverflowError: int too big to convert
>>> x.bit_length()
208
>>> nbytes, rem = divmod(x.bit_length(), 8)
>>> if rem:
    ...     nbytes += 1
>>> x.to_bytes(nbytes, 'little')
b'\x03X\xf1\x82iT\x96\xac\xc7c\x16\xf3\xb9\xcf...\xd0'
>>> 
```

# 3.6 复数运算

# 3.6.1 问题

我们的代码在同最新的 Web 认证方案交互时遇到了奇点（singularity）问题，而唯一的解决方案是在复平面解决。或者也许只需要利用复数完成一些计算就可以了。

# 3.6.2 解决方案

复数可以通过 complex(real, imag)函数来指定，或者通过浮点数再加上后缀 j 来指定也行。示例如下：

```haskell
>> a = complex(2, 4)  
>> b = 3 - 5j  
>> a 
```

$(2 + 4j)$ $\ggg$ b (3-5j) $\ggg$

实部、虚部以及共轭值可以很方便地提取出来，示例如下：

```txt
>>> a.real   
2.0   
>>> a.imag   
4.0   
>>> a.conjugate()   
(2-4j)   
>>> 
```

此外，所有常见的算术运算操作都适用于复数：

>>>a $^+$ b   
(5-1j)   
>>>a\*b   
(26+2j)   
>>>a/b   
(-0.4117647058823529+0.6470588235294118j)   
>>>abs(a)   
4.47213595499958   
>>>

最后，如果要执行有关复数的函数操作，例如求正弦、余弦或平方根，可以使用 cmath模块：

```txt
>>> import cmath
>>> cmath.sin(a)
(24.83130584894638-11.356612711218174j)
>>> cmath.cos(a)
(-11.36423470640106-24.814651485634187j)
>>> cmath.exp(a)
(-4.829809383269385-5.5920560936409816j)
>>>
```

# 3.6.3 讨论

Python 中大部分和数学相关的模块都可适用于复数。例如，如果使用 numpy 模块，可以很直接地创建复数数组，并对它们执行操作：

```txt
>>> import numpy as np  
>>> a = np.array([2+3j, 4+5j, 6-7j, 8+9j])  
>>> a 
```

```javascript
array([2.+3.j,4.+5.j,6.-7.j,8.+9.j])  
>>>a+2  
array([4.+3.j,6.+5.j,8.-7.j,10.+9.j])  
>>>np.sin(a)  
array([9.15449915-4.16890696j,-56.16227422-48.50245524j, -153.20827755-526.47684926j，4008.42651446-589.49948373j])  
>>> 
```

Python 中的标准数学函数默认情况下不会产生复数值，因此像这样的值不会意外地出现在代码里。例如：

```txt
>>> import math
>>> math.sqrt(-1)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ValueError: math domain error
>>> 
```

如果希望产生复数结果，那必须明确使用 cmath 模块或者在可以感知复数的库中声明对复数类型的使用。示例如下：

```txt
>>> import cmath  
>>> cmath.sqrt(-1)  
1j  
>>> 
```

# 3.7 处理无穷大和 NaN

# 3.7.1 问题

我们需要对浮点数的无穷大、负无穷大或 NaN（not a number）进行判断测试。

# 3.7.2 解决方案

Python 中并没有特殊的语法用来表示这些特殊的浮点数值，但是它们可以通过 float()来创建。示例如下：

```txt
>>> a = float('inf')  
>>> b = float('-inf')  
>>> c = float('nan')  
>>> a  
inf  
>>> b  
-inf 
```

```txt
>>>c   
nan   
>>> 
```

要检测是否出现了这些值，可以使用 math.isinf()和 math.isnan()函数。示例如下：

```txt
>>>math.isinf(a)   
True   
>>>math.isnan(c)   
True   
>>> 
```

# 3.7.3 讨论

要获得关于这些特殊的浮点数值的详细信息，应该参考 IEEE 754 规范。但是，这里有几个棘手的细节问题需要搞清楚，尤其是当涉及比较操作和操作符时可能出现的问题。

无穷大值在数学计算中会进行传播。例如：

```txt
>>> a = float('inf')  
>>> a + 45  
inf  
>>> a * 10  
inf  
>>> 10 / a  
0.0  
>>> 
```

但是，某些特定的操作会导致未定义的行为并产生 NaN 的结果。例如：

```txt
>>> a = float('inf')  
>>> a/a  
nan  
>>> b = float('-inf')  
>>> a + b  
nan  
>>> 
```

NaN 会通过所有的操作进行传播，且不会引发任何异常。例如：

```txt
>>> c = float('nan')
>>> c + 23
nan
>>> c / 2
nan
>>> c * 2 
```

```txt
nan   
>>>math.sqrt(c)   
nan   
>>> 
```

有关 NaN，一个微妙的特性是它们在做比较时从不会被判定为相等。例如：

```txt
>>> c = float('nan')
>>> d = float('nan')
>>> c == d
False
>>> c is d
False
>>> 
```

正因为如此，唯一安全检测 NaN 的方法是使用 math.isnan()，正如本节示例代码中的那样。

有时候程序员希望在出现无穷大或 NaN 结果时可以修改 Python 的行为，让它抛出异常。fpectl模块可以用来调整这个行为，但是在标准Python 中它是没有开启的，而且这个模块是同平台相关的，只针对专家级的程序员使用。可以参见 Python 在线文档（http://docs.python.org/3/library/fpectl.html）以获得进一步的细节。

# 3.8 分数的计算

# 3.8.1 问题

仿佛进入时光机一样，我们突然发现自己在做涉及分数处理的小学家庭作业。或者也许我们正在为自己的木材商店编写测量计算方面的代码。

# 3.8.2 解决方案

fractions 模块可以用来处理涉及分数的数学计算问题。示例如下：

```txt
>>> from fractions import Fraction
>>> a = Fraction(5, 4)
>>> b = Fraction(7, 16)
>>> print(a + b)
27/16
>>> print(a * b)
35/64
>>> # Getting numerator/denominator
>>> c = a * b 
```

```txt
>>> c.numerator   
35   
>>> c.denominator   
64   
>>> # Converting to a float   
>>> float(c)   
0.546875   
>>> # Limiting the denominator of a value   
>>> print(c limit_denominator(8))   
4/7   
>>> # Converting a float to a fraction   
>>> x = 3.75   
>>> y = Fraction(*x.as_integer_ratio())   
>>> y   
Fraction(15, 4)   
>>> 
```

# 3.8.3 讨论

在大多数程序中，涉及分数的计算问题并不常见。但是在有些场景中使用分数还是有道理的。比如，允许程序接受以分数形式给出的单位计量并执行相应的计算，这样可以避免用户手动将数据转换为 Decimal对象或浮点数。

# 3.9 处理大型数组的计算

# 3.9.1 问题

我们需要对大型的数据集比如数组或网格（grid）进行计算。

# 3.9.2 解决方案

对于任何涉及数组的计算密集型任务，请使用 NumPy 库。NumPy 的主要特性是为Python 提供了数组对象，比标准 Python中的列表有着更好的性能表现，因此更加适合于做数学计算。下面是一个简短的示例，用来说明列表同 NumPy 数组在行为上的几个重要不同之处：

```txt
>>> # Python lists  
>>> x = [1, 2, 3, 4]  
>>> y = [5, 6, 7, 8]  
>>> x * 2 
```

```txt
[1, 2, 3, 4, 1, 2, 3, 4]  
>>> x + 10  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
TypeError: can only concatenate list (not "int") to list  
>>> x + y  
[1, 2, 3, 4, 5, 6, 7, 8]  
>>> # Numpy arrays  
>>> import numpy as np  
>>> ax = np.array([1, 2, 3, 4])  
>>> ay = np.array([5, 6, 7, 8])  
>>> ax * 2  
array([2, 4, 6, 8])  
>>> ax + 10  
array([11, 12, 13, 14])  
>>> ax + ay  
array([6, 8, 10, 12])  
>>> ax * ay  
array([5, 12, 21, 32])  
>>> 
```

可以看到，有关数组的几个基本数学运算在行为上都有所不同。特别是，NumPy 中的数组在进行标量运算（例如 $\mathbf { a x } \ast 2$ 或 $\mathrm { a x } + 1 0$ ）时是针对逐个元素进行计算的。此外，当两个操作数都是数组时，NumPy 数组在进行数学运算时会针对数组的所有元素进行计算，并产生出一个新的数组作为结果。

由于数学操作会同时施加于所有的元素之上，这一事实使得对整个数组的计算变得非常简单和快速。比方说，如果想计算多项式的值：

```txt
>>> def f(x):
...
...
return 3*x**2 - 2*x + 7
...
>>> f(ax)
array([8,15,28,47])
>>> 
```

NumPy 提供了一些“通用函数”的集合，它们也能对数组进行操作。这些通用函数可作为 math 模块中所对应函数的替代。示例如下：

```txt
>>> np.sqrt(ax)
array([1, 1.41421356, 1.73205081, 2.])
>>> np.cos(ax)
array([0.54030231, -0.41614684, -0.9899925, -0.65364362])
>>> 
```

使用 NumPy 中的通用函数，其效率要比对数组进行迭代然后使用 math 模块中的函数每次只处理一个元素快上百倍。因此，只要有可能就应该使用这些通用函数。

在底层，NumPy数组的内存分配方式和 C或者 Fortran 一样。即，它们是大块的连续内存，由同一种类型的数据组成。正是因为这样，NumPy才能创建比通常 Python中的列表要大得多的数组。例如，如果想创建一个 $1 0 0 0 0 \times 1 0 0 0 0$ 的二维浮点数组，这根本不是问题：

>>>grid $\equiv$ np.zeros(shape=(10000,10000)，dtype $\equiv$ float)   
>>>grid   
array([ [0.，0.，0.，..，0.，0.，0.], [0.，0.，0.，..，0.，0.，0.], [0.，0.，0.，..，0.，0.，0.], ...， [0.，0.，0.，..，0.，0.，0.], [0.，0.，0.，..，0.，0.，0.], [0.，0.，0.，..，0.，0.，0.]）   
>>>

所有的常用操作仍然可以同时施加于所有的元素之上：

>>>grid $+ = 10$ >>>grid   
array([10.，10.，10.，...，10.，10.，10.], [10.，10.，10.，...，10.，10.，10.], [10.，10.，10.，...，10.，10.，10.], ...， [10.，10.，10.，...，10.，10.，10.，]， [10.，10.，10.，...，10.，10.，10.]])   
>>>np.singrid)   
array([[-0.54402111，-0.54402111，-0.54402111，...，-0.54402111, -0.54402111，-0.54402111], [-0.54402111，-0.54402111，-0.54402111，...，-0.54402111, -0.54402111，-0.54402111]， [-0.54402111，-0.54402111，-0.54402111，...，-0.54402111, -0.54402111，-0.5440211]），   
>>>

关于 NumPy，一个特别值得提起的方面就是 NumPy 扩展了 Python 列表的索引功能尤其是针对多维数组时更是如此。为了说明，我们先构造一个简单的二维数组然后做些试验：

>>> a $=$ np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

>>> a

```txt
array([[1,2，3，4]，[5，6，7，8]，[9，10，11，1
```

```txt
>>> #Select row 1   
>>>a[1]   
array([5,6，7，8]) 
```

```txt
>>> #Selectcolumn1   
>>>a[:,1]   
array([2，6，10])
```

```txt
>>> # Select a subregion and change it  
>>> a[1:3, 1:3]  
array([[6, 7], [10, 11]]) 
```

```erlang
>>> a[1:3, 1:3] += 10  
>>> a  
array([[1, 2, 3, 4], [5, 16, 17, 8], [9, 20, 21, 1] 
```

>>> # Broadcast a row vector across an operation on all rows

```txt
>>>a+[100，101，102，103] array([[101，103，105，107]， [105，117，119，111]， [109，121，123，115])
```

```txt
>>>a   
array([1，2，3，4]， [5，16，17，8]， [9，20，21，12])
```

```txt
>>> # Conditional assignment on an array  
>>> np.where(a < 10, a, 10)  
array([[1, 2, 3, 4], [5, 10, 10, 8], [9, 10, 10, 10]]) 
```

```txt
>>> 
```

# 3.9.3 讨论

Python 中大量的科学和工程类函数库都以 NumPy作为基础，它也是广泛使用中的最为庞大和复杂的模块之一。尽管如此，对于NumPy我们还是可以从构建简单的例子开始，逐步试验，最后实现一些有用的应用。

提到 NumPy 的用法，一个相对来说比较常见的导入方式是 import numpy as np，正如我们给出的示例中那样，这么做缩短了名称，方便我们每次在程序中输入。

要获得更多信息，一定要去看看 NumPy 的官方站点 http://www.numpy.org。

# 3.10 矩阵和线性代数的计算

# 3.10.1 问题

我们需要执行矩阵和线性代数方面的操作，比如矩阵乘法、求行列式、解线性方程等。

# 3.10.2 解决方案

NumPy 库中有一个 matrix 对象可用来处理这种情况。Matrix 对象和 3.9 节中描述的数组对象有些类似，但是在计算时遵循线性代数规则。下面的例子展示了几个重要的特性：

```python
>>> import numpy as np
>>> m = np.matrix([[1, -2, 3], [0, 4, 5], [7, 8, -9]])
>>> m
matrix([[1, -2, 3],
[0, 4, 5],
[7, 8, -9]])
>>> # Return transpose
>>> m.T
matrix([[1, 0, 7],
[-2, 4, 8],
[3, 5, -9]])
>>> # Return inverse
>>> m.I
matrix([[0.33043478, -0.02608696, 0.09565217],
[-0.15217391, 0.13043478, 0.02173913],
[0.12173913, 0.09565217, -0.0173913]])
>>> # Create a vector and multiply 
```

```erlang
>>>v=np.matrix([[2],[3],[4]])   
>>>v   
matrix([[2], [3], [4]])   
>>>m\*v   
matrix([[8], [32], [2]])   
>>> 
```

更多的操作可在 numpy.linalg 子模块中找到。例如：

```txt
>>> import numpy.linalg
>>> # Determinant
>>> numpy.linalg.det(m)
-229.9999999999983
>>> # Eigenvalues
>>> numpy.linalg.eigvals(m)
array([-13.11474312, 2.75956154, 6.35518158])
>>> # Solve for x in mx = v
>>> x = numpy.linalg.solve(m, v)
>>> x
matrix([[0.96521739],
[0.17391304],
[0.46086957]])
>>> m * x
matrix([[2.],
[3.],
[4.]])
>>> v
matrix([[2],
[3],
[4]])
>>> 
```

# 3.10.3 讨论

显然，线性代数是个庞大的课题，远超出了本书的范围。但是，如果需要处理矩阵和向量，NumPy 是个很好的起点。请访问 http://www.numpy.org 以获得更多详细的信息。

# 3.11 随机选择

# 3.11.1 问题

我们想从序列中随机挑选出元素，或者想生成随机数。

# 3.11.2 解决方案

random 模块中有各种函数可用于需要随机数和随机选择的场景。例如，要从序列中随机挑选出元素，可以使用 random.choice()：

```txt
>>> import random
>>> values = [1, 2, 3, 4, 5, 6]
>>> random.choice(values)
2
>>> random.choice(values)
3
>>> random.choice(values)
1
>>> random.choice(values)
4
>>> random.choice(values)
6 
```

如果想取样出 N 个元素，将选出的元素移除以做进一步的考察，可以使用 random.sample()：

```txt
>>> random.sample(values, 2)
[6, 2]
>>> random.sample(values, 2)
[4, 3]
>>> random.sample(values, 3)
[4, 3, 1]
>>> random.sample(values, 3)
[5, 4, 1]
>>> 
```

如果只是想在序列中原地打乱元素的顺序（洗牌），可以使用 random.shuffle()：

```txt
>>> random shuffle(values)
>>> values
[2, 4, 6, 5, 3, 1]
>>> random shuffle(values)
>>> values 
```

```txt
[3，5，2，1，6，4]  
>>>
```

要产生随机整数，可以使用 random.randint()：

```txt
>>> random.randint(0,10)  
2  
>>> random.randint(0,10)  
5  
>>> random.randint(0,10)  
0  
>>> random.randint(0,10)  
7  
>>> random.randint(0,10)  
10  
>>> random.randint(0,10)  
3  
>>> 
```

要产生 0 到 1 之间均匀分布的浮点数值，可以使用 random.random()：

```diff
>>> random.random()
0.9406677561675867
>>> random.random()
0.133129581343897
>>> random.random()
0.4144991136919316
>>> 
```

如果要得到由 N 个随机比特位所表示的整数，可以使用 random.getrandbits()：

```diff
>>> random.randint(200)
335837000776573622800628485064121869519521710558559406913275
>>> 
```

# 3.11.3 讨论

random模块采用马特赛特旋转算法（Mersenne Twister，也称为梅森旋转算法）来计算随机数。这是一个确定性算法，但是可以通过 random.seed()函数来修改初始的种子值。示例如下：

```txt
random.seed() #Seed based on system time or os.urandom()   
random.seed(12345) #Seed based on integer given   
random.seed(b'bytedata') #Seed based on byte data 
```

除了以上展示的功能外，random 模块还包含有计算均匀分布、高斯分布和其他概率分布的函数。比如，random.uniform()可以计算均匀分布值，而 random.gauss()则可计算出

正态分布值。请查阅文档以获得对其他所支持的分布的相关信息。

random 模块中的函数不应该用在与加密处理相关的程序中。如果需要这样的功能，考虑使用 ssl 模块中的函数来替代。例如，ssl.RAND_bytes()可以用来产生加密安全的随机字节序列。

# 3.12 时间换算

# 3.12.1 问题

我们的代码需要进行简单的时间转换工作，比如将日转换为秒，将小时转换为分钟等。

# 3.12.2 解决方案

我们可以利用 datetime 模块来完成不同时间单位间的换算。例如，要表示一个时间间隔，可以像这样创建一个 timedelta 实例：

```txt
>>> from datetime import timedelta
>>> a = timedelta(days=2, hours=6)
>>> b = timedelta(hours=4.5)
>>> c = a + b
>>> c.days
2
>>> cseconds
37800
>>> c(seconds / 3600
10.5
>>> c.totalSeconds() / 3600
58.5
>>> 
```

如果需要表示特定的日期和时间，可以创建 datetime 实例并使用标准的数学运算来操纵它们。示例如下：

```txt
>>> from datetime import datetime
>>> a = datetime(2012, 9, 23)
>>> print(a + timethelta(days=10))
2012-10-03 00:00:00
>>> b = datetime(2012, 12, 21)
>>> d = b - a
>>> d(days
89
>>> now = datetimetoday() 
```

```txt
>>> print(now)   
2012-12-21 14:54:43.094063   
>>> print(now + timedelta(minutes=10))   
2012-12-21 15:04:43.094063   
>>> 
```

当执行计算时，应该要注意的是 datetime 模块是可正确处理闰年的。示例如下：

```txt
>>> a = datetime(2012, 3, 1)  
>>> b = datetime(2012, 2, 28)  
>>> a - b  
datetime.timedelta(2)  
>>> (a - b).days  
2  
>>> c = datetime(2013, 3, 1)  
>>> d = datetime(2013, 2, 28)  
>>> (c - d).days  
1  
>>> 
```

# 3.12.3 讨论

对于大部分基本的日期和时间操控问题，datetime 模块已足够满足要求了。如果需要处理更为复杂的日期问题，比如处理时区、模糊时间范围、计算节日的日期等，可以试试 dateutil 模块。

为了举例说明，可以使用 dateutil.relativedelta()函数完成许多同 datetime 模块相似的时间计算。然而，dateutil 的一个显著特点是在处理有关月份的问题时能填补一些 datetime模块留下的空缺（可正确处理不同月份中的天数）。示例如下：

```python
>>> a = datetime(2012, 9, 23)
>>> a + timedelta(months=1)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: 'months' is an invalid keyword argument for this function
>>> from dateutil.relativedelta import relativedelta
>>> a + relativedelta(months=+1)
datetimedatetime(2012, 10, 23, 0, 0)
>>> a + relativedelta(months=+4)
datetimedatetime(2013, 1, 23, 0, 0)
>>> # Time between two dates
>>> b = datetime(2012, 12, 21) 
```

>>>d=b-a   
>>>d   
datetime.timedelta(89)   
>>>d $=$ relatedelta(b,a)   
>>>d   
relativeldelta(months $\coloneqq +2$ days $\coloneqq +28$ 1   
>>>d.months   
2   
>>>d(days   
28   
>>>

# 3.13 计算上周 5 的日期

# 3.13.1 问题

我们希望有一个通用的解决方案能找出一周中上一次出现某天时的日期。比方说上周五是几月几号？

# 3.13.2 解决方案

Python 的 datetime 模块中有一些实用函数和类可以帮助我们完成这样的计算。关于这个问题，一个优雅、通用的解决方案看起来是这样的：

from datetime import datetime, timedelta   
weekdays $=$ ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']   
def getprevious_byday(dayname, start_date $\equiv$ None): if start_date is None: start_date $=$ datetimetoday() day_num $=$ start_date.weekday() day_num_target $=$ weekdays.index(dayname) days_ago $=$ (7 + day_num - day_num_target) % 7 if days_ago $= = 0$ : days_ago $= 7$ target_date $=$ start_date - timedelta(days=days_ago) return target_date

在交互式解释器环境中使用这个函数看起来是这样的：

```txt
>>> datetimetoday() # For reference  
datetimedatetime(2012, 8, 28, 22, 4, 30, 263076) 
```

```txt
>>> getprevious_byday('Monday')  
datetimedatetime(2012, 8, 27, 22, 3, 57, 29045)  
>>> getprevious_byday('Tuesday') # Previous week, not today  
datetimedatetime(2012, 8, 21, 22, 4, 12, 629771)  
>>> getprevious_byday('Friday')  
datetimedatetime(2012, 8, 24, 22, 5, 9, 911393)  
>>> 
```

可选的 start_date 参数可以通过另一个 datetime 实例来提供。例如：

```prolog
>>> getprevious_byday('Sunday', datetime(2012, 12, 21))  
datetimedatetime(2012, 12, 16, 0, 0)  
>>> 
```

# 3.13.3 讨论

上面的解决方案将起始日期和目标日期映射到它们在一周之中的位置上（周一为第 0天，依此类推）。然后用取模运算计算上一次目标日期出现时到起始日期为止一共经过了多少天。之后，从起始日期中减去一个合适的 timedelta 实例就得到了我们所要的日期。

如果需要执行大量类似的日期计算，最好安装python-dateutil包。例如，下面这个例子是使用 dateutil 模块中的 relativedelta()函数来执行同样的计算：

```diff
>>> from datetime import datetime
>>> from dateutil(relativedelta import relativedelta
>>> from dateutil.rrule import *
>>> d = datetime-now()
>>> print(d)
2012-12-23 16:31:52.718111
>>> # Next Friday
>>> print(d + relativedelta.weekday=FR))
2012-12-28 16:31:52.718111
>>> 
>>> # Last Friday
>>> print(d + relativedelta.weekday=FR(-1)))
2012-12-21 16:31:52.718111
>>> 
```

# 3.14 找出当月的日期范围

# 3.14.1 问题

我们有一些代码需要循环迭代当月中的每个日期，我们需要一种高效的方法来计算出

日期的范围。

# 3.14.2 解决方案

对日期进行循环迭代并不需要事先构建一个包含所有日期的列表。只需计算出范围的开始和结束日期，然后在迭代时利用 datetime.timedelta 对象来递增日期就可以了。

下面这个函数可接受任意的 datetime 对象，并返回一个包含本月第一天和下个月第一天日期的元组。示例如下：

from datetime import datetime, date, timedelta   
import calendar   
def get_month_range(start_date=None): if start_date is None: start_date $=$ date today().replace(day=1) _, days_in_month $=$ calendar.monthrange(start_date.year, start_date.month) end_date $=$ start_date $^+$ timedelta(days=days_in_month)   
return (start_date, end_date)

当准备好这个函数后，对日期范围做循环迭代就变得非常简单了：

```python
>>> a_day = timedelta(days=1)  
>>> first_day, last_day = get_month_range()  
>>> while first_day < last_day:  
... print(first_day)  
... first_day += a_day  
...  
2012-08-01  
2012-08-02  
2012-08-03  
2012-08-04  
2012-08-05  
2012-08-06  
2012-08-07  
2012-08-08  
2012-08-09  
#... and so on... 
```

# 3.14.3 讨论

上面的代码首先计算出相应月份中第一天的日期。一种快速求解的方法是利用 date 或者 datetime 对象的 replace()方法，只要将属性 days 设为 1 就可以了。关于 replace()方法，一个好的方面就是它创建出的对象和我们的输入对象类型是一致的。因此，如果输入是一个 date 实例，那得到的结果也是 date 实例。同样，如果输入是 datetime 实例，得到的也是 datetime 实例。

此外，我们用calendar.monthrange()函数来找出待求解的月份中有多少天。当需要得到有关日历方面的基本信息时，calendar 模块都会非常有用。monthrange()是其中唯一的一个可返回元组的函数，元组中包含当月第一个工作日的日期①以及当月的天数（28～31）。

一旦知道了这个月中有多少天，那么结束日期就可以通过在起始日期上加上一个合适的 timedelta 对象来表示。尽管很微不足道，但本节给出的解决方案中一个重要的方面就是结束日期并不包含在范围内（因为它实际上是下个月的第一天）。这刚好应对了Python 中切片和 range操作的行为，这些操作永远不会将结束点包含在内。

要循环迭代日期范围，我们这里采用了标准的算术以及比较操作符。比如，timedelta实例可用来递增日期，而<操作符用来检查当前日期是否超过了结束日期。

最理想的方法是创建一个专门处理日期的函数，而且用法和 Python 内建的 range()一样。幸运的是，用生成器来实现这样一个函数真的是非常容易：

```python
def date_range(start, stop, step):
    while start < stop:
        yield start
        start += step 
```

下面是使用这个函数的示例：

```txt
>>> for d in date_range(matime(2012, 9, 1), datetime(2012, 10, 1), timedelta(hours=6)):  
... print(d)  
...  
2012-09-01 00:00:00  
2012-09-01 06:00:00  
2012-09-01 12:00:00  
2012-09-01 18:00:00  
2012-09-02 00:00:00  
2012-09-02 06:00:00  
...  
>>> 
```

这里要再一次说明，之所以上述实现会如此简单，一个很重要的原因就在于日期和时间可以通过标准的算术和比较操作符来进行操作。

# 3.15 将字符串转换为日期

# 3.15.1 问题

我们的应用程序接收到字符串形式的临时数据，但是我们想将这些字符串转换为

datetime 对象，以此对它们执行一些非字符串的操作。

# 3.15.2 解决方案

一般来说，Python 中的标准模块datetime 是用来处理这种问题的简单方案。示例如下：

```txt
>>> from datetime import datetime
>>> text = '2012-09-20'
>>> y = datetime.strptime(text, '%Y-%m-%d')
>>> z = datetime现已()
>>> diff = z - y
>>> diff
datetime.timedelta(3, 77824, 177393)
>>> 
```

# 3.15.3 讨论

datetime.strptime()方法支持许多格式化代码，比如 $\% \mathrm { Y }$ 代表以 4 位数字表示的年份，而$\% \mathrm { m }$ 代表以 2位数字表示的月份。同样值得一提的是，这些格式化占位符也可以反过来用在将datetime 对象转换为字符串上。如果需要以字符串形式来表示 datetime 对象并且想让输出格式变得美观时，这就能派上用场了。

比如，假设有一些代码生成了 datetime 对象，但是需要将它们格式化为美观、方便人们阅读的日期形式，以便将其放在自动生成的信件或报告的开头处：

```prolog
>>> z  
datetimedatetime(2012, 9, 23, 21, 37, 4, 177393)  
>>> nice_z = datetime strftime(z, '%A %B %d, %Y')  
>>> nice_z  
'Sunday September 23, 2012'  
>>> 
```

这里值得一提的是 strptime()的性能通常比我们想象的还要糟糕许多，这是因为该函数是用纯 Python 代码实现的，而且需要处理各种各样的系统区域设定。如果要在代码中解析大量的日期，而且事先知道日期的准确格式，那么自行实现一个解决方案可能会获得巨大的性能提升。例如，如果知道日期是以“YYYY-MM-DD”的形式表示的，可以像这样自己编写一个函数：

from datetime import datetime   
def parse_ymd(s): year_s,mon_s，day_s $=$ s.split('-'） return datetime(int(year_s)，int(mon_s)，int(day_s))

我们对此进行了测试，上面这个函数比 datetime.strptime()快了 7 倍多。如果需要处理大量涉及日期的数据时，这很可能就是需要考虑的问题了。

# 3.16 处理涉及到时区的日期问题

# 3.16.1 问题

我们有一个电话会议定在芝加哥时间 2012 年 12 月 21 日上午 9:30 举行。那么在印度班加罗尔的朋友应该在当地时间几点出现才能赶上会议？

# 3.16.2 解决方案

对于几乎任何涉及时区的问题，都应该使用 pytz 模块来解决。这个 Python 包提供了奥尔森时区数据库，这也是许多语言和操作系统所使用的时区信息标准。

pyzt模块主要用来本地化由 datetime 库创建的日期。例如，下面这段代码告诉我们如何以芝加哥时间来表示日期：

```txt
>>> from datetime import datetime
>>> from pytz import timezone
>>> d = datetime(2012, 12, 21, 9, 30, 0)
>>> print(d)
2012-12-21 09:30:00
>>> # Localize the date for Chicago
>>> central = timezone('US/Central')
>>> loc_d = central.localize(d)
>>> print(loc_d)
2012-12-21 09:30:00-06:00
>>> 
```

一旦日期经过了本地化处理，它就可以转换为其他的时区。要知道同一时间在班加罗尔是几点，可以这样做：

```python
>>> # Convert to Bangalore time
>>> bang_d = loc_d.astimezone(timezone('Asia/Kolkata'))
>>> print(bang_d)
2012-12-21 21:00:00+05:30 
```

如果打算对本地化的日期做算术计算，需要特别注意夏令时转换和其他方面的细节。比如，2013 年美国的标准夏令时于本地时间3 月13 日凌晨2 点开始（此时时间要往前拨一小时）。如果直接进行算术计算就会得到错误的结果。例如：

```txt
>>> d = datetime(2013, 3, 10, 1, 45)  
>>> loc_d = central.localize(d) 
```

```txt
>>> print(loc_d)  
2013-03-10 01:45:00-06:00  
>>> later = loc_d + timedelta(minutes=30)  
>>> print(later)  
2013-03-10 02:15:00-06:00 # WRONG! WRONG!  
>>> 
```

结果是错误的，因为上面的代码没有把本地时间中跳过的 1 小时给算上。要解决这个问题，可以使用 timezone 对象的 normalize()方法。示例如下：

```txt
>>> from datetime import timedelta
>>> later = central.normalize(loc_d + timedelta(minutes=30))
>>> print(later)
2013-03-10 03:15:00-05:00
>>> 
```

# 3.16.3 讨论

为了不让我们的头炸掉，通常用来处理本地时间的方法是将所有的日期都转换为 UTC（世界统一时间）时间，然后在所有的内部存储和处理中都使用 UTC时间。示例如下：

```txt
>>> print(loc_d)  
2013-03-10 01:45:00-06:00  
>>> utc_d = loc_d. astimezone(pytz.utc)  
>>> print(UTC_d)  
2013-03-10 07:45:00+00:00  
>>> 
```

一旦转换为UTC时间，就不用担心夏令时以及其他那些麻烦事了。因此，我们可以像之前那样对日期执行普通的算术运算。如果需要将日期以本地时间输出，只需将其转换为合适的时区即可。示例如下：

>>>later_UTC $\equiv$ UTC_d $^+$ timedelta(minutes=30)   
>>>print(later_UTC. astimezone (central))   
2013-03-10 03:15:00-05:00   
>>>

在同时区打交道时，一个常见的问题是如何知道时区的名称？例如，在本节的示例中我们怎么知道“Asia/Kolkata”才是表示印度时间的正确时区呢？要找出时区名称，可以考察一下 pyzt.country_timezones，这是一个字典，可以使用 ISO 3166 国家代码作为key 来查询。示例如下：

```txt
>>> pytz country_timezones['IN']
['Asia/Kolkata']
>>> 
```

当读到这里的时候，根据 PEP 431的描述，为了增强对时区的支持 pyzt模块可能将不再建议使用。但是，本节中提到的许多建议依然是适用的（即，建议使用 UTC时间等）。

# 迭代器和生成器

迭代是 Python 中最强有力的特性之一。从高层次看，我们可以简单地把迭代看做是一种处理序列中元素的方式。但是这里还有着更多的可能，比如创建自己的可迭代对象、在 itertools 模块中选择实用的迭代模式、构建生成器函数等。本章的目标是解决有关迭代中的一些常见问题。

# 4.1 手动访问迭代器中的元素

# 4.1.1 问题

我们需要处理某个可迭代对象中的元素，但是基于某种原因不能也不想使用 for 循环。

# 4.1.2 解决方案

要手动访问可迭代对象中的元素，可以使用 next()函数，然后自己编写代码来捕获StopIteration 异常。例如，下面这个例子采用手工方式从文件中读取文本行：

```python
with open('/etc/passwd') as f:  
    try:  
        while True:  
            line = next(f)  
            print(line, end='')  
except StopIteration:  
    pass 
```

一般来说，StopIteration 异常是用来通知我们迭代结束的。但是，如果是手动使用 next()（就像例子中那样），也可以命令它返回一个结束值，比如说 None。示例如下：

```txt
with open('/etc/passwd') as f: while True: 
```

line $=$ next(f, None)   
if line is None: break   
print(line,end $\equiv$ '）

# 4.1.3 讨论

大多数情况下，我们会用 for 语句来访问可迭代对象中的元素。但是，偶尔也会碰到需要对底层迭代机制做更精细控制的情况。因此，了解迭代时实际发生了些什么是很有帮助的。

下面的交互式例子对迭代时发生的基本过程做了解释说明：

```txt
>>> items = [1, 2, 3]  
>>> # Get the iterator  
>>> it = iter(items)  # Invokes items.__iter__(  
>>> # Run the iterator  
>>> next(it)  # Invokes it.__next__(  
1  
>>> next(it)  
2  
>>> next(it)  
3  
>>> next(it)  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
StopIteration  
>>> 
```

本章后面的示例将对迭代技术进行扩展，因此假定读者对基本的迭代协议已有所了解。请确保将这第一个例子深深刻在脑海里。

# 4.2 委托迭代

# 4.2.1 问题

我们构建了一个自定义的容器对象，其内部持有一个列表、元组或其他的可迭代对象。我们想让自己的新容器能够完成迭代操作。

# 4.2.3 解决方案

一般来说，我们所要做的就是定义一个__iter__()方法，将迭代请求委托到对象内部持有的容器上。示例如下：

```python
class Node: def __init__(self, value): Self._value = value self._children = []   
def __repr__(self): return 'Node(!r)'.format(self._value)   
def add_child(self, node): self._children.append(node)   
def __iter__(self): return iter(self._children)   
#Example   
if __name__ == '__main__': root = Node(0) child1 = Node(1) child2 = Node(2) root.add_child(child1) root.add_child(child2) for ch in root: print(ch) #Outputs Node(1), Node(2) 
```

在这个例子中，__iter__()方法只是简单地将迭代请求转发给对象内部持有的_children属性上。

# 4.2.3 讨论

Python 的迭代协议要求__iter__()返回一个特殊的迭代器对象，由该对象实现的__next__()方法来完成实际的迭代。如果要做的只是迭代另一个容器中的内容，我们不必担心底层细节是如何工作的，所要做的就是转发迭代请求。

示例中用到的 iter()函数对代码做了一定程度的简化。iter(s)通过调用 s.__iter__()来简单地返回底层的迭代器，这和 len(s)调用 s.__len__()的方式是一样的。

# 4.3 用生成器创建新的迭代模式

# 4.3.1 问题

我们想实现一个自定义的迭代模式，使其区别于常见的内建函数（即 range()、reversed()等）。

# 4.3.2 解决方案

如果想实现一种新的迭代模式，可使用生成器函数来定义。这里有一个生成器可产生某个范围内的浮点数：

def fringe(start,stop,increment): $\mathbf{x} =$ start while $x <   1$ stop: yield x $\mathrm{x + =}$ increment

要使用这个函数，可以使用 for 循环对其迭代，或者通过其他可以访问可迭代对象中元素的函数（例如 sum()、list()等）来使用。示例如下：

```txt
>>> for n in fringe(0, 4, 0.5):  
... print(n)  
...  
0  
0.5  
1.0  
1.5  
2.0  
2.5  
3.0  
3.5  
>>> list(frange(0, 1, 0.125))  
[0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875]  
>>> 
```

# 4.3.3 讨论

函数中只要出现了 yield 语句就会将其转变成一个生成器。与普通函数不同，生成器只会在响应迭代操作时才运行。这里有一个实验性的例子，我们可以试试看，以了解这样的函数的底层机制究竟是如何运转的：

```txt
>>> def countdown(n):
...
...
print('Starting to count from', n)
...
while n > 0:
...
yield n
...
n -= 1
...
print('Done!')
...
>>> # Create the generator, notice no output appears
>>> c = countdown(3)
>>> c 
```

```txt
<generator object countdown at 0x1006a0af0> 
```

```txt
>>> # Run to first yield and emit a value 
```

```txt
>>> next (c) 
```

```txt
Starting to count from 3 
```

```txt
3 
```

```txt
>>> # Run to the next yield 
```

```txt
>>> next (c) 
```

```txt
>>> # Run to next yield 
```

```txt
>>> next (c) 
```

```txt
1 
```

```txt
>>> # Run to next yield (iteration stops) 
```

```txt
>>> next (c) 
```

```txt
Done! 
```

```txt
Traceback (most recent call last): 
```

```txt
File "<stdin>", line 1, in <module> 
```

```txt
StopIteration 
```

```txt
>>> 
```

这里的核心特性是生成器函数只会在响应迭代过程中的“next”操作时才会运行。一旦生成器函数返回，迭代也就停止了。但是，通常用来处理迭代的 for语句替我们处理了这些细节，因此一般情况下不必为此操心。

# 4.4 实现迭代协议

# 4.4.1 问题

我们正在构建一个自定义的对象，希望它可以支持迭代操作，但是也希望能有一种简单的方式来实现迭代协议。

# 4.4.2 解决方案

目前来看，要在对象上实现可迭代功能，最简单的方式就是使用生成器函数。在 4.2 节中，我们用 Node 类来表示树结构。也许你想实现一个迭代器能够以深度优先的模式遍历树的节点。下面是可能的做法：

```txt
class Node: 
```

```python
def __init__(self, value): 
```

```python
self._value = value 
```

```python
self._children = []
def __repr__(self):
    return 'Node({!r})'.format(self._value)
def add_child(self, node):
    self._children.append(node)
def __iter__(self):
    return iter(self._children)
def depth_first(self):
    yield self
    for c in self:
        yield from c.depth_first()
# Example
if __name__ == '__main__':
    root = Node(0)
    child1 = Node(1)
    child2 = Node(2)
    root.add_child(child1)
    root.add_child(child2)
    child1.add_child(Node(3))
    child1.add_child(Node(4))
    child2.add_child(Node(5))
    for ch in root.depth_first():
        print(ch)
# Outputs Node(0), Node(1), Node(3), Node(4), Node(2), Node(5) 
```

在这份代码中，depth_first()的实现非常易于阅读，描述起来也很方便。它首先产生出自身，然后迭代每个子节点，利用子节点的 depth_first()方法（通过 yield from 语句）产生出其他元素。

# 4.4.3 讨论

Python 的迭代协议要求__iter__()返回一个特殊的迭代器对象，该对象必须实现__next__()方法，并使用 StopIteration 异常来通知迭代的完成。但是，实现这样的对象常常会比较繁琐。例如，下面的代码展示了 depth_first()的另一种实现，这里使用了一个相关联的迭代器类。

```python
class Node: def __init__(self, value): self._value = value 
```

```python
def __repr__(self):
    return 'Node(!r)'.format(self._value)
def add_child(self, other_node):
    self._children.append(other_node)
def __iter__(self):
    return iter(self._children)
def depth_first (self):
    return DepthFirstIterator(self)
class DepthFirstIterator(object):
    ...
Depth-first traversal
...
def __init__(self, start_node):
    self._node = start_node
    self._children_iter = None
    self._child_iter = None
def __iter__(self):
    return self
def __next__(self):
    # Return myself if just started; create an iterator for children
    if self._children_iter is None:
        self._children_iter = iter(self._node)
        return self._node
# If processing a child, return its next item
elif self._child_iter:
    try:
        nextchild = next(self._child_iter)
    return nextchild
except StopIteration:
    self._child_iter = None
    return next(self)
# Advance to the next child and start its iteration
else:
    self._child_iter = next(self._children_iter).depth_first()
return next(self) 
```

DepthFirstIterator类的工作方式和生成器版本的实现相同但是却复杂了许多，因为迭代器必须维护迭代过程中许多复杂的状态，要记住当前迭代过程进行到哪里了。坦白说，没人喜欢编写这样令人费解的代码。把迭代器以生成器的形式来定义就皆大欢喜了。

# 4.5 反向迭代

# 4.5.1 问题

我们想要反向迭代序列中的元素。

# 4.5.2 解决方案

可以使用内建的 reversed()函数实现反向迭代。示例如下：

```txt
>>> a = [1, 2, 3, 4]  
>>> for x in reversed(a):  
... print(x)  
...  
4  
3  
2  
1 
```

反向迭代只有在待处理的对象拥有可确定的大小，或者对象实现了__reversed__()特殊方法时，才能奏效。如果这两个条件都无法满足，则必须首先将这个对象转换为列表。示例如下：

```matlab
Print a file backwards  
f = open('somefile')  
for line in reversed(list(f)):  
    print(line, end='') 
```

请注意，像上述代码中那样将可迭代对象转换为列表可能会消耗大量的内存，尤其是当可迭代对象较大时更是如此。

# 4.5.3 讨论

许多程序员都没有意识到如果他们实现了__reversed__()方法，那么就可以在自定义的类上实现反向迭代。示例如下：

```python
class Countdown: def __init__(self, start): self.start = start 
```

```python
Forward iterator
def __iter__(self):
    n = self.start
    while n > 0:
        yield n
        n -= 1
# Reverse iterator
def __reversed__(self):
    n = 1
    while n <= self.start:
        yield n
        n += 1 
```

定义一个反向迭代器可使代码变得更加高效，因为这样就无需先把数据放到列表中，然后再反向去迭代列表了。

# 4.6 定义带有额外状态的生成器函数

# 4.6.1 问题

我们想定义一个生成器函数，但是它还涉及一些额外的状态，我们希望能以某种形式将这些状态暴露给用户。

# 4.6.2 解决方案

如果想让生成器将状态暴露给用户，别忘了可以轻易地将其实现为一个类，然后把生成器函数的代码放到__iter__()方法中即可。示例如下：

from collections import deque   
class linehistory: def__init__(self,lines,histlen $= 3$ ： self-lines $\equiv$ lines self_history $\equiv$ deque(maxlen $\equiv$ histlen) def__iter__(self): for lineno,line in enumerate(selfLines,1): self_history.append((lineno,line)) yield line def clear(self): self_history.clear()

要使用这个类，可以将其看做是一个普通的生成器函数。但是，由于它会创建一个类

实例，所以可以访问内部属性，比如 history 属性或者 clear()方法。示例如下：

with open('somefile.txt') as f: lines $=$ linehistory(f) for line in lines: if'python'in line: for lineno,hline in lines-history: print('{}:{}'.format(lineno,hline)， $\mathrm{end = ''}$ ）

# 4.6.3 讨论

有了生成器之后很容易掉入一个陷阱，即，试着只用函数来解决所有的问题。如果生成器函数需要以不寻常的方式同程序中其他部分交互的话（比如暴露属性，允许通过方法调用来获得控制等），那就会导致出现相当复杂的代码。如果遇到了这种情况，就像示例中做的那样，用类来定义就好了。将生成器函数定义在__iter__()方法中并没有对算法做任何改变。由于状态只是类的一部分，这一事实使得我们可以很容易将其作为属性和方法来提供给用户交互。

上面所示的方法有一个潜在的微妙之处，那就是如果打算用除了 for循环之外的技术来驱动迭代过程的话，可能需要额外调用一次 iter()。比方说：

```txt
>>> f = open('somefile.txt')
>>> lines = linehistory(f)
>>> nextlines)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: 'linehistory' object is not an iterator
>>> # Call iter() first, then start iterating
>>> it = iterlines)
>>> next(it)
'hello world\n'
>>> next(it)
'this is a test\n'
>>> 
```

# 4.7 对迭代器做切片操作

# 4.7. 1 问题

我们想对由迭代器产生的数据做切片处理，但是普通的切片操作符在这里不管用。

# 4.7.2 解决方案

要对迭代器和生成器做切片操作，itertools.islice()函数是完美的选择。示例如下：

```txt
>>> def count(n):
    ...
    while True:
        ...
        yield n
        ...
        n += 1
...
>>> c = count(0)
>>> c[10:20]
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: 'generator' object is not subscriptable
>>> # Now using islice()
>>> import itertools
>>> for x in itertools.islice(c, 10, 20):
    ...
    print(x)
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    ...
    #
...
>>> 
```

# 4.7.3 讨论

迭代器和生成器是没法执行普通的切片操作的，这是因为不知道它们的长度是多少（而且它们也没有实现索引）。islice()产生的结果是一个迭代器，它可以产生出所需要的切片元素，但这是通过访问并丢弃所有起始索引之前的元素来实现的。之后的元素会由islice 对象产生出来，直到到达结束索引为止。

需要重点强调的是 islice()会消耗掉所提供的迭代器中的数据。由于迭代器中的元素只能访问一次，没法倒回去，因此这里就需要引起我们的注意了。如果之后还需要倒回去访问前面的数据，那也许就应该先将数据转到列表中去。

# 4.8 跳过可迭代对象中的前一部分元素

# 4.8.1 问题

我们想对某个可迭代对象做迭代处理，但是对于前面几个元素并不感兴趣，只想将它们丢弃掉。

# 4.8.2 解决方案

itertools 模块中有一些函数可用来解决这个问题。第一个是 itertools.dropwhile()函数。要使用它，只要提供一个函数和一个可迭代对象即可。该函数返回的迭代器会丢弃掉序列中的前面几个元素，只要它们在所提供的函数中返回 True 即可①。这之后，序列中剩余的全部元素都会产生出来。

为了说明，假设我们正在读取一个文件，文件的开头有一系列的注释行。示例如下：

```txt
>>> with open('/etc/passwd') as f:  
... for line in f:  
... print(line, end='')  
...  
##  
# User Database  
#  
# Note that this file is consulted directly only when the system is running # in single-user mode. At other times, this information is provided by # Open Directory.  
...  
##  
nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false root:*:0:0:System Administrator:/var/root:/bin/sh  
...  
>>> 
```

如果想跳过所有的初始注释行，这里有一种方法：

```txt
>>> from itertools import dropwhile
>>> with open('/etc/passwd') as f:
...     for line in dropwhile(lambda line: line.startswith('#'), f):
...         print(line, end='')
...
nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false
root:*:0:0:System Administrator:/var/root:/bin/sh
...
>>> 
```

这个例子是根据测试函数的结果来跳过前面的元素。如果恰好知道要跳过多少个元素，那么可以使用 itertools.islice()。示例如下：

```python
>>> from itertools import islice  
>>> items = ['a', 'b', 'c', 1, 4, 10, 15] 
```

```prolog
>>> for x in islice(items, 3, None):  
... print(x)  
1  
4  
10  
15  
>>> 
```

在这个例子中，islice()的最后一个参数None用来表示想要前3个元素之外的所有元素，而不是只要前 3 个元素（即，表示切片[3:]，而不是[:3]）。

# 4.8.3 讨论

dropwhile()和 islice()都是很方便实用的函数，可以利用它们来避免写出如下所示的混乱代码：

with open('/etc/passwd') as f: #Skip over initial comments while True: line $=$ next(f, $\prime$ ) if not line.startswith('#'): break   
# Process remaining lines   
while line: #Replace with useful processing print(line,end $= ^{\prime \prime}$ 1 line $=$ nextf, None

只丢弃可迭代对象中的前一部分元素和对全部元素进行过滤也是有所区别的。例如，本节第一个示例也许可以重写为如下代码：

with open('/etc/passwd') as f: lines $=$ (line for line in f if not line.startswith('#')) for line in lines: print(line, end $\equiv$ '

这么做显然会丢弃开始部分的注释行，但这同样会丢弃整个文件中出现的所有注释行。而本节开始给出的解决方案只会丢弃元素，直到有某个元素不满足测试函数为止。那之后的所有剩余元素全部会不经过筛选而直接返回。

最后应该要强调的是，本节所展示的技术可适用于所有的可迭代对象，包括那些事先无法确定大小的对象也是如此。这包括生成器、文件以及类似的对象。

# 4.9 迭代所有可能的组合或排列

# 4.9. 1 问题

我们想对一系列元素所有可能的组合或排列进行迭代。

# 4.9.2 解决方案

为了解决这个问题，itertools 模块中提供了 3 个函数。第一个是 itertools.permutations()—它接受一个元素集合，将其中所有的元素重排列为所有可能的情况，并以元组序列的形式返回（即，将元素之间的顺序打乱成所有可能的情况）。示例如下：

```python
>>> items = ['a', 'b', 'c']
>>> from itertools import permutations
>>> for p in permutations(items):
...
...     print(p)
...
('a', 'b', 'c')
('a', 'c', 'b')
('b', 'a', 'c')
('b', 'c', 'a')
('c', 'a', 'b')
('c', 'b', 'a')
>>> 
```

如果想得到较短长度的所有全排列，可以提供一个可选的长度参数。示例如下：

```txt
>>> for p in permutations(items, 2):
... print(p)
...
('a', 'b')
('a', 'c')
('b', 'a')
('b', 'c')
('c', 'a')
('c', 'b')
>>> 
```

使用 itertools.combinations()可产生输入序列中所有元素的全部组合形式。示例如下：

```python
>>> from itertools import combinations
>>> for c in combinations(items, 3):
...     print(c)
...
('a', 'b', 'c') 
```

```python
>>> for c in combinations(items, 2):
...     print(c)
...
('a', 'b')
('a', 'c')
('b', 'c')
>>> for c in combinations(items, 1):
...     print(c)
...
('a',)
('b',)
('c',)
>>> 
```

对于 combinations()来说，元素之间的实际顺序是不予考虑的。也就是说，组合('a', 'b')和组合('b', 'a')被认为是相同的组合形式（因此只会产生出其中一种）。

当产生组合时，已经选择过的元素将从可能的候选元素中移除掉（即，如果'a'已经选过了，那么就将它从考虑范围中去掉）。itertools.combinations_with_replacement()函数解放了这一限制，允许相同的元素得到多次选择。示例如下：

```prolog
>>> for c in combinations_with_replacement(items, 3):
... print(c)
...
('a', 'a', 'a')
('a', 'a', 'b')
('a', 'a', 'c')
('a', 'b', 'b')
('a', 'b', 'c')
('a', 'c', 'c')
('b', 'b', 'b')
('b', 'b', 'c')
('b', 'c', 'c')
('c', 'c', 'c')
>>> 
```

# 4.9.3 讨论

本节只演示了一部分 itertools模块的强大功能。尽管我们肯定可以自己编写代码来产生排列和组合，但这么做大概需要我们好好思考一番。当面对看起来很复杂的迭代问题时，应该总是先去查看 itertools 模块。如果问题比较常见，那么很可能已经有现成的解决方案了。

# 4.10 以索引-值对的形式迭代序列

# 4.10.1 问题

我们想迭代一个序列，但是又想记录下序列中当前处理到的元素索引。

# 4.10.2 解决方案

内建的 enumerate()函数可以非常漂亮地解决这个问题：

```txt
>>> my_list = ['a', 'b', 'c']
>>> for idx, val in enumerate(my_list):
...
...
0 a
1 b
2 c 
```

如果要打印出规范的行号（这种情况下一般是从 1 开始而不是 0），可以传入一个 start参数作为起始索引：

```python
>>> my_list = ['a', 'b', 'c']
>>> for idx, val in enumerate(my_list, 1):
...
...
...
1 a
2 b
3 c 
```

这种情况特别适合于跟踪记录文件中的行号，当想在错误信息中加上行号时就特别有用了。示例如下：

def parse_datafilename): with open(filename, 'rt') as f: for lineno, line in enumerate(f, 1): fields $=$ line.split() try: count $=$ int.fields[1]) except ValueError as e: print('Line {}: Parse error: {}.format(lineno,e))

enumerate()可以方便地用来跟踪记录特定的值出现在列表中的偏移位置。比如，如果想将文件中的单词和它们所出现的行之间建立映射关系，则可以通过使用 enumerate()来

将每个单词映射到文件行相应的偏移位置来实现。示例如下：

word_summery $=$ defaultdict(list)   
with open('myfile.txt'，'r')asf: lines $=$ f.readlines()   
foridx,line in enumeratelines): #Createalistofwordsincurrentline words $\equiv$ [w.strip().lower()forw in line.split（)] forword in words: word_summery[word].append(idx)

处理完文件之后，如果打印 word_summary，将得到一个字典（准确地说是 defaultdict），而且每个单词都是字典的键。每个单词键所对应的值就是由行号组成的列表，表示这个单词曾出现过的所有行。如果单词在一行之中出现过2 次，那么这个行号就会记录 2次，这使得我们可以识别出文本中各种简单的韵律。

# 4.10.3 讨论

对于那些可能想自己保存一个计数器的场景，enumerate()函数是个不错的替代选择，而且会更加便捷。我们可以像这样编写代码：

lineno $= 1$ for line in f: #Process line   
lineno $+ = 1$

但是，通常更加优雅的做法是使用 enumerate()：

```python
for lineno, line in enumerate(f):
    # Process line
... 
```

enumerate()的返回值是一个 enumerate 对象实例，它是一个迭代器，可返回连续的元组。元组由一个索引值和对传入的序列调用 next()而得到的值组成。

尽管只是个很小的问题，这里还是值得提一下。有时候，当在元组序列上应用 enumerate()时，如果元组本身也被分解展开的话就会出错。要正确处理元组序列，必须像这样编写代码：

data $=$ [(1,2)，(3，4)，(5，6)，(7，8)] #Correct! for n，(x，y）in enumerate(data):

```txt
...   
# Error!   
for n, x, y in enumerate(data): 
```

# 4.11 同时迭代多个序列

# 4.11.1 问题

我们想要迭代的元素包含在多个序列中，我们想同时对它们进行迭代。

# 4.11.2 解决方案

可以使用 zip()函数来同时迭代多个序列。示例如下：

```txt
>>> xpts = [1, 5, 4, 2, 10, 7]  
>>> ypts = [101, 78, 37, 15, 62, 99]  
>>> for x, y in zip(xpts, ypts):  
... print(x, y)  
...  
1 101  
5 78  
4 37  
2 15  
10 62  
7 99  
>>> 
```

zip(a, b)的工作原理是创建出一个迭代器，该迭代器可产生出元组(x, y)，这里的 x 取自序列 a，而 y 取自序列 $\mathbf { b }$ 。当其中某个输入序列中没有元素可以继续迭代时，整个迭代过程结束。因此，整个迭代的长度和其中最短的输入序列长度相同。示例如下：

```txt
>>> a = [1, 2, 3]  
>>> b = ['w', 'x', 'y', 'z']  
>>> for i in zip(a, b):  
... print(i)  
...  
(1, 'w')  
(2, 'x')  
(3, 'y')  
>>> 
```

如果这种行为不是所需要的，可以使用 itertools.zip_longest()来替代。示例如下：

```python
>>> from itertools import zip_longest
>>> for i in zip_longest(a,b):
...     print(i)
...
(1,'w')
(2,'x')
(3,'y')
(None,'z')
>>> for i in zip_longest(a,b, fillvalue=0):
...     print(i)
...
(1,'w')
(2,'x')
(3,'y')
(0,'z')
>>> 
```

# 4.11.3 讨论

zip()通常用在需要将不同的数据配对在一起时。例如，假设有一列标题和一列对应的值，示例如下：

```txt
headers = ['name', 'shares', 'price']
values = ['ACME', 100, 490.1] 
```

使用 zip()，可以将这些值配对在一起来构建一个字典，就像这样：

```txt
s = dict(zip(headers, values)) 
```

此外，如果试着产生输出的话，可以编写这样的代码：

```python
for name, val in zip(headers, values):
    print(name, ':', val) 
```

尽管不常见，但是 zip()可以接受多于 2 个序列作为输入。在这种情况下，得到的结果中元组里的元素数量和输入序列的数量相同。示例如下：

```txt
>>> a = [1, 2, 3]  
>>> b = [10, 11, 12]  
>>> c = ['x', 'y', 'z']  
>>> for i in zip(a, b, c):  
... print(i)  
...  
(1, 10, 'x')  
(2, 11, 'y')  
(3, 12, 'z')  
>>> 
```

最后需要重点强调的是，zip()创建出的结果只是一个迭代器。如果需要将配对的数据保存为列表，那么请使用 list()函数。示例如下：

```txt
>>> zip(a, b)
<zip object at 0x1007001b8>
>>> list(zip(a, b))
[(1, 10), (2, 11), (3, 12)]
>>> 
```

# 4.12 在不同的容器中进行迭代

# 4.12.1 问题

我们需要对许多对象执行相同的操作，但是这些对象包含在不同的容器内，而我们希望可以避免写出嵌套的循环处理，保持代码的可读性。

# 4.12.2 解决方案

itertools.chain()方法可以用来简化这个任务。它接受一系列可迭代对象作为输入并返回一个迭代器，这个迭代器能够有效地掩盖一个事实—你实际上是在对多个容器进行迭代。为了说明清楚，请考虑下面这个例子：

```txt
>>> from itertools import chain
>>> a = [1, 2, 3, 4]
>>> b = ['x', 'y', 'z']
>>> for x in chain(a, b):
...     print(x)
...
1
2
3
4
x
y
z
>>> 
```

在程序中，chain()常见的用途是想一次性对所有的元素执行某项特定的操作，但是这些元素分散在不同的集合中。比如：

```txt
Various working sets of items  
active_items = set()  
inactive_items = set()  
# Iterate over all items 
```

```python
for item in chain(active_items, inactive_items):
    # Process item
    ... 
```

采用 chain()的解决方案比下面这种写两个单独的循环要优雅得多：

```txt
for item in active_items: # Process item   
for item in inactive_items: # Process item 
```

# 4.12.3 讨论

itertools.chain()可接受一个或多个可迭代对象作为参数，然后它会创建一个迭代器，该迭代器可连续访问并返回你提供的每个可迭代对象中的元素。尽管区别很小，但是chain()比首先将各个序列合并在一起然后再迭代要更加高效。示例如下：

Inefficient   
for $\mathbf{x}$ in a $^+$ b:   
# Better   
for x in chain(a, b):

第一种情况中， ${ \mathrm { ~ a ~ } } + { \mathrm { ~ b ~ } }$ 操作产生了一个全新的序列，此外还要求 a和b 是同一种类型。chain()并不会做这样的操作，因此如果输入序列很大的话，在内存的使用上 chain()就会高效得多，而且当可迭代对象之间不是同一种类型时也可以轻松适用。

# 4.13 创建处理数据的管道

# 4.13.1 问题

我们想以流水线式的形式对数据进行迭代处理（类似 UNIX 下的管道）。比方说我们有海量的数据需要处理，但是没法完全将数据加载到内存中去。

# 4.13.2 解决方案

生成器函数是一种实现管道机制的好方法。为了说明，假设我们有一个超大的目录，其中都是想要处理的日志文件：

```txt
foo/   
access-log-012007.gz   
access-log-022007.gz   
access-log-032007.gz   
...   
access-log-012008   
bar/   
access-log-092007.bz2   
...   
access-log-022008 
```

假设每个文件都包含如下形式的数据行：

```shell
124.115.6.12 -- [10/Jul/2012:00:18:50 -0500] "GET /robots.txt ..." 200 71
210.212.209.67 -- [10/Jul/2012:00:18:51 -0500] "GET /ply/ ..." 200 11875
210.212.209.67 -- [10/Jul/2012:00:18:51 -0500] "GET /favicon.ico ..." 404 369
61.135.216.105 -- [10/Jul/2012:00:20:04 -0500] "GET /blog/atom.xml ..." 304 -
...
... 
```

要处理这些文件，可以定义一系列小型的生成器函数，每个函数执行特定的独立任务。示例如下：

```python
import os   
import fnmatch   
import gzip   
import bz2   
import re   
def gen_find(filepat, top): 'Find all filenames in a directory tree that match a shell wildcard pattern 'for path, drlist, filelist in os.walk(top): for name in fnmatch.filter(filelist, filepat): yield os.path.join(path,name)   
def gen_openers(filenames): Open a sequence of filenames one at a time producing a file object. The file is closed immediately when proceeding to the next iteration.. for filename in filenames: if filename.endsWith(.gz'): f = gzip.open(filename, 'rt') elif filename.endsWith(.bz2'): f = bz2.open(filename, 'rt') 
```

else: f = openfilename, 'rt') yield f f.close()   
def gen_concatenate(iteratorators): Chain a sequence of iterators together into a single sequence. for it in iterators: yield from it   
def gen_grep(pattern, lines): Look for a regex pattern in a sequence of lines pat $=$ re.compile(pattern) for line in lines: if pat.search(line): yield line

现在可以简单地将这些函数堆叠起来形成一个数据处理的管道。例如，要找出所有包含关键字 python 的日志行，只需要这么做：

```python
lognames = gen_find('access-log*', 'www')
files = gen_openers(lognames)
lines = gen_concatenate(files)
pylines = gen_grep(['(i)python', lines)
for line in pylines:
    print(line) 
```

如果稍后想对管道进行扩展，甚至可以在生成器表达式中填充数据。比如，下面这个版本可以找出传送的字节数并统计出总字节量：

```python
lognames = gen_find('access-log*', 'www')
files = gen_openers(lognames)
lines = gen_concatenate(files)
pylines = gen_grep('/?i')python', lines)
bytecolumn = (line.rsplit(None,1)[1] for line in pylines)
bytes = (int(x) for x in bytecolumn if x != '-')
print('Total', sum(bytes)) 
```

# 4.13.3 讨论

将数据以管道的形式进行处理可以很好地适用于其他广泛的问题，包括解析、读取实时的数据源、定期轮询等。

要理解这些代码，很重要的一点是领会 yield 语句的含义。在这里 yield 语句表现为数据的生产者，而 for 循环表现为数据的消费者。当生成器被串联起来时，在迭代中每个yield 语句都为管道中下个阶段的处理过程产生出数据。在最后那个例子中，sum()函数实际上在驱动这整个程序，每一次都从生成器管道中取出一份数据。

这种方法的一个优点在于每个生成器函数都比较短小而且功能独立。正因为如此，编写和维护都很容易。在许多情况下，由于它们是如此的通用，因此可以在其他上下文中得到重用。最终，将这些组件粘合在一起的代码读起来就像一份食谱一样简单，因此也更容易理解。

这种方法在内存使用的高效性上也同样值得夸耀。如果目录中有着海量的文件要处理，上述展示的代码仍然可以正常工作。实际上，由于处理过程的迭代特性，这里只会用到非常少的内存。

关于gen_concatenate()函数还有一些非常微妙的地方需要说明。这个函数的目的是将输入序列连接为一个长序列行。itertools.chain()函数可以实现类似的功能，但是这需要将所有的可迭代对象指定为它的参数才行。在这个特定的例子中，这么做将涉及一行这样的代码：lines $=$ itertools.chain(*files)，这会导致 gen_opener()生成器被完全耗尽。由于这个生成器产生的是打开的文件序列，它们在下一个迭代步骤中会被立刻关闭，因此这里不能用 chain()。我们展示的解决方案避免了这个问题。

此外，gen_concatenate()函数中也出现了实现委托给一个子生成器的 yield from 语句。语句 yield from it 简单地使 gen_concatenate()函数发射出所有由生成器 it 产生的值。这一点将在 4.14 节中做进一步的描述。

最后但同样重要的是，应该指出管道方法并不会总是适用于每一个数据处理问题。有时候我们需要马上处理所有的数据。但是，就算是这种情况，使用生成器管道可以在逻辑上将问题分解成一种工作流程。

David Beazley 在他的“针对系统程序员之生成器技巧”教程报告（http://www.dabeaz.com/generators）中已经对这些技术做了广泛的探讨。可以参阅他的教程以获得更多的示例。

# 4.14 扁平化处理嵌套型的序列

# 4.14.1 问题

我们有一个嵌套型的序列，想将它扁平化处理为一列单独的值。

# 4.14.2 解决方案

这个问题可以很容易地通过写一个带有 yield from语句的递归生成器函数来解决。示例如下：

from collections import iterable   
def flatten(items, ignore_types=(str, bytes)): for $\mathbf{x}$ in items: if isinstance(x, iterable) and not isinstance(x, ignore_types): yield from flatten(x) else: yield x   
items $= [1,2,[3,4,[5,6],7],8]$ #Produces 1 2 3 4 5 6 7 8   
for x in flatten(items): print(x)

在上述代码中，isinstance(x, Iterable)简单地检查是否有某个元素是可迭代的。如果确实有，那么就用yield from将这个可迭代对象作为一种子例程进行递归，将它所有的值都产生出来。最后得到的结果就是一个没有嵌套的单值序列。

代码中额外的参数 ignore_types 和对 not isinstance(x, ignore_types)的检查是为了避免将字符串和字节串解释为可迭代对象，进而将它们展开为单独的一个个字符。这使得嵌套型的字符串列表能够以大多数人所期望的方式工作。示例如下：

```txt
>>> items = ['Dave', 'Paula', ['Thomas', 'Lewis']]  
>>> for x in flatten(items):  
... print(x)  
...  
Dave  
Paula  
Thomas  
Lewis  
>>> 
```

# 4.14.3 讨论

如果想编写生成器用来把其他的生成器当做子例程调用，yield from是个不错的快捷方式。如果不这么用，就需要编写有额外 for 循环的代码，比如这样：

```python
def flatten(items, ignore_types=(str, bytes)):  
    for x in items:  
        if isinstance(x, iterable) and not isinstance(x, ignore_types):  
            for i in flatten(x):  
                yield i  
            else:  
                yield x 
```

尽管只是个小小的改变，但是使用 yield from语句感觉更好，也使得代码变得更加清晰。前面提到，对字符串和字节串的额外检查是为了避免将这些类型的对象展开为单独的字符。如果还有其他类型是不想要展开的，可以为 ignore_types 参数提供不同的值来确定。

最后应该要提到的是，yield from 在涉及协程（coroutine）和基于生成器的并发型高级程序中有着更加重要的作用。请参见 12.12 节中的另一个示例。

# 4.15 合并多个有序序列，再对整个有序序列进行迭代

# 4.15.1 问题

我们有一组有序序列，想对它们合并在一起之后的有序序列进行迭代。

# 4.15.2 解决方案

对于这个问题，heapq.merge()函数正是我们所需要的。示例如下：

```txt
>>> import heapq
>>> a = [1, 4, 7, 10]
>>> b = [2, 5, 6, 11]
>>> for c in heapq.merge(a, b):
...
...
...
1
2
4
5
6
7
10
11 
```

# 4.15.3 讨论

heapq.merge 的迭代性质意味着它对所有提供的序列都不会做一次性读取。这意味着可以利用它处理非常长的序列，而开销却非常小。例如，下面这个例子告诉我们如何合并两个有序的文件：

```python
import heapq   
with open('sorted_file_1'，'rt') as file1，\ open('sorted_file_2') 'rt' as file2， 
```

```python
open('merged_file', 'wt') as outf:  
for line in heapq.merge(file1, file2):  
    outf.write(line) 
```

需要重点强调的是，heapq.merge()要求所有的输入序列都是有序的。特别是，它不会首先将所有的数据读取到堆中，或者预先做任何的排序操作。它也不会对输入做任何验证，以检查它们是否满足有序的要求。相反，它只是简单地检查每个输入序列中的第一个元素，将最小的那个发送出去。然后再从之前选择的序列中读取一个新的元素，再重复执行这个步骤，直到所有的输入序列都耗尽为止。

# 4.16 用迭代器取代 while 循环

# 4.16.1 问题

我们的代码采用 while 循环来迭代处理数据，因为这其中涉及调用某个函数或有某种不常见的测试条件，而这些东西没法归类为常见的迭代模式。

# 4.16.2 解决方案

在涉及 I/O 处理的程序中，编写这样的代码是很常见的：

CHUNKSIZE $= 8192$

```python
def reader(s):
    while True:
        data = s.recv(CHUNKSIZE)
        if data == b': 
            break
        process_data(data) 
```

这样的代码常常可以用 iter()来替换，比如：

```python
def reader(s):
    for chunk in iter(lambda: s.recv(CHUNKSIZE), b''): process_data(data) 
```

如果对这样的代码能否正常工作持有怀疑态度，可以用一个有关文件处理的类似例子试验一下：

```python
>>> import sys
>>> f = open('/etc/passwd')
>>> for chunk in iter(lambda: f.read(10), True):
    ... n = sys_STDout.write(chunk) 
```

```txt
...   
nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false   
root:*:0:0:System Administrator:/var/root:/bin/sh   
daemon:*:1:1:System Services:/var/root:/usr/bin/false   
_uucp:*:4:4:Unix to Unix Copy Protocol:/var/spool/uucp:/usr/sbin/uucico   
>>> 
```

# 4.16.3 讨论

关于内建函数 iter()，一个少有人知的特性是它可以选择性接受一个无参的可调用对象以及一个哨兵（结束）值作为输入。当以这种方式使用时，iter()会创建一个迭代器，然后重复调用用户提供的可调用对象，直到它返回哨兵值为止。

这种特定的方式对于需要重复调用函数的情况，比如这些涉及I/O 的问题，有很好的效果。比如，如果想从 socket 或文件中按块读取数据，通常会重复调用 read()或者 recv()，然后紧跟着检测是否到达文件结尾。而我们给出的解决方案简单地将这两个功能合并为一个单独的 iter()调用。解决方案中对 lambda 的使用是为了创建一个不带参数的可调用对象，但是还是可以对 recv()或 read()提供所需要的参数。

# 文件和 I/O

任何程序都需要处理输入和输出。本章介绍了处理各种不同类型文件时的惯用方法，包括文本和二进制文件的处理、文件编码以及其他一些相关的内容。用来处理文件名和目录相关的技术也有涵盖。

# 5.1 读写文本数据

# 5.1.1 问题

我们需要对文本数据进行读写操作，但这个过程有可能针对不同的文本编码进行，比如 ASCII、UTF-8 或 UTF-16 编码。

# 5.1.2 解决方案

可以使用 open()函数配合 rt 模式来读取文本文件的内容。示例如下：

```python
Read the entire file as a single string with open('somefile.txt', 'rt') as f: data = f.read() # Iterate over the lines of the file with open('somefile.txt', 'rt') as f: for line in f: # process line 
```

类似地，要对文本文件执行写入操作，可以使用 open()函数的 wt 模式来完成。如果待操作的文件已存在，那么这会清除并覆盖其原先的内容。示例如下：

Write chunks of text data   
with open('somefile.txt'，'wt') as f: f.write(text1) f.write(text2) ... #Redirected print statement   
with open('somefile.txt'，'wt') as f: print(line1,file $\equiv$ f) print(line2,file $\equiv$ f)

如果要在已存在文件的结尾处追加内容，可以使用 open()函数的 at 模式。

默认情况下，文件的读取和写入采用的都是系统默认的文本编码方式，这可以通过sys.getdefaultencoding()来查询。在大多数机器上，这项设定都被设置为 utf-8。如果我们知道正在读取或写入的文本采用的是另外一种编码方式，那么可以为 open()函数提供一个可选的编码参数。示例如下：

```python
with open('somefile.txt', 'rt', encoding='latin-1') as f: 
```

Python 可以识别出几百种可能的文本编码。但是，一些常见的编码方式不外乎是 ascii、latin-1、utf-8 以及 utf-16。如果要同 Web 应用程序打交道，采用 utf-8 编码通常是比较保险的。ascii编码对应于范围 $\mathrm { U } { + } 0 0 0 0$ 到 $\mathrm { U } { + } 0 0 7 \mathrm { F }$ 中的7 比特字符。latin-1 编码则是字节 $0 \sim 2 5 5$ 对 Unicode 字符 $\mathrm { U } { + } 0 0 0 0$ 到 $\mathrm { U } { + } 0 0 \mathrm { F F }$ 的直接映射。关于 latin-1 编码，值得注意的一点是，当读取到未知编码的文本时是不会产生解码错误的。以 latin-1 方式读取文件可能不会产生完全正确的解码文本，但是要从中提取出有用的数据仍然是足够了。此外，如果稍后将数据重新写入到文件中，那么原始的输入数据将得到保留。

# 5.1.3 讨论

一般来说，读写文本文件都是非常简单直接的。但是，这里还是有几个微妙的细节需要引起注意。首先，我们在示例中采用了 with 语句，这会为使用的文件创建一个上下文环境（context）。当程序的控制流程离开 with 语句块后，文件将自动关闭。我们并不是一定要使用 with 语句，但是如果不用的话请确保要记得手动关闭文件：

```python
f = open('somefile.txt', 'rt')
data = f.read()
f.close() 
```

另一个细微的问题是关于换行符的识别，在 UNIX 和 Windows 上它们是不同的（即，$\mathrm { \backslash n }$ 和 $\mathrm { \backslash r } \mathrm { \backslash n }$ 之争）。默认情况下，Python 工作在“通用型换行符”模式下。在该模式中，所有常见的换行格式都能识别出来。在读取时会将换行符转换成一个单独的\n 字符。

同样地，在输出时换行符\n会被转换为当前系统默认的换行符。如果你不想要这种“翻译”行为，可以给 open()函数提供一个 newline $= "$ '的参数，示例如下：

```txt
Read with disabled newline translation with open('somefile.txt', 'rt', newline='') as f: 
```

为了说明其中的区别，我们会在下面的例子中看到，如果在 UNIX 机器上读取由Windows 系统编码的包含有原始数据 hello world!\r\n 的文本时，会出现什么结果：

```txt
>>> # Newline translation enabled (the default)  
>>> f = open('hello.txt', 'rt')  
>>> f.read()  
'hello world!\n'  
>>> # Newline translation disabled  
>>> g = open('hello.txt', 'rt', newline=True)  
>>> g.read()  
'hello world!\r\n'  
>>> 
```

最后一个问题是关于文本文件中可能出现的编码错误。当我们读取或写入文本文件时，可能会遇到编码或解码错误。例如：

```python
>>> f = open('sample.txt', 'rt', encoding='ascii')
>>> f.read()
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "/usr/local/lib/python3.3/encodings/ascii.py", line 26, in decode
    return codecs ASCii Decode(input, self mistakes) [0]
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position
12: ordinal not in range(128)
>>> 
```

如果遇到这个错误，这通常表示没有以正确的编码方式来读取文件。应该仔细阅读要读取的文本的相关规范，并检查自己的操作是否正确（例如不要用 latin-1 编码方式读取，换成 utf-8 或者任何所需的编码方式）。如果还是有可能出现编码错误，则可以为open()函数提供一个可选的 errors 参数来处理错误。下面是几个常见的错误处理方案的例子：

```python
>>> # Replace bad chars with Unicode U+ffdd replacement char
>>> f = open('sample.txt', 'rt', encoding='ascii', errors='replace')
>>> f.read()
'Spicy Jalape?o!"
>>> # Ignore bad chars entirely
>>> g = open('sample.txt', 'rt', encoding='ascii', errors='ignore') 
```

```txt
>>>g.read() 'Spicy Jalapeo!' >>> 
```

如果常常在摆弄 open()函数的 encoding 和 errors 参数，并为此做了大量的技巧性操作（hacks），那就适得其反了，因为生活本不应该如此艰难。关于文本，第一条守则就是只需要确保总是采用正确的文本编码形式即可。当对此抱有疑问时，请使用默认的编码设定（通常是 utf-8）。

# 5.2 将输出重定向到文件中

# 5.2.1 问题

我们想将 print()函数的输出重定向到一个文件中。

# 5.2.2 解决方案

对于这个问题，只需要像这样为 print()函数加上 file 关键字参数即可：

```python
with open('somefile.txt', 'rt') as f:  
    print('Hello World!', file=f) 
```

# 5.2.3 讨论

对于这个主题确实没多少东西可说。只是要确保文件是以文本模式打开的。如果文件是以二进制模式打开的话，打印就会失败。

# 5.3 以不同的分隔符或行结尾符完成打印

# 5.3.1 问题

我们想通过 print()函数输出数据，但是同时也希望修改分隔符或者行结尾符。

# 5.3.2 解决方案

可以在 print()函数中使用 sep 和 end 关键字参数来根据需要修改输出。示例如下：

```prolog
>>> print('ACME', 50, 91.5)  
ACME 50 91.5  
>>> print('ACME', 50, 91.5, sep='',')  
ACME, 50, 91.5  
>>> print('ACME', 50, 91.5, sep='', end='!!\n')  
ACME, 50, 91.5!!  
>>> 
```

使用 end 参数也是在输出中禁止打印出换行符的方式。示例如下：

```python
>>> for i in range(5):
...     print(i)
...
0
1
2
3
4
>>> for i in range(5):
...     print(i, end='')
...
0 1 2 3 4 >>> 
```

# 5.3.3 讨论

除了空格之外，当还需要用其他字符来分隔文本时，通常在 print()函数中通过 sep 关键字参数指定一个不同的分隔符就是最简单的方法了。有时候我们会看到有的程序员会利用 str.join()来实现同样的效果。例如：

```txt
>>> print('', '.join('ACME','50','91.5'))  
ACME, 50, 91.5  
>>> 
```

str.join()的问题就在于它只能处理字符串。这意味着我们常常得做些转换才能让其正常工作。比如说：

```txt
>>> row = ('ACME', 50, 91.5)  
>>> print('', '.join(row))  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
TypeError: sequence item 1: expected str instance, int found  
>>> print('', '.join(str(x) for x in row))  
ACME, 50, 91.5  
>>> 
```

其实不必这么大费周折，只要用 print()函数就可以办到了：

```txt
>>> print(*row, sep='', 'ACME,50,91.5  
>>> 
```

# 5.4 读写二进制数据

# 5.4.1 问题

我们需要读写二进制数据，比如图像、声音文件等。

# 5.4.2 解决方案

使用 open()函数的 rb 或者 wb 模式就可以实现对二进制数据的读或写。示例如下：

```python
Read the entire file as a single byte string with open('somefile.bin', 'rb') as f: data = f.read() # Write binary data to a file with open('somefile.bin', 'wb') as f: f.write(b'Hello World') 
```

当读取二进制数据时，很重要的一点是所有的数据将以字节串（byte string）的形式返回，而不是文本字符串。同样地，当写入二进制数据时，数据必须是以对象的形式来提供，而且该对象可以将数据以字节形式暴露出来（即，字节串、bytearray 对象等）。

# 5.4.3 讨论

当读取二进制数据时，由于字节串和文本字符串之间存在微妙的语义差异，这可能会造成一些潜在的问题。特别要注意的是，在做索引和迭代操作时，字节串会返回代表该字节的整数值而不是字符串。示例如下：

```txt
>>> # Text string  
>>> t = 'Hello World'  
>>> t[0]  
'H'  
>>> for c in t:  
... print(c)  
...  
H  
e  
1  
1  
o  
...  
>>> # Byte string  
>>> b = b'Hello World'  
>>> b[0]  
72  
>>> for c in b:  
... print(c)  
...  
72  
101 
```

```txt
108   
108   
111   
...   
>>> 
```

如果需要在二进制文件中读取或写入文本内容，请确保要进行编码或解码操作。示例如下：

```javascript
with open('somefile.bin', 'rb') as f: data = f.read(16) text = datadecode('utf-8')   
with open('somefile.bin', 'wb') as f: text = 'Hello World' f.write(text.encode('utf-8')) 
```

关于二进制 I/O，一个鲜为人知的行为是，像数组和 C 结构体这样的对象可以直接用来进行写操作，而不必先将其转换为 byte 对象。示例如下：

```python
import array  
nums = array.array('i', [1, 2, 3, 4])  
with open('data.bin','wb') as f:  
    f.write(nums) 
```

这种行为可适用于任何实现了所谓的“缓冲区接口（buffer interface）”的对象。该接口直接将对象底层的内存缓冲区暴露给可以在其上进行的操作。写入二进制数据就是这样一种操作。

有许多对象还支持直接将二进制数据读入到它们底层的内存中，只要使用文件对象的readinto()方法就可以了。示例如下：

```txt
>>> import array
>>> a = array.array('i', [0, 0, 0, 0, 0, 0, 0, 0])
>>> with open('data.bin', 'rb') as f:
...
...
16
>>> a
array('i', [1, 2, 3, 4, 0, 0, 0, 0])
>>> 
```

但是，使用这项技术时需要特别小心，因为这常常是与平台特性相关的，而且可能依赖于字（word）的大小和字节序（即大端和小端）等属性。请参见5.9 节中的另一个例子，在该例中我们将二进制数据读入到一个可变缓冲区（mutable buffer）中。

# 5.5 对已不存在的文件执行写入操作

# 5.5.1 问题

我们想将数据写入到一个文件中，但只在该文件已不在文件系统中时才这么做。

# 5.5.2 解决方案

这个问题可以通过使用 open()函数中鲜为人知的 x 模式替代常见的 w 模式来解决。示例如下：

```txt
>>> with open('somefile', 'wt') as f:
...     f.write('Hello\n')
...
>>> with open('somefile', 'xt') as f:
...     f.write('Hello\n')
...
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
FileExistsError: [Errno 17] File exists: 'somefile'
>>> 
```

如果文件是二进制模式的，那么用 xb 模式代替 xt 即可。

# 5.5.3 讨论

本节中的示例以一种非常优雅的方式解决了一个常会在写文件时出现的问题（即，意外地覆盖了某个已存在的文件）。另一种解决方案是首先像这样检查文件是否已存在：

```txt
>>> import os
>>> if not os.path.exists('somefile():
... with open('somefile', 'wt') as f:
...     f.write('Hello\n')
...
else:
...     print('File already exists!')
...
File already exists!
>>> 
```

很明显，使用 x 模式更加简单直接。需要注意的是，x 模式是 Python 3 中对 open()函数的扩展。在早期的 Python 版本或者在 Python 的实现中用到的底层 C 函数库里都不存在这样的模式。

# 5.6 在字符串上执行 I/O 操作

# 5.6.1 问题

我们想将一段文本或二进制字符串写入类似于文件的对象上。

# 5.6.2 解决方案

使用 io.StringIO()和 io.BytesIO()类来创建类似于文件的对象，这些对象可操作字符串数据。示例如下：

```txt
>>> s = io.StringIO()
>>> s.write('Hello World\n')
12
>>> print('This is a test', file=s)
15
>>> # Get all of the data written so far
>>> s.getvalue()
'Hello World\nThis is a test\n'
>>> 
>>> # Wrap a file interface around an existing string
>>> s = io.StringIO('Hello\nWorld\n')
>>> s.read(4)
'Hell'
>>> s.read()
'o\nWorld\n'
>>> 
```

io.StringIO 类只能用于对文本的处理。如果要操作二进制数据，请使用 io.BytesIO。示例如下：

```txt
>>>s = ioBytesIO()   
>>>s.write(b'binary data')   
>>>s.getvalue()   
b'binary data'   
>>> 
```

# 5.6.3 讨论

当出于某种原因需要模拟出一个普通文件时，这种情况下 StringIO 和 BytesIO 类是最为适用的。例如，在单元测试中，可能会使用 StringIO 来创建一个文件型的对象，对象中包含了测试用的数据。之后我们可将这个对象发送给一个可以接受普通文件的函数。

请注意，StringIO和 BytesIO 实例是没有真正的文件描述符来对应的。因此，它们没法工作在需要一个真正的系统级文件例如文件、管道或套接字的代码环境中。

# 5.7 读写压缩的数据文件

# 5.7. 1 问题

我们需要读写以 gzip 或 bz2 格式压缩过的文件中的数据。

# 5.7.2 解决方案

gzip 和 bz2 模块使得同这类压缩型文件打交道变得非常简单。这两个模块都提供了 open()的其他实现，可用于处理压缩文件。例如，要将压缩文件以文本形式读取，可以这样处理：

#gzip compression   
import gzip   
with gzip.open('somefile.gz'，'rt')as f: text $=$ f.read()   
#bz2 compression   
import bz2   
with bz2.open('somefile.bz2'，'rt') as f: text $=$ f.read()

与之相似，要写入压缩数据，可以这样处理：

```python
gzip compression   
import gzip   
with gzip.open('somefile.gz'，'wt')as f: f.write(text)   
# bz2 compression   
import bz2   
with bz2.open('somefile.bz2'，'wt') as f: f.write(text) 
```

如示例代码所示，以上所有的 I/O 操作都会采用文本形式并执行 Unicode 编码/解码操作。如果想处理二进制数据，请使用 rb 或 wb 模式。

# 5.7.3 讨论

大部分情况下读写压缩数据都是简单而直接的。但是请注意，选择正确的文件模式是至关重要的。如果没有指定模式，那么默认的模式是二进制，这会使得期望接受文本的程序崩溃。gzip.open()和 bz2.open()所接受的参数与内建的 open()函数一样，也支持encoding、errors、newline 等关键字参数。

当写入压缩数据时，压缩级别可以通过 compresslevel 关键字参数来指定，这是可选的。示例如下：

```python
with gzip.open('somefile.gz', 'wt', compresslevel=5) as f:  
    f.write(text) 
```

默认级别是 9，代表着最高的压缩等级。低等级的压缩可带来更好的性能表现，但压缩比就没有那么大。

最后，gzip.open()和 bz2.open()有一个较少提到的特性，那就是它们能够对已经以二进制模式打开的文件进行叠加操作。示例如下：

```python
import gzip  
f = open('somefile.gz', 'rb')  
with gzip.open(f, 'rt') as g:  
    text = g.read() 
```

这种行为使得 gzip 和 bz2 模块可以同各种类型的类文件对象比如套接字、管道和内存文件一起工作。

# 5.8 对固定大小的记录进行迭代

# 5.8.1 问题

与其按行来迭代文件，我们想对一系列固定大小的记录或数据块进行迭代。

# 5.8.2 解决方案

可以利用 iter()和 functools.partial()来完成这个巧妙的技巧，示例如下：

```python
from functools import partial  
RECORD_SIZE = 32  
with open('somefile.data', 'rb') as f:  
    records = iter(partial(f.read, RECORD_SIZE), b')  
    for r in records: 
```

示例中的 records 对象是可迭代的，它会产生出固定大小的数据块直到到达文件结尾。但是请注意，如果文件大小不是记录大小的整数倍的话，那么最后产生出的那个数据块可能比所期望的字节数要少。

# 5.8.3 讨论

关于 iter()函数，一个少有人知的特性是，如果传递一个可调用对象及一个哨兵值给它，那么它可以创建出一个迭代器。得到的迭代器会重复调用用户提供的可迭代对象，直到返回的值为哨兵值为止，此时迭代过程停止。

在我们给出的解决方案中，functools.partial 用来创建可调用对象，每次调用它时都从文

件中读取固定的字节数。b''在这里用作哨兵值，当读取到文件结尾时就会返回这个值，此时迭代过程结束。

最后但也很重要的是，解决方案中展示的文件是以二进制模式打开的。对于读取固定大小的记录，这恐怕是最为常见的情况了。如果要针对文本文件，那么按行读取（默认的迭代行为）更为普遍一些。

# 5.9 将二进制数据读取到可变缓冲区中

# 5.9. 1 问题

我们想将二进制数据直接读取到一个可变缓冲区中，中间不经过任何拷贝环节。也许我们想原地修改数据再将它写回到文件中去。

# 5.9.2 解决方案

要将数据读取到可变数组中，使用文件对象的 readinto()方法即可。示例如下：

import os.path   
def readInto_buffer(filename): buf $=$ bytearray(os.path.getsize(filename)) with open(filename, 'rb') as f: f.readinto(buf) return buf

下面来演示这个函数的用法：

```txt
>>> # Write a sample file  
>>> with open('sample.bin', 'wb') as f:  
... f.write(b'Hello World')  
>>> buf = readInto_buffer('sample.bin')  
>>> buf  
bytearray(b'Hello World')  
>>> buf[0:5] = b'Hallo'  
>>> buf  
bytearray(b'Hallo World')  
>>> with open('newsample.bin', 'wb') as f:  
... f.write(buf)  
...  
11  
>>> 
```

# 5.9.3 讨论

文件对象的 readinto()方法可用来将数据填充到任何预分配好的数组中，这包括 array 模

块或者 numpy 这样的库所创建的数组。与普通的 read()方法不同的是，readinto()是为已存在的缓冲区填充内容，而不是分配新的对象然后再将它们返回。因此，可以用 readinto()来避免产生额外的内存分配动作。例如，如果正在读取一个由相同大小的记录所组成的二进制文件，可以像这样编写代码：

```python
record_size = 32 # Size of each record (adjust value)  
buf = ndarray(record_size)  
with open('somefile', 'rb') as f:  
while True:  
    n = f.readinto(buf)  
    if n < record_size:  
        break  
    # Use the contents of buf  
    ... 
```

这里用到的另一个有趣的特性应该就是内存映像（memoryview）了，它使得我们可以对已存在的缓冲区做切片处理，但是中间不涉及任何拷贝操作，我们甚至还可以修改它的内容。示例如下：

>>>buf   
bytearray(b'Hello World')   
>>>m1 $=$ memoryview(buf)   
>>>m2 $=$ m1[-5:]   
>>>m2   
<memory at 0x100681390>   
>>>m2[:] $=$ b'WORLD'   
>>>buf   
bytearray(b'Hello WORLD')   
>>>

使用f.readinto()需要注意的一点是，必须总是确保要检查它的返回值，即实际读取的字节数。

如果字节数小于所提供的缓冲区大小，这可能表示数据被截断或遭到了破坏（例如，如果期望读取到一个准确的字节数时）。

最后，可以在各种库模块找到那些带有“into”的函数（例如 recv_into()、pack_into()等）。Python 中有许多模块都已经支持直接 I/O 访问了，可用来填充或修改数组和缓冲区中的内容。

请参见 6.12 节中那个解释二进制结构体和 memoryview 用法的示例，那个例子明显要更加高级一些。

# 5.10 对二进制文件做内存映射

# 5.10.1 问题

我们想通过内存映射的方式将一个二进制文件加载到可变的字节数组中，这样可以随机访问其内容，或者是实现就地修改。

# 5.10.2 解决方案

可以使用 mmap 模块实现对文件的内存映射操作。下面给出一个实用函数，以可移植的方式演示如何打开一个文件并对它进行内存映射操作：

import os   
import mmap   
def memory_map(filename, access=mmap.ACESS_WRITE): size $=$ os.path.size(filename) fd $=$ os.open(filename,os.O_RDWR) return mmap.mmap(fd,size,access $\equiv$ access)

要使用这个函数，需要准备一个已经创建好的文件并为之填充一些数据。下面的例子告诉我们如何创建一个初始文件，然后将其扩展为所需要的大小：

```txt
>>> size = 1000000  
>>> with open('data', 'wb') as f:  
... f.seek(size-1)  
... f.write(b'\x00')  
...  
>>> 
```

下面是用 memory_map()函数对文件内容做内存映射操作的例子：

```python
>>> m = memory_map('data')
>>> len(m)
1000000
>>> m[0:10]
b'\x00\x00\x00\x00\x00\x00\x00\x00\x00'
>>> m[0]
0
>>> # Reassign a slice
>>> m[0:11] = b'Hello World'
>>> m.close()
>>> # Verify that changes were made
>>> with open('data', 'rb') as f: 
```

```txt
print(f.read(11))   
b'Hello World'   
>>> 
```

由 mmap()返回的 mmap 对象也可以当做上下文管理器使用，在这种情况下，底层的文件会自动关闭。示例如下：

```txt
>>> with memory_map('data') as m:  
... print(len(m))  
... print(m[0:10])  
...  
1000000  
b'Hello World'  
>>> m.closed  
True  
>>> 
```

默认情况下，memory_map()函数打开的文件既可以读也可以写。对数据的任何修改都会拷贝回原始的文件中。如果需要只读访问，可以为 access 参数提供 mmap.ACCESS_READ值。示例如下：

```python
m = memory_map(filename, mmap.Access_READ) 
```

如果只想在本地修改数据，并不想将这些修改写回到原始文件中，可以使用 mmap.ACCESS_COPY 参数：

```txt
m = memory_map(filename, mmap.ACESS copying) 
```

# 5.10.3 讨论

通过 mmap 将文件映射到内存中后，我们能够以高效和优雅的方式对文件的内容进行随机访问。比方说，与其打开文件后通过组合各种 seek()、read()和 write()调用来访问，不如简单地将文件映射到内存，然后通过切片操作来访问数据。

通常，由 mmap()暴露出的内存看起来就像一个 bytearray 对象。但是，利用 memoryview能够以不同的方式来解读数据。比如：

```python
>>> m = memory_map('data')
>>> # Memoryview of unsigned integers
>>> v = memoryview(m).cast('I')
>>> v[0] = 7
>>> m[0:4]
b'\x07\x00\x00\x00'
>>> m[0:4] = b'\x07\x01\x00\x00'
>>> v[0] 
```

```txt
263  
>>> 
```

应该强调的是，对某个文件进行内存映射并不会导致将整个文件读到内存中。也就是说，文件并不会拷贝到某种内存缓冲区或数组上。相反，操作系统只是为文件内容保留一段虚拟内存而已。当访问文件的不同区域时，文件的这些区域将被读取并按照需要映射到内存区域中。但是，文件中从未访问过的部分会简单地留在磁盘上。这一切都是以透明的方式在幕后完成的。

如果有多个 Python 解释器对同一个文件做了内存映射，得到的 mmap 对象可用来在解释器之间交换数据。也就是说，所有的解释器可以同时读/写数据，在一个解释器中对数据做出的修改会自动反映到其他的解释器上。很明显，这里需要一些额外的步骤来处理同步问题，但是有时候可用这种方法作为通过管道或 socket传输数据的替代方式。

本节中的示例已经尽量以通用的形式实现，能够在 UNIX 和 Windows 上都适用。请注意，对于 mmap()的使用，不同的平台上会存在一些差异。此外，还有选项可用来创建匿名的内存映射区域。如果对此感兴趣，请确保仔细阅读有关这个主题的 Python 文档（http://docs.python.org/3/library/mmap.html）。

# 5.11 处理路径名

# 5.11.1 问题

我们需要处理路径名以找出基文件名、目录名、绝对路径等相关的信息。

# 5.11.2 解决方案

要操纵路径名，可以使用 os.path 模块中的函数。下面是一个交互式的例子，用来说明其中一些核心的功能：

```python
>>> import os
>>> path = '/Users/beazley/Data/data.csv'
>>> # Get the last component of the path
>>> os.path:baselname(path)
'data.csv'
>>> # Get the directory name
>>> os.path/dirname(path)
'/Users/beazley/Data'
>>> # Join path components together
>>> os.path.join('tmp', 'data', os.path.baselname(path)) 
```

```txt
'tmp/data/data.csv'  
>>> # Expand the user's home directory  
>>> path = '/Data/data.csv'  
>>> os.path.expanduser(path)  
'/Users/beazley/Data/data.csv'  
>>> # Split the file extension  
>>> os.path.splitext(path)  
{'~/Data/data', '.csv'}  
>>> 
```

# 5.11.3 讨论

对于任何需要处理文件名的问题，都应该使用 os.path 模块而不是通过使用标准的字符串操作来自己实现这部分功能。部分原因是为了考虑可移植性。os.path 模块知道 UNIX和 Windows 系统之间的一些差异，能够可靠地处理类似 Data/data.csv 和 Data\data.csv这样的文件名。其次，我们真的不应该花时间去重造轮子。通常最好是直接使用那些已经提供了的功能。

应该值得一提的是，os.path 模块中还有许多功能没有在本节中展示出来。可以参阅文档以获得更多同文件测试、符号链接等功能相关的函数。

# 5.12 检测文件是否存在

# 5.12.1 问题

我们需要检测某个文件或目录是否存在。

# 5.12.2 解决方案

可以通过 os.path 模块来检测某个文件或目录是否存在。示例如下：

```txt
>>> import os
>>> os.path.exists('/etc/passwd')
True
>>> os.path.exists('/tmp/spam')
False
>>>
```

之后可以执行进一步的测试来查明这个文件的类型。如果文件不存在的话，下面这些检测就会返回 False：

```txt
>>> # Is a regular file 
```

```txt
>>> os.path.isfile('/etc/passwd')  
True  
>>> # Is a directory  
>>> os.path.isdir('/etc/passwd')  
False  
>>> # Is a symbolic link  
>>> os.path.islink('/usr/local/bin/python3')  
True  
>>> # Get the file linked to  
>>> os.path.realpath('/usr/local/bin/python3')  
'/usr/local/bin/python3.3'  
>>> 
```

如果需要得到元数据（即，文件大小或修改日期），这些功能在 os.path 模块中也有提供：

```txt
>>> os.path.getsize('/etc/passwd')  
3669  
>>> os.path.gettime('/etc/passwd')  
1272478234.0  
>>> import time  
>>> time.ctime(os.path.gettime('/etc/passwd'))  
'Wed Apr 28 13:10:34 2010'  
>>> 
```

# 5.12.3 讨论

利用 os.path 模块来对文件做检测是简单而直接的。也许在编写脚本时唯一需要注意的事情就是关于权限的问题了——尤其是获取元数据的操作。比如：

```txt
>>> os.path.getsize('/Users/guido/Desktop/foo.txt')  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
    File "/usr/local/lib/python3.3/genericpath.py", line 49, in getsizereeturn os.stat(filename).st_size  
PermissionError: [Errno 13] Permission denied: '/Users/guido/Desktop/foo.txt'  
>>> 
```

# 5.13 获取目录内容的列表

# 5.13.1 问题

我们想获取文件系统中某个目录下所包含的文件列表。

# 5.13.2 解决方案

可以使用 os.listdir()函数来获取目录中的文件列表。示例如下：

```python
import os  
names = os.listdir('somedir') 
```

这么做会得到原始的目录文件列表，包括所有的文件、子目录、符号链接等。如果需要以某种方式来筛选数据，可以考虑利用列表推导式结合 os.path 模块中的各种函数来完成。示例如下：

import os.path   
#Get all regular files   
names $=$ [name for name in os.listdir('somedir') if os.path.isfile(os.path.join('somedir'，name))]   
#Get all dirs   
dirnames $=$ [name for name in os.listdir('somedir') if os.path.isdir(os.path.join('somedir'，name))]

字符串的 startswith()和 endswith()方法对于筛选目录中的内容也同样有用。比如：

```coffeescript
pyfiles = [name for name in os.listdir('somedir')  
    if name.endsWith(.py')] 
```

至于文件名的匹配，可能会想到用 glob 或者 fnmatch 模块。示例如下：

import glob   
pyfiles $=$ glob.glob('somedir/\*.py')   
from fnmatch import fnmatch   
pyfiles $=$ [name for name in os.listdir('somedir') if fnmatch(name,'\*.py')]

# 5.13.3 讨论

得到目录中内容的列表很简单，但是这只会带来目录中每个条目的名称。如果想得到一些附加的元数据，比如文件大小、修订日期等，要么使用os.path模块中的其他函数，要么使用 os.stat()函数。要收集这些数据，请参见示例：

```txt
Example of getting a directory listing  
import os  
import os.path  
import glob  
pyfiles = glob.glob('*.py') 
```

Get file sizes and modification dates   
name_sz_date $=$ [(name, os.path.size(name), os.path.gettime(name)) for name in pyfiles]   
for name, size, mtime in name_sz_date: print(name, size, mtime)   
# Alternative: Get file metadata   
file_metadata $=$ [(name, os.stat(name)) for name in pyfiles]   
for name, meta in file_metadata: print(name, meta.st_size, meta.st_mtime)

最后但也很重要的是，请注意有关文件名编码时会出现的一些微妙问题。一般来说，像 os.listdir()这样的函数返回的条目都会根据系统默认的文件名编码方式来进行解码处理。但是，有可能在特定的条件下会遇到无法解码的文件名。5.14 节和 5.15 节中有更多关于处理这样的名称时应该注意的细节。

# 5.14 绕过文件名编码

# 5.14.1 问题

我们想对使用了原始文件名的文件执行I/O 操作，这些文件名没有根据默认的文件名编码规则来解码或编码。

# 5.14.2 解决方案

默认情况下，所有的文件名都会根据 sys.getfilesystemencoding()返回的文本编码形式进行编码和解码。例如：

```txt
>>>sys.getfilesystemencoding() 'utf-8'   
>>> 
```

如果基于某些原因想忽略这种编码，可以使用原始字节串来指定文件名。示例如下：

```python
>>> # Write a file using a unicode filename  
>>> with open('jalape\xf1o.txt', 'w') as f:  
... f.write('Spicy!')  
...  
6  
>>> # Directory listing (decoded)  
>>> import os  
>>> os.listdir().') 
```

```txt
['jalapeño.txt']  
>>> # Directory listing (raw)  
>>> os.listdir(b’.')  # Note: byte string  
[b'jalapen\xcc\x83o.txt']  
>>> # Open file with raw filename  
>>> with open(b'jalapen\xcc\x83o.txt') as f:  
...     print(f.read())  
...  
Spicy!  
>>> 
```

在上两个操作中可以看到，当给同文件相关的函数比如 open()和 os.listdir()提供字节串参数时，对文件名的处理就发生了微小的改变。

# 5.14.3 讨论

一般情况下，不应该去担心有关文件名编码和解码的问题——普通的文件名操作应该能正常工作。但是，有许多操作系统可能会允许用户通过意外或恶意的方式创建出文件名不遵守期望的编码规则的文件。这样的文件名可能会使得处理大量文件的 Python程序莫名其妙地崩溃。

在读取目录和同文件名打交道时，以原始的未解码的字节作为文件名就可以避免这样的问题，只是编程的时候要麻烦一些。

请参见 5.15 节中关于打印出无法解码的文件名的相关示例。

# 5.15 打印无法解码的文件名

# 5.15.1 问题

我们的程序接收到一个目录内容的列表，但是当程序试着打印出文件名时，会出现UnicodeEncodeError 异常并伴随着一条难以理解的提示信息：“不允许代理（surrogatesnot allowed）”，然后程序就崩溃了。

# 5.15.2 解决方案

当打印来路不明的文件名时，可以使用下面的方式来避免出现错误：

```python
def bad Filename(filename):
    return repr(filename) [1:-1]
try: 
```

```python
printfilename)   
except UnicodeEncodeError:   
print(badfilename(filename)) 
```

# 5.15.3 讨论

当程序必须去操纵文件系统时，本节提到了一个一般情况下很罕见但却非常令人头疼的问题。默认情况下，Python 假设所有的文件名都是根据 sys.getfilesystemencoding()返回的编码形式进行编码的。但是，某些文件系统不一定会强制执行这种编码约束，因此会允许文件以不恰当的编码方式来命名。这并不常见，但是总有某些用户会做出些愚蠢的事情，意外地创建出这么一个文件来（即，可能在某些有问题的代码中将不恰当的文件名传给 open()）。因此危险总是存在的。

当执行类似os.listdir()这样的命令时，错误的文件名会使 Python 陷入窘迫的境地。一方面 Python 不能直接丢弃错误的名字，而另一方面它也无法将文件名转为合适的文本字符串。对于这个问题，Python 的解决方案是在文件名中取出一个无法解码的字节值\xhh，将其映射到一个所谓的“代理编码（surrogate encoding）”中，代理编码由 Unicode 字符\udchh 来表示。参见下面的示例，在一个有缺陷的目录列表中包含着一个名为 bäd.txt的文件，该文件名的编码方式为 Latin-1 而不是 UTF-8，我们来看看显示出来的结果：

```python
>>> import os
>>> files = os.listdir('./')
>>> files
['spam.py', 'b\udce4d.txt', 'foo.txt']
>>> 
```

如果代码只是用来操纵文件名或者甚至是将文件名传递给函数（比如 open()），一切都能正常工作。只有当想把文件名输出时才会陷入麻烦（即，打印到屏幕、记录到日志上等）。具体而言，如果试着打印上面这个列表，程序就会崩溃：

```txt
>>> for name in files:
...     print(name)
...
spam.py
Traceback (most recent call last):
    File "<stdin>", line 2, in <module>
UnicodeEncodeError: 'utf-8' codec can't encode character '\udce4' in position 1: surrogates not allowed
>>> 
```

崩溃的原因在于字符\udce4 不是合法的 Unicode 字符。它实际上是 2字符组合的后半部分，这个组合称为代理对（surrogate pair）。但是，由于前半部分丢失了，因此是非法的 Unicode。所以，唯一能成功产生输出的方式是，当遇到有问题的文件名时采取纠正措施。比如，将代码改为下面的方式就能产生出结果了：

```txt
>>> for name in files:  
... try:  
... print(name)  
... except UnicodeEncodeError:  
... print(bad_filename(name))  
...  
spam.py  
b\udce4d.txt  
foo.txt  
>>> 
```

函数 bad_filename()要实现什么功能很大程度上取决于自己的选择。比如，另一种选择是以其他方式重新编码，就像这样：

```python
def bad Filename(filename):
    temp = filename.encode(sys.getfilesystemencoding(), errors='surrogatescape')
    return tempdecode('latin-1') 
```

如果使用上面这个版本的 bad_filename()，就会产生如下的输出：

```txt
>>> for name in files:  
... try:  
... print(name)  
... except UnicodeEncodeError:  
... print(badfilename(name))  
...  
spam.py  
bãd.txt  
foo.txt  
>>> 
```

大部分读者可能都会忽略这一节的内容。但是，如果要编写完成关键任务的脚本，需要可靠地与文件名以及文件系统打交道，那么就需要好好考虑本节的内容。否则，可能就需要周末被叫去办公室调试一个看似无法理解的错误。

# 5.16 为已经打开的文件添加或修改编码方式

# 5.16.1 问题

我们想为一个已经打开的文件添加或修改 Unicode 编码，但不必首先将其关闭。

# 5.16.2 解决方案

如果想为一个以二进制模式打开的文件对象添加 Unicode 编码/解码，可以用

io.TextIOWrapper()对象将其包装。示例如下：

import urllib.request   
import io   
u $=$ urllib.request(urlopen('http://www.python.org')   
f $=$ io.TextIOWrapper(u,encoding $\coloneqq$ 'utf-8')   
text $=$ f.read()

如果想修改一个已经以文本模式打开的文件的编码方式，可以在用新的编码替换之前的编码前，用detach()方法将已有的文本编码层移除。下面是修改sys.stdout编码的例子：

```python
>>> import sys
>>> sys.stdoutencoding
'UTF-8'
>>> sys.stdout = io.TextIOWrapper(sys.stdout.derach(), encoding='latin-1')
>>> sys.stdoutencoding
'latin-1'
>>> 
```

这么做可能会破坏终端上的输出，这里只是用做说明使用。

# 5.16.3 讨论

I/O 系统是以一系列的层次来构建的。我们可以通过下面这个涉及文本文件的简单例子来观察这些层次：

>>>f $\equiv$ open('sample.txt'，'w')   
>>>f   
<_io.TextIOwrapper name $=$ 'sample.txt' mode $=$ 'w' encoding $=$ 'UTF-8'>   
>>>f_buffer   
<_ioBSDedWriter name $=$ 'sample.txt'>   
>>>f_buffer.raw   
<_io.FileIO name $=$ 'sample.txt' mode $=$ 'wb'>   
>>>

在这个例子中，io.TextIOWrapper 是一个文本处理层，它负责编码和解码 Unicode。而io.BufferedWriter 是一个缓冲 I/O 层，负责处理二进制数据。最后，io.FileIO 是一个原始文件，代表着操作系统底层的文件描述符。添加或修改文本的编码涉及添加或修改最上层的 io.TextIOWrapper 层。

作为一般的规则，直接通过访问上面展示的属性来操纵不同的层次是不安全的。比如，如果用这种技术来修改编码的话，看看会出现什么情况：

>>>f   
<io.TextIOwrapper name $\equiv$ 'sample.txt' mode $\equiv$ 'w' encoding $\equiv$ 'UTF-8'>   
>>>f $=$ io.TextIOwrapper(f.buffer,encoding $\equiv$ 'latin-1')

>>>f   
<io.TextIOWrapper name $\equiv$ 'sample.txt' encoding $\equiv$ 'latin-1'>   
>>f.write('Hello')   
Traceback (most recent call last): File "<stdin>", line 1, in <module>   
ValueError: I/O operation on closed file.   
>>>

这根本不起作用，因为f 之前的值已经被销毁，在这个过程中导致底层的文件被关闭。detach()方法将最上层的 io.TextIOWrapper 层同文件分离开来，并将下一个层次（io.BufferedWriter）返回。在这之后，最上层将不再起作用。示例如下：

```txt
>>> f = open('sample.txt', 'w')
>>> f
<io.TextIOWrapper name='sample.txt' mode='w' encoding='UTF-8']
>>> b = fdetach()
>>> b
<io_bufferedWriter name='sample.txt']
>>> f.write('hello')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ValueError: underlying buffer has been detached
>>> 
```

一旦完成分离，就可以为返回的结果添加一个新的最上层。示例如下：

>>>f $\equiv$ io.TextIOWrapper(b,encoding $\coloneqq$ 'latin-1')   
>>>f   
<io.TextIOWrapper name $\equiv$ 'sample.txt' encoding $\equiv$ 'latin-1'>   
>>>

尽管这里我们已经展示了如何修改编码方式，其实也可以利用这项技术来修改文本行的处理、错误处理机制以及其他有关文件处理方面的行为。示例如下：

```html
>>> sys.stdout = io.TextIOWrapper(sys.stdoutdetach(), encoding='ascii',
...
...
...
>>> print('Jalape\u00f1o')
Jalape&#241;o
>>> 
```

在输出中，我们注意到非 ASCII 字符 ñ 已经被&#241 所取代了。

# 5.17 将字节数据写入文本文件

# 5.17.1 问题

我们想将一些原始字节写入到以文本模式打开的文件中。

# 5.17.2 解决方案

只需要简单的将字节数据写入到文件底层的 buffer 中就可以了。示例如下：

```txt
>>> import sys
>>> sys.stdout.write(b'Hello\n')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: must be str, not bytes
>>> sys.stdout.buffer.write(b'Hello\n')
Hello
5
>>> 
```

同样地，我们也可以从文本文件中读取二进制数据，只要通过 buffer 属性来读取即可。

# 5.17.3 讨论

I/O 系统是以不同的层次来构建的。文本文件是通过在缓冲的二进制模式文件之上添加一个Unicode编码/解码层来构建的。buffer属性简单地指向底层的文件。如果访问该属性，就可以绕过文本编码/解码层了。

例子中的 sys.stdout可以被视为特殊情况。默认情况下，sys.stdout总是以文本模式打开的。但是，如果要编写一个需要将二进制数据转储到标准输出的脚本，就可以使用上面演示的技术来绕过文本编码层。

# 5.18 将已有的文件描述符包装为文件对象

# 5.18.1 问题

我们有一个以整数值表示的文件描述符，它已经同操作系统中已打开的 I/O 通道建立起了联系（即，文件、管道、socket等）。而我们希望以高级的Python 文件对象来包装这个文件描述符。

# 5.18.2 解决方案

文件描述符与一般打开的文件相比是有区别的。区别在于，文件描述符只是一个由操作系统分配的整数句柄，用来指代某种系统 I/O 通道。如果刚好有这样一个文件描述符，就可以通过 open()函数用 Python 文件对象对其进行包装。这很简单，只需将整数形式的文件描述符作为第一个参数取代文件名就可以了。示例如下：

```txt
Open a low-level file descriptor import os 
```

```python
fd = os.open('somefile.txt', os.O_WRONLY | os.O_CREAT)  
# Turn into a proper file  
f = open(fd, 'wt')  
f.write('hello world\n')  
f.close() 
```

当高层的文件对象被关闭或销毁时，底层的文件描述符也会被关闭。如果不想要这种行为，只需给 open()提供一个可选的 closefd=False 参数即可。示例如下：

```matlab
Create a file object, but don't close underlying fd when done  
f = open(fd, 'wt', closefd=False)  
... 
```

# 5.18.3 讨论

在 UNIX系统上，这种包装文件描述符的技术可以用来方便地对以不同方式打开的 I/O通道（即，管道、socket 等）提供一个类似于文件的接口。例如，下面是一个有关 socket的例子：

from socket import socket, AF_INET, SOCK_STREAM   
def echo_client(client_sock, addr): print('Got connection from', addr) #Make text-mode file wrappers for socket reading/writing client_in $=$ open(client_sock.fileno(), 'rt', encoding $\equiv$ 'latin-1', closefd $\equiv$ False) client_out $=$ open(client_sock.fileno(), 'wt', encoding $\equiv$ 'latin-1', closefd $\equiv$ False) #Echo lines back to the client using file I/O for line in client_in: client_out.write(line) client_out.flush() client_sock.close()   
def echo_server(address): sock $=$ socket(AF_INET, SOCK_STREAM) sock.bind(address) sock.listen(1) while True: client,addr $=$ sock.accept() echo_client(client,addr)

需要重点强调的是，上面的例子仅仅只是用来说明内建的 open()函数的一种特性，而且只能工作在基于 UNIX 的系统之上。如果想在 socket 上加上一个类似文件的接口，并

且需要做到跨平台，那么就应该使用 socket的 makefile()方法来替代。但是，如果不需要考虑可移植性的话，就会发现上面给出的解决方案在性能上要比makefile()高出不少。

也可以利用这项技术为一个已经打开的文件创建一种别名，使得它的工作方式能够稍微区别于首次打开时的样子。比方说，下面这段代码告诉我们如何创建一个文件对象，使得它能够在 stdout上产生出二进制数据（通常 stdout 是以文本模式打开的）：

```python
import sys
# Create a binary-mode file for stdout
bstdout = open(sys.stdout.fileno(), 'wb', closefd=False)
bstdout.write(b'Hello World\n')
bstdout.flush() 
```

尽管我们可以将一个已存在的文件描述符包装成一个合适的文件，但是请注意，并非所有的文件模式都可以得到支持，而且某些特定类型的文件描述符可能还带有有趣的副作用（尤其是在面对错误处理、文件结尾的情况时）。具体的行为也可能因为操作系统的不同而有所区别。特别是，上面所有的示例代码都没法在非 UNIX 系统上工作。因此，最基本的底线就是需要对自己的实现进行彻底的测试，确保代码能够按照期望的方式工作。

# 5.19 创建临时文件和目录

# 5.19.1 问题

当程序运行时，我们需要创建临时文件或目录以便使用。在这之后，我们可能希望将这些文件和目录销毁掉。

# 5.19.2 解决方案

tempfile 模块中有各种函数可以用来完成这个任务。要创建一个未命名的临时文件，可以使用 tempfile.TemporaryFile:

from tempfile import TemporaryFile   
with TemporaryFile('w+t') as f: #Read/write to the file f.write('Hello World\n') f.write('Testing\n') #Seek back to beginning and read the data f.seek(0) data $=$ f.read()   
# Temporary file is destroyed

或者如果你喜欢的话，也可以像这样使用文件：

```python
f = TemporaryFile('w+t')  
# Use the temporary file  
...  
f.close()  
# File is destroyed 
```

TemporaryFile()的第一个参数是文件模式，通常以 $\mathbf { W } + \mathbf { t }$ 处理文本模式而以 $\mathbf { w } { + } \mathbf { b }$ 处理二进制数据。这个模式可同时支持读写，在这里是很有用的，因为关闭文件后再来修改模式实际上会销毁文件对象。此外，TemporaryFile()也可以接受和内建的 open()函数一样的参数。示例如下：

```python
with TemporaryFile('w+t', encoding='utf-8', errors='ignore') as f: 
```

在大多数 UNIX 系统上，由TemporaryFile()创建的文件都是未命名的，而且在目录中也没有对应的条目。如果想解放这种限制，可以使用 NamedTemporaryFile()来替代。示例如下：

```python
from tempfile import NamedTemporaryFile with NamedTemporaryFile('w+t') as f: print('filename is:','f.name) # File automatically destroyed 
```

这里，在已打开文件的 f.name 属性中就包含了临时文件的文件名。如果需要将它传给其他需要打开这个文件的代码时，这就显得很有用了。对于 TemporaryFile()而言，结果文件会在关闭时自动删除。如果不想要这种行为，可以提供一个 delete=False 关键字参数。示例如下：

```python
with NamedTemporaryFile('w+t', delete=True) as f:  
    print('filename is:', f.name) 
```

要创建一个临时目录，可以使用 tempfile.TemporaryDirectory()来实现。示例如下：

```python
from tempfile import TemporaryDirectory  
with TemporaryDirectory() as dirname:  
    print('dirname is:", dirname)  
    # Use the directory  
    ...  
# Directory and all contents destroyed 
```

# 5.19.3 讨论

要和临时文件还有临时目录打交道，最方便的方式就是使用 TemporaryFile()、NamedTemporaryFile()以及 TemporaryDirectory()这三个函数了。因为它们能自动处理有关创建和清除的所有步骤。从较低的层次来看，也可以使用 mkstemp()和 mkdtemp()来创建临时文件和目录。示例如下：

```txt
>>> import tempfile
>>> tempfile.mkstemp()
(3, '/var/folders/7W/7WZl5sfZEF0pljrEB1UMWE+++TI/-Tmp-/tmp7fefhv')
>>> tempfile.mkdtemp()
'/var/folders/7W/7WZl5sfZEF0pljrEB1UMWE+++TI/-Tmp-/tmp5wvcv6'
>>> 
```

但是，这些函数并不会进一步去处理文件管理的任务。例如，mkstemp()函数只是简单地返回一个原始的操作系统文件描述符，然后由我们自行将其转换为一个合适的文件。同样地，如果想将文件清理掉的话，这个任务也是由我们自己完成。

一般情况下，临时文件都是在系统默认的区域中创建的，比如/var/tmp 或者类似的地方。要找出实际的位置，可以使用 tempfile.gettempdir()函数。示例如下：

```txt
>>> tempfile.gettempdir()
'/var/folders/7W/7WZ15sfZEF0pljrEB1UMWE+++TI/-Tmp-'
>>> 
```

所有同临时文件相关的函数都允许使用prefix、suffix和 dir 关键字参数来覆盖目录。例如：

>>>f $\equiv$ NamedTemporaryFile(prefix $\coloneqq$ 'mytemp', suffix $=$ '.txt', dir $= ^{\prime} /$ tmp')   
>>>f.name   
'/tmp/mytemp8ee899.txt'   
>>>

最后但也很重要的是，在可能的范围内，tempfile 模块创建的临时文件都是以最安全的方式来进行的。这包括只为当前用户提供可访问的权限，并且在创建文件时采取了相应的步骤来避免出现竞态条件。请注意，在不同的平台下这可能会有一些区别。因此，对于更精细的要点，应该确保自己去查阅官方文档（http://docs.python.org/3/library/tempfile.html）。

# 5.20 同串口进行通信

# 5.20.1 问题

我们想通过串口读取和写入数据，典型情况下是同某种硬件设备进行交互（即，机器

人或传感器）。

# 5.20.2 解决方案

尽管可以直接通过 Python 内建的 I/O 原语来完成这个任务，但对于串口通信来说，最好还是使用pySerial包比较好。这个包使用起来非常简单，要打开一个串口，只要使用这样的代码即可：

```python
import serial  
ser = serial Serial('/dev/tty.usbmodem641', # Device name varies  
baudrate=9600,  
bytesize=8,  
parity='N',  
stopbits=1) 
```

设备名称可能会根据设备的类型和操作系统而有所不同。比如，在 Windows 上，可以使用 0、1 这样的数字代表设备来打开通信端口，比如“COM0”和“COM1”。一旦打开后，就可以通过 read()、readline()和 write()调用来读写数据了。示例如下：

```txt
ser.write(b'G1 X50 Y50\r\n')  
resp = ser.readline() 
```

从这一点来看，大部分情况下的串口通信任务应该是非常简单的。

# 5.20.3 讨论

尽管表面上看起来很简单，串口通信常常会变得相当混乱。应该使用一个像 pySerial这样的包的原因就在于它对一些高级特性提供了支持（即，超时处理、流控、刷新缓冲区、握手机制等）。比如，如果想开启 RTS-CTS 握手，只要简单地为 Serial()提供一个 rtsct $\ c =$ True 关键字参数即可。pySerial 提供的文档非常棒，所以在这里多解释也没多大用处。

请记住所有涉及串口的 I/O 操作都是二进制的。因此，确保在代码中使用的是字节而不是文本（或者根据需要执行适当的文本编码/解码操作）。当需要创建以二进制编码的命令或者数据包时，struct 模块也会起到不少作用。

# 5.21 序列化 Python 对象

# 5.21.1 问题

我们需要将 Python 对象序列化为字节流，这样就可以将其保存到文件中、存储到数据库中或者通过网络连接进行传输。

# 5.21.2 解决方案

序列化数据最常见的做法就是使用 pickle 模块。要将某个对象转储到文件中，可以这样做：

```python
import pickle  
data = ... # Some Python object  
f = open('somefile', 'wb')  
pickle.dump(data, f) 
```

要将对象转储为字符串，可以使用 pickle.dumps()：

```python
s = pickle.dumps(data) 
```

如果要从字节流中重新创建出对象，可以使用 pickle.load()或者 pickle.loads()函数。示例如下：

```python
#Restore from a file  
f = open('somefile', 'rb')  
data = pickle.load(f)  
#Restore from a string  
data = pickle_load(s) 
```

# 5.21.3 讨论

对于大部分程序来说，只要掌握 dump()和 load()函数的用法就可以高效地利用 pickle模块了。pickle 模块能够兼容大部分 Python 数据类型和用户自定义的类实例。如果正在使用的库可以保存/恢复 Python 对象到数据库中，或者通过网络传输对象，那么很有可能就在使用 pickle。

pickle 是一种 Python 专有的自描述式的数据编码。说到自描述，因为序列化的数据中包含有每个对象的开始和结束以及有关对象类型的信息。因此，不需要担心应该如何定义记录——pickle 就能完成了。例如，如果需要处理多个对象，可以这么做：

```python
>>> import pickle
>>> f = open('somedata', 'wb')
>>> pickle.dump([1, 2, 3, 4], f)
>>> pickle dump('hello', f)
>>> pickle dump({'Apple', 'Pear', 'Banana'}, f)
>>> f.close()
>>> f = open('somedata', 'rb')
>>> pickle.load(f)
[1, 2, 3, 4]
>>> pickle.load(f) 
```

```txt
'hello'  
>>> pickle.load(f)  
{'Apple', 'Pear', 'Banana'}  
>>> 
```

可以对函数、类以及实例进行 pickle 处理，但由此产生的数据只会对代码对象所关联的名称进行编码。例如：

```haskell
>>> import math
>>> import pickle.
>>> pickle crate(math.cos)
b'\x80\x03cmath\ncos\nq\x00'
>>> 
```

当对数据做反序列化处理时，会假设所有所需的源文件都是可用的。模块、类以及函数会根据需要自动导入。对于需要在不同机器上的解释器之间共享 Python 数据的应用，这会成为一个潜在的维护性问题，因为所有的机器都必须能够访问到相同的源代码。

![](images/c269e3d5f1db1c7190c4ba4de9a9961b0d774cf3700caada773eca0b3f36cf1b.jpg)

# 警告

绝对不能对非受信任的数据使用 pickle.load()。由于会产生副作用，pickle会自动加载模块并创建实例。但是，了解 pickle是如何运作的骇客可以故意创建出格式不正确的数据，使得 Python 解释器有机会去执行任意的系统命令。因此，有必要将 pickle 限制为只在内部使用，解释器和数据之间要能够彼此验证对方。

某些特定类型的对象是无法进行 pickle 操作的。这些对象一般来说都会涉及某种外部系统状态，比如打开的文件、打开的网络连接、线程、进程、栈帧等。用户自定义的类有时候可以通过提供__getstate__()和__setstate__()方法来规避这些限制。如果定义了这些方法，pickle.dump()就会调用__getstate__()来得到一个可以被 pickle 处理的对象。同样地，在 unpickle 的时候就会调用__setstate__()了。为了说明，下面这个类在内部定义了一个线程，但是仍然可以进行 pickle/unpickle操作：

countdown.py   
import time   
import threading   
class Countdown: def init_self, n): self.n = n self.thr $=$ threading.Thread(target $\equiv$ self.run) self.thr.daemon $=$ True self.thr.start()

```python
def run(self):
    while self.n > 0:
        print('T-minus', self.n)
        self.n -= 1
        time.sleep(5)
def __getstate__(self):
    return self.n
def __setstate__(self, n):
    self.__init__(n) 
```

用下面的代码试验一下 pickle 操作：

```python
>>> import countdown
>>> c = countdown.Countdown(30)
>>> T-minus 30
T-minus 29
T-minus 28
...
>>> # After a few moments
>>> f = open('cstate.p', 'wb')
>>> import pickle
>>> pickle.dump(c, f)
>>> f.close() 
```

现在退出 Python，重启之后再试试这个：

>>>f $=$ open('cstate.p'，'rb')   
>>>pickle.load(f)   
countdown.Countdown object at 0x10069e2d0>   
T-minus19   
T-minus18   
…

可以看到线程又魔法般地重新焕发出生命了，而且是从上次执行 pickle 操作时剩下的计数开始执行。

对于大型的数据结构，比如由 array 模块或 numpy 库创建的二进制数组，pickle就不是一种特别高效的编码了。如果需要移动大量的数组型数据，那么最好简单地将数据按块保存在文件中，或者使用更加标准的编码，比如 HDF5（由第三方库支持）。

由于 pickle 是 Python 的专有特性，而且同源代码的关联紧密，因此不应该把 pickle 作为长期存储的格式。比如说，如果源代码发生改变，那么存储的所有数据就会失效且

变得无法读取。坦白说，要将数据保存在数据库和归档存储中，最好使用一种更加标准的数据编码，比如 XML、CSV 或者 JSON。这些编码方式的标准化程度更高，许多编程语言都支持，而且更能适应于源代码的修改。

最后但同样重要的是，请注意 pickle 模块中有着大量的选项和棘手的阴暗角落。对于大部分常见的用途，我们不必担心这些问题。但是如果要构建一个大型的应用，其中要用pickle来做序列化的话，那么就应该好好参考官方文档（http://docs.python.org/3/library/pickle.html）。

# 数据编码与处理

本章主要关注的重点是利用 Python 来处理以各种常见编码形式所呈现出的数据，比如CSV 文件、JSON、XML 以及二进制形式的打包记录。与数据结构那章不同，本章不会把重点放在特定的算法之上，而是着重处理数据在程序中的输入和输出问题上。

# 6.1 读写 CSV 数据

# 6.1. 1 问题

我们想要读写 CSV 文件中的数据。

# 6.1.2 解决方案

对于大部分类型的 CSV 数据，我们都可以用 csv 库来处理。比如，假设在名为 stocks.csv的文件中包含有如下的股票市场数据：

```csv
Symbol,Price,Date,Time,Change,Volume   
"AA",39.48,"6/11/2007","9:36am"-0.18,181800   
"AIG",71.38,"6/11/2007","9:36am"-0.15,195500   
"AXP",62.58,"6/11/2007","9:36am"-0.46,935000   
"BA",98.31,"6/11/2007","9:36am"+0.12,104800   
"C",53.08,"6/11/2007","9:36am"-0.25,360900   
"CAT",78.29,"6/11/2007","9:36am"-0.23,225400 
```

下面的代码示例告诉我们如何将这些数据读取为元组序列：

```python
import csv  
with open('stocks.csv') as f:  
    f_csv = csv-reader(f)  
    headers = next(f_csv) 
```

```txt
for row in f_csv: # Process row ... 
```

在上面的代码中，row 将会是一个元组。因此，要访问特定的字段就需要用到索引，比如 row[0]（表示 Symbol）和 row[4]（表示 Change）。

由于这样的索引常常容易混淆，因此这里可以考虑使用命名元组。示例如下：

```python
from collections import namedtuple  
with open('stock.csv') as f:  
    f_csv = csv-reader(f)  
    headings = next(f_csv)  
    Row = namedtuple('Row', headings)  
    for r in f_csv:  
        row = Row(*r)  
        # Process row 
```

这样就可以使用每一列的标头比如 row.Symbol 和 row.Change 来取代之前的索引了。应该要指出的是，这个方法只有在每一列的标头都是合法的 Python 标识符时才起作用。如果不是的话，就必须调整原始的标头（比如，把非标识符字符用下划线或其他类似的符号取代）。

另一种可行的方式是将数据读取为字典序列。可以用下面的代码实现：

import csv   
with open('stocks.csv') as f: f_csv $=$ csvxDictReader(f) for row in f_csv: #process row

在这个版本中，可以通过行标头来访问每行中的元素。比如，row['Symbol']或者row['Change']。

要写入 CSV数据，也可以使用 csv模块来完成，但是要创建一个写入对象。示例如下：

```python
headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']  
rows = [(AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]  
with open('stocks.csv', 'w') as f:  
    f_csv = csv.writer(f)  
    f_csv.writerow(headers) 
```

```python
f_csv.writerowrows(rows) 
```

如果数据是字典序列，那么可以这样处理：

```python
headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']
rows = [['Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007',
        'Time':'9:36am', 'Change):-0.18, 'Volume':181800],
        {'Symbol':'AIG', 'Price':71.38, 'Date':'6/11/2007'],
        'Time':'9:36am', 'Change):-0.15, 'Volume':195500},
        {'Symbol':'AXP', 'Price':62.58, 'Date':'6/11/2007'],
        'Time':'9:36am', 'Change):-0.46, 'Volume':935000},
] 
```

# 6.1.3 讨论

应该总是选择使用 csv 模块来处理，而不是自己手动分解和解析 CSV 数据。比如，我们可能会倾向于写出这样的代码：

```python
with open('stocks.csv') as f:  
    for line in f:  
        row = line.split('\n')  
        # process row  
    ... 
```

这种方式的问题在于仍然需要自己处理一些令人厌烦的细节问题。比如说，如果有任何字段是被引号括起来的，那么就要自己去除引号。此外，如果被引用的字段中恰好包含有一个逗号，那么产生出的那一行会因为大小错误而使得代码崩溃（因为原始数据也是用逗号分隔的）。

默认情况下，csv库被实现为能够识别微软 Excel所采用的 CSV 编码规则。这也许是最为常见的 CSV 编码规则了，能够带来最佳的兼容性。但是，如果查阅 csv 的文档，就会发现有几种方法可以将编码微调为其他的格式（例如，修改分隔字符等）。比方说，如果想读取以 tab 键分隔的数据，可以使用下面的代码：

```python
Example of reading tab-separated values with open('stock.tsv') as f:  
    f_tsv = csvreader(f, delimiter=''\t')  
    for row in f_tsv:  
        # Process row 
```

如果正在读取 CSV 数据并将其转换为命名元组，那么在验证列标题时要小心。比如，某个 CSV 文件中可能在标题行中包含有非法的标识符字符，就像下面的示例这样①：

```csv
Street Address,Num-Premises,Latitude,Longitude 5412 N CLARK,10,41.980262,-87.668452 
```

这会使得创建命名元组的代码出现 ValueError 异常。要解决这个问题，应该首先整理标题。例如，可以对非法的标识符字符进行正则替换，示例如下：

import re   
with open('stock.csv') as f:   
f_csv $=$ csv-reader(f)   
headers $=$ [re.sub(['^a-zA-Z_'], '\_,h) for h in next(f_csv)]   
Row $=$ namedtuple('Row', headers)   
for r in f_csv:   
row $=$ Row(\*r)   
# Process row

此外，还需要重点强调的是，csv 模块不会尝试去解释数据或者将数据转换为除字符串之外的类型。如果这样的转换很重要，那么这就是我们需要自行处理的问题。下面这个例子演示了对 CSV 数据进行额外的类型转换：

col_types $=$ [str, float, str, str, float, int]   
with open('stocks.csv') as f:   
f_csv $=$ csv-reader(f)   
headers $=$ next(f_csv)   
for row in f_csv: #Apply conversions to the row items row $=$ tuple(convert(value) for convert, value in zip(col_types, row))

作为另外一种选择，下面这个例子演示了将选中的字段转换为字典：

```python
print('Reading as dicts with type conversion')  
field_types = [ ('Price', float), ('Change', float), ('Volume', int)]  
with open('stocks.csv') as f:  
    for row in csv DictReader(f):  
        row.update((key, conversion(row[key]))  
        for key, conversion in field_types)  
            print(row) 
```

一般来说，对于这样的转换都应该小心为上。在现实世界中，CSV 文件可能会缺少某些值，或者数据损坏了，以及出现其他一些可能会使类型转换操作失败的情况，这都是很常见的。因此，除非可以保证数据不会出错，否则就需要考虑这些情况（也许需要加上适当的异常处理代码）。

最后，如果我们的目标是通过读取 CSV 数据来进行数据分析和统计，那么应该看看Pandas 这 个 Python 包 （ http://pandas.pydata.org ）。 Pandas 中 有 一 个 方 便 的 函 数pandas.read_csv()，能够将 CSV 数据加载到 DataFrame 对象中。之后，就可以生成各种各样的统计摘要了，还可以对数据进行筛选并执行其他类型的高级操作。6.13 节中给出了一个这样的例子。

# 6.2 读写 JSON 数据

# 6.2.1 问题

我们想读写以 JSON（JavaScript Object Notation）格式编码的数据。

# 6.2.2 解决方案

json 模块中提供了一种简单的方法来编码和解码 JSON 格式的数据。这两个主要的函数就是json.dumps()以及json.loads()。这两个函数在命名上借鉴了其他序列化处理库的接口，比如 pickle。下面的示例展示了如何将 Python 数据结构转换为 JSON：

import json   
data $=$ { 'name'：'ACME'， 'shares'：100, 'price'：542.23   
}   
json_str $=$ json.dumps(data)

而接下来的示例告诉我们如何把 JSON 编码的字符串再转换回 Python 数据结构：

```txt
data = json.dumpsjson_str) 
```

如果要同文件而不是字符串打交道的话，可以选择使用 json.dump()以及 json.load()来编码和解码 JSON 数据。示例如下：

```txt
Writing JSON data with open('data.json', 'w') as f: json.dump(data, f) 
```

```txt
Reading data back with open('data.json', 'r') as f: data = json.load(f) 
```

# 6.2.3 讨论

JSON 编码支持的基本类型有 None、bool、int、float 和 str，当然还有包含了这些基本类型的列表、元组以及字典。对于字典，JSON 会假设键（key）是字符串（字典中的任何非字符串键都会在编码时转换为字符串）。要符合 JSON 规范，应该只对 Python列表和字典进行编码。此外，在 Web 应用中，把最顶层对象定义为字典是一种标准做法。

JSON 编码的格式几乎与 Python 语法一致，只有几个小地方稍有不同。比如，True 会被映射为 true，False 会被映射为false，而 None 会被映射为null。下面的示例展示了编码看起来是怎样的：

```python
>>> json.dumps(False)
'false'
>>> d = {'a': True, ... 'b': 'Hello', ... 'c': None}
>>> json.dumps(d)
{'{"b": "Hello", "c": null, "a": true}'} 
```

如果要检查从 JSON 中解码得到的数据，那么仅仅将其打印出来就想确定数据的结构通常是比较困难的——尤其是如果数据中包含了深层次的嵌套结构或者有许多字段时。为了帮助解决这个问题，考虑使用 pprint 模块中的 pprint()函数。这么做会把键按照字母顺序排列，并且将字典以更加合理的方式进行输出。下面的示例展示了应该如何对Twitter 上的搜索结果以漂亮的格式进行输出：

```python
>>> from urllib.request import urlopen
>>> import json
>>> u = urlopen('http://search.twitter.com/search.json?q=python&cpp=5')
>>> resp = json.load(u.read().decode('utf-8'))
>>> from pprint import pprint
>>> pprint(res)
{'completed_in': 0.074, 'max_id': 264043230692245504, 'max_id_str': '264043230692245504', 'next_page':}?page=2&max_id=264043230692245504&q=python&cpp=5', 'page':1, 'query': 'python', 'refresh_url':'?since_id=264043230692245504&q=python', 
```

```python
'results':[{‘created_at': 'Thu, 01 Nov 2012 16:36:26 +0000', 'from_user': ... }, {‘created_at': 'Thu, 01 Nov 2012 16:36:14 +0000', 'from_user': ... }, {‘created_at': 'Thu, 01 Nov 2012 16:36:13 +0000', 'from_user': ... }, {‘created_at': 'Thu, 01 Nov 2012 16:36:07 +0000', 'from_user': ... } {‘created_at': 'Thu, 01 Nov 2012 16:36:04 +0000', 'from_user': ... }}], 'results_per_page': 5, 'since_id': 0, 'since_id_str': '0'} 
```

一般来说，JSON 解码时会从所提供的数据中创建出字典或者列表。如果想创建其他类型的对象，可以为 json.loads()方法提供 object_pairs_hook 或者 object_hook 参数。例如，下面的示例展示了我们应该如何将 JSON数据解码为OrderedDict（有序字典），这样可以保持数据的顺序不变：

>>>s $=$ {"name":"ACME","shares":50,"price":490.1}   
>>>from collections import OrderedDict   
>>>data $\equiv$ json.load(s,object_pairs hook=OrderedDict)   
>>>data   
OrderedDict([(['name'，'ACME']，('shares'，50)，('price'，490.1)]])   
>>>

而下面的代码将 JSON 字典转变为 Python 对象：

```python
>>> class JSONObject:
...
def __init__(self, d):
...
self.__dict__ = d
...
>>> data = json.dumps(s, object凇=JSONObject)
>>> data.name
'ACME'
>>> data.share
50
>>> data.price 
```

```txt
490.1  
>>> 
```

在上一个示例中，通过解码 JSON 数据而创建的字典作为单独的参数传递给了_init__()。之后就可以自由地根据需要来使用它了，比如直接将它当做对象的字典实例来用。

有几个选项对于编码 JSON来说是很有用的。如果想让输出格式变得漂亮一些，可以在json.dumps()函数中使用 indent 参数。这会使得数据能够像 pprint()函数那样以漂亮的格式打印出来。示例如下：

```txt
>>> print/json.dumps(data))
{"price": 542.23, "name": "ACME", "shares": 100}
>>> print/json.dumps(data, indent=4))
{
    "price": 542.23,
    "name": "ACME",
    "shares": 100
} 
```

如果想在输出中对键进行排序处理，可以使用 sort_keys 参数：

```html
>>> printjson.dumps(data，sort_keys=True)）{"name":"ACME","price":542.23,"shares":100}>>> 
```

类实例一般是无法序列化为 JSON 的。比如说：

```python
>>> class Point:
...
def __init__(self, x, y):
...
self.x = x
...
self.y = y
...
>>> p = Point(2, 3)
>>> json.dumps(p)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "/usr/local/lib/python3.3/json/_init_.py", line 226, in dumps
        return _defaultEncoder.encode(obj)
    File "/usr/local/lib/python3.3/json/encoder.py", line 187, in encode
        chunks = selfCoder(o, _one_shot=True)
    File "/usr/local/lib/python3.3/json/encoder.py", line 245, in iterencode
        return _iterencode(o, 0)
    File "/usr/local/lib/python3.3/json/encoder.py", line 169, in default
        raise TypeError(repr(o) + " is not JSONSerializable") 
```

```txt
TypeError: __main__.Point object at 0x1006f2650> is not JSONSerializable >>> 
```

如果想序列化类实例，可以提供一个函数将类实例作为输入并返回一个可以被序列化处理的字典。示例如下：

```python
def serialize_instance(obj):
    d = {'__classname': type(obj).__name}
    d.update(args(obj))
    return d 
```

如果想取回一个实例，可以编写这样的代码来处理：

Dictionary mapping names to known classes   
classes $=$ { 'Point' : Point   
}   
def unserialize_object(d): clfname $\equiv$ d.pop(_classname_,None) if clfname: clf = classes[clfname] obj $=$ clf._new_(cls) #Make instance without calling init_ for key, value in d.items(): setattr(obj, key, value) return obj   
else: return d

最后给出如何使用这些函数的示例：

```python
>>> p = Point(2,3)  
>>> s = json.dumps(p, default=serialize_instance)  
>>> s  
{'''classname': "Point", "y": 3, "x": 2}'  
>>> a = json.dumps(s, object_HOOK=unserialize_object)  
>>> a  
<__main__.Point object at 0x1017577d0>  
>>> a.x  
2  
>>> a.y  
3  
>>> 
```

json 模块中还有许多其他的选项，这些选项可用来控制对数字、特殊值（比如 NaN）等的底层解释行为。请参阅文档（http://docs.python.org/3/library/json.html）以获得进一

步的细节。

# 6.3 解析简单的 XML 文档

# 6.3.1 问题

我们想从一个简单的 XML 文档中提取出数据。

# 6.3.2 解决方案

xml.etree.ElementTree 模块可用来从简单的 XML 文档中提取出数据。为了说明，假设想对 Planet Python（http://planet.python.org）上的 RSS 订阅做解析并生成一个总结报告。下面的脚本可以完成这个任务：

from urllib.request import urlopen   
from xml.etree.ElementTree import parse   
# Download the RSS feed and parse it   
u $=$ urlopen('http://planet.python.org/rss20.xml') doc $=$ parse(u)   
# Extract and output tags of interest   
for item in doc.findfind('channel/item'): title $=$ item.findall('title') date $=$ item.findall('pubDate') link $=$ item.findall('link') print(title) print(date) print(link) print()

如果运行上面的脚本，会得到类似这样的输出：

```txt
Steve Holden: Python for Data Analysis  
Mon, 19 Nov 2012 02:13:51 +0000  
http://holdenweb.blogspot.com/2012/11/python-for-data-analysis.html  
Vasudev Ram: The Python Data model (for v2 and v3)  
Sun, 18 Nov 2012 22:06:47 +0000  
http://jugad2.blogspot.com/2012/11/the.python-data-model.html  
Python Diary: Been playing around with Object Databases  
Sun, 18 Nov 2012 20:40:29 +0000  
http://www.pythonondiary.com/blog/Nov.18,2012/been-...-object-databases.html 
```

```csv
Vasudev Ram: Wakari, Scientific Python in the cloud  
Sun, 18 Nov 2012 20:19:41 +0000  
http://jugad2.blogspot.com/2012/11/wakari-scientific-python-in-cloud.html  
Jesse Jiryu Davis: Toro: synchronization primitives for Tornado coroutines  
Sun, 18 Nov 2012 20:17:49 +0000  
http://feedproxy.google.com/~r/EmptysquarePython/~3/_DOZT2Kd0hQ/ 
```

显然，如果想做更多的处理，就需要将 print()函数替换为其他更加有趣的处理函数。

# 6.3.3 讨论

在许多应用中，同XML 编码的数据打交道是很常见的事情。这不仅是因为XML作为一种数据交换格式在互联网中使用广泛，而且 XML 还是用来保存应用程序数据（例如文字处理、音乐库等）的常用格式。本节后面的讨论假设读者已经熟悉 XML 的基本概念。

在许多情况下，XML如果只是简单地用来保存数据，那么文档结构就是紧凑而直接的。例如，上面示例中的 RSS 订阅源看起来类似于如下的 XML 文档：

```xml
<?xml version="1.0"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/">
<channel>
    <title>Planet Python</title>
    <link:http://planet.python.org></link>
    <language>en</language>
    <description>Planet Python - http://planet.python.org.</description>
    <item>
        <title>Steve Holden: Python for Data Analysis</title>
        <guid>http://holdenweb.blogspot.com/...-data-analysis.html</guid>
        <link:http://holdenweb.blogspot.com/...-data-analysis.html</link>
        <description>..</description>
        <pubDate>Mon, 19 Nov 2012 02:13:51 +0000</pubDate>
    </item>
    <item>
        <title>Vasudev Ram: The Python Data model (for v2 and v3)</title>
        <guid>http://jugad2.blogspot.com/...-data-model.html</guid>
        <link:http://jugad2.blogspot.com/...-data-model.html</link>
        <description>..</description>
        <pubDate>Sun, 18 Nov 2012 22:06:47 +0000</pubDate>
    </item>
    <item>
        <title>Python Diary: Been playing around with Object Databases</title>
        <guid>http://www.pythonordiary.com/...-object-databases.html</guid>
        <link:http://www.pythonordiary.com/...-object-databases.html</link>
        <description>..</description>
    </item>
</channel> 
```

<pubDate>Sun,18 Nov 2012 20:40:29 $+0000$ </pubDate>  
</item>  
</channel>  
</rss>

xml.etree.ElementTree.parse()函数将整个 XML 文档解析为一个文档对象。之后，就可以利用 find()、iterfind()以及 findtext()方法查询特定的 XML 元素。这些函数的参数就是特定的标签名称，比如 channel/item 或者 title。

当指定标签时，需要整体考虑文档的结构。每一个查找操作都是相对于一个起始元素来展开的。同样地，提供给每个操作的标签名也是相对于起始元素的。在示例代码中，对 doc.iterfind('channel/item')的调用会查找所有在“channel”元素之下的“item”元素。doc 代表着文档的顶层（顶层“rss”元素）。之后对 item.findtext()的调用就相对于已找到的“item”元素来展开。

每个由ElementTree模块所表示的元素都有一些重要的属性和方法，它们对解析操作十分有用。tag 属性中包含了标签的名称，text 属性中包含有附着的文本，而 get()方法可以用来提取出属性（如果有的话）。示例如下：

```txt
>>> doc
<xml.etree(ElementTree(ElementTree object at 0x101339510>
>>> e = doc.find('channel/title')
>>> e
Element 'title' at 0x10135b310>
>>> e.tag
'title'
>>> e.text
'Planet Python'
>>> e.get('some_attribute')
>>> 
```

应该要指出的是 xml.etree.ElementTree 并不是解析 XML 的唯一选择。对于更加高级的应用，应该考虑使用 lxml。lxml采用的编程接口和 ElementTree 一样，因此本节中展示的示例能够以同样的方式用 lxml 实现。只需要将第一个导入语句修改为 from lxml.etreeimport parse 即可。lxml 完全兼容于 XML 标准，这为我们提供了极大的好处。此外，lxml运行起来非常快速，还提供验证、XSLT 以及 XPath 这样的功能支持。

# 6.4 以增量方式解析大型 XML 文件

# 6.4.1 问题

我们需要从一个大型的 XML 文档中提取出数据，而且对内存的使用要尽可能少。

# 6.4.2 解决方案

任何时候，当要面对以增量方式处理数据的问题时，都应该考虑使用迭代器和生成器。下面是一个简单的函数，可用来以增量方式处理大型的 XML 文件，它只用到了很少量的内存：

from xml.etree.ElementTree import iterable   
def parse_and_removefilename, path): path_parts $=$ path.split('/') doc $=$ iterable(filename,'('start'，'end')) #Skip the root element next(doc) tag_stack $= []$ elem_stack $= []$ for event,elem in doc: if event $= =$ 'start': tag_stack.append(elem.tag) elem_stack.append(elem) elif event $= =$ 'end': if tag_stack $= =$ path_parts: yield elem elem_stack[-2].remove(elem) try: tag_stack.pop() elem_stack.pop() except IndexError: pass

要测试这个函数，只需要找一个大型的 XML 文件来配合测试即可。这种大型的 XML文件常常可以在政府以及数据公开的网站上找到。比如，可以下载芝加哥的坑洞数据库 XML。在写作本书时，这个下载文件中有超过 100000 行的数据，它们按照如下的方式编码：

<response> <row> <row ...> <creation_date $\gimel$ 2012-11-18T00:00:00</creation_date> <statusCompleted</status> <completion_date $2012 - 11 - 18$ T00:00:00</completion_date> <service_request_number>12-01906549</service_request_number> <type_of_service_request>Pot Hole in Street</type_of_service_request> <currentActivity>Final Outcome</currentActivity> <mostrecent_action>CDOT Street Cut ... Outcome</mostrecent_action>

```xml
<street_address>4714 S TALMAN AVE</street_address> <zip>60632</zip> <x Coordinate>1159494.68618856</x Coordinate> <y Coordinate>1873313.83503384</y Coordinate> <ward>14</ward> <policeDistrict>9</policeDistrict> <community_area>58</community_area> <latitude>41.808090232127896</latitude> <longitude>-87.69053684711305</longitude> <location latitude="41.808090232127896" longitude=-87.69053684711305" /> </row> <row ...> <creation_date>2012-11-18T00:00:00</creation_date> <statusCompleted</status> <completion_date>2012-11-18T00:00:00</completion_date> <service_request_number>12-01906695</service_request_number> <type_of_service_request>Pot Hole in Street</type_of_service_request> <currentActivity>Final Outcome</currentActivity> <mostrecent_action>CDOT Street Cut ... Outcome</mostrecent_action> <street_address>3510 W NORTH AVE</street_address> <zip>60647</zip> <x Coordinate>1152732.14127696</x Coordinate> <y Coordinate>1910409.38979075</y Coordinate> <ward>26</ward> <policeDistrict>14</policeDistrict> <community_area>23</community_area> <latitude>41.91002084292946</latitude> <longitude>-87.71435952353961</longitude> <location latitude="41.91002084292946" longitude=-87.71435952353961" /> </row> </row> </response> 
```

假设我们想编写一个脚本来根据坑洞的数量对邮政编码（ZIP code）进行排序。可以编写如下的代码来实现：

from xml.etree.ElementTree import parse   
from collections import Counter   
potholes_byzip $=$ Counter()   
doc $=$ parse('potholes.xml')   
for pothole in doc.findrow/row'): potholes_byzip[pothole.findall('zip']) $+ = 1$

```python
for zipcode, num in potholes_by_zip.best_common():
    print(zipcode, num) 
```

这个脚本存在的唯一问题就是它将整个XML 文件都读取到内存中后再做解析。在我们的机器上，运行这个脚本需要占据 $4 5 0 \mathrm { M B }$ 内存。但是如果使用下面这份代码，程序只做了微小的修改：

```python
from collections import Counter  
potholes_byZIP = Counter()  
data = parse_and_remove('potholes.xml', 'row/row')  
for pothole in data:  
    potholes_byZIP[pothole.findtext('zip']) += 1  
for zipcode, num in potholes_byZIP.best_common():  
    print(zipcode, num) 
```

这个版本的代码运行起来只用了 7 MB 内存——多么惊人的提升啊！

# 6.4.3 讨论

本节中的示例依赖于 ElementTree模块中的两个核心功能。首先，iterparse()方法允许我们对 XML 文档做增量式的处理。要使用它，只需提供文件名以及一个事件列表即可。事件列表由 1 个或多个 start/end，start-ns/end-ns 组成。iterparse()创建出的迭代器产生出形式为（event，elem）的元组，这里的 event 是列出的事件，而 elem 是对应的 XML元素。示例如下：

```txt
>>> data = iterable('potholes.xml', ('start', 'end'))
>>> next(data)
('start', <Element 'response' at 0x100771d60})
>>> next(data)
('start', <Element 'row' at 0x100771e68})
>>> next(data)
('start', <Element 'row' at 0x100771fc8])
>>> next(data)
('start', <Element 'creation_date' at 0x100771f18})
>>> next(data)
('end', <Element 'creation_date' at 0x100771f18})
>>> next(data)
('start', <Element 'status' at 0x1006a7f18})
>>> next(data)
('end', <Element 'status' at 0x1006a7f18>) 
```

当某个元素首次被创建但是还没有填入任何其他数据时（比如子元素），会产生 start

事件，而end事件会在元素已经完成时产生。尽管没有在本节示例中出现，start-ns和end-ns事件是用来处理 XML 命名空间声明的。

在这个示例中，start 和 end 事件是用来管理元素和标签栈的。这里的栈代表着文档结构中被解析的当前层次（current hierarchical），同时也用来判断元素是否匹配传递给parse_and_remove()函数的请求路径。如果有匹配满足，就通过 yield 将其发送给调用者。

紧跟在 yield 之后的语句就是使得 ElementTree 能够高效利用内存的关键所在：

```javascript
elem_stack[-2].remove(elem) 
```

这一行代码使得之前通过 yield 产生出的元素从它们的父节点中移除。因此可假设其再也没有任何其他的引用存在，因此该元素被销毁进而可以回收它所占用的内存。

这种迭代式的解析以及对节点的移除使得对整个文档的增量式扫描变得非常高效。在任何时刻都能构造出一棵完整的文档树。然而，我们仍然可以编写代码以直接的方式来处理 XML 数据。

这种技术的主要缺点就是运行时的性能。当进行测试时，将整个文档先读入内存的版本运行起来大约比增量式处理的版本快 2 倍。但是在内存的使用上，先读入内存的版本占用的内存量是增量式处理的60 倍多。因此，如果内存使用量是更加需要关注的因素，那么显然增量式处理的版本才是大赢家。

# 6.5 将字典转换为 XML

# 6.5.1 问题

我们想将 Python 字典中的数据转换为 XML。

# 6.5.2 解决方案

尽管 xml.etree.ElementTree 库通常用来解析 XML 文档，但它同样也可以用来创建 XML文档。例如，考虑下面这个函数：

from xml.etree(ElementTree import Element   
def dict_to_xml tag, d): T' Turn a simple dict of key/value pairs into XML ' element $\equiv$ Element (tag) for key, val in d.items(): child $=$ Element(key) child.text $=$ str(val)

```txt
elem.append(child) return elem 
```

下面是使用这个函数的示例：

>>>s $=$ {'name:'GOOG'，'shares':100，'price':490.1}   
>>>e $=$ dict_to_xml('stock',s)   
>>>e   
元素'to stock'at0x1004b64c8>   
>>>

转换的结果是一个 Element 实例。对于 I/O 操作来说，可以利用 xml.etree.ElementTree中的 tostring()函数将其转换为字节串。示例如下：

```txt
>>> from xml.etree.ElementTree import tostring
>>> tostring(e)
b'<stock><price>490.1</price><shares>100</shares><name>GOOG</name></stock}'
>>> 
```

如果想为元素附加上属性，可以使用 set()方法实现：

```txt
>>> e.set(_id,'1234')  
>>> tostring(e)  
b'<stock_id="1234"><price>490.1</price><shares>100</shares><name>G00G</name>  
</stock>'  
>>> 
```

如果需要考虑元素间的顺序，可以创建 OrderedDict（有序字典）来取代普通的字典。参见 1.7 节中对有序字典的介绍。

# 6.5.3 讨论

当创建 XML 时，也许会倾向于只使用字符串来完成。比如：

```python
def dict_to_xml_str tag, d):  
    '''  
    Turn a simple dict of key/value pairs into XML  
    '''  
parts = ['<{}>''.format tag]  
for key, val in d.items():  
    parts.append('<{}>{1}</{}>''.format(key, val))  
parts.append('/{}')'.format tag)  
return''.join(parts) 
```

问题在于如果尝试手工处理的话，那么这就是在自找麻烦。比如，如果字典中包含有特殊字符时会发生什么？

>>>d $=$ { 'name':'<spam>'   
>>> # String creation   
>>>dict_to_xml_str('item',d)   
'<item><name></name></item>'   
>>>#ProperXMLcreation   
>>>e $=$ dict_to_xml('item',d)   
>>>tosstring(e)   
b'<item><name>&lt;spam&gt;/</name></item>'   
>>>

请注意在上面这个示例中，字符<和>分别被&lt;和&gt;取代了。

下面的提示仅供参考。如果需要手工对这些字符做转义处理，可以使用 xml.sax.saxutils中的 escape()和 unescape()函数。示例如下：

```txt
>>> from xml sax.saxutils import escape,unescape
>>> escape('<spam>')
'<'amp; spam&gt;'
>>> unescape(_)
'<spam}'
>>>
```

为什么说创建 Element实例要比使用字符串好？除了可以产生出正确的输出外，其他的原因在于这样可以更加方便地将 Element实例组合在一起，创建出更大的XML 文档。得到的Element实例也能够以各种方式进行处理，完全不必担心解析 XML 文本方面的问题。最重要的是，我们能够站在更高的层面上对数据进行各种处理，只在最后把结果作为字符串输出即可。

# 6.6 解析、修改和重写 XML

# 6.6.1 问题

我们想读取一个 XML 文档，对它做些修改后再以 XML 的方式写回。

# 6.6.2 解决方案

xml.etree.ElementTree 模块可以轻松完成这样的任务。从本质上来说，开始时可以按照通常的方式来解析文档。例如，假设有一个名为pred.xml的文档，它看起来是这样的：

```xml
<?xml version="1.0"?>
<stop>
    <id>14791</id> 
```

```html
<nm>Clark &amp; Balmoral</nm> <sri> <rt>22</rt> <d>North Bound</d> <dd>North Bound</dd> </sri> <cr>22</cr> <pre> <pt>5 MIN</pt> <fd>Howard</fd> <v>1378</v> <rn>22</rn> </pre> <pre> <pt>15 MIN</pt> <fd>Howard</fd> <v>1867</v> <rn>22</rn> </pre> </stop> 
```

下面的示例采用 ElementTree 来读取这个文档，并对文档的结构作出修改：

```python
>>> from xml.etree.ElementTree import parse, Element
>>> doc = parse('pred.xml')
>>> root = doc.getroot()
>>> root
Element 'stop' at 0x100770cb0>
>>> # Remove a few elements
>>> root.remove(root.find('sri'))
>>> root.remove(root.find('cr'))
>>> # Insert a new element after <nm>...
>>> root.getchildren().index(root.find('nm'))
1
>>> e = Element('spam')
>>> e.text = 'This is a test'
>>> root.insert(2, e)
>>> # Write back to a file
>>> doc.write('newpred.xml', xml declarations=True)
>>> 
```

这些操作的结果产生了一个新的 XML 文档，看起来是这样的：

<?xml version $\equiv$ 1.0' encoding $\equiv$ 'us-ascii'?> <stop>

```html
<id>14791</id> <nm>Clark &amp; Balmoral</nm> <spam>This is a test</spam><pre> <pt>5 MIN</pt> <fd>Howard</fd> <v>1378</v> <rn>22</rn> </pre> <pre> <pt>15 MIN</pt> <fd>Howard</fd> <v>1867</v> <rn>22</rn> </pre> </stop> 
```

# 6.6.3 讨论

修改 XML 文档的结构是简单直接的，但是必须记住所有的修改主要是对父元素进行的，我们把它当做是一个列表一样对待。比如说，如果移除某个元素，那么就利用它的直接父节点的 remove()方法完成。如果插入或添加新的元素，同样要使用父节点的insert()和 append()方法来完成。这些元素也可以使用索引和切片操作来进行操控，比如element[i]或者是 element[i:j]。

如果需要创建新的元素，可以使用 Element 类来完成，我们本节给出的示例中已经这么做了。这在 6.5 节中有更进一步的描述。

# 6.7 用命名空间来解析 XML 文档

# 6.7.1 问题

我们要解析一个 XML 文档，但是需要使用 XML 命名空间来完成。

# 6.7.2 解决方案

考虑使用了命名空间的如下 XML 文档：

```xml
<?xml version="1.0" encoding="utf-8"?>
<top>
    <author>David Beazley</author>
    <content>
        <html xmlns="http://www.w3.org/1999/xhtml">
            <head>
                <title>Hello World</title>
            </header>
        </footer>
    </footer>
</footer> 
```

```txt
</head> <body> <h1>Hello World!</h1> </body> </html> </content> </top> 
```

如果解析这个文档并尝试执行普通的查询操作，就会发现没那么容易实现，因为所有的东西都变得特别冗长 嗦：啰

```txt
>>> # Some queries that work  
>>> doc.findtext('author')  
'David Beazley'  
>>> doc.find('content')  
Element 'content' at 0x100776ec0>  
>>> # A query involving a namespace (doesn't work)  
>>> doc.find('content/html')  
>>> # Works if fully qualified  
>>> doc.find('content/{http://www.w3.org/1999/xhtml}html')  
Element '{http://www.w3.org/1999/xhtml}html' at 0x1007767e0>  
>>> # Doesn't work  
>>> doc.findtext('content/{http://www.w3.org/1999/xhtml}html/head/title')  
>>> # Fully qualified  
>>> doc.findtext('content/{http://www.w3.org/1999/xhtml}html/'  
... '{http://www.w3.org/1999/xhtml}title')  
'Hello World'  
>>> 
```

通常可以将命名空间的处理包装到一个通用的类中，这样可以省去一些麻烦：

```python
class XMLNamespaces: def __init__(self, **kwargs): self-namespaces = {}. for name, url in kwargs.items(): self.register(name, url) def register(self, name, url): self-namespaces[name] = {''+url+''} def __call__(self, path): return path.format_map(self-namespaces) 
```

要使用这个类，可以按照下面的方式进行：

```txt
>>> ns = XMLNamespaces(htmL='http://www.w3.org/1999/xhtml')
>>> doc.find(ns('content/{html}html'))
Element '{http://www.w3.org/1999/xhtml}html' at 0x1007767e0>
>>> doc.findall(ns('content/{html}html/{html}head/{html}title'))
'Hello World'
>>> 
```

# 6.7.3 讨论

对包含有命名空间的 XML 文档进行解析会非常繁琐。XMLNamespaces 类的功能只是用来稍微简化一下这个过程，它允许在后序的操作中使用缩短的命名空间名称，而不必去使用完全限定的 URI。

不幸的是，在基本的ElementTree解析器中不存在什么机制能获得有关命名空间的进一步信息。但是如果愿意使用 iterparse()函数的话，还是可以获得一些有关正在处理的命名空间范围的信息。示例如下：

```txt
>>> from xml.etree(ElementTree import(IR) )
>>> for EVT, elem in(IR):
    print(vect, elem)
...
end <Element 'author' at 0x10110de10>
start-ns ('', 'http://www.w3.org/1999/xhtml')
end <Element '{http://www.w3.org/1999/xhtml}' at 0x1011131b0>
end <Element '{http://www.w3.org/1999/xhtml}' at 0x1011130a8>
end <Element '{http://www.w3.org/1999/xhtml}' at 0x101113310>
end <Element '{http://www.w3.org/1999/xhtml}' at 0x101113260>
end <Element '{http://www.w3.org/1999/xhtml}' at 0x10110df70>
end-ns None
end <Element 'content' at 0x10110de68>
end <Element 'top' at 0x10110dd60>
>>> elem # This is the topmost element
<Element 'top' at 0x10110dd60>
>>> 
```

最后要提到的是，如果正在解析的文本用到了除命名空间之外的其他高级XML 特性，那么最好还是使用 lxml 库。比方说，lxml 对文档的 DTD 验证、更加完整的 XPath 支持和其他的高级 XML 特性提供了更好的支持。本节提到的技术只是为解析操作做了一点修改，使得这个过程能够稍微简单一些。

# 6.8 同关系型数据库进行交互

# 6.8.1 问题

我们需要选择、插入或者删除关系型数据库中的行数据。

# 6.8.2 解决方案

在 Python 中，表达行数据的标准方式是采用元组序列。例如：

stocks $=$ [('GOOG',100,490.1),('AAPL',50,545.75),('FB',150,7.45),('HPQ',75,33.2),]

当数据以这种形式呈现时，通过 Python 标准的数据库 API（在 PEP 249 中描述）来同关系型数据库进行交互相对来说就显得很直接了。该 API 的要点就是数据库上的所有操作都通过 SQL 查询来实现。每一行输入或输出数据都由一个元组来表示。

为了说明，我们可以使用 Python 自带的 sqlite3 模块。如果正在使用一个不同的数据库（如 MySQL、Postgres 或者 ODBC），就需要安装一个第三方的模块来支持。但是，底层的编程接口即使不完全相同的话也几乎是一致的。

第一步是连接数据库。一般来说，要调用一个 connect()函数，提供类似数据库名称、主机名、用户名、密码这样的参数以及一些其他需要的细节。示例如下：

```txt
>>> import sqlite3
>>> db = sqlite3.connect('database.db')
>>> 
```

要操作数据的话，下一步就要创建一个游标（cursor）。一旦有了游标，就可以开始执行 SQL 查询了。示例如下：

```txt
>>> c = db.cursor()
>>> c.exec('create table portfolio (symbol text, shares integer, price real)')
<sqlite3.Cursor object at 0x10067a730>
>>> db.commit()
>>> 
```

要在数据中插入行序列，可以采用这样的语句：

```txt
>>> c.executemany('insert into portfolio values (?,?,?)', stocks) <sqlite3.Cursor object at 0x10067a730>   
>>> db.commit()   
>>> 
```

要执行查询操作，可以使用下面这样的语句：

```txt
>>> for row in db.execute('select * from portfolio'):  
... print(row)  
... 
```

```txt
('GOOG', 100, 490.1)  
('AAPL', 50, 545.75)  
('FB', 150, 7.45)  
('HPQ', 75, 33.2)  
>>> 
```

如果想执行的查询操作需要接受用户提供的输入参数，请确保用?隔开参数，就像下面的示例这样：

```python
>>> min_price = 100
>>> for row in db.execute('select * from portfolio where price >= ?', (min_price,)): 
...     print(row)
...
( 'GOOG', 100, 490.1) 
( 'AAPL', 50, 545.75) 
```

# 6.8.3 讨论

从较低的层次来看，同数据库的交互其实是一件非常直截了当的事。只要组成 SQL 语句然后将它们传给底层的模块就可以更新数据库或者取出数据了。尽管如此，这里还是有一些比较棘手的细节问题需要针对每种情况逐项考虑。

其中一个比较复杂的问题就是将数据库中的数据映射到 Python 的类型中。对于像日期这样的条目，最常见的是使用 datetime 模块中的 datetime 实例，或者也可能是 time 模块中用到的系统时间戳（system timestamps）。对于数值型的数据，尤其是涉及小数的金融数据，这些数字可以用 decimal 模块中的 Decimal 实例来表示。不幸的是，确切的映射关系会因数据库后端的不同而有所区别，因此必须去阅读相关的文档。

另一个极其重要的问题是需要考虑组成 SQL 语句的字符串。我们绝对不应该用 Python的字符串格式化操作符（即 $\%$ ）或者.format()方法来创建这种字符串。如果给这样的格式化操作符提供的值是来自于用户的输入，那么这就等于将你的程序敞开大门迎接SQL 注入攻击（参见 http://xkcd.com/327）。在查询操作中，特殊的?通配符会指示数据库后端启用自己的字符串替换机制，这样才能做到安全（希望如此）。

可悲的是，数据库后端对通配符的支持并不一致。有许多模块采用的是?或%s，而其他一些可能会使用不同的符号，比如用:0 或者:1 来代表参数。这里再次说明，必须查阅正在使用的数据库模块的文档资料。数据库模块的 paramstyle 属性中也包含有关于引用样式的相关信息。

对于简单地三趸将数据从数据库表项中取出和输入，使用数据库 API 通常足够了。如果要处理更加复杂的任务，那么使用一种更高层次的接口就显得很有意义了，比如那些提供有对象关系映射组件（object-relational mapper，ORM）的接口。像 SQLAlchemy

（http://www.sqlalchemy.org）这样的库允许数据库表项以 Python 类的形式来描述，在执行数据库操作时可隐藏大部分底层的 SQL。

# 6.9 编码和解码十六进制数字

# 6.9.1 问题

我们需要将十六进制数组成的字符串解码为字节流，或者将字节流编码为十六进制数。

# 6.9.2 解决方案

如果需要编码或解码由十六进制数组成的原始字符串，可以使用 binascii 模块。示例如下：

```txt
>>> # Initial byte string
>>> s = b'hello'
>>> # Encode as hex
>>> import binascii
>>> h = binascii.b2a.hex(s)
>>> h
b'68656c6c6f'
>>> # Decode back to bytes
>>> binascii.a2b_hex(h)
b'hello'
>>> 
```

同样的功能也可以在 base64 模块中找到。示例如下：

```txt
>>> import base64
>>> h = base64.b16encode(s)
>>> h
b'68656C6C6F'
>>> base64.b16decode(h)
b'hello'
>>> 
```

# 6.9.3 讨论

对于大部分情况而言，采用上面给出的函数对十六进制数进行转换都是简单直接的。这两种技术的主要区别在于大写转换。base64.b16decode()和 base64.b16encode()函数只能对大写形式的十六进制数进行操作，而 binascii 模块能够处理任意一种情况。

此外还需要重点提到的是编码函数产生的输出总是字节串。如果要将其强制转换为

Unicode 输出，可能需要增加一些额外的解码操作。示例如下：

```txt
>>> h = base64.b16encode(s)  
>>> print(h)  
b'68656C6C6F'  
>>> print(hdecode('ascii'))  
68656C6C6F  
>>> 
```

当解码十六进制数时，b16decode()和 a2b_hex()函数可接受字节串或 Unicode 字符串作为输入。但是，这些字符串中必须只能包含 ASCII 编码的十六进制数字。

# 6.10 Base64 编码和解码

# 6.10.1 问题

我们需要采用 Base64 编码来对二进制数据做编码解码操作。

# 6.10.2 解决方案

base64 模块中有两个函数——b64encode()和 b64decode()——它们正是我们所需要的。示例如下：

```txt
>>> # Some byte data  
>>> s = b'hello'  
>>> import base64  
>>> # Encode as Base64  
>>> a = base64.b64encode(s)  
>>> a  
b'aGVsbG8='  
>>> # Decode from Base64  
>>> base64.b64decode(a)  
b'hello'  
>>> 
```

# 6.10.3 讨论

Base64 编码只能用在面向字节的数据上，比如字节串和字节数组。此外，编码过程的输出总是一个字节串。如果将 Base64 编码的数据同 Unicode 文本混在一起，那么可能需要执行一个额外的解码步骤。示例如下：

```txt
>>> a = base64.b64encode(s).decode('ascii')  
>>> a 
```

```txt
'aGVsbG8='  
>>> 
```

当解码 Base64 数据时，字节串和 Unicode 文本字符串都可以作为输入。但是，Unicode字符串中只能包含 ASCII 字符才行。

# 6.11 读写二进制结构的数组

# 6.11.1 问题

我们想将数据编码为统一结构的二进制数组，然后将这些数据读写到 Python 元组中去。

# 6.11.2 解决方案

要同二进制数据打交道的话，我们可以使用 struct 模块。下面的示例将一列 Python 元组写入到一个二进制文件中，通过 struct 模块将每个元组编码为一个结构。

from struct import Struct   
def writeRecords(record, format, f): 'Write a sequence of tuples to a binary file of structures. record_struct $=$ Struct.format) for r in records: f.write(record_struct.pack(\*r))   
#Example   
if_name $= =$ main': records $= [ (1,2.3,4.5),$ $(6,7.8,9.0)$ $(12,13.4,56.7)]$ with open('data.b', 'wb') as f: writeRecords(record,'<idd',f)

如果要将这个文件重新读取为一列 Python 元组的话，有好几种方法可以实现。首先，如果打算按块以增量式的方式读取文件的话，可以按照下面的示例来实现：

from struct import Struct   
def readRecordsformat,f): record_struct $\equiv$ Struct.format) chunks $=$ iter( lambda: f.read(record_struct.size), b'') return (record_struct unpack(chunk) for chunk in chunks)

```python
Example  
if __name__ == __main__:  
    with open('data.b', 'rb') as f:  
        for rec in readRecords(<odd>, f):  
            # Process rec 
```

如果只想用一个 read()调用将文件全部读取到一个字节串中，然后再一块一块的做转换，那么可以编写如下的代码：

from struct import Struct   
def unpack_recordscformat,data): record_struct $=$ Struct(struct(struct) return (record_struct.unpack_from(data, offset) for offset in range(0, len(data), record_struct.size)) #Example if_name $\equiv$ 'main': with open('data.b', 'rb') as f: data $=$ f.read() for rec in unpack_recordsc'<idd', data): #Processrec

在这两种情况下得到的结果都是一个可迭代对象，它能够产生出之前保存在文件中的那些元组。

# 6.11.3 讨论

对于那些必须对二进制数据编码和解码的程序，我们常会用到 struct模块。要定义一个新的结构，只要简单地创建一个 Struct实例即可：

```txt
Little endian 32-bit integer, two double precision floats record_struct = Struct('<idd') 
```

结构总是通过一组结构化代码来定义，比如 i、d、f 等（参见 Python 的文档http://docs.python.org/3/library/struct.html）。这些代码同特定的二进制数据相对应，比如32 位整数、64 位浮点数、32 位浮点数等。而第一个字符<指定了字节序。在这个例子中表示为“小端序”。将字符修改为>就表示为大端序，或者用!来表示网络字节序。

得到的Struct实例有着多种属性和方法，它们可用来操纵那种类型的结构。size 属性包含了以字节为单位的结构体大小，这对于 I/O 操作来说是很有用的。pack()和 unpack()

方法是用来打包和解包数据的。示例如下：

```txt
>>> from struct import Struct
>>> record_struct = Struct('<idd')
>>> record_struct.size
20
>>> record_struct.pack(1, 2.0, 3.0)
b'\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00''
>>> record_struct unpack()
(1, 2.0, 3.0)
>>> 
```

有时候我们会发现 pack()和 unpack()会以模块级函数（module-level functions）的形式调用，就像下面的示例这样：

```txt
>>> import struct
>>> struct.pack('<idd', 1, 2.0, 3.0)
b'\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00''
>>> struct unpack('<idd', _)
(1, 2.0, 3.0)
>>> 
```

这么做行的通，但是比起创建一个单独的 Struct 实例来说还是显得不那么优雅，尤其是如果相同的结构会在代码中多处出现时。通过创建一个 Struct 实例，我们只用指定一次格式化代码，所有有用的操作都被漂亮地归组到了一起（通过实例方法来调用）。如果需要同结构打交道的话，这么做肯定会使得代码更容易维护（因为只需要修改一处即可）。

用来读取二进制结构的代码中涉及一些有趣而且优雅的编程惯用法（programmingidioms）。在函数read_records()中，我们用 iter()来创建一个迭代器，使其返回固定大小的数据块（参见5.8节）。这个迭代器会重复调用一个用户提供的可调用对象（即，lambda:f.read(record_struct.size)）直到它返回一个指定值为止（即，b''），此时迭代过程结束。

示例如下：

>>>f = open('data.b', 'rb')   
>>>chunks $=$ iter(lambda:f.read(20),b')   
>>>chunks   
<callable iterator object at 0x10069e6d0>   
>>>forCHKinchunks: print(chk)   
... b'\\01\\x00\\x00\\x00FFFFFF\\x02@\\x00\\x00\\x00\\x00\\x00\\x00\\x00"@'   
b'\\x06\\x00\\x00\\x0333333\\x1f@\\x00\\x00\\x00\\x00\\x00"@'   
b'\\x0c\\x00\\x00\\x00\\xcd\\xcc\\xcc\\xcc\\xcc\*@\\x9a\\x99\\x99\\x99YL@'   
>>>

创建一个可迭代对象的原因之一在于这么做允许我们通过一个生成器表达式来创建 records记录，就像解决方案中展示的那样。如果不采用这种方式，那么代码看起来就会像这样：

def readRecords.format, f): record_struct $=$ Struct(format) while True: ck $=$ f.read(record_struct.size) if chk $\equiv$ b': break yield record_struct unpack(chk) return records

在函数 unpack_records()中我们采用了另一种方法。这里使用的 unpack_from()方法对于从大型的二进制数组中提取出二进制数据是非常有用的，因为它不会创建任何临时对象或者执行内存拷贝动作。我们只需提供一个字节串（或者任意的数组），再加上一个字节偏移量，它就能直接从那个位置上将字段解包出来。

如果用的是 unpack()而不是 unpack_from()，那么需要修改代码，创建许多小的切片对象并且还要计算偏移量。示例如下：

def unpackRecords-format, data): record_struct $=$ Struct.format) return (record_struct.unpack(data[offset:offset + record_struct.size]) for offset in range(0, len(data), record_struct.size))

这个版本的实现除了读取数据变得更加复杂之外，还需要完成许多工作，因为它得计算很多偏移量，拷贝数据，创建小的切片对象。如果打算从已读取的大型字节串中解包出许多结构的话，那么 unpack_from()是更加优雅的方案。

我们可能会想在解包记录时利用 collections 模块中的 namedtuple 对象。这么做允许我们在返回的元组上设定属性名。示例如下：

```python
from collections import namedtuple  
Record = namedtuple('Record', ['kind', 'x', 'y'])  
with open('data.p', 'rb') as f:  
    records = (Record(*r) for r in readRecords(['<idd', f])  
for r in records:  
    print(r kind, r.x, r.y) 
```

如果正在编写一个需要同大量的二进制数据打交道的程序，最好使用像 numpy 这样的库。比如，与其将二进制数据读取到元组列表中，不如直接将数据读入到结构化的数组中，就像这样：

```txt
>>> import numpy as np 
```

```prolog
>>>f = open('data.b', 'rb')
>>>records = np.fromfile(f, dtype='<i,<d,<d')
>>>records
array([[1, 2.3, 4.5), (6, 7.8, 9.0), (12, 13.4, 56.7)], dtype=[[('f0', ' <i4'), ('f1', ' <f8'), ('f2', ' <f8')])
>>>records[0]
(1, 2.3, 4.5)
>>>records[1]
(6, 7.8, 9.0)
>>> 
```

最后但同样重要的是，如果面对的任务是从某种已知的文件结构中读取二进制数据（例如图像格式、shapefile、HDF5 等），请先检查是否已有相应的 Python 模块可用。没必的话就别去重复发明轮子了。

# 6.12 读取嵌套型和大小可变的二进制结构

# 6.12.1 问题

我们需要读取复杂的二进制编码数据，这些数据中包含有一系列嵌套的或者大小可变的记录。这种数据包括图片、视频、shapefile（zh.wikipedia.org/zh-cn/Shapefile）等。

# 6.12.2 解决方案

struct模块可用来编码和解码几乎任何类型的二进制数据结构。为了说明本节中提到的这种数据，假设我们有一个用Python数据结构表示的点的集合，这些点可用来组成一系列的三角形：

polys $= [ \begin{array}{l} \end{array} ]$ [ (1.0, 2.5), (3.5, 4.0), (2.5, 1.5)],  
[ (7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0)],  
[ (3.4, 6.3), (1.2, 0.5), (4.6, 9.2)],

现在假设要将这份数据编码为一个二进制文件，这个文件的文件头可以表示为如下的形式：

<table><tr><td>字节</td><td>类型</td><td>描述</td></tr><tr><td>0</td><td>int</td><td>文件代码（0x1234，小端）</td></tr><tr><td>4</td><td>double</td><td>x的最小值（小端）</td></tr><tr><td>12</td><td>double</td><td>y的最小值（小端）</td></tr><tr><td>20</td><td>double</td><td>x的最大值（小端）</td></tr><tr><td>28</td><td>double</td><td>y的最大值（小端）</td></tr><tr><td>36</td><td>int</td><td>三角形数量（小端）</td></tr></table>

紧跟在这个文件头之后的是一系列的三角形记录，每条记录编码为如下的形式：

<table><tr><td>字节</td><td>类型</td><td>描述</td></tr><tr><td>0</td><td>int</td><td>记录长度（N字节）</td></tr><tr><td>4-N</td><td>Points</td><td>(X,Y)坐标，以浮点数表示</td></tr></table>

要写入这个文件，可以使用如下的 Python 代码：

import struct   
import itertools   
def write_polysfilename, polys): # Determine bounding box flattened $=$ list(itertools.chain(\*polys)) min_x $=$ min(x for x,y in flattened) max_x $=$ max(x for x,y in flattened) min_y $=$ min(y for x,y in flattened) max_y $=$ max(y for x,y in flattened)   
with open(filename,'wb') as f: f.write(struct.pack('<idddi', 0x1234, min_x,min_y, max_x,max_y, len(poly))) for poly in polys: size $=$ len(poly)\*struct.calysize('<dd') f.write(struct.pack('<i'，size+4)) for pt in poly: f.write(struct.pack('<dd'，\*pt))   
#Call it with our polygon data   
write_polys('polys.bin'，polys)

要将结果数据回读的话，可以利用 struct.unpack()函数写出相似的代码，只是在编写的时候将所执行的操作反转即可（即，用 unpack()取代之前的 pack()）。示例如下：

import struct   
def read_polysfilename): with open(filename, 'rb') as f: #Read the header header $=$ f.read(40) file_code,min_x,min_y,max_x,max_y,num_polys $\equiv$ \struct unpack('<iddddi',header)

polys $= []$ for n in range(num_polys): pbytes, $=$ struct unpack('<i',f.read(4)) poly $= []$ for m in range(pbytes // 16): pt $=$ struct unpack('<dd',f.read(16)) poly.append(pt) polys.append(poly)   
return polys

尽管这份代码能够工作，但是其中混杂了一些 read 调用、对结构的解包以及其他一些细节，因此代码比较杂乱。如果用这样的代码去处理一个真正的数据文件，很快就会变的更加混乱。因此，很明显需要寻求其他的解决方案来简化其中的一些步骤，将程序员解放出来，把精力集中在更加重要的问题上。

在本节剩余的部分中，我们将逐步构建出一个用来解释二进制数据的高级解决方案，目的是让程序员提供文件格式的高层规范，而将读取文件以及解包所有数据的细节部分都隐藏起来。先提前给读者预警，本节后面的代码可能是本书中最为高级的示例，运用了多种面向对象编程和元编程的技术。请确保仔细阅读本节的讨论部分，并且需要来回翻阅其他章节，交叉参考。

首先，当我们读取二进制数据时，文件中包含有文件头和其他的数据结构是非常常见的。尽管 struct模块能够将数据解包为元组，但另一种表示这种信息的方式是通过类。下面的代码正是这么做的：

import struct   
class StructField: Descriptor representing a simple structure field def init_self, format, offset): self.format $\equiv$ format self-offset $=$ offset def get_self,instance,cls): if instance is None: return self else: r $=$ struct unpack_from(self.format, instance._buffer,self-offset) return r[0] if len(r) $= = 1$ else r   
class Structure: def init_self,bytedata):

代码中使用了描述符（descriptor）来代表每一个结构字段。每个描述符中都包含了一个 struct 模块可识别的格式代码（format）以及相对于底层内存缓冲区的字节偏移（offset）。在__get__()方法中，通过 struct.unpack_from()函数从缓冲区中解包出对应的值，这样就不用创建额外的切片对象或者执行拷贝动作了。

Structure类只是用作基类，它接受一些字节数据并保存在由 StructField 描述符所使用的底层内存缓冲区中。这样一来，在 Structure 类中用到的 memoryview()，意图就非常清楚了。

使用这份代码，现在就可以将结构定义为高层次的类，将前面表格中用来描述文件格式的信息都映射到类的定义中。示例如下：

```python
class PolyHeader (Structure):  
    file_code = StructField('<i', 0)  
    min_x = StructField('<d', 4)  
    min_y = StructField('<d', 12)  
    max_x = StructField('<d', 20)  
    max_y = StructField('<d', 28)  
    num_polys = StructField('<i', 36) 
```

下面的示例使用这个类来读取之前写入的三角形数据的文件头：

>>>f $\equiv$ open('polys.bin'，'rb')   
>>>phead $=$ PolyHeader(f.read(40))   
>>>phead.file_code $= = 0x1234$ True   
>>>phead.min_x   
0.5   
>>>phead.min_y   
0.5   
>>>phead.max_x   
7.0   
>>>phead.max_y   
9.2   
>>>phead.num_polys   
3   
>>>

这么做挺有趣的，但是这种方法还存在许多问题。第一，尽管得到了便利的类接口，但代码比较冗长，需要用户指定许多底层的细节（比如，重复使用 StructField、指定偏移量等）。得到的结果中，这个类也缺少一些常用的便捷方法，比如提供一种方式来计算结构的总大小。

任何时候当面对这种过于冗长的类定义时，都应该考虑使用类装饰器（classdecorator）或者元类（metaclass）。元类的功能之一是它可用来填充许多底层的实现细节，把这份负担从用户身上拿走。举个例子，考虑下面这个元类和稍微修改过的Structure 类：

```python
class StructureMeta(type):
    ...
Metaclass that automatically creates StructField descriptors
def __init__(self, clsname, bases, clsdict):
    fields = getattr(self, '__fields__],[])
    byte_order = ""
    offset = 0
    for format, fieldname in fields:
        if format.startswith(['<',>'',』',』',『@']):
            byte_order = format[0]
            format = format[1:] 
            format = byte_order + format
            setattr(self, fieldname, StructField.format, offset))
            offset += struct.calysize(method)
            setattr(self, 'struct_size', offset)
class Structure(metaclass=StructureMeta):
    def __init__(self, bytedata):
        self._buffer = bytedata
@classmethod
def from_file(cls, f):
    return cls(f.read(cls_struct_size)) 
```

使用这个新的 Structure 类，现在就可以像这样编写结构的定义了：

```python
class PolyHeader (Structure):  
    fields = [  
        ('<i', 'file_code'),  
        ('d', 'min_x'),  
        ('d', 'min_y'),  
        ('d', 'max_x'),  
        ('d', 'max_y'),  
        ('i', 'num_polys')  
    ] 
```

可以看到，现在的定义要简化得多。新增的类方法 from_file()也使得从文件中读取数据变得更加简单，因为现在不需要了解数据的结构大小等细节问题了。比如，现在可以这么做：

>>>f $=$ open('polys.bin'，'rb')   
>>>phead $=$ PolyHeader.from_file(f)   
>>>phead.file_code $\equiv = 0x1234$ True   
>>>phead.min_x   
0.5   
>>>phead.min_y   
0.5   
>>>phead.max_x   
7.0   
>>>phead.max_y   
9.2   
>>>phead.num_polys   
3   
>>>

一旦引入了元类，就可以为其构建更多智能化的操作。比如说，假设想对嵌套型的二进制结构提供支持。下面是对这个元类的修改，以及对新功能提供支持的描述符定义：

```python
class NestedStruct:   
```
Descriptor representing a nested structure   
```
def __init__(self, name, struct_type, offset): 
    self.name = name 
    self.struct_type = struct_type 
    self-offset = offset
def __get__(self, instance, cls): 
    if instance is None: 
        return self 
    else: 
        data = instance._buffer[self-offset: 
            self-offset+self.struct_type结构性量] 
        result = self.struct_type(data) 
        # Save resulting structure back on instance to avoid 
        # further recomputation of this step 
        setattr(self, self.name, result) 
        return result  
class StructureMeta(type):   
```
Metaclass that automatically creates StructField descriptors   
```
def __init__(self, clsname, bases, clsdict): 
```

```python
fields = getattr(self, 'fields', [],  
byte_order = ''  
offset = 0  
for format, fieldname in fields:  
    if isinstance.format, StructureMeta):  
        setattr(self, fieldname, NestedStruct(method, format, offset))  
        offset += format.struct_size  
    else:  
        if format.startswith(['<', '> ', '!', '@')]:  
            byte_order = format[0]  
            format = format[1:]  
            format = byte_order + format  
            setattr(self, fieldname, StructField(method, offset))  
            offset += struct.calysize(format)  
setattr(self, 'struct_size', offset) 
```

在这份代码中，NestedStruct描述符的作用是在一段内存区域上定义另一个结构①。这是通过在原内存缓冲区中取一个切片，然后在这个切片上实例化给定的结构类型来实现的。由于底层的内存缓冲区是由 memoryview 来初始化的，因此这个切片操作不会涉及任何额外的内存拷贝动作。相反，它只是在原来的内存中“覆盖”上新的结构实例。此外，要避免重复的实例化动作，这个描述符会利用8.10节中提到的技术将内层结构对象保存在该实例上。

使用这种新的技术，现在就可以像这样编写代码了：

```python
class Point (Structure):
    _fields_ = [
        ('<d', 'x'),
        ('d', 'y')
    ]
class PolyHeader (Structure):
    _fields_ = [
        ('<i', 'file_code'),
        (Point, 'min'), # nested struct
        (Point, 'max'), # nested struct
        ('i', 'num_polys')
    ] 
```

太神奇了，一切都还是按照所期望的方式正常运转。示例如下：

```txt
>>> f = open('polys.bin', 'rb')
>>> phead = PolyHeader.from_file(f)
>>> phead.file_code == 0x1234
True 
```

```txt
>>>phead.min # Nested structure   
<__main__.Point object at 0x1006a48d0>   
>> phead.min.x   
0.5   
>> phead.min.y   
0.5   
>> phead.max.x   
7.0   
>> phead.max.y   
9.2   
>> phead.num_polys   
3   
>> 
```

到目前为止，我们已经成功开发了一个用来处理固定大小记录的框架。但是对于大小可变的组件又该如何处理呢？比如说，这份三角形数据文件的剩余部分中包含有大小可变的区域。

一种处理方法是编写一个类来简单代表一块二进制数据，并附带一个通用函数来负责以不同的方式来解释数据的内容。这和 6.11 节中的代码关系紧密：

```python
class SizedRecord: def __init__(self, bytedata): self._buffer = memoryview(bytedata) @ class method def from_file(cls, f, size_fmt, includes_size=True): sz_nbytes = struct.calysize(size_fmt) sz_bytes = f.read(sz_nbytes) sz, = struct unpack(size_fmt, sz_bytes) buf = f.read(sz - includes_size * sz_nbytes) return cls(buf) def iter_as(self, code): if isinstance(code, str): s = struct.Struct(code) for off in range(0, len(self._buffer), s.size): yield s.unpack_from(self._buffer, off) elif isinstance(code, StructureMeta): size = code.struct_size for off in range(0, len(self._buffer), size): data = self._buffer[off:off+size] yield code(data) 
```

这里的类方法 SizedRecord.from_file()是一个通用的函数，用来从文件中读取大小预定

好的数据块，这在许多文件格式中都是很常见的。对于输入参数，它可接受结构的格式代码，其中包含有编码的大小（以字节数表示）。可选参数 includes_size 用来指定字节数中是否要包含进文件头的大小。下面的示例展示如何使用这份代码来读取三角形数据文件中那些单独的三角形：

>>>f $\equiv$ open('polys.bin', 'rb')   
>>>phead $=$ PolyHeader.from_file(f)   
>>>phead.num_polys   
3   
>>>polydata $=$ [ SizedRecord.from_file(f,'<i') ... for n in range(phead.num_polys)]   
>>>polydata   
\[<  _{\text{主}}\] .SizedRecord object at 0x1006a4d50>, <  _{\text{主}}\] .SizedRecord object at 0x1006a4f50>, <_main_.SizedRecord object at 0x10070da90>]   
>>>

可以看到，SizedRecord 实例的内容还没有经过解释。要做到这一点，可以使用 iter_as()方法。该方法可接受一个结构格式代码或者 Structure 类作为输入。这给了我们极大的自由来选择如何解释数据。比如：

```python
>>> for n, poly in enumerate(polydata):
...     print('Polygon', n)
...     for p in poly.iterator_as('<dd':
...     print(p)
...
Polygon 0
(1.0, 2.5)
(3.5, 4.0)
(2.5, 1.5)
Polygon 1
(7.0, 1.2)
(5.1, 3.0)
(0.5, 7.5)
(0.8, 9.0)
Polygon 2
(3.4, 6.3)
(1.2, 0.5)
(4.6, 9.2)
>>> 
>>> for n, poly in enumerate(polydata):
...     print('Polygon', n)
...     for p in poly.iterator_as(Point):
...     print(p.x, p.y) 
```

```txt
...   
Polygon 0   
1.02.5   
3.54.0   
2.51.5   
Polygon 1   
7.01.2   
5.13.0   
0.57.5   
0.89.0   
Polygon 2   
3.46.3   
1.20.5   
4.69.2   
>>> 
```

现在我们把所有的东西结合起来。下面是 read_polys()函数的另一种实现：

```python
class Point (Structure):
    _fields_ = [
        ('<d', 'x'),
        ('d', 'y')
    ]
class PolyHeader (Structure):
    _fields_ = [
        ('<i', 'file_code'),
        (Point, 'min'),
        (Point, 'max'),
        ('i', 'num_polys')
    ]
def read_polys (filename):
    polys = []
with open (filename, 'rb') as f:
    phead = PolyHeader.from_file(f)
    for n in range (phead.num_polys):
        rec = SizedRecord.from_file(f, ':i')
        poly = [(p.x, p.y)
        for p in rec_iter_as(Point)]
        polys.append(poly)
    return polys 
```

# 6.12.3 讨论

本节展示了多种高级编程技术的实际应用，这些技术包括描述符、惰性求值、元类、

类变量以及 memoryview。只是它们都用于一个非常具体的目的而已。

本节给出的实现中，一个非常重要的特性就是强烈基于惰性展开（lazy-unpacking）的思想。每当创建出一个 Structure 实例时，__init__()方法只是根据提供的字节数据创建出一个 memoryview，除此之外别的什么都不做。具体而言就是这个时候不会进行任何的解包或其他与结构相关的操作。采用这种方法的一个动机是我们可能只对二进制记录中的某几个特定部分感兴趣。与其将整个文件解包展开，不如只对实际要访问到的那几个部分解包即可。

要实现惰性展开和对值进行打包，StructField 描述符就派上用场了。用户在_fields_中列出的每个属性都会转换为一个 StructField 描述符，它保存着相关属性的结构化代码和相对于底层内存缓冲区的字节偏移量。当我们定义各种各样的结构化类型时，元类StructureMeta 用来自动创建出这些描述符。使用元类的主要原因在于这么做能以高层次的描述来指定结构的格式，完全不用操心底层的细节问题，因而能极大地简化用户的操作。

元类StructureMeta中有一个微妙的方面需要注意，那就是它将字节序给规定死了。也就说，如果有任何属性指定了字节序（<指代小端序，而>指代大端序），那么这个字节序就适用于该属性之后的所有字段。这种行为可避免我们产生额外的键盘输入，同时也使得在定义字段时可以切换字节序。比如说，我们可能会碰到下面这样更加复杂的数据：

```python
class ShapeFile (Structure):  
    _fields_ = [ ('>i', 'file_code'), # Big endian 这里是大端, 后两个属性都是大端  
        ('20s', 'unused'),  
        ('i', 'file_length'),  
        ('<i', 'version'), # Little endian 切换为小端, 后续所有的属性都是小端  
        ('i', 'shape_type'),  
        ('d', 'min_x'),  
        ('d', 'min_y'),  
        ('d', 'max_x'),  
        ('d', 'max_y'),  
        ('d', 'min_z'),  
        ('d', 'max_z'),  
        ('d', 'min_m'),  
        ('d', 'max_m') ] 
```

前文中提到，解决方案中对 memoryview()的使用起到了避免内存拷贝的作用。当结构数据开始出现嵌套时，memoryview可用来在相同的内存区域中覆盖上不同的结构定义。这种行为十分微妙，它考虑到了切片操作在 memoryview 和普通的字节数组上的不同行为。如果对字节串或字节数组执行切片操作的话，通常都会得到一份数据的拷贝，但memoryview就不会这样——切片只是简单地覆盖在已有的内存之上。因此，这种方法更加高效。

还有一些相关的章节会帮助我们对解决方案中用到的技术进行扩展。8.13 节中采用描述符构建了一个类型系统。8.10 节中介绍了有关惰性计算的性质，这个和 NestedStruct描述符的实现有一定的相关性。9.19 节中有一个例子采用元类来初始化类的成员，这个和 StructureMeta 类采用的方式非常相似。我们可能也会对 Python 标准库中 ctypes 模块的源代码产生兴趣，因为它对定义数据结构、对数据结构的嵌套以及类似功能的支持和我们的解决方案比较相似。

# 6.13 数据汇总和统计

# 6.13.1 问题

我们需要在大型数据库中查询数据并由此生成汇总或者其他形式的统计数据。

# 6.13.2 解决方案

对于任何涉及统计、时间序列以及其他相关技术的数据分析问题，都应该使用 Pandas库（http://pandas.pydata.org）。

为了小试牛刀，下面这个例子使用 Pandas 来分析芝加哥的老鼠和啮齿动物数据库（https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Rodent-Baiting/97t6-zrhs）。在写作本书时，这个 CSV 文件中有大约 74 000 条数据：

```python
>>> import pandas
>>> # Read a CSV file, skipping last line
>>> rats = pandas.read_csv('rats.csv', skip_footer=1)
>>> rats
<class 'pandas.core.frame.DataFrame>
Int64Index: 74055 entries, 0 to 74054
Data columns:
Creation Date 74055 non-null values
Status 74055 non-null values
Completion Date 72154 non-null values
Service Request Number 74055 non-null values
Type of Service Request 74055 non-null values
Number of Premises Baited 65804 non-null values
Number of Premises with Garbage 65600 non-null values
Number of Premises with Rats 65752 non-null values
Current Activity 66041 non-null values
Most Recent Action 66023 non-null values
Street Address 74055 non-null values
ZIP Code 73584 non-null values
X Coordinate 74043 non-null values 
```

Y Coordinate 74043 non-null values   
Ward 74044 non-null values   
Police District 74044 non-null values   
Community Area 74044 non-null values   
Latitude 74043 non-null values   
Longitude 74043 non-null values   
Location 74043 non-null values   
dtypes: float64(11), object(9)   
>>> # Investigate range of values for a certain field   
>>> rats['Current Activity'].unique()   
array([nan, Dispatch Crew, Request Sanitation Inspector], dtype $\equiv$ object)   
>>> # Filter the data   
>>> crew_dispached $=$ rats[rats['Current Activity'] $= =$ 'Dispatch Crew']   
>>> len(crew_dispached)   
65676   
>>>   
>>> # Find 10 most rat-infested ZIP codes in Chicago   
>>> crew_dispached['ZIP Code'].value_counts()[10]   
60647 3837   
60618 3530   
60614 3284   
60629 3251   
60636 2801   
60657 2465   
60641 2238   
60609 2206   
60651 2152   
60632 2071   
>>>   
>>> # Group by completion date   
>>> dates $=$ crew_dispached.groupby('Completion Date')   
<spandas.core.groupby.DataFrameGroupBy object at 0x10d0a2a10>   
>>> len(dates)   
472   
>>>   
>>> # Determine counts on each day   
>>> date_counts $=$ dates.size()   
>>> date_counts[0:10]   
Completion Date   
01/03/2011 4   
01/03/2012 125

```txt
01/04/2011 54  
01/04/2012 38  
01/05/2011 78  
01/05/2012 100  
01/06/2011 100  
01/06/2012 58  
01/07/2011 1  
01/09/2012 12  
>>>  
>>> # Sort the counts  
>>> date_counts.sort()  
>>> date_counts[-10:]  
Completion Date  
10/12/2012 313  
10/21/2011 314  
09/20/2011 316  
10/26/2011 319  
02/22/2011 325  
10/26/2012 333  
03/17/2011 336  
10/13/2011 378  
10/14/2011 391  
10/07/2011 457  
>>> 
```

你没看错，2011 年 10 月 7 号对于老鼠来说的确是非常忙碌的一天。

# 6.13.3 讨论

Pandas 是一个庞大的库，它还有更多的功能，但我们无法在此一一描述。但是，如果需要分析大型的数据集、将数据归组、执行统计分析或者其他类似的任务，那么 Pandas绝对值得一试。

由 Wes McKinney 所著的 Python for Data Analysis（O’Reilly）一书中也包含了更多的内容。

用 def 语句定义的函数是所有程序的基石。本章的目的是向读者展示一些更加高级和独特的函数定义以及使用模式。主题包括默认参数、可接受任意数量参数的函数、关键字参数、参数注解以及闭包。此外，有关利用回调函数实现巧妙的控制流以及数据传递的问题也有涉及。

# 7.1 编写可接受任意数量参数的函数

# 7.1.1 问题

我们想编写一个可接受任意数量参数的函数。

# 7.1.2 解决方案

要编写一个可接受任意数量的位置参数的函数，可以使用以*开头的参数。示例如下：

```python
def avg(first, *rest):
return (first + sum(rest)) / (1 + len(rest))
# Sample use
avg(1, 2)      # 1.5
avg(1, 2, 3, 4)    # 2.5 
```

在这个示例中，rest是一个元组，它包含了其他所有传递过来的位置参数。代码在之后的计算中会将其视为一个序列来处理。

如果要接受任意数量的关键字参数，可以使用以**开头的参数。示例如下：

```txt
import html 
```

```python
def make_element(name, value, **attrs):  
    keyvals = ['%s=%s'] % item for item in attrs.items()  
    attr_str = '.join(keyvals)  
    element = '< {name} {attrs} >{value}</ {name}>' format (  
        name=name,  
        attr=attr_str,  
        value=html.escape(value))  
    return element  
# Example  
# Creates `<item size="large" quantity="6">Albatross</item>'  
make_element('item', 'Albatross', size='large', quantity=6)  
# Creates '&lt;spam&gt; </p>'  
make_element('p', '&lt;spam&gt') 
```

在这里 attrs 是一个字典，它包含了所有传递过来的关键字参数（如果有的话）。

如果想要函数能同时接受任意数量的位置参数和关键字参数，只要联合使用*和**即可。示例如下：

```txt
def anyargs(*args, **kwargs): print(args) # A tuple print ( kwargs) # A dict 
```

在这个函数中，所有的位置参数都会放置在元组 args 中，而所有的关键字参数都会放置在字典 kwargs 中。

# 7.1.3 讨论

在函数定义中，以*打头的参数只能作为最后一个位置参数出现，而以**打头的参数只能作为最后一个参数出现。在函数定义中存在一个很微妙的特性，那就是在*打头的参数后仍然可以有其他的参数出现。

```txt
def a(x, \*args，y)： pass   
def b(x,\*args，y,\*\*kwargs）： pass
```

这样的参数称之为 keyword-only参数（即，出现在*args 之后的参数只能作为关键字参数使用）。7.2 节中会做进一步的讨论。

# 7.2 编写只接受关键字参数的函数

# 7.2.1 问题

我们希望函数只通过关键字的形式接受特定的参数。

# 7.2.2 解决方案

如果将关键字参数放置在以*打头的参数或者是一个单独的*之后，这个特性就很容易实现。示例如下：

```python
def recv(maxsize, *, block):
    'Receives a message'
    pass
recv(1024, True)  # TypeError
recv(1024, block=True)  # Ok 
```

这项技术也可以用来为那些可接受任意数量的位置参数的函数来指定关键字参数。示例如下：

```python
def minimum(*values, clip=None):  
    m = min(values)  
    if clip is not None:  
        m = clip if clip > m else m  
    return m  
minimum(1, 5, 2, -5, 10) # Returns -5  
minimum(1, 5, 2, -5, 10, clip=0) # Returns 0 
```

# 7.2.3 讨论

当指定可选的函数参数时，keyword-only 参数常常是一种提高代码可读性的好方法。比如，考虑下面这个调用：

```txt
msg = recv(1024, False) 
```

如果某些人不熟悉 recv()的工作方式，他们可能会搞不清楚这里的 False 参数到底表示了什么意义。而另一方面，如果这个调用可以写成下面这样的话，那就显得清晰多了：

```txt
msg = recv(1024, block=False) 
```

在有关**kwargs 的技巧中，使用 keyword-only 参数常常也是很可取的。因为当用户请

求帮助信息时，它们可以适时地显示出来：

```txt
>>> help(recv)  
Help on function recv in module __main__:  
recv(maxsize, *, block)  
Receives a message 
```

keyword-only 参数在更加高级的上下文环境中同样也能起到作用。比如说，可以用来为函数注入参数，这些函数利用*args 和**kwargs接受所有的输入参数。可参见 9.11节中的示例。

# 7.3 将元数据信息附加到函数参数上

# 7.3.1 问题

我们已经编写好了一个函数，但是希望能为参数附加上一些额外的信息，这样其他人可以对函数的使用方法有更多的认识和了解。

# 7.3.2 解决方案

函数的参数注解可以提示程序员该函数应该如何使用，这是很有帮助的。比如说，考虑下面这个带参数注解的函数：

```erlang
def add(x:int, y:int) -> int: return x + y 
```

Python 解释器并不会附加任何语法意义到这些参数注解上。它们既不是类型检查也不会改变 Python 的行为。但是，参数注解会给其他阅读源代码的人带来有用的提示。一些第三方工具和框架可能也会为注解加上语法含义。这些注解也会出现在文档中：

```txt
>>> help(add)  
Help on function add in module __main_:  
add(x: int, y: int) -> int  
>>> 
```

尽管可以将任何类型的对象作为函数注解附加到函数定义上（比如，数字、字符串、实例等），但是通常只有类和字符串才显得最有意义。

# 7.3.3 讨论

函数注解只会保存在函数的__annotations__属性中。示例如下：

```txt
>>>add._annotations_   
{'y':<class 'int>'，'return':<class 'int>'，'x':<class 'int'>} 
```

尽管函数注解有着许多潜在的用途，但它们的主要功能也许就是丰富一下文档内容了。因为 Python 中并没有类型声明，所以如果只是简单地阅读一下源代码就想知道打算给函数传递什么对象常常是比较困难的。函数注解就可以带给我们更多的提示。

请参见 9.20 节中的高级示例，那个例子展示了如何利用函数注解来实现函数重载。

# 7.4 从函数中返回多个值

# 7.4.1 问题

我们想从函数中返回多个值。

# 7.4.2 解决方案

要从函数中返回多个值，只要简单地返回一个元组即可。示例如下：

>>> def myfun(): return 1,2,3   
>>>a,b,c $=$ myfun()   
>>>a   
1   
>>>b   
2   
>>>c   
3

# 7.4.3 讨论

尽管看起来 myFun()返回了多个值，但实际上它只创建了一个元组而已。这看起来有点奇怪，但是实际上元组是通过逗号来组成的，不是那些圆括号。示例如下：

```txt
>>> a = (1, 2) #With parentheses   
>>>a   
(1,2)   
>>>b=1,2 #Without parentheses   
>>>b   
(1,2)   
>>> 
```

当调用的函数返回了元组，通常会将结果赋值给多个变量，就像示例中那样。实际上这就是简单的元组解包，我们在1.1 节中就已经提到过了。返回的值也可以只赋给一个单独的变量：

```txt
>>> x = myfun ()  
>>> x  
(1, 2, 3)  
>>> 
```

这样 x 就代表整个元组。

# 7.5 定义带有默认参数的函数

# 7.5.1 问题

我们想定义一个函数或者方法，其中有一个或多个参数是可选的并且带有默认值。

# 7.5.2 解决方案

表面上看定义一个带有可选参数的函数是非常简单的——只需要在定义中为参数赋值，并确保默认参数出现在最后即可。示例如下：

```python
def spam(a, b=42):
    print(a, b)
spam(1) # Ok. a=1, b=42
spam(1, 2) # Ok. a=1, b=2 
```

如果默认值是可变容器的话，比如说列表、集合或者字典，那么应该把 None 作为默认值，代码应该像这样编写：

```python
Using a list as a default value def spam(a, b=None): if b is None: b = [] 
```

如果不打算提供一个默认值，只是想编写代码来检测可选参数是否被赋予了某个特定的值，那么可以采用下面的惯用手法：

_no_value $=$ object()   
def spam(a,b $\equiv$ _no_value): if b is _no_value: print('No b value supplied')

这个函数的行为是这样的：

>>> spam(1)   
No b value supplied   
>>> spam(1, 2) # $b = 2$ >>> spam(1, None) # $b = \text{None}$ >>>

请仔细区分不传递任何值和传递 None 之间的区别。

# 7.5.3 讨论

定义带有默认参数的函数看似很容易，但其实并不像看到的那么简单。

首先，对默认参数的赋值只会在函数定义的时候绑定一次。可用下面这个例子做下试验：

```txt
>> x = 42  
>>> def spam(a, b=x):  
... print(a, b)  
...  
>>> spam(1)  
1 42  
>>> x = 23 # Has no effect  
>>> spam(1)  
1 42  
>>> 
```

注意到修改变量x 的值（x被作为函数参数的默认值）并没有对函数产生任何效果。这是因为默认值已经在函数定义的时候就确定好了。

其次，给默认参数赋值的应该总是不可变的对象，比如 None、True、False、数字或者字符串。特别要注意的是，绝对不要编写这样的代码：

```txt
def spam(a, b = [], # NO! 
```

如果这么做了就会陷入到各种麻烦之中。如果默认值在函数体之外被修改了，那么这种修改将在之后的函数调用中对参数的默认值产生持续的影响。示例如下：

```txt
>>> def spam(a, b=[]):  
... print(b)  
... return b  
...  
>>> x = spam(1)  
>>> x  
[ ]  
>>> x.append(99)  
>>> x.append('Yow!') 
```

```txt
>>>x   
[99,'Yow!']   
>>> spam(1) #Modified list gets returned!   
[99,'Yow!']   
>>> 
```

这很可能不是所期望的结果。要避免出现这种问题，最好按照解决方案中的做法，使用 None 作为默认值并在函数体中增加一个对默认值的检查。

当检测默认参数是否为 None 时，本节示例的关键之处在于对 is 操作符的运用。有时候人们会犯这样的错误：

```python
def spam(a, b=None):
    if not b: # NO! Use 'b is None' instead
        b = []
... 
```

这里出现的问题在于尽管 None 会被判定为 False，可是还有许多其他的对象（比如长度为 0 的字符串、列表、元组、字典等）也存在这种行为。因此，上面示例给出的条件检测会将某些特定的输入也判定为 False，从而错误地忽略掉这些输入值。示例如下：

```txt
>>> spam(1) # OK  
>>> x = []  
>>> spam(1, x) # Silent error. x value overwritten by default  
>>> spam(1, 0) # Silent error. 0 ignored  
>>> spam(1, '') # Silent error. '' ignored  
>>> 
```

本节最后讨论的内容更加巧妙——在函数中检测是否对可选参数提供了某个特定值（可以是任意值）。这里最为棘手的地方在于我们不能用 None、0 或者 False当做默认值来检测用户是否提供了参数（因为所有这些值都是完全合法的参数，用户极有可能将它们当做参数）。因此，需要用其他的办法来检测。

要解决这个问题，可以利用 object()创建一个独特的私有实例，就像解决方案中给出的那样（即，变量_no_value）。在函数中，可以用这个特殊值来同用户提供的参数做相等性检测，以此判断用户是否提供了参数。这里主要考虑到对于用户来说，把_no_value实例作为输入参数几乎是不可能的。因此，如果要判断用户是否提供了某个参数，_no_value 就成了一个可以用来安全比较的值。

这里用到的 object()可能看起来很不常见。object 作为 Python 中几乎所有对象的基类而存在。可以创建 object 的实例，但是它们没有任何值得注意的方法，也没有任何实例数据，因此一般来说我们对它是毫无兴趣的（因为底层缺少__dict__字典，我们甚至没法为它设置任何属性）。唯一可做的就是检测相等性，这也使得它们可作为特殊值来使

用，就像我们给出的解决方案中那样。

# 7.6 定义匿名或内联函数

# 7.6. 1 问题

我们需要提供一个短小的回调函数为 sort()这样的操作所用，但是又不想通过 def 语句编写一个单行的函数。相反，我们更希望能有一种简便的方式来定义“内联”式的函数。

# 7.6.2 解决方案

像这种仅仅完成表达式求值的简单函数可以通过 lambda 表达式来替代。示例如下：

```txt
>>> add = lambda x, y: x + y  
>>> add(2,3)  
5  
>>> add('hello', 'world')  
'helloworld'  
>>> 
```

这里用到的 lambda 表达式与下面的函数定义有着相同的功能：

```txt
>>> def add(x, y):
...
...
return x + y
...
>>> add(2, 3)
5
>>> 
```

一般来说，lambda 表达式可用在如下的上下文环境中，比如排序或者对数据进行整理时：

>>>names $=$ ['David Beazley'，'Brian Jones', Raymond Hettinger'，'Ned Batchelder']   
>>>sorted names, key= lambda name: name.split()[-1].lower()) ['Ned Batchelder'，'David Beazley'，'Raymond Hettinger'，'Brian Jones']   
>>>

# 7.6.3 讨论

尽管 lambda表达式允许定义简单的函数，但它的局限性也很大。具体来说，我们只能指定一条单独的表达式，这个表达式的结果就是函数的返回值。这意味着其他的语言特性比如多行语句、条件分支、迭代和异常处理统统都无法使用。

不使用 lambda 表达式也可以愉快地编写出大量的 Python 代码。但是，还是时不时会在一些程序中见到 lambda 的身影。比如有的人会编写很多微型函数来对各种表达式进行求值，或者在需要用户提供回调函数的时候，这时 lambda 表达式就能派上用场了。

# 7.7 在匿名函数中绑定变量的值

# 7.7. 1 问题

我们利用 lambda 表达式定义了一个匿名函数，但是也希望可以在函数定义的时候完成对特定变量的绑定。

# 7.7.2 解决方案

考虑下列代码的行为：

```txt
>> x = 10  
>> a = lambda y: x + y  
>> x = 20  
>> b = lambda y: x + y  
>> 
```

现在请问自己一个问题，a(10)和 b(10)的结果是多少？如果觉得结果是 20 和 30 的话，那就错了：

```txt
>> a(10)  
30  
>> b(10)  
30  
>>> 
```

这里的问题在于 lambda 表达式中用到的 $\mathbf { X }$ 是一个自由变量，在运行时才进行绑定而不是定义的时候绑定。因此，lambda 表达式中 $\mathbf { X }$ 的值应该是在执行时确定的，执行时 x的值是多少就是多少。示例如下：

```txt
>> x = 15  
>> a(10)  
25  
>> x = 3  
>> a(10)  
13  
>>> 
```

如果希望匿名函数可以在定义的时候绑定变量，并保持值不变，那么可以将那个值作

为默认参数实现，就像下面这样：

```txt
>>> x = 10  
>>> a = lambda y, x=x: x + y  
>>> x = 20  
>>> b = lambda y, x=x: x + y  
>>> a(10)  
20  
>>> b(10)  
30  
>>> 
```

# 7.7.3 讨论

本节中提到的问题一般比较容易出现在那些对 lambda函数过于“聪明”的应用上。比方说，通过列表推导来创建一列 lambda表达式，或者在一个循环中期望 lambda表达式能够在定义的时候记住迭代变量。示例如下：

>>>funcs $=$ [lambda x: $\mathbf{x} + \mathbf{n}$ for n in range(5)]   
>>>for f infuncs: print(f(0))   
4   
4   
4   
4   
4   
>>>

我们可以注意到所有的函数都认为n的值为 4，也就是迭代中的最后一个值。我们再和下面的代码做下对比：

>>>funcs $=$ [lambda x, $\mathrm{n = n}$ : $\mathbf{x} + \mathbf{n}$ for n in range(5)]   
>>>for f infuncs: . print(f0))   
0   
1   
2   
3   
4   
>>>

可以看到，现在函数可以在定义的时候捕获到 n 的值了。

# 7.8 让带有 N 个参数的可调用对象以较少的参数形式调用

# 7.8.1 问题

我们有一个可调用对象可能会以回调函数的形式同其他的 Python 代码交互。但是这个可调用对象需要的参数过多，如果直接调用的话会产生异常。

# 7.8.2 解决方案

如果需要减少函数的参数数量，应该使用 functools.partial()。函数 partial()允许我们给一个或多个参数指定固定的值，以此减少需要提供给之后调用的参数数量。为了说明这个过程，假设有这么一个函数：

```python
def spam(a, b, c, d):
    print(a, b, c, d) 
```

现在考虑用 partial()来对参数赋固定的值：

```txt
>>> from functools import partial
>>> s1 = partial(spam, 1)      # a = 1
>>> s1(2, 3, 4)
1 2 3 4
>>> s1(4, 5, 6)
1 4 5 6
>>> s2 = partial(spam, d=42)    # d = 42
>>> s2(1, 2, 3)
1 2 3 42
>>> s2(4, 5, 5)
4 5 5 42
>>> s3 = partial(spam, 1, 2, d=42)    # a = 1, b = 2, d = 42
>>> s3(3)
1 2 3 42
>>> s3(4)
1 2 4 42
>>> s3(5)
1 2 5 42
>>> 
```

我们可以观察到 partial()对特定的参数赋了固定值并返回了一个全新的可调用对象。这个新的可调用对象仍然需要通过指定那些未被赋值的参数来调用。这个新的可调用对象将传递给 partial()的固定参数结合起来，统一将所有的参数传递给原始的函数。

# 7.8.3 讨论

本节提到的技术对于将看似不兼容的代码结合起来使用是大有裨益的。下面我们用一系列的示例来帮助理解。

第一个例子是，假设有一列以元组(x, y)来表示的点坐标。可以用下面的函数来计算两点之间的距离：

points $= [1,2)$ ,(3,4),(5,6),78)   
import math   
def distance(p1,p2): x1,y1 $\equiv$ p1 $\mathrm{x2}$ ,y2 $\equiv$ p2 returnmath.hypot(x2-x1,y2-y1)

现在假设想根据这些点之间的距离来对它们排序。列表的 sort()方法可接受一个 key 参数，它可用来做自定义的排序处理。但是它只能和接受单参数的函数一起工作（因此和 distance()是不兼容的）。下面我们用 partical()来解决这个问题：

```prolog
>>> pt = (4, 3)  
>>> points.sort(key=partial(distance, pt))  
>>> points  
[(3, 4), (1, 2), (5, 6), (7, 8)]  
>>> 
```

我们可以对这个思路进行扩展，partial()常常可用来调整其他库中用到的回调函数的参数签名。比方说，这里有一段代码利用 multiprocessing 模块以异步方式计算某个结果，并将这个结果传递给一个回调函数。该回调函数可接受这个结果以及一个可选的日志参数：

```python
def output_result(result, log=None):
    if log is not None:
        log.debug('Got: %r', result)
# A sample function
def add(x, y):
    return x + y
if __name__ == '__main__':
    import logging
    from multiprocessing import Pool
    from functools import partial
    logging-basicConfig(level=logging DEBUG) 
```

```python
log = logging.getLogger('test')  
p = Pool()  
p.apply_async(add, (3, 4), callback=partial(output_result, log=log))  
p.close()  
p.join() 
```

当我们在 apply_async()中指定回调函数时，额外的日志参数是通过 partical()来指定的。multiprocessing 模块对于这些细节根本一无所知——它只通过单个参数来调用回调函数。

作为类似的例子，考虑一下我们在编写网络服务器程序时面对的问题。有了 socketserver模块，这一切相对来说都变得很简单了。比方说，下面有一个简单的 echo 服务程序：

```python
from socketserver import RequestHandler, TCPServer 
```

```python
class EchoHandler StreamRequestHandler): def handle(self): for line in self.rfile: self.wfile.write(b'GOT:' + line) serv = TCPServer('', 15000), EchoHandler) servServe_forever() 
```

现在，假设我们想在EchoHandler 类中增加一个__init__()方法，让它接受一个额外的配置参数。示例如下：

class EchoHandler StreamRequestHandler): # ack is added keyword-only argument. \*args, \*\*kwarges are # any normal parameters supplied (which are passed on) def __init__(self,\*args,ack,\*\*kwarges): self.ack $=$ ack super()._init_(\*args,\*\*kwarges) def handle(self): for line in self.rfile: self.wfile.write(self.ack $^+$ line)

如果做了上述改动，现在就会发现没法简单地将其插入到 TCPServer 类中了。事实上，你会发现代码会产生如下的异常：

```txt
Exception happened during processing of request from ('127.0.0.1', 59834)  
Traceback (most recent call last):  
...  
TypeError: __init__() missing 1 required keyword-only argument: 'ack' 
```

初看上去，除了修改 socketserver 的源代码或者采用一些拐弯抹角的技法外，似乎没别

的办法修正这份代码了。但是，利用 partical()就能轻松解决这个问题。只用在 partial()中提供 ack 的参数值即可，就像下面这样：

```python
from functools import partial  
serv = TCPServer(['', 15000), partial(EchoHandler, ack=b'RECEIVED:'))  
servServe_forever() 
```

在这个例子里，__init__()方法中对参数 ack 的指定看起来有些滑稽，但它是以 keyword-only 参数的形式来指定的。有关 keyword-only 参数的讨论可以在 7.2 节中找到。

有时候也可以通过 lambda 表达式来替代 partial()。比如，上面这几个例子也可以采用这样的语句来实现：

```python
points.sort(key=lambda p: distance(pt, p))  
p.apply_async(add, (3, 4), callback=lambda result: output_result(result, log))  
serv = TCPServer(['', 15000), lambda *args, **kwargs: EchoHandler(*args, ack=b'RECEIVED: ', **kwargs)) 
```

这些代码也能正常工作，但是却显得很 嗦，而且也让人觉得读起来很困惑。使用 啰partial()会使得你的意图更加明确（即，为某些参数提供默认值）。

# 7.9 用函数替代只有单个方法的类

# 7.9. 1 问题

我们有一个只定义了一个方法的类（除__init__()方法外）。但是，为了简化代码，我们更希望能够只用一个简单的函数来替代。

# 7.9.2 解决方案

在许多情况下，只有单个方法的类可以通过闭包（closure）将其转换成函数。考虑下面这个例子，这个类允许用户通过某种模板方案来获取 URL。

from urllib.request import urlopen   
classUrlTemplate: def__init__(self,template): self.template $\equiv$ template defopen(self,\*\*kwargs): returnurlopen(self.template.format_map(k kwargs))

```python
Example use. Download stock data from yahoo  
yahoo = UriTemplate('http://finance.yahoo.com/d/quotes.csv?s={names}&f={fields}')  
for line in yahoo.open names='IBM,AAPL,FB', fields='sl1c1v'):  
    print(linedecode('utf-8')) 
```

这个类可以用一个简单的函数来取代：

defurltemplate template): def opener $\text{假}$ kwarges): return urlopen template.format_map (kwarges)) return opener #Example use yahoo $=$ urltemplate('http://finance.yahoo.com/d/quotes.csv?s={names}&f={fields}') for line in yahoo names $=$ 'IBM,AAPL,FB'，fields $=$ 'sllclv'): print(linedecode('utf-8'))

# 7.9.3 讨论

在许多情况下，我们会使用只有单个方法的类的唯一原因就是保存额外的状态给类方法使用。比方说，UrlTemplate 类的唯一目的就是将 template 的值保存在某处，这样就可以在 open()方法中用上它了。

按照我们给出的解决方案，使用嵌套函数或者说闭包常常会显得更加优雅。简单来说，闭包就是一个函数，但是它还保存着额外的变量环境，使得这些变量可以在函数中使用。闭包的核心特性就是它可以记住定义闭包时的环境。因此，在这个解决方案中，opener()函数可以记住参数 template 的值，然后在随后的调用中使用该值。

无论何时，当在编写代码中遇到需要附加额外的状态给函数时，请考虑使用闭包。比起将函数放入一个“全副武装”的类中，基于闭包的解决方案通常更加简短也更加优雅。

# 7.10 在回调函数中携带额外的状态

# 7.10.1 问题

我们正在编写需要使用回调函数的代码（比如，事件处理例程、完成回调等），但是希望回调函数可以携带额外的状态以便在回调函数内部使用。

# 7.10.2 解决方案

本节中提到的对回调函数的应用可以在许多库和框架中找到——尤其是那些和异步处

理相关的库和框架。为了说明和测试的目的，我们首先定义下面的函数，它会调用一个回调函数：

def apply_async(func, args,\*, callback): # Compute the result result $=$ func(\*args) #Invoke the callback with the result callback(result)

在现实世界中，类似这样的代码可能会完成各种高级的处理任务，这会涉及线程、进程和定时器等，但我们这里主要关注的不是这些。相反，我们只是把注意力集中在对回调函数的调用上。下面的示例展示了上述代码应该如何使用：

```python
>>> def print_result(result):
...
...
def add(x, y):
...
return x + y
...
apply_async(add, (2, 3), callback=print_result)
Got: 5
apply_async(add, ('hello', 'world'), callback=print_result)
Got: helloworld
>>> 
```

我们会注意到函数 print_result()仅接受一个单独的参数，也就是 result。这里并没有传入其他的信息到函数中。有时候当我们希望回调函数可以同其他变量或者部分环境进行交互时，缺乏这类信息就会带来问题。

一种在回调函数中携带额外信息的方法是使用绑定方法（bound-method）而不是普通的函数。比如，下面这个类保存了一个内部的序列号码，每当接收到一个结果时就递增这个号码。

```python
class ResultHandler: def __init__(self): self(sequence = 0 def handler(self, result): self(sequence += 1 print(['{}']Got:{}.format(self(sequence,result)) 
```

要使用这个类，可以创建一个类实例并将绑定方法 handler 当做回调函数来用：

```txt
>>> r = ResultHandler()
>>> apply_async(add, (2, 3), callback=r.handle)
[1] Got: 5 
```

```txt
>>> apply_async(add, ('hello', 'world'), callback=rhandler)
[2] Got: helloworld
>>> 
```

作为类的替代方案，也可以使用闭包来捕获状态。示例如下：

def makehandler(   ): sequence $= 0$ def handler(result): nonlocal sequence sequence += 1 print('[]Got: \{\}'.format(sequence, result)) return handler

下面是使用闭包的例子：

```txt
>>> handler = make_handler()
>>> apply_async(add, (2, 3), callback=handler)
[1] Got: 5
>>> apply_async(add, ('hello', 'world'), callback=handler)
[2] Got: helloworld
>>> 
```

除此之外还有一种方法，有时候可以利用协程（coroutine）来完成同样的任务：

def makehandler(): sequence $= 0$ while True: result $\equiv$ yield sequence $+ = 1$ print('{}\}Got:\{\}''.format(sequence,result))

对于协程来说，可以使用它的 send()方法来作为回调函数，就像下面这样：

```txt
>>> handler = make_handler()
>>> nexthandler) # Advance to the yield
>>> apply_async(add, (2, 3), callback=handler.send)
[1] Got: 5
>>> apply_async(add, ('hello', 'world'), callback=handler.send)
[2] Got: helloworld
>>> 
```

最后但也同样重要的是，也可以通过额外的参数在回调函数中携带状态，然后用 partial()来处理参数个数的问题（见 7.8 节）。示例如下：

```txt
>>>class SequenceNo: def__init__(self): 
```

```python
... self(sequence = 0
...
>>> def handler(result, seq):
    ... seq(sequence += 1
    ... print(['{}'] Got: {}.format(seq(sequence, result))
...
>>> seq = SequenceNo()
>>> from functools import partial
>>> apply_async(add, (2, 3), callback=partialTEEHandler, seq=seq)
[1] Got: 5
>>> apply_async(add, ('hello', 'world'), callback=partialTEEHandler, seq=seq)
[2] Got: helloworld
>>> 
```

# 7.10.3 讨论

基于回调函数的软件设计常常会面临使代码陷入一团乱麻的风险。部分原因是因为从代码发起初始请求开始到回调执行的这个过程中，回调函数常常是与这个环境相脱离的。因此，在发起请求和处理结果之间的执行环境就丢失了。如果想让回调函数在涉及多个步骤的任务处理中能够继续执行，就必须清楚应该如何保存和还原相关的状态。

主要有两种方法可用于捕获和携带状态。可以在类实例上携带状态（将状态附加到绑定方法上），也可以在闭包中携带状态。这两种方法中，闭包可能要显得更轻量级一些，而且由于闭包也是由函数构建的，这样显得会更加自然。这两种方法都可以自动捕获所有正在使用的变量。因此，这就使得我们不必担心哪个具体的状态需要保存起来（根据代码自动决定哪些需要保存）。

如果使用闭包，那么需要对可变变量多加留意。在给出的解决方案中，nonlocal声明用来表示变量 sequence 是在回调函数中修改的。没有这个声明，将得到错误提示。

将协程用作回调函数的有趣之处在于这种方式和采用闭包的方案关系紧密。从某种意义上说，协程甚至更为清晰，因为这里只出现了一个单独的函数。此外，变量都可以自由地进行修改，不必担心 nonlocal 声明。可能存在的缺点在于人们对协程的理解程度不如其他的 Python 特性。使用协程时还有几个小技巧需要掌握，比如在使用协程前需要先对其调用一次next()，这在实践中常常容易忘记。不过，协程还有其他的潜在用途，比如定义内联的回调函数（在下一节中讲解）。

如果所有需要做的就是在回调函数中传入额外的值，那么最后提到的那个有关 partial()的技术是很管用的。有时候我们也会看到用 lambda 表达式来实现同样的功能：

```txt
>> apply_async(add, (2, 3), callback=lambda r: handler(r, seq))  
[1] Got: 5  
>>> 
```

要查看更多的示例请参见 7.8 节。在那一节中我们展示了如何利用 partial()来修改函数的参数签名。

# 7.11 内联回调函数

# 7.11.1 问题

我们正在编写使用回调函数的代码，但是担心小型函数在代码中大肆泛滥，程序的控制流会因此而失控。我们希望能有某种方法使代码看起来更像一般的过程式步骤。

# 7.11.2 解决方案

我们可以通过生成器和协程将回调函数内联到一个函数中。为了说明，假设有一个函数会按照下面的方式调用回调函数（参见 7.10 节）：

```python
def apply_async(func, args, *, callback): # Compute the result result = func(*args) #Invoke the callback with the result callback(result) 
```

现在看看接下来的支持代码，这里涉及一个 Async 类和 inlined_async 装饰器：

from queue import Queue   
from functools import wraps   
class Async: def_init_self, func, args): self.func $=$ func self.args $=$ args   
def inlined_async(func): @wraps(func) def wrapper(\*args): f $=$ func(\*args) result_queue $=$ Queue() result_queue.put(None) while True: result $=$ result_queue.get() try: a $=$ f.send(result) apply_async(a.func, a.args, callback $\equiv$ result_queue.put) except StopIteration:

```txt
break return wrapper 
```

这两段代码允许我们通过 yield 语句将回调函数变为内联式的，示例如下：

```python
def add(x, y):
    return x + y
@inlined_async
def test():
    r = yield Async(add, (2, 3))
    print(r)
    r = yield Async(add, ('hello', 'world'))
    print(r)
    for n in range(10):
        r = yield Async(add, (n, n))
        print(r)
print('Goodbye') 
```

如果调用 test()，会得到这样的输出结果：

```txt
5   
helloworld 0   
2   
4   
6   
8   
10   
12   
14   
16   
18   
Goodbye 
```

除了那个特殊的装饰器和对 yield 的使用之外，我们会发现代码中根本就没有出现回调函数（它们只是隐藏在幕后了）。

# 7.11.3 讨论

本节将真正考验一下读者对回调函数、生成器以及程序控制流方面的掌控情况。

首先，在涉及回调函数的代码中，问题的关键就在于当前的计算会被挂起，然后在稍后某个时刻再得到恢复。当计算得到恢复时，回调函数将得以继续处理执行。示例中的 apply_async()函数对执行回调函数的关键部分做了简单的说明，尽管在现实世界中这会复杂得多（涉及线程、进程、事件处理例程等）。

将计算挂起之后再恢复，这个思想非常自然地同生成器函数对应了起来。具体来说就是 yield 操作使得生成器函数产生出一个值然后就挂起，后续调用生成器的__next__()或者 send()方法会使得它再次启动。

鉴于此，本节的核心就在 inline_async()装饰器函数中。关键点就是对于生成器函数的所有 yield 语句装饰器都会逐条进行跟踪，一次一个。为了做到这点，我们创建了一个队列用来保存结果，初始时用 None 来填充。之后通过循环将结果从队列中取出，然后发送给生成器，这样就会产生下一次 yield，此时就会接收到 Async 的实例。然后在循环中查找函数和参数，开始异步计算 apply_async()。但是，这个过程中最为隐蔽的部分就在于这里没有使用普通的回调函数，回调过程被设定到队列的 put()方法中了。

此时应该可以精确描述到底都发生了些什么。主循环会迅速回到顶层，并在队列中执行一个 get()操作。如果有数据存在，那它就一定是由 put()回调产生的结果。如果什么都没有，操作就会阻塞，等待之后某个时刻会有结果到来。至于结果要如何产生，这取决于 apply_async()函数的实现。

如果对这些疯狂的东西能否正常工作抱有怀疑，可以结合多进程库让异步操作在单独的进程中执行，以此测试该方案：

```python
if _name_ == '__main__':  
    import multiprocessing  
pool = multiprocessingPOOL()  
apply_async = pool.applyasync  
# Run the test function  
test() 
```

我们会发现这个方案的确能正常工作，但是要理清这其中的控制流程可能需要喝掉不少咖啡了。

将精巧的控制流隐藏在生成器函数之后，这种做法可以在标准库以及第三方包中找到。比如说，contextlib 模块中的@contextmanager 装饰器也使用了类似的令人费解的技巧，将上下文管理器的入口点和出口点通过一个 yield 语句粘合在了一起。著名的 Twisted库（http://twistedmatrix.com）中也有着类似的内联回调技巧。

# 7.12 访问定义在闭包内的变量

# 7.12.1 问题

我们希望通过函数来扩展闭包，使得在闭包内层定义的变量可以被访问和修改。

# 7.12.2 解决方案

一般来说，在闭包内层定义的变量对于外界来说完全是隔离的。但是，可以通过编写存取函数（accessor function，即 getter/setter 方法）并将它们作为函数属性附加到闭包上来提供对内层变量的访问支持。示例如下：

```python
def sample() : n = 0 # Closure function def func(): print('n=', n) # Accessor methods for n def get_n(): return n def set_n(value): nonlocal n n = value # Attach as function attributes func.get_n = get_n func.set_n = set_n return func 
```

下面是使用这份代码的示例：

>>>f $=$ sample()   
>>>f()   
n=0   
>>>f.set_n(10)   
>>>f()   
n=10   
>>>f.get_n()   
10   
>>>

# 7.12.3 讨论

这里主要用到了两个特性使得本节讨论的技术得以成功实施。首先，nonlocal声明使得编写函数来修改内层变量成为可能。其次，函数属性能够将存取函数以直接的方式附加到闭包函数上，它们工作起来很像实例的方法（尽管这里并没有涉及类）。

对本节提到的技术稍作扩展就可以让闭包模拟成类实例。我们所要做的就是将内层函数拷贝到一个实例的字典中然后将它返回。示例如下：

import sys   
class ClosureInstance: def init(self, locals=None): if locals is None: locals $=$ sys._getframe(1).f_locals # Update instance dictionary with callables self._dict_.update((key, value) for key, value in locals.items() if callable(value)) # Redirect special methods def len(self): return self._dict_['_len_']() #Example use   
def Stack(): items $= []$ def push(item): items.append(item) def pop(): return items.pop() def len(): return len(items) return ClosureInstance()

下面的交互式会话说明了这种方法确实能完成任务：

>>>s $=$ Stack()   
>>s   
<__main_.ClosureInstance object at 0x10069ed10> >>>s.push(10)   
>>s.push(20)   
>>s.push('Hello')   
>>>len(s)   
3   
>>>s.pop() 'Hello'   
>>>s.pop()   
20   
>>>s.pop()   
10   
>>>

有趣的是，这份代码运行起来比使用一个普通的类定义要稍微快一些。比如，我们可能会用下面这个类来做对比测试：

```python
class Stack2: def __init__(self): self.items = [] def push(self, item): self.items.append(item) def pop(self): return self.items.pop() def __len__(self): return len(self.items) 
```

如果进行对比测试，将得到类似如下的结果：

```diff
>>> from timeit import timeit
>>> # Test involving closures
>>> s = Stack()
>>> timeit('s.push(1);s.pop()), 'from __main__ import s')
0.9874754269840196
>>> # Test involving a class
>>> s = Stack2()
>>> timeit('s.push(1);s.pop()), 'from __main__ import s')
1.0707052160287276
>>> 
```

我们可以看到，采用闭包的版本要快大约 $8 \%$ 。测试中的大部分时间都花在对实例变量的直接访问上，闭包要更快一些，这是因为不用涉及额外的 self 变量。

Raymond Herringer 在这个思路的基础上设计出了一种更加“恐怖”的变种。但是，在自己的代码中应该对这种奇技淫巧持谨慎的态度。请注意，相比一个真正的类，这种方法是相当怪异的。比如，像继承、属性、描述符或者类方法这样的主要特性在这种方法中都是无法使用的。我们还需要玩一些花招才能让特殊方法正常工作（比如，参考 ClosureInstance 中对__len__()的实现）。

最后，这么做会使得阅读你代码的人犯糊涂。他们会想知道这么做看起来和一个普通的类定义相比有什么区别（当然了，他们也想知道为什么这么做会运行的更快一些）。尽管如此，这仍然是个有趣的例子，它告诉我们对闭包内部提供访问机制能够实现出什么样的功能。

从全局的角度考虑，为闭包增加方法可能会有着更多的实际用途，比如我们想重置内部状态、刷新缓冲区、清除缓存或者实现某种形式的反馈机制（feedback mechanism）。

# 类与对象

本章的重点是为大家介绍一些与类定义相关的常见编程模式。主题包括让对象支持常见的 Python 特性、特殊方法的使用、封装、继承、内存管理以及一些有用的设计模式。

# 8.1 修改实例的字符串表示

# 8.1.1 问题

我们想修改打印实例所产生的输出，使输出结果能更有意义。

# 8.1.2 解决方案

要修改实例的字符串表示，可以通过定义__str__()和__repr__()方法来实现。示例如下：

class Pair: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return 'Pair({0.x!r}, {0.y!r})'.format(self) def __str__(self): return '(' $\{0.\mathrm{x}!\mathrm{s}\}$ , $\{0.y!\mathrm{s}\})$ ).format(self)

特殊方法__repr__()返回的是实例的代码表示（code representation），通常可以用它返回的字符串文本来重新创建这个实例①。内建的 repr()函数可以用来返回这个字符串，当缺少交互式解释环境时可用它来检查实例的值。特殊方法__str__()将实例转换为一个字符串，这也是由 str()和 print()函数所产生的输出。示例如下：

```erlang
>>> p = Pair(3, 4)  
>>> p  
Pair(3, 4) # __repr__() output  
>>> print(p)  
(3, 4) # __str__() output  
>>> 
```

本节给出的实现中也展示了在进行格式化输出时应该如何使用不同的字符串表示。尤其是，特殊的格式化代码!r 表示应该使用__repr__()的输出，而不是默认的__str__()。我们可以在前文给出的 Pair 类上做做实验：

```txt
>>> p = Pair(3, 4)  
>>> print('p is {0!r}'.format(p))  
p is Pair(3, 4)  
>>> print('p is {0}'.format(p))  
p is (3, 4)  
>>> 
```

# 8.1.3 讨论

定义__repr__()和__str__()通常被认为是好的编程实践，因为这么做可以简化调试过程和实例的输出。比方说，我们只用通过打印实例，程序员就能了解到更多有关这个实例内容的有用信息。

对于__repr__()，标准的做法是让它产生的字符串文本能够满足 $\mathrm { e v a l } ( \mathrm { r e p r ( x ) } ) = = { \bf X } _ { \mathrm { \ell } }$ 。如果不可能办到或者说不希望有这种行为，那么通常就让它产生一段有帮助意义的文本，并且以<和>括起来。示例如下：

>>>f $=$ open('file.dat')   
>>>f   
<io.TextIOWrapper name $\equiv$ 'file.dat' mode $\equiv$ r' encoding $\equiv$ 'UTF-8'>   
>>>

如果没有定义__str__()，那么就用__repr__()的输出当做备份。

解决方案中对 format()函数的使用看起来似乎有点意思。格式化代码{0.x}用来指代参数 0 的 $\mathbf { X }$ 属性。因此在下面的函数中，0 实际上就代表实例 self：

```python
def __repr__(self):
    return 'Pair({0.x!r}, {0.y!r})'.format(self) 
```

这个实现还可以有另外一种方式，可以使用 $\%$ 操作符和下面的代码来完成：

```python
def __repr__(self):
    return 'Pair(%r, %r)' % (self.x, self.y) 
```

# 8.2 自定义字符串的输出格式

# 8.2.1 问题

我们想让对象通过 format()函数和字符串方法来支持自定义的输出格式。

# 8.2.2 解决方案

要自定义字符串的输出格式，可以在类中定义__format__()方法。示例如下：

格式 $\equiv$ { 'ymd':{'d.year}-{d.month}-{d.day} '， 'mdy'：{'d.month}/{d.day}/{d.year} '， 'dmy'：{'d.day}/{d.month}/{d.year} ' }

class Date: def __init__(self, year, month, day): self.year = year self.month = month self.day = day def __format__(self, code): if code ==': code $=$ 'ymd' fmt = _formscode] return fmt.format(d=self)

Date 类的实例现在可以支持如下的格式化操作了：

```txt
>>> d = Date(2012, 12, 21)  
>>> format(d)  
'2012-12-21'  
>>> format(d, 'mdy')  
'12/21/2012'  
>>> 'The date is {:ymd}'.format(d)  
'The date is 2012-12-21'  
>>> 'The date is {:mdy}'.format(d)  
'The date is 12/21/2012'  
>>> 
```

# 8.2.3 讨论

__format__()方法在 Python 的字符串格式化功能中提供了一个钩子。需要重点强调的是，

对格式化代码的解释完全取决于类本身。因此，格式化代码几乎可以为任何形式。举例来说，考虑下面的 datetime 模块的示例：

```txt
>>> from datetime import date
>>> d = date(2012, 12, 21)
>>> format(d)
'2012-12-21'
>>> format(d, '%A, %B %d, %Y')
'Friday, December 21, 2012'
>>> 'The end is {:d %b %Y}. Goodbye'.format(d)
'The end is 21 Dec 2012. Goodbye'
>>> 
```

对于内建类型来说，有一些标准的格式化转换形式。请参阅 string 模块的文档（http://docs.python.org/3/library/string.html）以获得正式的规范。

# 8.3 让对象支持上下文管理协议

# 8.3.1 问题

我们想让对象支持上下文管理协议（context-management protocol，通过 with 语句触发）。

# 8.3.2 解决方案

要让对象能够兼容 with 语句，需要实现__enter__()和__exit__()方法。比方说，考虑下面这个表示网络连接的类：

from socket import socket, AF_INET, SOCK_STREAM   
```python
class LazyConnection: def __init__(self, address, family=AF_INET, type=SOCK_STREAM): self.address = address self.family = AF_INET self.type = SOCK_STREAM self.sock = None def __enter__(self): if self.sock is not None: raise ValueError('Already connected') self.sock = socket(self.family, self.type) self.sock.connect(self.address) return self.sock def __exit__(self, exc Ty, exc_val, tb): 
```

self.sock.close()

self.sock $=$ None

这个类的核心功能就是表示一条网络连接，但是实际上在初始状态下它并不会做任何事情（比如，它并不会建立一条连接）。相反，网络连接是通过 with 语句来建立和关闭的（这正是上下文管理的基本需求）。示例如下：

from functools import partial   
conn $=$ LazyConnection('www.python.org'，80)) #Connection closed   
with conn as s: #conn._enter_() executes: connection open s.send(b'GET /index.html HTTP/1.0\r\n') s.send(b'Host: www.python.org\r\n') s.send(b'\r\n') resp $=$ b'.join(iter(partial(s.recv,8192)，b')) #conn._exit_(） executes: connection closed

# 8.3.3 讨论

要编写一个上下文管理器，其背后的主要原则就是我们编写的代码需要包含在由 with语句定义的代码块中。当遇到 with 语句时，__enter__()方法首先被触发执行。__enter__()的返回值（如果有的话）被放置在由 as限定的变量当中。之后开始执行with 代码块中的语句。最后，__exit__()方法被触发来执行清理工作。

这种形式的控制流与 with 语句块中发生了什么情况是没有关联的，出现异常时也是如此。实际上，__exit__()方法的三个参数就包含了异常类型、值和对挂起异常的追溯（如果出现异常的话）。__exit__()方法可以选择以某种方式来使用异常信息，或者什么也不干直接忽略它并返回None作为结果。如果__exit__()返回 True，异常就会被清理干净，好像什么都没发生过一样，而程序也会立刻继续执行 with 语句块之后的代码。

这项技术有一个微妙的地方，那就是 LazyConnection 类是否可以通过多个 with 语句以嵌套的方式使用 socket 连接。正如我们给出的代码那样，一次只允许创建一条单独的socket 连接。当 socket 已经在使用时，如果尝试重复使用 with 语句就会产生异常。我们可以对这个实现稍做修改来绕过这个限制，示例如下：

from socket import socket, AF_INET, SOCK_STREAM

class LazyConnection:

```python
def __init__(self, address, family=AF_INET, type=SOCK_STREAM):
    self.address = address
    self.family = AF_INET
    self.type = SOCK_STREAM 
```

```python
selfconnections = []
def __enter__(self):
    sock = socket(self.family, self.type)
    sock.connect(self.address)
    selfconnections.append(sock)
    return sock
def __exit__(self, exc Ty, exc_val, tb):
    selfconnections.pop().close()
# Example use
from functools import partial
conn = LazyConnection('www.python.org', 80))
with conn as s1:
    ...
    with conn as s2:
        ...
        # s1 and s2 are independent sockets 
```

在第二个版本中，LazyConnection成了一个专门生产网络连接的工厂类。在内部实现中，我们把一个列表当成栈使用来保存连接。每当__enter__()执行时，由它产生一个新的连接并添加到栈中。而__exit__()方法只是简单地将最近加入的那个连接从栈中弹出并关闭它。这个修改很微不足道，但是这样就可以允许用嵌套式的 with 语句一次创建出多个连接了。

上下文管理器最常用在需要管理类似文件、网络连接和锁这样的资源的程序中。这些资源的关键点在于它们必须显式地进行关闭或释放才能正确工作。例如，如果获得了一个锁，之后就必须确保要释放它，否则就会有死锁的风险。通过实现__enter__()和_exit__()，并且利用 with 语句来触发，这类问题就可以很容易地避免了。因为__exit__()方法中的清理代码无论如何都会保证运行的。

有关上下文管理器的另一种构想可以在 contextmanager 模块中找到，请参阅 9.22 节。本节示例的线程安全版本可以在 12.6 节中找到。

# 8.4 当创建大量实例时如何节省内存

# 8.4.1 问题

我们的程序创建了大量的（比如百万级）实例，为此占用了大量的内存。

# 8.4.2 解决方案

对于那些主要用作简单数据结构的类，通常可以在类定义中增加__slot__属性，以此来大量减少对内存的使用。示例如下：

```python
class Date: _slots_ = ['year', 'month', 'day'] def __init__(self, year, month, day): self.year = year self.month = month self.day = day 
```

当定义了__slots__属性时，Python 就会针对实例采用一种更加紧凑的内部表示。不再让每个实例都创建一个__dict__字典，现在的实例是围绕着一个固定长度的小型数组来构建的，这和一个元组或者列表很相似。在__slots__中列出的属性名会在内部映射到这个数组的特定索引上。使用__slots__带来的副作用是我们没法再对实例添加任何新的属性了——我们被限制为只允许使用__slots__中列出的那些属性名。

# 8.4.3 讨论

使用__slots__节省下来的内存根据创建的实例数量以及保存的属性类型而有所不同。但是，一般来说使用的内存量相当与将数据保存在元组中。为了有一个直观的感受，我们举个例子：在 64 位版本的 Python 中，不使用__slots__保存一个单独的 Date 实例，则需要占用 428 字节的内存。如果定义了__slots__，内存用量将下降到 156 字节。在一个需要同时处理大量 Date 实例的程序中，这将显著减少总的内存用量。

尽管__slots__看起来似乎是一个非常有用的特性，但是在大部分代码中都应该尽量别使用它。Python 中有许多部分都依赖于传统的基于字典的实现。此外，定义了__slots__属性的类不支持某些特定的功能，比如多重继承。就大部分情况而言，我们应该只针对那些在程序中被当做数据结构而频繁使用的类上采用__slots__技法（例如，如果你的程序创建了上百万个特定的类实例）。

关于__slots__有一个常见的误解，那就是这是一种封装工具，可以阻止用户为实例添加新的属性。尽管这的确是使用__slots__所带来的副作用，但这绝不是使用__slots__的原本意图。相反，人们一直以来都把__slots__当做一种优化工具。

# 8.5 将名称封装到类中

# 8.5.1 问题

我们想将“私有”数据封装到类的实例上，但是又需要考虑到 Python 缺乏对属性的访

问控制问题。

# 8.5.2 解决方案

与其依赖语言特性来封装数据，Python 程序员们更期望通过特定的命名规则来表达出对数据和方法的用途。第一个规则是任何以单下划线（_）开头的名字应该总是被认为只属于内部实现。比如：

```python
class A: def __init__(self): self._internal = 0 # An internal attribute self(public = 1 # A public attribute def public_method(self): A public method ... def __internal_method(self): 
```

Python 本身并不会阻止其他人访问内部名称。但是如果有人这么做了，则被认为是粗鲁的，而且可能导致产生出脆弱不堪的代码。应该要提到的是，以下划线打头的标识也可用于模块名称和模块级的函数中。比如，如果见到有模块名以下划线打头（例如，_socket），那么它就属于内部实现。同样地，模块级的函数比如 sys._getframe()使用起来也要格外小心。

我们应该在类定义中也见到过以双下划线（__）打头的名称。例如：

class B: def __init__(self): self._private $= 0$ def __private_method(self): def public_method(self): ... self._private_method()

以双下划线打头的名称会导致出现名称重整（name mangling）的行为。具体来说就是上面这个类中的私有属性会被分别重命名为_B__private 和_B__private_method。此时你可能会问，类似这样的名称重整其目的何在？答案就是为了继承——这样的属性不能通过继承而覆盖。示例如下：

```python
class C(B):
    def __init__(self):
        super().__init() 
```

```python
self._private = 1    # Does not override B._private
# Does not override B._private_method()
def _private_method(self):
    ...
} 
```

这 里 ， 私 有 名 称 __private 和 __private_method 会 被 重 命 名 为 _C__private 和 _C__private_method，这和基类 B 中的重整名称不同。

# 8.5.3 讨论

“私有”属性存在两种不同的命名规则（单下划线和双下划线），这一事实引出了一个显而易见的问题：应该使用哪种风格？对于大部分代码而言，我们应该让非公有名称以单下划线开头。但是，如果我们知道代码中会涉及子类化处理，而且有些内部属性应该对子类进行隐藏，那么此时就应该使用双下划线开头。

此外还应该指出的是，有时候可能想定义一个变量，但是名称可能会和保留字产生冲突。基于此，应该在名称最后加上一个单下划线以示区别。比如：

```txt
lambda_ = 2.0 # Trailing _ to avoid clash with lambda keyword 
```

这里不采用以下划线开头的原因是避免在使用意图上发生混淆（例如，如果采用下划线开头的形式，那么可能会被解释为这么做是为了避免名称冲突，而不是作为私有数据的标志）。在名称尾部加一个单下划线就能解决这个问题。

# 8.6 创建可管理的属性

# 8.6.1 问题

在对实例属性的获取和设定上，我们希望增加一些额外的处理过程（比如类型检查或者验证）。

# 8.6.2 解决方案

要自定义对属性的访问，一种简单的方式是将其定义为 property①。比如说，下面的代码定义了一个 property，增加了对属性的类型检查：

```python
class Person: def __init__(self, first_name): self.first_name = first_name #getter function @property 
```

```python
def first_name(self):
    return self._first_name
# Setter function
@first_name setter
def first_name(self, value):
    if not isinstance(value, str):
        raise TypeError('Expected a string')
        self._first_name = value
# Deleter function (optional)
@first_name.dequeue
def first_name(self):
    raise AttributeError("Can't delete attribute") 
```

在上述代码中，一共有三个互相关联的方法，它们必须有着相同的名称。第一个方法是一个 getter 函数，并且将 first_name 定义为了 property 属性。其他两个方法将可选的 setter和 deleter 函数附加到了 first_name 属性上。需要重点强调的是，除非 first_name 已经通过 $@$ property 的方式定义为了 property 属性，否则是不能定义 $@$ first_name.setter 和$@$ first_name.deleter 装饰器的。

property 的重要特性就是它看起来就像一个普通的属性，但是根据访问它的不同方式，会自动触发 getter、setter 以及 deleter 方法。示例如下：

```python
>>> a = Person('Guido')
>>> a.first_name    # Calls the getters
'Guido'
>>> a.first_name = 42    # Calls the setter
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "prop.py", line 14, in first_name
raise TypeError('Expected a string')
TypeError: Expected a string
>>> del a.first_name
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
AttributeError: can't delete attribute
>>> 
```

当我们实现一个 property时，底层的数据（如果有的话）仍然需要被保存到某个地方。因此在 get 和 set 方法中，可以看到我们是直接对_first_name 进行操作的，这就是数据实际保存的地方。此外，你可能会问为什么在__init__()方法中设定的是 self.first_name而不是 self._first_name 呢？在这个例子中，property 的全部意义就在于我们设置属性时可以执行类型检查。因此，很有可能你想让这种类型检查在初始化的时候也可以进行。

因此，在__init__()中设置 self.first_name，实际上会调用到 setter 方法（因此就会跳过self.first_name 而去访问 self._first_name）。

对于已经存在的 get 和 set 方法，同样也可以将它们定义为 property。示例如下：

```python
class Person: def __init__(self, first_name): self.set_first_name(first_name) # Getter function def get_first_name(self): return self._first_name # Setter function def set_first_name(self, value): if not instanceof(value, str): raise TypeError('Expected a string') self._first_name = value # Deleter function (optional) def del_first_name(self): raise ValueError("Can't delete attribute") # Make a property from existing get/set methods name = property(get_first_name, set_first_name, del_first_name) 
```

# 8.6.3 讨论

property 属性实际上就是把一系列的方法绑定到一起。如果检查类的 property 属性，就会发现 property 自身所持有的属性 fget、fset 和 fdel 所代表的原始方法。示例如下：

```txt
>>> Person.first_name.fget
<function Person.first_name at 0x1006a60e0>
>>> Person.first_name.fset
<function Person.first_name at 0x1006a6170>
>>> Person.first_name.fdel
<function Person.first_name at 0x1006a62e0>
>>> 
```

一般来说我们不会直接去调用 fget 或者 fset，但是当我们访问 property 属性时会自动触发对这些方法的调用。

只有当确实需要在访问属性时完成一些额外的处理任务时，才应该使用 property。有时候 Java 程序员会觉得所有的访问都需要通过 getter 和 setter 来处理，那么他们的代码就应该是下面这个样子：

```python
class Person: def __init__(self, first_name): self.first_name = name @property def first_name(self): return self._first_name @first_name setter def first_name(self, value): self._first_name = value 
```

如果 property 并不会完成任何额外的处理任务，就不要把代码写成上面这个样子。第一，这么做会使得代码变得更加 嗦，对其他人来说也比较困惑。第二，这么做会让程序啰变慢很多。最后，这么做不会给设计带来真正的好处。特别是如果稍后决定要对某个普通的属性增加额外的处理步骤时，可以在不修改已有代码的情况下将这个属性提升为一个 property。这是因为代码中访问一个属性的语法并不会改变（即，访问普通属性和访问 property属性的代码写法是一样的）。

property 也可以用来定义需要计算的属性。这类属性并不会实际保存起来，而是根据需要完成计算。示例如下：

import math   
class Circle: def init(self, radius): self.radius = radius @property def area(self): return math.pi \* self_radius $^{**}$ 2 @property def perimeter(self): return 2 \*math.pi \* self.radius

这里对 property 的使用使得实例的接口变得非常统一，radius、area 以及 perimeter 都能够简单地以属性的形式进行访问，而不必将属性和方法调用混在一起使用了。示例如下：

```txt
>>> c = Circle(4.0)  
>>> c.radius  
4.0  
>>> c.area  # 注意这里没有()  
50.26548245743669  
>>> c.perimeter  # 这里也没有()  
25.132741228718345  
>>>
```

尽管 property 带来了优雅的编程接口，但有时候我们还是希望能够直接使用 getter 和 setter

函数。比如说：

```txt
>>> p = Person('Guido')
>>> p.get_first_name()
'Guido'
>>> p.set_first_name('Larry')
>>> 
```

这种情况常常会出现在当 Python 代码需要被集成到一个更为庞大的系统基础设施或者程序的时候。比方说，也许有一个 Python 类需要根据远程过程调用（RPC）或者分布式对象插入到一个大型的分布式系统中。在这种情况下，直接显式地采用get/set 方法（作为普通的方法调用）要比通过 property 来隐式调用这类函数更加方便和简单。

最后但也同样重要的是，不要编写那种定义了大量重复性 property 的代码。示例如下：

```python
class Person: def __init__(self, first_name, last_name): self.first_name = first_name self.last_name = last_name @property def first_name(self): return self._first_name @first_name setter def first_name(self, value): if not instanceof(value, str): raise TypeError('Expected a string') self._first_name = value #Repeated property code,but for a different name (bad!) @property def last_name(self): return self._last_name @last_name setter def last_name(self, value): if not instanceof(value, str): raise TypeError('Expected a string') self._last_name = value 
```

重复的代码会导致代码膨胀，容易出错，而且代码也十分丑陋。事实证明，利用描述符或者闭包能够更好地完成同样的任务，具体请参见 8.9 节和 9.21 节。

# 8.7 调用父类中的方法

# 8.7. 1 问题

我们想调用一个父类中的方法，这个方法在子类中已经被覆盖了。

# 8.7.2 解决方案

要调用父类（或称超类）中的方法，可以使用 super()函数完成。示例如下：

```python
class A: def spam(self): print('A.spam')   
class B(A): def spam(self): print('B.spam') super().spam() #Call parent spam() 
```

super()函数的一种常见用途是调用父类的__init__()方法，确保父类被正确地初始化了：

```python
class A: def __init__(self): self.x = 0   
class B(A): def __init__(self): super().__init_(self.y = 1 
```

另一种常见用途是当覆盖了 Python 中的特殊方法时，示例如下：

```python
class Proxy: def __init__(self, obj): self._obj = obj # Delegate attribute lookup to internal obj def __getattr__(self, name): return getattr(self._obj, name) # Delegate attribute assignment def __setattr__(self, name, value): if name.startswith(_): super().__setattr__(name, value) # Call original __setattr_ else: setattr(self._obj, name, value) 
```

在上述代码中，__setattr__()的实现里包含了对名称的检查。如果名称是以一个下划线（_）开头的，它就通过 super()去调用原始的__setattr__()实现。否则，就转而对内部持有的对象 self._obj 进行操作。这看起来有点意思，但是 super()即使在没有显式列出基类的情况下也是可以工作的。

# 8.7.3 讨论

如何正确使用 super()函数，这实际上是人们在 Python 中理解的最差的知识点之一。偶尔我们会看到一些代码直接调用父类中的方法，就像这样：

```python
class Base: def init_self): print('Base._init_')   
class A(Base): def init_self: Base._init_(self) print('A._init_') 
```

尽管对于大部分代码来说这么做都“行得通”，但是在涉及多重继承的代码里，就会导致出现奇怪的麻烦。比如，考虑下面这个例子：

```python
class Base: def init_(self): print('Base._init_')   
class A(Base): def init_(self): Base._init_(self) print('A._init_')   
class B(Base): def init_(self): Base._init_(self) print('B._init_')   
class C(A,B): def init_(self): A._init_(self) B._init_(self) print('C._init_') 
```

如果运行上面的代码，会发现 Base.__init__()方法被调用了两次。如下所示：

```txt
>> c = C()
Base._init_ 
```

```txt
A._init_
Base._init_
B._init_
C._init_
>>> 
```

也许调用两次 Base.__init__()并没什么害处，但是也可能刚好相反。如果从另一方面考虑，将代码修改为使用 super()，那么一切就都能正常工作了：

```python
class Base: def __init__(self): print('Base.__init__')   
class A(Base): def __init__(self): super().__init_(print('A.__init__'))   
class B(Base): def __init__(self): super().__init_(print('B.__init__'))   
class C(A,B): def __init__(self): super().__init_() # Only one call to super() here print('C.__init__') 
```

当使用这个新版的代码时，就会发现每个__init__()方法都只调用了一次：

```txt
>>> c = C()
Base._init_
B._init_
A._init_
C._init_
>>> 
```

要理解其中的缘由，我们需要退一步，先讨论一下 Python 是如何实现继承的。针对每一个定义的类，Python 都会计算出一个称为方法解析顺序（MRO）的列表①。MRO 列表只是简单地对所有的基类进行线性排列。示例如下：

```txt
>>> C.__mro_
(<class __main__.C>, <class __main__.A>, <class __main__.B>, <class __main__.Base>, <class 'object>)  
>>> 
```

要实现继承，Python 从 MRO 列表中最左边的类开始，从左到右依次查找，直到找到待查的属性时为止。

而 MRO 列表本身又是如何确定的呢？这里用到了一种称为 C3 线性化处理（C3Linearization）的技术。为了不陷入到艰深的数学理论中，简单来说这就是针对父类的一种归并排序，它需要满足 3 个约束：

先检查子类再检查父类；  
有多个父类时，按照 MRO 列表的顺序依次检查；  
• 如果下一个待选的类出现了两个合法的选择，那么就从第一个父类中选取。

老实说，所有需要的知道的就是 MRO 列表中对类的排序几乎适用于任何定义的类层次结构（class hierarchy）。

当使用 super()函数时，Python 会继续从 MRO 中的下一个类开始搜索。只要每一个重新定义过的方法（也就是覆盖方法）都使用了 super()，并且只调用了它一次，那么控制流最终就可以遍历整个 MRO 列表，并且让每个方法只会被调用一次。这就是为什么在第二个例子中 Base.__init__()不会被调用两次的原因。

关于 super()，一个有些令人惊讶的方面是，它并不是一定要关联到某个类的直接父类上，甚至可以在没有直接父类的类中使用它。例如，考虑下面这个类：

```python
class A: def spam(self): print('A.spam') super().spam() 
```

如果试着使用这个类，会发现这完全行不通：

```txt
>>> a = A()
>>> a.spam()
A.spam
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "<stdin>", line 4, in spam
AttributeError: 'super' object has no attribute 'spam'
>>> 
```

但是，如果把这个类用于多重继承时看看会发生什么：

```python
>>>class B: def spam(self): . print('B.spam')   
>>>class C(A,B)：
```

pass $\dots$ $\dots$ >> $c = C()$ >>> c.spam()   
A.spam   
B.spam   
>>>

这里我们会发现在类 A 中使用的 super().spam()实际上居然调用到了类 B 中的 spam()方法——B 和 A 是完全不相关的！这一切都可以用类 C 的 MRO 列表来解释：

```txt
>>> C.__mro__(<class 'main.C'>, <class 'main.A'>, <class 'main.B'>, <class 'object'})  
>>> 
```

我们常常会在定义混合类（mixin class）时以这种方式使用 super()。请参见 8.13 和 8.18 节。

但是，由于 super()可能会调用到我们不希望调用的方法，那么这里有一些应该遵守的基本准则。首先，确保在继承体系中所有同名的方法都有可兼容的调用签名（即，参数数量相同，参数名称也相同）。如果 super()尝试去调用非直接父类的方法，那么这就可以确保不会遇到麻烦。其次，确保最顶层的类实现了这个方法通常是个好主意。这样沿着 MRO 列表展开的查询链会因为最终找到了实际的方法而终止。

在 Python 社区中，关于 super()的使用有时候会成为争论的焦点。但是，公平地说，我们应该在现代的代码中使用它。Raymond Hettinger 在博客中写过一篇题为“Python’ssuper() considered Super!”的文章，文章中列举了更多的示例和理由来说明为什么 super()会是超级有用的工具①。

# 8.8 在子类中扩展属性

# 8.8.1 问题

我们想在子类中扩展某个属性的功能，而这个属性是在父类中定义的。

# 8.8.2 解决方案

考虑如下的代码，这里我们定义了一个属性 name：

```python
class Person: def __init__(self, name): self.name = name 
```

```python
Getter function   
@property   
def name(self): return self._name   
# Setter function   
@name setter   
def name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._name = value   
# Deleter function   
@name.deleter   
def name(self): raise ValueError("Can't delete attribute") 
```

下面我们从 Person 类中继承，然后在子类中扩展 name 属性的功能：

```python
class SubPerson(Person):
    @property
def name(self):
    print('Getting name')
    return super().name
    @name setter
def name(self, value):
    print('Setting name to', value)
    super(SubPerson, SubPerson).name._set__(self, value)
    @name.dequeue
def name(self):
    print('Deleting name')
    super(SubPerson, SubPerson).name._delete__(self) 
```

下面是使用这个新类的示例：

>>>s $=$ SubPerson('Guido') Setting name to Guido   
>> s.name   
Getting name   
'Guido'   
>> s.name $=$ 'Larry'   
Setting name to Larry   
>> s.name $= 42$

```txt
Traceback (most recent call last): File "<stdin>", line 1, in <module> File "example.py", line 16, in name raise TypeError('Expected a string') TypeError: Expected a string >>> 
```

如果只想扩展属性中的其中一个方法，可以使用下面的代码实现：

```python
class SubPerson(Person):
    @Person.name.getter
    def name(self):
        print('Getting name')
    return super().name 
```

或者，如果只想扩展 setter，可以这样：

```python
class SubPerson(Person):
    @Person.name setter
    def name(self, value):
        print('Setting name to', value)
        super(SubPerson, SubPerson).name._set__(self, value) 
```

# 8.8.3 讨论

在子类中扩展属性会引入一些非常微妙的问题，因为属性其实是被定义为 getter、setter和 deleter 方法的集合，而不仅仅只是单独的方法。因此，当我们扩展一个属性时，需要弄清楚是要重新定义所有的方法还是只针对其中一个方法做扩展。

在第一个例子中，所有的属性方法都被重新定义了。在每个方法中，我们利用 super()函数来调用之前的实现。在 setter 函数中，对 super(SubPerson, SubPerson).name.set__(self, value)的调用并不是错误，下面我们来解释一下。为了调用到 setter 之前的实现，需要把控制流传递到之前定义的 name 属性的__set__()方法中去。但是，唯一能调用到这个方法的方式就是以类变量而不是实例变量的方式去访问。这正是super(SubPerson, SubPerson)操作所完成的任务。

如果只想重新定义其中的一个方法，只使用 $@$ property 是不够的。例如，下面这样的代码是无法工作的：

```python
class SubPerson(Person):
    @property        # Doesn't work
    def name(self):
        print('Getting name')
        return super().name 
```

如果试着使用这份代码，就会发现 setter 函数完全消失不见了：

```txt
>>> s = SubPerson('Guido')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "example.py", line 5, in __init__self.name = name
AttributeError: can't set attribute
>>> 
```

相反，我们应该将代码修改为解决方案中的那样：

```python
class SubPerson(Person):
    @Person.getter
    def name(self):
        print('Getting name')
        return super().name 
```

当这么做之后，所有之前定义过的属性方法都会被拷贝过来，而getter 函数则会被替换掉。现在可以按照预期的方式工作了：

```txt
>>> s = SubPerson('Guido')
>>> s.name
Getting name
'Guido'
>>> s.name = 'Larry'
>>> s.name
Getting name
'Larry'
>>> s.name = 42
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "example.py", line 16, in name
    raise TypeError('Expected a string')
TypeError: Expected a string
>>> 
```

在这个特定的解决方案中，我们没法以更加一般化的名称来替换硬编码的类名 Person。如果不清楚哪个基类定义了属性，则应该采用这样的方案：重新定义所有的属性方法，并利用 super()来调用之前的实现。

值得一提的是，本节展示的第一个技术同样也可以用来扩展描述符（见 8.9节）。示例如下：

A descriptor   
class String: def init_self, name): self.name $=$ name def get_self,instance,cls):

if instance is None: return self return instance._dict_[self.name] def _set_(self, instance, value): if not isinstance(value, str): raise TypeError('Expected a string') instance._dict_[self.name] = value   
# A class with a descriptor   
class Person: name $=$ String('name') def init_(self, name): self.name $=$ name   
# Extending a descriptor with a property   
class SubPerson(Person): @property def name(self): print('Getting name') return super().name @name setter def name(self, value): print('Setting name to', value) super(SubPerson, SubPerson).name._set_(self, value) @name.dequeue   
def name(self): print('Deleting name') super(SubPerson, SubPerson).name._delete_(self)

最后要说的是，当读到这里的时候，我们应该会觉得在子类中重定义 setter 和 deleter 的工作多少得到了一些简化。虽然这里给出的解决方案仍然能够正常工作，但是在 Python的问题报告页面中提到的这个 bug（http://bugs.python.org/issue14965）可能会使得在未来的 Python 版本中产生出一种更加清晰的解决方案。

# 8.9 创建一种新形式的类属性或实例属性

# 8.9.1 问题

我们想创建一种新形式的实例属性，它可以拥有一些额外的功能，比如说类型检查。

# 8.9.2 解决方案

如果想创建一个新形式的实例属性，可以以描述符类的形式定义其功能。示例如下：

```python
Descriptor attribute for an integer type-checked attribute class Integer: def __init__(self, name): self.name = name def __get__(self, instance, cls): if instance is None: return self else: return instance._dict_[self.name] def __set__(self, instance, value): if not isinstance(value, int): raise TypeError('Expected an int') instance._dict_[self.name] = value def __delete__(self, instance): del instance._dict_[self.name] 
```

所谓的描述符就是以特殊方法__get__()、__set__()和__delete__()的形式实现了三个核心的属性访问操作（对应于get、set和delete）的类。这些方法通过接受类实例作为输入来工作。之后，底层的实例字典会根据需要适当地进行调整。

要使用一个描述符，我们把描述符的实例放置在类的定义中作为类变量来用。示例如下：

```python
class Point:  
    x = Integer('x')  
    y = Integer('y')  
def __init__(self, x, y):  
    self.x = x  
    self.y = y 
```

当这么做时，所有针对描述符属性（即，这里的 x 或 y）的访问都会被__get__()、__set__()和__delete__()方法所捕获。示例如下：

```txt
>>> p = Point(2, 3)  
>>> p.x # Calls Point.x_get_(p, Point)  
2  
>>> p.y = 5 # Calls Point.y_set_(p, 5)  
>>> p.x = 2.3 # Calls Point.x_set_(p, 2.3)  
Traceback (most recent call last): 
```

```txt
File "<stdin>", line 1, in <module> File "descrip.py", line 12, in __set__ raise TypeError('Expected an int') TypeError: Expected an int >>> 
```

每个描述符方法都会接受被操纵的实例作为输入。要执行所请求的操作，底层的实例字典（即__dict__属性）会根据需要适当地进行调整。描述符的 self.name 属性会保存字典的键，通过这些键可以找到存储在实例字典中的实际数据。

# 8.9.3 讨论

对于大多数 Python 类的特性，描述符都提供了底层的魔法，包括@classmethod、$@$ staticmethod、 $@$ property 甚至__slots__。

通过定义一个描述符，我们可以在很底层的情况下捕获关键的实例操作（get、set、delete），并可以完全自定义这些操作的行为。这种能力非常强大，这也是那些编写高级程序库和框架的作者们所使用的最为重要的工具之一。

关于描述符，常容易困惑的地方就是它们只能在类的层次上定义，不能根据实例来产生。因此，下面这样的代码是无法工作的：

```python
Does NOT work   
class Point: def init_(self, x, y): self.x = Integer('x') # No! Must be a class variable self.y = Integer('y') self.x = x self.y = y 
```

此外，在实现__get__()方法时比想象中的还要复杂一些：

```python
Descriptor attribute for an integer type-checked attribute class Integer: def __get__(self, instance, cls): if instance is None: return self else: return instance._dict_[self.name] 
```

_get__()看起来多少有些复杂的原因在于实例变量和类变量之间是有区别的。如果是以类变量的形式访问描述符，参数 instance 应该设为 None。在这种情况下，标准做法就是简单地返回描述符实例本身（尽管此时做任何类型的自定义处理也是允许的）。示

例如下：

```diff
>>> p = Point(2,3)
>>> p.x      # Calls Point.x._get_(p, Point)
2
>>> Point.x      # Calls Point.x._get_(None, Point)
<__main__.Integer object at 0x100671890>
>>> 
```

描述符常常会作为一个组件出现在大型的编程框架中，其中还会涉及装饰器或者元类。正因为如此，对描述符的使用可能隐藏得很深，几乎看不到痕迹。例如，下面是一些更加高级的基于描述符的代码，其中还用到了类装饰器：

# Descriptor for a type-checked attribute   
class Typed: def __init__(self, name, expected_type): self.name $=$ name self(expected_type $=$ expected_type def __get__(self, instance, cls): if instance is None: return self else: return instance._dict_[self.name] def __set__(self, instance, value): if not instanceof(value, self expects_type): raise TypeError('Expected' $^+$ str(self expects_type)) instance._dict_[self.name] $=$ value def __delete__(self, instance): del instance._dict_[self.name]   
# Class decorator that applies it to selected attributes   
def typeassert(\*\*kwargs): def decorate(cls): for name, expected_type in kwargs.items(): # Attach a Typed descriptor to the class setattr(cls, name,Typed(name, expected_type)) return cls return decorate   
# Example use   
@typeassert(name $\equiv$ str, shares $\equiv$ int, price $\equiv$ float)   
class Stock: def __init__(self, name, shares, price):

```txt
self.name = name  
self.share = shares  
self.price = price 
```

最后，应该强调的是，如果只是想访问某个特定的类中的一种属性，并对此做定制化处理，那么最好不要编写描述符来实现。对于这个任务，用property属性方法来完成会更加简单（见 8.6 节）。在需要大量重用代码的情况下，描述符会更加有用（例如，我们希望在自己的代码中大量使用描述符提供的功能，或者将其作为库来使用）。

# 8.10 让属性具有惰性求值的能力

# 8.10.1 问题

我们想将一个只读的属性定义为 property 属性方法，只有在访问它时才参与计算。但是，一旦访问了该属性，我们希望把计算出的值缓存起来，不要每次访问它时都重新计算。

# 8.10.2 解决方案

定义一个惰性属性最有效的方式就是利用描述符类来完成，示例如下：

class lazyproperty: def __init__(self, func): self FUNC $=$ func def __get__(self,instance,cls): if instance is None: return self else: value $=$ self FUNC.instance) setattr(object,self FUNC._name_,value) return value

要使用上述代码，可以像下面这样在某个类中使用它：

import math   
class Circle: def __init__(self, radius): self.radius = radius @lazyproperty def area(self): print('Computing area') return math.pi \* self_radius $^{**}$ 2

```txt
@lazyproperty   
def perimeter(self): print('Computing perimeter') return 2 \* math.pi \* self.radius 
```

下面的交互式会话说明了这是如何工作的：

```txt
>>> c = Circle(4.0)  
>>> c.radius  
4.0  
>>> c.area  
Computing area  
50.26548245743669  
>>> c.area  
50.26548245743669  
>>> c.perimeter  
Computing perimeter  
25.132741228718345  
>>> c.perimeter  
25.132741228718345  
>>> 
```

请注意，这里的“Computing area”和“Computing perimeter”只打印了一次。

# 8.10.3 讨论

在大部分情况下，让属性具有惰性求值能力的全部意义就在于提升程序性能。例如，除非确实需要用到这个属性，否则就可以避免进行无意义的计算。本节给出的解决方案正是应对于此，而且利用了描述符的微妙特性，使得能够以高效的方式来达成。

在 8.9 节中讲过，当把描述符放到类的定义体中时，访问它的属性会触发__get__()、__set__()和__delete__()方法得到执行。但是，如果一个描述符只定义了__get__()方法，则它的绑定关系比一般情况下要弱化很多（much weaker binding）。特别是，只有当被访问的属性不在底层的实例字典中时，__get__()方法才会得到调用。

示例中的 lazyproperty 类通过让__get__()方法以 property 属性相同的名称来保存计算出的值。这么做会让值保存在实例字典中，可以阻止该 property 属性重复进行计算。仔细观察下面的示例就能发现这一点：

```txt
>>> c = Circle(4.0)  
>>> # Get instance variables  
>>> vars(c)  
{'radius': 4.0} 
```

```txt
>>> # Compute area and observe variables afterward  
>>> c.area  
Computing area  
50.26548245743669  
>>> vars(c)  
{'area': 50.26548245743669, 'radius': 4.0}  
>>> # Notice access doesn't invoke property anymore  
>>> c.area  
50.26548245743669  
>>> # Delete the variable and see property trigger again  
>>> del c.area  
>>> vars(c)  
{'radius': 4.0}  
>>> c.area  
Computing area  
50.26548245743669  
>>> 
```

本节讨论的技术有一个潜在的缺点，即，计算出的值在创建之后就变成可变的（mutable）了。示例如下：

```txt
>>> c.area  
Computing area  
50.26548245743669  
>>> c.area = 25  
>>> c.area  
25  
>>> 
```

如果需要考虑可变性的问题，可以使用另外一种方式实现，但执行效率会稍打折扣：

```python
def lazyproperty(func):
    name ='_lazy_' + func.__name__
    @property
def lazy(self):
    if hasattr(self, name):
        return getattr(self, name)
    else:
        value = func(self)
        setattr(self, name, value)
        return value
    return lazy 
```

如果使用这个版本的实现，就会发现 set 操作是不允许执行的。示例如下：

```txt
>>> c = Circle(4.0)  
>>> c.area  
Computing area  
50.26548245743669  
>>> c.area  
50.26548245743669  
>>> c.area = 25  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
AttributeError: can't set attribute  
>>> 
```

但是，这种方式的缺点就是所有的 get 操作都必须经由属性的 getter 函数来处理。这比直接在实例字典中查找相应的值要慢一些。

更多有关 property 和可管理属性的信息，请参见 8.6 节。描述符在 8.9 节已有详尽的讲解。

# 8.11 简化数据结构的初始化过程

# 8.11.1 问题

我们编写了许多类，把它们当做数据结构来用。但是我们厌倦了编写高度重复且样式相同的__init__()函数。

# 8.11.2 解决方案

通常我们可以将初始化数据结构的步骤归纳到一个单独的__init__()函数中，并将其定义在一个公共的基类中。示例如下：

class Structure: # Class variable that specifies expected fields _fields $=$ [] def init_self, \*args): if len(args) $! =$ len(self._fields): raise TypeError('Expected {} arguments'.format(len(self._fields))) # Set the arguments for name, value in zip(self._fields, args): setattr(self, name, value) # Example class definitions if_name $= =$ 'main': class Stock(Structure): _fields $=$ ['name', 'shares', 'price']

```python
class Point (Structure):
    _fields = ['x', 'y']
class Circle (Structure):
    _fields = ['radius']
def area(self):
    return math.pi * self.radius ** 2 
```

如果使用这些类，就会发现它们非常易于构建。示例如下：

```python
>>> s = Stock('ACME', 50, 91.1)
>>> p = Point(2, 3)
>>> c = Circle(4.5)
>>> s2 = Stock('ACME', 50)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "structure.py", line 6, in __init__
    raise TypeError('Expected {} arguments'.format(len(self._fields)))
TypeError: Expected 3 arguments 
```

我们应该提供对关键字参数的支持，这里有几种设计上的选择。一种选择就是对关键字参数做映射，这样它们就只对应于定义在_fields 中的属性名。示例如下：

class Structure: _fields $=$ [] def init_self, \*args, \*\*kwargs): if len(args) $>$ len(self._fields): raise TypeError('Expected {} arguments'.format(len(self._fields))) # Set all of the positional arguments for name, value in zip(self._fields, args): setattr(self, name, value) # Set the remaining keyword arguments for name in self._fields[len(args)]: setattr(self, name, kwargs.pop(name)) # Check for any remaining unknown arguments if kwargs: raise TypeError('Invalid argument(s): {}.format('', '.join(kwargs))') # Example use if_name $= =$ 'main': class Stock(Structure): _fields $=$ ['name', 'shares', 'price']

```python
s1 = Stock('ACME', 50, 91.1)  
s2 = Stock('ACME', 50, price=91.1)  
s3 = Stock('ACME', shares=50, price=91.1) 
```

另一种可能的选择是利用关键字参数来给类添加额外的属性，这些额外的属性是没有定义在_fields 中的。示例如下：

class Structure: # Class variable that specifies expected fields _fields $=$ [] def init_self, \*args, \*\*kwargs): if len(args) $! =$ len(self._fields): raise TypeError('Expected {} arguments'.format(len(self._fields))) # Set the arguments for name, value in zip(self._fields, args): setattr(self, name, value) # Set the additional arguments (if any) extra_args $=$ kwargs.keys() - self._fields for name in extra_args: setattr(self, name, kwargs.pop(name)) if kwargs: raise TypeError('Duplicate values for {}.format', '.join(kwargs))) # Example use if_name $= =$ 'main': class Stock(Structure): _fields $=$ ['name','shares','price'] s1 $=$ Stock('ACME',50,91.1) s2 $=$ Stock('ACME',50,91.1, date $= 8 / 2 / 2012$

# 8.11.3 讨论

如果要编写的程序中有大量小型的数据结构，那么定义一个通用型的__init__()方法会特别有用。相比于下面这样手动编写每个__init__()方法，这么做可使得代码量大大减少：

class Stock: def__init__(self，name，shares，price): self.name $=$ name self.share $=$ shares self.price $=$ price

class Point: def __init__(self, x, y): self.x = x self.y = y   
class Circle: def __init__(self, radius): self.radius = radius def area(self): return math.pi \* self_radius $^ { \text{其} }$ 2

我们给出的实现中，一个微妙之处在于使用了 setattr()函数来设定属性值。与之相反的是，有人可能会倾向于直接访问实例字典。示例如下：

class Structure: # Class variable that specifies expected fields _fields $=$ [] def __init__(self, \*args): if len(args) $! =$ len(self._fields): raise TypeError('Expected {} arguments'.format(len(self._fields))) # Set the arguments (alternate) self._dict_.update(zip(self._fields,args))

尽管这么做行得通，但像这样假设子类的实现通常是不安全的。如果某个子类决定使用__slots__或者用 property（也可以是描述符）包装了某个特定的属性，直接访问实例字典就会产生崩溃。我们给出的解决方案已经尽可能地做到通用，不会对子类的实现做任何假设。

这种技术的一个潜在缺点就是会影响到 IDE（集成开发环境）的文档和帮助功能。如果用户针对某个特定的类寻求帮助，那么所需的参数将不会以正常的形式来表述。示例如下：

```txt
>>> help(Stock)   
Help on class Stock in module __main_:   
class Stock(Structure)   
...   
Methods inherited from Structure:   
| _init_(self, \*args, \*\*kwargs)   
1   
>>> 
```

这些问题可以通过在__init__()函数中强制施行类型签名来解决，相关内容请参阅9.16节。应该指出的是，也可以采用所谓的“frame hack”技巧来实现自动化的实例变量初始化处理，只要编写一个功能函数即可。示例如下：

```python
def init_fromlocals(self):
    import sys
    locs = sys._getframe(1).f_locals
    for k, v in locs.items():
        if k != 'self':
            setattr(self, k, v)
class Stock:
    def __init__(self, name, shares, price):
        init_fromlocals(self) 
```

在这种方法中，函数 init_fromlocals()利用 sys._getframe()来获取调用方的局部变量。如果在__init__()方法中首先调用这个函数，那么获取到的局部变量就和传递给__init__()方法的参数是一致的，可以轻松用来设定属性。尽管这种方法可以避免在 IDE 中出现获取到不一致的调用签名问题，但比起解决方案中提供的方法要慢上 $50 \%$ ，也需要程序员输入更多的代码，这种方法在幕后也做了更加复杂的操作。如果我们的代码不需要这种额外的能力，那么通常更简单的方案会更好。

# 8.12 定义一个接口或抽象基类

# 8.12.1 问题

我们想定义一个类作为接口或者是抽象基类，这样可以在此之上执行类型检查并确保在子类中实现特定的方法。

# 8.12.2 解决方案

要定义一个抽象基类，可以使用 abc 模块。示例如下：

from abc import ABCMeta, abstractmethod   
class IStream(metaclass $\equiv$ ABCMeta): @abstractmethod def read(self,maxbytes=-1): pass @abstractmethod def write(self,data): pass

抽象基类的核心特征就是不能被直接实例化。例如，如果尝试这么做，会得到错

误提示：

```ruby
a = IStream() #TypeError: Can't instantiate abstract class # IStream with abstract methods read, write 
```

相反，抽象基类是用来给其他的类当做基类使用的，这些子类需要实现基类中要求的那些方法。示例如下：

```txt
class SocketStream(IStream): def read(self, maxbytes=-1): ... def write(self, data): 
```

抽象基类的主要用途是强制规定所需的编程接口。例如，一种看待 IStream 基类的方式就是在高层次上指定一个接口规范，使其允许读取和写入数据。显式检查这个接口的代码可以写成如下形式：

```python
def serialize(obj, stream):
    if not isinstance(stream, IStream):
        raise TypeError('Expected an IStream') 
```

我们可能会认为这种形式的类型检查只有在子类化抽象基类（ABC）时才能工作，但是抽象基类也允许其他的类向其注册，然后实现所需的接口。例如，我们可以这样做：

```python
import io   
# Register the built-in I/O classes as supporting our interface   
ISream.register(io.IOBase)   
# Open a normal file and type check   
f = open('foo.txt')   
isinstance(f, ISream) # Returns True 
```

应该提到的是， $@$ abstractmethod 同样可以施加到静态方法、类方法和 property 属性上。只要确保以合适的顺序进行添加即可，这里 $@$ abstractmethod 要紧挨着函数定义。示例如下：

from abc import ABCMeta, abstractmethod   
class A(metaclass $\equiv$ ABCMeta): @property @abstractmethod def name(self): pass

```python
@name setter   
@abstractmethod   
def name(self, value): pass   
@klassmethod   
@abstractmethod   
def method1(cls): pass   
@staticmethod   
@abstractmethod   
def method2(): pass 
```

# 8.12.3 讨论

标准库中已经预定义好了一些抽象基类。collections模块中定义了多个和容器还有迭代器（序列、映射、集合等）相关的抽象基类。numbers库中定义了和数值对象（整数、浮点数、复数等）相关的抽象基类。io 库中定义了和 I/O 处理相关的抽象基类。

可以使用这些预定义好的抽象基类来执行更加一般化的类型检查。下面是一些例子：

import collections   
```txt
Check if x is a sequence   
if isinstance(x, collections.Sequence): 1   
# Check if x is iterable   
if isinstance(x, collections.Iterable): 1   
# Check if x has a size   
if isinstance(x, collections.Sized): 1   
# Check if x is a mapping   
if isinstance(x, collections+mapping): 
```

应该提到的是，在写作本节时，某些库和模块并没有像我们所期望的那样利用预定义好的抽象基类。例如：

from decimal import Decimal   
import numbers $\mathbf{x} =$ Decimal('3.4')   
isinstance(x, numbers.Real) # Returns False

虽然从技术上说 3.4 是一个实数，由于我们无意中将浮点数和小数混在一起，这里的类型检查没有起到应有的作用。因此，如果使用了抽象基类的功能，明智的做法是仔细编写测试用例来验证其行为是否是所期待的。

尽管抽象基类使得类型检查变得更容易了，但不应该在程序中过度使用它。Python 的核心在于它是一种动态语言，它带来了极大的灵活性。如果处处都强制实行类型约束，则会使得代码变得更加复杂，而这本不应该如此。我们应该拥抱Python的灵活性。

# 8.13 实现一种数据模型或类型系统

# 8.13.1 问题

我们想定义各种各样的数据结构，但是对于某些特定的属性，我们想对允许赋给它们的值强制添加一些限制。

# 8.13.2 解决方案

在这个问题中，基本上我们面对的任务就是在设定特定的实例属性时添加检查或者断言。为了做到这点，需要对每个属性的设定做定制化处理，因此应该使用描述符来完成。

下面的代码使用描述符实现了一个类型系统以及对值进行检查的框架：

Base class. Uses a descriptor to set a value   
class Descriptor: def init_(self, name $\equiv$ None, \*\*opts): self.name $=$ name for key, value in opts.items(): setattr(self, key, value) def set_(self, instance, value): instance._dict_[self.name] $=$ value   
#Descriptor for enforcing types   
classTyped(Descriptor): expected_type $=$ type(None)

```python
def __set__(self, instance, value):
    if not isinstance(value, self(expected_type):
        raise TypeError('expected' + str(self(expected_type))
        super().__set__(instance, value)
# Descriptor for enforcing values
class Unsigned(Descriptor):
    def __set__(self, instance, value):
        if value < 0:
            raise ValueError('Expected >= 0')
        super().__set__(instance, value)
class MaxSized(Descriptor):
    def __init__(self, name=None, **args):
        if 'size' not in args:
            raise ValueError('missing size option')
        super().__init__(name, **args)
    def __set__(self, instance, value):
        if len(value) >= self.size:
            raise ValueError('size must be <' + str(self.size))
        super().__set__(instance, value) 
```

这些类可作为构建一个数据模型或者类型系统的基础组件。让我们继续，下面这些代码实现了一些不同类型的数据：

```python
class Integer(Typed):
    expected_type = int
class UnsignedInteger(Integer, Unsigned):
    pass
class Float(Typed):
    expected_type = float
class UnsignedFloat FLOAT, Unsigned):
    pass
class String(Typed):
    expected_type = str
class SizedString(String, MaxSized):
    pass 
```

有了这些类型对象，现在就可以像这样定义一个类了：

class Stock: #Specify constraints name $=$ SizedString('name',size $\equiv$ 8) shares $=$ UnsignedInteger('shares') price $=$ UnsignedFloat('price') def init_(self, name, shares, price): self.name $=$ name self.share $=$ shares self.price $=$ pric

有了这些约束后，就会发现现在对属性进行赋值是会进行验证的。示例如下：

```python
>>> s = Stock('ACME', 50, 91.1)
>>> s.name
'ACME'
>>> sshares = 75
>>> sshares = -10
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "example.py", line 17, in __set__
        super().__set__(instance, value)
    File "example.py", line 23, in __set__
        raise ValueError('Expected >= 0')
ValueError: Expected >= 0
>>> s.price = 'a lot'
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "example.py", line 16, in __set__
        raise ValueError('expected + str(self(expected_type))
TypeError: expected <class 'float']
>>> s.name = 'ABRACADABRA'
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "example.py", line 17, in __set__
        super().__set__(instance, value)
    File "example.py", line 35, in __set__
        raise ValueError('size must be <' + str(self.size))
ValueError: size must be < 8
>>> 
```

可以运用一些技术来简化在类中设定约束的步骤。一种方法是使用类装饰器，示例如下：

```python
Class decorator to apply constraints def check_attributes(\*\*kwargs): def decorate(cls): 
```

```python
for key, value in kwargs.items():
    if isinstance(value, Descriptor):
        value.name = key
        setattr(cls, key, value)
    else:
        setattr(cls, key, value(key))
    return cls
    return decorate
# Example
@check_attributes(name= SizedString(size=8),
                  shares=UnsignedInteger,
                  price=UnsignedFloat)
class Stock:
    def __init__(self, name, shares, price):
        self.name = name
        self.share = shares
        self.price = price 
```

另一种方法是使用元类，示例如下：

A metaclass that applies checking   
class checkedmeta(type): def new (cls, clsname, bases, methods): #Attach attribute names to the descriptors for key, value in methods.items(): ifisinstance(value, Descriptor): value.name $=$ key return type._new(cls,clsname,bases,methods) #Example   
class Stock(metaclass=checkedmeta): name $=$ SizedString(size $\coloneqq$ 8) shares $=$ UnsignedInteger() price $=$ UnsignedFloat() def init_(self,name,shares,price): self.name $=$ name self.share $=$ shares self.price $=$ price

# 8.13.3 讨论

本节涉及了好几种高级技术，包括描述符、mixin 类、对 super()的使用、类装饰器以及元类。在这里涵盖所有这些主题的基础知识显然是不现实的，读者可以在其他章节中找到相关的示例（参阅 8.9、8.18、9.12 以及 9.19 节）。但是，还是有几个微妙之处值

得我们讨论。

首先，在 Descriptor 基类中会发现有一个__set__()方法，但是却没有与之对应的__get__()方法。如果一个描述符所做的仅仅只是从底层的实例字典中提取出具有相同名称的值，那么定义__get__()就是不必要的了。实际上，在这里定义__get__()反而会让程序运行得更慢。因此，本节只会把重点放在对__set__()的实现上。

本节中各个描述符类的总体设计是基于 mixin 类的。例如，Unsigned 和 MaxSized 类是用来和其他从 Typed 类中继承而来的描述符类混合在一起使用的。要处理某种特定的数据类型，我们使用多重继承来将所需要的功能联合在一起使用。

我们也会注意到所有描述符的__init__()方法已经被编写为具有相同的签名形式，其中涉及关键字参数**opts。MaxSized 类会在 opts 中寻找它所需要的属性，但是会将其传递给基类 Descriptor，然后在基类中完成实际的设定。像这样的组合类（尤其是 mixin），一个棘手的地方在于我们并非总是知道这些类是如何串联起来的，或者 super()到底会调用些什么。基于这个原因，需要保证让所有可能出现的组合类都能正常工作。

各种类型类（type classs）的定义比如 Integer、Float 以及 String 展示了一项有用的技术，即，使用类变量来定制化实现。描述符 Typed 仅仅是寻找一个 expected_type 属性，该属性是由那些子类所提供的。

使用类装饰器或者元类常常可以简化用户代码。我们会发现在这些例子中，用户不再需要多次输入属性名了。示例如下：

# Normal   
class Point: $\begin{array}{rl} & {\mathrm{x} = \mathrm{Integer}('x')}\\ & {\mathrm{y} = \mathrm{Integer}('y')} \end{array}$ #Metaclass   
class Point (metaclass=checkedmeta): $\begin{array}{l}\mathrm{x} = \mathrm{Integer}()\\ \mathrm{y} = \mathrm{Integer}() \end{array}$

实现类装饰器和元类的代码会扫描类字典，寻找描述符。当找到描述符后，它们会根据键的值自动填入描述符的名称。

在所有方法中，类装饰器可以提供最大的灵活性和稳健性。第一，这种解决方案不依赖于任何高级的机制，比如说元类。第二，装饰器可以很容易地根据需要在类定义上添加或者移除。例如，在装饰器中，可以有一个选项来简单地忽略掉添加的检查机制。这样就能让检查机制可以根据需要随意打开或关闭（调试环境对比生产环境）。

最后，采用类装饰器的解决方案也可以用来取代 mixin 类、多重继承以及对 super()函数

的使用。下面就是使用类装饰器的备选方案：

# Base class. Uses a descriptor to set a value

class Descriptor:

```python
def __init__(self, name=None, **args):  
    self.name = name  
    for key, value in args.items():  
        setattr(self, key, value) 
```

```python
def __set__(self, instance, value):
    instance.__dict__(self.name) = value 
```

# Decorator for applying type checking

def Typed(expected_type, cls=None):

```txt
if cls is None: return lambda cls:Typed(expected_type, cls) 
```

```python
super_set = cls._set_
def __set__(self, instance, value):
    if not isinstance(value, expected_type):
        raise TypeError('expected' + str(expected_type))
    super_set(self, instance, value)
cls._set_ = __set_
return cls 
```

# Decorator for unsigned values

```python
def Unsigned(cls):
    super_set = cls._set_
    def _set__(self, instance, value):
        if value < 0:
            raise ValueError('Expected >= 0')
        super_set(self, instance, value)
        cls._set_ = _set_
    return cls 
```

# Decorator for allowing sized values

```python
def MaxSized(cls):
    super_init = cls.__init__
    def __init__(self, name=None, **args):
        if 'size' not in args:
            raise TypeError('missing size option')
        super_init(self, name, **args)
        cls.__init__ = __init__
    super_set = cls.__set__ 
```

```python
def __set__(self, instance, value):
    if len(value) >= self.size:
        raise ValueError('size must be <' + str(self.size))
        super_set(self, instance, value)
        cls.__set__ = __set__
        return cls
# Specialized descriptors
@Typed(int)
class Integer(Descriptor):
    pass
@Unsigned
class UnsignedInteger(Integer):
    pass
@Typed(float)
class Float(Descriptor):
    pass
@Unsigned
class UnsignedFloat FLOAT):
    pass
@Typed(str)
class String(Descriptor):
    pass
@Max Sized
class SizedString(String):
    pass 
```

在这个备选方案中定义的类能够像之前那样以完全相同的方式工作（之前的示例代码都不改变），只是每个部分都会比以前运行得更快。例如，对设定一个类型属性做简单的计时测试就能发现，采用类装饰器的方案运行速度要比采用 mixin 类的方案几乎快上 $100 \%$ 。读到这里你难道还会不开心吗？

# 8.14 实现自定义的容器

# 8.14.1 问题

我们想实现一个自定义的类，用来模仿普通的内建容器类型比如列表或者字典的行为。但是，我们并不完全确定需要实现什么方法来完成。

# 8.14.2 解决方案

collections 库中定义了各种各样的抽象基类，当实现自定义的容器类时它们会非常有用。为了说明清楚，假设我们希望自己的类能够支持迭代操作。要做到这点，只要简单地从 collections.Iterable 中继承即可，就像下面这样：

```txt
import collections   
class Acollections.Iterable): pass 
```

从 collections.Iterable 中继承的好处就是可以确保必须实现所有所需的特殊方法。如果不这么做，那么在实例化时就会得到错误信息：

```txt
>>> a = A()
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: Can't instantiate abstract class A with abstract methods __iter__ >>> 
```

要修正这个错误，只要在类中实现所需的__iter__()方法即可（参见 4.2 和 4.7 节）。

在 collections 库中还有其他一些值得一提的类，包括 Sequence、MutableSequence、Mapping、MutableMapping、Set 以及 MutableSet。这些类中有许多是按照功能层次的递增来进行排列的（例如，Container、Iterable、Sized、Sequence 以及 MutableSequence就是一种递增式的排列）。再次说明，只要简单地对这些类进行实例化操作，就可以知道需要实现哪些方法才能让自定义的容器具有相同的行为：

```python
>>> import collections
>>> collections.Sequence()
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: Can't instantiate abstract class Sequence with abstract methods \
__getitem__, __len__
>>> 
```

下面有个简单的例子。我们在自定义类中实现了上述所需的方法，创建了一个 Sequence类，且元素总是以排序后的顺序进行存储（我们的例子实现的不是很高效，但能说明大意）：

import collections   
import bisect   
class SortedItemscollections.Sequence): def __init__(self,initial $\equiv$ None):

下面是使用这个类的例子：  
```python
self._items = sorted(initial) if initial is None else []
# Required sequence methods
def __getitem__(self, index):
    return self._items[index]
def __len__(self):
    return len(self._items)
# Method for adding an item in the right location
def add(self, item):
    bisect.insert(self._items, item) 
```

```txt
>>> items = SortedItems([5, 1, 3])
>>> list(items)
[1, 3, 5]
>>> items[0]
1
>>> items[-1]
5
>>> items.add(2)
>>> list(items)
[1, 2, 3, 5]
>>> items.add(-10)
>>> list(items)
[-10, 1, 2, 3, 5]
>>> items[1:4]
[1, 2, 3]
>>> 3 in items
True
>>> len(items)
5
>>> for n in items:
...     print(n)
...
-10
1
2
3
5
>>> 
```

可以看到，SortedItems 的实例所表现出的行为和一个普通的序列对象完全一样，并且支持所有常见的操作，包括索引、迭代、len()、是否包含（in 操作符）甚至是分片。

顺便说一句，本节中用到的 bisect 模块能够方便地让列表中的元素保持有序。bisect.insort()函数能够将元素插入到列表中且让列表仍然保持有序。

# 8.14.3 讨论

从 collections 库中提供的抽象基类继承，可确保我们的自定义容器实现了所有所需的方法。但是，这种继承也便于我们做类型检查。

例如，我们的自定义容器将能够满足各种各样的类型检查：

```haskell
>>> items = SortedItems()
>>> import collections
>>> isinstance(items, collections.Iterable)
True
>>> isinstance(items, collections.Sequence)
True
>>> isinstance(items, collections.Container)
True
>>> isinstance(items, collections.Sized)
True
>>> isinstance(items, collections.mapping)
False
>>> 
```

collections模块中的许多抽象基类还针对常见的容器方法提供了默认实现。为了说明，假设有一个类从 collections.MutableSequence 中继承而来，就像这样：

class ItemscollectionsicutableSequence): def init_(self, initial $\equiv$ None): self._items $=$ list(initial) if initial is None else [] #Required sequence methods def __getitem__(self,index): print('Getting:,'index) return self._items[index] def __setitem__(self,index, value): print('Setting:,'index, value) self._items[index] $=$ value def __delitem__(self,index): print('Deleting:'，index) del self._items[index] def insert(self,index, value): print('Inserting:'，index, value)

```python
self._items.insert(index, value)  
def __len__(self):  
    print('Len')  
    return len(self._items) 
```

如果创建一个 Items 实例，就会发现它几乎支持列表所有的核心方法（例如 append()、remove()、count()等）。这些方法在实现的时候只使用了所需要的那些特殊方法。下面的交互式会话说明了这一点：

```txt
>>> a = Items([1, 2,  
>>> len(a)  
Len  
3  
>>> a.append(4)  
Len  
Inserting: 3 4  
>>> a.append(2)  
Len  
Inserting: 4 2  
>>> a.count(2)  
Getting: 0  
Getting: 1  
Getting: 2  
Getting: 3  
Getting: 4  
Getting: 5  
2  
>>> a.remove(3)  
Getting: 0  
Getting: 1  
Getting: 2  
Deleting: 2  
>>> 
```

本节仅仅只对 Python 的抽象类功能做了简要的介绍。numbers 模块中提供了与数值数据类型相关的类似的抽象基类。要获得更多有关如何创建自己的抽象基类的信息，请参阅 8.12 节。

# 8.15 委托属性的访问

# 8.15.1 问题

我们想在访问实例的属性时能够将其委托（delegate）到一个内部持有的对象上，这可

以作为继承的替代方案或者是为了实现一种代理机制。

# 8.15.2 解决方案

简单地说，委托是一种编程模式。我们将某个特定的操作转交给（委托）另一个不同的对象实现。通常来说，最简单的委托形式看起来是这样的：

```python
class A: def spam(self, x): pass def foo(self): pass   
class B: def __init__(self): self._a = A() def spam(self, x): # Delegate to the internal self._a instance return self._a.spam(x) def foo(self): # Delegate to the internal self._a instance return self._a.foo() def bar(self): pass 
```

如果仅有几个方法需要委托，编写像上面那样的代码是非常简单的。但是，如果有许多方法都需要委托，另一种实现方式是定义__getattr__()方法，就像下面这样：

```python
class A: def spam(self, x): pass def foo(self): pass   
class B: def __init__(self): self._a = A() def bar(self): pass 
```

```python
Expose all of the methods defined on class A def __getattr__(self, name): return getattr(self._a, name) 
```

_getattr__()方法能用来查找所有的属性。如果代码中尝试访问一个并不存在的属性，就会调用这个方法。在上面的代码中，我们在访问 B 中未定义的方法时就能把这个操作委托给 A。示例如下：

```python
b = B()
b.bar() # Calls B.bar() (exists on B)
b.spam(42) # Calls B._getattr('spam') and delegates to A.spam 
```

委托的另一个例子就是在实现代理时。示例如下：

```txt
A proxy class that wraps around another object, but # exposes its public attributes 
```

```python
class Proxy: def __init__(self, obj): self._obj = obj # Delegate attribute lookup to internal def __getattr__(self, name): print('getattr:','name) return getattr(self._obj, name) # Delegate attribute assignment def __setattr__(self, name, value): if name.startswith(_): super().__setattr__(name, value) else: print('setattr:','name, value) setattr(self._obj, name, value) # Delegate attribute deletion def __delattr__(self, name): if name.startswith(_): super().__delattr__(name) else: print('delattr:','name) delattr(self._obj, name) 
```

要使用这个代理类，只要简单地用它包装另一个实例即可。示例如下：

```python
class Spam: def __init__(self, x): 
```

$\mathrm{self.x} = \mathrm{x}$ def bar(self, y): print('Spam.bar:',' self.x,y) #Create an instance s $=$ Spam(2) #Create a proxy around it p $=$ Proxy(s) #Access the proxy print(p.x)#Outputs 2 p.bar(3)#Outputs"Spam.bar:23" p.x $= 37$ #Changes s.x to 37

通过自定义实现属性的访问方法，就可以对代理进行定制化处理，让其表现出不同的行为（例如，访问日志、只允许只读访问等）。

# 8.15.3 讨论

委托有时候可以作为继承的替代方案。例如，不要编写下面这样的代码：

```python
class A: def spam(self, x): print('A.spam',x) def foo(self): print('A.foo')   
class B(A): def spam(self,x): print('B.spam') super().spam(x) def bar(self): print('B.bar') 
```

用到了委托的实现方案则会是这样：

```python
class A: def spam(self, x): print('A.spam',x) def foo(self): print('A.foo') 
```

```python
class B: def __init__(self): self._a = A() def spam(self, x): print('B.spam',x) self._a.spam(x) def bar(self): print('B.bar') def __getattr__(self,name): return getattr(self._a，name) 
```

有时候当直接使用继承可能没多大意义，或者我们想更多地控制对象之间的关系（例如只暴露出特定的方法、实现接口等），此时使用委托会很有用。

当使用委托来实现代理时，这里还有几个细节需要注意。首先，__getattr__()实际上是一个回滚（fallback）方法，它只会在某个属性没有找到的时候才会调用。因此，如果访问的是代理实例本身的属性（例如本例中的_obj 属性），这个方法就不会被触发调用。其次，__setattr__()和__delattr__()方法需要添加一点额外的逻辑来区分代理实例本身的属性和内部对象_obj 上的属性。常用的惯例是代理类只委托那些不以下划线开头的属性（即，代理类只暴露内部对象中的“公有”属性）。

同样需要重点强调的是__getattr__()方法通常不适用于大部分名称以双下划线开头和结尾的特殊方法。例如，考虑下面这个类：

```python
class ListLike: def __init__(self): self._items = [] def __getattr__(self, name): return getattr(self._items, name) 
```

如果尝试创建一个 ListLike 对象，就会发现它能支持常见的列表方法，例如 append()和 insert()。但是，却无法支持 len()、查找元素等操作。示例如下：

```txt
>>> a = ListLike()
>>> a.append(2)
>>> a.insert(0, 1)
>>> a.sort()
>>> len(a)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: object of type 'ListLike' has no len()
>>> a[0] 
```

```txt
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: 'ListLike' object does not support indexing
>>> 
```

要支持不同的操作，必须自行手动委托相应的特殊方法。示例如下：

```python
class ListLike: def __init__(self): self._items = [] def __getattr__(self, name): return getattr(self._items, name) # Added special methods to support certain list operations def __len__(self): return len(self._items) def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __delitem__(self, index): del self._items[index] 
```

请参见 11.8 节中的另一个例子，我们在创建代理类时利用委托来完成远端过程调用。

# 8.16 在类中定义多个构造函数

# 8.16.1 问题

我们正在编写一个类，但是想让用户能够以多种方式创建实例，而不局限于__init__()提供的这一种。

# 8.16.2 解决方案

要定义一个含有多个构造函数的类，应该使用类方法。下面是一个简单的示例：

import time   
class Date: # Primary constructor def init_(self, year, month, day): self.year $=$ year self.month $=$ month self.day $=$ day

Alternate constructor   
@classmethod   
def today(cls): t $=$ time.localtime() returncls(t.tm_year,t.tm_mon,t.tm_mday)

要使用这个备选的构造函数，只要把它当做函数来调用即可，例如 Date.today()。示例如下：

```python
a = Date(2012, 12, 21) # Primary  
b = Date today() # Alternate 
```

# 8.16.3 讨论

类方法的一大主要用途就是定义其他可选的构造函数。类方法的一个关键特性就是把类作为其接收的第一个参数（cls）。我们会注意到，类方法中会用到这个类来创建并返回最终的实例。尽管十分微不足道，但正是这一特性使得类方法能够在继承中被正确使用。示例如下：

```python
class NewDate(Date):
    pass
c = Date today()  # Creates an instance of Date (cls=Date)
d = NewDatetoday()  # Creates an instance of NewDate (cls=NewDate) 
```

当定义一个有着多个构造函数的类时，应该让__init__()函数尽可能简单——除了给属性赋值之外什么都不做。如果需要的话，可以在其他备选的构造函数中选择实现更高级的操作。

与单独定义一个类方法不同的是，我们可能会倾向于让__init__()方法支持不同的调用约定。示例如下：

```python
class Date: def __init__(self, *args): if len(args) == 0: t = time.localtime() args = (t.tm_year, t.tm_mon, t.tm_mday) self.year, self.month, self.day = args 
```

尽管这种技术在某些情况下是行得通的，但常常会使代码变得难以理解也不好维护。比如说，这种实现不会展示出有用的帮助字符串（没有参数名称）。此外，创建 Date实例的代码也会变得不那么清晰。比较下面几种方式就能很容易看出区别：

```txt
a = Date(2012, 12, 21) # Clear. A specific date.  
b = Date() # ??? What does this do? 
```

```txt
Class method version c = Date today() # Clear. Today's date. 
```

根据上面的示例，Date.today()会调用 Date.__init__()方法，以合适的年份、月份和日期为参数实例化一个 Date 对象。如果有必要的话，甚至可以不调用__init__()方法就创建出类实例。我们将在下一节描述这种技术。

# 8.17 不通过调用 init 来创建实例

# 8.17.1 问题

我们需要创建一个实例，但是出于某些原因想绕过__init__()方法，用别的方式来创建。

# 8.17.2 解决方案

可以直接调用类的__new__()方法来创建一个未初始化的实例。例如，考虑下面这个类：

class Date: def__init__(self, year, month, day): self.year $=$ year self.month $=$ month self.day $=$ day

采用下面的方法可以在不调用__init__()的情况下创建一个 Date 实例：

```txt
>>> d = Date._new_(Date)
>>> d
<__main__.Date object at 0x1006716d0>
>>> d.year
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
AttributeError: 'Date' object has no attribute 'year'
>>> 
```

可以看到，得到的实例是未经初始化的。因此，给实例变量设定合适的初始值现在就成了我们的责任。示例如下：

```python
>>> data = {'year':2012, 'month':8, 'day':29}  
>>> for key, value in data.items():  
... setattr(d, key, value)  
...  
>>> d.year  
2012 
```

```txt
>>>d.month   
8   
>>> 
```

# 8.17.3 讨论

当需要以非标准的方式来创建实例时常常会遇到需要绕过__init__()的情况。比如反序列化（deserializing）数据，或者实现一个类方法将其作为备选的构造函数，都属于这种情况。例如，在前面给出的 Date 类中，有人可能会定义一个可选的构造函数 today()：

from time import localtime   
```python
class Date: def __init__(self, year, month, day): self.year = year self.month = month self.day = day @ classmethod def today (cls): d = cls._new_(cls) t = localtime() d.year = t.tm_year d.month = t.tm_mon d.day = t.tm_mday return d 
```

类似地，假设正在反序列化 JSON 数据，要产生一个下面这样的字典：

```python
data = {'year': 2012, 'month': 8, 'day': 29} 
```

如果想将这个字典转换为一个 Date 实例，只要使用解决方案中给出的技术即可。

当需要以非标准的方式创建实例时，通常最好不要对它们的实现做过多假设。因此，一般来说不要编写直接操纵底层实例字典__dict__的代码，除非能保证它已被定义。否则，如果类中使用了__slots__、property 属性、描述符或者其他高级技术，那么代码就会崩溃。通过使用 setattr()来为属性设定值，代码就会尽可能的通用。

# 8.18 用 Mixin 技术来扩展类定义

# 8.18.1 问题

我们有一些十分有用的方法，希望用它们来扩展其他类的功能。但是，需要添加方法

的这些类之间并不一定属于继承关系。因此，没法将这些方法直接关联到一个共同的基类上。

# 8.18.2 解决方法

本节提到的问题在需要对类进行定制化处理时通常会出现。例如，某个库提供了一组基础类以及一些可选的定制化方法，如果用户需要的话可以自行添加。

为了说明清楚，现在假设我们有兴趣将各式各样的定制化处理方法（例如，日志记录、类型检查等）添加到映射型对象（mapping object）上。下面有一组 mixin 类来完成这项任务：

```python
class LoggedMappingMixin: Add logging to get/set/delete operations for debugging. _.slots_ = () def __*_ (self, key): print('Getting ' + str(key)) return super().__*_ (key) def __*_ (self, key, value): print('Setting {} = {!r}').format(key, value)) return super().__*_ (key, value) def __*_ (self, key): print('Deleting ' + str(key)) return super().__*_ (key) class SetOnceMappingMixin: Only allow a key to be set once. ____________ slots_ = () def __*_ (self, key, value): if key in self: raise ValueError(str(key) + ' already set') return super().__*_ (key, value) class StringKeysMappingMixin: Restrict keys to strings only ____________ slots_ = () 
```

```python
def __setitem__(self, key, value):
    if not isinstance(key, str):
        raise TypeError('keys must be strings')
    return super().__setitem__(key, value) 
```

这些类本身是无用的。实际上，如果实例化它们中的任何一个，一点儿有用的事情都做不了（除了会产生异常之外）。相反，这些类存在的意义是要和其他映射型类通过多重继承的方式混合在一起使用。示例如下：

```python
>>> class LoggedDict(LoggedMappingMixin, dict):
...
...
...
>>> d = LoggedDict()
>>> d['x'] = 23
Setting x = 23
>>> d['x']
Getting x
23
>>> del d['x']
Deleting x
>>> from collections import defaultdict
>>> class SetOnzeDefaultDict(SetOnceMappingMixin, defaultdict):
...
...
>>> pass
...
>>> d = SetOnzeDefaultDict(list)
>>> d['x'].append(2)
>>> d['y'].append(3)
>>> d['x'].append(10)
>>> d['x'] = 23
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "mixin.py", line 24, in __setitem__
    raise KeyError(str(key) + ' already set')
KeyError: 'x already set'
>>> from collections import OrderedDict
>>> class StringOrderedDict(StringKeysMappingMixin,
...
...                  SetOnceMappingMixin,
...
...                  OrderedDict):
...
>>> d = StringOrderedDict()
>>> d['x'] = 23
>>> d[42] = 10 
```

```python
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "mixin.py", line 45, in __setitem__
    ---
TypeError: keys must be strings
>>> d['x'] = 42
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "mixin.py", line 46, in __setitem__
    __slots__ = ()
    File "mixin.py", line 24, in __setitem__
    if key in self:
        KeyError: 'x already set'
>>> 
```

在上面的示例中，可以发现这些 mixin 类和其他已有的类（例如：dict、defaultdict、OrderedDict）结合在了一起。当它们混合在一起时，所有的类通过一起工作提供所需的功能。

# 8.18.3 讨论

Python 标准库中到处都是 mixin 类的身影，大部分都是为了扩展其他类的功能而创建的，就和我们展示的示例类似。mixin 类也是多重继承的主要用途之一。例如，如果正在编写网络功能方面的代码，通常可以使用 socketserver 模块中的 ThreadingMixIn 类来为其他网络相关的类添加对线程的支持。例如，下面是一个多线程版的 XML-RPC 服务器：

```python
from xmlrpc.server import SimpleXMLRPCServer   
from socketserver import ThreadingMixIn   
class ThreadedXMLRPCServer(ThreadingMixIn, SimpleXMLRPCServer): pass 
```

我们在大型的库和框架中也能常看到 mixin 类——同样地，一般都是为了对已有的类增加一些可选的功能特性。

关于 mixin 类的理论，历史上有着许多讨论。但是，我们不再深入挖掘所有的细节，只需要记住几个重要的实现细节就够了。

首先，mixin 类绝不是为了直接实例化而创建的。例如，本节中所有的 mixin 类都不能独自工作。它们必须同另一个实现了所需的映射功能的类混合在一起用才行。同样地，socketserver 模块中的 ThreadingMixIn 类也必须同某个合适的 server 类混合在一起用才行——光靠它自己没用。

其次，mixin 类一般来说是没有状态的。这意味着 mixin 类没有__init__()方法，也没有

实例变量。在本节中，我们定义的__slots__ $\mathbf { \varepsilon } = \left( \mathbf { \varepsilon } \right)$ 就是一种强烈的提示，这表示 mixin 类没有属于自己的实例数据。

如果考虑定义一个拥有__init__()方法以及实例变量的 mixin 类，请注意这里会有极大的风险，因为这个类并不知道自己要和哪些其他的类混合在一起。因此，任何要创建出的实例变量都必须以某种方式加以命名，以此避免出现命名冲突。此外，mixin 类的__init__()方法必须要能合适地调用其他混合进来的类的__init__()方法。一般来说这很难实现，因为不知道其他类的参数签名是什么。至少，我们必须得实现非常通用的参数签名，这需要用到*arg、**kwargs。如果 mixin 类的__init__()方法自身还带有参数，那么那些参数应该只能通过关键字来指定，并且在命名上还得和其他参数区分开，避免命名冲突。对于定义了一个__init__()方法且接受一个关键字参数的 mixin 类，下面给出了一种可能的实现方法：

```python
class RestrictKeysMixin: def __init__(self, *args, _restrict_key_type, **kwargs): self._restrict_key_type = _restrict_key_type super().__init__(*args, **kwargs) def __setitem__(self, key, value): if not isinstance(key, self._restrict_key_type): raise TypeError('Keys must be' + str(self._restrict_key_type)) super().__setitem__(key, value) 
```

下面的例子展示了这个类应该如何使用：

```python
>>> class RDict(RestrictKeysMixin, dict):
...
...
>>> d = RDict(_restrict_key_type=str)
>>> e = RDict([( 'name', 'Dave'), ('n', 37), _restrict_key_type=str)
>>> f = RDict(name='Dave', n=37, _restrict_key_type=str)
>>> f
{'n': 37, 'name': 'Dave'}
>>> f[42] = 10
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "mixin.py", line 83, in __setitem__
    raise TypeError('Keys must be ' + str(self._restrict_key_type))
TypeError: Keys must be <class 'str'> 
```

在这个例子中，可以注意到初始化 RDict()时仍然带有可被 dict()所接受的参数，但是有一个额外的关键字参数 restrict_key_type 是提供给 mixin 类的。

最后，使用 super()函数是必要的，这也是编写 mixin 类的关键部分。在解决方案中，这些类重新定义了一些特定的关键方法，比如__getitem__()和__setitem__()。但是，它们也需要调用这些方法的原始版本。通过使用 super()，将这个任务转交给了方法解析顺序（MRO）上的下一个类。本节中的这部分内容对于 Python 新手来说可能不是那么容易理解，因为我们在没有父类的类中使用了 super()（初看上去感觉好像是个错误）。但是，对于类似下面这样的类定义：

```python
class LoggedDict(LoggedMappingMixin, dict):
    pass 
```

在LoggedMappingMixin 中使用super()函数会把任务转交到多重继承列表中的下一个类上。也就是说，在 LoggedMappingMixin 中调用 super().__getitem__()实际上会调用dict.__getitem__()。如果没有这种行为，mixin 类根本没法正常工作。

实现 mixin 的另一种方法是利用类装饰器。例如，考虑如下的代码：

```python
def LoggedMapping(cls):
    cls_getitem = cls._getitem_
    cls_setitem = cls._setitem_
    cls_delitem = cls._delitem_
    def __delitem__(self, key):
        print('Getting' + str(key))
        return cls_getitem(self, key)
    def __delitem__(self, key, value):
        print('Setting {} = {!r} '.format(key, value))
        return cls_setitem(self, key, value)
    def __delitem__(self, key):
        print('Deleting' + str(key))
        return cls_delitem(self, key)
    cls._getitem = __getitem_
    cls._setitem = __setitem_
    cls._delitem = __delitem_
    return cls 
```

我们把这个函数作为装饰器添加到类定义上。例如：

```txt
@LoggedMapping class LoggedDict (dict): pass 
```

如果试着这么做，就会发现能得到相同的行为，但是却完全不再涉及多重继承了。相

反，装饰器在这里只是对类定义做了一点点修改，从而替换掉特定的方法。有关类装饰器的更多细节可在 9.12 节中找到。

8.13 节中有一个高级的示例，其中同时用到了 mixin 技术和类装饰器。

# 8.19 实现带有状态的对象或状态机

# 8.19.1 问题

我们想实现一个状态机，或者让对象可以在不同的状态中进行操作。但是我们并不希望代码里会因此出现大量的条件判断。

# 8.19.2 解决方案

在某些应用程序中，我们可能会让对象根据某种内部状态来进行不同的操作。例如，考虑下面这个代表网络连接的类：

```python
class Connection: def __init__(self): self.state = 'CLOSED' def read(self): if self.state != 'OPEN': raise ValueError('Not open') print('reading') def write(self, data): if self.state != 'OPEN': raise ValueError('Not open') print('writing') def open(self): if self.state == 'OPEN': raise ValueError('Already open') self.state = 'OPEN' def close(self): if self.state == 'CLOSED': raise ValueError('Already closed') self.state = 'CLOSED' 
```

这份代码为我们提出了几个难题。首先，由于代码中引入了许多针对状态的条件检查，代码变得很复杂。其次，程序的性能下降了。因为普通的操作如读（read()）和写（write()）总是要在处理前先检查状态。

一个更加优雅的方式是将每种操作状态以一个单独的类来定义，然后在 Connection 类中使用这些状态类。示例如下：

```python
class Connection: def __init__(self): self.new_state(ClosedConnectionState) def new_state(self, newstate): self._state = newstate # Delegate to the state class def read(self): return self._state.read(self) def write(self, data): return self._state.write(self, data) def open(self): return self._state.open(self) def close(self): return self._state.close(self)   
#Connection state base class   
class ConnectionState: @staticmethod def read(conn): raise NotImplementedError() @staticmethod def write(conn, data): raise NotImplementedError() @staticmethod def open(conn): raise NotImplementedError() @staticmethod def close(conn): raise NotImplementedError() #Implementation of different states   
class ClosedConnectionState(ConnectionState): @staticmethod def read(conn): 
```

```python
raise RuntimeError('Not open')   
@staticmethod def write(conn, data): raise RuntimeError('Not open') @staticmethod def open(conn): conn.new_state(OpenConnectionState) @staticmethod def close(conn): raise RuntimeError('Already closed')   
class OpenConnectionState(ConnectionState): @staticmethod def read(conn): print('reading') @staticmethod def write(conn, data): print('writing') @staticmethod def open(conn): raise RuntimeError('Already open') @staticmethod def close(conn): conn.new_state(ClosedConnectionState) 
```

下面的交互式会话说明了这些类的用法：

```python
>>> c = Connection()
>>> c._state
<class__main__.ClosedConnectionState>
>>> c.read()
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "example.py", line 10, in read
        return self._state.read(self)
    File "example.py", line 43, in read
        raise ValueError('Not open')
RuntimeError: Not open
>>> c.open() 
```

```txt
>>> c._state  
<class__main__.OpenConnectionState'>  
>>> c.read()  
reading  
>>> c.write('hello')  
writing  
>>> c.close()  
>>> c._state  
<class__main__.ClosedConnectionState'>  
>>> 
```

# 8.19.3 讨论

编写含有大量复杂的条件判断并和各种状态纠缠在一起的代码是难以维护和解读的。本节给出的解决方案通过将各个状态分解为单独的类来避免这个问题。

可能看起来有些奇怪，这里每种状态都用类和静态方法来实现，在每个静态方法中都把 Connection 类的实例作为第一个参数。产生这种设计的原因在于我们决定在不同的状态类中不保存任何实例数据。相反，所有的实例数据应该保存在 Connection 实例中。将所有的状态放在一个公共的基类下，这么做的大部分原因是为了帮助组织代码，并确保适当的方法得到了实现。在基类方法中出现的 NotImplementedError 异常是为了确保在子类中实现了所需的方法。作为替代方案，可以考虑使用 8.12 节中描述过的抽象基类。

另一种实现方法是考虑去直接修改实例的__class__属性。示例如下：

```python
class Connection: def __init__(self): self.new_state(ClosedConnection) def new_state(self, newstate): self._class_ = newstate def read(self): raise NotImplementedError() def write(self, data): raise NotImplementedError() def open(self): raise NotImplementedError() def close(self): raise NotImplementedError() 
```

```python
class ClosedConnection(Connection): def read(self): raise ValueError('Not open') def write(self, data): raise ValueError('Not open') def open(self): self.new_state(OpenConnection) def close(self): raise ValueError('Already closed')   
class OpenConnection(Connection): def read(self): print('reading') def write(self, data): print('writing') def open(self): raise ValueError('Already open') def close(self): self.new_state(ClosedConnection) 
```

这种实现方法的主要特点就是消除了额外的间接关系。这里不再将 Connection 和ConnectionState 作为单独的类来实现，现在我们将它们合并在一起了。随着状态的改变，实例也会修改自己的类型。示例如下：

```txt
>>> c = Connection()
>>> c
<__main__.ClosedConnection object at 0x1006718d0>
>>> c.read()
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "state.py", line 15, in read
raise ValueError('Not open')
RuntimeError: Not open
>>> c.open()
>>> c
<__main__.OpenConnection object at 0x1006718d0>
>>> c.read()
reading
>>> c.close() 
```

```txt
>>>c  
<__main__.ClosedConnection object at 0x1006718d0>  
>>> 
```

面向对象编程的拥趸不喜欢这种直接修改实例的__class__属性的做法。但是在技术上是允许这么做的。此外，这么做也会让代码的执行速度更快些，因为现在调用 connection上的所有方法都不必再经过一层额外的间接步骤了。

最后，无论上面哪种技术对于实现复杂的状态机都是很有用的——尤其是在那些可能出现大量的 if-elif-else 块的代码中。示例如下：

# Original implementation   
class State:   
```python
def __init__(self):
    self.state = 'A'
def action(self, x):
    if state == 'A':
        # Action for A
            ...
        state = 'B'
    elif state == 'B':
        # Action for B
            ...
        state = 'C'
    elif state == 'C':
        # Action for C
            ...
        state = 'A' 
```

# Alternative implementation   
class State:   
```python
def __init__(self):
    self.new_state(State_A)
def new_state(self, state):
    self._class_ = state
def action(self, x):
    raise NotImplementedError() 
```

class State_A(State):   
```python
def action(self, x):
    # Action for A
    ...
    self.new_state(State_B) 
```

```python
class State_B(State):
    def action(self, x):
        # Action for B
        ...
        self.new_state(State_C)
class State_C(State):
    def action(self, x):
        # Action for C
        ...
        self.new_state(State_A) 
```

本节大体上是基于 Design Patterns: Elements of Resuable Object-Oriented Software（Addison-Wesley, 1995）一书中有关状态模式的内容来编写的。

# 8.20 调用对象上的方法，方法名以字符串形式给出

# 8.20.1 问题

我们想调用对象上的某个方法，现在这个方法名保存在字符串中，我们想通过它来调用该方法。

# 8.20.2 解决方案

对于简单的情况，可能会使用 getattr()，示例如下：

```python
import math   
class Point: def init_self, x, y): self.x = x self.y = y def repr_self): return 'Point(!r:{!r:})'.format(self.x,self.y) def distance(self,x,y): return math.hypot(self.x - x,self.y - y)   
p = Point(2,3)   
d = getattr(p,'distance') (0,0) # Calls pdistance(0,0) 
```

另一种方法是使用 operator.methodcaller()。示例如下：

```python
import operator
operator.method caller('distance', 0, 0) (p) 
```

如果想通过名称来查询方法并提供同样的参数反复调用该方法，那么 operator.methodcall()是很有用的。例如，如果你要对一整列点对象排序：

points $=$ [ Point(1,2), Point(3,0), Point(10,-3), Point(-5,-7), Point(-1,8), Point(3,2)] #Sort by distance from origin (0,0) points.sort(key $\equiv$ operator.methodcoder('distance',0,0))

# 8.20.3 讨论

调用一个方法实际上涉及两个单独的步骤，一是查询属性，二是函数调用。因此，要调用一个方法，可以使用 getattr()来查询相应的属性。要调用查询到的方法，只要把查询的结果当做函数即可。

operator.methodcall()创建了一个可调用对象，而且把所需的参数提供给了被调用的方法。我们所要做的就是提供恰当的 self 参数即可。示例如下：

```txt
>>> p = Point(3, 4)  
>>> d = operator.method caller('distance', 0, 0)  
>>> d(p)  
5.0  
>>> 
```

通过包含在字符串中的名称来调用方法，这种方式时常出现在需要模拟 case 语句或者访问者模式的变体中。下一节中将有更加高级的示例。

# 8.21 实现访问者模式

# 8.21.1 问题

我们需要编写代码来处理或遍历一个由许多不同类型的对象组成的复杂数据结构，每种类型的对象处理的方式都不相同。例如遍历一个树结构，根据遇到的树节点的类型来执行不同的操作。

# 8.21.2 解决方案

本节提到的这个问题常常出现在由大量不同类型的对象组成的数据结构的程序中。为了说明，假设我们正在编写一个表示数学运算的程序。要实现这个功能，程序中会用到一些类，示例如下：

class Node: pass   
classUnaryOperator(Node): def__init__(self, operand): self.operand $=$ operand   
class BinaryOperator(Node): def__init__(self,left,right): self.left $=$ left self.right $=$ right   
class Add(BinaryOperator): pass   
class Sub(BinaryOperator): pass   
class Mul(BinaryOperator): pass   
class Div(BinaryOperator): pass   
class Negate(UnaryOperator): pass   
class Number(Node): def__init__(self, value): self.value $=$ value

之后，我们可以用这些类来构建嵌套式的数据结构，就像这样：

Representation of $1 + 2 * (3 - 4) / 5$ t1 = Sub(Number(3), Number(4))  
t2 = Mul(Number(2), t1)  
t3 = Div(t2, Number(5))  
t4 = Add(Number(1), t3)

问题不在创建这些数据结构上，而是在稍后编写处理它们的代码时。例如，给定一个表达式，程序可能要做很多事情，比如产生输出、生成指令、执行字节码到机器码的翻译等。

为了能让处理过程变得通用，一种常见的解决方案就是实现所谓的“访问者模式”。我们需要使用类似下面这样的类：

class NodeVisitor: def visit(self, node): methname $=$ 'visit_' + type(node).__name_ meth $=$ getattr(self, methname, None) ifmethisNone: meth $=$ self.Generic.visit returnmeth(node)   
def generic.visit(self,node): raiseRuntimeError('No{}method'.format('visit_' + type(node).__name_))

要使用这个类，程序员从该类中继承并实现各种visit_Name()方法，这里的Name 应该由节点的类型来替换。例如，如果想对表达式求值，那么可以编写这样的代码：

classEvaluator(NodeVisitor): def visit_Number(self,node): return node.value def visit_Add(self,node): return self.visit(node.left) $^+$ self.visit(node.right) def visit_Sub(self,node): return self.visit(node.left)-self.visit(node.right) def visit_Mul(self,node): return self.visit(node.left)\*self.visit(node.right) def visit_Div(self,node): return self.visit(node.left)/self.visit(node.right) def visit_Negate(self,node): return -node.operand

下面这个例子展示如何使用这个类来计算前面生成的表达式：

```txt
>>> e =Evaluator()  
>>> e.visit(t4)  
0.6  
>>> 
```

作为另一个完全不同的例子，下面这个类可以将表达式翻译为堆栈机（stack machine）上的指令序列：

class StackCode(NodeVisitor): def generate_code(self, node): selfinstructions $=$ [] self.visit(node) return self.inSTRUCTIONS def visit_Number(self, node): self.instructions.append('PUSH', node.value)) def binop(self, node, instruction): self.visit(node.left) self.visit(node.right) self.instructions.append((instruction,) def visit_Add(self, node): self.binop(node,'ADD') def visit_Sub(self, node): self.binop(node,'SUB') def visit_Mul(self, node): self.binop(node,'MUL') def visit_Div(self, node): self.binop(node,'DIV') def unaryop(self, node, instruction): self.visit(node.operand) self.instructions.append((instruction,) def visit_Negate(self, node): self.unaryop(node,'NEG')

如何使用这个类呢？示例如下：

```txt
>>>s = StackCode()   
>>>s.generator_code(t4)   
[('PUSH'，1)，('PUSH'，2)，('PUSH'，3)，('PUSH'，4)，('SUB'), (MUL)，('PUSH'，5)，('DIV'，)，('ADD')，)]   
>>>
```

# 8.21.3 讨论

本节涵盖了两个核心思想。首先是设计策略，即把操作复杂数据结构的代码和数据结

构本身进行解耦。也就是说，本节中没有任何一个 Node 类的实现有对数据进行操作。相反，所有对数据的处理都放在特定的 NodeVisitor 类中实现。这种隔离使得代码变得非常通用。

本节的第二个核心思想在于对访问者类本身的实现。在访问者中，你想根据某些值比如节点类型来调度不同的处理方法。一种幼稚的做法是会编写大量的 if 语句，就像下面这样：

class NodeVisitor: def visit(self, node): nodetype $=$ type(node).__name__ ifnodetype $\equiv$ 'Number': return self.visit_Number(node) elif nodetype $\equiv$ 'Add': return self.visit_Add(node) elif nodetype $\equiv$ 'Sub': return self.visit_Sub(node)

但是，很快就会发现这种做法明显行不通。除了非常繁琐之外，运行速度也很慢。如果想添加或修改要处理的节点类型则会难以维护。相反，如果通过一些小技巧将方法名构建出来，再利用 getattr()函数来获取方法则会好得多。解决方案中的 generic_visit()不应该匹配到任何处理方法，它是一种异常回退机制。在本节中，generic_visit()会抛出一个异常来警告程序员遇到了一个未知的节点类型。

在每个访问者类中，常常会通过对 visit()方法进行递归调用来完成计算。示例如下：

```python
classEvaluator(NodeVisitor): def visit_Add(self, node): return self.visit(node.left) + self.visit(node.right) 
```

正是由于递归才使得访问者类可以遍历整个数据结构。本质上说就是不断调用 visit()直到到达某个终止节点，比如示例中的Number。递归和其他操作的确切顺序完全取决于应用程序。

应该提到的是，这种调度方法的技术在其他语言中也常用来模拟开关行为或者条件语句。例如，如果我们正在编写一个 HTTP 框架，我们在类中也会实现类似的方法调度：

class HTTPHandler: def handle(self, request): methname $=$ 'do_' + request.request_method getattr(self, methname)(request) def do_GET(self, request):

```python
def do_POST(self, request):
    ...
def do_HEAD(self, request):
    ... 
```

访问者模式的一个缺点就是需要重度依赖于递归。如果要处理一个深度嵌套的数据结构，那么有可能会达到 Python 的递归深度限制（查看 sys.getrecursionlimit()的结果）。要避免这个问题，可以在构建数据结构时做一些特定的选择。例如，可以使用普通的Python 列表来替代链表，或者在每个节点中聚合更多数据，使得数据变得扁平化而不是深度嵌套。

也可以尝试利用生成器和迭代器实现非递归式的遍历算法，具体内容可参见 8.22 节。

在有关解析和编译的程序中使用访问者模式是非常常见的。在 Python 自带的 ast 模块中可以找到一个实现。除了可以遍历树结构之外，在遍历的同时还允许对数据结构进行改写或转换（例如添加节点或移除节点）。具体细节可查看 ast 模块的源码。9.24 节中展示了一个利用 ast模块来处理 Python 源代码的例子。

# 8.22 实现非递归的访问者模式

# 8.22.1 问题

我们使用访问者模式来遍历一个深度嵌套的树结构，但由于超出了 Python 的递归限制而崩溃。我们想要去掉递归，但依旧保持访问者模式的编程风格。

# 8.22.2 解决方案

巧妙利用生成器有时候可用来消除树的遍历或查找算法中的递归。在 8.21 节中，我们已经给出了一个访问者类。下面是这个类的另一种实现方式，通过堆栈和生成器来驱动计算，完全不使用递归。

import types   
class Node: pass   
import types   
class NodeVisitor: def visit(self, node): stack $=$ [node] last_result $\equiv$ None while stack:

try: last $=$ stack[-1] ifisinstance(last,types.GeneratorType): stack.append(last.send(last_result)) last_result $\equiv$ None elif isinstance(last, Node): stack.append(self._visit(layer.pop())) else: last_result $\equiv$ stack.pop() except StopIteration: stack.pop() return last_result   
def _visit(self,node): methname $=$ 'visit_' + type(node)._.name_ meth $=$ getattr(self,methname,None) ifmethis None: meth $=$ self.Generic.visit return meth(node)   
def generic.visit(self,node): raiseRuntimeError('No{}method'.format('visit_' + type(node)._.name_))

如果使用这个类，就会发现配合之前已有的代码（可能使用了递归），程序仍然可以正常工作。实际上，我们可以用其替换上一节中的访问者类实现。例如，考虑下面的代码，其中涉及表达式树：

```python
classUnaryOperator(Node): def __init__(self, operand): self.operand = operand   
class BinaryOperator(Node): def __init__(self, left, right): self.left = left self.right = right   
class Add(BinaryOperator): pass   
class Sub(BinaryOperator): pass   
class Mul(BinaryOperator): pass 
```

```python
class Div(BinaryOperator):
    pass
class Negate(UnaryOperator):
    pass
class Number(Node):
    def __init__(self, value):
        self.value = value
# A sample visitor class that evaluates expressions
classEvaluator(NodeVisitor):
    def visit_Number(self, node):
        return node.value
    def visit_Add(self, node):
        return self.visit(node.left) + self.visit(node.right)
    def visit_Sub(self, node):
        return self.visit(node.left) - self.visit(node.right)
    def visit_Mul(self, node):
        return self.visit(node.left) * self.visit(node.right)
    def visit_Div(self, node):
        return self.visit(node.left) / self.visit(node.right)
    def visit_Negate(self, node):
        return -self.visit(node.opand)
if __name__ == '__main__':
    # 1 + 2*(3-4) / 5
    t1 = Sub(Number(3), Number(4))
    t2 = Mul(Number(2), t1)
    t3 = Div(t2, Number(5))
    t4 = Add(Number(1), t3)
    # Evaluate it
    e =Evaluator()
print(e.visit(t4)) # Outputs 0.6 
```

上述代码在处理简单的表达式时是没有问题的。但是，Evaluator 的实现中使用了递归，如果嵌套层次太深的话程序就会崩溃。示例如下：

```txt
>>> a = Number(0)  
>>> for n in range(1, 100000): 
```

```txt
a = Add(a, Number(n))  
...  
>>> e =Evaluator()  
>>> e.visit(a)  
Traceback (most recent call last):  
...  
File "visitor.py", line 29, in _visit  
return meth(node)  
File "visitor.py", line 67, in visit_Add  
return self.visit(node.left) + self.visit(node.right)  
RuntimeError: maximum recursion depth exceeded  
>>> 
```

现在，我们把 Evaluator 类稍微修改一下：

classEvaluator(NodeVisitor): def visit_Number(self, node): return node.value def visit_Add(self, node): yield (yield node.left) $^+$ (yield node.right) def visit_Sub(self, node): yield (yield node.left) - (yield node.right) def visit_Mul(self, node): yield (yield node.left) \* (yield node.right) def visit_Div(self, node): yield (yield node.left) / (yield node.right) def visit_Negate(self, node): yield-(yield node.operand)

如果再次尝试同样的试验，会发现程序突然就可以正常工作了，真是神奇！

```txt
>>> a = Number(0)  
>>> for n in range(1, 100000):  
... a = Add(a, Number(n))  
...  
>>> e = Evaluator()  
>>> e.visit(a)  
4999950000  
>>> 
```

如果想在任意一个方法中添加自定义的处理，程序依然可以正常工作。示例如下：

classEvaluator(NodeVisitor): def visit_Add(self,node): print('Add:,'node) lhs $=$ yield node.left print('left $\coloneqq$ '，lhs) rhs $=$ yield node.right print('right $\coloneqq$ '，rhs) yield lhs $^+$ rhs

下面是示例输出：

```txt
>>> e =Evaluator()
>>> e.visit(t4)
Add: __main__.Add object at 0x1006a8d90>
left= 1
right= -0.4
0.6
>>> 
```

# 8.22.3 讨论

本节很好地展示了如何利用生成器和协程来控制程序的执行流。这种令人费解的技巧常常能带来很大的优势。要理解本节的内容，需要深入了解几个要点。

首先，在有关遍历树结构的问题中，为了避免使用递归，常见的策略就是利用栈或者队列来实现算法。例如，深度优先遍历完全可以实现为将第一个遇到的节点压入栈中，一旦处理结束再将其弹出。解决方案中给出的visit()方法的核心就是按照这个思路实现的。算法一开始会将初始节点压入 stack 列表中（这里的栈以 Python 列表的形式来实现），然后继续运行直到栈为空为止。在执行算法的时候，栈会根据树结构的深度进行增长。

第二个要点在于生成器中 yield 语句的行为。当遇到 yield 语句时，生成器会产生出一个值然后暂停执行。本节正是利用这个特性来取代递归。例如，现在我们不用像这样编写递归式的表达式了：

value $=$ self.visit(node.left)

我们用下面这条语句来替代：

```txt
value = yield node.left 
```

在幕后，这条语句会将 node.left 节点发送回给 visit()方法。之后，visit()就可以为该节点调用合适的 visit_Name()方法了。从某种意义上说，这几乎和递归算法恰好相反。也

就是说，现在不是通过递归调用 visit()来遍历树节点了，而是在处理的过程中用 yield语句来暂停计算。因此，yield 本质上可当做一种信号来告诉算法当前处在 yield 状态的节点需要先被处理，之后剩下的处理才可以继续进行。

本节中最后一个需要考虑的问题是如何传递结果。当我们使用生成器函数时，我们不能再使用 return 语句来发送结果了（这么做会产生 SyntaxError 异常）。因此，yield 语句必须来承担这个责任。在本节中，如果由yield语句产生出的值是非节点类型（non-Nodetype）的，则认为该值是要发送给计算过程中的下一个步骤的。这正是在代码中使用变量 last_return 的目的所在。一般来说，last_return 将保存某个访问方法上一次产生出的值。这个值会作为 yield 语句的返回值发送到上一个执行的方法中。例如，在下面的代码中：

```txt
value = yield node.left 
```

变量value将获得 last_return 的值，而这个值正是在为节点node.left调用访问方法时返回的结果。

以上所有要点都可以在下面的代码片段中找到：

try:   
last $=$ stack[-1]   
if isinstance(last, types.generatorType): stack.append(last.send(last_result)) last_result $\equiv$ None   
elif isinstance(last, Node): stack.append(self._visit(layer.pop()))   
else: last_result $\equiv$ stack.pop()   
except StopIteration: stack.pop()

这段代码简单地查看栈顶并决定下一步该做什么。如果是生成器，那么就调用它的 send()方法将上次得到的结果（如果有结果的话）添加到栈中以待后续处理。send()返回的值和传给 yield 语句的值是相同的。因此，在 yield node.left 这样的语句中，send()返回的就是 Node 的实例 node.left，并会将其放置在栈的顶部。

如果栈顶是一个 Node 实例，那么该实例会被替换为在该节点上调用合适的访问方法所得到的结果。正是因为这样，我们完全避免了对递归的使用。之前我们是在各个访问方法中以递归的方式直接调用 visit()（参见上一节解决方案中的实现），现在不必这么做了。只要在各个访问方法中使用 yield，那么程序就能正常工作。

最后，如果栈顶元素为其他值，则可认为这是某种类型的返回值。我们将其从栈中弹出然后保存到 last_result 中。如果栈中的下一个元素是生成器，那么就将它作为 yield

语句的返回值发送出去。应该提到的是，visit()的最后一个返回值也会赋给 last_result。这样就使得本节中的代码也能适用于传统的递归实现。如果没有用到生成器，last_result就保存着代码中 return 语句的返回值。

本节中一个潜在的危险在于产生 Node 和非 Node 值之间的区别。在我们的实现中会自动遍历所有的 Node 实例。这意味着我们不能把 Node 当做返回值来进行传递。在实践中，这也许无关紧要。但是如果确实有这个需求，就需要对算法做轻微的调整。例如，可以通过引入另一个类来解决：

class Visit: def __init__(self, node): self.node = node   
class NodeVisitor: def visit(self, node): stack $=$ [Visit(node)] last_result $\equiv$ None while stack: try: last $=$ stack[-1] if isinstance(last, types.generatorType): stack.append(last.send(last_result)) last_result $\equiv$ None elif isinstance(last, Visit): stack.append(self._visit.stack.pop().node)) else: last_result $\equiv$ stack.pop() except StopIteration: stack.pop() return last_result   
def _visit(self, node):methname $=$ 'visit_' + type(node).__name_ meth $=$ getattr(self, methname, None) if meth is None: meth $=$ self.Generic.visit return meth(node)   
def generic.visit(self, node): raise ValueError('No { method'.format('visit_' + type(node).__name_))

根据上面的实现，现在访问方法看起来就是这样的了：

```txt
classEvaluator(NodeVisitor): 
```

```python
def visit_Add(self, node):
    yield (yield Visit(node.left)) + (yield Visit(node.right))
def visit_Sub(self, node):
    yield (yield Visit(node.left)) - (yield Visit(node.right))
... 
```

看过本节之后，你可能会倾向于去实现一种不涉及 yield 的解决方案。但是，这么做会使得我们必须在代码中处理本节中已经提到过的诸多问题。例如，要消除对递归的使用，需要维护一个栈。也需要有某种方法来管理对树结构的遍历以及调用各种访问者方法的逻辑。没有生成器的帮助，这种代码将演变成一锅大杂烩，其中混杂着对栈的操作、回调函数以及其他的组件。坦白说，使用 yield的主要优势在于我们能够以优雅的风格编写出非递归式的代码，而且看起来和递归式的实现几乎一样。

# 8.23 在环状数据结构中管理内存

# 8.23.1 问题

我们的程序中创建了环状的数据结构（例如树、图、观察者模式等），但是在内存管理上却遇到了麻烦。

# 8.23.2 解决方案

环状数据结构的一个简单例子就是树了，这里父节点指向它的孩子，而孩子节点又会指回它们的父节点。对于像这样的代码，我们应该考虑让其中一条连接使用 weakref库中提供的弱引用机制。示例如下：

import weakref  
```python
class Node: def __init__(self, value): self.value = value self._parent = None self.children = [] def __repr__(self): return 'Node(!r:)''.format(self.value) # property that manages the parent as a weak-reference @property def parent(self): return self._parent if self._parent is None else self._parent() 
```

@parent setter   
def parent(self, node): self._parent $=$ weakref.ref(node)   
def add_child(self, child): self.children.append(child) child.parent $=$ self

这种实现可以让父节点安静地被回收。示例如下：

```txt
>>> root = Node('parent')  
>>> c1 = Node('child')  
>>> root.add_child(c1)  
>>> print(c1.parent)  
Node('parent')  
>>> del_root  
>>> print(c1.parent)  
None  
>>> 
```

# 8.23.3 讨论

环状数据结构是 Python 中一个多少需要一些技巧才能处理好的方面，需要仔细学习。因为普通的垃圾收集规则并不适用于环状数据结构。例如，考虑下面的代码：

Class just to illustrate when deletion occurs   
class Data: def_del_(self): print('Data._del_') # Node class involving a cycle   
class Node: def_init_(self): self.data $\equiv$ Data() self.parent $\equiv$ None self.children $=$ [] def add_child(self, child): self.children.append(child) child.parent $=$ self

现在，试用上面的代码，做些试验来看看有关垃圾收集中的一些微妙问题：

```txt
>>> a = Data()
>>> del a
# Immediately deleted
Data._del_ 
```

```txt
>>> a = Node()
>>> del a # Immediately deleted
Data._del_
>>> a = Node()
>>> a.add_child(Node())
>>> del a # Not deleted (no message)
>>> 
```

可以看到，除了最后那种涉及成环的情况，其他的对象都可以立刻得到删除。原因在于 Python 的垃圾收集器是基于简单的引用计数规则来实现的。当对象的引用计数为 0时就会被立刻删除掉。而对于环状数据结构来说这绝不可能发生。因为在最后那种情况中，由于父节点和子节点互相引用对方，引用计数不会为 0。

要处理环状数据结构，还有一个单独的垃圾收集器会定期运行。但是，一般来说我们不知道它会在何时运行。因此，没法知道环状数据结构具体会在何时被回收。如果有必要的话，可以强制运行垃圾收集器，但这么做相比于全自动的垃圾收集会有一些笨拙。

```txt
>>> import gc
>>> gc.collection()
# Force collection
Data._del_
Data._del_
>>> 
```

如果环中的对象实现了自己的__del__方法的话，则情况会更糟。例如，假设有下面这样的代码：

Class just to illustrate when deletion occurs   
class Data: def_del_(self): print('Data._del_') # Node class involving a cycle   
class Node: def_init_(self): self.data $\equiv$ Data() self.parent $\equiv$ None self.children $= []$ #NEVERDEFINELIKETHIS. #Only here to illustrate pathological behavior def_del_(self): del_self.data del.parent del_childne def add_child(self,child):

```txt
self.children.append(child)  
child.parent = self 
```

在这种情况下，数据结构对象永远不会被垃圾收集，我们的程序会因此而出现内存泄露！如果动手尝试一下，会发现Data.__del__消息完全没有被打印出来——即使是强制执行垃圾收集也不会：

```txt
>>> a = Node()
>>> a.add_child(Node())
>>> del a # No message (not collected)
>>> import gc
>>> gc.collection() # No message (not collected)
>>> 
```

弱引用通过消除循环引用来解决这个问题。本质上说，弱引用就是一个指向对象的指针，但不会增加对象本身的引用计数。可以通过weakref库来创建弱引用。示例如下：

```txt
>>> import weakref
>>> a = Node()
>>> a_ref = weakref.ref(a)
>>> a_ref
<weakref at 0x100581f70; to 'Node' at 0x1005c5410>
>>> 
```

要提领（dereference）一个弱引用，可以像函数一样来调用它。如果提领后得到的对象还依然存在，那么就返回对象，否则就返回 None。由于原始对象的引用计数并没有增加，因此可以按照普通的方式来删除它。示例如下：

```txt
>>> print(a_ref())
<__main__.Node object at 0x1005c5410>
>>> del a
Data.__del__
>>> print(a_ref())
None
>>> 
```

通过使用弱引用，就会发现因为循环引用而出现的问题都不存在了。一旦某个对象不再被使用了，会立刻执行垃圾收集处理。请参阅 8.25 节中另一个有关弱引用的示例。

# 8.24 让类支持比较操作

# 8.24.1 问题

我们想使用标准的比较操作符（如 $> =$ 、!=、 $< =$ 等）在类实例之间进行比较，但是又不

想编写大量的特殊方法。

# 8.24.2 解决方案

通过为每种比较操作符实现一个特殊方法，Python 中的类可以支持比较操作。例如，要支持 $> =$ 操作符，可以在类中定义一个__ge__()方法。虽然只定义一个方法不算什么，但如果要实现每种可能的比较操作，那么实现这么多特殊方法则很快会变得繁琐。

functools.total_ordering装饰器可用来简化这个过程。要使用它，可以用它来装饰一个类，然后定义__eq__()以及另一个比较方法（__lt__、__le__、__gt__或者__ge__）。那么装饰器就会自动为我们实现其他的比较方法。

作为示例，让我们来构建一些房子并为其添加一些房间吧，然后根据房子的大小来进行比较：

from functools import total_ordering   
class Room: def init_self, name, length, width): self.name $=$ name self.length $=$ length self.width $=$ width self(square_feet $=$ self.length \* self.width   
@total_ordering   
class House: def init_self, name, style): self.name $=$ name self.style $=$ style selfRooms $=$ list() @property def living_space_footage(self): return sum(r(square_feet for r in selfRooms) def add_room(self, room): selfRooms.append(room) def_str_self): return '\{\}':\{\} square foot \{\}''.format(self.name, self.living_space_footage, self.style) def_eq_self, other): return self.living_space_footage $= =$ other.living_space_footage

```python
def _lt__(self, other):
    return self.living_space_footage < other.living_space_footage 
```

这里，House 类已经用 $@$ total_ordering 来进行装饰了。我们定义了__eq__()和__lt__()来根据房间的总面积对房子进行比较。只需要定义这两个特殊方法就能让其他所有的比较操作正常工作。示例如下：

```txt
Build a few houses, and add rooms to them  
h1 = House('h1', 'Cape')  
h1.add_room(Room('Master Bedroom', 14, 21))  
h1.add_room(Room('Living Room', 18, 20))  
h1.add_room(Room('Kitchen', 12, 16))  
h1.add_room(Room('Office', 12, 12))  
h2 = House('h2', 'Ranch')  
h2.add_room(Room('Master Bedroom', 14, 21))  
h2.add_room(Room('Living Room', 18, 20))  
h2.add_room(Room('Kitchen', 12, 16))  
h3 = House('h3', 'Split')  
h3.add_room(Room('Master Bedroom', 14, 21))  
h3.add_room(Room('Living Room', 18, 20))  
h3.add_room(Room('Office', 12, 16))  
h3.add_room(Room('Kitchen', 15, 17))  
houses = [h1, h2, h3]  
print('Is h1 bigger than h2?', h1 > h2) # prints True  
print('Is h2 smaller than h3?', h2 < h3) # prints True  
print('Is h2 greater than or equal to h1?', h2 >= h1) # Prints False  
print('Which one is biggest?', max(houses)) # Prints 'h3: 1101-square-foot Split'  
print('Which is smallest?', min(houses)) # Prints 'h2: 846-square-foot Ranch' 
```

# 8.24.3 讨论

如果我们曾经编写过代码让类支持所有的基本比较操作符，那么装饰器 total_ordering对我们而言就并非那么神奇：它从字面上定义了从每个比较方法到其他所有需要该方法的映射关系。因此，如果在类中定义了__lt__()，那么就会利用它来构建其他所有的比较操作符。实际上就是在类中填充以下方法：

```python
class House: def__eq_self,other): deflt_self,other) #Methods created by @total_ordering 
```

```python
__le__ = lambda self, other: self < other or self == other
__gt__ = lambda self, other: not (self < other or self == other)
__ge__ = lambda self, other: not (self < other)
__ne__ = lambda self, other: not self == other 
```

的确，自行编写这些方法并不难，但 $@$ total_ordering 让这一过程变得更加简单了。

# 8.25 创建缓存实例

# 8.25.1 问题

当创建类实例时我们想返回一个缓存引用，让其指向上一个用同样参数（如果有的话）创建出的类实例。

# 8.25.2 解决方案

本节提到的这个问题常常出现在当我们想确保针对某一组输入参数只会有一个类实例存在时。现实中的例子包括一些库的行为，比如在 logging模块中，给定的一个名称只会关联到一个单独的 logger 实例。示例如下：

```txt
>>> import logging
>>> a = logging.getLogger('foo')
>>> b = logging.getLogger('bar')
>>> a is b
False
>>> c = logging.getLogger('foo')
>>> a is c
True
>>> 
```

要实现这一行为，应该使用一个与类本身相分离的工厂函数。示例如下：

The class in question   
class Spam: def__init__(self,name): self.name $=$ name   
#Caching support   
importweakref _spam_cache $=$ weakref.WeakValueDictionary()   
def get_spam(name): ifname not in_spam_cache: s $=$ Spam(name)

```python
_spam_cache[name] = s  
else:  
    s = _spam_cache[name]  
return s 
```

如果你用上述实现，会发现 Spam 类的行为和之前展示的效果一样：

```txt
>>> a = get_spam('foo')  
>>> b = get_spam('bar')  
>>> a is b  
False  
>>> c = get_spam('foo')  
>>> a is c  
True  
>>> 
```

# 8.25.3 讨论

要想修改实例创建的规则，编写一个特殊的工厂函数常常是一种简单的方法。此时，一个常被提到的问题就是是否可以用更加优雅的方式来完成呢？

例如，我们可能会考虑重新定义类的__new__()方法：

```python
Note: This code doesn't quite work  
import weakref  
class Spam:  
    _spam_cache =weakref.WeakValueDictionary()  
    def __new__(cls, name):  
        if name in cls._spam_cache:  
            return cls._spam_cache[name]  
        else:  
            self = super().__new__(cls)  
            cls._spam_cache[name] = self  
            return self  
    def __init__(self, name):  
        print('Initializing Spam')  
        self.name = name 
```

初看上去，上面的代码似乎可以完成任务。但是，主要的问题在于__init__()方法总是会得到调用，无论对象实例有无得到缓存都是如此。示例如下：

>>>s $=$ Spam('Dave') Initializing Spam $\gg t =$ Spam('Dave')

```txt
Initializing Spam  
>>>s is t  
True  
>>> 
```

这种行为很可能不是我们想要的。因此，要解决实例缓存后会重复初始化的问题，需要采用一个稍有些不同的方法。

本节中对弱引用的运用与垃圾收集有着极为重要的关系。当维护实例缓存时，只要在程序中实际用到了它们，那么通常希望将对象保存在缓存中。WeakValueDictionary 会保存着那些被引用的对象，只要它们存在于程序中的某处即可。否则，当实例不再被使用时，字典的键就会消失。示例如下：

```txt
>>> a = get_spam('foo')  
>>> b = get_spam('bar')  
>>> c = get_spam('foo')  
>>> list(_spam_cache)  
['foo', 'bar']  
>>> del a  
>>> del c  
>>> list(_spam_cache)  
['bar']  
>>> del b  
>>> list(_spam_cache)  
[] 
```

对于许多程序而言，使用本节中给出的框架代码通常就足够了。但是，还可以考虑一些更加高级的实现技术。

我们立刻能想到的是，本节中的解决方案需要依赖全局变量以及一个与原始的类定义相分离的工厂函数。一种改进方式是将缓存代码放到另一个单独的管理类中，然后将这些组件粘合在一起：

importweakref   
classCachedSpamManager: def__init__(self): self._cache $=$ weakest.WeakValueDictionary() defget_spam(self，name): ifname not in self._cache:s $=$ Spam(name) self._cache[name] $=$ s else:s $=$ self._cache[name] returns

```python
def clear(self):
    self._cache.clear()
class Spam:
    manager = CachedSpamManager()
    def __init__(self, name):
        self.name = name
    def get_spam(name):
        return Spammanager.get_spam(name) 
```

这种方法的特点就是为潜在的灵活性提供了更多支持。例如，我们可以实现不同类型的缓存管理机制（以单独的类来实现），然后附加到Spam类中替换掉默认的缓存实现。其他的代码（比如 get_spam）不需要修改就能正常工作。

另一种设计上的考虑是到底要不要将类的定义暴露给用户。如果什么都不做的话，用户可以很容易创建出实例，从而绕过缓存机制：

```txt
>> a = Spam('foo')  
>> b = Spam('foo')  
>> a is b  
False  
>> 
```

如果预防出现这种行为对程序而言很重要，我们可以采取特定的步骤来避免。例如，可以在类名前加一个下划线，例如_Spam，这样至少可以提醒用户不应该直接去访问它。

或者，如果想为用户提供更强的提示，暗示他们不应该直接实例化 Spam对象，可以让__init__()方法抛出一个异常，然后用一个类方法来实现构造函数的功能，就像下面这样：

```python
class Spam: def __init__(self, *args, **kwargs): raise ValueError("Can't instantiate directly") # Alternate constructor @klassmethod def __new__(cls, name): self = cls.__new__(cls) self.name = name 
```

要使用上述代码，可以将实现缓存机制的代码修改为使用 Spam._new()来创建实例，而不是使用通常所见的 Spam()。示例如下：

import weakref  
class CachedSpamManager: def __init__(self): self._cache =weakref.WeakValueDictionary() def get_spam(self，name): if name not in self._cache: s $=$ Spam._new(name） #Modified creation self._cache[name] $\equiv$ s else:s $=$ self._cache[name] return s

尽管还有更加极端的方法来隐藏 Spam 类的可见性，但也许最好不要把问题想的过于复杂。在类名前添加下划线或者用类方法作为构造函数通常就足以给程序员带来提示了。

通过使用元类，缓存机制以及其他的创建模式（creational pattern）通常能够以更加优雅的方式得以解决。关于元类，请参阅 9.13 节。

软件开发中最重要的一条真理就是“不要重复自己的工作（Don’t repeat yourself）”。也就是说，任何时候当需要创建高度重复的代码（或者需要复制粘贴源代码）时，通常都需要寻找一个更加优雅的解决方案。在Python 中，这类问题常常会归类为“元编程”。简而言之，元编程的主要目标是创建函数和类，并用它们来操纵代码（比如说修改、生成或者包装已有的代码）。Python 中基于这个目的的主要特性包括装饰器、类装饰器以及元类。但是，还有许多其他有用的主题——包括对象签名、用 exec()来执行代码以及检查函数和类的内部结构——也进入了我们的视野。本章的主要目的是探讨各种元编程技术，通过示例来讲解如何利用这些技术来自定义 Python 的行为，使其能满足我们不同寻常的需求。

# 9.1 给函数添加一个包装

# 9.1.1 问题

我们想给函数加上一个包装层（wrapper layer）以添加额外的处理（例如，记录日志、计时统计）。

# 9.1.2 解决方案

如果需要用额外的代码对函数做包装，可以定义一个装饰器函数。示例如下：

```python
import time   
from functools import wraps   
def timethis(func): 
```

```python
Decorator that reports the execution time.
...
@wraps(func)
def wrapper(*args, **kwargs):
    start = time.time()
    result = func(*args, **kwargs)
    end = time.time()
    print(func.__name__, end-start)
    return result
return wrapper 
```

下面是使用这个装饰器的示例：

```txt
>>> @timethis  
... def countdown(n):  
...     '    Counts down  
...     '    while n > 0:  
...         n -= 1  
...  
>>> countdown(100000)  
countdown 0.008917808532714844  
>>> countdown(1000000)  
countdown 0.87188299392912  
>>> 
```

# 9.1.3 讨论

装饰器就是一个函数，它可以接受一个函数作为输入并返回一个新的函数作为输出。当像这样编写代码时：

```txt
@timethis def countdown(n): 
```

和单独执行下列步骤的效果是一样的：

```python
def countdown(n):
    ...
    countdown = timethis(countdown) 
```

顺便插一句，内建的装饰器比如 $@$ staticmethod、@classmethod 以及 $@$ property 的工作方式也是一样的。比如说，下面这两个代码片段的效果是相同的：

```txt
class A: @classmethod 
```

```python
def method(cls):
    pass
class B:
    # Equivalent definition of a class method
    def method(cls):
        pass
method = classmethod(method) 
```

装饰器内部的代码一般会涉及创建一个新的函数，利用*args 和**kwargs 来接受任意的参数。本节示例中的 wrapper()函数正是这么做的。在这个函数内部，我们需要调用原来的输入函数（即被包装的那个函数，它是装饰器的输入参数）并返回它的结果。但是，也可以添加任何想要添加的额外代码（例如计时处理）。这个新创建的 wrapper 函数会作为装饰器的结果返回，取代了原来的函数。

需要重点强调的是，装饰器一般来说不会修改调用签名，也不会修改被包装函数返回的结果。这里对*args 和**kwargs的使用是为了确保可以接受任何形式的输入参数。装饰器的返回值几乎总是同调用 func(*args, **kwargs)的结果一致，这里的 func 就是那个未被包装过的原始函数。

当初次学习装饰器时，通过一些简单的例子来入门是很容易的，就像本节给出的那个计时的例子一样。但是，如果打算在生产环境中编写装饰器，那么这里还有一些细节需要考虑。比方说，我们的解决方案中对装饰器 $@$ wraps(func)的使用就是一个容易忘记但是却很重要的技术，它可以用来保存函数的元数据。这方面的内容将在下一节中描述。如果我们要编写自己的装饰器函数，那么接下来的几节将会补充一些很重要的细节。

# 9.2 编写装饰器时如何保存函数的元数据

# 9.2.1 问题

我们已经编写好了一个装饰器，但是当将它用在一个函数上时，一些重要的元数据比如函数名、文档字符串、函数注解以及调用签名都丢失了。

# 9.2.2 解决方案

每当定义一个装饰器时，应该总是记得为底层的包装函数添加 functools 库中的 $@$ wraps装饰器。示例如下：

```python
import time  
from functools import wraps 
```

```python
def timethis(func):
    ...
   Decorator that reports the execution time.
    ...
    @wraps(func)
def wrapper(*args, **kwargs):
    start = time.time()
    result = func(*args, **kwargs)
    end = time.time()
print(func.__name__, end-start)
return result
return wrapper 
```

下面是使用这个装饰器的示例，并且展示了如何检视结果函数的元数据：

```txt
>>> @timethis
... def countdown(n:int):
...
...
...
...
...
...
while n > 0:
...
    n -= 1
...
>>> countdown(100000)
countdown 0.008917808532714844
>>> countdown.__name__
'countdown'
>>> countdown.__doc__
'\n\tCounts down\n\t'
>>> countdown.__annotations__
{'n': <class 'int>} 
```

# 9.2.3 讨论

编写装饰器的一个重要部分就是拷贝装饰器的元数据。如果忘记使用 $@$ wraps，就会发现被包装的函数丢失了所有有用的信息。例如，如果忽略 $@$ wraps，上面这个例子中的元数据看起来就是这样的：

```txt
>>> countdown.__name__
'wrapper'
>>>countdown.__doc__
>>>countdown.__annotations__
{} 
```

$@$ wraps 装饰器的一个重要特性就是它可以通过__wrapped__属性来访问被包装的那个函数。例如，如果希望直接访问被包装的函数，则可以这样做：

```txt
>>>countdown._wrapped_(100000)   
>>> 
```

_wrapped__属性的存在同样使得装饰器函数可以合适地将底层被包装函数的签名暴露出来。例如：

```txt
>>> from inspect import signature   
>>> print(sigature(countdown))   
(n:int)   
>>> 
```

常会提到的一个问题是如何让装饰器直接拷贝被包装的原始函数的调用签名（即，不使用*args 和**kwargs）。一般来说，如果不采用涉及生成代码字符串和 exec()的技巧，那么这很难实现。坦白地说，通常我们最好还是使用 $@$ wraps。这样可以依赖于一个事实，即，底层的函数签名可以通过访问__wrapped__属性来传递。有关函数签名的更多信息可以参阅 9.16 节。

# 9.3 对装饰器进行解包装

# 9.3.1 问题

我们已经把装饰器添加到一个函数上了，但是想“撤销”它，访问未经过包装的那个原始函数。

# 9.3.2 解决方案

假设装饰器的实现中已经使用了 $@$ wraps（参见 9.2 节），一般来说我们可以通过访问_wrapped__属性来获取对原始函数的访问。示例如下：

>>> @somedecorator   
>>> def add(x, y): return $\mathrm{x + y}$ >>> orig_add $=$ add._wrapped_   
>>> orig_add(3,4)   
7   
>>>

# 9.3.3 讨论

直接访问装饰器背后的那个未包装过的函数对于调试、反射（introspection，也有译为

“自省”）以及其他一些涉及函数的操作是很有帮助的。但是，本节讨论的技术只有在实现装饰器时利用 functools 模块中的 $@$ wraps对元数据进行了适当的拷贝，或者直接设定了__wrapped__属性时才有用。

如果有多个装饰器已经作用于某个函数上了，那么访问__wrapped__属性的行为目前是未定义的，应该避免这种情况。在Python 3.3中，这么做会绕过所有的包装层。例如，假设有如下的代码：

from functools import wraps   
def decorator1(func): @wraps(func) def wrapper $\text{巧}$ args, \*\*kwarges): print('Decorator 1') return func $\text{巧}$ args, \*\*kwarges) return wrapper   
def decorator2(func): @wraps(func) def wrapper $\text{巧}$ args, \*\*kwarges): print('Decorator 2') return func $\text{巧}$ args, \*\*kwarges) return wrapper   
@decorator1   
@decorator2   
def add(x,y): return x + y

当调用装饰过的函数以及通过__wrapped__属性调用原始函数时就会出现这样的情况：

```prolog
>>>add(2,3)   
Decorator1   
Decorator2   
5   
>>>add._wrapped_(2,3)   
5   
>>> 
```

然而，这种行为已经被报告为一个 bug 了（参见 http://bugs.python.org/issue17482），可能会在今后释出的版本中修改为暴露出合适的装饰器链（decorator chain）。

最后但同样重要的是，请注意并不是所有的装饰器都使用了 $@$ wraps，因此有些装饰器的行为可能与我们期望的有所区别。特别是，由内建的装饰器 $@$ staticmethod 和@classmethod

创建的描述符（descriptor）对象并不遵循这个约定（相反，它们会把原始函数保存在__func__属性中）。所以，具体问题需要具体分析，每个人遇到的情况可能不同。

# 9.4 定义一个可接受参数的装饰器

# 9.4.1 问题

我们想编写一个可接受参数的装饰器函数。

# 9.4.2 解决方案

让我们用一个例子来说明接受参数的过程。假设我们想编写一个为函数添加日志功能的装饰器，但是又允许用户指定日志的等级以及一些其他的细节作为参数。下面是定义这个装饰器的可能做法：

from functools import wraps   
import logging   
def logged(level，name $\equiv$ None，message $\equiv$ None）： Addloggingtoafunction.levelis thelogging level, name is the logger name,and message is the log message.If name and message aren't specified, they default to the function's module and name. defdecorate func)： logname $\equiv$ name if name else func._module log $\equiv$ logging.getLogger(logname) logmsg $\equiv$ message if message else func._name_ @wraps(func) defwrapper $(^{*}\arg s$ $^ { \text{喜} }$ kwarges): log.log(level,logmsg) return func $(^{*}\arg s$ $^ { \text{喜} }$ kwarges) return wrapper return decorate   
#Example use @logged(logging DEBUG)   
defadd(x,y): return x + y   
@logged(logging.CRITICAL,'example')   
def spam():

```lua
print('Spam!')
```

初看上去这个实现显得很有技巧性，但其中的思想相对来说是很简单的。最外层的logged()函数接受所需的参数，并让它们对装饰器的内层函数可见。内层的 decorate()函数接受一个函数并给它加上一个包装层。关键部分在于这个包装层可以使用传递给logged()的参数。

# 9.4.3 讨论

编写一个可接受参数的装饰器是需要一些技巧的，因为这会涉及底层的调用顺序。具体来说，如果有这样的代码：

```txt
@decorator(x, y, z)  
def func(a, b): pass 
```

装饰的过程会按照下列方式来进行：

```python
def func(a, b):
    pass
    func = decorator(x, y, z) (func) 
```

请仔细观察，decorator(x, y, z)的结果必须是一个可调用对象，这个对象反过来接受一个函数作为输入，并对其进行包装。请参见 9.7 节中另一个关于让装饰器接受参数的例子。

# 9.5 定义一个属性可由用户修改的装饰器

# 9.5.1 问题

我们想编写一个装饰器来包装函数，但是可以让用户调整装饰器的属性，这样在运行时能够控制装饰器的行为。

# 9.5.2 解决方案

下面给出的解决方案对上一节的示例进行了扩展，引入了访问器函数（accessorfunction），通过使用nonlocal关键字声明变量来修改装饰器内部的属性。之后把访问器函数作为函数属性附加到包装函数上。

```python
from functools import wraps, partial import logging 
```

```txt
Utility decorator to attach a function as an attribute of obj 
```

```python
def attachwrapper(obj, func=None):
    if func is None:
        return partial(att Cruiser, obj)
    setattr(obj, func._name_, func)
    return func
def logged(level, name=None, message=None):
    Add logging to a function. level is the logging level, name is the logger name, and message is the log message. If name and message aren't specified, they default to the function's module and name.
    def decorate(func):
        logname = name if name else func._module_
        log = logging.getLogger(logname)
        logmsg = message if message else func._name_
        @wraps(func)
        def wrapper(*args, **kwargs):
            log.log(level, logmsg)
            return func(*args, **kwargs)
        # Attach setter functions
        @attachwrapper wraps)
        def set_level(newlevel):
            nonlocal level
            level = newlevel
        @attachwrapper好不好)
        def set_message(newmsg):
            nonlocal logmsg
            logmsg = newmsg
    return wrapper
    return decorate
# Example use
@logged(logging DEBUG)
def add(x, y):
    return x + y
@logged(logging.CRITICAL, 'example')
def spam():
    print('Spam!') 
```

下面的交互式会话展示了在完成上面的定义之后对各项属性的修改：

```diff
>>> import logging
>>> logging-basicConfig(level=logging DEBUG)
>>> add(2, 3)
DEBUG:__main__:add
5
>>> # Change the log message
>>> add.set_message('Add called')
>>> add(2, 3)
DEBUG:__main__:Add called
5
>>> # Change the log level
>>> add.set_level(logging.WARNING)
>>> add(2, 3)
WARNING:__main__:Add called
5
>>> 
```

# 9.5.3 讨论

本节示例的关键就在访问器函数（即，set_message()和 set_level()），它们以属性的形式附加到了包装函数上。每个访问器函数允许对 nonlocal变量赋值来调整内部参数。

这个示例中有一个令人惊叹的特性，那就是访问器函数可以跨越多个装饰器层进行传播（如果所有的装饰器都使用了 $@$ functools.wraps 的话）。例如，假设引入了一个额外的装饰器，比如 9.2 节中的 $@$ timethis，然后编写了如下的代码：

```txt
@timethis  
@logged(logging DEBUG)  
def countdown(n):  
    while n > 0:  
        n -= 1 
```

就会发现访问器函数依然可以正常工作：

```txt
>>> countdown(10000000)
DEBUG:__main__:countdown
countdown 0.8198461532592773
>>> countdown.set_level(logging.WARNING)
>>> countdown.set_message("Counting down to zero")
>>> countdown(10000000)
WARNING:__main__:Counting down to zero
countdown 0.8225970268249512 
```

>>>

如果把装饰器的顺序像下面这样颠倒一下，就会发现访问器函数还是能够以相同的方式工作。

@logged(logging DEBUG) @timethis def countdown(n): while $n > 0$ n $\equiv$ 1

尽管这里没有给出，我们也可以通过添加如下额外的代码来实现用访问器函数返回内部的状态值：

...   
@attachwrapper(wrapper)   
def get_level(): return level   
# Alternative   
wrapper.get_level $=$ lambda: level

本节中一个极为微妙的地方在于为什么要在一开始使用访问器函数。比方说，我们可能会考虑其他的方案，完全基于对函数属性的直接访问，示例如下：

... @wraps func) def wrapper(\*args, \*\*kwargs): wrapper.log.log(wrapper.level, wrapper.logmsg) return func(\*args, \*\* kwargs) # Attach adjustable attributes wrapper.level $=$ level wrapper.logmsg $=$ logmsg wrapper.log $=$ log

这种方法只能用在最顶层的装饰器上。如果在当前顶层的装饰器上又添加了一个装饰器（比如示例中的 $@$ timethis），这样就会隐藏下层的属性使得它们无法被修改。而使用访问器函数可以绕过这个限制。

最后但同样重要的是，本节展示的解决方案可以作为类装饰器的一种替代方式，我们在 9.9 节中会继续谈到相关的主题。

# 9.6 定义一个能接收可选参数的装饰器

# 9.6.1 问题

我们想编写一个单独的装饰器，使其既可以像@decorator 这样不带参数使用，也可以像@decorator(x, y, z)这样接收可选参数。但是，由于简单的装饰器和可接收参数的装饰器之间存在不同的调用约定（calling convention），这样看来似乎并没有直接的方法来处理。

# 9.6.2 解决方案

我们对 9.5 节中记录日志的代码做了修改，定义了一个可接受可选参数的装饰器：

from functools import wraps, partial   
import logging   
def logged func=None，\*，level=logging DEBUG，name $\equiv$ None，message $\equiv$ None）: if func is None: return partial(logged, level $\equiv$ level, name $\equiv$ name, message $\equiv$ message) logname $=$ name if name else func._module_ log $=$ logging.getLogger(logname) logmsg $=$ message if message else func._name_ @wraps func) def wrapper(\*args，\*\*kwargs): log.log(level, logmsg) return func(\*args，\*\* kwargs) return wrapper   
#Example use   
@logged   
def add(x,y): return x + y   
@logged(level=logging.CRITICAL，name $\equiv$ 'example')   
def spam(): print('Spam!')

从示例中可以看到，现在这个装饰器既能够以简单的形式（即 $@$ logged）使用，也可以提供可选的参数给它（即， $@$ logged(level=logging.CRITICAL, name='example')）。

# 9.6.3 讨论

本节提到的实际上是一种编程一致性（programming consistency）的问题。当使用装饰

器时，大部分程序员习惯于完全不使用任何参数，或者就像示例中那样使用参数。从技术上来说，如果装饰器的所有参数都是可选的，那么可以像这样来使用：

```txt
@logged() defadd(x，y）： returnx+y
```

但是这和我们常见的形式不太一样，如果程序员忘记加上那个额外的圆括号就可能会导致常见的使用错误。本节提到的技术可以让装饰器以一致的方式使用，既可以带括号也可以不带括号。

要理解代码是如何工作的，就需要对装饰器是如何施加到函数上，以及对它们的调用约定有着透彻的理解才行。以一个简单的装饰器为例：

```txt
Example use @logged def add(x, y): return x + y 
```

调用顺序是这样的：

```python
def add(x, y):
    return x + y
add = logged(add) 
```

在这种情况下，被包装的函数只是作为第一个参数简单地传递给 logged。因而，在解决方案中，logged()的第一个参数就是要被包装的那个函数。其他所有的参数都必须有一个默认值。

对于一个可接受参数的装饰器，例如：

```python
@logged(level=logging.CRITICAL, name='example')  
def spam():
    print('Spam!') 
```

其调用顺序是这样的：

```python
def spam():
    print('Spam!')
    spam = logged(level=logging.CRITICAL, name='example')(spam) 
```

在初次调用 logged()时，被包装的函数并没有传递给 logged。因此在装饰器中，被包装的函数必须作为可选参数。这样一来，反过来迫使其他的参数都要通过关键字来指定。此外，当传递了参数后装饰器应该返回一个新函数，要包装的函数就作为参数传递给这个新函数（见 9.5 节）。要做到这一点，我们在解决方案中利用 functools.partial 来实现这个聪明的技巧。具体来说，它只是返回了一个部分完成的版本，除了要被包装的

函数之外，其他所有的参数都已经确定好了。关于 partial()的使用，可参阅 7.8 节以获取更多的细节。

# 9.7 利用装饰器对函数参数强制执行类型检查

# 9.7.1 问题

我们想为函数参数添加强制性的类型检查功能，将其作为一种断言或者与调用者之间的契约。

# 9.7.2 解决方案

在给出解决方案代码之前，本节的目标是提供一种手段对函数的输入参数类型做强制性的类型检查。下面这个简短的示例说明了这种思想：

```txt
>>> @typeassert(int, int)
... def add(x, y):
... return x + y
...
>>> 
>>> add(2, 3)
5
>>> add(2, 'hello')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "contract.py", line 33, in wrapper
TypeError: Argument y must be <class 'int>
>>> 
```

现在，让我们看看装饰器@typeassert 的实现：

from inspect import signature   
from functools import wraps   
def typeassert(\*ty_args, \*\*ty_karges): def decorate func): # If in optimized mode, disable type checking if not __debug_: return func # Map function argument names to supplied types sig $=$ signature(func) bound_types $=$ sig.bind_partial(\*ty_args, \*\*ty_karges).arguments

@wraps func) def wrapper(\*args, \*\*kwargs): bound_values $=$ sig.bind(\*args, \*\* kwargs) #Enforce type assertions across supplied arguments for name, value in bound_valuesarguments.items(): if name in bound_types: if not isinstance(value, bound_types[name]): raise TypeError( Argument{}must be {}.format(name, bound_types[name]) ) return func(\*args, \*\*kwargs) return wrapper return decorate

我们会发现这个装饰器相当灵活，既允许指定函数参数的所有类型，也可以只指定一部分子集。此外，类型既可以通过位置参数来指定，也可以通过关键字参数来指定。示例如下：

```txt
>>> @typeassert(int, z=int)
... def spam(x, y, z=42):
...     print(x, y, z)
...
>>> spam(1, 2, 3)
1 2 3
>>> spam(1, 'hello', 3)
1 hello 3
>>> spam(1, 'hello', 'world')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "contract.py", line 33, in wrapper
TypeError: Argument z must be <class 'int>
>>> 
```

# 9.7.3 讨论

本节展示了一个高级的装饰器例子，引入了一些重要且有用的概念。

首先，装饰器的一个特性就是它们只会在函数定义的时候应用一次。在某些情况下，我们可能想禁止由装饰器添加的功能。为了做到这点，只要让装饰器函数返回那个未经过包装的函数即可。在解决方案中，如果全局变量__debug__被设为 False，下列代码就会返回未修改过的函数（当 Python 解释器以-O或-OO 的优化模式执行的话，则属于这种情况）。

```txt
def decorate(func): 
```

```txt
If in optimized mode, disable type checking  
if not __debug__:  
    return func 
```

接下来，编写这个装饰器比较棘手的地方在于要涉及对被包装函数的参数签名做检查。在这里，我们可选择的工具应该是 inspect.signature()函数。简单来说，这个函数允许我们从一个可调用对象中提取出参数签名信息。示例如下：

```python
>>> from inspect import signature
>>> def spam(x, y, z=42):
    ...
    pass
...
>>> sig = signature(spam)
>>> print(sig)
(x, y, z=42)
>>> sig.params
mappingproxy(OrdereredDict(['x', <Parameter at 0x10077a050 'x'],
('y', <Parameter at 0x10077a158 'y'],
('z', <Parameter at 0x10077a1b0 'z>)]))
>>> sig.params['z'].name
'z'
>>> sig.params['z'].default
42
>>> sig.params['z'].kind
<_ParameterKind: 'POSITIONAL_OR_KEYWORD>
>>> 
```

在装饰器实现的第一部分中，我们使用签名的 bind_partial()方法来对提供的类型到参数名做部分绑定。下面的示例说明了其中发生了些什么：

```txt
>>> bound_types = sig.bind_partial(int, z=int)  
>>> bound_types  
<inspect.BoundArguments object at 0x10069bb50>  
>>> bound_typesarguments  
OrderedDict([( 'x', <class 'int'>), ('z', <class 'int'>)]  
>>> 
```

在这个部分绑定中，我们会注意到缺失的参数被简单地忽略掉了（即，这里没有对参数 y 做绑定）。但是，绑定过程中最重要的部分就是创建了有序字典 bound_types.arguments。这个字典将参数名以函数签名中相同的顺序映射到所提供的值上。在我们的装饰器中，这个映射包含了我们打算强制施行的类型断言。

在由装饰器构建的包装函数中用到了 sig.bind()方法。bind()就如同 bind_partial()一样，只是它不允许出现缺失的参数。因此，下面的示例中必须给出所有的参数：

```txt
>>> bound_values = sig.bind(1, 2, 3)  
>>> bound_valuesarguments  
OrderedDict(['x', 1), ('y', 2), ('z', 3)])  
>>> 
```

利用这个映射，要强制施行断言相对来说就很简单了：

```python
>>> for name, value in bound_valuesarguments.items(): if name in bound_typesarguments: if not isinstance(value, bound_typesarguments[name]): raise TypeError()   
>>> 
```

解决方案中一个多少有些微妙的地方是，对于具有默认值的参数，如果未提供参数，则断言机制不会作用在其默认值上。例如，下面的代码可以工作，即使 items 的默认值是“错误”的类型：

```txt
>>> @typeassert(int, list)  
... def bar(x, items=None):  
... if items is None:  
... items = []  
... items.append(x)  
... return items  
>>> bar(2)  
[2]  
>>> bar(2,3)  
Traceback (most recent call last):  
File "<stdin>", line 1, in <module>  
File "contract.py", line 33, in wrapper  
TypeError: Argument items must be <class 'list'>  
>>> bar(4, [1, 2, 3])  
[1, 2, 3, 4]  
>>> 
```

最后一点关于设计上的讨论应该就是装饰器参数与函数注解（function annotation）的对比了。例如，为什么不把装饰器实现为检查函数注解呢？

@typeassert   
def spam(x:int，y，z:int $= 42$ ： print(x,y,z)

不使用函数注解的一个可能原因在于函数的每个参数只能赋予一个单独的注解。因此，如果把注解用于类型断言，则它们就不能用在别处了。此外，装饰器@typeassert 不能用于使用了注解的函数还有另一个原因。如同解决方案中展示的那样，通过使用装饰

器参数，这个装饰器变得更加通用了，可以用于任何函数——即使是使用了注解的函数也是如此。

关于函数签名对象的更多信息可以在 PEP 362（http://www.python.org/dev/peps/ pep-0362）以及 inspect 模块的文档（http://docs.python.org/3/library/inspect.html）中找到。9.16 节中也有一个额外的示例可供参考。

# 9.8 在类中定义装饰器

# 9.8.1 问题

我们想在类中定义一个装饰器，并将其作用于其他的函数或者方法上。

# 9.8.2 解决方案

在类中定义一个装饰器是很直接的，但是首先我们需要理清装饰器将以什么方式来应用。具体来说就是以实例方法还是以类方法的形式应用。下面的示例说明了这些区别：

```python
from functools import wraps   
class A: #Decorator as an instance method def decorator1(self, func): @wraps(func) def wrapper(\*args, \*\*kwargs): print('Decorator 1') return func(\*args, \*\* kwargs) return wrapper #Decorator as a class method @classmethod def decorator2(cls, func): @wraps(func) def wrapper(\*args, \*\*kwargs): print('Decorator 2') return func(\*args, \*\* kwargs) return wrapper 
```

下面的示例展示了这两种装饰器会如何应用：

```txt
As an instance method  
a = A()  
@adecorator1 
```

```python
def spam(): pass   
#As a class method @Adecorator2   
def grok(): pass 
```

如果观察得够仔细，就会发现其中一个装饰器来自于实例 a，而另一个装饰器来自于类 A。

# 9.8.3 讨论

在类中定义装饰器乍看起来可能有些古怪，但是在标准库中也可以找到这样的例子。尤其是，内建的装饰器@property 实际上是一个拥有 getter()、setter()和 deleter()方法的类，每个方法都可作为一个装饰器。示例如下：

class Person: #Create a property instance first_name $=$ property() #Apply decorator methods @first_name.getter def first_name(self): return self._first_name @first_name setter def first_name(self, value): if not isinstance(value,str): raise TypeError('Expected a string') self._first_name $=$ value

至于为什么要定义成这种形式，关键原因在于这里的多个装饰器方法都在操纵 property实例的状态。因此，如果需要装饰器在背后记录或合并信息，这就是个很明智的方法。

当编写类中的装饰器时，一个常见的困惑就是如何在装饰器代码中恰当地使用 self 或cls 参数。尽管最外层的装饰器函数比如 decorator1()或 decorator2()需要提供一个 self 或cls 参数（因为它们是类的一部分），但内层定义的包装函数一般不需要包含额外的参数。这就是为什么示例中两个装饰器创建的 wrapper()函数并没有包含 self 参数的原因。唯一一种可能会用到这个参数的场景就是需要在包装函数中访问实例的某个部分。否则，就不必为此操心。

关于把装饰器定义在类的内部，还有最后一个微妙的方面需要考虑。那就是它们在继

承中的潜在用途。例如，假设想把定义在类 A 中的装饰器施加于定义在子类 B中的方法上。要做到这点，需要像这样编写代码：

```python
class B(A):
    @Adecorator2
    def bar(self):
        pass 
```

特别是，这里的装饰器必须定义为类方法，而且使用时必须显式地给出父类A的名称。不能使用像@B.decoator2 这样的名称，因为在定义该方法的时候类 B 根本就没有创建出来。

# 9.9 把装饰器定义成类

# 9.9.1 问题

我们想用装饰器来包装函数，但是希望得到的结果是一个可调用的实例。我们需要装饰器既能在类中工作，也可以在类外部使用。

# 9.9.2 解决方案

要把装饰器定义成类实例，需要确保在类中实现__call__()和__get__()方法。例如，下面的代码定义了一个类，可以在另一个函数上添加一个简单的性能分析层：

import types   
from functools import wraps   
class Profiled: def__init__(self, func): wraps(func)(self) self.ncalls $= 0$ def__call__(self,\*args,\*\*kwargs): self.ncalls $+ = 1$ return self._wrapped_(\*args,\*\* kwargs) def__get__(self,instance,cls): if instance is None: return self else: return types.MethodType(self,instance)

要使用这个类，可以像一个普通的装饰器一样，要么在类中要么在类外部使用：

@Profiled   
def add(x,y): return $\mathbf{x} + \mathbf{y}$ class Spam: @Profiled def bar(self,x): print(self,x)

下面的交互式会话展示了这些函数是如何工作的：

>>>add(2,3)   
5   
>>>add(4,5)   
9   
>>>add.ncalls   
2   
>>>s $=$ Spam()   
>>>s.bar(1)   
<_main_.Spam object at 0x10069e9d0>1   
>>>s.bar(2)   
<_main_.Spam object at 0x10069e9d0>2   
>>>s.bar(3)   
<_main_.Spam object at 0x10069e9d0>3   
>>>Spam.bar.ncalls   
3

# 9.9.3 讨论

把装饰器定义成类通常是简单明了的。但是，这里有一些相当微妙的细节值得做进一步的解释，尤其是计划将装饰器应用在实例的方法上时。

首先，这里对 functools.wraps()函数的使用和在普通装饰器中的目的一样——意在从被包装的函数中拷贝重要的元数据到可调用实例中。

其次，解决方案中所展示的__get__()方法常常容易被忽视。如果省略掉__get__()并保留其他所有的代码，会发现当尝试调用被装饰的实例方法时会出现怪异的行为。

例如：

>>>s $=$ Spam()   
>>>s.bar(3)   
Traceback (most recent call last):   
...   
TypeError: spam() missing 1 required positional argument:'x'

出错的原因在于每当函数实现的方法需要在类中进行查询时，作为描述符协议

（descriptor protocol）的一部分，它们的__get__()方法都会被调用，这部分内容在 8.9 节中已描述过。在这种情况下，__get__()的目的是用来创建一个绑定方法对象（最终会给方法提供 self 参数）。下面的例子说明了其中的机理：

>>>s $=$ Spam()   
>>>def grok(self，x): pass   
>>>grok._get_(s,Spam)   
<body method Spam.grok of __main_.Spam object at 0x100671e90>>   
>>>

在本节中，__get__()方法在这里确保了绑定方法对象会恰当地创建出来。type.MethodType()手动创建了一个绑定方法在这里使用。绑定方法只会在使用到实例的时候才创建。如果在类中访问该方法，__get__()的 instance 参数就设为 None，直接返回Profiled 实例本身。这样就使得获取实例的 ncalls 属性成为可能。

如果想在某些方面避免这种混乱，可以考虑装饰器的替代方案，即 9.5 节中描述过的利用闭包和 nonlocal变量。示例如下：

import types   
from functools import wraps   
def profiled(func): ncalls $= 0$ @wraps(func) def wrapper $(^{*}\arg ,^{**}k\arg s)$ : nonlocal ncalls ncalls $+ = 1$ return func $(^{*}\arg ,^{**}k\arg s)$ wrapper.ncalls $=$ lambda: ncalls return wrapper   
#Example   
@profiled   
def add(x,y): return x + y

这个例子使用起来和之前的方案几乎一致，除了现在访问 ncalls时是以函数属性的形式来进行。示例如下：

```txt
>> add(2, 3)  
5  
>> add(4, 5)  
9 
```

```txt
>>> add.ncalls()
2
>>> 
```

# 9.10 把装饰器作用到类和静态方法上

# 9.10.1 问题

我们想在类或者静态方法上应用装饰器。

# 9.10.2 解决方案

将装饰器作用到类和静态方法上是简单而直接的，但是要保证装饰器在应用的时候需要放在 $@$ classmethod 和 $@$ staticmethod 之前。示例如下：

import time   
from functools import wraps   
# A simple decorator   
def timethis(func): @wraps(func) def wrapper(\*args, \*\*kwargs): start $=$ time.time() r $=$ func(\*args,\*\* kwargs) end $=$ time.time() print(end-start) return r return wrapper   
# Class illustrating application of the decorator to different kinds of methods   
class Spam: @timethis def instance_method(self, n): print(self, n) while n>0: n $= = 1$ @classmethod @timethis def class_method(cls, n): print(cls, n) while n>0: n $= = 1$

@staticmethod   
@timethis   
def static_method(n): print(n) while $n > 0$ .. n $- = 1$

上面代码中的类和静态方法应该能够正常工作，此外还为它们添加了额外的计时功能：

>>>s $=$ Spam()   
>>>s.instance_method(1000000)   
<main.Spam object at 0x1006a6050>1000000   
0.11817407608032227   
>>>Spam.class_method(1000000)   
<class'_main>.Spam'>1000000   
0.11334395408630371   
>>>Spam(static_method(1000000)   
1000000   
0.11740279197692871   
>>>

# 9.10.3 讨论

如果装饰器的顺序搞错了，那么将得到错误提示。例如，如果像下面这样使用装饰器：

```matlab
class Spam:   
...   
@timethis   
@staticmethod   
def static_method(n): print (n) while n > 0: n -- 1 
```

这样的话，调用 static_method 将崩溃：

```txt
>>> Spam(static_method(1000000))  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
    File "timethis.py", line 6, in wrapper  
        start = time.time()  
TypeError: 'staticmethod' object is not callable  
>>> 
```

这里的问题在于@classmethod 和 $@$ staticmethod 并不会实际创建可直接调用的对象。相反，它们创建的是特殊的描述符对象（参见 8.9 节中对描述符的讲解）。因此，如果尝

试在另一个装饰器中像函数那样使用它们，装饰器就会崩溃。确保这些装饰器出现在@classmethod 和 $@$ staticmethod 之前就能解决这个问题。

本节提到的技术有一个至关重要的应用场景，那就是在抽象基类中定义类方法和静态方法，这也是在 8.12 节中谈到的主题。例如，如果想定义一个抽象类方法，可以使用下面的代码来完成：

```python
from abc import ABCMeta, abstractmethod 
```

```python
class A(metaclass=ABCMeta):
    @lassmethod
    @abstractmethod
    def method(cls):
        pass 
```

在上述代码中，@classmethod 和 $@$ abstractmethod 出现的顺序是很重要的。如果将这两个装饰器调换一下位置，那么就会产生崩溃。

# 9.11 编写装饰器为被包装的函数添加参数

# 9.11.1 问题

我们想编写一个装饰器为被包装的函数添加额外的参数。但是，添加的参数不能影响到该函数已有的调用约定。

# 9.11.2 解决方案

可以使用 keyword-only 参数将额外的参数注入到函数的调用签名中。考虑如下的装饰器：

from functools import wraps   
def optionalDebug func) @wraps func) def wrapper $\text{串}$ args，debug $=$ False，\*\*kwarges): if debug: print('Calling'，func._name_） return func $\text{串}$ args，\*\*kwarges) return wrapper

下面的示例展示了装饰器是如何工作的：

```txt
>>> @optionalDebug
... def spam(a,b,c): 
```

```prolog
print(a,b,c)   
>>> spam(1,2,3)   
1 2 3   
>>> spam(1,2,3, debug=True)   
Calling spam   
1 2 3   
>>> 
```

# 9.11.3 讨论

为被包装的函数添加额外的参数并不是装饰器最常见的用法。但是，对于避免某些特定的代码重复模式来说是一项有用的技术。例如，如果有一些这样的代码：

```python
def a(x, debug=False):
    if debug:
        print('Calling a')
    ...
def b(x, y, z, debug=False):
    if debug:
        print('Calling b')
    ...
def c(x, y, debug=False):
    if debug:
        print('Calling c')
    ... 
```

可以将这些代码重构为如下形式：

```txt
@optionalDebug  
def a(x):  
    ...  
@optionalDebug  
def b(x, y, z):  
    ...  
@optionalDebug  
def c(x, y):  
    ... 
```

本节给出的实现依赖于这样一个事实，即 keyword-only 参数可以很容易地添加到那些以*args 和**kwargs 作为形参的函数上。keyword-only 参数会作为特殊情况从随后的调用中挑选出来，调用函数时只会使用剩下的位置参数和关键字参数。

这里有个棘手的问题需要考虑。在添加的参数和被包装函数的参数之间可能会出现潜在的名称冲突问题。例如，如果把 $@$ optional_debug 装饰器作用到一个已经把 debug 作为参数的函数上，此时就会出错。要解决这个问题，需要添加额外的检查：

```python
from functools import wraps   
import inspect   
def optional_debug func): if 'debug' in inspect.getargspec(func).args: raise TypeError('debug argument already defined') @wraps func) def wrapper(\*args, debug=False, \*\*kwargs): if debug: print('Calling', func._name_) return func(\*args, \*\* kwargs) return wrapper 
```

本节中最后一个需要考虑修改的地方在于如何恰当地管理函数签名。精明的程序员会意识到被包装函数的签名是错误的。例如：

```txt
>>> @optionalDebug  
... def add(x,y):  
... return x+y  
...  
>>> import inspect  
>>> print(inspectsignature(add))  
(x,y)  
>>> 
```

这可以通过如下的修改来解决：

from functools import wraps   
import inspect   
def optional_debug func): if 'debug' in inspect.getargspec(func).args: raise TypeError('debug argument already defined') @wraps(func) def wrapper(\*args, debug=False, \*\*kwargs): if debug: print('Calling',func._name_) return func(\*args,\*\* kwargs) sig $=$ inspectsignature(func)

```python
parms = list(sig.params.values())
parms.append(inspect.getParameter('debug',
                  inspect.getParameter(KEYWORD_ONLY,
                  default=False))
wrapper._signature_ = sig.replace_parameters=parms)
return wrapper 
```

修改之后，现在包装函数的签名就能正确反映出 debug 参数了。示例如下：

```txt
>>> @optional_debug  
... def add(x, y):  
... return x + y  
...  
>>> print(inspectsignature(add))  
(x, y, *, debug=False)  
>>> add(2, 3)  
5  
>>> 
```

要获得更多有关函数签名方面的信息，可参阅 9.16 节。

# 9.12 利用装饰器给类定义打补丁

# 9.12.1 问题

我们想检查或改写一部分类的定义，以此来修改类的行为，但是不想通过继承或者元类的方式来做。

# 9.12.2 解决方案

对于类装饰器来说这是绝佳的应用场景。比方说，下面有一个类装饰器重写了_getattribute__特殊方法，为其加上了日志记录功能。

```python
def log_getattribute(cls):
    # Get the original implementation
    orig_getattribute = cls.__getattribute__
    # Make a new definition
    def new_getattribute(self, name):
        print('getting:', name)
        return orig_getattribute(self, name)
    # Attach to the class and return
    cls.__getattribute__ = new_getattribute 
```

```python
return cls   
#Example use   
@log_getattribute   
class A: def __init__(self,x): self.x=x def spam(self): pass 
```

如果试着使用解决方案中给出的类，就会得到以下结果：

```txt
>> a = A(42)  
>> a.x  
getting: x  
42  
>> a.spam()  
getting: spam  
>>> 
```

# 9.12.3 讨论

类装饰器常常可以直接作为涉及混合类（mixin）或者元类等高级技术的替代方案。例如，对于解决方案中的例子，另一种可选的实现方法是使用继承：

```python
class LoggedGetattribute: def __attribute__(self, name): print('getting:','name) return super().__attribute__(name) #Example:   
class A(LoggedGetattribute): def __init__(self,x): self.x = x def spam(self): pass 
```

这么做可行，但是要想理解其中的原理，则必须对方法解析顺序（MRO）、super()以及其他有关继承方面的知识有所了解（详见 8.7 节）。从某种意义上说，类装饰器这种解决方案要更加直接，而且不会在继承体系中引入新的依赖关系。事实证明，由于不依赖对 super()函数的使用，运行速度也会稍快一些。

如果要将多个类装饰器作用于某个类之上，那么可能需要考虑添加的顺序问题。例如，如果某个装饰器是用全新的实现来替换一个类方法，而另一个装饰器只是对已有的方法做包装，添加一些额外的逻辑处理，那么很可能需要先将第一个装饰器作用于类上。

请参考 8.13 节中另一个关于类装饰器的示例。

# 9.13 利用元类来控制实例的创建

# 9.13.1 问题

我们想改变实例创建的方式，以此来实现单例模式、缓存或者其他类似的特性。

# 9.13.2 解决方案

作为 Python 程序员，大家都应该知道如果定义了一个类，那么创建实例时就好像在调用一个函数一样。示例如下：

```python
class Spam: def __init__(self, name): self.name = name  
a = Spam('Guido')  
b = Spam('Diana') 
```

如果想定制化这个步骤，则可以通过定义一个元类并以某种方式重新实现它的__call__()方法。为了说明这个过程，假设我们不想让任何人创建出实例：

class NoInstances(type): def__call__(self，\*args，\*\*kwargs)： raise TypeError("Can't instantiate directly") #Example   
classSpam(metaclass $\equiv$ NoInstances）： @staticmethod def grok(x): print('Spam.grok')

在这种情况下，用户可以调用定义的静态方法，但是没法以普通的方式创建出实例。示例如下：

```txt
>>> Spam.grok(42)   
Spam.grok   
>> s = Spam()   
Traceback (most recent call last): File "<stdin>", line 1, in <module> File "example1.py", line 7, in __call__ raise TypeError("Can't instantiate directly")   
TypeError: Can't instantiate directly   
>>> 
```

现在，假设我们想实现单例模式（即，这个类只能创建唯一的一个实例）。相对来说这就很直接了，示例如下：

```python
class Singleton(type): def __init__(self, *args, **kwargs): self._instance = None super().__init__(*args, **kwargs) def __call__(self, *args, **kwargs): if self._instance is None: self._instance = super().__call__(*args, **kwargs) return self._instance else: return self._instance # Example class Spam(metaclass=Singleton): def __init__(self): print('Creating Spam') 
```

在这种情况下，这个类只能创建出唯一的实例。示例如下：

```txt
>>> a = Spam()
Creating Spam
>>> b = Spam()
>>> a is b
True
>>> c = Spam()
>>> a is c
True
>>> 
```

最后，假设我们想创建缓存实例（cached instance，8.25 节有介绍）。我们用一个元类来实现：

import weakref   
class Cached(type): def __init__(self, \*args, \*\*kwargs): super(）.__init__(\*args，\*\* kwargs) self._cache $=$ weakest.WeakValueDictionary() def __call__(self,\*args): if args in self._cache: return self._cache[args] else: obj $=$ super(）.__call_(\*args) self._cache[args] $=$ obj

return obj   
#Example   
class Spam(metaclass=Cached): def__init__(self,name): print('CreatingSpam({!r})'.format(name)) self.name $=$ name

下面的交互式会话展示了这个类的行为：

```txt
>>> a = Spam('Guido')  
Creating Spam('Guido')  
>>> b = Spam('Diana')  
Creating Spam('Diana')  
>>> c = Spam('Guido')  # Cached  
>>> a is b  
False  
>>> a is c  # Cached value returned  
True 
```

# 9.13.3 讨论

通过元类来实现各种创建实例的模式常常比那些不涉及元类的解决方案要优雅。如果不用元类，那就得将类隐藏在某种额外的工厂函数之后。例如，要实现单例模式，可能会用到下面这种技巧：

class_Spam: def_init_self): print('Creating Spam') _spam_instance $=$ None   
defSpam(): global_spam_instance if_spam_instanceisnotNone: return_spam_instance else: _spam_instance $=$ _Spam() return_spam_instance

尽管使用元类的解决方案涉及许多更加高级的概念，但最终的代码看起来会更加清晰，也没有那么多所谓的技巧。

参见8.25 节以得到更多关于创建缓存实例、弱引用（weak reference）以及其他细节方面的信息。

# 9.14 获取类属性的定义顺序

# 9.14.1 问题

我们想自动记录下属性和方法在类中定义的顺序，这样就能利用这个顺序来完成各种操作（例如序列化处理、将属性映射到数据库中等）。

# 9.14.2 解决方案

要获取类定义体中的有关信息，可以通过元类来轻松实现。在下面的示例中，元类使用 OrderedDict（有序字典）来获取描述符的定义顺序：

```python
from collections import OrderedDict
# A set of descriptors for various types
class Typed:
    _expected_type = type(None)
    def __init__(self, name=None):
        self._name = name
    def __set__(self, instance, value):
        if not isinstance(value, self._expected_type):
            raise TypeError('Expected' + str(self._expected_type))
            instance._dict_[self._name] = value
class Integer(Typed):
    _expected_type = int
class Float(Typed):
    _expected_type = float
class String(Typed):
    _expected_type = str
# Metaclass that uses an OrderedDict for class body
class OrderedMeta(type):
    def __new__(cls, clsname, bases, clsdict):
        d = dict(clsdict)
        order = []
    for name, value in clsdict.items():
        if isinstance(value, Typed):
            value._name = name
            order.append(name) 
```

d['order'] $=$ order return type._new_(cls, clsname, bases, d) @classmethod def _prepare_(cls, clsname, bases): return OrderedDict()

在这个元类中，描述符的定义顺序是通过使用OrderedDict在执行类的定义体时获取到的。得到的结果会从字典中提取出来然后保存到类的属性_order 中。这之后，类方法能够以各种方式使用属性_order。例如，下面这个简单的类利用这个顺序实现了一个方法，用来将实例数据序列化为一行 CSV 数据：

```python
class Structure (metaclass=OrderedMeta):
    def as_csv(self):
        return '.join(str(getattr(self, name)) for name in self._order) # Example use
class Stock (Structure):
    name = String()
    shares = Integer()
    price = Float()
    def __init__(self, name, shares, price):
        self.name = name
        self.share = shares
        self.price = price 
```

下面的交互式会话展示了如何使用例子中的 Stock 类：

```txt
>>> s = Stock('GOOG', 100, 490.1)
>>> s.name
'GOOG'
>>> s.as_csv()
'GOOG,100,490.1'
>>> t = Stock('AAPL','a lot', 610.23)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "dupmethod.py", line 34, in __init__
TypeError: shares expects <class 'int>
>>> 
```

# 9.14.3 讨论

本节的全部核心就在__prepare__()方法上，该特殊方法定义在元类 OrderedMeta 中。该方法会在类定义一开始的时候立刻得到调用，调用时以类名和基类名称作为参数。它必须返回一个映射型对象（mapping object）供处理类定义体时使用。由于返回的是 OrderedDict 实例而不是普通的字典，因此类中各个属性间的顺序就可以方便地得

到维护。

如果想使用自定义的字典型对象，那么对上述功能进行扩展也是有可能的。例如，考虑下面这个解决方案，它可以拒绝类中出现重复的定义：

from collections import OrderedDict   
class NoDupOrderedDict(OrderedDict): def __init__(self, clsname): self.clsname $=$ clsname super().__init_(） def __setitem__(self, name, value): if name in self: raise TypeError({} already defined in \{\}.'.format(name,self.clsname)) super().__setitem__(name, value)   
class OrderedMeta(type): def __new__(cls, clsname, bases, clsdict): d $=$ dict(clsdict) d['order'] $=$ [name for name in clsdict if name[0] !='_'] return type._new__(cls, clsname, bases, d) @lassmethod def __prepare__(cls, clsname, bases): return NoDupOrderedDict(clsname)

如果使用这个元类，并且创建一个类让它拥有重复的属性，看看会发生什么吧：

>>>class A(metaclass $\equiv$ OrderedMeta): def spam(self): pass def spam(self): Pass Traceback (most recent call last): File "<stdin>", line 1, in <module> File "<stdin>", line 4, in A File "dupmethod2.py", line 25, in __setitem__(name,self.clsname))   
TypeError: spam already defined in A   
>>>

本节中最后一个需要考虑的重要部分是在元类的__new__()方法中对自定义的字典应该如何处理。尽管我们在类定义中使用的是其他形式的字典，当创建最终的类对象时，还是需要将这个字典转换为一个合适的dict实例才行。这正是 $\mathrm { d } =$ dict(clsdict)这行代码

的目的所在。

能够获取到类属性的定义顺序看起来似乎微不足道，但对于某些特定类型的应用来说却是非常重要的功能。例如，在一个对象关系映射器（ORM）中，类的编写方式可能同示例中展示的那样很相似：

```python
class Stock (Model):
    name = String()
    shares = Integer()
    price = Float() 
```

而在底层，可能想获取到属性定义的顺序，以此将对象映射到数据库表项中的元组或者行上（即，和示例中 as_csv()方法实现的功能类似）。本节给出的解决方案是非常直截了当的，而且通常情况下比其他可选的方法要更简单（一般会通过在描述符类中维护一个隐藏的计数器来实现）。

# 9.15 定义一个能接受可选参数的元类

# 9.15.1 问题

我们想定义一个元类，使得在定义类的时候能够提供可选的参数。这样的话在创建类型的时候可以对处理过程进行控制或配置。

# 9.15.2 解决方案

在定义类的时候，Python 允许我们在 class 语句中通过使用 metaclass 关键字参数来指定元类。例如，在抽象基类中我们可以这样指定元类：

from abc import ABCMeta, abstractmethod   
class IStream(metaclass=ABCMeta): @abstractmethod def read(self,maxsize $\equiv$ None): pass @abstractmethod def write(self,data): pass

但是，在自定义的元类中我们还可以提供额外的关键字参数，就像这样：

```python
class Spam(metaclass=MyMeta, debug=True, synchronize=True): 
```

要在元类中支持这样的关键字参数，需要保证在定义__prepare__()、__new__()以及__init__()方法时使用 keyword-only 参数来指定它们，就像下面这样：

```python
class MyMeta(type): # Optional @classmethod def _prepare_(cls, name, bases, \*, debug=False, synchronize=False): # Custom processing return super()._prepare_(name, bases) # Required def _new_(cls, name, bases, ns, \*, debug=False, synchronize=False): # Custom processing return super()._new_(cls, name, bases, ns) # Required def _init_(self, name, bases, ns, \*, debug=False, synchronize=False): # Custom processing super()._init_(name, bases, ns) 
```

# 9.15.3 讨论

要对元类添加可选的关键字参数，需要理解类创建过程中所涉及的所有步骤。这是因为额外的参数会传递给每一个与该过程相关的方法。__prepare__()方法是第一个被调用的，用来创建类的名称空间，这是在处理类的定义体之前需要完成的。一般来说，这个方法只是简单地返回一个字典或者其他的映射型对象。__new__()方法用来实例化最终得到的类型对象，它会在类的定义体被完全执行完毕后才调用。最后调用的是__init__()方法，用来执行任何其他额外的初始化步骤。

当编写元类时，比较常见的做法是只定义一个__new__()或者__init__()方法，而不会同时定义这两者。但是，如果打算接受额外的关键字参数，那么这两个方法都必须提供，并且要提供可兼容的函数签名。默认的__prepare__()方法可接受任意的关键字参数，只是会忽略它们。唯一一种需要自行定义__prepare__()方法的情况就是当额外的参数多少会影响到名称空间的创建管理时。

本节中使用了 keyword-only 参数，这也反映出一个事实，即这样的参数在创建类的过程中只会以关键字形式提供。

用关键字参数来配置元类也可以看做是通过类变量来实现同一目标的另一种方式。例如我们也可以这样实现对类的配置：

```txt
class Spam(metaclass=MyMeta): 
```

debug $=$ True  
synchronize $=$ True  
...

通过提供额外参数的方式来实现，这么做的优点在于它们不会污染类的名称空间。因为这些参数只对于类的创建而言有意义，对于类中需要执行的语句来说是没有实际意义的。此外，它们对于__prepare__()方法来说是可见的，而该方法会在处理类定义体中任何语句之前先得到运行。而另一方面，类变量只能被元类的__new__()和__init__()方法访问。

# 9.16 在*args 和**kwargs 上强制规定一种参数签名

# 9.16.1 问题

我们已经编写了一个使用*args 和**kwargs 作为参数的函数或者方法，这样使得函数成为通用型的（即，可接受任意数量和类型的参数）。但是我们也想对传入的参数做检查，看看它们是否匹配了某个特定的函数调用签名。

# 9.16.2 解决方案

任何关于操作函数调用签名的问题，都应该使用 inspect 模块中的相应功能。这里我们尤其感兴趣的是 Signature 和 Parameter 这两个类。下面用一个交互式的例子来说明如何创建一个函数签名：

```prolog
>>> from inspect import Signature, Parameter
>>> # Make a signature for a func(x, y=42, *, z=None)
>>> parms = [Parameter('x', Parameter.POPISTIONAL_OR_KEYWORD), ...
...
...
...     Parameter('y', Parameter.POPISTIONAL_OR_KEYWORD, default=42),
...
...
...     Parameter('z', Parameter.POPISTIONAL_OR_KEYWORD_ONLY, default=None)]
>>> sig = Signatureparms)
>>> print(sig)
(x, y=42, *, z=None)
>>> 
```

一旦有了签名对象，就可以通过对象的 bind()方法轻松将其绑定到*args 和**kwargs 上。示例如下：

```python
>>> def func(*args, **kwargs):  
...        bound_values = sig.bind(*args, **kwargs)  
...        for name, value in bound_values.args.items():  
...        print(name, value)  
...  
>>> # Try various examples 
```

```txt
>>> func(1, 2, z=3)
x 1
y 2
z 3
>>> func(1)
x 1
>>> func(1, z=3)
x 1
z 3
>>> func(y=2, x=1)
x 1
y 2
>>> func(1, 2, 3, 4)
Traceback (most recent call last):
...
...
File "/usr/local/lib/python3.3inspect.py", line 1972, in _bind raise TypeError('too many positional arguments')
TypeError: too many positional arguments
>>> func(y=2)
Traceback (most recent call last):
...
...
File "/usr/local/lib/python3.3inspect.py", line 1961, in _bind raise TypeError(msg) from None
TypeError: 'x' parameter lacking default value
>>> func(1, y=2, x=3)
Traceback (most recent call last):
...
...
File "/usr/local/lib/python3.3inspect.py", line 1985, in _bind {'arg:r'}.
TypeError: multiple values for argument 'x'
>>> 
```

可以看到，将签名对象绑定到传入的参数上会强制施行所有常见的函数调用规则，包括要求必传的参数（例子中为 x）、默认值、重复的参数等。

关于强制施行函数签名，这里有一个更为具体的例子。在代码中，基类定义了一个极其通用的__init__()方法，但是子类只提供一种期望接受的签名形式。

from inspect import Signature, Parameter   
defmake_sig(\*names): parms $=$ [Parameter(name，Parameter.POsiTIONAL_OR_KEYWORD) fornameinnames] returnSignature(parms)   
class Structure:

\_signature $\equiv$ make_sig() def init_self,\*args, \*\*kwargs): bound_values $=$ self._signature_.bind(\*args, \*\* kwargs) for name, value in bound_valuesarguments.items(): setattr(self, name, value) #Example use   
class Stock(Structure): _signature $\equiv$ make_sig('name','shares', 'price')   
class Point(Structure): _signature $\equiv$ make_sig('x'，'y')

下面的交互式会话说明了 Stock 类是如何工作的：

```haskell
>>> import inspect
>>> print(inspectsignature(Stock))
(name, shares, price)
>>> s1 = Stock('ACME', 100, 490.1)
>>> s2 = Stock('ACME', 100)
Traceback (most recent call last):
...
TypeError: 'price' parameter lacking default value
>>> s3 = Stock('ACME', 100, 490.1, shares=50)
Traceback (most recent call last):
...
TypeError: multiple values for argument 'shares'
>>> 
```

# 9.16.3 讨论

当需要编写通用型的库、编写装饰器或者实现代理时，使用形参为*args 和**kwargs 的函数是非常常见的。但是，这种函数的一个缺点就是如果想实现自己的参数检查机制，代码很快就会变的笨拙而混乱。这方面的例子可参考 8.11 节。使用签名对象则能简化这个步骤。

在解决方案的最后一个例子中，如果使用自定义的元类来创建签名对象也是很有意义的。下面的示例展示了这种替代方案是如何实现的：

from inspect import Signature, Parameter   
defmake_sig(\*names): parms $=$ [Parameter(name，Parameter.POsiTIONAL_OR_KEYWORD) fornameinnames] returnSignature(parms)

class StructureMeta(type): def_new(_cls, clsname, bases, clsdict): clsdict['_signature_'] = make_sig(*clsdict.get('_fields',[]) return super()._new(_cls, clsname, bases, clsdict)   
class Structure(metaclass $\equiv$ StructureMeta): _fields $= []$ def_init_self,\*args,\*\*karges): bound_values $=$ self._signature_.bind(\*args,\*\*karges) for name, value in bound_valuesarguments.items(): setattr(self, name, value)   
#Example   
class Stock(Structure): _fields $=$ ['name','shares', 'price']   
class Point(Structure): _fields $=$ ['x'，'y']

当定义定制化的签名时，把签名对象保存到一个特殊的属性__signature__中常常是很有用的。如果这么做了，使用了 inspect 模块的代码在执行反射（introspection）操作时将能够获取到签名并将其作为函数的调用约定。示例如下：

```haskell
>>> import inspect
>>> print(inspectsignature(Stock))
(name, shares, price)
>>> print(inspectsignature.Point))
(x, y)
>>> 
```

# 9.17 在类中强制规定编码约定

# 9.17.1 问题

我们的程序由一个庞大的类继承体系组成，我们想强制规定一些编码约定（或者做一些诊断工作），使得维护这个程序的程序员能够轻松一些。

# 9.17.2 解决方案

如果想对类的定义进行监控，通常可以用元类来解决。一个基本的元类通常可以通过从 type 中继承，然后重定义它的__new__()或者__init__()方法即可。示例如下：

```python
class MyMeta(type): def __new__(self, clsname, bases, clsdict): # clsname is name of class being defined 
```

```ruby
# bases is tuple of base classes # clsdict is class dictionary return super()._new_(cls, clsname, bases, clsdict) 
```

另一种方式是定义__init__()：

```python
class MyMeta(type): def__init__(self,clsname,bases,clsdict): super().__init__(clsname,bases,clsdict) #clsnameisnameofclassbeingdefined #basesistupleofbaseclasses #clsdictisclassdictionary 
```

要使用元类，一般来说会将其作用到一个顶层基类上，然后让其他子类继承之。示例如下：

```python
class Root (metaclass=MyMeta):
    pass
class A(Root):
    pass
class B(Root):
    pass 
```

元类的一个核心功能就是允许在定义类的时候对类本身的内容进行检查。在重新定义的__init__()方法中，我们可以自由地检查类字典、基类以及其他更多信息。此外，一旦为某个类指定了元类，该类的所有子类都会自动继承这个特性。因此，聪明的框架实现者可以在庞大的类继承体系中为其中一个顶层基类指定一个元类，然后就可以获取到位于该基类之下的所有子类的定义了。

下面是一个有些异想天开的例子，这里的元类可用来拒绝类定义中包含大小写混用的方法名（也许这就是为了恶心一下 Java程序员）：

```python
class NoMixedCaseMeta(type): def new (cls, clsname, bases, clsdict): for name in clsdict: if name.lower() != name: raise TypeError('Bad attribute name:' + name) return super().new (cls, clsname, bases, clsdict)   
class Root(metaclass=NoMixedCaseMeta): pass 
```

```python
class A(Root):
    def foo_bar(self): # Ok 
```

```txt
pass   
class B(Root): def fooBar(self): #TypeError pass 
```

作为一个更加高级而且有用的例子，下面定义的元类可检查子类中是否有重新定义的方法，确保它们的调用签名和父类中原始的方法相同。

from inspect import signature   
import logging   
class MatchSignaturesMeta(type): def init_self,clsname,bases,clsdict): super().init(clsname,bases,clsdict) sup $=$ super(self,self) for name, value in clsdict.items(): if name.startswith('_') or not callable(value): continue #Get the previous definition (if any) and compare the signatures prev_dfn $\equiv$ getattr(super,name,None) if prev_dfn: prev_sig $\equiv$ signature(prev_dfn) val_sig $\equiv$ signature(value) if prev_sig $! =$ val_sig: logging.warning('Signature mismatch in %s. %s != %s', value._qualname_, prev_sig, val_sig)   
#Example   
class Root(metaclass=MatchSignaturesMeta): pass   
class A(Root): def foo(self,x,y): pass   
def spam(self,x,\*z): pass   
#Class with redefined methods,but slightly different signatures   
class B(A): def foo(self,a,b): pass   
def spam(self,x,z): pass

如果运行上述代码，将得到如下的输出：

WARNING:root:Signature mismatch in B.spam. (self, x, *, z) != (self, x, z)

WARNING:root:Signature mismatch in B.foo. (self, x, y) $\ ! =$ (self, a, b)

类似这样的告警信息可能对于捕获微妙的程序 bug 会很有帮助。比方说，某个方法依赖于传递给它的关键字参数，如果子类修改了参数名称那么就会崩溃。

# 9.17.3 讨论

在一个大型的面向对象程序中，有时候通过元类来控制类的定义会十分有用。元类可以监视类的定义，可用来警告程序员那些可能会被忽视的潜在问题（比如使用了不兼容的方法签名）。

有些人可能会认为像这种错误最好用程序分析工具或者 IDE 来捕获。确实，这类工具是非常有用的。但是，如果正在创建一个由其他人使用的框架或者库，那么通常是无法控制其他开发者的开发流程的（如果他们不用这类工具怎么办？）。因此，对于特定类型的应用，在元类中做一些额外的检查是很有意义的，这类检查常常使得产品有着更好的用户体验。

至于在元类中是重新定义__new__()还是__init__()，这取决于我们打算如何使用得到的结果类。__new__()会在类创建之前先得到调用，当元类想以某种方式修改类的定义时（通过修改类字典中的内容）一般会用这种方法。而__init__()方法会在类已经创建完成之后才得到调用，如果想编写代码同完全成形（fully formed）的类对象打交道，那么重新定义__init__()会很有用。在最后那个示例中我们必须重新定义__init__()。因为这里用到了 super()函数来查找父类中的定义，而这只有当类实例已经被创建出来且方法解析顺序（MRO）已经设定之后才行得通。

最后那个示例也展示了对 Python 函数签名对象的使用。从本质上说，元类首先获取类中的每一个可调用型的定义（函数、方法等），然后查找它们是否在基类中也有一个定义，如果有的话就通过 inspect.signature()来比较它们的调用签名是否一致。

最后但同样重要的是，super(self, self)这行代码并不存在输入错误。当使用元类时，很重要的一点是要意识到 self 实际上是一个类对象。因此，这行代码实际上是用来寻找位于类层次结构中更高层次上的定义，它们组成了 self 的父类。

# 9.18 通过编程的方式来定义类

# 9.18.1 问题

我们编写的代码最终需要创建一个新的类对象。我们想到将组成类定义的源代码发送

到一个字符串中，然后利用类似 exec()这样的函数来执行，但是我们希望能有一个更加优雅的解决方案。

# 9.18.2 解决方案

我们可以使用函数 types.new_class()来实例化新的类对象。所有要做的就是提供类的名称、父类名组成的元组、关键字参数以及一个用来产生类字典（class dictionary）的回调，类字典中包含着类的成员。示例如下：

```python
# stock.py
# Example of making a class manually from parts
# Methods
def __init__(self, name, shares, price):
    self.name = name
    self.share = shares
    self.price = price
def cost(self):
    return self.share * self.price
cls_dict = {
    __init__() : __init__,
    'cost': cost,
} 
```

这么做会产生一个普通的类对象，和所期望的结果一样：

```txt
>>>s = Stock('ACME'，50，91.1)   
>>>s   
<stock.Stock object at 0x1006a9b10>   
>>>s.cost()   
4555.0   
>>> 
```

在调用完 types.new_class()之后对 Stock.__module__的赋值操作是这个解决方案中的微妙之处。每当定义一个类时，其__module__属性中包含的名称就是定义该类时所在的模块名。这个名称会用来为__repr__()这样的方法产生输出，同时也会被各种库所用，比如 pickle。因此，为了让创建的类成为一个“正常”的类，需要保证将__module_

属性设置妥当。

如果想创建的类还涉及一个不同的元类，可以在 types.new_class()的第三个参数中进行指定。示例如下：

```python
>>> import abc
>>> Stock = types.new_class('Stock', (), {'metaclass': abc.ABCMeta'}, ...
...
>>> lambda ns: ns.update(cls_dict))
...
>>> Stock.__module__ = __name__
>>> Stock
<class__main__.Stock>
>>> type(Stock)
<class 'abc.ABCMeta'> 
```

第三个参数中还可以包含其他的关键字参数。例如，下面这个类定义：

```txt
class Spam(Base, debug=True, typecheck=False): 
```

转换成 new_class()调用后是这样的：

```python
Spam = types.new_class('Spam', (Base),{'debug': True, 'typecheck': False},lambda ns: ns.update(cls_dict)) 
```

new_class()的第四个参数是最为神秘的。但它实际上是一个接受映射型对象的函数，用来产生类的命名空间。这通常都会是一个字典，但实际上可以是任何由__prepare__()方法（见 9.14 节）返回的对象。这个函数应该使用 update()方法或者其他的映射操作为命名空间中添加新的条目。

# 9.18.3 讨论

能够制造出新的类对象在某些特定的上下文中会很有用。其中一个我们比较熟悉的例子和 collections.namedtuple()函数有关。示例如下：

```prolog
>>> Stock = collections+namedtuple('Stock', ['name', 'shares', 'price'])  
>>> Stock  
<class__main__.Stock'>  
>>> 
```

和我们本节展示的技术不同，namedtuple()使用了 exec()。但是，下面这个简单的函数可直接创建出类：

```txt
import operator  
import types 
```

import sys   
def namedtuple(classname, fieldnames): #Populate a dictionary of field property accessors cls_dict $=$ { name: property operator.itemgetter(n)) for n, name in enumerate(fieldnames) } #Make a _new_ function and add to the class dict def _new_(cls, \*args): if len(args) != lenFIELDnames): raise TypeError('Expected {} arguments'.format(len.fieldnames))) return tuple._new_(cls, args) cls_dict['new'] $=$ new_ #Make the class cls $=$ types.new_class(classname, (tuple,)，{}, lambda ns: ns.update(cls_dict)) #Set the module to that of the caller cls._module $=$ sys._getframe(1).f_globals['name']) return cls

上述代码的最后部分利用了所谓的“frame hack”技巧，通过 sys._getframe()来获取调用者所在的模块名称。有关 frame hack 的另一个例子可在 2.15 节中找到。

下面的示例展示了上述代码是如何工作的：

```txt
>>> Point = named_tuple('Point', ['x', 'y'])  
>>> Point  
<class__main__.Point>  
>>> p = Point(4, 5)  
>>> len(p)  
2  
>>> p.x  
4  
>>> p.y  
5  
>>> p.x = 2  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
AttributeError: can't set attribute  
>>> print('%s %s' % p)  
4 5  
>>> 
```

本节使用的技术中一个重要的方面在于对元类提供了适当支持。我们可能会倾向于通

过直接实例化一个元类来创建类。示例如下：

```txt
Stock = type('Stock', (), cls_dict) 
```

这种方法的问题在于它忽略了某些重要的步骤，比如调用元类的__prepare__()方法。通过采用 types.new_class()，可以保证所有必要的初始化步骤都能得到执行。例如，在types.new_class()中给定的第四个参数是一个回调函数，它所接受的映射型对象正是由__prepare__()方法返回的。

如果只想执行准备步骤，可以使用 types.prepare_class()。示例如下：

# import types

```python
metaclass, kwargs, ns = types.prepare_class('Stock', (), {'metaclass': type}) 
```

这么做会找到合适的元类并调用它的__prepare__()方法。元类、剩下的关键字参数以及准备好的命名空间都会得到返回。

要获得更多信息，请参考 PEP 3115（http://www.python.org/dev/peps/pep-3115）以及Python 的相关文档（http://docs.python.org/3/reference/datamodel.html%23metaclasses）。

# 9.19 在定义的时候初始化类成员

# 9.19.1 问题

我们想在定义类的时候对部分成员进行初始化，而不是在创建类实例的时候完成。

# 9.19.2 解决方案

在定义类的时候执行初始化或者配置操作是元类的经典用途。从本质上说，元类是在定义类的时候触发执行，此时可以执行额外的步骤。

下面的示例采用这种思想创建了一个类似于 collections 模块中命名元组的类：

import operator   
class StructTupleMeta(type): def__init__(cls，\*args，\*\*kwargs): super(）.__init__(\*args，\*\* kwargs) forn，namein enumerate(cls._fields): setattr(cls，name，property operator.itemgetter(n)))   
class StructTuple(tuple，metaclass $\equiv$ StructTupleMeta): _fields $= []$ defnew_（cls，\*args）:

```python
if len(args) != len(cls._fields):
    raise ValueError('{} arguments required'.format(len(cls._fields)))
return super().__new__(cls, args) 
```

上述代码允许我们定义简单的基于元组的数据结构，示例如下：

class Stock(StructTuple): _fields = ['name', 'shares', 'price']   
class Point(StructTuple): _fields $=$ ['x'，'y']

可以看看如何使用它们：

>>>s $\equiv$ Stock('ACME'，50，91.1)   
>>>s   
('ACME'，50，91.1)   
>>>s[0]   
'ACME'   
>>>s.name   
'ACME'   
>>>s.share\*s.price   
4555.0   
>>>s.share $= 23$ Traceback (most recent call last): File "<stdin>",line 1,in <module>   
AttributeError: can't set attribute   
>>>

# 9.19.3 讨论

本节中，类 StructTupleMeta 接受类属性_fields 中的属性名称，并将它们转换为属性方法，使得这些方法能够访问到元组的某个特定槽位。函数 operator.itemgetter()创建了一个访问器函数（accessor function），而函数 property()将其转换成一个 property 属性。

本节中最为棘手的部分在于如何知道不同的初始化步骤在什么时候发生。StructTupleMeta中的__init__()方法针对每个定义的类只会调用一次。参数 cls 代表着所定义的类。从本质上说，我们给出的代码利用类变量_fields 来接受新定义的类，然后为其添加一些新的部分。

类 StructTuple 作为公共基类让用户从它继承。类中的__new__()方法负责产生新的实例。这里对__new__()的使用有些不同寻常，部分原因在于我们修改了元组的调用签名，这使得现在的调用约定看起来就和普通的调用方式一致了：

```python
s = Stock('ACME', 50, 91.1) # OK  
s = Stock('ACME', 50, 91.1)) # Error 
```

和__init__()不同，__new__()方法会在类实例创建出来之前得到触发。由于元组是不可变对象（immutable），一旦它们被创建出来就无法再做任何修改了。因此，__init__()方法在类实例创建的过程中触发的时机太晚，以至于没法按我们想要的方式修改实例。这就是为什么我们要定义__new__()的原因。

尽管本节的内容比较短小，但通过仔细地学习和研究后，读者对于 Python 类的定义、类实例的创建过程以及元类和类中不同的特殊方法将在何时得到调用有着深刻的理解，这是大有益处的。

PEP 422（http://www.python.org/dev/peps/pep-0422）中还提供了一种可选的替代方案来完成本节中描述的任务。但是，在写作本书时，这份 PEP 还没有得到采纳和接受。尽管如此，如果你使用的 Python 版本要高于 3.3 的话还是值得去看一看的。

# 9.20 通过函数注解来实现方法重载

# 9.20.1 问题

我们已经学习过函数参数注解方面的知识，而我们想利用这种技术通过基于参数类型的方式来实现多分派（multiple-dispatch，或称为方法重载）。但是并不清楚这其中要涉及哪些技术，甚至对于这么做是否为一个好主意还存有疑虑。

# 9.20.2 解决方案

本节的思想基于一个简单的事实——即，由于 Python 允许对参数进行注解，那么如果可以像下面这样编写代码就好了：

class Spam: def bar(self, x:int, y:int): print('Bar 1:','x,y) def bar(self,s:str，n:int $= 0$ ： print('Bar2：'，s，n) $\mathbf{s} = \mathbf{Spam}()$ s.bar(2,3） #Prints Bar 1:23   
s.bar('hello') #Prints Bar 2: hello0

下面的解决方案正是应对于此，我们使用了元类以及描述符来实现：

```python
# multiple.py
import inspect
import types 
```

```python
class MultiMethod:   
```
Represents a single multimethod.   
```
def __init__(self, name): 
    self._methods = {}
    self._name_ = name  
def register(self, meth): 
    ...
    Register a new method as a multimethod
    ...
    sig = inspectsignature(meth)  
# Build a type signature from the method's annotations  
types = []
for name, parm in sig_parameters.items(): 
    if name == 'self': 
        continue 
    if parm_annotation is inspect.getParameter.empty: 
        raise TypeError(   )
        'Argument {} must be annotated with a type'.format(name) 
    ) 
if not isinstanceParm_annotation, type): 
    raise TypeError(   )
        'Argument {} annotation must be a type'.format(name) 
    ) 
if parm.default is not inspect.getParameter.empty: 
    self._methods[tuple(types)] = meth 
    types.append(parm_annotation) 
self._methods[tuple(types)] = meth  
def __call__(self, *args): 
    ...
Call a method based on type signature of the arguments
    ...
types = tuple(type(arg) for arg in args[1:])
meth = self._methods.get.types, None) 
if meth: 
    return meth(*args) 
else: 
```

```python
def __get__(self, instance, cls):
    Descripior method needed to make calls work in a class
    if instance is not None:
        return types.MethodType(self, instance)
    else:
        return self
class MultiDict dictates:
    Special dictionary to build multimethods in a metaclass
    def __setitem__(self, key, value):
        if key in self:
            # If key already exists, it must be a multimethod or callable
            current_value = self[key]
            if isinstance(current_value, MultiMethod):
                current_value.register(value)
            else:
                mvalue = MultiMethod(key)
                mvalue.register(current_value)
                mvalue.register(value)
                super().__setitem__(key, mvalue)
    else:
        super().__setitem__(key, value)
class MultipleMeta(type):
    Metaclass that allows multiple dispatch of methods
    def __new__(cls, clsname, bases, clsdict):
        return type.__new__(cls, clsname, bases, dict(clsdict))
@metaclass
def __prepare__(cls, clsname, bases):
    return MultiDict() 
```

要使用这个类，可以像这样编写代码：

```python
class Spam(metaclass=MultipleMeta):
    def bar(self, x:int, y:int):
        print('Bar 1:','x,y')
    def bar(self, s:str, n:int = 0):
        print('Bar 2:','s,n) 
```

```python
Example: overloaded __init__  
import time  
class Date(metaclass=MultipleMeta):  
    def __init__(self, year: int, month:int, day:int):  
        self.year = year  
        self.month = month  
        self.day = day  
    def __init__(self):  
        t = time.localtime()  
        self.__init__(t.tm_year, t.tm_mon, t.tm_mday) 
```

下面的交互式会话可验证我们的代码是否能按预期工作：

```txt
>>> s = Spam()
>>> s.bar(2, 3)
Bar 1: 2 3
>>> s.bar('hello')
Bar 2: hello 0
>>> s.bar('hello', 5)
Bar 2: hello 5
>>> s.bar(2, 'hello')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "multiple.py", line 42, in __call__
raise TypeError('No matching method for types {}.format.types))
TypeError: No matching method for types (<class 'int>", <class 'str>) 
```

# 9.20.3 讨论

老实说，本节中出现了大量的“魔法”才使得这个方案能适用于现实环境中的代码。但是，这个方案深入挖掘了元类和描述符的内部工作原理，并强化了其中的一些概念。因此，就算可能不会直接应用本节中的方案，但其中的一些思想可能会影响到其他涉

及元类、描述符和函数注解方面的编程技术。

相对来说，上述实现中的主要思想是比较简单的。元类 MutipleMeta 使用__prepare__()方法来提供一个定制化的类字典，将其作为 MultiDict的一个类实例。与普通的字典不同，当设定字典中的条目时，MultiDict 会检查条目是否已经存在。如果已经存在，则重复的条目会被合并到 MultiMethod 的一个类实例中去。

MultiMethod 的类实例会通过构建一个从类型签名到函数的映射关系来将方法收集到一起。在构建的时候，我们通过函数注解来收集这些签名并构建出映射关系。这些都是在 MultiMethod.register()方法中完成的。关于这个映射，一个至关重要的地方在于为了实现多方法重载，因此必须给所有的参数都指定类型，否则就会出错。

为了让 MultiMethod 的类实例能够表现为一个可调用对象，我们实现了__call__()方法。该方法通过所有的参数（除了 self 之外）构建出一个类型元组，然后在内部的映射关系中找到对应的方法并调用它。实现__get__()方法是为了让 MultiMethod 的类实例能够在类定义中正常工作。在我们给出的实现中，__get__()方法被用来创建合适的绑定方法。示例如下：

```txt
>>> b = s.bar  
>>> b  
<bound method Spam.bar of __main__.Spam object at 0x1006a46d0>  
>>> b._self_  
<_main__.Spam object at 0x1006a46d0>  
>>> b._func_  
<_main__.MultiMethod object at 0x1006a4d50>  
>>> b(2, 3)  
Bar 1: 2 3  
>>> b('hello')  
Bar 2: hello 0  
>>> 
```

诚然，本节中虽然涉及多项编程技术，但不幸的是我们还需要考虑一下其中存在的局限性。第一，这个解决方案中不能使用关键字参数。示例如下：

```txt
>>> s.bar(x=2, y=3)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: __call__() got an unexpected keyword argument 'y'
>>> s.bar(s='hello')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: __call__() got an unexpected keyword argument 's'
>>> 
```

也许可以通过某种方式增加对关键字参数的支持，但这就需要一种完全不同的方法来实现方法映射了。问题的根源在于关键字参数不是以某种特定顺序出现的。当和位置参数混在一起时，我们很快会得到一堆杂乱排列的参数，迫使我们不得不在__call__()方法中以某种方式进行整理。

本节给出的方案对于继承的支持也非常有限。例如，下面的代码是无法工作的：

```python
class A: pass   
class B(A): pass   
class C: pass   
class Spam(metaclass=MultipleMeta): def foo(self, x:A): print('Foo 1:','x) def foo(self, x:C): print('Foo 2:'，x) 
```

失败的原因在于注解 $\mathbf { \boldsymbol { x } } \mathbf { \cdot } \mathbf { \boldsymbol { A } }$ 无法匹配到子类的实例上（比如 B 的实例）。示例如下：

```txt
>>> s = Spam()
>>> a = A()
>>> s.foo(a)
Foo 1: __main__.A object at 0x1006a5310>
>>> c = C()
>>> s.foo(c)
Foo 2: __main__.C object at 0x1007a1910>
>>> b = B()
>>> s.foo(b)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "multiple.py", line 44, in __call__
raise TypeError('No matching method for types {}.format.types))
TypeError: No matching method for types (<class '__main__.B'>,) 
```

除了使用元类和函数注解之外，还可以通过装饰器来实现类似的功能。示例如下：

```python
import types classmultimethod: 
```

```python
def __init__(self, func):  
    self._methods = {}  
    self._name_ = func._name_  
    self._default = func  
def match(self, *types):  
    def register(func):  
        ndeunders = len(func.__ defaults__ ) if func.__ defaults__ else 0  
        for n in range(ndeunders + 1):  
            self._methods[type(len(type) - n] = func  
            return self  
        return register  
def __call__(self, *args):  
    types = tuple(type(arg) for arg in args[1:])  
    meth = self._methods.get.types, None)  
    if meth:  
        return meth(*args)  
    else:  
        return self._default(*args)  
def __get__(self, instance, cls):  
    if instance is not None:  
        return types.MethodType(self, instance)  
    else:  
        return self 
```

要使用装饰器的版本，可以像这样编写代码：

```python
class Spam: @multimethod def bar(self, \*args): #Default method called if no match raise TypeError('No matching method for bar') @bar.match(int,int) def bar(self,x,y): print('Bar 1:'，x，y) @bar.match(str,int) def bar(self,s,n=0): print('Bar 2:'，s，n) 
```

采用装饰器的解决方案和前面的实现方案有着相同的局限性（即，不支持关键字参数，对继承的支持不佳）。

如果不出什么意外，最好还是不要在通用的代码中使用多分派。在一些特殊情况下这或许会是有意义的，比如某个程序需要根据某种形式的模式匹配来分派不同的方法。例如，在 8.21 节中描述过的访问者模式也许可以改写到一个类中，通过某种方式来使用多方法分派。但是除此之外，选择更加简单的方案通常都绝不会是个坏主意（不同的方法使用不同的名称即可）。

考虑通过不同的方式来实现多分派的思想已经在 Python 用户社区中存在多年了。对于这个主题的讨论，请参见 Python 之父 Guido van Rossum 发表的一篇博文“Five-MinuteMultimethods in Python”（http://www.artima.com/weblogs/viewpost.jsp?thread=101605）。

# 9.21 避免出现重复的属性方法

# 9.21.1 问题

我们正在编写一个类，而我们不得不重复定义一些执行了相同任务的属性方法，比如说做类型检查。我们想简化代码，解决代码重复的问题。

# 9.21.2 解决方案

考虑下面这个简单的类，这里的属性都用 property 方法进行了包装：

```python
class Person: def __init__(self, name, age): self.name = name self.age = age   
@property def name(self): return self._name   
@name setter def name(self, value): if not isinstance(value, str): raise TypeError('name must be a string') self._name = value   
@property def age(self): return self._age   
@age setter def age(self, value): if not isinstance(value, int): 
```

```python
raise TypeError('age must be an int') self._age = value 
```

可以看到，我们编写的很多代码仅仅只是强制对属性值做类型断言。每当看到自己的代码变成这个样子时，应该考虑通过各种不同的方法来简化代码。一种可能的方式就是创建一个函数，让它为我们定义这个属性并返回给我们。示例如下：

```python
def typed_property(name, expected_type):
    storage_name = '(' + name
    @property
    def prop(self):
        return getattr(self, storage_name)
    @prop setter
    def prop(self, value):
        if not isinstance(value, expected_type):
            raise TypeError(['{} must be a {}.format(name, expected_type))
            setattr(self, storage_name, value)
        return prop
# Example use
class Person:
    name = typed_property('name', str)
    age = typed_property('age', int)
    def __init__(self, name, age):
        self.name = name
        self.age = age 
```

# 9.21.3 讨论

本节说明了内层函数或者闭包的一个重要特性——即，用它们编写出的代码工作起来很像宏。示例中的函数 typed_property()可能看起来有点怪，但它实际上只是在为我们生成属性代码，并返回产生的属性对象。因此，当在类中使用它时就好像把出现在typed_property()中的代码放置到了类定义中一样。尽管 getter 和 setter 属性方法访问的是局部变量，比如 name、expected_type 和 storage_name，这也没问题——那些值都保存在闭包中了。

如果使用函数 functools.partial()，还可以让本节中的示例变得更加有趣。例如，可以这么做：

```python
from functools import partial  
String = partial(typed_property, expected_type=str)  
Integer = partial(typed_property, expected_type=int) 
```

Example:   
class Person: name $=$ String('name') age $=$ Integer('age') def init_(self, name, age): self.name $=$ name self.age $=$ age

这份代码看起来就很像我们在 8.13 节中展示的一些类型系统描述符的代码了。

# 9.22 以简单的方式定义上下文管理器

# 9.22.1 问题

我们想实现新形式的上下文管理器，然后在 with 语句中使用。

# 9.22.2 解决方案

编写一个新的上下文管理器，其中最直接的一种方式就是使用 contextlib 模块中的@contextmanager 装饰器。在下面的示例中，我们用上下文管理器来计时代码块的执行时间：

import time   
from contextlib import contextmanager   
@contextmanager   
def timethis.label): start $=$ time.time() try: yield finally: end $=$ time.time() print({}：{}'.format.label，end-start)) #Example use   
with timethis('counting'): n $= 1000000$ while $\mathrm{n} > 0$ .. n $= 1$

在 timethis()函数中，所有位于 yield 之前的代码会作为上下文管理器的__enter__()方法来执行。而所有位于 yield 之后的代码会作为__exit__()方法执行。如果有异常产生，则会在 yield 语句中抛出。

下面是一个更加高级的上下文管理器，其中实现了对列表对象的处理：

@contextmanager   
def list_transaction(orig_list): working $=$ list(orig_list) yield working orig_list[:] $=$ working

这里采用的思路就是只有当整个代码块执行结束且没有产生任何异常时，此时对列表做出的修改才会真正生效。下面用一个例子来说明：

```txt
>>> items = [1, 2, 3]  
>>> with list_transaction(items) as working:  
...     working.append(4)  
...     working.append(5)  
...  
>>> items  
[1, 2, 3, 4, 5]  
>>> with list_transaction(items) as working:  
...     working.append(6)  
...     working.append(7)  
...     raise ValueError('oops')  
...  
Traceback (most recent call last):  
    File "<stdin>", line 4, in <module>  
RuntimeError: oops  
>>> items  
[1, 2, 3, 4, 5] 
```

# 9.22.3 讨论

一般来说，要编写一个上下文管理器，需要定义一个带有__enter__()和__exit__()方法的类，就像下面这样：

import time   
class timethis: def__init__(self, label): self.label $=$ label def__enter__(self): self.start $=$ time.time() def_exit_self,exc_ty,exc_val,exc tb): end $=$ time.time() print({}：{}'.format(self.label，end-self.start))

虽然这么做也并非很难，但是比起直接使用 $@$ contextmanager 还是要繁琐许多。

$@$ contextmanager 只适用于编写自给自足型（self-contained）的上下文管理器函数。如果有一些对象（比如文件、网络连接或者锁）需要支持在 with 语句中使用，那么还是需要分别实现__enter__()和__exit__()方法。

# 9.23 执行带有局部副作用的代码

# 9.23.1 问题

我们正在使用 exec()在调用方的作用域下执行一段代码，但是当执行结束后，得到的结果似乎在当前作用域下是不可见的。

# 9.23.2 解决方案

为了更好地理解这个问题，我们做一个小小的实验。首先，我们在全局命名空间下执行一段代码：

```txt
>> a = 13  
>>> exec('b = a + 1')  
>>> print(b)  
14  
>>> 
```

现在，让我们在一个函数内部再次做同样的实验：

```txt
>>> def test():
    ...
        a=13
        exec('b = a + 1')
        ...
        print(b)
...
>>> test()
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "<stdin>", line 4, in test
NameError: global name 'b' is not defined
>>> 
```

可以看到，我们遇到了 NameError 异常，就好像 exec()语句从未实际执行过一样。如果打算将 exec()的执行结果用在稍后的计算中，那么这就成了问题。

要解决这类问题，需要使用 locals()函数在调用 exec()之前获取一个保存了局部变量的字典。紧接着，就可以从本地字典中提取出修改过的值。示例如下：

```rust
>>> def test(): 
```

```txt
a = 13  
loc = locals()  
exec('b = a + 1')  
b = loc['b']  
print(b)  
>>> test()  
14  
>>> 
```

# 9.23.3 讨论

在实践中要正确使用 exec()其实是非常具有技巧性的。事实上，在大多数考虑使用 exec()的情况中，可能存在着更加优雅的解决方案（例如装饰器、闭包、元类等）。

但是如果仍然必须使用 exec()，本节列出了一些正确使用它的原则。默认情况下，exec()是在调用方的局部和全局作用域中执行代码的。然而在函数内部，传递给 exec()的局部作用域是一个字典，而这个字典是实际局部变量的一份拷贝。因此，如果在 exec()中执行的代码对局部变量做出了任何修改，这个修改绝不会反映到实际的局部变量中去。下面我们用另一个例子来演示这个效果：

```txt
>>> def test1():  
... x = 0  
... exec('x += 1')  
... print(x)  
...  
>>> test1()  
0  
>>> 
```

正如解决方案中展示的那样，当调用 locals()来获取局部变量时，传递给 exec()的是局部变量的拷贝。而在 exec()执行完毕之后，通过检查字典中的值，就能获取到修改过的变量值。下面的实验可验证这一点：

```python
>>> def test2():
...
...
...     x = 0
...
...     loc = locals()
...
...
...     print('before: ', loc)
...
...     exec('x += 1')
...
...     print('after: ', loc)
...
...     print('x=', x)
...
...
>>> test2()
before: {'x': 0}
after: {'loc': {...}, 'x': 1} 
```

$\mathrm{x} = 0$ $\ggg$

仔细观察最后一步的输出。除非从 loc 中将修改过的值写回 $\mathbf { X }$ ，否则变量 $\mathbf { X }$ 会保持不变。

每当使用 locals()时都需要小心操作的顺序问题。每次调用它时，locals()将会接受局部变量的当前值，然后覆盖字典中的对应条目。观察下面这个实验的结果：

```txt
>>> def test3(   ):  
... x = 0  
... loc = locals()  
... print (loc)  
... exec('x += 1')  
... print (loc)  
... locals()  
... print (loc)  
...  
>>> test3(   )  
{'x': 0}  
{'loc': ..., 'x': 1}  
{'loc': ..., 'x': 0}  
>>> 
```

注意最后对 locals()的调用是如何导致 x 被覆盖的。

除了使用 locals()之外，另一种可选的方式是自己创建字典并传递给 exec()。示例如下：

```txt
>>> def test4(   ):  
... a = 13  
... loc = \{ 'a': a \}  
... glb = \{ \}  
... exec('b = a + 1', glb, loc)  
... b = loc['b']  
... print (b)  
>>> test4(   )  
14 
```

对于大部分针对 exec()的应用，这可能就是优秀的实践方式了。我们需要确保 exec()中访问的变量在全局和局部字典中经过恰当的初始化。

最后但同样重要的是，在使用 exec()之前，应该问问自己是否还有其他可选的方案。许多可能会考虑使用 exec()的问题都可以用闭包、装饰器、元类或者其他元编程的特性来替代。

# 9.24 解析并分析 Python 源代码

# 9.24.1 问题

我们想编写程序来解析 Python 源代码并对此进行一些分析工作。

# 9.24.2 解决方案

大部分程序员都知道 Python 可以执行以字符串形式提供的源代码。示例如下：

```txt
>>>x=42   
>>>eval('2+3\*4+x')   
56   
>> exec('for i in range(10):print(i)')   
0   
1   
2   
3   
4   
5   
6   
7   
8   
9   
>>> 
```

但是，我们可以使用 ast 模块将 Python 源代码编译为一个抽象语法树（AST），这样就可以分析源代码了。示例如下：

```python
>>> import ast
>>> ex = ast.parse('2 + 3*4 + x', mode='eval')
>>> ex
<_astExpression object at 0x1007473d0>
>>> ast.dump(ex)
"Expression(body=BinOp(left=BinOp(leftNum(n=2), op=Add(), right=BinOp(left=Num(n=3), op=Mult(), right=Num(n=4))), op=Add(), right=Name(id='x', ctx=Load())"
>>> top = ast.parse('for i in range(10): print(i)', mode='exec')
>>> top
<_astModule object at 0x100747390>
>>> ast.dump(top)
"Module(body=[For(target=Name(id='i', ctx=Store()), 
```

```python
iter=Call(func=Name(id='range', ctx=Load(), args=[Num(n=10)], keywords=[[], starargs=None, kwargs=None), body=[Expr(value=Call(func=Name(id='print', ctx=Load()), args=[Name(id='i', ctx=Load()), keywords=[[], starargs=None, kwargs=None)), otherwise]") 
```

对语法树的分析需要读者自己做一些研究，但总的来说，语法树是由一些 AST 节点组成的。同这些节点打交道的最简单的方式就是定义一个访问者类，在类中实现各种visit_NodeName()方法，这里的NodeName 可以匹配到所感兴趣的节点上来。下面的示例给出了这样一个类，它可以记录那些被加载、保存和删除过的名称信息。

import ast   
class CodeAnalyzer(ast.NodeVisitor): def__init__(self): self.load $\equiv$ set() selfstored $=$ set() selfdeleted $=$ set() def visit_Name(self,node): ifisinstance(node.ctx,ast.Load): self.load.add(node.id) elif isinstance(node.ctx,ast.Storage): selfstored.add(node.id) elif isinstance(node.ctx,ast.Del): selfdeleted.add(node.id) #Sample usage if_name $= =$ 'main': # Some Python code code $= 111$ for i in range(10): print(i) del i ['# Parse into an AST top $=$ ast.parse(code，mode $\coloneqq$ 'exec') #Feed the AST to analyze name usage c $=$ CodeAnalyzer() c.visit(top) print('Loaded:','c Loaded) print('Stored:','c.stored) print('Deleted:','cdeleted)

如果运行这个程序，会得到如下的输出：

```javascript
Loaded:{'i'，'range'，'print'}   
Stored:{'i'}   
Deleted:{'i'} 
```

最后，AST 节点可以通过 compile()函数进行编译并执行。示例如下：

```diff
>>> exec.compile(top, "<stdin>", 'exec'))
0
1
2
3
4
5
6
7
8
9
>>> 
```

# 9.24.3 讨论

由于可以分析源代码并从中得到有用的信息，这一事实可以让我们开始编写各种各样的代码分析、代码优化或者验证工具。例如，与其盲目地将一些代码片段传递给 exec()这样的函数，不如先将代码转换为一棵 AST 树，然后检查其中的一些细节来看看代码要完成哪些任务。也可以编写工具来检查模块的整份源码，并在此之上进行一些静态分析。

应该要提到的是，如果确实知道自己要做什么，那么也可以重写 AST 来表示新的源码。下面给出了一个装饰器的示例，可以降低函数体中可被全局访问的名称的数量。这是通过重新解析函数体的源码并重写 AST，然后再重新创建函数的源码对象来实现的。

# namelower.py   
import ast   
import inspect   
# Node visitor that lowers globally accessed names into   
# the function body as local variables.   
class NameLower(ast.NodeVisitor): def __init__(self, lowered_names): selfelowed_names $=$ lowered_names def visit_FunctionDef(self,node): # Compile some assignments to lower the constants code $= \mathrm{^{\prime}{}_{-}}$ globals $=$ globals()\\n'

code $+=$ '\n'.join(\{0 \} $=$ __globals[\{'0\}]".format(name) for name in self.lowered_names)   
code_ast $=$ ast.parse(code,mode $\equiv$ 'exec') #Inject new statements into the function body node.body[:0] $=$ code_ast.body # Save the function object   
selfFUNC $=$ node   
# Decorator that turns global names into locals   
def lower_names(\*namelist): def lower FUNC: srclines $=$ inspect.getsource FUNC).splitlines() #Skip source lines prior to the @lower_names decorator for n,line in enumerate(srclines): if '@lower_names' in line: break src $=$ '\\n'.join(srclines[n+1:]) # Hack to deal with indented code if src.startswith(('\,',\t'))： src $=$ 'if 1:\\n' + src top $=$ ast.parse(src,mode $\equiv$ 'exec') #Transform the AST cl $=$ NameLower(namelist) cl.visit(top) # Execute the modified AST temp $=$ {} exec.compile(top,'','exec'),temp, temp) # Pull out the modified code object func._code_ $=$ temp[func._name_.]._code_ return func return lower

要使用上述代码，可以编写如下形式的代码：

```python
INCR = 1
@lower_names('INCR')
def countdown(n):
    while n > 0:
        n -= INCR 
```

这样，我们的装饰器就会将函数 countdown()的源码改写为如下的形式：

```python
def countdown(n):
    __globals = globals()
    INCR = __globals['INCR']
    while n > 0:
        n -= INCR 
```

在性能测试中，这使得该函数的运行速度快了大约 $20 \%$ 。

现在，是否应该将这个装饰器作用到所有的函数上呢？很可能不是如此。但是，这个例子对一些非常高级的技术做了很好的说明。我们可以通过操纵 AST、源代码以及其他一些技术来实现这些高级特性。

本节的灵感源自 ActiveState 上一个类似的例子（http://code.activestate.com/recipes/277940-decorator-for-bindingconstants-at-compile-time/），在那个例子中我们操纵了 Python 的字节码。用 AST 来实现则是一种层次更高的方法，可能会更直接一些。有关字节码方面的更多内容可参考下一节。

# 9.25 将 Python 源码分解为字节码

# 9.25.1 问题

我们想将 Python 源码分解为解释器所使用的底层字节码，以此了解代码在底层的详细细节。

# 9.25.2 解决方案

dis 模块可用来将任何 Python 函数分解为字节码序列。示例如下：

```python
>>> def countdown(n):
...     while n > 0:
...         print('T-minus', n)
...         n -= 1
...         print('Blastoff!')
...
>>> import dis
>>> dis.dis(countdown)
2      0 SETUP_LOOP          39 (to 42)
>>     3 LOAD_FAST           0 (n)
6 LOADCONST            1 (0)
9COMPARE_OP              4 (>)
12POP_JUMP_IFFalse       41 
```

```txt
3 15 LOAD.Global0 (print) 18LOADCONST2(T-minus') 21LOAD_FAST0(n) 24CALL_FUNCTION 2(2 positional,0 keyword pair) 27POP_TOP   
4 28LOAD_FAST0(n) 31LOADCONST3(1) 34INPLACE_SUBTRACT 35STORE_FAST0(n) 38JUMP_ABSOLUTE 3   
>>41POP_BLOCK   
5 >>42LOAD.Global0 (print) 45LOADCONST4('Blastoff!') 48CALL_FUNCTION1(1 positional,0 keyword pair) 51POPTOP0(None) 52LOADCONST55RETURN_VALUE   
>>> 
```

# 9.25.3 讨论

如果需要在非常底层的层次下研究程序的行为，那么 dis 模块会非常有帮助（例如，如果打算了解一些性能方面的特点时）。

由函数 dis()所翻译出的原始字节码序列是这样的：

```txt
>>> countdown.__code__ . co_code
b"\"x00| \\x00\\x00d\\x01\\x00k\\x04\\x00r) \\x00t\\x00\\x00d\\x02\\x00| \\x00\\x00\\x83
\backslash x02\\x00\\x01| \\x00\\x00d\\x03\\x008} \\x00\\x00q\\x03\\x00Wt\\x00\\x00d\\x04\\x00\\x83
\backslash x01\\x00\\x01d\\x00\\x00S"
>>> 
```

如果想自行解释这段代码，需要使用定义在 opcode模块中的一些常量。示例如下：

```snap
>>> c = countdown._code_.co_code  
>>> import opcode  
>>> opcode.opname[c[0]]  
>>> opcode.opname[c[0]]  
'SETUP_LOOP'  
>>> opcode.opname[c[3]]  
'LOAD_FAST'  
>>> 
```

讽刺的是，dis 模块中居然没有任何函数能够让我们以可编程的方式来轻松处理这些字

节码。下面这个生成器函数会接受原始的字节码序列，并将其转换为对应的操作代码和参数。

import opcode   
def generate_opcodes(codebytes): extended_arg $= 0$ $\mathrm{i} = 0$ n $=$ len(codebytes) while i $<  \mathbf{n}$ . op $=$ codebytes[i] $\mathrm{i} + = 1$ if op $> =$ opcode.HAVE.ArgUMENT: oparg $=$ codebytes[i] $^+$ codebytes[i+1]\*256 $^+$ extended_arg extended_arg $= 0$ $\mathrm{i} + = 2$ if op $= =$ opcode.EXTENDED.Arg: extended_arg $=$ oparg\*65536 continue else: oparg $=$ None yield (op, oparg)

要使用这个函数，可以像这样操作：

```txt
>>> for op, oparg in generate_opcodes(countdown._code_.co_code):
... print(op, opcode.opname[op], oparg)
...
120 SETUP_LOOP 39
124 LOAD_FAST 0
100LOADCONST1
107COMPARE_OP4
114POP_JUMP_IFFalse41
116LOAD.Global0
100LOADCONST2
124LOAD_FAST0
131CALL_FUNCTION2
1POP_TOP None
124LOAD_FAST0
100LOADCONST3
56INPLACE_SUBTRACTNone
125STORE_FAST0
113JUMP_ABSOLUTE3
87POP_BLOCKNone
116LOAD.Global0
100LOADCONST4 
```

```txt
131CALL_FUNCTION1 1POP_TOP None 100LOADConst0 83RETURN_VALUENone >>> 
```

下面介绍一个鲜为人知的小技巧，我们可以将任何函数中感兴趣的原始字节码替换掉。这需要多做一些工作才能实现，下面给出的示例展示了其中需要涉及的技巧：

```python
>>> def add(x, y):
    return x + y
...
>>> c = add._code_
>>> c
<code object add at 0x1007beed0, file "<stdin>", line 1>
>>> c.co_code
b'|\x00\x00|\x01\x00\x17S'
>>> # Make a completely new code object with bogus byte code
>>> import types
>>> newbytecode = b'xxxxxx'
>>> nc = types.CodeType(c.co_argcount, c.co_kwonlyargcount,
.... c.co_nlocals, c.co_stacksize, c.co_flags, newbytecode, c.co_consts,
.... c.co_names, c.co_varnames, c.cofilename, c.co_name,
.... c.co_firstlineno, c.co_lnotab)
>>> nc
<code object add at 0x10069fe40, file "<stdin>", line 1>
>>> add._code_ = nc
>>> add(2,3)
Segmentation fault 
```

使用这么花哨的技巧是很有可能将解释器弄崩溃的。但是，对于那些需要做高级优化和开发元编程工具的开发者来说，他们可能会真地倾向于去重新改写字节码。本节最后这个例子展示了如何去做。读者可以在 ActiveState 上看到另一个实际应用中的例子（http://code.activestate.com/recipes/277940-decorator-for-buildingconstants-at-cimpile- time）。

# 模块和包

模块和包是任何大型项目的核心，就连 Python 安装程序本身也是一个包。本章的重点涉及有关模块和包的常见编程技术，例如如何组织包、将大型的模块分解成多个文件以及创建命名空间包（namespace package）。此外，本章也提到了关于自定义 import 语句行为的操作。

# 10.1 把模块按层次结构组织成包

# 10.1.1 问题

我们想把代码按照一定的层次结构组织成包。

# 10.1.2 解决方案

创建一个软件包结构是很简单的。只要把代码按照所希望的方式在文件系统上进行组织，并确保每个目录中都定义了一个 init .py 文件即可。例如：

```txt
graphics/  
__init__.py  
primitive/  
__init__.py  
line.py  
fill.py  
text.py  
formats/  
__init__.py  
png.py  
jpg.py 
```

一旦完成后，就可以执行各种各样的 import 语句了，比如：

```python
import graphics.primitive.line  
from graphics.primitive import line  
import graphics.formats.jpg as jpg 
```

# 10.1.3 讨论

定义一个具有层次结构的模块就如同在文件系统上创建目录结构一样简单。 init .py文件的目的就是包含可选的初始化代码，当遇到软件包中不同层次的模块时会触发运行。比如，如果写下 import graphics 语句，文件 graphics/ init .py 会被导入并形成graphics 命名空间中的内容。对于 import graphics.formats.jpg 这样的导入语句，文件 graphic/init .py 和 graphics/formats/ init .py 都会在最终导入文件 graphics/formats/jpg.py 之前优先得到导入。

在大部分情况下，把 init .py 文件留空也是可以的。但是，在某些特定的情况下_init_.-py文件中是需要包含代码的。例如，可以用_init_.-py文件来自动加载子模块，示例如下：

```txt
graphics/forms/_init_.py 
```

```python
from . import jpg  
from . import png 
```

有了这样一个文件，用户只需要使用一条单独的 import graphics.formats 语句就可以导入 jpg和 png 模块了，不需要再去分别导入 graphics.formats.jpg 和 graphics.formats.png。

其他关于 init .py 文件的常见用法包括从多个文件中把定义统一到一个单独的逻辑命名空间中，这有时候会在分解模块时用到。我们在 10.4 节中会讨论分解模块的问题。

一些精明的程序员会注意到在Python 3.3中就算不存在 init .py文件似乎也可以执行包的导入操作。如果不定义 init .py，那么实际上是创建了一个称之为“命名空间包”（namespace package）的东西，我们会在 10.5节讨论这个主题。如果刚开始创建一个新的包，那么做法都是一样的，包括 init .py 文件也是一样。

# 10.2 对所有符号的导入进行精确控制

# 10.2.1 问题

当用户使用 from module import *语句时，我们希望对从模块或包中导入的符号进行精确控制。

# 10.2.2 解决方案

在模块中定义一个变量__all__，用来显式列出可导出的符号名。示例如下：

somemodule.py   
def spam(): pass   
def grok(): pass $\mathrm{blah} = 42$ #Onlyexport'spam'and'grok'_all $=$ ['spam', 'grok']

# 10.2.3 讨论

尽管我们强烈反对使用 from module import *这样的导入语句，但是在定义了大量符号的模块中还是能常看到这种用法。如果对此无动于衷的话，这种形式的导入会把所有不以下划线开头的符号名全部导出。换句话说，如果定义了__all__，那么只有显式列出的符号名才会被导出。

如果将__all__定义成一个空的列表，那么任何符号都不会被导出。如果__all__中包含有未定义的名称，那么在执行 import 语句时会产生一个 AttributeError 异常。

# 10.3 用相对名称来导入包中的子模块

# 10.3.1 问题

我们将代码组织成了一个包，想从其中一个子模块中导入另一个子模块，但是又不希望在 import 语句中硬编码包的名称。

# 10.3.2 解决方案

要在软件包的子模块中导入同一个包中其他的子模块，请使用相对名称来导入。例如，假设有一个名为 mypackage 的包，它在文件系统上组织成如下的形式：

```txt
mypackage/ init.py A/ init.py spam.py 
```

```txt
grok.py   
B/ init.py bar.py 
```

如果模块 mypackage.A.spam 希望导入位于同一个目录中的模块 grok，那么它应该包含一条这样的 import 语句：

```txt
mypackage/A/spam.py 
```

```python
from . import grok 
```

如果模块 mypackage.A.spam 希望导入位于不同目录中的模块 B.bar，可以使用下面的import 语句来完成：

```txt
mypackage/A/spam.py 
```

```python
from ..B import bar 
```

上面这两条 import 语句都是相对于 spam.py 文件的位置来进行操作的，而且其中没有包含最顶层包的名称。

# 10.3.3 讨论

在包的内部，要在其中一个子模块中导入同一个包中其他的子模块，既可以通过给出完整的绝对名称，也可以通过上面示例中采用的相对名称来完成导入。示例如下：

```txt
mypackage/A/spam.py 
```

```python
from mypackage.A import grok # OK  
from . import grok # OK  
import grok # Error (not found) 
```

使用绝对名称的缺点在于这么做会将最顶层的包名称硬编码到源代码中，这使得代码更加脆弱，如果想重新组织一下结构会比较困难。例如，如果修改了包的名称，将不得不搜索所有的源代码文件并修改硬编码的名称。类似地，硬编码名称使得其他人很难移动这部分代码。例如，也许有人想安装两个不同版本的包，只通过名字来区分它们。如果采用相对名称导入，那么不会有任何问题，但是采用绝对名称导入则会使程序崩溃。

import语句中的.和..语法可能看起来比较有趣，把它们想象成指定目录名即可。.意味着在当前目录中查找，而..B 表示在../B 目录中查找。这种语法只能用在 from xx import yy这样的导入语句中。示例如下：

```python
from . import grok # OK  
import . grok # ERROR 
```

尽管看起来似乎可以利用相对导入来访问整个文件系统，但实际上是不允许跳出定义包的那个目录的。也就是说，利用句点的组合形式进入一个不是 Python 包的目录会使得导入出现错误。

最后，应该要提到的是相对导入只在特定的条件下才起作用，即，模块必须位于一个合适的包中才可以。特别是，位于脚本顶层目录的模块不能使用相对导入。此外，如果包的某个部分是直接以脚本的形式执行的，这种情况下也不能使用相对导入。例如：

```batch
% python3 mypackage/A/spam.py # Relative imports fail 
```

另一方面，如果使用-m 选项来执行上面的脚本，那么相对导入就可以正常工作了。示例如下：

```batch
% python3 -m mypackage.A.spam # Relative imports work 
```

有关包的相对导入的更多背景知识，请参阅 PEP 328（http://www.python.org/dev/peps/pep-0328）。

# 10.4 将模块分解成多个文件

# 10.4.1 问题

我们想将一个模块分解成多个文件。但是，我们不想破坏现在已经在使用这个模块的代码，而是希望可以将多个单独的文件在逻辑上统一成一个单独的模块。

# 10.4.2 解决方案

可以通过将模块转换为包的方式将模块分解成多个单独的文件。考虑下面这个简单的模块：

```python
mymodule.py   
class A: def spam(self): print('A.spam')   
class B(A): def bar(self): print('B.bar') 
```

假设想将 mymodule.py 分解为两个文件，每个文件中包含一个类的定义。要做到这点，可以从把 mymodule.py 替换成目录 mymodule 开始。在这个新的目录中创建如下的文件：

```txt
mymodule/ init.py a.py b.py 
```

在文件 a.py 中填入下面的代码：

```python
a.py   
class A: def spam(self): print('A.spam') 
```

而在文件 b.py 中填入下面的代码：

```python
b.py   
from .a import A   
class B(A): def bar(self): print('B.bar') 
```

最后在文件 init .py 中将这两个文件绑定在一起：

```python
__init.py from .a import A from .b import B
```

如果遵循以上的步骤，那么现在 mypackage 包在逻辑上就成为了一个单独的模块：

```txt
>>> import mymodule
>>> a = mymodule.A()
>>> a.spam()
A.spam
>>> b = mymodule.B()
>>> b.bar()
B.bar
>>> 
```

# 10.4.3 讨论

本节主要考虑的是一个设计上的问题。即，我们希望用户使用大量的小型模块，还是希望他们只使用一个单独的模块。例如，在一个庞大的代码库中，我们可以把所有的东西都分解成单独的文件，并让用户写下大量的 import语句，就像下面这样：

```python
from mymodule.a import A  
from mymodule.b import B 
```

这么做行得通，但是也给用户带来了很大的负担，因为他们需要知道不同的组件都存放在哪个文件中。通常，更加简单的方式是将事情统一起来，只用一条单独的 import语句即可：

```python
from mymodule import A, B 
```

对于这后一种情况，通常可以把 mymodule 想象成一个大型的源文件。但是，本节为大家演示了如何在逻辑上把多个文件拼接成一个单独的命名空间的技术。关键之处在于创建一个包目录，并通过 init .py 文件将各个部分粘合在一起。

当分解模块时，需要对跨文件名的引用多加小心。例如，在本节示例中，class B 需要把 class A当做基类来访问。我们采用from .a import A 这种相对于包的导入方式来获取class A 的定义。

本节全篇都在使用相对于包的导入方式，避免在源代码中硬编码顶层模块名。这么做使得修改模块名或者将模块代码移动到别处都变得更加容易了（参见 10.3 节）。

可以对本节提到的技术进行扩展，引入“惰性”导入的概念。由前面的示例可知，init.py文件一次性将所有需要的组件都导人进来。但是，对于非常庞大的模块，也许只希望在实际需要的时候才加载那些组件。为了实现这个目的，下面对__init__.py文件做了些微修改：

```python
# __init__.py
def A():
    from .a import A
    return A()
def B():
    from .b import B
    return B()
```

在这个版本中，class A 和 class B 已经由函数取代了，当首次访问它们时会加载所需的类。对于用户来说这不会有太大差别。示例如下：

```txt
>>> import mymodule
>>> a = mymodule.A()
>>> a.spam()
A.spam
>>> 
```

惰性加载的主要缺点在于会破坏继承和类型检查机制。例如，我们可能需要稍微修改一下代码：

```python
if isinstance(x, mymodule.A): # Error
...
if isinstance(x, mymodule.a.A): # Ok
... 
```

有关惰性加载在真实世界中的应用，可以参考标准库中 multiprocessing/ init .py 中的源代码。

# 10.5 让各个目录下的代码在统一的命名空间下导入

# 10.5.1 问题

我们有一个庞大的代码库，其中有很多部分可能是由不同的人来维护和发布的。每个部分都组织成一个目录，就像包一样。但是，与其把每个部分都安装为单独命名的包，我们更想把所有的部分联合在一起，用一个统一的前缀来命名。

# 10.5.2 解决方案

基本上来说，这里的问题就是我们想定义一个顶层的 Python 包，把它作为命名空间来管理大量单独维护的子模块。这个问题常常会在大型的应用程序框架中出现，框架开发人员希望鼓励用户发布自己的插件或者附加的包。

要使各个单独的目录统一在一个公共的命名空间下，可以把代码像普通的 Python 包那样进行组织。但是对于打算合并在一起的组件，这些目录中的 init .py 文件则需要忽略。为了说明这个过程，假设 Python 代码位于两个不同的目录中：

```txt
foo-package/ spam/ blah.py   
bar-package/ spam/ grok.py 
```

在这两个目录中，spam 用来作为公共的命名空间。注意到这两个目录中都没有出现_init.-py文件。

现在如果将 foo-package 和 bar-package 都添加到 Python 的模块查询路径中，然后尝试

做一些导入操作，看看会发生什么：

```txt
>>> import sys
>>> sys.path extend(['foo-package', 'bar-package'])  
>>> import spamblah  
>>> import spam.grok  
>>> 
```

将会注意到这两个不同的包目录魔法般地合并在了一起，我们可以随意导入 spam.blah或者 spam.grok，不会遇到任何问题。

# 10.5.3 讨论

这里的工作原理用到了一种称之为“命名空间包”（namespace package）的特性。基本上来说，命名空间包是一种特殊的包，设计这种特性的意图就是用来合并不同目录下的代码，把它们放在统一的命名空间之下进行管理，就像示例中展示的那样。对于大型的框架而言，这种特性是很有帮助的。因为这样允许把框架的某些部分分解成单独安装的包。这样也使得人们可以轻松地制作第三方插件和针对框架的其他扩展。

创建命名空间包的关键之处在于确保在统一命名空间的顶层目录中不包含 init .py文件。当导入包的时候，这个缺失的 init .py 文件会导致发生一些有趣的事情。解释器并不会因此而产生一个错误，相反，解释器开始创建一个列表，把所有恰好包含有这个包名的目录都囊括在内。此时就创建出了一个特殊的命名空间包模块，且在_path__变量中会保存一份只读形式的目录列表。示例如下：

```python
>>> import spam
>>> spam._path_
_NamespacePath(['foo-package/spam', 'bar-package/spam'])
```

__path__变量中保存的目录可用来进一步定位包中的子模块（例如，当导入 spam.grok或者 spam.blah 时）。

命名空间包的一个重要特性就是任何人都可以用自己的代码来扩展命名空间中的内容。例如，假设在自己的目录下添加了代码：

```txt
my-package/ spam/ custom.py 
```

如果把自己的代码目录和其他的包一起添加到 sys.path 中，那么就可以无缝地同其他的spam 包合并在一起：

```txt
>>> import spam/custom  
>>> import spam.grok 
```

```txt
>>> import spamblah
>>> 
```

作为一种调试工具，想知道某个包是不是用来当做命名空间包的主要方式就是检查它的__file__属性。如果缺少这个属性，这个包就是命名空间。这也可以从包对象的字符串表示中看出来，如果是命名空间的话，其中会有“namespace”的字样。示例如下：

```txt
>>> spam._file_
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
AttributeError: 'module' object has no attribute '.file'
>>> spam
<module 'spam' (namespace)>
>>> 
```

有关命名空间包的更多信息，可以在 PEP 420（http://www.python.org/dev/peps/pep-0420）中找到。

# 10.6 重新加载模块

# 10.6.1 问题

因为对模块的源代码做了修改，我们想重新加载一个已经加载过了的模块。

# 10.6.2 解决方案

要重新加载一个之前已加载过的模块，可以使用 imp.reload()来实现。示例如下：

```txt
>>> import spam
>>> import imp
>>> imp reload(spam)
<module 'spam' from './spam.py>
>>> 
```

# 10.6.3 讨论

在开发和调试阶段，重新加载模块这一招常常很有用。但是一般来说在生产环境中这么做是不安全的，因为它并不会总是按照期望的方式工作。

reload()操作会擦除模块底层字典（__dict__）的内容，并通过重新执行模块的源代码来刷新它。模块对象本身的标识并不会改变（即，调用 id()的结果）。因此，这个操作会使得已经导入到程序中的模块得到更新。

但是，对于使用了 from module import name 这样的语句导入的定义，reload()是不会去更新的。为了说明其中的过程，考虑下面的代码：

```python
# spam.py
def bar():
    print('bar')
def grok():
    print('grok') 
```

现在开启一个新的交互式会话：

```txt
>>> import spam
>>> from spam import grok
>>> spam.bar()
bar
>>> grok()
grok
>>> 
```

不要退出 Python，现在去编辑 spam.py 的源代码，把 grok()函数修改成下面这样：

```python
def grok():
    print('New grok') 
```

现在返回交互式会话执行 reload()操作，并做以下的试验：

```txt
>>> import imp
>>> imp reload(spam)
<module 'spam' from './spam.py>
>>> spam.bar()
bar
>>> grok()
# Notice old output
grok
>>> spam.grok()
# Notice new output
New grok
>>> 
```

在这个例子中，将会发现有两个版本的 grok()函数都被加载进来了。一般来说这不会是我们期望的结果，而且最终会变成让我们头疼的噩梦。

基于这个原因，在生产环境的代码中应该要避免重新加载模块。但是在调试或者在交互式会话中，当需要尝试一些新想法时这么做也未尝不可。

# 10.7 让目录或 zip 文件成为可运行的脚本

# 10.7.1 问题

我们的程序已经从一个简单的脚本进化为一个涉及多个文件的应用。我们希望能有某

种简单的方法让用户来运行这个程序。

# 10.7.2 解决方案

如果应用程序已经进化为由多个文件组成的“庞然大物”，则可以把它们放在专属的目录中，并为之添加一个 main .py 文件。例如，可以创建一个这样的目录：

```txt
myapplication/ spam.py bar.py grok.py __main__.py 
```

如果有 main .py，就可以在顶层目录中运行 Python 解释器，就像下面这样：

```txt
bash % python3 myapplication 
```

解释器会把 main .py 文件作为主程序来执行。

这项技术在我们把所有的代码打包进一个 zip 文件中时同样有效。示例如下：

```batch
bash % ls  
spam.py bar.py grok.py __main__.py  
bash % zip -r myapp.zip *.py  
bash % python3 myapp.zip  
... output from __main__.py ... 
```

# 10.7.3 讨论

创建一个目录或 zip 文件，并在其中添加一个 main .py，这是一种打包规模较大的Python 应用程序的可行方法。但这和安装到 Python标准库中的包有所不同，在这种情况下，代码并不是作为标准库中的模块来使用的。相反，这里只是把代码打包起来方便给其他人执行。

由于目录和 zip 文件同普通文件相比有一些小的区别，我们可能也想添加一个 shell 脚本来让执行步骤变得更加简单。例如，如果代码位于一个名为 myapp.zip 的文件中，则可以像下面这样创建一个顶层的脚本：

```txt
!/usr/bin/env python3 /usr/local/bin/myapp.zip 
```

# 10.8 读取包中的数据文件

# 10.8.1 问题

我们的代码需要读取包中的一个数据文件，我们要尽可能的以可移植的方式来处理。

# 10.8.2 解决方案

假设包是按照下列方式组织的：

```batch
mypackage/ __init__.py somedata.dat spam.py 
```

现在假设文件 spam.py 要读取 somedata.dat 中的内容。要做到这点，可以使用下列代码来完成：

```python
# spam.py
import pkgutil
data = pkgutil.get_data(_package__, 'somedata.dat') 
```

得到的结果会保存在变量 data 中。这是一个字节串，其中包含了文件的原始内容。

# 10.8.3 讨论

要读取一个数据文件，我们可能会倾向于编写代码利用内建的 I/O 函数（比如 open()）来完成。但是，这种方法存在几个问题。

首先，对于一个包来说，它无法控制解释器的当前工作目录。因此，任何I/O操作都必须使用文件名的绝对路径。由于每个模块都在__file__变量中保存了全路径，所以要获取文件的位置并非不可能，但是会很麻烦。

其次，包通常都会安装为.zip 或者.egg 文件，它们和文件系统中普通的目录保存文件的方式不同。因此如果尝试用 open()打开包含在归档（archive）中的数据文件，这根本行不通。

pkgutil.get_data()函数是一种高级的工具，无论包以什么样的形式安装或安装到了哪里，都能够用它来获取数据文件。它能够完成工作并把文件内容以字节串的形式返回给我们。

get_data()的第一个参数是包含有包名的字符串。我们可以直接提供这个字符串，或者使用__package__这个特殊变量。第二个参数是要获取的文件相对于包的名称。如果有必要，可以使用标准的 UNIX 路径名规则进入不同的目录中，只要最后的目录仍然在包的内部即可。

# 10.9 添加目录到 sys.path 中

# 10.9.1 问题

我们有一些 Python 代码无法导入，因为它们不在 sys.path 列出的目录中。我们想将新

的目录添加到 Python 的路径里，但又不想将其硬编码到代码中。

# 10.9.2 解决方案

有两种常见的方法可以将新的目录添加到 sys.path 中去。第一，可以通过使用PYTHONPATH 环境变量来添加。示例如下：

```txt
bash % env PythonPATH=/some/dir:/other/dir python3  
Python 3.3.0 (default, Oct 4 2012, 10:17:33)  
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin  
Type "help", "copyright", "credits" or "license" for more information.  
>>> import sys  
>>> sys.path  
['', '/some/dir', '/other/dir', ...]  
>>> 
```

在一个用户程序中，这个环境变量可以在程序启动时或者通过某种形式的 shell 脚本来设定。

第二种方法是创建一个.pth 文件，然后像下面这样将目录列出来：

```txt
myapplication.pth /some/dir /other/dir 
```

这个.pth 文件需要放在 Python 的其中一个 site-packages 目录中，一般来说位于/usr/local/lib/python3.3/site-packages 或者~/.local/lib/python3.3/site-packages。在解释器启动的时候，只要.pth 文件中列出的目录存在于文件系统上，那么它们就会被添加到sys.path 中。如果是要添加到整个系统级的 Python 解释器上，那么安装.pth 文件可能需要管理员权限。

# 10.9.3 讨论

如果在确定文件的位置时遇到了麻烦，可能会倾向于编写代码来手动调整 sys.path 的值。示例如下：

```python
import sys
sys.path.insert(0, '/some/dir')
sys.path.insert(0, '/other/dir') 
```

尽管这可以“工作”，但在实践中这种做法极度脆弱，应该尽可能避免这种做法。这种方法的部分问题在于将目录名称硬编码到了源码中。如果要将代码转移到一个新的位置时，这就会产生维护方面的问题了。通常更好的方法是在其他的地方对路径做配置，不用去直接编辑代码。

有时候，如果利用模块级的变量比如__file__来精心构建一个合适的绝对路径，也能够规避硬编码目录所带来的问题。示例如下：

```python
import sys  
from os.path import abspath, join, dirname  
sys.path.insert(0, abspath(dirname['__file__'], 'src')) 
```

上面的代码将 src 目录添加到了 sys.path 中，而且 src 目录和执行插入操作的代码所在的目录是相同的。

目录 site-packages 通常是第三方模块和包安装的位置。如果代码也按照这种方式安装，那么这就是它们所处的位置。尽管用来配置路径的.pth 文件必须出现在 site-packages 中，但其中记录的路径可以指向系统中任何希望的目录。因此，可以选择让代码保存在一个完全不同的目录中，只要这些目录都包含在.pth 文件中即可。

# 10.10 使用字符串中给定的名称来导入模块

# 10.10.1 问题

我们已经有了需要导入的模块名称，但是这个名称保存在一个字符串中。我们想在字符串上执行 import 命令。

# 10.10.2 解决方案

当模块或包的名称以字符串的形式给出时，可以使用 importlib.import_module()函数来手动导入这个模块。示例如下：

```txt
>>> import importlib
>>> math = importlib importedModule('math')
>>> math.sin(2)
0.9092974268256817
>>> mod = importlib importedModule('urllib.request')
>>> u = mod(urlopen('http://www.python.org'))
>>> 
```

import_module 基本上和 import 完成的步骤相同，但是 import_module 会把模块对象作为结果返回给你。我们只需要将它保存在一个变量里，之后把它当做普通的模块使用即可。

如果要同包打交道，import_module()也可以用来实现相对导入。但是，需要提供一个额外的参数。示例如下：

```python
import importlib 
```

```python
Same as 'from . import b'  
b = importlib import_module('.b', __package__) 
```

# 10.10.3 讨论

采用 import_module()手动导入模块的需求最常出现在当编写代码以某种方式来操作或包装模块时。例如，也许正在实现一个自定义的导入机制，需要通过模块的名称来完成加载并给加载进来的代码打上补丁。

在较老的代码中，有时候会看到用内建的__import__()函数来实现导入。尽管这样也行得通，但 importlib.import_module()通常要更容易使用一些。

请参见 10.11 节中有关自定义导入过程的高级示例。

# 10.11 利用 import 钩子从远端机器上加载模块

# 10.11.1 问题

我们想对 Python 的 import 语句做定制化处理，实现以透明的方式从远端机器上加载模块。

# 10.11.2 解决方案

首先我们要对安全性方面的问题做严肃的免责声明。本节讨论的思路和技术如果缺少某种额外的安全和认证机制的保护将变得非常糟糕。也就是说，本节的主要目标实际上是对Python 中 import语句的内部工作原理做了深入的探讨。如果能消化本节的内容并理解内部的工作原理，那么将为此打下坚实的基础。今后面对定制化 import的操作，无论是出于什么目的，我们都能应对自如。好了，让我们继续吧。

本节的核心目标是扩展 import 语句的功能。有好几种方法可以实现这个目标，但是为了说明起见，我们先把 Python 代码按照下列方式进行组织：

```txt
testcode/
spam.py
fib.py
grok/
    __init__ .py
    blah.py 
```

这些文件的内容无关紧要，但是可以在每个文件中放一些简单的语句和函数，这样我们可以进行测试，当它们被导入时可以看到输出的结果。例如：

```txt
spam.py   
print("I'm spam") 
```

```python
def hello(name):
    print('Hello %s' % name)
# fib.py
print("I'm fib")
def fib(n):
    if n < 2:
        return 1
    else:
        return fib(n-1) + fib(n-2)
# grok/_init_.py
print("I'm grok._init.")
# grok/blah.py
print("I'm grok.blah") 
```

这么做的目的是允许这些文件能够以模块的形式从远端访问。也许最简单的方式就是在 Web 服务器上来发布这些模块。只需要进入 testcode 目录，然后像下面这样运行Python 即可：

```batch
bash % cd testcode  
bash % python3 -m http.server 15000  
Serving HTTP on 0.0.0.0 port 15000 ... 
```

让服务器一直运行，然后启动一个新的 Python 解释器进程。确保可以通过 urllib 来访问这些远程文件。示例如下：

```python
>>> from urllib.request import urlopen
>>> u = urlopen('http://localhost:15000/fib.py')
>>> data = u.read().decode('utf-8')
>>> print(data)
# fib.py
print("I'm fib")
def fib(n):
    if n < 2:
        return 1
    else:
        return fib(n-1) + fib(n-2)
>>> 
```

从服务器中加载源代码，这一思想将是本章余下内容的基础。具体来说就是，与其通过 urlopen()函数手动从服务器上把源代码抓取下来，不如自定义 import 语句的行为，使

其能够在幕后以透明的方式实现同样的目的。

第一种用来加载远程模块的方法就是创建一个显式的加载函数。示例如下：

import imp   
import urllib.request   
import sys   
def load_module(url): u $=$ urllib.request(urlopen(url) source $\equiv$ u.read().decode('utf-8') mod $=$ sys/modules.setdefault(url,imp.new_module(url)) code $=$ compile.source,url,'exec') mod._file_ $=$ url mod._package_ $=$ '' exec(code，mod._dict_) return mod

这个函数只是用来下载源代码的，利用 compile()函数将其编译为 code 对象，然后在新创建的模块对象的字典中执行它。下面是使用这个函数的方法：

```txt
>>> fib = load_module('http://localhost:15000/fib.py')  
I'm fib  
>>> fib.fib(10)  
89  
>>> spam = load_module('http://localhost:15000/spam.py')  
I'm spam  
>>> spam.hello('Guido')  
Hello Guido  
>>> fib  
<module 'http://localhost:15000/fib.py' from 'http://localhost:15000/fib.py'>  
>>> spam  
<module 'http://localhost:15000/spam.py' from 'http://localhost:15000/spam.py'>  
>>> 
```

可以看到，对于简单的模块这么做是可行的。但是，这个功能并没有嵌入到常用的 import语句中。而且如果要支持更加高级的组件，比如包，就需要扩展代码，这都需要做更多的工作才能实现。

更加高级的方法是创建一个自定义的导入器（importer）。实现这个目的的第一种方法是创建一个称之为元路径导入器（meta path importer）的组件。示例如下：

```txt
urlimport.py   
import sys   
import importlib.abc 
```

```python
import imp   
from urllib.request import urlopen   
from urllib.error import HTTPError, URLErro   
from html.parser import HTMLParser 
```

```python
Debugging  
import logging  
log = logging.getLogger(_name__) 
```

Get links from a given URL   
def_getlinks(url): class LinkParser(HTMLParser): def handle_starttag(self, tag, attrs): if tag $= =$ 'a': attrs $=$ dictattrs) links.add(attrs.get('href').rstrip('/')) links $=$ set()

```txt
try: log.debug('Getting links from %s' %url) u = urlopen(url) parser = LinkParser() parser/feed(u.read().decode('utf-8')) except Exception as e: log.debug('Could not get links. %s', e) log.debug('links: %r', links) return links 
```

classUrlMetaFinder(importlib.abc.MetaPathFinder): def__init__(self，baseurl): self._baseurl $=$ baseurl self._links $\equiv$ { } self._loaders $\equiv$ { baseurl：UrlModuleLoader(baseurl)} deffind_module(self，fu1name，path=None): log.debug('find_module:fullname=%r，path=%r'，fu1name，path) if path is None: baseurl $=$ self._baseurl else: if not path[0].startswith(self._baseurl): return None baseurl $=$ path[0] parts $=$ fullname.split（'.）） basename $=$ parts[-1] log.debug('find_module：baseurl=%r，basename=%r'，baseurl，basename)

```python
# Check link cache
if hostname not in self._links:
    self._links[baseurl] = _getlinks(baseurl)
# Check if it's a package
if hostname in self._links(baseurl):
    log.debug('find_module: trying package %r', fullname)
    fullurl = self._baseurl + '/' + hostname
# Attempt to load the package (which accesses __init__.py)
loader =UrlPackageLoader(fullurl)
try:
    loader.load_module(fullname)
    self._links[fullurl] = _getlinks(fullurl)
    self._loaders[fullurl] =UrlModuleLoader(fullurl)
    log.debug('find_module: package %r loaded', fullname)
except ImportError as e:
    log.debug('find-module: package failed. %s', e)
loader = None
return loader
# A normal module
filename = hostname + '.py'
if hostname in self._links(baseurl):
    log.debug('find_module: module %r found', fullname)
return self._loaders(baseurl]
else:
    log.debug('find-module: module %r not found', fullname)
return None
def invalidate_caches(self):
    log.debug('invalidating link cache')
self._links.clear()
# Module Loader for a URL
classUrlModuleLoader(importlib.abc.SourceLoader):
def __init__(self, baseurl):
    self._baseurl = baseurl
    self._source_cache = {}
def module_repr(self, module):
return'<urlmodule %r from %r>' % (module.__name__, module.__file__) # Required method
def loadModule(self, fullname): 
```

code $=$ self.get_code(fullname) mod $=$ sys/modules.setdefault(fullname,imp.new_module(fullname)) mod._file $\equiv$ self.get_filename(fullname) mod._loader $\equiv$ self mod._package $\equiv$ fullname.rpartition('.') [0] exec(code，mod._dict_） return mod   
# Optional extensions def get_code(self，fullname): src $=$ self.get_source(fullname) return compile(src,self.get_filename(fullname)，'exec') def get_data(self，path): pass def get_filename(self，fullname): return self._baseurl $+ / ^{\prime} +$ fullname.split（.）[-1] $^+$ '.py' def get_source(self，fullname): filename $=$ self.get_filename(fullname) log.debug('loader:reading $\% \mathbb{r}^{\prime}$ ,filename) if filename in self._source_cache: log.debug('loader: cached $\% \mathbb{r}^{\prime}$ ,filename) return self._source_cache[filename] try: u $=$ urlopen(filename) source $=$ u.read().decode('utf-8') log.debug('loader: $\% \mathbb{r}$ loaded',filename) self._source_cache[filename] $=$ source return source except (HTTPError, URLError) as e: log.debug('loader: $\% \mathbb{r}$ failed. $\% \mathbb{s}'$ ,filename,e) raise ImportError("Can't load $\% \mathbb{s}''$ %filename) def is-package(self，fullname): return False   
#Package loader for a URL classUrlPackageLoader(UrlModuleLoader): def loadModule(self，fullname): mod $=$ super().load_module(fullname) mod._path $=$ [self._baseurl] mod._package $\equiv$ fullname

```python
def getFilename(self,fullname):
    return self._baseurl + '/' + '.init.py'
def ispackage(self,fullname):
    return True
# Utility functions for installing/uninstalling the loader _installed_meta_cache = {}
def install_meta(address):
    if address not in _installed_meta_cache:
        finder =UrlMetaFinder(address)
        _installed_meta_cache[address] = finder
        sys.meta_path.appendfinder)
        log.debug('%r installed on sys.meta_path', finder)
def remove_meta(address):
    if address in _installed_meta_cache:
        finder = _installed_meta_cache.pop(address)
        sys.meta_path.removefinder)
        log.debug('%r removed from sys.meta_path', finder) 
```

下面的交互式会话展示了应该如何使用上述代码：  
```python
>>> # importing currently fails
>>> import fib
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ImportError: No module named 'fib'
>>> # Load the importer and retry (it works)
>>> import urlimport
>>> urlimport.install_meta('http://localhost:15000')
>>> import fib
I'm fib
>>> import spam
I'm spam
>>> import grokblah
I'm grok.__init__
I'm grokblah
>>> grokblah.__file__
' http://localhost:15000/grok/blah.py'
>>> 
```

在这个特定的解决方案中，我们把一个特殊的查询对象——UrlMetaFinder 的实例安装到 sys.meta_path 的最后一个条目中。每当要导入模块时就会在 sys.meta_path 中查找对

应的查询对象，以此来寻找模块。在这个示例中，如果在所有正常的位置上都找不到所需的模块，此时 UrlMetaFinder实例就成了最后的救命稻草，会触发它来寻找所需的模块。

作为一般的实现方法，UrlMetaFinder 类对用户指定的URL 进行包装。在内部，查询器会通过给定的 URL 构建一组合法的链接。当出现导入的动作时，用模块名来同已知的链接进行对比。如果有匹配，此时就用 UrlModuleLoader 类来从远端机器上加载模块的源代码并创建出最终的模块对象作为结果。缓存链接的一个原因是为了避免对重复的导入做不必要的 HTTP 请求。

自定义导入功能的第二种方式是编写一个钩子（hook），直接将其插入到 sys.path 变量中，用来识别特定的目录命名模式。下面我们给 urlimport.py 文件添加以下的类和支持函数：

```python
# import import.py
# ... include previous code above ...
# Path finder class for a URL
classUrlPathFinder(importlib.abc.PathEntryFinder):
    def __init__(self, baseurl):
        self._links = None
        self._loader = ModuleLoader(baseurl)
        self._baseurl = baseurl
def findloader(self, fullname):
    log.debug('findloader: %r', fullname)
    parts = fullname.split可能导致:
       部分内容是空格，因此，部分可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因此，部分内容可能为空格。
   部分内容是空格，因为文件名是空格
    # Check link cache
    if self._links is None:
        self._links = []
        # See discussion
        self._links = _getlinks(self._baseurl)
    # Check if it's a package
    if basename in self._links:
        log.debug('findloader: trying package %r', fullname)
        fullurl = self._baseurl + '/' + basename
        # Attempt to load the package (which accesses __init__.py)
        loader = UrLPackageLoader(fullurl)
        try:
            loader.loadModule(fullname)
            log.debug('findloader: package %r loaded', fullname)
        except ImportError as e:
            log.debug('findloader: %r is a namespace package', fullname)
```

```python
loader = None
return (loader, [fullurl])
# A normal module
filename = filename + '.py'
if filename in self._links:
    log.debug('findloader: module %r found', fullname)
    return (self._loader,[])
else:
    log.debug('findloader: module %r not found', fullname)
    return (None,[])
def invalidate_caches(self):
    log.debug('invalidating link cache')
    self._links = None
# Check path to see if it looks like a URL
_url_path_cache = {}
def handle_url(path):
    if path.startswith(['http://', 'https://']):
        log.debug('Handle path? %s. [Yes)', path)
        if path in_url_path_cache:
            finder = _url_path_cache[path]
        else:
            finder = URlPathFinder(path)
       _url_path_cache[path] = finder
        return finder
    else:
        log.debug('Handle path? %s. [No)', path)
def install_path-hook():
    sys.pathierocludes.append(handle_url)
    sys.path importer_cache.clear()
    log.debug('Installing handle_url')
def remove_path-hook():
    sys.pathierocludes.remove(handle_url)
    sys.path importer_cache.clear()
    log.debug('Removing handle_url') 
```

要使用这个基于路径的查询器，只需要将 URL 添加到 sys.path 中。例如：

```txt
>>> # Initial import fails  
>>> import fib  
Traceback (most recent call last): 
```

```txt
File "<stdin>", line 1, in <module>  
ImportError: No module named 'fib'  
>>> # Install the path hook  
>>> import urlimport  
>>> urlimport.install_path-hook()  
>>> # Imports still fail (not on path)  
>>> import fib  
Traceback (most recent call last):  
    File "<stdin>", line 1, in <module>  
ImportError: No module named 'fib'  
>>> # Add an entry to sys.path and watch it work  
>>> import sys  
>>> sys.path.append('http://localhost:15000')  
>>> import fib  
I'm fib  
>>> import grokblah  
I'm grok.__init__  
I'm grokblah  
>>> grokblah.__file__  
' http://localhost:15000/grok/blah.py'  
>>> 
```

最后这个例子的关键在于 handle_url()函数，我们将它添加到了 sys.path_hooks 中。当开始处理 sys.path 中的条目时，位于 sys.path_hooks 中的函数就被调用。如果这些函数中有任何一个返回了一个查询对象（finder object），就用这个查询对象来尝试为 sys.path中的条目加载模块。

应该要指出的是，从远端导入的模块使用起来和其他的模块一样。示例如下：

```python
>>> fib  
<urlmodule 'fib' from 'http://localhost:15000/fib.py'>  
>>> fib.__name__  
'fib'  
>>> fib.__file__  
' http://localhost:15000/fib.py'  
>>> import inspect  
>>> print(inspect.getsource(fib))  
# fib.py  
print("I'm fib")  
def fib(n):  
    if n < 2: 
```

```txt
return 1 else: return fib(n-1) + fib(n-2) >>> 
```

# 10.11.3 讨论

在继续深入讨论本节中提到的技术之前，应该要强调的是 Python 的模块、包和导入机制是整个语言中最为复杂的部分之 就算是最有经验的 Python 程序员对这部分的理解往往也不尽人意，除非他们愿意竭尽所能去挖掘底层的原理。有几份重要的文档值得去阅读，包括 importlib 模块的文档（http://docs.python.org/3/library/importlib.html）以及 PEP 302（http://www.python.org/dev/peps/pep-0302）。本书不会重复文档中的内容，但会讨论其中的一些要点。

首先，如果想创建一个新的模块对象（module object），可以使用 imp.new_module()函数。示例如下：

```txt
>>> import imp
>>> m = imp.new_module('spam')
>>> m
<module 'spam>
>>> m.__name__
'spam'
>>> 
```

模块对象通常会有几个意料之中的属性，包括__file__（所加载模块的源文件名）和_package__（包的名称，如果有的话）。

其次，模块会被解释器做缓存处理。模块缓存可以在字典 sys.modules 中找到。由于存在缓存处理，通常我们就把缓存和模块的创建联合成一个单独的步骤来做。示例如下：

```coffeescript
>>> import sys
>>> import imp
>>> m = sys modules.setdefault('spam', imp.new_module('spam'))
>>> m
<module 'spam>
>>> 
```

这么做的主要原因是如果某个给定名称的模块已经存在的话，就会直接得到已经创建好的模块了。示例如下：

```txt
>>> import math
>>> m = sys modules.setdefault('math', imp.new_module('math'))
>>> m
<module 'math' from '/usr/local/lib/python3.3/lib-dynload/math.so'> 
```

```txt
>> m.sin(2)  
0.9092974268256817  
>> m.cos(2)  
-0.4161468365471424  
>> 
```

由于创建模块是很简单的，所以可以直接编写简单的函数来处理，就像本节第一部分中的 load_module()函数那样。这种方法的缺点是对于更加复杂的情况，处理会变得相当棘手，例如导入包的时候。为了能够处理包，将不得不重新实现大部分的底层逻辑，而这些东西已经是普通的 import 语句实现过了的（例如，检查目录、查找__init__.py文件、执行这些文件、建立起路径等）。

由于这种复杂性，因此通常更好的选择是直接扩展 import 语句的功能，而不是去定义自己的处理函数，这正是为何要这么做的原因之一。

扩展 import 语句是简单而直接的，但是其中涉及多个部件。从最高层级来看，import操作要处理一系列“元路径”查询器，这些查询器可以在 sys.meta_path 中找到。如果输出它的值，可以看到如下的输出：

```txt
>>> from pprint import pprint
>>> pprint(sys.meta_path)
[<class "_frozen-importlib.BuiltInImporter>", <class "_frozen-importlib.FrozenImporter>", <class "_frozen-importlib.PathFinder">]
>>> 
```

当执行一条语句比如 import fib 时，解释器会遍历 sys.meta_path 中的查询器对象，并调用它们的 find_module()方法以此来找到合适的模块加载器。用实验的方式来观察这一过程会对我们的理解有所帮助，因此我们这里定义如下的类并尝试做以下的操作：

```python
>>> class Finder:
...
def find_module(self, fullname, path):
...
print('Looking for', fullname, path)
...
return None
...
import sys
sys.meta_path.insert(0, Finder()) # Insert as first entry
>>> import math
Looking for math None
>>> import types
Looking for types None
>>> import threading
Looking for threading None
Looking for time None
Looking for traceback None 
```

```txt
Looking for linecache None Looking for tokenizer None Looking for token None >> 
```

注意在每个 import 操作中 find_module()方法是如何被触发执行的。在这个方法中，参数 path 的作用是用来处理包的。当导入的是包时，参数 path 表示的是包的__path__属性中列出的目录列表。需要检查这些路径来找出包中的子模块。例如，注意 xml.etree和 xml.etree.ElementTree 的路径设定：

```txt
>>> import xml.etree.ElementTree
Looking for xml None
Looking for xml.etree ['/usr/local/lib/python3.3/xml']
Looking for xml.etree.ElementTree ['/usr/local/lib/python3.3/xml/etree']
Looking for warnings None
Looking for contextlib None
Looking for xml.etree.ElementPath ['/usr/local/lib/python3.3/xml/etree']
Looking for _elementtree None
Looking for copy None
Looking for org None
Looking for pyexpat None
Looking for ElementC14N None
>>> 
```

查询器对象在 sys.meta_path 中的位置是至关重要的。做个试验，将查询器对象从列表头移动到列表尾部，然后尝试做几个导入操作：

```python
>>> del sys.meta_path[0]  
>>> sys.meta_path.append(Finder())  
>>> import urllib.request  
>>> import datetime 
```

现在看不到任何输出了，因为现在的导入操作被 sys.meta_path 中的其他条目处理了。在这种情况下，只有在导入并不存在的模块时才会触发我们自定义的查询器：

```python
>>> import fib
Looking for fib None
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ImportError: No module named 'fib'
>>> import xml.superfast
Looking for xml.superfast ['/usr/local/lib/python3.3/xml']
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ImportError: No module named 'xml.superfast'
>>> 
```

我们可以安装一个查询器以捕获未知的模块，这一事实正是本节中给出的UrlMetaFinder 类的核心所在。在 sys.meta_path 的尾部添加一个 UrlMetaFinder 实例，把它当做导入操作的最后一道保险。如果请求的模块名无法被其他导入机制所定位，那么就由这个查询器来负责处理。当处理包的导入时还需要注意一些事项。具体来说就是，对于 path 参数的值，我们需要检查看它是否以我们注册到查询器中的 URL 开头。如果不是，那么子模块肯定属于其他查询器负责处理的范畴，这里就应该忽略。

对于包的附加处理可以在 UrlPackageLoader 类中找到。这个类不是去导入包的名称，而是尝试加载底层的 init .py 文件，最后它还会设定模块的__path__属性。最后这一步是非常关键的，因为当加载包的子模块时，这个设定的值会传递给接下来的find_module()调用。

基于路径的 import 钩子是对这些思想的一种扩展，只是其中的机制有所不同。如你所知，sys.path 是一个路径列表，其中保存的是 Python 查询模块的路径。例如：

```txt
>>> from pprint import pprint
>>> import sys
>>> pprint(sys.path)
['',
'/usr/local/lib/python33.zip',
'/usr/local/lib/python3.3',
'/usr/local/lib/python3.3/plat-darwin',
'/usr/local/lib/python3.3/lib-dynload',
'/usr/local/lib/../3.3/site-packages']
>>> 
```

sys.path 中的每一个条目都会同一个查询器对象关联起来。我们可以通过打印sys.path_importer_cache 来查看这些查询器：

```python
>>> pprint(sys.path/importer_cache)
{''.': FileFinder('.', '/usr/local/lib/python3.3': FileFinder('/usr/local/lib/python3.3'), '/usr/local/lib/python3.3/'': FileFinder('/usr/local/lib/python3.3/'), '/usr/local/lib/python3.3/collections': FileFinder(...python3.3/collections'), '/usr/local/lib/python3.3/encodings': FileFinder(...python3.3/encodings'), '/usr/local/lib/python3.3/lib-dynload': FileFinder(...python3.3/lib-dynload'), '/usr/local/lib/python3.3/plat-darwin': FileFinder(...python3.3/plat-darwin'), '/usr/local/lib/python3.3/site-packages': FileFinder(...python3.3/site-packages'), '/usr/local/lib/python33.zip': None} 
```

sys.path_importer_cache 比 sys.path 要大的多，因为前者会针对所有已知代码将被加载的目录记录下相应的查询器。这包括了包中的子目录，而这些信息通常是不包含在sys.path 中的。

当执行 import fib 时，sys.path 中的目录会按顺序逐个接受检查。对于每个目录，名称fib 会被传递给 sys.path_importer_cache 中与目录相关联的查询器。我们也可以创建自己的查询器，并将其添加到 sys.path_importer_cache 中。比如下面这个实验：

```python
>>> class Finder:
...
def findloader(self, name):
    print('Looking for', name)
    return(None,[])
...
import sys
# Add a "debug" entry to the importer cache
sys.path importer_cache['debug'] = Finder()
# Add a "debug" directory to sys.path
sys.path.insert(0, 'debug')
...
import threading
Looking for threading
Looking for time
Looking for traceback
Looking for linecache
Looking for tokenizer
Looking for token
>>> 
```

这里，我们以 debug 为名称安装了一个新的缓存条目，并把 debug 作为 sys.path 的第一个条目。在之后所有的导入动作中，就会发现自己定义的查询器都会被触发执行。但是，由于它只会返回(None, [])，因此处理流程只是简单地继续前往下一个条目执行。

sys.path_importer_cache 中的内容是由保存在 sys.path_hooks 中的一系列函数来控制的。尝试做下面这个实验，它会清空缓存，并添加一个新的路径检查函数到 sys.path_hooks中去：

```txt
>>> sys.path importing_cache.clear()
>>> def check_path(path):
...
...
print('Checking', path)
...
raise ImportError()
...
>>> sys.pathmare.insert(0, check_path)
>>> import fib
Checked debug
Checking .
Checking /usr/local/lib/python33.zip
Checking /usr/local/lib/python3.3
Checking /usr/local/lib/python3.3/plat-darwin
Checking /usr/local/lib/python3.3/lib-dynload
Checking /Users/beazley/.local/lib/python3.3/site-packages 
```

```txt
Checking /usr/local/lib/python3.3/site-packages Looking for fib Traceback (most recent call last): File "<stdin>", line 1, in <module> ImportError: No module named 'fib' >>> 
```

可以看到，针对 sys.path 中的每一个条目，check_path()函数都会得到调用。但是由于会产生 ImportError 异常，因此实际上什么也没发生（只是跳转到 sys.path_hooks 中的下一个函数继续执行检查）。

利用 sys.path 的处理机制，我们可以安装一个自定义的路径检查函数来查找特定的文件名模式，比如 URL。示例如下：

```python
>>> def check_url(path):
    ...     if path.startswith('http://'): 
        ...         return Finder()
    ...         else:
        ...             raise ImportError()
    ...
>>> sys.path.append('http://localhost:15000')
>>> sys.pathmarens[0] = check_url
>>> import fib
Looking for fib      # Finder output!
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ImportError: No module named 'fib'
>>> # Notice installation of Finder in sys.path importer_cache
>>> sys.path importer_cache['http://localhost:15000']
<__main__.Finder object at 0x10064c850>
>>> 
```

这就是本节最后这部分内容的核心工作原理。从本质上说，就是在 sys.path_hooks中安装一个用来寻找 URL 的自定义路径检查函数。当遇到这个自定义检查函数时，会产生一个新的 UrlPathFinder 实例并将其安装到 sys.path_importer_cache 中。从此之后，对于所有的导入语句，只要在遍历 sys.path的过程中遇到这个部分，就会尝试使用自定义的查询器了。

基于路径的导入器在处理包的时候多少需要一些技巧，与 find_loader()方法的返回值也有关系。对于简单的模块来说，find_loader()返回一个元组(loader, None)，这里的 loader是将要导入这个模块的加载器实例。

对于一个普通的包来说，find_loader()返回一个元组(loader, path)，这里的 loader 是将要

导入这个包的加载器实例（并且会执行 init .py），而 path 是一个目录列表，这些目录会组成包的__path__属性的初始值。比方说，如果 URL 是 http://localhost:15000，并且某个用户执行了 import grok 语句，find_loader()返回的路径就会是['http://localhost:15000/grok']。

find_loader()还必须负责处理命名空间包（见10.5 节）的情况。命名空间包是一个合法的包，其目录名存在但其中并不包含 init .py 文件。对于这种情况，find_loader()必须返回一个元组(None, path)，这里的 path 是一个目录列表，这些目录组成了包的_path__属性，并和普通的包一样，认为在这些目录中定义有 init .py 文件（实际并不存在）。对于这种情况，import 导入机制会继续检查 sys.path 中的目录。如果找到了更多的命名空间包，那么所有找到的结果路径都会连接在一起以形成一个最终的命名空间包。有关命名空间包的更多信息请参见 10.5 节。

在我们的解决方案中，在处理包的时候还运用了递归的思想，虽然看起来并不明显但它同样能够工作。所有的包都包含一个内部的路径设置，这可以在__path__属性中找到。示例如下：

```txt
>>> import xml.etree.ElementTree
>>> xml._path_
['/usr/local/lib/python3.3/xml']  
>>> xml.etree._path_
['/usr/local/lib/python3.3/xml/etree']
>>> 
```

正如我们提到过的，__path__的设置是由 find_loader()方法的返回值来控制的。但是，之后对__path__的处理也是由 sys.path_hooks 中的函数来处理的。因此，当加载包中的子模块时，__path__中的条目是由handle_url()函数来负责检查的。这会导致创建出新的UrlPathFinder 实例并添加到 sys.path_importer_cache 中。

在我们的实现中，最后一个棘手的部分是考虑 handle_url()函数的行为以及它同内部使用的函数_get_links()之间的交互。如果我们的查询器实现中用到了其他的模块（例如urllib.request），那么有可能这些模块会在查询器工作的过程中发起进一步的模块导入请求。这就会导致 handle_url()和查询器的其他部分以循环递归的形式执行下去。为了应对这种可能，我们给出的实现中对已经创建出的查询器维护了一个缓存对象（每条URL对应一个缓存）。这就避免了重复创建查询器的问题。此外，下面的代码片段确保了查询器在获取初始链接的过程中不会去响应任何导入请求：

```python
# Check link cache
if self._links is None:
    self._links = []
    # See discussion
    self._links = __getlinks(self._baseurl) 
```

在其他的实现中可能不需要做上述检查，但是对于这个涉及 URL 的例子，这么做是必

需的。

最后，查询器中的 invalidate_caches()是一个实用的方法，当源代码需要发生改变时用来清除内部的缓存。当用户调用 importlib.invalidate_caches()时会触发该方法执行。如果想让URL导入器重新读取链接列表，可以使用这个方法。这么做可能是为了能够访问到新添加的文件。

对比以上两种方法（修改 sys.meta_path 或者使用 path 钩子）有助于站在更高层的角度来看待问题。利用 sys.meta_path 安装的导入器可以自由地以任何它们所希望的方式来处理模块。例如，它们可以从数据库中取出模块加载，或者以根本不同于普通的模块/包处理的方式来完成导入。这种自由同样意味着这样的导入器需要做更多的簿记（bookkeeping）和内部管理工作。这也解释了为何在 UrlMetaFinder 的实现中需要自己实现链接缓存、加载器以及其他一些细节。另一方面，基于路径的钩子方法则同 sys.path的处理联系得更为紧密。由于同 sys.path的关系紧密，以这种扩展方式加载的模块在特征上倾向于与程序员通常使用的普通模块和包相同。

假设你的大脑现在还没有完全爆炸，理解和试验本节中技术的关键方法可能就是添加日志记录了。我们可以开启日志功能并尝试如下的试验：

```python
>>> import logging
>>> logging-basicConfig(level=logging DEBUG)
>>> import urllib
>>> urllib install_path-hook()
DEBUG:urlimport:Installing handle_url
>>> import fib
DEBUG:urlimport:Handle path? /usr/local/lib/python33.zip. [No]
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ImportError: No module named 'fib'
>>> import sys
>>> sys.path.append('http://localhost:15000')
>>> import fib
DEBUG:urlimport:Handle path? http://localhost:15000. [Yes]
DEBUG:urlimportGetting links from http://localhost:15000
DEBUG:urlimport:links: {'spam.py', 'fib.py', 'grok'}
DEBUG:urlimport.findloader: 'fib'
DEBUG:urlimport.findloader: module 'fib' found
DEBUG:urlimportloader: reading 'http://localhost:15000/fib.py'
DEBUG:urlimportloader: 'http://localhost:15000/fib.py' loaded
I'm fib
>>> 
```

最后但同样重要的是，睡前在枕头下备好 PEP 302（http://www.python.org/dev/peps/pep-0302）和 importlib 的文档，花些时间读一读它们也许是很明智的。

# 10.12 在模块加载时为其打补丁

# 10.12.1 问题

我们想对已有的模块打补丁或对其中的函数添加装饰器。但是，我们只希望当模块实际得到导入的时候才这么做，之后再在别处使用。

# 10.12.2 解决方案

这个问题的关键在于我们想针对正在加载的模块执行响应操作。当某个模块得到加载时，也许我们想触发某种形式的回调函数来通知这一事实。

这个问题可以使用 10.11 节中讨论过的 import 钩子机制来解决。下面是一种可能的解决方案：

```python
# postimport.py
import importlib
import sys
from collections import defaultdict
post-importhooks = defaultdict(list)
class PostImportFinder:
    def __init__(self):
        self._skip = set()
    def find_module(self, fullname, path):
        if fullname in self._skip:
            return None
            self._skip.add(fullname)
            return PostImportLoader(self)
class PostImportLoader:
    def __init__(self, finder):
        self._finder = finder
    def loadModule(self, fullname):
        importlib import module(fullname, sys modules)[fullname]
        for func in post-importhooks:
            funcmodule()
            self._finder._skip.remove(fullname) 
```

```python
def when Imported(fullname):
    def decorate(func):
        if fullname in sys/modules:
            func(sys/modules[fullname])
        else:
            post-importHooks[fullname].append(func)
        return func
    return decorate
sys.meta_path.insert(0, PostImportFinder()) 
```

要使用这份代码，就要用到 when_imported()装饰器。示例如下：

```python
>>> from postimport import when Imported
>>> @when Imported('threading')
... def warn Threads(mod):
    ...
print('Threads? Are you crazy?')
...
>>> import threading
Threads? Are you crazy?
>>> 
```

作为一个更加实际的例子，也许想在已有的定义上添加装饰器，比如：

from functools import wraps   
from postimport import when Imported   
def logged func): @wraps(func) def wrapper $\text{串} _ { \text{串} }$ \*\*kwarges): print('Calling',func._name_,args,kwarges) return func $\text{串} _ { \text{串} }$ \*\*kwarges) return wrapper   
#Example   
@when Imported('math')   
defadd Logging(mod): mod.cos $=$ logged(mod.cos) mod.sin $=$ logged(mod.sin)

# 10.12.3 讨论

本节依赖于 10.11 节中讨论过的 import 钩子技术，做了少许修改。

首先， $@$ when_imported 装饰器的作用是注册在导入时需要触发执行的处理函数。这个

装饰器检查 sys.modules，看某个模块是否已经加载了。如果是，则立刻调用处理函数。否则就将处理函数添加到_post_import_hooks 字典中去。定义_post_import_hooks 的目的只是简单地用来收集所有的已经针对每个模块所注册的处理对象。原则上，对于一个给定的模块可以注册多个处理程序。

在导入模块之后，要触发_post_import_hooks 中挂起的操作，PostImportFinder 类的实例就要安装到 sys.meta_path 中的首元素位置上。如果回顾一下 10.11 节就会知道，sys.meta_path 中包含一列用来定位模块位置的查询器对象。通过将 PostImportFinder 类的实例安装到列表的首元素位置，那么它就能捕获所有的模块导入动作。

但是在本节中，PostImportFinder 的作用不是用来加载模块，而是触发相应的处理流程来完成整个导入动作。要做到这一点，实际的导入被委托给 sys.meta_path 中的其他查询器来完成。不要尝试直接实现这一步骤，相反，我们是在 PostImportLoader 类中递归调用函数 imp.import_module()来完成的。为了避免出现无限递归的情况，我们在PostImportFinder 类中维护了一个集合，其中包含所有当前正处于加载过程中的模块。如果某个模块名属于这个集合，PostImportFinder 只会简单地忽略它。这就是导致 import请求会传递给 sys.meta_path 中其他查询器处理的原因。

在一个模块已经通过 imp.import_module()加载之后，所有当前注册到_post_import_hooks 中的处理例程都会以新加载的模块作为参数得到调用。从这一刻开始，处理例程就可以自由地对模块做任何想做的操作了。

本节所展示的方法中一个主要的特性就是对模块的打补丁操作是以无缝的方式进行的，与所感兴趣的模块的实际位置和加载方式无关。只需要简单地编写一个处理函数并用@when_imported()进行装饰，从那一刻起所有的操作就能魔法般地工作起来。

本节中需要注意的一个问题是对于已经使用 imp.reload()显式重新加载的模块，本节给出的方法是无效的。也就是说，如果重新加载一个之前加载过的模块，处理例程是不会再次得到触发的（我们有了更多的理由不要在生产环境中使用 reload()）。而另一方面，如果从sys.modules中删除模块然后再重新导入，就会发现处理例程会再次触发执行。

更多有关 post-import 钩子的信息可以在 PEP 369（http://www.python.org/dev/peps/pep-0369）中找到。在写作本节时，这份 PEP 已经被作者撤销了，原因是它同当前的 importlib 模块的实现相比显得过时了，但是利用本节所展示的方法实现自己的解决方案已经足够简单了。

# 10.13 安装只为自己所用的包

# 10.13.1 问题

我们想安装一个第三方的包，但是没有权限在系统 Python 中安装其他的包。另一种情

况是，也许我们只想安装一个给自己使用的包，而不是给系统中的所有用户使用。

# 10.13.2 解决方案

Python 有一个用户级的安装目录，通常位于类似~./local/lib/python3.3/site-packages 这样的目录下。要让包强制安装到这个目录下，只要在安装命令后添加--user 选项即可。示例如下：

```batch
python3 setup.py install -user 
```

或者

```batch
pip install --user package name 
```

用户级的 site-package 目录通常会在 sys.path 中出现，且位于系统级的 site-package 目录之前。因此，采用这种技术安装的包比已经安装到系统中的包优先级要高（尽管并不会总是这样，这取决于第三方包管理工具的具体行为，比如 distribute 或 pip）。

# 10.13.3 讨论

一般来说，包会被安装到系统级的 site-package 目录下，可以在例如/usr/local/lib/python3.3/site-packages 的位置上找到。但是安装到系统级的目录下通常都需要有管理员权限，并且要使用 sudo命令。就算我们有权限执行这样的命令，使用 sudo安装一个新的且没有经过实践证明的包也可能会给我们带来麻烦。

把包安装到用户级的目录中通常是一种有效的规避方案，这么做允许我们创建一个自定义的安装。

另一种解决方案是可以创建一个虚拟环境，这正是我们在下一节要讨论的主题。

# 10.14 创建新的 Python 环境

# 10.14.1 问题

我们想创建一个新的 Python 环境，在新环境中可以自由地安装模块和包。但是，我们并不想为此安装一个新的 Python 拷贝或者做出任何可能会影响到系统级 Python 安装的修改。

# 10.14.2 解决方案

可以通过 pyvenv 命令创建一个新的“虚拟”环境。这个命令被安装到同 Python 解释器一样的目录中，在 Windows 下可能位于 Scripts 目录下。这里有一个示例：

```batch
bash % pyvenv Spam  
bash % 
```

提供给 pyvenv 的名称就是将要创建的目录名称。创建好之后，Spam 目录看起来是这样的：

```txt
bash % cd Spam  
bash % ls  
bin include lib pyvenv.cfg  
bash % 
```

在 bin 目录下，我们会发现有一个可使用的 Python 解释器。示例如下：

```python
bash % Spam/bin/python3   
Python 3.3.0 (default, Oct 6 2012, 15:45:22)   
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin   
Type "help", "copyright", "credits" or "license" for more information.   
>>> from pprint import pprint   
>>> import sys   
>>> pprint(sys.path)   
['', '/usr/local/lib/python33.zip', '/usr/local/lib/python3.3', '/usr/local/lib/python3.3/plat-darwin', '/usr/local/lib/python3.3/lib-dynload', '/Users/beazley/Spam/lib/python3.3/site-packages']   
>>> 
```

这个解释器的核心特征就是它的 site-package 目录已经被设定为同新创建的环境相关联了。如果决定安装第三方的包，那么它们就会被安装到这里，而不是普通的系统级site-package 目录中。

# 10.14.3 讨论

创建虚拟环境大部分的原因都是为了安装和管理第三方的包。可以在示例中看到，sys.path 变量中包含的目录来自于普通的系统级 Python，但是 site-package 目录已经被重新定位到新的目录上了。

有了新的虚拟环境，下一步通常是安装一个包管理器，比如 distribute 或者 pip。当安装这类工具和其他的包时，只需确保使用的是虚拟环境中的解释器即可。这样的话，安装的包应该就会放在新创建的 site-packages 目录下了。

尽管虚拟环境看起来好像是 Python 安装的一份拷贝，但它实际上只是由几个文件和一些符号链接所组成。所有的标准库文件和解释器执行文件都来自于原来的 Python 安装包。因此，创建这样的环境非常简单方便，几乎不占用什么系统资源。

默认情况下，虚拟环境是完全干净且不包含任何第三方插件的。如果想将已经安装过的包引入，使其作为虚拟环境的一部分，那么可以使用选项--system-site-packages 来创建虚拟环境。示例如下：

```batch
bash % pyvenv --system-site-packages Spam bash %
```

有关 pyvenv 和虚拟环境的更多信息可以在 PEP 405（http://www.python.org/dev/peps/pep-0405）中找到。

# 10.15 发布自定义的包

# 10.15.1 问题

我们编写了一个有用的库，想将其分发给其他人使用。

# 10.15.2 解决方案

如果打算将代码发布出去，首先要做的就是为自己的库起一个独一无二的名称并清理代码的目录结构。例如，一个典型的程序库结构看起来大致是这样的：

```txt
projectname/  
README.txt  
Doc/  
documentation.txt  
projectname/  
__init__.py  
foo.py  
bar.py  
utils/  
__init__.py  
spam.py  
grok.py  
examples/  
helloworld.py 
```

要使得包能够发布出去，首先编写一个 setup.py 文件，看起来是这样的：

setup.py   
from distutils.core import setup   
setup(name $\equiv$ 'projectname', version $= 1.0$ author $\equiv$ Your Name', author_email $\equiv$ 'you@youraddress.com', url $\equiv$ http://www.you.com/projectname', packages $\equiv$ ['projectname', 'projectname.utils'],

接下来要创建一个 MANIFEST.in 文件，并在其中列出各种希望包含在包中的非源代码文件：

```txt
MANIFEST.in include *.txt recursive-include examples \* recursive-include Doc \* 
```

确保 setup.py 和 MANIFEST.in 文件位于包的顶层目录。一旦做完这些，应该就能够通过命令来创建一个源代码级的分发包了：

```batch
% bash python3 setup.py sdist 
```

根据不同的系统平台，这么做会创建出像 projectname-1.0.zip 或者 projectname-1.0.tar.gz这样的文件。如果一切顺利，这个文件就可用来发布给其他人或者上传到 PythonPackage Index（http://pypi.python.org）上了。

# 10.15.3 讨论

对于纯 Python 代码来说，编写一个简单的 setup.py 文件通常是很直接的。但其中一个潜在的问题是我们必须手动列出包中的每一个子目录。一个常见的错误就是只列出了包的顶层目录而忘记包含进包中的子模块。这就是为什么在 setup.py 中对包的规格说明里包含了列表 packages=['projectname', 'projectname.utils']的原因。

大多数 Python 程序员都知道，如今有许多第三方的包管理工具，包括安装、发布相关的，等等。这些第三方工具中有一些可用来替代标准库中的distutils 库。请注意，如果要依赖这些包，那么用户可能没法安装使用我们的软件，除非他们也安装了所需的包管理工具。基于此，只要我们尽可能让事情变得简单，那就几乎不会出什么错误。最低要求是请确保代码可以通过使用标准的 Python 3 安装方式来安装。如果还有别的安装包可用，那么可以将它们作为可选项以支持额外的功能。

打包和发布涉及 C 语言扩展的代码则会变得复杂得多。在第 15 章有关 C 语言扩展的章节中谈到了一些关于此的细节。具体请参见 15.2 节。

# 网络和 Web 编程

本章涵盖了在网络应用和分布式应用中使用 Python 的各种主题。主题可划分为使用Python 编写客户端程序来访问已有的服务，以及使用 Python 实现网络服务端程序。本章也提到了编写代码使多个解释器协同工作或者互相通信的常见技术。

# 11.1 以客户端的形式同 HTTP 服务交互

# 11.1.1 问题

我们需要以客户端的形式通过 HTTP 协议访问多种服务。比如，下载数据或者同一个基于 REST 的 API 进行交互。

# 11.1.2 解决方案

对于简单的任务来说，使用 urllib.request 模块通常就足够了。比方说，要发送一个简单的 HTTP GET 请求到远端服务器上，可以这样做：

```python
from urllib import request, parse
# Base URL being accessed
url = 'http://htppbin.org/get'
# Dictionary of query parameters (if any)
parms = {
    'name1': 'value1',
    'name2': 'value2'
} 
```

querystring $=$ parse.urlencode(parms) #Make a GET request and read the response u $=$ request.urlopenurl+?' $^+$ querystring) resp $\equiv$ u.read()

如果需要使用 POST 方法在请求主体（request body）中发送查询参数，可以将参数编码后作为可选参数提供给 urlopen()函数，就像这样：

from urllib import request, parse   
# Base URL being accessed   
url $=$ 'http://httpbin.org/post'   
#Dictionary of query parameters (if any)   
parms $=$ { 'name1': 'value1', 'name2': 'value2'   
}   
# Encode the query string   
querystring $=$ parse(urencodeParms)   
# Make a POST request and read the response   
u $=$ request(urlopen(url, querystring.encode('ascii'))   
resp $=$ u.read()

如果需要在发出的请求中提供一些自定义的 HTTP 头，比如修改 user-agent 字段，那么可以创建一个包含字段值的字典，并创建一个 Request 实例然后将其传给 urlopen()。示例如下：

from urllib import request, parse   
...   
#Extra headers   
headers $=$ { 'User-agent': 'none/ofyourbusiness', 'Spam': 'Eggs'   
}   
req $=$ request.Request(url, querystring.encode('ascii'), headers=headers)   
#Make a request and read the response   
u $=$ request(urlopen(request)   
resp $=$ u.read()

如果需要交互的服务比上面的例子都要复杂，也许应该去看看 requests 库（http://pypi.

python.org/pypi/requests）。比如，下面这个示例采用 requests 库重新实现了上面的操作：

import requests   
# Base URL being accessed   
url $=$ 'http://httpbin.org/post'   
# Dictionary of query parameters (if any)   
parms $=$ { namel':value1', name2':value2'   
}   
#Extra headers   
headers $=$ { User-agent':none/ofyourbusiness', Spam':Eggs'   
}   
resp $=$ requests.post(url,data=parms, headers=headers)   
# Decoded text returned by the request   
text $=$ resp.text

关于 requests库，一个值得一提的特性就是它能以多种方式从请求中返回响应结果的内容。从上面的代码来看，resp.text 带给我们的是以 Unicode 解码的响应文本。但是，如果去访问 resp.content，就会得到原始的二进制数据。另一方面，如果访问 resp.json，那么就会得到 JSON 格式的响应内容。

下面这个示例利用 requests 库来发起一个 HEAD 请求，并从响应中提取出一些 HTTP头数据的字段：

import requests   
resp $=$ requests.head('http://www.python.org/index.html')   
status $=$ resp.status_code   
last_modified $=$ resp Headers['last-modified']   
content_type $=$ resp headers['content-type']   
content_length $=$ resp headers['content-length']

下面的示例使用 requests 库通过基本的认证在 Python Package Index（也就是 pypi）上执行了一个登录操作：

```python
import requests 
```

```python
resp = requests.get('http://pypi.python.org/pypi?action=login', auth=(‘user','password')) 
```

下面的示例使用 requests 库将第一个请求中得到的 HTTP cookies 传递给下一个请求：

import requests   
#First request   
resp1 $\equiv$ requests.get(url)   
...   
#Second requests with cookies received on first requests   
resp2 $\equiv$ requests.get(url,cookies=resp1cookies)

最后但也同样重要的是，下面的例子使用 requests 库来实现内容的上传：

import requests   
url $=$ 'http://htppbin.org/post'   
files $=$ {'file':('data.csv',open('data.csv', 'rb'))}   
r $=$ requests.post(url, files=files)

# 11.1.3 讨论

对于确实很简单的HTTP 客户端代码，通常使用内建的 urllib 模块就足够了。但是，如果要做的不仅仅只是简单的 GET 或 POST 请求，那就真的不能再依赖它的功能了。这时候就是第三方模块比如 requests 大显身手的时候了。

举个例子，如果我们决定坚持使用标准的程序库而不考虑像 requests 这样的第三方库，那么也许就不得不使用底层的 http.client 模块来实现自己的代码。比方说，下面的代码展示了如何执行一个 HEAD 请求：

from http.client import HTTPConnection   
from urllib import parse   
c $=$ HTTPConnection('www.python.org'，80)   
c.request('HEAD'，'/index.html')   
resp $=$ c.getresponse()   
print('Status'，resp.status)   
for name，value in resp.getheaders(): print(name，value)

同样地，如果必须编写涉及代理、认证、cookies 以及其他一些细节方面的代码，那么使用 urllib 就显得特别别扭和 嗦。比方说，下面这个示例实现在 啰 Python package index上的认证：

importurllib.request   
auth $=$ urllib.request(HTTPBasicAuthHandler()   
auth.add_password('pypi','http://pypi.python.org','username','password')   
opener $=$ urllib.request.build_opener(auth) $\mathbf{r} =$ urllib.request.Request('http://pypi.python.org/pypi action $\equiv$ login') $\mathrm{u} =$ opener.open(r)   
resp $=$ u.read()   
#From here.You can access more pages using opener

坦白说，所有这些操作在 requests 库中都变得简单得多。

在开发过程中测试 HTTP 客户端代码常常是很令人沮丧的，因为所有棘手的细节问题都需要考虑（例如 cookies、认证、HTTP 头、编码方式等）。要完成这些任务，考虑使用 httpbin 服务（http://httpbin.org）。这个站点会接收发出的请求，然后以 JSON 的形式将响应信息回传回来。下面是一个交互式的例子：

```python
>>> import requests
>>> r = requests.get('http://httpbin.org/get?name=Dave&n=37',
...
...
... headers = {'User-agent': 'goaway/1.0'}
>>> resp = r.json
>>> resp['headers']
{'User-Agent': 'goaway/1.0', 'Content-Length': '', 'Content-Type': '', 'Accept-Encoding': 'gzip, deflate, compress', 'Connection':
'keep-alive', 'Host': 'httpbin.org', 'Accept': '*/'}
>>> resp['args']
{'name': 'Dave', 'n': '37'}
>>> 
```

在要同一个真正的站点进行交互前，先在 httpbin.org 这样的网站上做实验常常是可取的办法。尤其是当我们面对 3 次登录失败就会关闭账户这样的风险时尤为有用（不要尝试自己编写 HTTP 认证客户端来登录你的银行账户）。

尽管本节没有涉及，requests 库还对许多高级的 HTTP 客户端协议提供了支持，比如OAuth。requests 模块的文档（http://docs.python-requests.org）质量很高（坦白说比在这短短一节的篇幅中所提供的任何信息都好），可以参考文档以获得更多的信息。

# 11.2 创建一个 TCP 服务器

# 11.2.1 问题

我们想实现一个通过 TCP 协议同客户端进行通信的服务器。

# 11.2.2 解决方案

创建 TCP 服务器的一种简单方式就是利用 socketserver 库。比如，下面是一个简单的echo 服务示例：

from socketserver import BaseRequestHandler, TCPServer   
class EchoHandler(BaseRequestHandler): def handle(self): print('Got connection from', self.client_address) while True: msg $=$ self.request.recv(8192) if not msg: break self.request.send(msg)   
if_name $\equiv$ main': serv $=$ TCPServer(('', 20000), EchoHandler) servServe_forever()

在这份代码中，我们定义了一个特殊的处理类，它实现了一个 handle()方法来服务于客户端的连接。这里的 request 属性就代表着底层的客户端 socket，而 client_address 中包含了客户端的地址。

要测试这个服务端程序，首先运行这个脚本，然后打开另一个 Python 进程并将其连接到服务端上：

```python
>>> from socket import socket, AF_INET, SOCK_STREAM
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s.connect('localhost', 20000)
>>> s.send(b'Hello')
5
>>> s.recv(8192)
b'Hello'
>>> 
```

在许多情况下，定义一个类型稍有不同的处理类可能会更加简单。下面的示例使用StreamRequestHandler 作为基类，给底层的 socket 加上了文件类型的接口：

```python
from socketserver import StreamRequestHandler, TCPServer   
class EchoHandler(StreamRequestHandler): def handle(self): print('Got connection from', self.client_address) #self.rfile is a file-like object for reading for line in self.rfile: 
```

```python
# self.wfile is a file-like object for writing
    self.wfile.write(line)
if __name__ == '__main__':
    serv = TCPServer(['', 20000), EchoHandler)
    servServe_forever() 
```

# 11.2.3 讨论

socketserver 模块使得创建简单的 TCP 服务器相对来说变得容易了许多。但是，应该要注意的是，默认情况下这个服务器是单线程的，一次只能处理一个客户端。如果想处理多个客户端，可以实例化 ForkingTCPServer 或者 ThreadingTCPServer 对象。示例如下：

```python
from socketserver import ThreadingTCPServer
...
if __name__ == '__main__':
    serv = ThreadingTCPServer(['', 20000), EchoHandler)
    servServe_forever() 
```

多进程和多线程服务器的问题在于它们会针对每一个客户端连接创建一个新的进程或线程。但是允许连接的客户端数量是没有上限的，因此一个怀有恶意的黑客可能会同时发起海量的连接使你的服务器挂掉。

如果需要考虑这个因素，则可以创建一个预先分配好的工作者线程或进程池。要做到这一点，我们首先创建一个普通的（非多线程/多进程）服务器实例，但是之后在多个线程中调用 serve_forever()方法。示例如下：

```python
...  
if _name_ == __main__:  
    from threading import Thread  
NWORKERS = 16  
serv = TCPServer('', 20000), EchoHandler)  
for n in range (NWORKERS):  
    t = Thread(target=servServe_forever)  
    t.daemon = True  
    t.start()  
servServe_forever() 
```

一般来说，TCPServer 在实例化时就会绑定并激活底层的 socket。但是，有时候我们可能会想通过设定 socket 选项来调整底层 socket 的行为。要做到这一点，可以提供bind_and_activate=False 参数，就像这样：

```python
if __name__ == __main__:
    serv = TCPServer((‘', 20000), EchoHandler, bind_and Activate=False) 
```

```txt
Set up various socket options  
serv(socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)  
#Bind and activate  
serv.server_bind()  
serv.server Activate()  
servServe_forever() 
```

上面给出的 socket 选项实际上是一种非常常见的设置。它允许服务器重新对之前使用过的端口号进行绑定。由于这个设置实在是太常用了，因此在 TCPServer 类中也提供了一个完成相同功能的类变量，在实例化服务器之前设定它即可，如下所示：

```python
...  
if __name__ == __main__':  
    TCPServer.allow_reuse_address = True  
    serv = TCPServer(['', 20000), EchoHandler)  
    servServe_forever() 
```

在这个解决方案中，我们展示了两个不同的基类（BaseRequestHandler 和 StreamRequesthandler）。StreamRequestHandler 类实际上要更灵活一些，而且可以通过指定额外的类变量来提供一些功能。比如：

import socket   
class EchoHandler(StreamRequestHandler): # Optional settings ( defaults shown) timeout $= 5$ #Timeout on all socket operations rbufsize $= -1$ #Read buffer size wbbufsize $= 0$ #Write buffer size disable_nagle_algorithm $\equiv$ False #Sets TCP_NODELAY socket option def handle(self): print('Got connection from', self.client_address) try: for line in self.rfile: #self.wfile is a file-like object for writing self.wfile.write(line) except socket_timeout: print('Timed out!')

最后，应该要指出的是大部分 Python 中的高级网络模块（例如 HTTP、XML-RPC 等）都是在 socketserver 的功能之上构建的。也就是说，直接使用 socket 库来实现服务器也并不会太困难。下面这个简单的示例就是直接通过 socket 来编写服务器程序：

```python
from socket import socket, AF_INET, SOCK_STREAM 
```

```python
def echo_handler(address, client_sock): 
```

```python
print('Got connection from {}.format(address))  
while True:  
    msg = client SOCK.recv(8192)  
    if not msg:  
        break  
    client SOCK.sendall(msg)  
    client SOCK.close()  
def echo_server(address, backlog=5):  
    sock = socket(AF_INET, SOCK_STREAM)  
    sock.bind(address)  
    sock.listen(backlog)  
while True:  
    client SOCK, client_addr = sock.accept()  
    echo_handler(client_addr, client SOCK)  
if __name__ == '__main__':  
    echo_server(['', 20000)) 
```

# 11.3 创建一个 UDP 服务器

# 11.3.1 问题

我们想实现一个采用 UDP 协议同客户端进行通信的服务器。

# 11.3.2 解决方案

同 TCP 一样，利用 socketserver 库也能很容易地创建出 UDP 服务器。比如，下面有一个简单的时间服务器程序：

```python
from socketserver import BaseRequestHandler, UDPServer  
import time  
class TimeHandler(BaseRequestHandler):  
    def handle(self):  
        print('Got connection from', self.client_address)  
        # Get message and client socket  
        msg, sock = self.request  
        resp = time.ctime()  
        sock.sendto(resp.encode('ascii'), self.client_address)  
if __name__ == '__main__':  
    serv = UDPServer(['', 20000), TimeHandler)  
    serv.listen_forever() 
```

同上一节类似，这里需要定义一个特殊的处理类，其中要实现一个handle()方法来处理客户端的连接。这里的 request 属性是一个元组，包含了这个服务器收到的数据报以及代表底层的 socket 对象。client_address 包含的是客户端的地址。

要测试这个服务器程序，先运行上面的脚本，然后另外打开一个 Python 进程并向服务端程序发送消息：

```python
>>> from socket import socket, AF_INET, SOCK_DGRAM
>>> s = socket(AF_INET, SOCK_DGRAM)
>>> s.sendto(b'', ('localhost', 20000))
0
>>> s.recvfrom(8192)
(b'Wed Aug 15 20:35:08 2012', ('127.0.0.1', 20000))
>>> 
```

# 11.3.3 讨论

一个典型的UDP服务器程序会接收到发自客户端的数据报（消息）以及客户端的地址。如果服务端要响应请求，它就发回一个数据报给客户端。对于数据报的发送和传输，应该使用 socket 对象的 sendto()和 recvfrom()方法。尽管传统的 send()和 recv()方法也能工作，但是在 UDP 通信中前面两种方法更为常用一些。

由于 UDP 通信底层不需要建立连接，因此 UDP 服务器常常比 TCP 服务器要容易编写得多。但是同时 UDP 也是不可靠的（例如，由于没有建立连接，消息可能会丢失）。因此，如何处理消息丢失的任务就交给了你自己。这个主题超出了本书的范围，但是如果可靠性对于你的程序来说很重要，一般来说就要引入序列号、重传、超时以及其他的机制来确保传输的可靠性。UDP 常用在对可靠性传输要求不那么高的应用中。例如，在类似多媒体流应用以及游戏中常会用到 UDP，因为在这类应用中根本不会倒退回去试图重传某个丢失的数据包（程序会简单地忽略丢弃的包，然后继续运行）。

UDPServer 类也是单线程的，这意味着它一次只能处理一个请求。在实践中，这个问题与 TCP 连接相比要小很多。但是如果要实现并发操作，可以实例化 ForkingUDPServer或者 ThreadingUDPServer：

```python
from socketserver import ThreadingUDPServer
...
if __name__ == '__main__':
    serv = ThreadingUDPServer(['',20000), TimeHandler)
    servServe_forever() 
```

直接通过 socket 来实现 UDP 服务器同样也不复杂。示例如下：

```python
from socket import socket, AF_INET, SOCK_DGRAM  
import time 
```

```python
def time_server(address):
    sock = socket(AF_INET, SOCK_DGRAM)
    sock.bind(address)
while True:
    msg, addr = sock.recvfrom(8192)
    print('Got message from', addr)
    resp = time.time()
    sock.sendto(response.encode('ascii'), addr)
if __name__ == '__main__':
    time_server(['', 20000)) 
```

# 11.4 从 CIDR 地址中生成 IP 地址的范围

# 11.4.1 问题

我们有一个类似于“123.45.67.89/27”这样的 CIDR（Classless InterDomain Routing）网络地址，我们想生成由该地址可表示的全部 IP 地址的范围（例如，“123.45.67.64”,“123.45.67.65”...，“123.45.67.95”）。

# 11.4.2 解决方案

利用 ipaddress 模块可轻松处理这样的计算。示例如下：

```txt
>>> import ipaddress
>>> net = ipaddress.ip_network('123.45.67.64/27')
>>> net
IPv4Network('123.45.67.64/27')
>>> for a in net:
...     print(a)
...
123.45.67.64
123.45.67.65
123.45.67.66
123.45.67.67
123.45.67.68
...
123.45.67.95
>>> 
>>> net6 = ipaddress.ip_network('12:3456:78:90ab:cd:ef01:23:30/125')
>>> net6 
```

```txt
IPv6Network('12:3456:78:90ab:cd:ef01:23:30/125')  
>>> for a in net6:  
... print(a)  
...  
12:3456:78:90ab:cd:ef01:23:30  
12:3456:78:90ab:cd:ef01:23:31  
12:3456:78:90ab:cd:ef01:23:32  
12:3456:78:90ab:cd:ef01:23:33  
12:3456:78:90ab:cd:ef01:23:34  
12:3456:78:90ab:cd:ef01:23:35  
12:3456:78:90ab:cd:ef01:23:36  
12:3456:78:90ab:cd:ef01:23:37  
>>> 
```

network 对象同样也支持像数组那样的索引操作。示例如下：

```txt
>>> net.num_addresses  
32  
>>> net[0]  
IPv4Address('123.45.67.64')  
>>> net[1]  
IPv4Address('123.45.67.65')  
>>> net[-1]  
IPv4Address('123.45.67.95')  
>>> net[-2]  
IPv4Address('123.45.67.94')  
>>> 
```

此外，还可以执行检查成员归属的操作：

```txt
>>> a = ipaddress.ip_address('123.45.67.69')  
>>> a in net  
True  
>>> b = ipaddress.ip_address('123.45.67.123')  
>>> b in net  
False  
>>> 
```

IP 地址加上网络号可以用来指定一个 IP 接口（interface）。比如：

```txt
>>>INET = ipaddress.ipInterface('123.45.67.73/27')
>>>INET.network
IPv4Network('123.45.67.64/27')
>>>INET.ip
IPv4Address('123.45.67.73')
>>> 
```

# 11.4.3 讨论

ipaddress 模块中有一些类可用来表示 IP 地址、网络对象以及接口。如果要编写代码以某种方式来操作网络地址的话（比如解析、打印、验证等），这就显得非常有帮助了。

需要注意的是，ipaddress 模块同其他网络相关的模块比如 socket 库之间的交互是有局限性的。特别是，通常不能用 IPv4Address 的实例作为地址字符串的替代。相反，必须显式地通过 str()将其转换为字符串。示例如下：

```python
>>> a = ipaddress.ip_address('127.0.0.1')
>>> from socket import socket, AF_INET, SOCK_STREAM
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s.connect((a, 8080))
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: Can't convert 'IPv4Address' object to str implicitly
>>> s.connect((str(a), 8080))
>>> 
```

请参阅“ipaddress 模块介绍”（http://docs.python.org/3/howto/ipaddress.html）一文，以获得更多的信息和高级用法的示例。

# 11.5 创建基于 REST 风格的简单接口

# 11.5.1 问题

我们希望通过一个基于 REST 风格的简单接口来对程序实现远程控制或交互。但是，我们又不想为此去安装一个成熟的 Web 编程框架。

# 11.5.2 解决方案

构建基于 REST 风格的接口最简单的方式之一就是根据 WSGI规范（在 PEP 3333 中描述，地址为 http://www.python.org/dev/peps/pep-3333）创建一个小型的库。示例如下：

```python
# resty.py
importcgi
def notfound_404(environ, start_response):
    start_response('404 Not Found', ['Content-type', 'text/plain']) return [b'Not Found'] 
```

class PathDispatcher:   
```python
def __init__(self):
    self.pathmap = {}
def __call__(self, environ, start_response):
    path = environ['PATH_INFO']
    params = CGI.FieldStorage(environ['wsgi.input'],
                     environ=envviron)
    method = environ['REQUEST_METHOD'].lower()
    environ['params'] = { key: params.getvalue(key) for key in params}
    handler = self.pathmap.get(method, path), notfound_404)
    return handler(environ, start_response)
def register(self, method, path, function):
    self.pathmap(method.lower(), path] = function
    return function 
```

要使用这个调度器，只用编写不同的处理函数即可，比如：

import time   
helloResp $= \text{�} ^ { \prime \prime } \text{�}$ <html> <head> <title>Hello {name}</title> </head> <body> <h1>Hello {name}!</h1> </body>   
</html>'   
def hello_world(environ, start_response): start_response('200 OK', [('Content-type','text/html')]) params $=$ environ['params'] resp $=$ _hello_response.format(name=params.get('name')) yield resp.encode('utf-8')   
_localtimeResp $= \text{�} ^ { \prime \prime } \text{�}$ <?xml version="1.0"?>   
<time> <year>{t.tm_year}</year> <month>{t.tm_mon}</month> <day>{t.tm_mday}</day> <hour>{t.tm_hour}</hour> <minute]{t.tm_min}</minute> <second>{t.tm_sec}</second>

</time>''   
deflocaltime(environ,start_response): start_response('200OK'，['Content-type'，'application/xml')]) resp $=$ _localtimeResp.format(t=time.localtime()) yieldresp.encode('utf-8')   
if_name $= =$ 'main': from resty import PathDispatcher from wsgiref.simple_server import make_server #Create the dispatcher and register functions dispatcher $=$ PathDispatcher() dispatcher.register('GET'，'/hello'，hello_world) dispatcher.register('GET'，'/localtime'，localtime) #Launch a basic server httpd $=$ make_server(''，8080，dispatcher) print('Serving on port 8080...') httpdserve_forever()

要测试这个服务程序，可以通过浏览器或者使用 urllib 来完成交互。示例如下：

```html
>>> u = urlopen('http://localhost:8080/hello?name=Guido')
>>> print(u.read().decode('utf-8'))
<html>
<head>
<title>Hello Guido</title>
</head>
<body>
<h1>Hello Guido!</h1>
</body>
</html>
>>> u = urlopen('http://localhost:8080/localtime')
>>> print(u.read().decode('utf-8'))
<?xml version="1.0"?>
<time>
<year>2012</year>
<month>11</month>
<day>24</day>
<hour>14</hour>
<minute>49</minute>
<second>17</second>
</time>
>>> 
```

# 11.5.3 讨论

在基于 REST风格的接口中，一般来说就是在编写响应常见HTTP 请求的程序。但是，与一个成熟的网站不同，通常我们只是在来回推送数据。这个数据可能会以各种标准的格式进行编码，比如 XML、JSON 或者 CSV。尽管看起来似乎微不足道，但是以这种方式提供 API 对于各种各样的应用程序都是非常有用的。

比如，长时间运行的程序可能会用 REST 风格的 API 来实现监控或诊断功能。大数据应用可以使用 REST 风格的接口来构建一个查询/提取数据的系统。REST 甚至可以用来控制硬件设备，比如机器人、传感器或者是灯泡。此外，REST API 在各式各样的客户端编程环境中都得到了很好的支持，比如 JavaScript、Android、iOS等。因此，有了这样的接口能够鼓励人们开发出更加复杂的应用来使用你的接口代码。

要实现一个简单的 REST 风格的接口，通常只要根据 Python 的 WSGI 标准来做就可以了。标准库是支持 WSGI 的，而且大多数第三方的 Web 框架也支持。因此，如果采用WSGI 标准的话，我们的代码使用起来将变得非常灵活。

在 WSGI 中，应用程序是以一个接受如下调用形式的可调用对象来实现的：

```python
importcgi   
def wsgi_app(env, start_response): 
```

参数 environ是一个字典，其中需要包含的值参考了许多Web 服务器比如Apache 所提供的 CGI 接口的启发。要提取出不同的字段，可以编写这样的代码来实现：

```python
def wsgi_app(environ, start_response):
    method = environ['REQUEST_METHOD']
    path = environ['PATH_INFO']
    # Parse the query parameters
    params =cgi.FieldStorage(environ['wsgi.input'], environ=envviron)
    ... 
```

示例中展示了一些常见的值。environ['REQUEST_METHOD']表示请求的类型（例如GET、POST、HEAD 等）。environ['PATH_INFO']表示所请求资源的路径。调用cgi.FieldStorage()可以从请求中提取出所提供的查询参数，并将它们放入到一个类似于字典的对象中以供稍后使用。

参数 start_response 是一个函数，必须调用它才能发起响应。start_response 的第一个参数是 HTTP 结果状态。第二个参数是一个元组序列，以(name, value)这样的形式组成响应的 HTTP 头。示例如下：

```python
def wsgi_app(environ, start_response):
    ...
    start_response('200 OK', ['Content-type', 'text/plain']) 
```

要返回数据，满足 WSGI 规范的应用程序必须返回字节串序列。这可以通过使用一个列表来完成：

```python
def wsgi_app(environ, start_response):
    ...
    start_response('200 OK', ['Content-type', 'text/plain'])) 
resp = []
resp.append(b'Hello World\n')
resp.append(b'Goodbye!\n')
return resp 
```

此外，也可以使用 yield 作为替代方案：

```python
def wsgi_app(environ, start_response):
    ...
    start_response('200 OK', ['Content-type', 'text/plain'])) yield b'Hello World\n'
    yield b'Goodbye!\n' 
```

需要重点强调的是，返回的结果必须使用字节串的形式。如果响应是由纯文本组成的，那就需要先将其编码为字节形式。当然了，这里并没有要求返回的结果是文本，因此可以轻松编写一个应用程序来生成图像。

尽管遵循 WSGI 规范的应用程序通常都被定义为函数，就如我们的示例那样，但是类实例同样也是可行的，只要它实现了合适的__call__()方法即可。示例如下：

```python
class WSGIAplication: def__init__(self): def__call__(self,environ,start_response) 
```

在本节中，我们已经采用这种技术创建了 PathDispatcher 类。这个调度器只负责管理一个字典，用来将方法和路径的映射关系传递给处理函数，除此之外什么都不做。当有请求到来时，提取出方法和路径然后将其分发给一个处理函数。此外，任何查询变量都会得到解析并放入到字典中以 environ['params']的形式保存（这个步骤非常常见，为了避免产生大量重复的代码，在调度器中统一处理是很有意义的）。

要使用这个调度器，只要创建一个实例并注册各种基于 WSGI 风格的应用程序函数到调度器中即可，就像本节示例中的那样。编写这些函数应该是非常直接的，只要遵循start_response()函数的规则并且以字节串作为输出即可。

当编写这样的函数时，对于字符串模板的使用需要特别小心。没人喜欢和一堆由 print()函数、XML以及各式各样的格式化操作杂揉在一起的代码打交道。在我们的解决方案中，采用的方式是定义三引号式的字符串模板然后在内部使用。这种方式使得之后修改输出的格式变得更加简单了（只要修改模板即可，而不用到处去修改代码）。

最后，使用 WSGI 的一个重要因素就是实现中没有任何部分是特定于某个具体的 Web服务器的。这正是 WSGI 规范所代表的全部意义——由于 WSGI 是与服务器以及框架无关的，我们应该可以将自己的应用程序部署到各式各样的服务器之上。在本节中，可以使用下面的代码来测试：

```python
if _name_ == 'main':
from wsgiref.simple_server import make_server
# Create the dispatcher and register functions
dispatcher = PathDispatcher()
...
# Launch a basic server
httpd = make_server('', 8080, dispatcher)
print('Serving on port 8080...')
httpdserve_forever() 
```

这样会创建出一个简单的服务器，可以用它来检查我们的实现是否都能正常工作。之后，当准备好将应用的规模扩展到更高的层次时，我们就可以修改实现代码让它工作在某个特定的服务器上。

WSGI 被有意设计为功能最简化的规范。比如，它不提供任何类似认证、cookies、重定向等高级功能的支持。这些功能由你自己实现并不算困难。但是，如果想得到更多的支持，可以考虑一些第三方的库，比如 WebOb（http://webob.org）或者 Paste（http://pythonpaste.org）。

# 11.6 利用 XML-RPC 实现简单的远端过程调用

# 11.6.1 问题

我们希望能有一种简单的方法可以在远端机器上运行的 Python 程序中执行函数或者方法。

# 11.6.2 解决方案

也许实现一个远端过程调用机制最简单的方式就是使用 XML-RPC 了。下面这个例子给出了一个简单的服务器，其中实现了键—值对的存储：

from xmlrpc.server import SimpleXMLRPCServer   
class KeyValueServer:   
rpc_methods_ $=$ ['get'，'set'，'delete'，'exists'，'keys'] def init_(self, address): self._data $\equiv$ {} self._serv $=$ SimpleXMLRPCServer(address, allow_none=True) for name in self._rpc_methods_: self._serv.register_function(getattr(self, name)) def get(self, name): return self._data[name] def set(self, name, value): self._data[name] $=$ value def delete(self, name): del self._data[name] def exists(self, name): return name in self._data def keys(self): return list(self._data) def serve_forever(self): self._servServe_forever() #Example if_name $= =$ 'main': kvserv $=$ KeyValueServer(''，15000） kvservserve_forever()

下面的代码展示了如何从客户端远程访问服务器：  
```python
>>> from xmlrpc.client import ServerProxy
>>> s = ServerProxy('http://localhost:15000', allow_none=True)
>>> s.set('foo', 'bar')
>>> s.set('spam', [1, 2, 3])
>>> s.keys()
['spam', 'foo']
>>> s.get('foo')
'bar'
>>> s.get('spam')
[1, 2, 3]
>>> s.delete('spam') 
```

```txt
>>>s_exists('spam') False   
>>> 
```

# 11.6.3 讨论

要配置一个简单的远端过程调用服务，可以用 XML-RPC 来轻松实现。所有要做的就是创建一个服务器实例，通过 register_function()方法注册处理函数，然后通过serve_forever()方法加载即可。本节给出的示例把所有的代码集合到了一起，将这些步骤打包到了一个类中，但是实际上并没有这个硬性要求。比如，我们可以创建一个这样的服务器：

```python
from xmlrpc.server import SimpleXMLRPCServer  
def add(x,y):  
    return x+y  
serv = SimpleXMLRPCServer(['', 15000))  
serv.register_function(add)  
servServe_forever() 
```

通过 XML-RPC 暴露出的函数只能处理几种特定类型的数据，比如字符串、数字、列表和字典。至于其他类型的数据则需要做进一步的研究。比方说，如果通过 XML-RPC传递一个实例，则只有它的实例字典会被处理：

```prolog
>>>class Point:   
def__init__(self,x,y): self.x=x   
... self.y=y   
p=Point(2,3)   
s.set('foo',p)   
s.get('foo')   
{'x':2,'y':3}   
>>> 
```

同样地，对二进制数据的处理也和我们期望的方式有所不同：

```txt
>>>s.set('foo'，b'HelloWorld')   
>>>s.get('foo')   
\xmlrpc.client(Binaryobjectat0x10131d410>   
>>>_.data   
b'HelloWorld'   
>>> 
```

作为一般的规则，不应该将 XML-RPC 服务作为公有 API 暴露给外部世界。通常，最佳应用场景是在内部网络中，我们可以编写涉及几台不同机器的简单的分布式应用

程序。

XML-RPC 的缺点在于它的性能。SimpleXMLRPCServer 是以单线程来实现的，尽管可以通过 11.2 节所示的方法配置为以多线程方式运行，但还是不适合用来扩展大型的应用。此外，由于 XML-RPC会将所有的数据序列化为XML 格式，因此就会比其他的方法要慢一些。但是，这种编码的优势在于许多其他的编程语言都能够理解。使用XML-RPC 的话，客户端程序就可以采用 Python 之外的语言来编写，同样可以访问你的服务。

抛开 XML-RPC 的局限性不说，如果需要以快速但并不完善（quick and dirty）的方式实现一个远端过程调用系统，那么了解一下 XML-RPC 还是很值得的。很多时候简单的方案就已经足够好了。

# 11.7 在不同的解释器间进行通信

# 11.7.1 问题

我们正运行着多个 Python 解释器的实例，有可能还是在不同的机器上。我们想通过消息在不同的解释器之间交换数据。

# 11.7.2 解决方案

如果使用 multiprocessing.connection 模块，那么在不同的解释器之间实现通信就很简单了。下面是一个实现了 echo 服务的简单示例：

```python
from multiprocessing.Connection import Listener  
import traceback  
def echo_client(conn):  
    try:  
        while True:  
            msg = conn.recv()  
            conn.send(msg)  
except EOFError:  
    print('Connection closed')  
def echo_server(address, authkey):  
    serv = Listener(address, authkey=authkey)  
while True:  
    try:  
        client = serv.accept()  
        echo_client(conn)  
except Exception: 
```

traceback.print exc() echo_server(''，25000)，authkey $\equiv$ b'peekaboo')

下面的客户端连接到服务器上并发送各种消息：

```python
>>> from multiprocessing.Connection import Client
>>> c = Client('localhost', 25000), authkey=b'peekaboo')
>>> c.send('hello')
>>> c.recv()
'hello'
>>> c.send(42)
>>> c.recv()
42
>>> c.send([1, 2, 3, 4, 5])
>>> c.recv()
[1, 2, 3, 4, 5]
>>> 
```

和低级的 socket不同，这里所有的消息都是完整无损的（每个由 send()发送的对象都会通过 recv()完整地接收到）。此外，对象都是通过 pickle 来进行序列化的。因此，任何同 pickle 兼容的对象都可以在连接之间传递和接收。

# 11.7.3 讨论

有许多软件包和库都实现了各种形式的消息传递，比如 ZeroMQ、Celery 等。作为备用方案，我们可能也会倾向于在底层的socket之上实现一个消息层。但是，有时候我们只想要一个简单的解决方案。multiprocessing.connection 库正是我们所需要的——只需要使用几个简单的原语（primitive），就能轻易地将各个解释器联系在一起并且在它们之间交换消息。

如果知道这些解释器会运行在同一台机器上，那么可以利用网络作为替代方案，比如UNIX 域 socket 或者 Windows 上的命名管道。要通过 UNIX 域 socket 创建连接，只要简单地将地址改为文件名即可，示例如下：

```txt
s = Listener('/tmp/myconn', authkey=b'peekaboo') 
```

要通过 Windows 命名管道创建连接，可以使用下面这样的文件名：

```python
s = Listener(r'\.\pipe\myconn', authkey=b'peekaboo') 
```

作为一般的规则，不应该使用 multiprocessing 模块来实现面向公众型的服务。传递给Client()和 Listener()的参数 authkey 在这里是为了帮助认证连接的对端节点。用错误的密钥来建立连接会产生异常。此外，这个模块最好适用于能长时间运行的连接（而不是大量的短连接）。例如，两个解释器可能会在开始的时候建立一条连接，然后在整个过程中都保持这个连接的活跃。

如果需要对连接实现更多的底层控制，那么就不要使用 multiprocessing 模块。比方说，如果要支持超时、非阻塞 I/O 或者任何类似的特性，那么最好使用另一个不同的库或者直接在 socket之上实现这些特性。

# 11.8 实现远端过程调用

# 11.8.1 问题

我们想在 socket、multiprocessing.connection 或者 ZeroMQ 这样的消息传递层之上实现简单的远端过程调用（RPC）。

# 11.8.2 解决方案

通过将函数请求、参数以及返回值用 pickle 进行编码，然后在解释器之间传递编码过的 pickle 字节串，RPC 是很容易实现的。下面的示例是一个简单的 RPC 处理例程，可以将其纳入到一个服务器程序中使用。

#rpcserver.py   
import pickle   
classRPCHandler: def__init__(self): self._functions $= \{\}$ defregister_function(self，func): self._functions[func._name_] $=$ func defhandle_connection(self,connection): try: while True: #Receive amessage func_name，args，kwrags $=$ pickle LOADs(connection.recv()) #Run the RPC and send a response try: r $=$ self._functions[func_name](\*args,\*\*kwrags) connection.send(pickle.dumps(r)) except Exception as e: connection.send(pickle.dumps(e)) except EOFError: pass

要使用这个处理例程，需要将其添加到一个消息服务器中。这里我们可以有多种选择，但相较而言 multiprocessing 库是一种简单的选择。下面是 RPC 服务器的示例：

from multiprocessing.Connection import Listener   
from threading import Thread   
defrpc_serverhandler,address，authkey): sock $=$ Listener(address，authkey $\equiv$ authkey) while True: client $=$ sock.accept() t $=$ Thread(target $\equiv$ handler.handle_connection，args=(client,)） t.daemon $=$ True t.start()   
# Some remote functions   
defadd(x,y): return x + y   
defsub(x,y): return x - y   
#Register with a handler   
handler $=$ RPCHandler() handler.register_function(add) handler.register_function(sub)   
#Run the server   
rpc_serverhandler，('localhost'，17000)，authkey $\equiv$ b'peekaboo')

要从远端的客户端中访问这个服务器，需要创建一个相应的RPC代理类来转发请求。示例如下：

import pickle   
class RPCProxy: def_init_self,connection): self._connection $=$ connection def_getattr_self,name): defdo_rpc(\*args,\*\*kwargs): self._connection.send(pickle.dumps((name，args,kkwargs))) result $=$ pickle.loadseelf._connection.recv()) ifisinstance(result,Exception): raise result return result return do_rpc

要使用这个代理类，只需要用它来包装发送给服务器端的连接即可。示例如下：

```python
>>> from multiprocessing.Connection import Client
>>> c = Client('localhost', 17000), authkey='b'peekaboo') 
```

```txt
>>> proxy = RPCProxy(c)
>>> proxy.add(2, 3)
5
>>> proxy.sub(2, 3)
-1
>>> proxy.sub([1, 2], 4)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "rpcserver.py", line 37, in do_rpc
    raise result
TypeError: unsupported operand type(s) for -: 'list' and 'int'
>>> 
```

应该指出的是，有许多消息处理层（比如 multiprocessing）已经用 pickle 将数据做了序列化处理。如果是这样的话，可以去掉对 pickle.dumps()和 pickle.loads()的调用。

# 11.8.3 讨论

RPCHandler 和 RPCProxy 类的总体思想相对来说都是比较简单的。如果客户端想调用一个远端函数，比如 foo(1, 2, $\scriptstyle \mathbf { Z } = 3 )$ )，代理类就创建出一个包含了函数名和参数的元组('foo', (1, 2), {'z':3})。这个元组经 pickle 序列化处理后通过连接发送出去。这些步骤都是在 RPCProxy 类__getattr__()方法返回的闭包 do_rpc()中执行的。服务器端接收到消息后执行反序列化处理，然后检查函数名是否已经注册过了。如果是注册过的函数，就用给定的参数调用该函数。把得到的结果（或者异常）进行 pickle 序列化处理然后再发送回去。

我们给出的示例依赖于 multiprocessing 模块来完成通信。但是，这种方法也适用于任何其他的消息通信系统。比如，如果想在 ZeroMQ 上实现 RPC，只要把连接对象用适当的 ZeroMQ socket 对象取代即可。

关于 pickle 的可靠性，需要重点考虑安全方面的问题（因为聪明的黑客可以创建出特定的消息，使得在执行反序列化处理时得以执行任意的函数）。特别是，绝对不能允许非受信任或者非授权的客户端执行 RPC 操作，我们肯定不希望对互联网上的任何机器都敞开大门。本节提到的技术应该只在位于防火墙之后的内部网络中使用，不要暴露给外部世界。

作为 pickle 的替代方案，我们可能会考虑使用 JSON、XML 或一些其他的数据编码来完成序列化操作。比如，如果用 json.loads()和 json.dumps()来取代本节中的 pickle.loads()和 pickle.dumps()的话，那么就可以轻松应用 JSON 编码了。示例如下：

```markdown
# jsonrpcserver.py
import json
class RPCHandler: 
```

```python
def __init__(self):
    self._functions = {}
def register_function(self, func):
    self._functions[func._name_] = func
def handle_connection(self, connection):
    try:
        while True:
            # Receive a message
            func_name, args, kwargs = json.dumps(connection.recv())
            # Run the RPC and send a response
            try:
                r = self._functions[func_name] (*args, **kwargs)
                connection.sendjson.dumps(str(e)))
            except Exception as e:
                connection.sendjson dumps(str(e)))
            except EOFError:
                pass
# jsonrpcclient.py
import json
class RPCProxy:
    def __init__(self, connection):
        self._connection = connection
def __getattr__(self, name):
    def do_rpc(*args, **kwargs):
        self._connection.sendjson dumps(name, args, kwargs))
        result = json loads(self._connection.recv())
        return result
    return do_rpc 
```

在实现 RPC 时一个比较复杂的问题是对待异常应该如何处理？最基本的要求是如果调用某个方法时抛出了异常，服务器不会因此而崩溃。但是，如何将异常信息回传给客户端则需要好好思考一番。如果正在使用 pickle，通常异常实例会经过序列化处理之后再在客户端重新抛出。如果在使用其他的编码协议，那就要考虑其他的方法了。至少，应该在响应中返回异常信息字符串。上面使用 JSON 编码的示例采用的正是这种方式。

有关 RPC 实现的另一个例子，看看在 XML-RPC 中使用的 SimpleXMLRPCServer 和ServerProxy 类的实现是很有帮助的，我们在 11.6 节已经描述过了。

# 11.9 以简单的方式验证客户端身份

# 11.9.1 问题

我们希望有一种简单的方式可以对在分布式系统中连接到各个服务器上的客户端进行身份验证，但是又不想使用像 SSL 那样的复杂组件。

# 11.9.2 解决方案

我们可以利用 hmac 模块实现一个握手连接来达到简单且高效的身份验证目的。下面是示例代码：

import hmac   
import os   
def client authenticateconnection,secret_key): 'Authentication client to a remote service. connection represents a network connection. secret_key is a key known only to both client/server.. ' message $=$ connection.recv(32) hash $\equiv$ hmac.new_secret_key, message) digest $=$ hashdigest() connection.send(digest)   
def server authenticateconnection,secret_key): Request client authentication. 'message $=$ os.urandom(32) connection.send(message) hash $\equiv$ hmac.new_secret_key, message) digest $=$ hashdigest() response $=$ connection.recv(len(digest)) return hmac(compare>digest,digest, response)

总体思路就是在发起连接时，服务器将一段由随机字节组成的消息发送给客户端（在本例中是由 os.urandom()返回的）。客户端和服务器通过 hmac 模块以及双方事先都知道的密钥计算出随机数据的加密 hash。客户端发送它计算出的摘要值（digest）给服务器，而服务器对摘要值进行比较，以此来决定是要接受还是拒绝这个连接。

对摘要值进行比较需要使用 hmac.compare_digest()函数。这个函数的实现可避免遭受基

于时序分析的攻击（timing-analysis attack），因此应该用它来对摘要值进行比较而不能用普通的比较操作符 $\scriptstyle ( = =$ ）。

要使用这些函数，我们可以将它们合并到已有的有关网络或消息处理的代码中。比如，如果用到了 socket，服务器端的代码看起来就是这样的：

from socket import socket, AF_INET, SOCK_STREAM   
secret_key $\equiv$ b'peekaboo'   
def echo_handler client_sock): if not server authenticate(client_sock, secret_key): client_sock.close() return while True: msg $=$ client_sock.recv(8192) if not msg: break client_sock.sendall(msg)   
def echo_server(address): s $=$ socket(AF_INET,SOCK_STREAM) s.bind(address) s.listen(5) while True: c,a $=$ s.accept() echohandler(c)   
echo_server('(','18000))

而在客户端，则可以这样处理：

```python
from socket import socket, AF_INET, SOCK_STREAM  
secret_key = b'peekaboo'  
s = socket(AF_INET, SOCK_STREAM)  
s.connect(['localhost', 18000])  
client authenticate(s, secret_key)  
s.send(b'Hello World')  
resp = s.recv(1024)  
... 
```

# 11.9.3 讨论

在内部消息系统以及进程间通信中常常会用 hmac 来验证身份。比如，如果正在编写的

系统需要实现跨集群的多进程间通信，就可以使用这种方法来确保只有获得许可的进程才能互相通信。事实上，在 multiprocessing 库中，当同子进程建立通信时在内部也是使用的基于 hmac 的身份验证方式。

需要重点强调的是，验证某个连接和加密连接可不是一回事。在经过验证的连接上，后续的通信都是以明文发送的，对于任何企图嗅探流量的人来说，这些消息都是可见的（尽管完成验证所需的密钥从来都没有传送过）。

hmac 所采用的身份验证算法是基于加密哈希函数的，比如 MD5 和 SHA-1，这些算法的细节描述可以在 IETF RFC 2104（http://tools.ietf.org/html/rfc2104.html）中找到。

# 11.10 为网络服务增加 SSL 支持

# 11.10.1 问题

我们想通过 socket 实现一个网络服务，要求服务器端和客户端可以通过 SSL 实现身份验证，并且对传输的数据进行加密。

# 11.10.2 解决方案

ssl 模块可以为底层的 socket 连接添加对 SSL 的支持。具体来说就是 ssl.wrap_socket()函数可接受一个已有的 socket，并为其包装一个 SSL 层。比方说，下面的示例展示了一个简单的 echo 服务，服务器对发起连接的客户端提供了一个服务器证书：

```python
from socket import socket, AF_INET, SOCK_STREAM
import ssl
KEYFILE = 'server_key.pem' # Private key of the server
CERTFILE = 'server_cert.pem' # Server certificate (given to client)
def echo_client(s):
    while True:
        data = s.recv(8192)
        if data == b': break
            s.send(data)
        close()
    print('Connection closed')
def echo_server(address):
    s = socket(AF_INET, SOCK_STREAM)
    s.bind(address)
    s.listen(1) 
```

Wrap with an SSL layer requiring client certsc   
s_SSL $=$ ssl unwrap_socket(s, keyfile $\equiv$ KEYFILE, certificate $\equiv$ CERTFILE, server_side $\equiv$ True ） #Wait for connections   
while True: try: c,a $=$ s_SSL.accept() print('Got connection',c,a) echo_client(c) except Exception as e: print({}：{}'.format(e._class._.name_e)) echo_server(''，20000))

下面的交互式会话展示了客户端是如何连接到服务器的。客户端要求服务器出示自己的证书并完成验证。

```python
>>> from socket import socket, AF_INET, SOCK_STREAM
>>> import ssl
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s_SSL = ssl+Wrapped_socket(s, cert_reqs=ssl.CERT_REQUIRED,
... ca_certs = 'server_cert.pem')
>>> s_SSL.connect(['localhost', 20000])
>>> s_SSL.send(b'Hello World?')
12
>>> s_SSL.recv(8192)
b'Hello World'
>>> 
```

这些底层的 socket 技巧所带来的问题在于，它们无法和已经通过标准库实现的网络服务很好地结合在一起。比方说，大部分的服务器端代码（HTTP、XML-RPC 等）实际上是基于 socketserver 库来实现的。客户端代码也是在更高的层面上来实现的。为已有的服务添加对 SSL 的支持是可以实现的，但是需要的方法稍有不同。

首先，对于服务器端来说可以通过混入类（mixin class）来添加对 SSL 的支持：

```python
import ssl   
class SSLMixin: Mixin class that adds support for SSL to existing servers based 
```

on the socketserver module.   
def init_self, \*args, keyfile $\equiv$ None, certfile $\equiv$ None, ca_certs $\equiv$ None, cert_reqs $\equiv$ ssl.NONE, \*\*kwargs): self._keyfile $\equiv$ keyfile self._certfile $\equiv$ certfile self._ca_certs $\equiv$ ca_certs self._cert_reqs $\equiv$ cert_reqs super().init_(*args, \*\*kwargs)   
def get_request(self): client,addr $=$ super().get_request() client_SSL $\equiv$ ssl wraps_SOCKET(client, keyfile $\equiv$ self._keyfile, certfile $\equiv$ self._certfile, ca_certs $\equiv$ self._ca_certs, cert_reqs $\equiv$ self._cert_reqs, serverSide $\equiv$ True) return client_SSL,addr

要使用这个混入类，可以将它和别的服务器类混合在一起使用。比如，下面的示例定义了一个运行在 SSL 之上的 XML-RPC服务器：

# XML-RPC server with SSL

from xmlrpc.server import SimpleXMLRPCServer

class SSLSimpleXMLRPCServer(SSLMixin, SimpleXMLRPCServer): pass

下面的 XML-RPC 服务器代码取自 11.6 节，只做了些许修改以支持 SSL：

import ssl   
from xmlrpc.server import SimpleXMLRPCServer   
from sslmixin import SSLmixin   
class SSLSimpleXMLRPCServerSSLmixin, SimpleXMLRPCServer): pass   
class KeyValueServer: _rpc_methods_ $=$ ['get', 'set', 'delete', 'exists', 'keys'] def __init__(self, \*args, \*\*kwargs): self._data $=$ {} self._serv $=$ SSLSimpleXMLRPCServer(\*args, allow_none $=$ True, \*\*kwargs)

```python
for name in self._rpc_methods:
    self._serv.register_function(getattr(self, name))
def get(self, name):
    return self._data[name]
def set(self, name, value):
    self._data[name] = value
def delete(self, name):
    del self._data[name]
def exists(self, name):
    return name in self._data
def keys(self):
    return list(self._data)
def serve_forever(self):
    self._servServe_forever()
if __name__ == __main__ :
KEYFILE='server_key.pem' # Private key of the server
CERTFILE='server_cert.pem' # Server certificate
kvserv = KeyValueServer(['', 15000),
keyfile=KEYFILE,
certfile=CERTFILE)
kvservServe_forever() 
```

要使用这个服务器，可以利用 xmlrpc.client 模块来完成连接。只要在 URL 中指定一个https:即可。示例如下：

```txt
>>> from xmlrpc.client import ServerProxy
>>> s = ServerProxy('https://localhost:15000', allow_none=True)
>>> s.set('foo','bar')
>>> s.set('spam', [1, 2, 3])
>>> s.keys()
['spam', 'foo']
>>> s.get('foo')
'bar'
>>> s.get('spam')
[1, 2, 3]
>>> s.delete('spam')
>>> s_exists('spam')
False
>>> 
```

SSL 客户端中一个比较复杂的问题在于如何执行额外的步骤来验证服务器证书，或者向服务器展示客户端的凭证（比如客户端证书）。不幸的是，似乎并没有标准的方法来完成这些任务，因此常常需要做一点研究。下面的示例展示了如何建立一条安全的XML-RPC连接来验证服务器证书：

from xmlrpc.client import SafeTransport, ServerProxy   
import ssl   
class VerifyCertSafeTransport (SafeTransport): def __init__(self, caffe, certfile $\equiv$ None, keyfile $\equiv$ None): SafeTransport._init_(self) self._ssl_context $=$ ssl_SSLContextSSL.PROTOCOLTLSv1) self._ssl_context.load_checklocations(cafile) if cert: self._ssl_context.load_cert_chain(certfile, keyfile) self._ssl_contextverify_mode $\equiv$ ssl.CERT_REQUIRED   
def make_connection(self, host): #Items in the passed dictionary are passed as keyword # arguments to the http.client.HTPSConnection() constructor. # The context argument allows an ssl SSLContext instance to # be passed with information about the SSL configuration s $=$ super().make_connection((host,{'context':self._ssl_context})) return s   
# Create the client proxy   
s $=$ ServerProxy('https://localhost:15000', transport=VerifyCertSafeTransport('server_cert.pem'), allow none $\equiv$ True)

我们前面给出的解决方案中是由服务器端向客户端发送证书，然后客户端来验证。其实，这个验证步骤可以在两个方向上进行（即，服务器到客户端、客户端到服务器）。如果服务器想要验证客户端，只要把启动服务器的代码修改为如下形式即可：

```python
if _name_ == 'main':
KEYFILE='server_key.pem' # Private key of the server
CERTFILE='server_cert.pem' # Server certificate
CA_Certs='client_cert.pem' # Certificates of accepted clients
kvserv =的重要性Server(['', 15000),
keyfile=KEYFILE,
certfile=CERTFILE,
ca_certs=CA_CERTS,
cert_reqs=ssl.CERT_REQUIRED, 
```

```cmake
kvservserve_forever() 
```

要让 XML-RPC 客户端发出自己的证书，需要把 ServerProxy 的初始化修改为如下方式：

Create the client proxy   
s $=$ ServerProxy('https://localhost:15000', transport $\equiv$ VerifyCertSafeTransport('server_cert.pem', 'client_cert.pem', 'client_key.pem'), allow_none=True)

# 11.10.3 讨论

要让本节中提到的技术能正常运转，这对于我们的系统配置能力和对 SSL 的理解都将是一场考验。也许最大的挑战就是按顺序获取密钥的初始配置、证书以及其他一些相关的细节。

我们来理清一下这里的需求，SSL 连接的每个端点一般来说都有一个私有的密钥和一个签名的证书文件。证书文件中包含有公有密钥，在每个连接中会发送给对端节点。对于面向大众的服务，证书一般会由证书授权机构如 Verisign、Equifax 或者类似的组织（需要付费的机构）来签名。要验证服务器端的证书，客户端会维护一个文件，其中包含有受信任的证书颁发机构所发布的证书。比方说，Web 浏览器维护着与主要的证书颁发机构相对应的证书，并且会在 HTTPS 连接中用这些证书来验证由 Web 服务器发送过来的证书的完整性。

为了验证本节提到的技术，可以创建一个被称之为自签名的证书。可以这样来实现：

```txt
bash %openssl req -new -x509 -days 365 -nodes -out server_cert.pem \
-keyout server_key.pem
Generating a 1024 bit RSA private key
...
>>> ++++
You are about to be asked to enter information that will be incorporated into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:US
State or Province Name (full name) [Some-State]:Illinois
Locality Name (eg, city) []:Chicago 
```

```txt
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Dabeaz, LLC   
Organizational Unit Name (eg, section) []:   
Common Name (eg, YOUR name) []:localhost   
Email Address []:   
bash % 
```

当创建证书时，各个字段的值常常是随机的。但是，“Common Name”字段通常会包含DNS服务器的主机名。如果只是在自己的机器上做下测试，可以用“localhost”来替代。否则就用运行服务器程序的机器域名。

这样配置的结果就是我们将得到一个 server_key.pem 文件，其中包含了私钥。它看起来是这样的：

```pem
-----BEGIN RSA PRIVATE KEY-----  
MIICXQIBAAKBgQCZrCNLoEyAKF+f9UNcFaz5Osa6jf7qkbU18si5xQrY3ZYC7juu  
nLldZLn/VbEFIITaUOqvBtPv1qUWTJGwga62VSG1oFE0ODIx3g2Nh4sRf+rySsx2  
L4442nx0z40vJQ7k6eRNHAUUnCL50+YvjyLyt7ryLSjSuKhCcJsbZgPwIDAQAB  
AoGAB5evrr7eyL4160tM5rHTeATlaLY3UBOe5Z8NX8Z6qLib/ucSX9AysviVD/6F  
3oD6z2aL8jbeJc1vHqjt0dC2dwmm32vVL8mRdyoAsQpWmiqXrkvpP4Bs104VpBeHw  
Qt8NSW9SFhceL3LEvw9M8i9MV39viih1ILyH8OuHdvJyFECQQDLEjl2d2ppxND9  
PoLqVFAirDfX2JnLTdWbc+M1a9Jdn3hKF8TcxfEnFVs5GavlMusicY5KB0ylYPb  
YbTvqKc7AkEAwbnRBO2VYEzSJZp2X0IZqP9ovWokkpYx+PE4+c6MySDgaMcigL7v  
WDIHJG1ChudD09GbENasDzyb2HAIW4CzQBAKDdkv+xow6gJx42Auc2WzTcUHCA  
exR/+BLpPrhKyzbvOQ8YvS5764SU0lu1LWs3G+wnRMvrRvlMCZKgggBjkCQQCG  
Jewto2+a+WkOKQXrNNScDE5aPTmZQc5waCYq4UmCZQc0jkUOiuN3ST1U5iuxRqfb  
V/yX6fw0qh+fLWtkOs/JAKA+okMSxZwqRtfogOFGBfWQ8/iKrnizeanTQ3L6scFXI  
CHZXdJ3XQ6umNxN7iJ7S/LDawolQfWkCfD9FYoxBlg  
-----END RSA PRIVATE KEY----- 
```

而在 server_cert.pem 中的服务器端证书看起来也很类似：

```pem
-----BEGIN CERTIFICATE-----  
MIIC+DCCAmGgAwIBAgIJAPMd+vi45js3MA0GCSqGISb3DQEBBBQUAMFwxCzAJBgNV BAYTALVTMREwDwyDVQQIEwhJbGxpbm9pzEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIG A1UEChMLRGFIZWF6LCBMTEMxEjAQBgNVBAMTCWxyV2FsaG9zdDAeFw0xMzAxMTEx ODQyMjdaFW0NDAXTExODQyMjdaMFwxCzAJBgNVBAYTAIVTMREwDwyDVQQIEwhJ bGxpbm9pzEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIGA1UECHMLRGFIZWF6LCBMTEMxEjAQBgNVBAMTCWxyV2FsaG9zdDCBnzANBqkqhkiG9w0BAQEFAAOBJQAwgYkCqYEa mawjs6BMgChfn/VDXBWs+TrGuo3+6pG1JfLIucUK2N2WAu47rpy9XWS5/1WxBSC E1DoLwbT79alFkyRsIGut1UhtaBRNDgyMd4NjYeLEX/q8krMdi+OONp8dM+DubyU O5OnkTRwGVFJwi+dPmL48i8re68i0orioQnCbGZyD8CAwEAAAOBwTCBvjAdBgNV HQ4EFgQUrtoLHHGxDiZTr26NMmgKJLJLFtIgwY4GA1UdIWBhjCBg4ArUroLHHGx iDZTr26NMmgKJLJLFtKhYKReMFwxCzAJBgNVBAYTAIVTMREwDwyDVQQIEwhJbGxp bm9pczEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIGA1UEChMLRGFIZWF6LCBMTEMxEjAQ BgNVBAMTCWxyV2FsaG9zdIIJAPMd+vi45js3MAwGA1UdEwQFMAMBAf8wDQYJKoZ I hvcNAQEFBQADgYEAFci+dqvMG4xFUTnbGVvZJPIZJDree6Nbt6AHQo9pOdaIMAu 
```

```pem
WsGcplSOaDNdKKz1+b2UT2Zp3AIW4Qd51bouSNnR4M/gnr9ZD1ZctFd3jS+C5XRp D3vvcW5lAnCCC80P6rXy7d7hTeFu5EYKtRGXNvVNd/06NALGDflrrOwxF3Y= ----END CERTIFICATE---- 
```

在与服务器相关的代码中，私钥和证书文件都需要传递给各种SSL相关的包装函数中。证书会呈现给客户端，而私钥则应该受到保护，就保留在服务器端。

在与客户端相关的代码中，我们需要维护一个特殊的文件，其中包含有合法的证书颁发机构信息，以此来验证服务器端的证书。如果没有这样的文件，那么至少可以把服务器端的证书拷贝一份放在客户端机器上，用这个拷贝作为验证的手段。在建立连接时，服务器会发送自己的证书，而我们就可以用已经保存好的证书来完成验证。

服务器也可以选择验证客户端的身份。要做到这一点，客户端需要有自己的私钥和证书。而服务器端也需要维护一个受信任的证书颁发机构信息文件，以此来验证客户端的证书。

如果真的想在网络服务中添加对 SSL 的支持，本节仅仅只是小试身手告诉你如何完成设置。你肯定需要参考有关文档（http://docs.python.org/3/library/ssl.html）以了解更多的细节。准备好花大量的时间对代码进行试验吧，直到程序能正常工作为止。

# 11.11 在进程间传递 socket 文件描述符

# 11.11.1 问题

我们正在运行多个 Python 解释器进程，想把一个打开的文件描述符从一个解释器传递到另一个解释器上。例如，也许这里有一个服务器进程负责接收连接，但是实际处理客户端的请求是通过另一个不同的解释器来完成的。

# 11.11.2 解决方案

要在进程间传递文件描述符，首选需要将进程连接在一起。在 UNIX 系统上，你可能要用到 UNIX 域 socket，而在 Windows 上可以使用命名管道。但是，与其同这些底层的进程间通信机制打交道，利用 multiprocessing 模块来建立这样的连接通常会简单得多。

一 旦 进 程 间 的 连 接 建 立 起 来 了 ， 就 可 以 使 用 multiprocessing.reduction 模 块 中 的send_handle()和 recv_handle()函数来在进程之间传送文件描述符了。下面的示例给出了基本用法：

```python
import multiprocessing  
from multiprocessing.reduce import recv_handle, send_handle  
import socket 
```

```python
def worker(in_p, out_p):
    out_p.close()
while True:
    fd = recv_handle(in_p)
print('CHILD: GOT FD', fd)
with socket(socket(AF_INET, socket.SOCK_STREAM, filename=fd) as s:
    while True:
        msg = s.recv(1024)
        if not msg:
            break
print('CHILD: RECV {!r}.format(msg))
        s.send(msg)
def server(address, in_p, out_p, worker.pid):
    in_p.close()
s = socket(socket(AF_INET, socket.SOCK_STREAM)
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)
s.bind(address)
s.listen(1)
while True:
    client, addr = s.accept()
print('SERVER: Got connection from', addr)
send_handle(out_p, client.fileno(), worker.pid)
client.close()
if __name__ == '__main__':
c1, c2 = multiprocessing.Pipe()
worker_p = multiprocessing.Process(target=worker, args=(c1, c2))
worker_p.start()
server_p = multiprocessing.Process(target=server,
args=(( '', 15000), c1, c2, worker_p.pid))
server_p.start()
c1.close()
c2.close() 
```

在这个示例中我们生成了两个进程，并且利用 multiprocessing 模块的 Pipe 对象将它们连接在一起。服务器进程打开一个 socket 并等待客户端的连接。工作者进程只是通过recv_handle()在管道上等待接收文件描述符。当服务器接收到一条连接时，它会将得到的 socket 文件描述符通过 send_handle()发送给工作者进程。工作者进程接管这个 socket并将数据回显给客户端直到连接关闭为止。

如果使用 Telnet 或者类似的工具去连接服务器，那么可能会看到如下的结果：

```txt
bash % python3 passfd.py  
SERVER: Got connection from ('127.0.0.1', 55543)  
CHILD: GOT FD 7  
CHILD: RECV b'Hello\r\n'  
CHILD: RECV b'World\r\n' 
```

这个例子中最重要的部分就是服务器端接收到的客户端 socket 实际上是由另一个进程去处理的。服务器仅仅只是将它转手出去、关闭它然后等待下一个连接。

# 11.11.3 讨论

有许多程序员甚至都没有意识到在进程之间传递文件描述符是可以实现的。这种技术在构建可扩展的系统时会是一件有用的工具。比如在多核机器上，我们会同时运行多个 Python 解释器实例，用传递文件描述符的方式来对每个解释器处理的客户端数量做负载均衡。

解决方案中出现的 send_handle()和 recv_handle()函数只能用于多进程连接的环境中。除了使用管道之外，还可以按照 11.7 节中介绍的方法来连接解释器，只要你用的是 UNIX域 socket 或者 Windows 命名管道就可以。比如，可以将服务器和工作者进程实现为完全分离的程序，可以分别启动。下面是服务器端的实现：

# servertmp.py   
from multiprocessing.Connection import Listener   
from multiprocessing.reduction import send_handle   
import socket   
def serverwork_address,port): #Wait for the worker to connect workserv $=$ Listenerwork_address，authkey $\equiv$ b'peekaboo') worker $=$ work serv.accept() worker pid $=$ worker.recv() #Now run a TCP/IP server and send clients to workers $\equiv$ socket(socket(AF_INET,socket.SOCK_STREAM)s.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,True)s.bind(''，port)) s.listen(1) while True: client,addr $=$ s.accept() print('SERVER:Got connection from'，addr) send_handleworker,client.fileno(),worker.pid) client.close()   
if_name $= =$ 'main': import sys

```python
if len(sys.argv) != 3:  
    print('Usage: server.py server_address port', file=sys.stderr)  
    raise SystemExit(1)  
server(sys.argv[1], int(sys.argv[2])) 
```

要运行这个服务器，可以输入这样的命令：python3 servermp.py /tmp/servconn 15000。下面是对应的客户端代码：

from multiprocessing.Connection import Client   
from multiprocessing.reduction import recv_handle   
import os   
from socket import socket, AF_INET, SOCK_STREAM   
def worker(server_address): serv $=$ Client(server_address, authkey $\equiv$ b'peekaboo') serv.send(os.getpid()) while True: fd $=$ recv_handle(serv) print('WORKER:GOT FD',fd) with socket(AF_INET,SOCK_STREAM,fileno $\equiv$ fd) as client: while True: msg $=$ client.recv(1024) if not msg: break print('WORKER:RECV{!r}.format(msg)) client.send(msg)   
if_name $= =$ 'main': import sys if len(sys.argv) $! = 2$ : print('Usage:worker.py server_address',file $\equiv$ sys.stderr) raise SystemExit(1) worker(sys.argv[1])

要运行工作者进程，可以输入 python3 workermp.py /tmp/servconn。得到的结果应该和使用 Pipe()的例子完全一样。

在底层，文件描述符的传递涉及创建 UNIX 域 socket，并且要用到 socket 的 sendmsg()方法。由于这个技术并不是广为人知，下面我们给出另一种不同的服务器实现，展示了如何利用 socket来传递文件描述符：

```python
# server.py   
import socket   
import struct   
def send_fd(sock, fd):   
    ' ' Send a single file descriptor.   
    ' ' sock.sendmsg([b'x'], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, struct.pack('i', fd)])) ack = sock.recv(2) assert ack == b'OK'   
def server的工作_address, port): # Wait for the worker to connect work_serv = socket(socket(AF_INET, socket.SOCK_STREAM) work_serv.bind(word_address) work_serv.listen(1) worker, addr = work_serv.accept() # Now run a TCP/IP server and send clients to workers s = socket(socket(AF_INET, socket.SOCK_STREAM) s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) s.bind(['',port)) s.listen(1) while True: client, addr = s.accept() print('SERVER: Got connection from', addr) send_fdworker, client.fileno()) client.close()   
if _name_ == '\_main': import sys if len(sys.argv) != 3: print('Usage: server.py server_address port', file=sys.stderr) raise SystemExit(1) server(sys.argv[1], int(sys.argv[2])) 
```

下面是用 socket 实现的工作者进程：

```python
worker.py  
import socket  
import struct  
def recv_fd(sock): 
```

```python
...
Receive a single file descriptor
...
msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(struct.calysize('i')))
cmd_level, cmd_type, cmd_data = ancdata[0]
assert cmd_level == socket.SOL_SOCKET_and_cmd_type == socket.SCM_RIGHTS
sock.sendall(b'OK')
return struct unpack('i', msg_data)[0]
def worker(server_address):
    serv = socket(socket(AF_INET, socket.SOCK_STREAM)
    serv.connect(server_address)
while True:
    fd = recv_fd(serv)
    print('WORKER: GOT FD', fd)
    with socket(socket(AF_INET, socket.SOCK_STREAM, fileno=fd) as client:
        while True:
            msg = client.recv(1024)
            if not msg:
                break
            print('WORKER: RECV {!r}.format(msg))
            client.send(msg)
if __name__ == '__main__':
    import sys
    if len(sys.argv) != 2:
        print('Usage: worker.py server_address', file=sys.stderr)
        raise SystemExit(1)
    worker(sys.argv[1]) 
```

如果打算在自己的程序中传递文件描述符，那么读一些相关的高级材料比如 W.RichardStevens 所著的 Unix Network Programming（Prentice Hall, 1990）是非常明智的。在 Windows上传递文件描述符需要使用与UNIX 不同的技术（本节未给出）。对于Windows 平台，建议学习一下multiprocessing.reduction的源码，研究其中的细节来看看到底是如何实现的。

# 11.12 理解事件驱动型 I/O

# 11.12.1 问题

我们可能已经听说过某些 Python 的包是基于“事件驱动”或“异步”I/O 的，但是并不

能完全理解这到底是什么意思，在底层它究竟是如何工作的，或者说如果使用了这样的技术会对程序产生什么影响。

# 11.12.2 解决方案

从根本上说，事件驱动 I/O是一种将基本的 I/O 操作（即，读和写）转换成事件的技术，而我们必须在程序中去处理这种事件。比方说，当在 socket 上收到数据时，这就成为一个“接收事件”，由我们提供的回调方法或者函数负责处理以此来响应这个事件。一个事件驱动型框架可能会以一个基类作为起始点，实现一系列基本的事件处理方法，就像下面的示例这样：

```python
class EventHandler: def fileno(self): 'Return the associated file descriptor' raise NotImplemented('must implement') def wants_to_receive(self): 'Return True if receiving is allowed' return False def handle_receive(self): 'Perform the receive operation' pass def wants_to_send(self): 'Return True if sending is requested' return False def handle_send(self): 'Send outgoing data' pass 
```

之后，就可以把这个类的实例插入到一个事件循环中，看起来是这样的：

import select   
def event_loop(handlers): while True: wants_recv $=$ [h for h in handlers if h.wants_to_receive() ] wants_send $=$ [h for h in handlers if h.wants_to_send() ] can_recv,can_send,_ $=$ select.select(wants_recv, wants_send，[]) for h in can_recv: h.handle_receive() for h in can_send: h.handle_send()

就这么简单！事件循环的核心在于 select()调用，它会轮询文件描述符检查它们是否处于活跃状态。在调用 select()之前，事件循环会简单地查询所有的处理方法，看它们是希望接收还是发送数据。然后把查询的结果以列表的方式提供给 select()。结果就是，select()会返回已经在接收或发送事件上就绪的对象列表。对应的 handle_receive()或者handle_send()方法就被触发执行。

要编写应用程序，就需要创建特定的 EventHandler 类的实例。比如，这里有两个简单的处理程序，用以说明两个基于 UDP 的网络服务：

import socket   
import time   
class UDPServer(EventHandler): def __init__(self, address): self.sock = socket(socket(AF_INET, socket.SOCK_DGRAM) self.sock.bind(address) def fileno(self): return self.sock.fileno() def wants_to_receive(self): return True   
class UDPTimeServer(UDPServer): def handle_receive(self): msg,addr $=$ self.sock.recvfrom(1) self.sock.sendto(time.etime().encode('ascii'),addr)   
class UDPEchoServer(UDPServer): def handle_receive(self): msg,addr $=$ self.sock.recvfrom(8192) self.sock.sendto(msg,addr)   
if_name $\equiv$ 'main': handlers $=$ [UDPTimeServer('',14000)),UDPEchoServer('',15000)] event_loop(handlers)

要测试这份代码，可以试着从另一个 Python 解释器中连接服务器：

```python
>>> from socket import *
>>> s = socket(AF_INET, SOCK_DGRAM)
>>> s.sendto(b'', ('localhost', 14000))
0
>>> s.recvfrom(128)
(b'Tue Sep 18 14:29:23 2012', ('127.0.0.1', 14000)) 
```

```txt
>>>s.sendto(b'Hello'，('localhost'，15000))   
5   
>>>s.recvfrom(128)   
(b'Hello'，('127.0.0.1'，15000))   
>>>
```

实现一个 TCP 服务器就要稍微复杂一些，因为每个客户端都涉及产生一个新的处理对象。下面是 TCP echo 客户端的示例：

class TCPServer(EventHandler): def __init__(self, address, clienthandler, handler_list): self.sock = socket(socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) self.sock.bind(address) self.sock.listen(1) self.clienthandler $\equiv$ clienthandler self.client_list $=$ handler_list def fileno(self): return self.sock.fileno() def wants_to_receive(self): return True def handle_receive(self): client,addr $\equiv$ self.sock.accept() #Add the client to the event loop's handler list selfhandler_list.append(self.clienthandler(client,selfhandler_list))

class TCPClient(EventHandler): def __init__(self, sock, handler_list): self.sock = sock selfhandler_list = handler_list self.outgoing $=$ bytearray() def fileno(self): return self.sock.fileno() def close(self): self.sock.close() #Remove myself from the event loop's handler list selfhandler_list.remove(self) def wants_to_send(self): return True if self.outgoing else False

```python
def handle_send(self):
    nsent = self.sock.send(self.outgoing)
    self.outgoing = self.outgoing[nsent:] 
```

这个 TCP 示例的关键在于需要从处理列表中添加和移除客户端。在每个连接中都会为客户端创建一个新的处理例程并添加到列表中。可是当连接关闭时，每个客户端都必须将它们自己从列表中移除出去。

如果运行这个程序并尝试用 Telnet 或者类似的工具来建立连接，就会看到服务器会将接收到的数据回送给你。这个程序应该能轻松处理多个不同的客户端。

# 11.12.3 讨论

几乎所有的事件驱动型框架的工作原理都和我们给出的解决方案相类似。实际的实现细节以及软件的总体架构可能会有较大的区别，但是核心部分都有一个循环来轮询socket的活跃性并执行响应操作。

事件驱动型 I/O 的一个潜在优势在于它可以在不使用线程和进程的条件下同时处理大量的连接。也就是说，select()调用（或者功能相同的其他调用）可用来监视成百上千个 socket，并且针对它们中间发生的事件作出响应。事件循环一次处理一个事件，不需要任何其他的并发原语参与。

事件驱动型 I/O 的缺点在于这里并没有涉及真正的并发。如果任何一个事件处理方法阻塞了或者执行了一个耗时较长的计算，那么就会阻塞整个程序的执行进程。不是以事件驱动风格实现的库函数调用起来也会有这个问题。总是会有这样的风险，即，某些库函数调用阻塞了，导致整个事件循环停滞不前。

对于阻塞型或者需要长时间运行的计算，可以通过将任务发送给单独的线程或者进程来解决。但是，将线程和进程同事件循环进行协调需要较高的技巧。下面的代码示例通过 concurrent.futures 模块来实现：

from concurrent.futures import ThreadPoolExecutor   
import os   
class ThreadPoolHandler(EventHandler): def init_self, nworkers): if os.name $= =$ 'posix': self.signal_done SOCK, self-done SOCK $\equiv$ socket(socketpair() else: server $=$ socket(socket(socket.AF_INET, socket.SOCK_STREAM) server.bind('127.0.0.1'，0)) server.listen(1) self.signal_done SOCK $\equiv$ socket(socket(AF_INET, socket.SOCK_STREAM) self.signal_done SOCK.connect(server.getsockname()) self-done SOCK,_ $=$ server.accept() server.close() self pending $= [ ]$ self.pool $=$ ThreadPoolExecutor(nworkers)   
def fileno(self): return self-done SOCK.fileno() #Callback that executes when the thread is done def complete(self, callback,r): self_pending.append(callback,r,result())) self.signal_done SOCK.send(b'x') # Run a function in a thread pool def run(self, func, args $= ()$ ,kwargs $= \{\}$ ，*,callback): r $=$ self.pool.submit(func,\*args,\*\*kwargs) r.add_done_callback(lambda r: self._complete(callback,r)) def wants_to_receive(self): return True # Run callback functions of completed work def handleReceive(self): #Invoke all pending callback functions for callback,result in self_pending: callback(result)

```python
self.done_sock.recv(1)  
self pending = [] 
```

在这份代码中，run()方法用来将任务以及任务完成时需要触发的回调函数一起提交到线程池中。实际的任务就提交给了 ThredPoolExecutor 实例。但是，一个非常棘手的地方在于需要考虑计算出的结果同事件循环之间的协调同步问题。为了实现这个目的，我们在底层创建了一对 socket，用来实现一种信号通知机制。当线程池中的任务完成后，它就执行类中的_complete()方法。该方法在对这些 socket 写入一字节的数据前，将暂停的回调函数和结果进行排队处理。fileno()方法可用来返回另一个 socket。因此，当写入这个字节时就会通知事件循环有事件发生了。当触发时，handle_receive()方法将执行所有之前提交过来的回调函数。坦白说，这足以把人的脑袋弄晕。

下面给出了一个简单的服务器实现，展示了如何利用线程池来执行需要长时间运行的计算任务：

```python
A really bad Fibonacci implementation
def fib(n):
    if n < 2:
        return 1
    else:
        return fib(n - 1) + fib(n - 2)
class UDPFibServer(UDPServer):
    def handle_receive(self):
        msg, addr = self.sock.recvfrom(128)
        n = int(msg)
        pool.run(fib, (n), callback= lambda r: selfrespond(r, addr))
    def respond(self, result, addr):
        self.sock.sendto(str(result).encode('ascii'), addr)
if __name__ == '__main__( )
pool = ThreadPoolHandler(16)
handlers = [pool, UDPFibServer(['', 16000)]
event_loop(handlers) 
```

要测试这个服务器，只需要运行上面的代码并通过另一个 Python 程序来做些试验：

from socket import \*   
sock $=$ socket(AF_INET,SOCK_DGRAM)   
for x in range(40): sock.sendto(str(x).encode('ascii'), ('localhost', 16000)) resp $=$ sock.recvfrom(8192) print (resp[O])

我们应该可以在许多不同的窗口中重复运行这个程序，这么做不会使其他的程序被阻

塞。但是我们运行的程序实例越多，它运行的速度也会越来越慢。

读完这一节后你会考虑使用这样的代码吗？很可能不会。相反，应该找一个功能完备的框架来完成同样的任务。但是，如果理解了本节提到的基本概念，就能理解这样的框架所采用的核心技术。作为基于回调的编程模型的替代方案，有时候也会在事件驱动型代码中使用协程。请参见 12.12 节中的示例。

# 11.13 发送和接收大型数组

# 11.13.1 问题

我们想通过网络连接发送和接收大型数组，其中的数据都是连续的，并且要求尽可能少地对数据进行拷贝。

# 11.13.2 解决方案

下面的函数利用 memoryview 对大型数组进行发送和接收：

# zerocopy.py   
def send_from(arr,dest): view $=$ memoryview(arr).cast('B') while len_view): nsent $=$ dest.send_view) view $=$ view[nsent:]   
def recvInto(arr,source): view $=$ memoryview(arr).cast('B') while len_view): nrecv $=$ source.recvinto_view) view $=$ view[nrecv:]

要测试这个程序，首先创建一个服务器和客户端程序，它们之间通过 socket进行连接。服务器端的代码如下：

```python
>>> from socket import *
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s.bind(['', 25000])
>>> s.listen(1)
>>> c, a = s.accept()
>>> 
```

客户端代码如下（在另一个单独的解释器中运行）：

```txt
>>> from socket import *
>>> c = socket(AF_INET, SOCK_STREAM)
>>> c.connect('localhost', 25000))
>>> 
```

现在来看看本节所关注的主要问题：可以在网络连接上传输大型的数组。这种情况下，数组可以通过 array模块或者 numpy来创建。示例如下：

```txt
# Server
>>> import numpy
>>> a = numpy.arange(0.0, 50000000.0)
>>> send_from(a, c)
>>> 
# Client
>>> import numpy
>>> a = numpy.zeros(shape=50000000, dtype(float)
>>> a[0:10]
array([0., 0., 0., 0., 0., 0., 0., 0.])
>>> recvInto(a, c)
>>> a[0:10]
array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
>>> 
```

# 11.13.3 讨论

在数据密集型的分布式计算以及采用并行编程技术的应用程序中，编写需要发送和接收大型数据块的程序是很常见的。但是为了实现这个目标，需要以某种方式将数据还原为原始的字节给底层的网络接口所用。我们可能还需要将数据分片为较小的块，因为大部分与网络相关的函数都无法一次性发送或者接收超大型的数据块。

一种方法是以某种方式将数据进行序列化处理——可能是将其转换为字节串的形式。但是，这么做通常都要对数据进行拷贝，这正是我们极力避免的。就算我们将数据逐块进行拷贝，代码中还是会产生大量的小块拷贝操作。

本节提到的技术对这个问题进行了规避，这是通过利用 memoryview 来实现的一个技巧。从本质上来说，memoryview 就是对已有数组的一层覆盖。不仅是这样，memoryview还可以转型为不同的类型，允许数据根据不同的方式进行解释。这正是下列语句的用意所在：

```txt
view = memoryview(arr).cast('B') 
```

上面的语句接受一个数组 arr，并将其转型为无符号字节的 memoryview。

这种形式的 memoryview 可以传递给与 socket 相关的函数，比如 sock.send()或者

send.recv_into()。在底层，这些方法可以直接同内存打交道。比如，sock.send()直接从内存中发送数据，不需要进行拷贝。针对接收操作，send.recv_into()会将 memoryview作为输入缓冲区来使用。

避免内存拷贝的问题解决了，剩下的问题就主要归结在与 socket 相关的函数一次只能处理一部分数据上。一般来说，需要调用多次 send()和recv_into()才能将整个数组传输完毕。别担心，每次操作后，memoryview 都会根据已经发送或者接收的字节数做切片处理，以产生一个新的 memoryview。这个新的 memoryview 同样也是一个内存覆盖层，因此根本不会产生任何拷贝。

这里还有一个问题就是接收方需要预先知道要对方要发送多少数据，这样接收方才可以预分配一个相应的数组，或者验证是否可以将接收到的数据直接放入已有的数组中。如果这对你来说存在问题，那么可以让发送方总是先发送数据的大小，后面再跟着数组数据。

# 并发

Python 很早就开始支持多种不同的并发编程方法，包括多线程、加载子进程以及各种涉及生成器函数的技巧。在本章中，我们会谈到有关并发编程的方方面面，包括常见的多线程编程技术以及实现并行处理的方法。

有经验的程序员都应该知道，并发编程中充满了潜在的危险。因此，本章的重点在于引导大家编写出更可靠以及更易于调试的代码。

# 12.1 启动和停止线程

# 12.1.1 问题

为了让代码能够并发执行，我们想创建线程并在合适的时候销毁。

# 12.1.2 解决方案

threading 库可用来在单独的线程中执行任意的 Python 可调用对象。要实现这一要求，可以创建一个 Thread 实例并为它提供期望执行的可调用对象。下面是一个简单的示例：

Code to execute in an independent thread   
import time   
def countdown(n): while $n > 0$ print('T-minus'，n) n $\text{一} = 1$ time.sleep(5)   
#Create and launch a thread

```python
from threading import Thread  
t = Thread(target=countdown, args=(10))  
t.start() 
```

当创建一个线程实例时，在调用它的 start()方法之前（需要提供目标函数以及相应的参数），线程并不会立刻开始执行。

线程实例会在它们自己所属的系统级线程（即， POSIX 线程或 Windows 线程）中执行，这些线程完全由操作系统来管理。一旦启动后，线程就开始独立地运行，直到目标函数返回为止。可以查询线程实例来判断它是否还在运行：

```python
if t.is_alive(): print('Still running') else: print('Completed') 
```

也可以请求连接（join）到某个线程上，这么做会等待该线程结束：

```txt
t.join() 
```

解释器会一直保持运行，直到所有的线程都终结为止。对于需要长时间运行的线程或者一直不断运行的后台任务，应该考虑将这些线程设置为 daemon（即，守护线程）。示例如下：

```python
t = Thread(target=countdown, args=(10), daemon=True)  
t.start() 
```

daemon 线程是无法被连接的。但是，当主线程结束后它们会自动销毁掉。

除了以上展示的两种操作外，对于线程没有太多别的操作可做了。比如说，终止线程、给线程发信号、调整线程调度属性以及执行任何其他的高级操作，这些功能都没有。如果想要这些功能，就需要自己去构建。

如果想要终止线程，这个线程必须要能够在某个指定的点上轮询退出状态，这就需要编程实现。比如，可以将线程放到下面这样的类中：

class CountdownTask: def __init__(self): self._running = True def terminate(self): self._running = False def run(self, n): while self._running and $n > 0$ .. print('T-minus',n)

n $= 1$ time.sleep(5)   
c = CountdownTask()   
t = Thread(target=c.run, args=(10,)）   
t.start()   
c terminat e(#) # Signal termination   
t joins#Wait for actual termination (if needed)

如果线程会执行阻塞性的操作比如I/O，那么在轮询线程的退出状态时如何实现同步将变得很棘手。比如，某个线程被永远阻塞在 I/O 操作上了，那么它就永远无法返回，以检查自己是否要被终止。要正确处理这个问题，需要小心地为线程加上超时循环。示例如下：

class IOTask: def terminate(self): self._running $\equiv$ False def run(self, sock): # sock is a socket sock.settimeout(5) # Set timeout period while self._running: # Perform a blocking I/O operation w/ timeout try: data $=$ sock.recv(8192) break except socket.timeout: continue #Continued processing .. # Terminated return

# 12.1.3 讨论

由于全局解释器锁（GIL）的存在，Python 线程的执行模型被限制为在任意时刻只允许在解释器中运行一个线程。基于这个原因，不应该使用 Python 线程来处理计算密集型的任务，因为在这种任务中我们希望在多个 CPU 核心上实现并行处理。Python 线程更适合于I/O 处理以及涉及阻塞操作的并发执行任务（即，等待I/O、等待从数据库中取出结果等）。

有时候我们会发现从 Thread 类中继承而来的线程类。比如：

```python
from threading import Thread 
```

```python
class CountdownThread( Thread): def __init__(self, n): super().__init() self.n = 0 def run(self): while self.n > 0: print('T-minus', self.n) self.n -= 1 time.sleep(5) c = CountdownThread(5) c.start() 
```

尽管这么做也能完成任务，但这在代码和 threading库之间引入了一层额外的依赖关系。意思就是说，上面的代码只能用在有关线程的上下文中，而我们之前展示的技术中编写的代码并不会显式依赖于 threading 库。把代码从这种依赖关系中解放出来，那么就可以使代码在其他可能不会涉及线程的上下文中也能得到重用。比如，我们可能会利用 multiprocessing 模块让代码在单独的进程中运行，代码看起来是这样的：

```python
import multiprocessing  
c = CountdownTask(5)  
p = multiprocessing.Process(target=c.run)  
p.start()  
... 
```

再一次申明，这只会在 CountdownTask 类独立于任何一种并发机制（线程、进程等）时才有用。

# 12.2 判断线程是否已经启动

# 12.2.1 问题

我们已经加载了一个线程，但是想知道它实际会在什么时候开始运行。

# 12.2.2 解决方案

线程的核心特征就是它们能够以非确定性的方式（即，何时开始执行、何时被打断、何时恢复执行完全由操作系统来调度管理，这是用户和程序员都无法确定的）独立执行。如果程序中有其他线程需要判断某个线程是否已经到达执行过程中的某个点，根据这个判断来执行后续的操作，那么这就产生了非常棘手的线程同步问题。要解决这类问题，我们可以使用 threading 库中的 Event 对象。

Event对象和条件标记（sticky flag）类似，允许线程等待某个事件发生。初始状态时事件被设置为0。如果事件没有被设置而线程正在等待该事件，那么线程就会被阻塞（即，进入休眠状态），直到事件被设置为止。当有线程设置了这个事件时，这会唤醒所有正在等待该事件的线程（如果有的话）。如果线程等待的事件已经设置了，那么线程会继续执行。

下面给出了一个简单的示例，使用 Event来同步线程的启动：

from threading import Thread, Event   
import time   
# Code to execute in an independent thread   
def countdown(n,started_evt): print('countdown starting') started_evt.set() while $\mathrm{n} > 0$ . print('T-minus',n) n $= 1$ time.sleep(5)   
# Create the event object that will be used to signal startup started_evt $=$ Event()   
# Launch the thread and pass the startup event   
print('Launching countdown')   
t $=$ Thread(target $=$ countldn，args=(10,started_evt))   
t.start()   
# Wait for the thread to start   
started_evt.wait()   
print('countdown is running')

当运行这段代码时，字符串“countdown is running”总是会在“countdown starting”之后显示。这里使用了事件来同步线程，使得主线程等待，直到 countdown()函数首先打印出启动信息之后才开始执行。

# 12.2.3 讨论

Event对象最好只用于一次性的事件。也就是说，我们创建一个事件，让线程等待事件被设置，然后一旦完成了设置，Event 对象就被丢弃。尽管可以使用 Event 对象的 clear()方法来清除事件，但是要安全地清除事件并等待它被再次设置这个过程很难同步协调，可能会造成事件丢失、死锁或者其他的问题（特别是，在设定完事件之后，我们无法

保证发起的事件清除请求就一定会在线程再次等待该事件之前被执行）①。

如果线程打算一遍又一遍地重复通知某个事件，那最好使用 Condition 对象来处理。比如，下面的代码实现了一个周期性的定时器，每当定时器超时时，其他的线程都可以感知到超时事件：

import threading   
import time   
class PeriodicTimer: def__init__(self,interval): self._interval $=$ interval self._flag $= 0$ self._cv $=$ threading.Condition() def start(self): t $=$ threading.Thread(target $\equiv$ self.run) t.daemon $=$ True t.start() def run(self): ' Run the timer and notify waiting threads after each interval ' while True: time.sleep(self._interval) with self._cv: self._flag $\hat{\mathbf{\alpha}} = 1$ self._cv.notify_all() def wait_for Tick(self): ' Wait for the next tick of the timer ' with self._cv: last_flag $=$ self._flag while last_flag $= =$ self._flag: self._cv.wait() #Example use of the timer   
ptimer $=$ PeriodicTimer(5)   
ptimer.start()   
# Two threads that synchronize on the timer

```python
def countdown(nticks):
    while nticks > 0:
        timer.wait_for Tick()
        print('T-minus', nticks)
        nticks -= 1
def countup(last):
    n = 0
    while n < last:
        timer.wait_for Tick()
        print('Counting', n)
        n += 1
threading.Thread(target=countdown, args=(10)).start()
threading.Thread(target=countup, args=(5)).start() 
```

Event对象的关键特性就是它会唤醒所有等待的线程。如果我们编写的程序只希望唤醒一个单独的等待线程，那么最好使用 Semaphore 或者 Condition 对象。

比方说，考虑下面使用了信号量（semaphore）的代码：

```python
# Worker thread
def worker(n, sema):
    # Wait to be signaled
    sema.acquire()
    # Do some work
    print('Working', n)
# Create some threads
sema = threading.Semaphore(0)
nworkers = 10
for n in range(nworkers):
    t = threading.Thread(target=worker, args=(n, sema,)) 
    t.start() 
```

执行上面的程序会启动一系列的线程，但是什么也不会发生。这些线程都会因为等待获取信号量而被阻塞。每次释放信号量时，只有一个工作者线程会被唤醒并投入运行。示例如下：

```txt
>>>sema.release() Working 0   
>>>sema.release() Working 1   
>>> 
```

如果编写的代码中涉及许多线程间同步的技巧，那么很容易就会让自己的脑袋转晕。一个更为明智的做法是利用队列或者 actor 模式来完成线程间的通信任务。队列将在下一节中描述。actor 模式将在 12.10 节中讲解。

# 12.3 线程间通信

# 12.3.1 问题

我们的程序中有多个线程，我们想在这些线程之间实现安全的通信或者交换数据。

# 12.3.2 解决方案

也许将数据从一个线程发往另一个线程最安全的做法就是使用 queue 模块中的 Queue（队列）了。要做到这些，首先创建一个 Queue 实例，它会被所有的线程共享。之后线程可以使用 put()或者 get()操作来给队列添加或移除元素。示例如下：

from queue import Queue   
from threading import Thread   
# A thread that produces data   
def producer(out_q): while True: # Produce some data ... out_q.put(data) # A thread that consumes data def consumer(in_q): while True: # Get some data data $=$ in_q.get() # Process the data tCreate the shared queue and launch both threads q $=$ Queue() t1 $=$ Thread(target $\equiv$ consumer, args $\coloneqq$ (q,) t2 $=$ Thread(target $\equiv$ producer, args $\coloneqq$ (q,) t1.start () t2.start()

Queue 实例已经拥有了所有所需的锁，因此它们可以安全地在任意多的线程之间共享。

当使用队列时，如何对生产者（producer）和消费者（consumer）的关闭过程进行同步协调需要用到一些技巧。这个问题的一般解决方法是使用一个特殊的终止值，当我们将它放入队列中时就使消费者退出。示例如下：

from queue import Queue

from threading import Thread   
# Object that signals shutdown _sentinel $=$ object()   
# A thread that produces data   
def producer(out_q): while running: # Produce some data out_q.put(data) # Put the sentinel on the queue to indicate completion out_q.put(_sentinel)   
# A thread that consumes data   
def consumer(in_q): while True: # Get some data data $=$ in_q.get() # Check for termination if data is _sentinel: in_q.put(_sentinel) break # Process the data

这个示例中有一个很微妙的功能，那就是当消费者接收到这个特殊的终止值后，会立刻将其重新放回到队列中。这么做使得在同一个队列上监听的其他消费者线程也能接收到终止值——因此可以一个一个地将它们都关闭掉。

尽管队列是线程间通信的最常见的机制，但是只要添加了所需的锁和同步功能，就可以构建自己的线程安全型的数据结构。最常见的做法是将你的数据结构和条件变量打包在一起。比如，下面的示例构建了一个线程安全的优先级队列。关于优先级队列我们在 1.5 节中已经讨论过。

import heapq   
import threading   
classPriorityQueue: def__init__(self): self._queue $= [ ]$ self._count $= 0$ self._cv $=$ threading Condition()

```python
def put(self, item, priority): with self._cv: heapq.heappush(self._queue, (-priority, self._count, item)) self._count += 1 self._cv.notify()   
def get(self): with self._cv: while len(self._queue) == 0: self._cv.wait() return heapq.heappop(self._queue)[-1] 
```

通过队列实现的线程间通信是一种单方向且不确定的过程。一般来说，我们无法得知接收线程（也就是消费者）何时会实际接收到消息并开始工作。但是，Queue 对象的确提供了一些基本的事件完成功能（completion feature）。下面的示例通过 task_done()和join()方法对此进行了说明：

from queue import Queue   
from threading import Thread   
# A thread that produces data   
def producer(out_q): while running: # Produce some data ... out_q.put(data)   
# A thread that consumes data   
def consumer(in_q): while True: # Get some data data $=$ in_q.get() # Process the data .. # Indicate completion in_q.task_done()   
# Create the shared queue and launch both threads $\mathbf{q} =$ Queue () t1 $=$ Thread(target $\equiv$ consumer, args $\equiv$ (q,) t2 $=$ Thread(target $\equiv$ producer, args $\equiv$ (q,) t1.start () t2.start ()   
# Wait for all produced items to be consumed q.join()

当消费者线程已经处理了某项特定的数据，而生产者线程需要对此立刻感知的话，那么就应该将发送的数据和一个 Event对象配对在一起，这样就允许生产者线程可以监视这一过程。示例如下：

from queue import Queue   
from threading import Thread,Event   
#A thread that produces data   
def producer(out_q): while running: # Produce some data ... #Make an (data, event) pair and hand it to the consumer EVT $=$ Event() out_q.put((data,evt)) ... #Wait for the consumer to process the item EVT.wait()   
#A thread that consumes data   
def consumer(in_q): while True: #Get some data data，evt $=$ in_q.get() #Process the data ... #Indicate completion EVT.set()

# 12.3.3 讨论

把多线程程序按照简单的队列机制来实现，这对于保持程序的清晰性而言通常是一种不错的方式。如果可以把所有的任务都分解成用简单的线程安全型队列来处理，就会发现不需要用锁和其他的底层同步原语把程序弄得一团糟了。此外，使用队列进行通信常常使得程序的设计可以在稍后扩展到其他类型的基于消息通信的模式上。例如，可以将程序分解为多个进程，甚至做成分布式系统，而这一切都不需要对底层基于队列的架构做大的改动。

一个值得注意的地方是，在线程中使用队列时，将某个数据放入队列并不会产生该数据的拷贝。因此，通信过程实际上涉及在不同的线程间传递对象的引用。如果需要关心共享状态，那么只传递不可变的数据结构（即，整数、字符串或者元组），要么就对排队的数据做深拷贝，这就显得合情合理了。示例如下：

```python
from queue import Queue  
from threading import Thread 
```

import copy   
#A thread that produces data   
def producer(out_q): while True: # Produce some data ... out_q.put (copy.deepcopy(data))   
# A thread that consumes data   
def consumer(in_q): while True: #Get some data data $=$ in_q.get() # Process the data

Queue对象提供的一些额外功能被证明在特定的上下文中是有帮助的。如果通过一个可选的大小参数来创建Queue对象，例如Queue(N)，那么这就在 put()操作阻塞生产者线程之前对可以入队列的元素个数进行了限制。如果在生产者产生数据和消费者处理数据的速度上存在差异时，给队列可容纳的元素个数设定一个上限值就显得很有意义了。例如，如果生产者产生数据的速度比消费数据的速度快得多时。另一方面，当队列满时将其阻塞同样也会在程序中产生意外的连锁效应，可能导致出现死锁或者运行效率低下。总的来说，线程间通信的控制流是一个看似简单实则困难的问题。如果曾经发现自己试图通过调整队列的大小来修正问题，那么这就表明程序的设计不够健壮或者存在固有的扩展问题。

get()和 put()方法都支持非阻塞和超时机制。示例如下：

```python
import queue  
q = queueQueue()  
try: data = q.get(block=False)  
except queue_empty:  
    ...  
try: q.put(item, block=False)  
except queue.Full:  
    ...  
try: data = q.get(timeout=5.0)  
except queue Empty: 
```

这两种机制都可用来避免在特定的队列操作上无限期阻塞下去的问题。比如，可以用非阻塞的 put()配合固定大小的队列来实现当队列满时不同类型的处理方法。比如，可以生成一条日志信息然后将数据丢弃：

```python
def producer(q):
    ...
    try:
        q.put(item, block=False)
except queueFULL:
    log.warn('queued item %r discarded!', item) 
```

如果想让消费者线程周期性地放弃 q.get()这样的操作，以便于它们检查类似结束标记（termination flag， 见 12.1 节）这样的情况，那么超时机制是很有用的。

```python
def consumer(q):
    while _running:
        try:
            item = q.get(timeout=5.0)
            # Process item
            ...
        except queue_empty:
            pass 
```

最后，这里还有一些很实用的方法，比如 q.qsize()、q.full()、q.empty()，它们能够告诉我们队列的当前大小和状态。但是，请注意所有这些方法在多线程环境中都是不可靠的。例如，对 q.empty()的调用可能会告诉我们队列是空的，但是在完成这个调用的同时，另一个线程可能已经往队列中添加了一个元素。坦白讲，在编写代码时最好不要依赖这些函数。

# 12.4 对临界区加锁

# 12.4.1 问题

我们的程序用到了多线程，我们想对临界区进行加锁处理以避免出现竞态条件（racecondition）。

# 12.4.2 解决方案

要想让可变对象安全地用在多线程环境中，可以利用threading库中的Lock对象来解决，示例如下：

import threading   
```python
class SharedCounter: A counter object that can be shared by multiple threads. def __init__(self, initial_value = 0): self._value = initial_value self._value_lock = threading.Lock() def incr(self, delta=1): 'Increment the counter with locking with self._value_lock: self._value += delta def decr(self, delta=1): 'Decrement the counter with locking with self._value_lock: self._value -= delta 
```

当使用with语句时，Lock 对象可确保产生互斥的行为——也就是说，同一时间只允许一个线程执行 with 语句块中的代码。with 语句会在执行缩进的代码块时获取到锁，当控制流离开缩进的语句块时释放这个锁。

# 12.4.3 讨论

线程的调度从本质上来说是非确定性的。正因为如此，在多线程程序中如果不用好锁就会使得数据被随机地破坏掉，以及产生我们称之为竞态条件的奇怪行为。要避免这些问题，只要共享的可变状态需要被多个线程访问，那么就得使用锁。

在比较老的 Python 代码中，我们常会看到显式地获取和释放锁的动作。例如，对上面的例子稍作修改：

import threading   
```python
class SharedCounter: A counter object that can be shared by multiple threads. def __init__(self, initial_value = 0): self._value = initial_value self._value_lock = threading.Lock() 
```

```python
def incr(self, delta=1):
    ...
    Increment the counter with locking
    ...
    self._value_lock.acquire()
    self._value += delta
    self._value_lock.release()
def decr(self, delta=1):
    ...
    Decrement the counter with locking
    ...
    self._value_lock.acquire()
    self._value -= delta
    self._value_lock.release() 
```

采用 with 语句会更加优雅，也不容易出错——尤其是如果程序刚好在持有锁的时候抛出了异常，而程序员可能忘记去调用 release()方法时更是如此（在这两种情况下，with语句会确保总是释放锁）。

要避免可能出现的死锁，用到了锁的程序应该以这样的方式来编写，即，每个线程一次只允许获取一把锁。如果无法做到，我们可能需要在程序中引入更为高级的避免死锁的技术，我们将在 12.5 节中进一步讨论。

在 threading 库中我们会发现还有其他的同步原语，比如 RLock 和 Semaphore 对象。一般来说，这些对象都有特殊的用途，不应该用这些对象对可变状态做简单的加锁处理。RLock 被称为可重入锁，它可以被同一个线程多次获取，主要用来编写基于锁的代码，或者基于“监视器”的同步处理。当某个类持有这种类型的锁时，只有一个线程可以使用类中的全部函数或者方法。例如，可以将 SharedCounter 类实现为如下形式：

import threading   
class SharedCounter: A counter object that can be shared by multiple threads. _lock $=$ threading.RLock() def init_(self, initial_value $= 0$ .. self._value $=$ initial_value def incr(self,delta $\coloneqq 1$ .. Increment the counter with locking with SharedCounter._lock:

```python
self._value += delta
def decr(self, delta=1):
    ...
    Decrement the counter with locking
    ...
    with SharedCounter._lock:
        self.incr(-delta) 
```

这份代码中只有一个作用于整个类的锁，它被所有的类实例所共享。不再将锁绑定到每个实例的可变状态上，现在这个锁是用来同步类中的方法的。具体来说，这个锁可确保每次只有一个线程可以使用类中的方法。但是和标准的锁不同的地方在于，对于已经持有了锁的方法可以调用同样使用了这个锁的其他方法（参考 decr()方法的实现）。

这个实现的特点之一是无论创建了多少个 counter 实例，都只会有一个锁存在。因此，当有大量counter对象存在时，这种方法对内存的使用效率要高很多。但是，可能存在的缺点是在使用了大量线程且需要频繁更新 counter的程序中，这么做会产生更多的锁争用问题。

Semaphore 对象是一种基于共享计数器的同步原语。如果计数器非零，那么 with 语句会递减计数器并且允许线程继续执行。当 with 语句块结束时计数器会得到递增。如果计数器为零，那么执行过程会被阻塞，直到由另一个线程来递增计数器为止。尽管Semaphore 可以和标准的 Lock 对象一样以相同的方式来使用，但是由于 Semaphore 的实现更为复杂，这会对程序的性能带来负面影响。除了简单的加锁功能之外，Semaphore对象对于那些涉及在线程间发送信号或者需要实现节流（throttling）处理的应用中更加有用。例如，如果想在代码中限制并发的总数，可以使用 Semaphore 来处理：

from threading importSemaphore   
import urllib.request   
#At most, five threads allowed to run at once _fetch_url_sema $=$ Semaphore(5)   
def fetch_url(url): with _fetch_url_sema: return urllib.request(urlopen(url)

如果对于线程同步原语背后的理论和实现感兴趣的话，可以参考几乎任何一本有关操作系统的教科书。

# 12.5 避免死锁

# 12.5.1 问题

我们正在编写一个多线程程序，线程一次需要获取不止一把锁，同时还要避免出现死锁。

# 12.5.2 解决方案

在多线程程序中，出现死锁的常见原因就是线程一次尝试获取了多个锁。例如，如果有一个线程获取到第一个锁，但是在尝试获取第二个锁时阻塞了，那么这个线程就有可能会阻塞住其他线程的执行，进而使得整个程序僵死。

避免出现死锁的一种解决方案就是给程序中的每个锁分配一个唯一的数字编号，并且在获取多个锁时只按照编号的升序方式来获取。利用上下文管理器来实现这个机制非常简单，示例如下：

import threading   
from contextlib import contextmanager   
# Thread-local state to stored information on locks already acquired _local $=$ threading.local()   
@contextmanager   
def acquire(\*locks): # Sort locks by object identifier locks $=$ sorted(locks,key $\equiv$ lambda x: id(x)) #Make sure lock order of previously acquired locks is not violated acquired $=$ getattr(_local,'acquired',[]) if acquired and max(idlock) for lock in acquired) $> =$ id(locks[0]): raiseRuntimeError('Lock Order Violation') #Acquire all of the locks acquired extend(locks) _local.acquired $=$ acquired try: for lock in locks: lock.acquire() yield finally: # Release locks in reverse order of acquisition for lock in reversed(locks): lock.release() del acquired[-len(locks):]

要使用这个上下文管理器，只用按照正常的方式来分配锁对象，但是当想同一个或多个锁打交道时就使用 acquire()函数。例如：

```python
import threading
x_lock = threading.Lock()
y_lock = threading.Lock() 
```

```python
def thread_1():
    while True:
        with acquire(x_lock, y_lock):
            print('Thread-1')
    def thread_2():
        while True:
            with acquire(y_lock, x_lock):
                print('Thread-2')
    t1 = threading.Thread(target=thread_1)
    t1.daemon = True
    t1.start()
    t2 = threading.Thread(target=thread_2)
    t2.daemon = True
    t2.start() 
```

如果运行这个程序，就会发现程序运行得很好，而且永远不会出现死锁——尽管在每个函数中对锁的获取是以不同的顺序来指定的。

这个例子的关键之处就在于 acquire()函数的第一条语句：根据对象的数字编号对锁进行排序。通过对锁进行排序，无论用户按照什么顺序将锁提供给 acquire()函数，它们总是会按照统一的顺序来获取。

这个解决方案中用到了线程本地存储（thread-local storage）来解决一个小问题。即，如果有多个 acquire()操作嵌套在一起，可以检测可能存在的死锁情况。例如，假设编写了如下的代码：

import threading   
x_lock $\equiv$ threading.Lock()   
y_lock $\equiv$ threading.Lock()   
def thread_1(): while True: with acquire(x_lock): with acquire(y_lock): print('Thread-1') def thread_2(): while True: with acquire(y_lock): with acquire(x_lock): print('Thread-2')   
t1 $=$ threading.Thread(target=thread_1)

t1.daemon $=$ True   
t1.start()   
t2 $=$ threading.Thread(target $\equiv$ thread_2)   
t2.daemon $=$ True   
t2.start()

如果运行这个版本的程序，其中一个线程将会因抛出异常而崩溃：

Exception in thread Thread-1:   
Traceback (most recent call last): File "/usr/local/lib/python3.3/threading.py", line 639, in bootstrapsinner self.run() File "/usr/local/lib/python3.3/threading.py", line 596, in run self._target(*self._args, **self._kwargs) File "deadlock.py", line 49, in thread_1 with acquire(y_lock): File "/usr/local/lib/python3.3/contextlib.py", line 48, in __enter__ return next(self.gen) File "deadlock.py", line 15, in acquire raise RuntimeError("Lock Order Violation") RuntimeError: Lock Order Violation $\ggg$

这个崩溃基于这样一个事实，即每个线程都会记住它们已经获取到的锁的顺序。acquire()函数会检查之前获取到的锁的列表，并对锁的顺序做强制性的约束：先获取到的锁的对象 ID 必须比后获取的锁的 ID 要小。

# 12.5.3 讨论

在多线程程序中，死锁是一个老生常谈的问题（也是操作系统教科书上的常见主题）。基本原则就是，只要可以保证线程一次只持有一把锁，那么程序就不会出现死锁。但是，一旦在同一时间获取了多个锁，那么什么事情都有可能发生。

检测死锁和从死锁中恢复是一个极其棘手的问题，而且也很少有优雅的解决方案。比方说，通常用来检测死锁和恢复的方案涉及对看门狗定时器的使用。随着线程的运行，它们会周期性地重置定时器，只要一切都运行正常那么就皆大欢喜。但是，如果程序中出现死锁，看门狗定时器最终就会超时。此时，程序就通过重新启动来完成“恢复”。

而避免死锁采用的则是另一种不同的策略，即，以一种根本就不会让程序进入死锁状态的方式来使用锁。前面介绍的解决方案中，总是严格按照对象 ID 的升序来获取锁，这个方法可以在数学上证明能够避免死锁状态。我们把证明的过程就留给读者当做练习吧（要点就是，严格以递增的顺序来获取锁就不会出现锁的循环依赖，而这正是出

现死锁的必要条件）。

作为最后一个例子，我们将讨论一个经典的线程死锁问题——即所谓的“哲学家就餐问题”。在这个问题中，有 5 位哲学家围坐在桌边，桌上有 5 碗米饭和 5 支筷子。每位哲学家代表着一个独立的线程，而每支筷子代表一把锁。在这个问题中，哲学家要么坐着思考要么就吃米饭。但是，要吃到米饭，哲学家需要两支筷子。不幸的是，如果所有的哲学家都伸手拿他们左手边的那支筷子，那么他们只能全都坐在那里，手里只拿着一支筷子最终饿死。真是可怕的景象。

采用我们前面的解决方案，下面是哲学家就餐问题的简单实现，可完全避免死锁：

import threading   
# The philosopher thread   
def philosopher(left,right): while True: with acquire(left,right): print(threading.currentThread(), 'eating')   
# The chopsticks (represented by locks) NSTICKS $= 5$ chopsticks $=$ [threading.Lock() for n in range(NSTICKS)]   
# Create all of the philosophers   
for n in range(NSTICKS): t $=$ threading.Thread(target $\equiv$ philosopher, args=(chopsticks[n],chopsticks[(n+1)% NSTICKS]) t.start()

最后但同样值得注意的是，为了避免死锁，所有的锁都必须使用我们前面给出的acquire()函数来获取。如果某些代码片段中是直接获取锁的，那么这个避免死锁的算法就不能奏效了。

# 12.6 保存线程专有状态

# 12.6.1 问题

我们需要保存当前运行线程的专有状态，这个状态对其他线程是不可见的。

# 12.6.2 解决方案

有时候在多线程程序中，我们需要保存专属于当前运行线程的状态。为了做到这点，可以通过 threading.local()来创建一个线程本地存储对象。在这个对象上保存和读取的属

性只对当前运行的线程可见，其他线程无法感知。

作为使用线程局部存储的一个有趣实例，考虑一下 LazyConnection 上下文管理器类，我们在 8.3节中首次定义了这个类。下面是稍微修改过的版本，可以安全应用于多线程环境中：

from socket import socket, AF_INET, SOCK_STREAM

import threading

class LazyConnection:

```python
def __init__(self, address, family=AF_INET, type=SOCK_STREAM):
    self.address = address
    self.family = AF_INET
    self.type = SOCK_STREAM
    self.local = threading.local()
def __enter__(self):
    if hasattr(self.local, 'sock'):
        raise ValueError('Already connected')
        self.local.sock = socket(self.family, self.type)
        self.local.sock.connect(self.address)
        return self.local.sock
def __exit__(self, exc Ty, exc_val, tb):
    self.local.sock.close()
del self.local.sock 
```

在这份代码中，请仔细观察对 self.local 属性的使用。它被初始化为 threading.local()的实例。之后，其他方法操作的 socket 都是被保存为 self.local.sock 的形式。这就足以使得 LazyConnection 的实例可以安全用于多线程环境中了。示例如下：

from functools import partial

```python
def testConn: with conn as s: s.send(b'GET /index.html HTTP/1.0\R\n') s.send(b'Host: www.python.org\R\n') s.send(b'\r\n') resp = b'.join(iter(partial(s.recv, 8192), b')) print('Got {} bytes'.format(lenresp))) if __name__ == '__main__': conn = LazyConnection(['www.python.org', 80)) t1 = threading.Thread(target=test, args=(conn,) t2 = threading.Thread(target=test, args=(conn,) 
```

```lua
t1.start()
t2.start()
t1.join()
t2.join() 
```

这么做能正常工作的原因在于每个线程实际上创建了自己专属的 socket 连接（以self.local.sock的形式保存）。因此，当不同的线程在 socket上执行操作时，它们并不会互相产生影响，因为它们都是在不同的 socket上完成操作的。

# 12.6.3 讨论

在大部分程序中，创建和操作线程专有状态都不会出现什么问题。但是万一出现问题了，通常是因为多个线程使用了同一个对象，而该对象需要操作某种系统资源，比如说 socket 或者文件。我们不能让一个单独的 socket 对象被所有线程共享，因为如果有多个线程同时对 socket 进行读或写，那么就会出现混乱。线程专有存储通过让这种资源只对一个线程可见，解决了这个问题。

在本节示例中，使用 threading.local()使得 LazyConnection 类支持每个线程一条连接，而不是之前的整个进程就一条连接。这是一个微妙但有趣的区别。

在底层，threading.local()实例为每个线程维护着一个单独的实例字典。所有对实例的常见操作比如获取、设定以及删除都只是作用于每个线程专有的字典上。每个线程使用一个单独的字典，正是这一事实使得不同线程的数据得到隔离。

# 12.7 创建线程池

# 12.7.1 问题

我们想创建一个工作者线程池用来处理客户端连接，或者完成其他类型的工作。

# 12.7.2 解决方案

concurrent.futures 库中包含有一个 ThreadPoolExecutor 类可用来实现这个目的。下面的示例是一个简单的 TCP 服务器，使用线程池来服务客户端：

```python
from socket import AF_INET, SOCK_STREAM, socket   
from concurrent.futures import ThreadPoolExecutor   
def echo_client(sock, client_addr): Handle a client connection . print('Got connection from'，client_addr) 
```

while True: msg = sock.recv(65536) if not msg: break sock.sendall(msg) print('Client closed connection') sock.close()   
def echo_server(addr): pool $=$ ThreadPoolExecutor(128) sock $=$ socket(AF_INET,SOCK_STREAM) sock.bind(addr) sock.listen(5) while True: client SOCK,client_addr $\equiv$ sock.accept() pool.submit(echo_client,client SOCK,client_addr)   
echo_server('',15000))

如果想手动创建自己的线程池，使用 Queue 来实现通常是足够简单的。下面的例子对上述代码做了修改，手动实现了线程池：

from socket import socket, AF_INET, SOCK_STREAM   
from threading import Thread   
from queue import Queue   
def echo_client(q): 111 Handle a client connection 111 sock, client_addr = q.get() print('Got connection from', client_addr) while True: msg = sock.recv(65536) if not msg: break sock.sendall(msg) print('Client closed connection') sock.close() def echo_server(addr, nworkers): #Launch the client workers $\mathbf{q} =$ Queue() for n in range(nworkers): t = Thread(target=echo_client, args=(q,) ) t.daemon $=$ True

t.start() # Run the server sock $=$ socket(AF_INET,SOCK_STREAM) sock.bind(addr) sock.listen(5) while True: client SOCK,client addr $=$ sock.accept() q.put((client SOCK，client_addr)) echo_server(''，15000)，128)

应该使用 ThreadPoolExecutor 而不是手动实现线程池。这么做的优势在于使得任务的提交者能够更容易从调用函数中取得结果。例如，可以像这样编写代码：

```python
from concurrent.futures import ThreadPoolExecutor  
import urllib.request  
def fetch_url(url):  
    u = urllib.request(urlopen(url)  
    data = u.read()  
    return data  
pool = ThreadPoolExecutor(10)  
# Submit work to the pool  
a = pool.submit(fetch_url, 'http://www.python.org')  
b = pool.submit(fetch_url, 'http://www.pypy.org')  
# Get the results back  
x = a(result)  
y = b(result) 
```

示例中的结果对象（即，a和 b）负责处理所有需要完成的阻塞和同步任务，从工作者线程中取回数据。特别是，a.result()操作会阻塞，直到对应的函数已经由线程池执行完毕并返回了结果为止。

# 12.7.3 讨论

一般来说，应该避免编写那种允许线程数量无限增长的程序。比如，看看下面这个服务器实现：

```python
from threading import Thread   
from socket import socket, AF_INET, SOCK_STREAM   
def echo_client(sock, client_addr): 111 Handle a client connection 
```

```python
```
print('Got connection from', client_addr)
while True:
    msg = sock.recv(65536)
    if not msg:
        break
        sock.sendall(msg)
print('Client closed connection')
sock.close()
def echo_server(addr, nworkers):
    # Run the server
    socket = socket(AF_INET, SOCK_STREAM)
    sock.bind(addr)
    sock.listen(5)
while True:
    client SOCK, client_addr = sock.accept()
    t = Thread(target=echo_client, args=(client SOCK, client_addr))
    t.daemon = True
    t.start()
echo_server(['', 15000)) 
```

尽管可以工作，但是无法阻止恶意用户对服务器发起拒绝服务攻击，从而导致服务器上创建了大量的线程，耗尽了系统资源而崩溃（因而进一步说明了使用线程的“邪恶”之处）。通过使用预先初始化好的线程池，就可以小心地为所能支持的并发总数设定一个上限值。

我们可能会担心创建大量线程所产生的影响。但是，在现代的系统上创建拥有几千个线程的线程池是不会有什么问题的。此外，让一千个线程等待工作并不会对其他部分的代码产生性能上的影响（休眠的线程什么也不做）。当然了，如果所有这些线程在同一时间被唤醒开始使用CPU的话那就是另一回事了①—— 尤其在有全局解释器锁（GIL）的情况下更是如此。一般来说，线程池只适用于处理 I/O 密集型的任务。

创建大型的线程池需要考虑的一个方面就是对内存的使用。比如说，在 OS X 上如果创建了两千个线程，系统显示 Python 进程占用了超过 9 GB 的虚拟内存。但是，这实际上是有一些误导的。当创建一个线程时，操作系统会占用一段虚拟内存来保存线程的执行栈（通常有 8 MB）。这段内存只有一小部分会实际映射到物理内存上。因此，如果看的更仔细一些，就会发现 Python 进程占用的物理内存远比虚拟内存要小（例如，创建两千个线程只使用了 70 MB 物理内存，不是 9 GB）。如果需要考虑虚拟内存的大小，可以使用 threading.stack_size()函数来将栈的大小调低。例如：

```txt
import threading  
threading.stack_size(65536) 
```

如果增加这个调用，然后重复试验创建两千个线程，就会发现 Python 进程现在只使用了大约 $2 1 0 \mathrm { M B }$ 虚拟内存，但使用的物理内存总量保持不变。注意，线程栈的大小必须至少有 32768 字节，通常会限制该值为系统内存页面大小（40968192 等）的整数倍。

# 12.8 实现简单的并行编程

# 12.8.1 问题

我们有一个执行了大量 CPU 密集型工作的程序，现在想让它利用多个 CPU 的优势运行得更快些。

# 12.8.2 解决方案

concurrent.futures 库中提供了一个 ProcessPoolExecutor 类，可用来在单独运行的 Python解释器实例中执行计算密集型的函数。但是为了使用这个功能，首先得有一些计算密集型的任务才行。让我们以一个简单但有实际意义的例子来说明。

假设有一个目录，里面全是 gzip 压缩格式的 Apache Web 服务器的日志文件：

```txt
logs/ 20120701.log.gz 20120702.log.gz 20120703.log.gz 20120704.log.gz 20120705.log.gz 20120706.log.gz 
```

进一步假设每个日志文件中包含有如下这样的文本行：

```shell
124.115.6.12 -- [10/Jul/2012:00:18:50 -0500] "GET /robots.txt ..." 200 71
210.212.209.67 -- [10/Jul/2012:00:18:51 -0500] "GET /ply/ ..." 200 11875
210.212.209.67 -- [10/Jul/2012:00:18:51 -0500] "GET /favicon.ico ..." 404 369
61.135.216.105 -- [10/Jul/2012:00:20:04 -0500] "GET /blog/atom.xml ..." 304 -
...
... 
```

下面是一个简单的脚本，它读取数据并标识出所有访问过 robots.txt 文件的主机：

```python
findrobots.py import gzip 
```

import io   
import glob   
def find Robots (filename): Find all of the hosts that access robots.txt in a single log file 'Robots $=$ set() with gzip.open(filename) as f: for line in io.TextIOWrapper(f,encoding $\equiv$ 'ascii'): fields $=$ line.split( if fields[6] $= =$ '/robots.txt': robots.add.fields[0]) return robots   
def find_all Robots(logdir): Find all hosts across and entire sequence of files files $=$ glob.glob(logdir+'/\*.log.gz') all Robots $=$ set( for robots in map找到了robots,files): all Robots.update(robots) return all Robots   
if _name $\equiv =$ 'main': robots $=$ find_all Robots('logs') for ipaddr in robots: print(ipaddr)

上面的程序以常用的map-reduce风格①来编写。函数find_robots()被映射到一系列的文件名上，将所有得到的结果合并成一个单独的结果（即 find_all_robots()函数中设置的 all_robots）。

现在假设想修改这个程序以利用多个 CPU 核心。这是很容易实现的——只需把 map()替换成一个类似的操作，并让它在 concurrent.futures 库中的进程池中执行即可。下面是稍微修改过的代码：

```txt
findrobots.py   
import gzip   
import io   
import glob   
from concurrent import futures 
```

```python
def find Robots(filename):
    ...
    Find all of the hosts that access robots.txt in a single log file
    ...
    robots = set()
    with gzip.open(filename) as f:
        for line in io.TextIOWrapper(f, encoding='ascii':
            fields = line.split()
            if fields[6] == '/robots.txt':
                robots.add.fields[0])
        return robots
def find_all Robots(logdir):
    ...
    Find all hosts across and entire sequence of files
    ...
    files = glob.glob(logdir+'/*.log.gz')
    all Robots = set()
    with futures.ProcessPoolExecutor() as pool:
        for robots in pool.mapfind Robots, files):
            all Robots.update(roots)
        return all Robots
if __name__ == '__main__':
    robots = find_all Robots('logs')
    for ipaddr in robots:
        print(ipaddr) 
```

经过这次修改，现在这个脚本在我们的四核机器上运行时要比之前的版本快3.5倍，得到的结果完全相同。实际的性能会根据机器上的 CPU 个数而有所不同。

# 12.8.3 讨论

ProcessPoolExecutor 的典型用法是这样的：

from concurrent.futures import ProcessPoolExecutor

```txt
with ProcessPoolExecutor() as pool:  
    ...  
    do work in parallel using pool 
```

在底层，ProcessPoolExecutor 创建了 N 个独立运行的 Python 解释器，这里的 N 就是在系统上检测到的可用的 CPU 个数。可以修改创建的 Python 进程个数，只要给ProcessPoolExecutor(N)提供一个可选的参数即可。进程池会一直运行，直到 with 语句

块中的最后一条语句执行完毕为止，此时进程池就会关闭。但是，程序会一直等待所有已经提交的任务都处理完毕为止。

提交到进程池中的任务必须定义成函数的形式。有两种方法可以提交任务。如果想并行处理一个列表推导式或者 map()操作，可以使用 pool.map()：

A function that performs a lot of   
def work(x): return result   
# Nonparallel code results $=$ mapwork,data)   
# Parallel implementation   
with ProcessPoolExecutor() as pool: results $=$ pool.mapwork,data)

还有一种方式就是可以通过 pool.submit()方法来手动提交一个单独的任务：

Some function   
def work(x): return result   
with ProcessPoolExecutor() as pool: #Example of submitting work to the pool future_result $=$ pool.submitwork,arg) #Obtaining the result (blocks until done) $\mathbf{r} =$ future_result,result(）

如果手动提交任务，得到的结果就是一个 Future 实例。要获取到实际的结果还需要调用它的 result()方法。这么做会阻塞进程，直到完成了计算并将结果返回给进程池为止。

与其让进程阻塞，也可以提供一个回调函数，让它在任务完成时得到触发执行。示例如下：

```python
def when_done(r):
    print('Got: ', r,result())
with ProcessPoolExecutor() as pool:
    future_result = pool.submit(work, arg)
    future_result.add_done_callback(when_done) 
```

用户提供的回调函数需要接受一个Future实例，必须用它才能获取到实际的结果（即，调用它的 result()方法）。

尽管进程池使用起来很简单，但是在设计规模更大的程序时有几个重要的因素需要考虑。我们在这里说明一下，各因素间不分先后顺序。

这种并行化处理的技术只适用于可以将问题分解成各个独立部分的情况。  
任务必须定义成普通的函数来提交。实例方法、闭包或者其他类型的可调用对象都是不支持并行处理的。  
• 函数的参数和返回值必须可兼容于 pickle 编码。任务的执行是在单独的解释器进程中完成的，这中间需要用到进程间通信。因此，在不同的解释器间交换数据必须要进行序列化处理。  
• 提交的工作函数不应该维护持久的状态或者带有副作用。除了简单的日志功能外，一旦子进程启动，将无法控制它的行为。因此，为了让思路保持清晰，最好让每件事情都保持简单，让任务在不会修改执行环境的纯函数（pure-function）中执行。  
进程池是通过调用 UNIX 上的 fork()系统调用来创建的。这么做会克隆出一个Python 解释器，在 fork()时会包含所有的程序状态。在 Windows 上，这么做会加载一个独立的解释器拷贝，但并不包含状态。克隆出来的进程在首次调用 pool.map()或者 pool.submit()方法之前不会实际运行。  
• 当将进程池和多线程技术结合在一起时需要格外小心。特别是，很可能我们应该在创建任何线程之前优先创建并加载进程池（例如，当程序启动时在主线程中创建进程池）。

# 12.9 如何规避 GIL 带来的限制

# 12.9.1 问题

我们已经听说过全局解释器锁（GIL），担心它会影响到多线程程序的性能。

# 12.9.2 解决方案

尽管 Python 完全支持多线程编程，但是在解释器的 C 语言实现中，有一部分并不是线程安全的，因此不能完全支持并发执行。事实上，解释器被一个称之为全局解释器锁（GIL）的东西保护着，在任意时刻只允许一个 Python 线程投入执行。GIL 带来的最明显的影响就是多线程的 Python 程序无法充分利用多个 CPU 核心带来的优势（即，一个采用多线程技术的计算密集型应用只能在一个 CPU 上运行）。

在讨论规避 GIL 的常用方案之前，需要重点强调的是，GIL 只会对 CPU 密集型的

程序产生影响（即，主要完成计算任务的程序）。如果我们的程序主要是在做 I/O 操作，比如处理网络连接，那么选择多线程技术常常是一个明智的选择。因为它们大部分时间都花在等待对方发起连接上了。实际上可以创建数以千计的 Python 线程，一点问题都没有。在现代的操作系统上运行这么多线程是不会有问题的，因此这不是应该担心的地方。

对于 CPU 密集型的程序，我们需要对问题的本质做些研究。例如，仔细选择底层用到的算法，这可能会比尝试将一个没有优化过的算法用多线程来并行处理所带来的性能提升要高得多。同样地，由于 Python 是解释型语言，往往只要简单地将性能关键的代码转移到用 C 语言扩展的模块中就可能得到极大的速度提升。类似 NumPy 这样的扩展模块对于加速涉及数组数据的特定计算也是非常高效的。最后但同样重要的是，还可以尝试其他的解释器实现，比如说使用了 JIT 编译优化技术的 PyPy（尽管在写作本书时 PyPy 还没有支持 Python 3）。

同样值得指出的是，使用多线程技术并不只是为了获得性能的提升。一个 CPU 密集型的程序可能会用多线程来管理图形用户界面、网络连接或者其他类型的服务。在这种情况下 GIL 实际上会带来更多的问题。因为如果某部分代码持有 GIL 锁的时间过长，那就会导致其他非 CPU 密集型的线程都阻塞住，这实在令人讨厌。实际上，一个写的很糟糕的 C 语言扩展模块会让这个问题变得更加严重，尽管代码中用 C 实现的部分会比之前要运行得更快①。

说了这么多，要规避 GIL 的限制主要有两种常用的策略。第一，如果完全使用 Python来编程，可以使用 multiprocessing 模块来创建进程池，把它当做协处理器来使用。例如，假设线程代码是这样的：

```python
#Performs a large calculation (CPU bound)   
def some_work(args): return result   
#A thread that calls the above function   
def some_thread(): while True: 
```

```txt
r = some_work(args) 
```

下面的示例告诉我们如何将代码修改为使用进程池的方式：

```python
# Processing pool (see below for initiazation)  
pool = None  
# Perform a large calculation (CPU bound)  
def some_work(args):  
    return result  
# A thread that calls the above function  
def some_thread():  
    while True:  
        r = pool.apply(some_work,(args))  
        ...  
# Initiate the pool  
if __name__ == '__main__':  
    import multiprocessing  
    pool = multiprocessingPOOL() 
```

这个使用进程池的例子通过一个巧妙的办法避开了 GIL 的限制。每当有线程要执行CPU 密集型的任务时，它就把任务提交到池中，然后进程池将任务转交给运行在另一个进程中的 Python 解释器。当线程等待结果的时候就会释放 GIL。此外，由于计算是在另一个单独的解释器中进行的，这就不再受到 GIL 的限制了。在多核系统上，将会发现采用这种技术能轻易地利用到所有的 CPU 核心。

第二种方式是把重点放在 C 语言扩展编程上。主要思想就是将计算密集型的任务转移到 C 语言中，使其独立于 Python，在 C 代码中释放 GIL。这是通过在 C 代码中插入特殊的宏来实现的：

```txt
include"Python.h"   
PyObject \*pyfunc(PyObject \*self，PyObject \*args）{ Py_BEGIN ALLOW_THREADs // Threaded C code Py_ENDALLOW Threads 
```

如果使用其他的工具来访问 C 代码，比如 ctypes 库或者 Cython，那么可能不需要做任何处理。比方说，ctypes 默认会在调用 C 代码时自动释放 GIL。

# 12.9.3 讨论

有许多程序员每当面对多线程程序性能方面的问题时，总是抱怨 GIL 是所有问题的根源。但是，这么做只是一种短视和幼稚的行为。举个现实中的例子吧，在多线程网络程序中出现神秘的“僵死”现象很可能是由于和 GIL 风马牛不相及的原因所造成的（例如，DNS 查询失败）。底线就是你需要认真研究自己的代码，判断 GIL 是否才是问题的原因。再次申明，CPU 密集型的处理才需要考虑 GIL，I/O 密集型的处理则不必。

如果打算使用进程池来规避 GIL，这需要涉及同另一个 Python 解释器之间进行数据序列化和通信的处理。为了让这种方法奏效，待执行的操作需要包含在以 def语句定义的Python 函数中（即，在这里lambda、闭包、可调用实例都是不可以的），而且函数参数和返回值必须兼容于 pickle 编码。此外，要完成的工作规模必须足够大，这样可以弥补额外产生的通信开销。

将多线程和进程池混在一起使用绝对是个让人头痛的好办法。如果打算将这些功能结合在一起使用，通常最好在创建任何线程之前将进程池作为单例（singleton）在程序启动的时候创建。之后，线程就可以使用相同的进程池来处理所有那些计算密集型的工作。

对于 C语言扩展模块，最重要的功能就是保持与 Python 解释器进程的隔离。也就是说，如果打算将任务从 Python 中转移到 C 来处理，需要确保 C 代码可以独立于 Python 执行。这意味着不使用 Python 的数据结构，也不调用 Python 的 C 语言 API。另一个需要考虑的就是要确保编写 C 语言扩展模块能够完成足够多的任务，这样才值得这么做。也就是说，这个扩展模块最好可以执行几百万次的计算，而不仅仅只是完成几个小规模的计算。

不用说，这些规避 GIL 限制的解决方案并非对所有问题都适用。例如，某些特定类型的应用如果分解到多个进程中处理，或者是将部分代码用 C来实现，效果都不会很好。对于这些类型的应用，需要找到自己的解决方案（例如，多个进程访问共享的内存区域、让多个解释器运行在同一个进程中，等等）。作为备选方案，我们还可以选择其他的解释器实现，比如 PyPy。

有关在 C 语言扩展中释放 GIL 的附加内容，可参见 15.7 节和 15.10 节。

# 12.10 定义一个 Actor 任务

# 12.10.1 问题

我们想要定义行为上类似于 actor 的任务，即采用所谓的 actor 模式来编程。

# 12.10.2 解决方案

actor 模式是最古老也是最简单的用来解决并发和分布式计算问题的方法之一。实际上，actor模式所暗含的简单性正是它的吸引力所在。总的来说，actor就是一个并发执行的任务，它只是简单地对发送给它的消息进行处理。作为对这些消息的响应，actor 会决定是否要对其他的 actor发送进一步的消息。actor任务之间的通信是单向且异步的。因此，消息的发送者并不知道消息何时才会实际传递，当消息已经处理完毕时也不会接收到响应或者确认。

把线程和队列结合起来使用很容易定义出 actor。示例如下：

from queue import Queue   
from threading import Thread,Event   
# Sentinel used for shutdown   
class ActorExit(Exception): pass   
class Actor: def __init__(self): self._mailbox $=$ Queue() def send(self，msg): ' Send a message to the actor ' self._mailbox.put(msg) def recv(self): 'Receive an incoming message ' msg $=$ self._mailbox.get() if msg is ActorExit: raise ActorExit() return msg def close(self): Close the actor,thus shutting it down ' self.send(ActorExit) def start(self):

```python
Start concurrent execution
```
self._terminated = Event()
t = Thread(target= self._bootstrap)
t.daemon = True
t.start()
def _bootstrap(self):
    try:
        self.run()
    except ActorExit:
        pass
    finally:
        self._terminated.set()
    def join(self):
        self._terminated.wait()
    def run(self):
        ...
        Run method to be implemented by the user
        ...
    while True:
        msg = self.recv()
# Sample ActorTask
class PrintActor(Actor):
    def run(self):
        while True:
            msg = self.recv()
print('Got: ', msg)
# Sample use
p = PrintActor()
p.start()
p.send('Hello')
p.send('World')
p.close()
p.join() 
```

在这个示例中，我们使用 actor 实例的 send()方法来发送消息。在底层，这会将消息放入到队列上，内部运行的线程会从队列中取出收到的消息处理。close()方法通过在队列中放置一个特殊的终止值（ActorExit）来关闭 actor。用户可以通过继承 Actor 类来定义新的 actor，并重新定义 run()方法来实现自定义的处理。用户自定义的代码可通过ActorExit 异常来捕获终止请求，如果合适的话可以处理这个异常（ActorExit 异常是在

recv()方法中抛出并传播的）。

如果将并发和异步消息传递的需求去掉，那么完全可以用生成器来定义一个最简化的actor 对象。示例如下：

def printActor(): while True: try: msg $\equiv$ yield #Get a message print('Got:'，msg) except GeneratorExit: print('Actor terminating')   
#Sample use   
p $=$ printActor()   
next(p) #Advance to the yield (ready to receive)   
p.send('Hello')   
p.send('World')   
p.close()

# 12.10.3 讨论

actor模式之所以吸引人，部分原因是由于它的简单性。在实践中只有一个核心的操作，那就是 send()。此外，在基于 actor 模式的系统中，“消息”的概念可以扩展到许多不同的方向。比方说，可以以元组的形式传递带标签的消息，让 actor 执行不同的操作：

```python
class TaggedActor(Actor):
    def run(self):
        while True:
            tag, *payload = self.recv()
            getattr(self, 'do_' + tag) (*payload)
        # Methods corresponding to different message tags
        def do_A(self, x):
            print('Running A', x)
        def do_B(self, x, y):
            print('Running B', x, y)
# Example
a = TaggedActor()
a.start()
a.send(['A', 1])  # Invokes do_A(1)
a.send(['B', 2, 3])  # Invokes do_B(2,3) 
```

再看另一个例子。这里有一个 actor 的变种，允许在工作者线程中执行任意的函数，并通过特殊的 Result对象将结果回传：

from threading import Event   
class Result: def init_self): self._evt $=$ Event() self._result $\equiv$ None def set_result(self, value): self._result $=$ value self._evt.set() def result(self): self._evt.wait() return self._result   
class Worker(Actor): def submit(self, func, \*args, \*\*kwargs): r $=$ Result() self.send((func, args, kwargs, r)) return r def run(self): while True: func, args, kwargs, r $=$ self.recv() r.set_result(func(*args, \*\*kwargs))   
#Example use worker $=$ Worker() worker.start() $\mathbf{r} =$ worker.submit(pow,2,3)   
print(r,result())

最后但同样重要的是，给任务“发送”一条消息这个概念是可以扩展到涉及多进程甚至是大型的分布式系统中的。例如，可以把 actor 对象的 send()方法实现为在 socket连接上传输数据，或者通过某种消息传递的基础架构（比如 AMQP、ZMQ 等）来完成传递。

# 12.11 实现发布者/订阅者消息模式

# 12.11.1 问题

我们要解决一个基于多线程间通信的问题，希望实现发布者/订阅者的消息模式。

# 12.11.2 解决方案

要实现发布者/订阅者消息模式，一般来说需要引入一个单独的“交换”或者“网关”这样的对象，作为所有消息的中介。也就是说，不是直接将消息从一个任务发往另一个任务，而是将消息发往交换中介，由中介将消息转发给一个或多个相关联的任务。下面给出了一个非常简单的消息交换的实现：

from collections import defaultdict   
class Exchange: def __init__(self): self._subscribers $=$ set() def attach(self, task): self._subscribers.add(task) def detach(self, task): self._subscribers.remove(task) def send(self, msg): for subscriber in self._subscribers: subscriber.send(msg) #Dictionary of all created exchanges _exchanges $=$ defaultdict (Exchange) #Return the Exchange instance associated with a given name def get_exchange(name): return _exchanges[name]

交换中介其实就是一个对象，它保存了活跃的订阅者集合，并提供关联、取消关联以及发送消息的方法。每个交换中介都由一个名称来标识，get_exchange()函数简单地返回同给定的名称相关联的那个 Exchange 对象。

下面是一个简单的例子，展示了如何使用交换中介：

Example of a task. Any object with a send() method   
class Task: def send(self, msg): .task_a $\equiv$ Task()   
task_b $=$ Task()   
# Example of getting an exchange

```txt
exc = get_exchange('name')
# Examples of subscribing tasks to it
exc.attach(task_a)
exc.attach(task_b)
# Example of sending messages
exc.send('msg1')
exc.send('msg2')
# Example of unsubscribing
excdetach(task_a)
excdetach(task_b) 
```

尽管关于这个主题还有许多不同的变种，但总体思路都是一样的。消息会先传递到一个中介，再由中介将消息传递给相关联的订阅者。

# 12.11.3 讨论

任务或者线程之间互相发送消息（通常以队列来实现），这个概念很流行也很容易实现。但是，如果使用订阅者/发布者模型来取代传统的做法，带来的好处常常被人们忽视。

首先，使用交换中介可以简化很多设定线程通信的工作。与其通过多个模块将线程连接在一起，现在只需要关心将线程连接到一个已知的交换中介上就行了。从某种意义上说这和 logging 库的工作方式很相似。在实践中，这么做可以使解耦程序中的多个任务变得更加容易。

其次，交换中介具有将消息广播发送给多个订阅者的能力，这打开了新通信模式的大门。比如说，我们可以实现带有冗余任务、广播或者扇出（fan-out）的系统。也可以构建调试以及诊断工具，将它们作为普通的订阅者关联到交换中介上。下面是一个简单的诊断类，可以显示发送的消息：

class DisplayMessages: def __init__(self): self.count $= 0$ def send(self, msg): self.count $+ = 1$ print('msg[{}]: $\{!r\}$ '.format(self.count，msg)) exc $=$ get_exchange('name') d $=$ DisplayMessages() exc.attach(d)

最后但同样重要的是，这种实现方式有一个显著的方面就是它能和各种类似于任务的

对象一起工作。比如，消息的接收者可以是 actor（12.10 节中描述了actor任务）、协程、网络连接，甚至只要实现了合适的 send()方法的对象都可以。

关于交换中介，一个可能存在的问题就是如何以适当的方式对订阅者进行关联和取消关联处理。为了能正确管理资源，每个已经关联上的订阅者最终都必须取消关联。这就导致出现类似于下面示例的编程模型：

```python
exc = get_exchange('name')
exc.attach(some_task)
try:
    ...
finally:
    exc_detach(some_task) 
```

从某种意义上说，这和使用文件、锁以及类似的资源对象很相似。经验告诉我们，程序员常常会忘记最后的 detach()。为了简化这个步骤，或许会考虑使用上下文管理协议。例如，为交换中介添加一个 subscribe()方法：

from contextlib import contextmanager   
from collections import defaultdict   
class Exchange: def __init__(self): self._ subscribers $=$ set() def attach(self, task): self._ subscribers.add(task) def detach(self, task): self._ subscribers.remove(task)   
@contextmanager   
def subscribe(self, \*tasks): for task in tasks: self.attach(task) try: yield finally: for task in tasks: selfdetach(task) def send(self, msg): for subscriber in self._ subscribers: subscriber.send(msg)   
#Dictionary of all created exchanges

_exchanges $=$ defaultdict(Exchange)   
#Return the Exchange instance associated with a given name   
def get_exchange(name): return _exchanges[name]   
#Example of using the subscribe() method   
exc $=$ get_exchange('name')   
with exc subscribing(task_a, task_b): ... exc.send('msg1') exc.send('msg2') ...   
#task_a and task_b detached here

最后应该要提到的是，对于交换中介这个思想其实还有许多种可能的扩展。例如，交换中介可以实现一整个消息通道的集合，或者对交换中介的名称施加模式匹配的规则。交换中介也可以扩展到分布式计算的应用中去（例如，将消息在不同机器上的任务之间进行路由）。

# 12.12 使用生成器作为线程的替代方案

# 12.12.1 问题

我们想用生成器（协程）作为系统线程的替代方案来实现并发。协程有时也称为用户级线程或绿色线程。

# 12.12.2 解决方案

要利用生成器来实现自己的并发机制，首选需要对生成器函数和 yield 语句的基本原理有所了解。特别是关于 yield 的基本行为，即，使得生成器暂停执行。由于可以暂停执行，因此可以编写一个调度器将生成器函数当做一种“任务”来对待，并通过使用某种形式的任务切换来交替执行这些任务。

为了说明这个思想，考虑下面两个生成器函数：

Two simple generator functions   
def countdown(n): while $n > 0$ print('T-minus'，n) yield n $\equiv$ 1 print('Blastoff!')

```python
def countup(n):
    x = 0
while x < n:
    print('Counting up', x)
yield
x += 1 
```

这些函数可能看起来有些滑稽，因为它们都使用了单独的 yield 语句。但是请考虑下面的代码，我们给出了一个简单的任务调度器实现：

from collections import deque   
class TaskScheduler: def__init__(self): self._task_queue $=$ deque() def new_task(self,task): Admit a newly started task to the scheduler \*\*\* self._task_queue.append(task) def run(self): \*\*\* Run until there are no more tasks \*\*\* while self._task_queue: task $=$ self._task_queue.popleft() try: #Run until the next yield statement next(task) self._task_queue.append(task) except StopIteration: #Generator is no longer executing pass   
#Example use   
sched $=$ TaskScheduler()   
sched.new_task(countdown(10))   
sched.new_task(countdown(5))   
sched.new_task(countup(15))   
sched.run()

在这份代码中，TaskScheduler 类以循环的方式运行了一系列的生成器函数——每个都运行到 yield 语句就暂停。例如，上面程序的输出如下：

```txt
T-minus 10  
T-minus 5  
Counting up 0  
T-minus 9  
T-minus 4  
Counting up 1  
T-minus 8  
T-minus 3  
Counting up 2  
T-minus 7  
T-minus 2  
... 
```

此时如果愿意的话，已经基本上实现了一个微型“操作系统”的核心。生成器函数就是任务，而 yield 语句就是通知任务需要暂停挂起的信号。调度器只是简单地轮流执行所有的任务，直到没有一个任务还能执行为止。

在实践中，我们很可能不会用生成器来实现像示例这么简单的并发处理。相反，当实现 actor 或者网络服务器时，可能会用生成器来取代线程。

下面的代码用生成器来实现 actor，完全没有用到线程：

from collections import deque   
class ActorScheduler:   
```python
def __init__(self):
    self._actors = {}
    self._msg_queue = deque()  # Mapping of names to actors
    def new Actor(self, name, actor):
        Admit a newly started actor to the scheduler and give it a name
        self._msg_queue.append((actor, None))
        self._actors[name] = actor
def send(self, name, msg):
    Send a message to a named actor
    actor = self._actors.get(name)
    if actor:
        self._msg_queue.append((actor, msg)) 
```

Run as long as there are pending messages.   
while self._msg_queue: actor, msg $=$ self._msg_queue.popleft() try: actor.send(msg) except StopIteration: pass   
#Example use   
if_name $\equiv$ 'main': def printer(): while True: msg $=$ yield print('Got:'，msg)   
def counter(sched): while True: #Receive the current count n $=$ yield if n $= = 0$ break # Send to the printer task sched.send('printer'，n) #Send the next count to the counter task (recursive) sched.send('counter',n-1)   
sched $=$ ActorScheduler() #Create the initial actors sched.newActor('printer', printer()) sched.newActor('counter', counter(sched))   
# Send an initial message to the counter to initiate sched.send('counter'，10000) sched.run()

这段代码的执行流程可能需要研究一番，但是关键点就在于挂起的消息组成的队列上。基本上，只要有消息需要传递，调度器就会运行。这里有一个值得注意的特性，counter生成器发送消息给自己并进入一个递归循环，但却并不会受到 Python 的递归限制。

下面有一个高级的示例，展示了如何使用生成器来实现一个并发型的网络应用：

```python
from collections import deque  
from select import select 
```

```txt
This class represents a generic yield event in the scheduler 
```

class YieldEvent:   
# Task Scheduler   
```python
def handle=Yield(self, sched, task):
    pass
def handleResume(self, sched, task):
    pass 
```

class Scheduler:   
```python
def __init__(self):
    self._numtasks = 0
    self._ready = deque()
    self._read_waiting = {}
    self._write_waiting = {}
    # Total num of tasks
    # Tasks ready to run
    # Tasks waiting to read
    # Tasks waiting to write 
```

# Poll for I/O events and restart waiting tasks   
```python
def _iopoll(self):
    rset, wset, eset = select(self._read_waiting,
                     self._write_waiting,[])
    for r in rset:
        EVT, task = self._read_waiting.pop(r)
        EVT.handle Resume(self, task)
    for w in wset:
        EVT, task = self._write_waiting.pop(w)
        EVT.handle Resume(self, task) 
```

```python
def new(self,task):
    ...
    Add a newly started task to the scheduler
    ...
    self._ready.append((task, None))
    self._numtasks += 1 
```

```python
def add_ready(self, task, msg=None):
    ...
    Append an already started task to the ready queue. msg is what to send into the task when it resumes.
    ...
    self._ready.append((task, msg)) 
```

# Add a task to the reading set   
```python
def _read_wait(self, fileno, EVT, task):
    self._read_waiting[fileno] = (evt, task) 
```

# Add a task to the write set   
```txt
def _write_wait(self, fileno, EVT, task): 
```

```python
def run(self):
    ...
    Run the task scheduler until there are no tasks
    ...
while self._numtasks:
    if not self._ready:
        self._iopoll()
        task, msg = self._ready.popleft()
        try:
            # Run the coroutine to the next yield
            r = task.send(msg)
            if isinstance(r, YieldEvent):
                r.handle=Yield(self, task)
            else:
                raise ValueError('unrecognized yield event')
except StopIteration:
    self._numtasks -= 1
# Example implementation of coroutine-based socket I/O
class ReadSocket(YieldEvent):
    def __init__(self, sock, nbytes):
        self.sock = sock
        self.nbytes = nbytes
    def handle=Yield(self, sched, task):
        sched._read_wait(self.sock.fileno(), self, task)
    def handleResume(self, sched, task):
        data = self.sock.recv(self.nbytes)
        sched.add_ready(task, data)
class WriteSocket(YieldEvent):
    def __init__(self, sock, data):
        self.sock = sock
        self.data = data
    def handle_Yield(self, sched, task):
        sched._write_wait(self.sock.fileno(), self, task)
    def handle Resume(self, sched, task):
        nsent = self.sock.send(self.data)
        sched.add_ready(task, nsent)
class AcceptSocket(YieldEvent):
    def __init__(self, sock):
        self.sock = sock
    def handle_Yield(self, sched, task):
        sched._read_wait(self.sock.fileno(), self, task) 
```

def handleResume(self,sched，task)： r $=$ self.sock.accept() sched.add_ready(task,r)

# Wrapper around a socket object for use with yield   
```python
class Socket(object):
    def __init__(self, sock):
        self._sock = sock
    def recv(self, maxbytes):
        return ReadSocket(self._sock, maxbytes)
    def send(self, data):
        return WriteSocket(self._sock, data)
    def accept(self):
        return AcceptSocket(self._sock)
    def __getattr__(self, name):
        return getattr(self._sock, name) 
```

if _name $= =$ 'main'： from socket import socket, AF_INET, SOCK_STREAM import time #Example of a function involving generators. This should # be called using line $=$ yield fromiline(sock) def readline(sock): chars $= []$ while True: c $=$ yield sock.recv(1) if not c: break chars.append(c) if $\mathrm{c} = =\mathrm{b}^{\prime}\backslash \mathrm{n}^{\prime}$ ： break return b''.join(chars)

# Echo server using generators   
class EchoServer: def __init__(self,addr,sched): self.sched $=$ sched sched.new(self.server_loop(addr)) def server_loop(self,addr): s $=$ Socket(socket(AF_INET,SOCK_STREAM)) s.bind(addr) s.listen(5) while True: $\mathrm{c},\mathrm{a} =$ yields.accept()

```python
print('Got connection from ', a)  
self.sched.new(self.client_handler(Socket(c)))  
def clienthandler(self,client):  
    while True:  
        line = yield fromiline(client)  
        if not line:  
            break  
            line = b'GOT:' + line  
        while line:  
            nsent = yield client.send(line)  
            line = line[nsent:]  
        client.close()  
    print('Client closed')  
sched = Scheduler()  
EchoServer(['',16000], sched)  
sched.run() 
```

这段代码显然需要花上一段时间来仔细研究。然而，这基本上实现了一个小型的操作系统。有一个队列（以deque 实现）用来保存处于就绪态的任务，还有等候区（以字典实现）用来保存因为等待 I/O 而进入休眠状态的任务。调度器很大程度上就是把任务在就绪队列和 I/O 等待区之间来回移动。

# 12.12.3 讨论

当构建基于生成器的并发框架时，使用一般形式的 yield 是最为常见的：

```python
def some generator(): result = yield data 
```

以这种形式使用 yield 的函数更常被称为“协程”。在调度器内部，yield 语句是在循环中按照如下方式来处理的：

f $=$ somegenerated()   
#Initial result. Is None to start since nothing has been computed result $=$ None   
while True: try: data $=$ f.send(result) result $=$ ...do some calculation .. except StopIteration: break

这里关于 result 的逻辑似乎有点令人费解。然而，传递给 send()的值就是用来定义当 yield语句恢复执行后返回的结果。因此，如果有 yield 语句要返回一个结果作为对之前产生的数据的响应，那么它就会在下一个 send()操作中返回。如果某个生成器函数刚刚启动，发送 None 给它会让它前进到第一个 yield 语句的位置。

除了可以发送值以外，还可以在生成器上执行 close()方法。这么做会导致在 yield 语句上产生一个无声的 GeneratorExit 异常，这会终止生成器的执行。如果需要的话，生成器可以捕获这个异常并执行清理操作。也可以使用生成器的 throw()方法在 yield 语句上产生一个任意的异常。任务调度器可能会使用这个异常给运行中的生成器传达错误信息。

最后那个示例中用到的 yield from 语句是用来实现协程的，它可以作为子例程（subroutine）或过程（procedure）从其他的生成器中调用。基本上来说，程序的控制流是以透明的方式转移到新的函数中的。与普通的生成器不同的是，采用 yield from调用的函数，其返回值可以成为 yield from 语句的结果。有关 yield from 的更多信息可以在PEP 380（http://www.python.org/dev/peps/pep-0380）中找到。

最后，如果要用生成器来编程，需要重点强调关于生成器的一些主要局限。尤其是，线程所提供的优势在生成器中都不复存在了。例如，如果执行了任何 CPU 密集型或者 I/O 阻塞型的代码，这就会使整个任务调度器挂起，直到完成全部操作为止。为了规避这个限制，唯一的选择就是将这个操作转移到一个可以独立运行的线程或进程中执行。另一个限制在于大多数 Python 库的实现还不能和基于生成器的线程很好地配合在一起使用。如果选择了这种方法，会发现需要为许多标准库函数编写新的替换版本。有关协程的基础背景和本节中所采用的技术，可以参阅 PEP 342（http://www.python.org/dev/peps/pep-0342）以及 David Beazley 在 PyCon 2009 上做的报告“A Curious Course On Coroutines and Concurrency”（http://www.dabeaz.com/coroutines）。

PEP 3156（http://www.python.org/dev/peps/pep-3156）中也采用了现代化的方法来处理异步 I/O，其中也涉及了协程。在实践中自己编写一个底层的协程调度器是极不可能的。然而，有许多流行的 Python 程序库都是以协程的思想为基础的，这包括 gevent（http://www.gevent.org）、greenlet（http://pypi.python.org/pypi/greenlet）、Stackless Python（http://www.stackless.com）以及其他类似的项目。

# 12.13 轮询多个线程队列

# 12.13.1 问题

我们有一组线程队列，想轮询这些队列来获取数据。很大程度上这和轮询一组网络连接来获取数据类似。

# 12.13.2 解决方案

对于轮询问题，我们常用的解决方案中涉及一个鲜为人知的技巧，即利用隐藏的环回（loopback）网络连接。基本上来说思路是这样的：针对每个想要轮询的队列（或任何对象），创建一对互联的 socket。然后对其中一个 socket 执行写操作，以此表示数据存在。另一个 socket 就传递给 select()或者类似的函数来轮询数据。下面用一些简单的代码来说明这个思路：

import queue   
import socket   
import os   
class PollableQueue(queueQueue): def __init__(self): super().__init_(） #Create a pair of connected sockets if os.name $= =$ 'posix': self._putsocket,self._getsocket $=$ socket(socketpair() else: #Compatibility on non-POSIX systems server $=$ socket(socket(socket.AF_INET,socket.SOCK_STREAM) server.bind((127.0.0.1'，0)) server.listen(1) self._putsocket $=$ socket(socket(socket.AF_INET,socket.SOCK_STREAM) self._putsocket.connect(server.getsockname()) self._getsocket， $\equiv$ server.accept() server.close()   
def fileno(self): return self._getsocket.fileno()   
def put(self, item): super().put(item) self._putsocket.send(b'x')   
def get(self): self._getsocket.recv(1) return super().get()

在这份代码中，我们定义了一种新的 Queue 实例，其底层有一对互联的 socket。在UNIX上用 socketpair()函数来建立这样的 socket 对是非常容易的。在 Windows 上，我们不得不使用示例中展示的方法来伪装 socket 对（这看起来有些怪异，首先创建一个服务器socket，之后立刻创建客户端 socket并连接到服务器上）。之后对 get()和 put()方法做了些微重构，在这些 socket上执行了少量的 I/O 操作。put()方法在将数据放入队列之后，

对其中一个 socket 写入了一个字节的数据。当要把数据从队列中取出时，get()方法就从另一个 socket中把那个单独的字节读出。

fileno()方法使得这个队列可以用类似 select()这样的函数来轮询。基本上来说，fileno()方法只是暴露出底层由 get()函数所使用的 socket 的文件描述符。

下面的代码示例定义了一个消费者，用来在多个队列上监视是否有数据到来：

import select   
import threading   
def consumer queues): ' ' Consumer that reads data on multiple queues simultaneously ' while True: can_read,_,_ $=$ select.select( queues,[],[]) for r in can_read: item $\equiv$ r.get() print('Got:'，item)   
q1 $=$ PollableQueue()   
q2 $=$ PollableQueue()   
q3 $=$ PollableQueue()   
t $=$ threading.Thread(target=consumer, args $\coloneqq$ ([q1,q2,q3]，))   
t.daemon $=$ True   
t.start()   
#Feed data to the queues   
q1.put(1)   
q2.put(10)   
q3.put('hello')   
q2.put(15)   
...

如果试着运行这段代码，就会发现不管把数据放入到哪个队列中，消费者最后都能接收到所有的数据。

# 12.13.3 讨论

要对非文件类型的对象比如队列做轮询操作，通常都是看起来简单做起来难。比如说，如果不采用本节展示的 socket 技术，那唯一的选择就是遍历所有的队列，分别判断每个队列是否为空，而且还得使用定时器（避免 CPU 利用率达到 $100 \%$ ）。就像下面这样：

```python
import time
def consumer共产ess): 
```

```python
while True:  
    for q in queues:  
        if not q.empty():  
            item = q.get()  
            print('Got:', item)  
    # Sleep briefly to avoid 100% CPU time.sleep(0.01) 
```

这对于某些特定类型的问题或许是行得通的，但是这么做很笨拙，而且还会引入奇怪的性能方面的问题。例如，如果新的数据添加到了一个队列中，那么至少有10 毫秒的时间才能检测到（对于一个现代的处理器来说，10 毫秒就好像下辈子那么漫长）。

如果将上面这种轮询方式同其他的轮询对象（比如 socket）混在一起使用，那么会遇到更多问题。例如，如果想同时轮询 socket和队列，那就不得不使用这样的代码①：

import select   
def event_loop(sockets, queues): while True: #polling with a timeout can_read,_ $\equiv$ select.select(sockets，[]，[]，0.01) for r in can_read: handle_read(r) for q in queues: if not q.empty(): item $=$ q.get() print('Got:'，item)

本节给出的解决方案解决了很多这样的问题。这就是通过把队列放在和 socket 同等的地位上来解决的。只要一个单独的 select()调用就可以轮询这两种对象的活跃性。不需要使用超时或其他基于时间的技巧来做周期性的检查。此外，如果数据添加到了队列中，消费者几乎能在同一时间得到通知。尽管底层的I/O 会带来一点小小的负载（即，在底层的 socket对上写入和读出一字节的数据），但由于可以获得更好的响应时间以及简化了代码的编写，因此这么做通常都是很值得的。

# 12.14 在 UNIX 上加载守护进程

# 12.14.1 问题

我们想编写一个程序，使它能够在UNIX或类UNIX的操作系统上以守护进程的方式运行。

# 12.14.2 解决方案

创建一个合适的守护进程需要以精确的顺序调用一系列的系统调用，并小心注意其中的细节。下面的代码定义了一个守护进程，附带还有当启动之后可以轻易让它停止运行的能力：

#!/usr/bin/env python3   
# daemon.py   
import os   
import sys   
import atexit   
import signal   
def daemonize(pidfile, \*, stdin $= 1$ /dev/null', stdout $= 1$ /dev/null', stderr $= 1$ /dev/null'): if os.path.exists(pidfile): raise ValueError('Already running') # First fork (detaches from parent) try: if os.fork() > 0: raise SystemExit(0) # Parent exit except OSError as e: raise ValueError('fork #1 failed.') os.chdir('/') os.umask(0) ossetsid() # Second fork (relinquish session leadership) try: if os.fork() > 0: raise SystemExit(0) except OSError as e: raise ValueError('fork #2 failed.') # Flush I/O buffers sys.stdout.flush() sys/stderr.flush() # Replace file descriptors for stdin, stdout, with open(stdin, 'rb', 0) as f: os dup2(f.fileno(), sysstdin.fileno()) with open(stdout, 'ab', 0) as f:

```python
os.dup2(f.fileno(), sys.stdout.fileno())
with open(stderr, 'ab', 0) as f:
    os.dup2(f.fileno(), sys stderr.fileno())
# Write the PID file
with open(pidfile, 'w') as f:
    print(os.getpid(), file=f)
# Arrange to have the PID file removed on exit/signal
atexit.register( lambda: os.remove(pidfile))
# Signal handler for termination (required)
def sigtermhandler(signo, frame):
    raise SystemExit(1)
signal.signal(signal.SIGTERM, sigtermhandler)
def main():
    import time
sys.stdout.write(' Daemon started with pid {},\n'.format(os.getpid'))
while True:
    sys.stdout.write(' Daemon Alive! {},\n'.format(time.etime'))
time.sleep(10)
if __name__ == '__main__':
PIDFILE = '/tmp/daemon.pid'
if len(sys.argv) != 2:
    print('Usage: {} [start|stop].format(sys.argv[0]), file=sys.stdev)
raise SystemExit(1)
if sys.argv[1] == 'start':
    try:
        daemonize(PIDFILE,
        stdout='/tmp/daemon.log',
        stderr='/tmp/dameon.log')
except TimeoutError as e:
    print(e, file=sys.stderr)
raise SystemExit(1)
main()
elif sys.argv[1] == 'stop':
if os.path.exists(PIDFILE):
    with open(PIDFILE) as f:
        os.kill(int(f.read(), signal.SIGTERM)
else: 
```

```python
print('Not running', file=sys.stdout) raise SystemExit(1)   
else: print('Unknown command \{!r\}'.format(sys.argv[1]), file=sys.stdout) raise SystemExit(1) 
```

要加载这个守护进程，需要使用下面这样的命令：

```txt
bash % daemon.py start  
bash % cat /tmp/daemon.pid  
2882  
bash % tail -f /tmp/daemon.pid  
 Daemon started with pid 2882  
 Daemon Alive! Fri Oct 12 13:45:37 2012  
 Daemon Alive! Fri Oct 12 13:45:47 2012  
... 
```

守护进程完全在后台运行，因此命令会立刻返回。但是，可以像上面的命令那样检查与守护进程相关的 pid 文件和日志。要停止这个守护进程，可以这样：

```batch
bash % daemon.py stop  
bash % 
```

# 12.14.3 讨论

本节定义的 daemonize()函数可以在程序启动的时候调用，这样可以使程序以守护进程的方式运行。daemonize()的函数签名采用的是 keyword-only 参数，这样当使用可选参数时能让意图显得更加清晰。这迫使用户必须这样来调用函数：

```m4
daemonize('daemon.pid',  
stdin='/dev/null,  
stdout='/tmp/daemon.log',  
stderr='/tmp/daemon.log') 
```

而不是使用下面这种神秘难懂的调用方式：

```javascript
Illegal. Must use keyword arguments daemonize('daemon.pid', '/dev/null', '/tmp/daemon.log', '/tmp/daemon.log') 
```

接下来创建守护进程的步骤就更加晦涩了。基本思想就是，首先，守护进程必须将它自己与父进程分离开来。这就是第一个 os.fork()操作完成后立刻终结父进程的目的所在。

当子进程成为孤儿后，就调用 os.setsid()创建一个全新的进程会话，并将子进程设为会话的头领。这么做也将子进程设为新的进程组的头领进程，并确保没有任何与之关联

的控制终端。如果这听起来显得很神奇，那么这实际上是为了以合适的方式将守护进程从终端中分离开来，并确保像信号这样的东西不会影响到守护进程的运行。

对 os.chdir()和 os.umask(0)的调用将改变当前的工作目录，并重置文件模式掩码。改变工作目录通常是个好主意，这样守护进程就不再工作于加载它的那个目录之下了。

第二个os.fork()调用是目前为止最为神秘的操作。这一步使得守护进程放弃获取一个新的控制终端的能力，并为自己和外部环境提供了更多的隔离（从根本上说，守护进程会放弃它的会话头领位置，因此不再具有打开控制终端的权限了）。尽管可以忽略这一步，但一般来说还是推荐这么做的。

一旦守护进程已经以合适的方式完成了分离，就执行后续的步骤来重新初始化标准 I/O流，使其指向由用户指定的文件。这一部分的处理实际上也需要用到一些技巧。在Python 解释器中可以找到多个与标准 I/O 流相关联的文件对象引用（比如 sys.stdout、sys.__stdout__等）。只关闭 sys.stdout 并为其重新赋值并不能保证可以正常工作，因为没法知道这么做是否可以修改到所有对 sys.stdout 的引用。相反，我们打开一个单独的文件对象，并通过 os.dup2()调用来取代当前由 sys.stdout 所使用的文件描述符。当这一切发生的时候，sys.stdout 原来所用的文件会被关闭，并用新的文件取代它的位置。必须要强调的是，任何已经作用于标准 I/O 流的文件编码或文本处理将保持原样。

守护进程的一种常见做法就是将它的进程 ID 写入到一个文件中，以便稍后给其他的程序使用。daemonize()函数最后的部分正是在执行写文件的操作，但同样也做了安排，可以让这个文件在程序终止时被删除。由 atexit.register()注册的函数可以在 Python 解释器退出时得到执行。针对信号 SIGTERM 所定义的信号处理例程同样需要以优雅得体的方式退出。这个信号处理例程只是发出 SystemExit()异常，除此之外什么都不做。这乍看起来是没有必要的，但如果没有这个信号处理例程，终止信号就会杀死解释器进程而不会执行注册到 atexit.register()中的清理函数。杀死守护进程的代码示例可以在程序结尾部分处理 stop 命令的地方找到。

更多关于编写守护进程的信息可以在 W.Richard Stevens 和 Stephen A.Rago 合著的Advanced Programming in the UNIX Environment，2nd Edition（Addison-Wesley，2005）中找到。尽管此书的重点是用 C 语言来编程，但所有的内容都很容易适应于 Python。因为所有所需的 POSIX 函数都可以在 Python 标准库中找到。

# 实用脚本和系统管理

有很多人把 Python 当做 shell脚本的替代，用来实现系统任务的自动化处理，比如操纵文件、配置系统等。本章的主要目标是描述编写脚本时常会遇到的一些任务。比如，解析命令行选项、操纵文件系统中的文件、获取有用的系统配置数据等。本书第 5 章中也包含了一些与文件和目录相关的信息。

# 13.1 通过重定向、管道或输入文件来作为脚本的输入

# 13.1.1 问题

我们希望自己编写的脚本能够接受任意一种对用户来说最为方便的输入机制。这应该包括从命令中产生输出给脚本、把文件重定向到脚本，或者只是在命令行中传递一个或者一列文件名给脚本。

# 13.1.2 解决方案

Python 内置的 fileinput 模块使得这一切变得非常简单。如果有一个类似下面这样的脚本：

#!/usr/bin/env python3   
import fileinput   
with fileinput(input() as f_input: for line in f_input: print(line, end $= 1$

那么已经可以让脚本按照上述所有的方式来接收输入了。如果将这个脚本保存为

filein.py 并使其成为可执行的，那么就能够完成下列所有的操作并得到期望的输出：

```shell
$ ls | ./filein.py
#Prints a directory listing to stdout.
$ ./filein.py /etc/passwd
#Reads /etc/passwd to stdout.
$ ./filein.py < /etc/passwd
#Reads /etc/passwd to stdout. 
```

# 13.1.3 讨论

函数 fileinput.input()创建并返回一个 FileInput 类的实例。除了包含有一些方便实用的帮助函数外，该实例还可以当做上下文管理器来用。因此，把所有这些结合在一起，如果我们编写一个脚本期望它能立刻从多个文件中打印输出，我们可以在输出中包含文件名和行号信息，就像下面这样：

```python
>>> import fileinput
>>> with fileinput(input('/etc/passwd')) as f:
>>>     for line in f:
...         print(ffilename(), f.lineno(), line, end='')
...
/etc/passwd 1    #
/etc/passwd 2   # User Database
/etc/passwd 3   #
<other output omitted> 
```

把它当做上下文管理器来使用可确保文件不再使用时会被关闭，此外我们这里还利用了 FileInput 实例的帮助函数来获取一些额外的信息。

# 13.2 终止程序并显示错误信息

# 13.2.1 问题

我们想让自己的程序在终止时向标准错误输出打印一条消息并返回一个非零的状态码。

# 13.2.2 解决方案

要让程序以这种方式终止，可以发出一个 SystemExit 异常，但是要提供错误信息作为参数。示例如下：

```txt
raise SystemExit('It failed!') 
```

这会导致提供的信息被打印到 sys.stderr 上，且程序退出时的状态码为 1。

# 13.2.3 讨论

本节的内容十分短小，但是解决了编写脚本时的一个常见问题。即，要终止一个程序，以前可能会倾向于编写这样的代码：

```python
import sys   
sys.stderr.write('It failed!\\n')   
raise SystemExit(1) 
```

现在，不需要再同这些 import 或者 sys.stderr 搅和在一起了，只需要给 SystemExit()提供一条错误信息即可。

# 13.3 解析命令行选项

# 13.3.1 问题

我们想编写一个程序用来解析在命令行中提供的各种选项（选项保存在 sys.argv 中）。

# 13.3.2 解决方案

可以用argparse 模块来解析命令行选项。我们用一个简单的例子来帮助说明这里的核心特性：

```python
# search.py
```
Hypothetical command-line tool for searching a collection of files for one or more text patterns.
```
import argparse
parser = argparse.ArgUMENTParser(description='Search some files')
parser.add_arguet (dest='filenames', metavar='filename', nargs='*')
parser.add_arguet ('-p', '-pat', metavar='pattern', required=True, dest='patterns', action='append',
            help='text pattern to search for')
parser.add_arguet ('-v', dest='verbose', action='store_true',
            help='verbose mode')
parser.add_arguet ('-o', dest='outfile', action='store',
            help='output file')
parser.add_arguet ('--speed', dest='speed', action='store',
            choices={'slow','fast'}, default='slow',
            help='search speed')
args = parser.parse_args()
# Output the collected arguments
print(args.filenames)
print(args~-patterns)
print(argsverbose) 
```

print(args.outfile)

print(args.speed)

这个程序定义了一个命令行解析器，其用法是这样的：

```txt
bash % python3 search.py -h  
usage: search.py [-h] [-p pattern] [-v] [-o OUTFILE] [--speed {slow,fast}]  
[filename [filename ...]]  
Search some files  
positional arguments:  
filename  
optional arguments:  
-h, --help show this help message and exit  
-p pattern, --pat pattern  
text pattern to search for  
-v verbose mode  
-o OUTFILE output file  
--speed {slow,fast} search speed 
```

接下来的交互式会话展示了数据在程序中的显示方式，请仔细观察 print()语句的输出。

```batch
bash % python3 search.py foo.txt bar.txt  
usage: search.py [-h] -p pattern [-v] [-o OUTFILE] [--speed {fast,slow}]  
[filename [filename ...]]  
search.py: error: the following arguments are required: -p/--pat  
bash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt  
filenames = ['foo.txt', 'bar.txt']  
patterns = ['spam', 'eggs']  
verbose = True  
outfile = None  
speed = slow  
bash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt -o results  
filenames = ['foo.txt', 'bar.txt']  
patterns = ['spam', 'eggs']  
verbose = True  
outfile = results  
speed = slow  
bash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt -o results \ --speed=fast  
filenames = ['foo.txt', 'bar.txt']  
patterns = ['spam', 'eggs']  
verbose = True  
outfile = results  
speed = fast 
```

如何对选项做进一步的处理则取决于程序员自己。只要把 print()替换成其他更有意义的函数即可。

# 13.3.3 讨论

argparse模块是标准库中最为庞大的模块之一，有着非常多的配置选项。本节仅展示其中的基本子集，可以通过使用这些子集来入门并进行扩展。

要解析命令行选项，首先需要创建一个 ArgumentParser 实例，并通过使用 add_argument()方法来添加想要支持的选项声明。在每个 add_argument()调用中，参数 dest 指定了用来保存解析结果的属性名称。而当产生帮助信息时会用到参数 metavar。参数 action则指定了与参数处理相关的行为，通常用 store 来表示存储单个值，或者用 append 来表示将多个值保存到一个列表中。

下面的参数表示将所有额外的命令行参数保存到一个列表中。在示例中，这条语句用来创建一个文件名列表：

```txt
parser.add_argument(dest='filenames', metavar='filename', nargs='*') 
```

接下来的参数设定了一个布尔标记，标记的值取决于参数是否有提供：

```python
parser.add_argument('--v',dest='verbose',action='store_true', help='verbose mode') 
```

下面的参数表示接受一个单独的值并将其保存为字符串：

```txt
parser.add_argument('-o',dest='outfile',action='store', help='output file') 
```

下面语句中指定的参数可允许命令行参数重复多次，并将所有的参数值保存在列表中。required 标记意味着参数必须至少要提供一次。使用-p 和--pat 表示这两种选项名称都是可接受的。

```python
parser.add_argument('p', "--pat', metavar='pattern', required=True, dest='patterns', action='append', help='text pattern to search for') 
```

最后，下面语句中指定的参数可接受一个值，但会将这个值同一组可能的选择做对比。

```javascript
parser.add_argument('--speed',dest='speed',action='store', choices={'slow','fast'},default='slow', help='search speed') 
```

一旦选项已经给出，只需要简单地执行 parser.parse()方法。这么做会处理 sys.argv 的值，并返回结果实例。每个命令行参数解析出的结果都会保存在由 dest 参数所指定的对应的属性中。

还有其他几种方法可用来解析命令行选项。例如，我们可能会倾向于自己手动处理 sys.argv或者使用 getopt模块（仿照类似的C库打造）。但是，如果采用这种方法，那就会导致重复编写很多 argparse 已经提供的代码。也许会遇到使用 optparse 库来解析命令行选项的代码。尽管 optparse 和 argparse 很相似，但后者更加现代化，在新项目中应该优先选择使用 argparse。

# 13.4 在运行时提供密码输入提示

# 13.4.1 问题

我们已经编写好了一个脚本，其中需要用户输入密码。但是由于脚本是用来做交互式使用的，我们想为用户提供密码输入提示（此时用户输入的密码不会显示在终端上）而不是将其硬编码到脚本中。

# 13.4.2 解决方案

在这种情况下，Python 的 getpass 模块正是你所需要的。它可以让我们非常方便地为用户提供密码输入，而不会将输入的密码显示在终端屏幕上。示例如下：

import getpass   
user $=$ getpass.getuser()   
passwd $=$ getpass.getpass()   
ifsvc_login(user,passwd):#You must writesvc_login() print('Yay!')   
else: print('Boo!')

在上述代码中，函数svc_login()是我们必须自行编写的代码，用来进一步处理输入的密码。显然，具体的处理步骤是特定于应用的。

# 13.4.3 讨论

注意，在上面的代码中，getpass.getuser()并不会提示用户输入用户名。相反，它会根据用户的 shell 环境使用当前的用户登录名。或者作为最后的手段，以本地系统的密码数据库（在支持 pwd 模块的平台上）为支撑。

如果为了更加可靠，想显式给用户提供用户名输入，那么可以使用内置的 input 函数：

```txt
user = input('Enter your username:') 
```

同样需要记得的是，在有些系统上可能不支持将输入给 getpass()方法的密码做隐藏处

理。在这种情况下，Python 会竭尽所能的发出预警信息（例如，在继续处理前警告你密码将以明文形式显示）。

# 13.5 获取终端大小

# 13.5.1 问题

我们需要获取终端的大小，以此对程序的输出做适当的格式化处理。

# 13.5.2 解决方案

可以使用 os.get_terminal_size()函数来办到：

```txt
>>> import os
>>> sz = os.get-terminal_size()
>>> sz
os终端_size(columns=80, lines=24)
>>> sz.columns
80
>>> sz-lines
24
>>> 
```

# 13.5.3 讨论

还有许多其他的方法来获取终端的大小，从读取环境变量到执行涉及 ioctl()和 TTY 的底层系统调用都可以。坦白地说，如果一个简单的调用就能解决问题，为什么还要操心那些细节呢？

# 13.6 执行外部命令并获取输出

# 13.6.1 问题

我们想执行一个外部命令并把输出保存为一个 Python 字符串。

# 13.6.2 解决方案

可以使用函数 subprocess.check_output()来完成。示例如下：

```python
import subprocess  
out_bytes = subprocess.check_output(['netstat', '-a']) 
```

这么做会运行指定的命令，并将输出结果以字节串的形式返回。如果需要将返回的结

果字节以文本的形式来解读，可以再增加一个解码的步骤。示例如下：

```python
out_text = out_bytesdecode('utf-8') 
```

如果执行的命令返回了一个非零的退出码，那么就会产生一个异常。下面的示例可以捕获错误并获取创建的输出以及退出码：

```python
try: out_bytes = subprocess.check_output(['cmd', 'arg1', 'arg2']) except subprocessCalledProcessError as e: out_bytes = e.output # Output generated before error code = e.returncode # Return code 
```

默认情况下，check_output()只会返回写入到标准输出中的结果。如果希望标准输出和标准错误输出都能获取到，那么可以使用参数 stderr：

```python
out_bytes = subprocess.check_output(['cmd', 'arg1', 'arg2'],
stderr=subprocess.STDOUT) 
```

如果需要执行一个带有超时机制的命令，可以使用参数 timeout：

try: out_bytes $=$ subprocess.check_output(['cmd','arg1'，'arg2'],timeout=5) except subprocess_TIMEOUT as e:

一般来说，命令的执行不需要依赖底层 shell 的支持（例如 sh、bash 等）。相反，我们提供的字符串列表会传递给底层的系统命令，比如 os.execve()。如果希望命令通过 shell来解释执行，只要将命令以简单的字符串形式提供并给定参数 shell=True 即可。如果打算让Python执行一个涉及管道、I/O重定向或其他特性的复杂shell命令时，这么做往往是很有用的。例如：

```python
out_bytes = subprocess.check_output('grep python | wc > out', shell=True) 
```

请注意，在shell下执行命令是有着潜在的安全威胁的，特别是当参数来自于用户的输入时更是如此。在这种情况下，shlex.quote()函数可用来正确引用包含在shell命令中的参数。

# 13.6.3 讨论

执行一个外部命令并获取输出，最简单的方法就是使用 check_output()函数了。但是，如果需要同一个子进程执行更加高级的通信，例如为其发送输入，那就需要采用不同的方法了。基于此，可以直接使用 subprocess.Popen 类。示例如下：

```txt
import subprocess 
```

```txt
Some text to send 
```

text $=$ b''   
hello world   
this is a test   
goodbye   
1   
# Launch a command with pipes   
p $=$ subprocess.Popen(['wc'], stdout $\equiv$ subprocess.PIPE, stdin $=$ subprocess.PIPE)   
# Send the data and get the output stdout, stderr $=$ pcommunicate(text)   
# To interpret as text,decode   
out $=$ stdoutdecode('utf-8')   
err $=$ stderrdecode('utf-8')

如果某个外部命令期望同一个真正的 TTY（即，终端设备）进行交互，那么 subprocess模块不适合同这样的外部命令进行通信。例如，我们不能用它来实现自动向用户请求输入密码的任务（比如一个 ssh 会话）。对于这种需求，需要使用第三方模块来完成，比如那些基于流行的“expect”族的工具（如 pexpect 或类似的工具）。

# 13.7 拷贝或移动文件和目录

# 13.7.1 问题

我们需要拷贝或移动文件和目录，但是不想通过调用 shell 命令来完成。

# 13.7.2 解决方案

shutil 模块中有着可移植的函数实现，可用来拷贝文件和目录，用法相当直接，示例如下：

```python
import shutil   
# Copy src to dst. (cp src dst)   
shutil.copy(src, dst)   
# Copy files, but preserve metadata (cp -p src dst)   
shutil.copy2(src, dst)   
# Copy directory tree (cp -R src dst)   
shutil.copytree(src, dst) 
```

# Move src to dst (mv src dst)

shutil.move(src, dst)

这些函数的参数全都是字符串，用来提供文件或目录的名称。如同注释中说明的那样，这些函数的底层语义是在尝试模仿类似的 UNIX 命令。

默认情况下，符号链接也适用于这些命令。例如，如果源文件是一个符号链接，那么目的文件将会是该链接所指向的文件的拷贝。如果只想拷贝符号链接本身，可以提供关键字参数 follow_symlinks，示例如下：

shutil.copy2(src, dst, follow_symlinks $=$ False)

如果想在拷贝的目录中保留符号链接，可以这么做：

shutil.copytree(src, dst, symlinks=True)

copytree()函数以可选的方式允许在拷贝的过程中忽略特定的文件和目录。为了做到这点，需要提供一个 ignore 函数，该函数以目录名和文件名作为输入参数，返回一列要忽略的名称作为结果。示例如下：

def ignore_pyc_files(dirname, filenames): return [name in filenames if name.endswith('.pyc')]

shutil.copytree(src, dst, ignore=ignore_pyc_files)

由于忽略文件名这种模式非常常见，已经有一个实用函数 ignore_patterns()提供给我们使用了。示例如下：

shutil.copytree(src, dst, ignore $=$ shutil.ignore_patterns('*~','*.pyc'))

# 13.7.3 讨论

大部分情况下用 shutil来拷贝文件和目录都是非常直接的。但是，需要注意的是，当考虑到文件的元数据时，类似 copy2()这样的函数只会尽最大努力来保存这些数据。一些基本信息像访问时间、创建时间以及权限信息总是会得到保存。但是属主、访问控制列表、资源派生（resource forks）以及其他扩展的文件元数据可能会也可能不会得到保存，这取决于操作系统底层和用户自身的访问权限。你很可能不会想去用 shutil.copytree()这样的函数来做系统备份。

当和文件名打交道时，确保使用的是 os.path 中的函数，这样可获得最佳的可移植性（尤其是如果需要同时运行于 UNIX 和 Windows 上时）。例如：

>>>filename $=$ '/Users/guido/programs/spam.py'   
>>>import os.path   
>>>os.path.baselname(filename)   
'spam.py'

```txt
>>> os.pathdirname(filename)  
'/Users/guido/programs'  
>>> os.path.split(filename)  
('/Users/guido/programs', 'spam.py')  
>>> os.path.join('/new/dir', os.path.baselname(filename))  
 '/new/dir/spam.py'  
>>> os.path.expanderuser('/~/guido/programs/spam.py')  
'/Users/guido/programs/spam.py'  
>>> 
```

用 copytree()来拷贝目录时，一个比较棘手的点在于错误处理。比如说，在拷贝过程中函数可能会遇到已经损坏的符号链接，或者由于权限问题导致有些文件无法访问等等诸如此类的问题。为了应对这些情况，所有遇到的异常都会收集到一个列表中并将其归组为一个单独的异常，在操作结束时抛出。示例如下：

```python
try:  
    shutil.copytree(src, dst)  
except shutil.Error as e:  
    for src, dst, msg in e.args[0]:  
        # src is source name  
        # dst is destination name  
        # msg is error message from exception  
        print(dest, src, msg) 
```

如果提供了关键字参数 ignore_dangling_sumlinks True，那么 copytree()将会忽略悬垂的符号链接。

本节中展示的函数可能是最常使用的了。但是，shutil 模块中还有许多有关拷贝数据的操作。进一步阅读相关的文档肯定是值得的，参见 http://docs.python.org/3/library/shutil.html。

# 13.8 创建和解包归档文件

# 13.8.1 问题

我们需要以常见的格式（如.tar、.tgz 或.zip）来创建或解包归档文件。

# 13.8.2 解决方案

shutil 模块中有两个函数——make_archive()和 unpack_archive()，它们正是我们所需要的。示例如下：

```python
>>> import shutil
>>> shutil unpack_archive('Python-3.3.0.tgz')
>>> shutil.make_archive('py33','zip','Python-3.3.0') 
```

```html
'/Users/beazley/Downloads/py33.zip'  
>>> 
```

make_archive()的第二个参数就是所期望的输出格式。要获取所支持的归档格式列表，可以使用 get_archive_formats()函数。示例如下：

```txt
>>>shutil.get_archiveFormats()   
[('bztar'，"bzip2'ed tar-file")，('gztar'，"gzip'ed tar-file")， ('tar'，'uncompressed tar file')，('zip'，'ZIP file')]]   
>>>
```

# 13.8.3 讨论

Python 中还有其他模块可用来处理各种归档格式的底层细节（例如 tarfile、zipfile、gzip、bz2 等）。但是，如果想做的只是创建或解包归档文件，那么确实没有必要使用如此底层的模块。可以直接使用 shutil 模块中的高层函数来解决。

这些函数有着许多额外的选项，可用于记录日志、文件权限等。可参阅 shutil模块的文档以获得更多的细节。

# 13.9 通过名称来查找文件

# 13.9.1 问题

我们需要编写一个涉及查找文件的脚本，比如给文件重命名或者日志归档程序。但是我们不想在 Python 脚本中调用 shell 实用程序，也不想提供特定的行为使得程序无法轻易地分发出去使用。

# 13.9.2 解决方案

搜索文件可使用 os.walk()函数，只要将顶层目录提供给它即可。下面示例中给出的函数用来查找一个特定的文件名，并将所有匹配结果的绝对路径打印出来：

#!/usr/bin/env python3.3   
import os   
def findfile(start, name): for relpath,dirs,files in os.walk(start): if name in files: full_path $=$ os.path.join(start,relipath,name) print(os.path.normpath(os.path.abspath(full_path)))   
if_name $\equiv$ 'main': findfile(sys.argv[1],sys.argv[2])

将这个脚本保存为 findfile.py 并从命令行运行，给定查找的起始点以及待匹配的文件名，就像下面这样：

bash % ./findfile.py . myfile.txt

# 13.9.3 讨论

os.walk()方法会为我们遍历目录层级，且对于进入的每个目录它都会返回一个 3 元组。这包含了正在检视的目录的相对路径、该目录中包含的所有目录名的列表，以及该目录中包含的所有文件名的列表。

对于每个元组，只需检查目标文件是否在 file 列表中即可。如果是，就用 os.path.join()来组成一个路径。为了避免出现可能像././foo//bar 这样的诡异路径，还要用到两个额外的函数来修正结果。第一个是 os.path.abspath()，它接受一个可能是相对的路径并将其组成绝对路径形式。第二个是 os.path.normpath()，它会将路径修正为标准化形式，从而帮助解决像双反斜线、多当前目录的多次引用等问题。

尽管这个脚本同 UNIX 平台上的 find 实用程序相比显得异常简单，但它具有跨平台的优势。此外，许多额外的功能都能够以可移植的方式加入进来而无需耗费太多精力。为了说明这一点，下面这个函数可打印出所有最近有修改过的文件：

#/usr/bin/env python3.3   
import os   
import time   
def modified_within(top, seconds): now $=$ time.time() for path,dirs,files in os.walk(top): for name in files: fullpath $=$ os.path.join(path,name) if os.path.exists(fullpath): mtime $=$ os.path.getmtime(fullpath) if mtime $>$ (now - seconds): print(fullpath)   
if_name_ $= =$ 'main': import sys if len(sys.argv) $! = 3$ : print('Usage:{}dir seconds'.format(sys.argv[0])) raise SystemExit(1)   
modified_within(sys.argv[1],float(sys.argv[2]))

在这个短小的函数之上构建更加复杂的操作并不会花费太多时间，只要使用 os、os.path、glob 以及类似模块中的各种功能即可。相关章节可参阅 5.11 节和 5.13 节。

# 13.10 读取配置文件

# 13.10.1 问题

我们想要读取以常见的.ini 格式所编写的配置文件。

# 13.10.2 解决方案

可以用 configparser 模块来读取配置文件。例如，假设有一个这样的配置文件：

; config.ini   
; Sample configuration file   
[installation] library $= \%$ (prefix)s/lib include $= \%$ (prefix)s/include bin $= \%$ (prefix)s/bin prefix=/usr/local   
# Setting related to debug configuration [debug] log Errors=true show warnings $\coloneqq$ False   
[server] port:8080 nworkers:32 pid-file=/tmp/spam.pid root=/www/root signature: Brought to you by the Python Cookbook

下面的示例告诉我们如何读取这个配置文件并提取出相应的值：

```txt
>>> from configparser import ConfigParser
>>>cfg = ConfigParser()
>>>cfg.read('config.ini')
['config.ini']
>>>cfgsections()
['installation', 'debug', 'server']
>>>cfg.get('installation', 'library') 
```

```txt
'/usr/local/lib'   
>>>cfg.getboolean('debug','logerrors')   
True   
>>>cfg.getint('server','port')   
8080   
>>>cfg.getint('server','nworkers')   
32   
>>>printcfg.get('server','signature'))   
Brought to you by the Python Cookbook 
```

如果需要，也可以使用 cfg.write()方法修改配置并写回到原文件中。示例如下：

```ini
>>>cfg.set('server','port','9000')
>>>cfg.set('debug','logerrors','False')
>>>import sys
>>>cfg.write(sys.stdout)
[installation]
library = %(prefix)s/lib
include = %(prefix)s/include
bin = %(prefix)s/bin
prefix = /usr/local
[debug]
log Errors = False
show warnings = False
[server]
port = 9000
nworkers = 32
pid-file = /tmp/spam.pid
root = /www/root
signature =
Brought to you by the Python Cookbook 
```

# 13.10.3 讨论

配置文件以易于人类阅读的格式对程序设定配置数据。在每个配置文件中，值被归组到不同的区段中（例如本例中的“installation”、“debug”和“server”），然后在每个区段中对各个变量设定值。

在配置文件和使用 Python 来编写的用于同样目的的源文件之间有着几个显著的区别。首先，前者的语法更加宽容和“草率”。例如，下面这些赋值语句的效果相同：

```txt
prefix=/usr/local  
prefix:/usr/local 
```

在配置文件中用到的名称也被认为是非大小写敏感的。例如：

```txt
>>>cfg.get('installation','PREFIX')'/usr/local'  
>>>cfg.get('installation','prefix')'/usr/local'  
>>> 
```

当解析值的时候，像 getboolean()这样的方法会检查任何合理的值。例如，下面这些语句的效果都是相同的：

```txt
log Errors = true  
log Errors = TRUE  
log errors = Yes  
log errors = 1 
```

和脚本不同，也许在配置文件和 Python 代码之间最大的区别在于配置文件不是按照从上到下的方式来执行的。相反，配置文件会被全部读取。如果其中出现变量替换的操作，则它们都是在文件全部读取之后才进行的。比如说在如下部分中，在其他变量使用 prefix 之前是否对它完成了赋值是无关紧要的。

```ini
[installation]
library=% (prefix)s/lib
include=% (prefix)s/include
bin=% (prefix)s/bin
prefix=/usr/local 
```

关于 ConfigParser，一个容易忽视的特性是它可以分别读取多个配置文件并将它们的结果合并成一个单独的配置。例如，假设某位用户创建了他们自己的配置文件，看起来是这样的：

; $\sim$ .config.ini   
[installation]   
prefix $=$ /Users/beazley/test   
[debug]   
log errors $\equiv$ False

这个文件可以单独读取，再同前面的配置合并在一起。示例如下：

```txt
>>> # Previously read configuration
>>>cfg.get('installation', 'prefix')
'/usr/local'
>>>#Merge in user-specific configuration
>>>import os
>>>cfg.read(os.path-expanduser('/~/.config.ini'))
[/Users/beazley/.config.ini']
>>>cfg.get('installation', 'prefix')
'/Users/beazley/test'
>>>cfg.get('installation', 'library')
'/Users/beazley/test/lib'
>>>cfg.getboolean('debug', 'logerrors')
False
>>> 
```

注意观察对变量 prefix 的修改是如何影响到其他相关的变量的，比如对 library 的设定。这么做行得通是因为对变量的插值操作是尽可能晚才执行的。可以通过下面的实验看出这一点：

```txt
>>>cfg.get('installation','library')  
'/Users/beazley/test/lib'  
>>>cfg.set('installation','prefix','/tmp/dir')  
>>>cfg.get('installation','library')  
'/tmp/dir/lib'  
>>> 
```

最后，需要重点提到的是 Python 并不能对在其他程序中使用的.ini 文件的全部特性都提供支持（例如 Windows 上的应用）。确保参考 configparser 的文档以获得更详细的语法和所支持特性的细节。

# 13.11 给脚本添加日志记录

# 13.11.1 问题

我们想让脚本和简单的程序可以将诊断信息写入到日志文件中。

# 13.11.2 解决方案

给简单的程序添加日志功能，最简单方法就是使用 logging 模块了。示例如下：

```txt
import logging def main(): 
```

```python
# Configure the logging system
loggingging/basicConfig(
    filename='app.log',
    level=loggingurious
)
# Variables (to make the calls that follow work)
hostname = 'www.python.org'
item = 'spam'
filename = 'data.csv'
mode = 'r'
# Example logging calls (insert into your program)
logging.critical('Host %s unknown', hostname)
logging.error("Couldn't find %r", item)
logging.warn('Feature is deprecated')
logging.info('Opening file %r, mode=%r', filename, mode)
logging.debug('Got here')
if __name__ == '__main__':
    main() 
```

这 5 个 logging 调用（critical()、error()、warning()、info()、debug()）分别代表着不同的严重级别，以降序排列。basicConfig()的 level 参数是一个过滤器，所有等级低于此设定的消息都会被忽略掉。

每个日志操作的参数都是一条字符串消息，后面跟着零个或多个参数。当产生日志消息时， $\%$ 操作符使用提供的参数来格式化字符串消息。

如果运行这个程序，文件 app.log 的内容将会是这样的：

```txt
CRITICAL:root:Host www.python.org unknown
ERROR:root:Could not find 'spam' 
```

如果想改变输出或输出的严重级别，可以通过修改调用 basicConfig()的参数来实现。示例如下：

loggingisticConfig( filename $\equiv$ 'app.log', level $=$ logging.WARNING, format $\equiv$ % (levelname)s:% (asetime)s:% (message)s')

修改后输出结果变成了下面这样：

```txt
CRITICAL:2012-11-20 12:27:13,595:Host www.python.org unknown  
ERROR:2012-11-20 12:27:13,595:Could not find 'spam'  
WARNING:2012-11-20 12:27:13,595:Feature is deprecated 
```

如上所示，日志的配置信息被直接硬编码到了程序中。如果想从配置文件中进行配置，把 basicConfig()调用修改成如下形式：

```python
import logging   
import logging.config   
def main(): # Configure the logging system logging.config.fileConfig('logconfig.ini') 
```

现在创建一个配置文件 logconfig.ini，看起来是这样的：

[loggers]   
keys=root   
[handlers]   
keys=defaultHandler   
[formatters]   
keys=defaultFormatter   
[logger_root]   
level=INFO   
handlers=defaultHandler   
qualname $\equiv$ root   
[handler_defaultHandler]   
class $\equiv$ FileHandler   
formatter $\equiv$ defaultFormatter   
args $=$ ('app.log'，'a')   
[formatter_defaultFormatter]   
format=%（},{name)s:%(message)s

如果想修改配置，直接编辑 logconfig.ini 文件即可。

# 13.11.3 讨论

对于 logging 模块来说有着上百万种高级的配置选项可用，但此时让我们暂时忽略这些细节。本节给出的解决方案对于简单的程序和脚本来说已经足够用了。只要保证在调用任何 logging 调用之前先调用 basicConfig()即可，这样你的程序将能够产生日志输出。

如果想让日志消息发送到标准错误输出而不是文件中，不要给 basicConfig()提供任何文

件名做参数即可。例如，可以这么做：

loggingisticConfig(level $=$ logging INFO)

关于 basicConfig()，一个微妙的地方在于它只能在程序中调用一次。如果稍后需要修改日志模块的配置，需要取得根日志对象（root logger）并直接对其做修改。示例如下：

```kotlin
logging.getLogger().level = logging DEBUG 
```

必须要强调的是，本节只展示了 logging模块的几个基本用法，还有相当多的高级定制化操作可做。对于这样的定制化操作，一个极佳的资源是“Logging Cookbook”（http://docs.python.org/3/howto/logging-cookbook.html）。

# 13.12 给库添加日志记录

# 13.12.1 问题

我们想给一个库添加日志功能，但是又不希望它影响那些没有使用日志功能的程序。

# 13.12.2 解决方案

对于想执行日志记录的库来说，应该创建一个专用的日志对象并将其初始化为如下形式：

# somelib.py   
import logging   
log $=$ logging.getLogger(_name_)   
log.addHandler(logging.NullHandler())   
# Example function (for testing)   
def func(): log.critical('A Critical Error!') log.debug('A debug message')

有了这样的配置，默认情况下将不会产生任何日志输出。例如：

```txt
>>> import somelib
>>> somelib.func()
>>> 
```

但是，如果日志系统得到适当的配置，则日志消息将开始出现。示例如下：

```txt
>>> import logging
>>> logging-basicConfig() 
```

```txt
>>> somelib.func()  
CRITICAL:somelib:A Critical Error!  
>>> 
```

# 13.12.3 讨论

库给日志带来了一个特殊的问题：即，使用日志的环境是未知的。一般来说，绝不应该在库代码中尝试去自行配置日志系统，或者对已有的日志配置做任何假设。因此，需要小心翼翼地提供隔离措施。

getLogger(__name__)创建了一个日志模块，其名称同调用它的模块名相同。由于所有的模块都是唯一的，这么做就创建了一个专用的日志对象，也就与其他的日志对象隔离开了。

log.addHandler(logging.NullHandler())操作绑定了一个空的处理例程到刚刚创建的日志对象上。默认情况下，空处理例程会忽略所有的日志消息。因此，如果用到了这个库且日志系统从未配置过，那么就不会出现任何日志消息或警告信息。

对单个库的日志记录可以独立地进行配置，不必管其他的日志设定。例如，考虑如下的代码：

```txt
>>> import logging
>>> logging-basicConfig(level=loggingurious)
>>> import somelib
>>> somelib.func()
CRITICAL:somelib:A Critical Error!
>>> # Change the logging level for 'somelib' only
>>> logging.getLogger('somelib').level=logging DEBUG
>>> somelib.func()
CRITICAL:somelib:A Critical Error!
DEBUG:somelib:A debug message
>>> 
```

这里，根日志对象已经被配置为只输出 ERROR 或更高等级的消息。但是，somelib库的日志等级已经被单独配置为输出调试消息了，这个设定的优先级要高于全局设定。

对于单个模块来说，能够像这样修改日志的设定会是一个非常有用的调试工具。因为我们不必去修改任何全局的日志设定了——当某个模块需要更多的日志输出时，只要针对这个模块修改日志等级即可。

“Logging HOWTO”（http://docs.python.org/3/howto/logging.html）一文中有关于配置logging 模块以及其他一些有用技巧的更多信息。

# 13.13 创建一个秒表计时器

# 13.13.1 问题

我们想记录执行各项任务所花费的时间。

# 13.13.2 解决方案

time 模块中包含了各种与计时相关的函数。但是，通常在这些函数之上构建更高层的接口来模拟秒表会更有用。示例如下：

import time   
class Timer: def init_self, func $\equiv$ time.perfcounter): self/elapsed $= 0.0$ self._func $=$ func self._start $=$ None def start(self): if self._start is not None: raiseRuntimeError('Already started') self._start $=$ self._func() def stop(self): if self._start is None: raiseRuntimeError('Not started') end $=$ self._func() self/elapsed $+ =$ end - self._start self._start $=$ None def reset(self): self/elapsed $= 0.0$ @property def running(self): return self._start is not None def _enter_self): self.start() return self   
def_exit_self,\*args): self.stop()

这个类定义了一个定时器，可以根据用户的需要启动、停止和重置它。Timer 类将总的花费时间记录在 elapsed 属性中。下面的示例展示了如何使用这个类：

```python
def countdown(n):
    while n > 0:
        n -= 1
# Use 1: Explicit start/stop
t = Timer()
t.start()
countdown(1000000)
t.stop()
print(t/elapsed)
# Use 2: As a context manager
with t:
    countdown(1000000)
print(t/elapsed)
with Timer() as t2:
    countdown(1000000)
print(t2/elapsed) 
```

# 13.13.3 讨论

本节提供了一个简单但非常有用的类，可用来进行计时和跟踪花费的时间。这个类也很好地演示了如何支持上下文管理协议以及对 with 语句的使用。

在进行计时测量时需要考虑底层所用到的时间函数。一般来说，像 time.time()或者time.clock()的计时精度根据操作系统的不同而有所区别。相反，time.perf_counter()函数总是会使用系统中精度最高的计时器。

如同前面所展示的，由 Timer 类记录的时间是系统时钟时间，其中包含了所有的休眠期时间。如果只想获取进程的 CPU 时间，可以用 time.process_time()取代。示例如下：

```python
t = Timer(time.process_time)  
with t:  
    countdown(1000000)  
print(t elapsed) 
```

time.perf_counter()和 time.process_time()都返回秒级的时间值（以浮点数表示）。但是单独这样一个时间值没有任何意义，要使得结果变得有意义，必须调用函数两次并计算两次时间的差。

有关计时和性能统计分析的更多主题请参阅 14.13 节。

# 13.14 给内存和 CPU 使用量设定限制

# 13.14.1 问题

我们想对运行在 UNIX 系统上的程序在内存和 CPU 的使用量上设定一些限制。

# 13.14.2 解决方案

resource 模块可用来执行这样的任务。例如，要限制 CPU 时间可以这样做：

import signal   
import resource   
import os   
def time_exceeded(signo, frame): print("Time's up!") raise SystemExit(1)   
def set_max_runtine(seconds): # Install the signal handler and set a resource limit soft, hard $=$ resource.getrlimit(resource.RLIMIT_CPU) resource.setrlimit(resource.RLIMIT_CPU, (seconds, hard)) signal(signal(signal.SIGXCPU, time_exceeded)   
if_name $\equiv$ __main': set_max_runtine(15) while True: pass

运行上述代码，当超时时会产生 SIGXCPU 信号。程序就会做清理工作然后退出。

要限制内存的使用，可以在使用的总地址空间上设定一个限制。示例如下：

import resource   
def limit_memory(maxsize): soft, hard $=$ resource.getrlimit(resource.RLIMIT_AS) resource.setrlimit(resource.RLIMIT_AS,(maxsize,hard))

当设定了内存限制后，如果没有更多的内存可用，程序就会开始产生 MemoryError 异常。

# 13.14.3 讨论

在本节中，我们通过 setrlimit()函数来为特定的资源设定软性和硬性限制。软性限制就是一个值，一般来说操作系统会通过信号机制来限制或通知进程。硬性限制代表着软

性限制值的上限。通常，这些值是由系统全局参数所控制的，它们会由系统管理员来设定。虽然可以降低硬性限制，但这个过程不能由用户进程来控制（就算这个进程要降低自己的硬性限制也不行）。

setrlimit()函数还可以用来设定比如子进程数量、可打开的文件数量等系统资源。可以参阅 resource 模块的文档以获得进一步的细节。

请注意，本节中的技术只能用于UNIX 系统，而且可能并不适用于所有的UNIX 变种。例如，当我们进行测试时发现在 Linux 上可以正常工作但在 OS X 上就不行。

# 13.15 加载 Web 浏览器

# 13.15.1 问题

我们想从脚本中加载一个浏览器并让它打开指定的 URL。

# 13.15.2 解决方案

webbrowser 模块可用来以独立于平台的方式加载浏览器。示例如下：

```txt
>>> import webbrowser
>>> webbrowser.open('http://www.python.org')
True
>>>
```

这会用默认的浏览器打开请求的页面。如果想对页面打开的方式有更多的控制，可以使用下列函数之一：

```txt
>>> # Open the page in a new browser window  
>>> webbrowser.open_new('http://www.python.org')  
True  
>>>  
>>> # Open the page in a new browser tab  
>>> webbrowser.open_new_tab('http://www.python.org')  
True
```

如果浏览器支持的话，这么做会尝试在一个新的浏览器窗口或标签页中打开页面。

如果想在特定的浏览器中打开页面，可以使用 webbrowser.get()函数来指定一个具体的浏览器。示例如下：

```txt
>>> c = webbrowser.get('firefox')
>>> c.open('http://www.python.org') 
```

```txt
True   
>>>c.open_new_tab('http://docs.python.org')   
True   
>>>
```

支持的浏览器名称的完整列表可以在 Python 文档（http://docs.python.org/3/library/webbrowser.html）中找到。

# 13.15.3 讨论

在许多脚本中，能够轻松加载一个浏览器可算是一项有用的操作。例如，也许脚本执行了某种服务器部署的任务，而我们想马上加载浏览器来验证这是否能够工作。或者某个程序把数据以 HTML 页面的形式输出，而我们只想打开浏览器查看结果。无论怎样，webbrowser 模块都是一种简单的解决方案。

# 测试、调试以及异常

测试是很棒的一件事，但调试就没那么有趣了吧。在 Python 解释器执行代码之前并没有编译器来分析你的代码，这一事实使得测试成为了开发中至关重要的部分。本章的目的是讨论一些与测试、调试以及异常处理相关的常见问题。本章不是为测试驱动开发（TDD）或者 unittest模块做简要的介绍。因此，我们假设读者已经对软件测试方面的一些概念有所了解。

# 14.1 测试发送到 stdout 上的输出

# 14.1.1 问题

我们的程序中有一个方法会将输出发送到标准输出上（sys.stdout）。这几乎总是表示它会把文本发送到屏幕上。我们想为自己的代码编写一个测试用例，以此证明只要给定合适的输入，则会在屏幕上显示合适的输出。

# 14.1.2 解决方案

利用 unittest.mock 模块的 patch()函数，很容易为单独的测试用例模拟出 sys.stdout。用完后可将其放回，不必使用临时变量或者在测试用例之间暴露出模拟的状态。

考虑下面这个位于 mymodule 模块中的函数：

mymodule.py   
defurlprint(protocol，host，domain): url $= 1$ {}://{}.{}'.format(protocol，host，domain) print(url)

内建的 print 函数默认情况下会把输出发往 sys.stdout。为了测试输出确实会发往

sys.stdout，可以利用一个对象作为 sys.stdout 的替身来模拟这种情况，然后对产生的结果做断言处理。利用 unittest.mock 模块的 patch()方法能够非常方便地替换对象，而且只在运行的测试用例的上下文中生效。在测试完成后，会立刻将所有的东西返回到它们的原始状态上。下面就是针对 mymodule 的测试代码：

```python
from io import StringIO   
from unittest import TestCase   
from unittestmock import patch   
import mymodule   
class TestUrLPrint(TestCase): def test_url_gets_to stdout(self): protocol \(=\) 'http' host \(=\) 'www' domain \(=\) 'example.com' expected_url \(=\) '\{\}://{}\{\}\backslash \mathrm{n}'\) .format(protocol,host,domain) with patch('sys/stdout',new \(\equiv\) StringIO()）as fake_out: mymodule UrLprint(protocol,host,domain) self.assertEqual(fake_out.getvalue(),expected_url) 
```

# 14.1.3 讨论

urlprint()函数接受三个参数，测试代码首先为每个参数设定一个哑值（dummy value）。变量 expected_url 被设定为包含期望输出的字符串。

要运行这个测试用例，unittest.mock.patch()函数用来当做上下文管理器，把 sys.stdout的值替换为一个 StringIO 对象。在这个过程中会创建一个模拟对象，即 fake_out 变量。可以在 with 语句块中使用 fake_out 来执行各种检查。当 with 语句块执行完毕后，patch()函数会非常方便地将所有状态还原为测试运行之前时的状态。

值得一提的是，某些特定的C扩展模块可能会通过设定 sys.stdout直接往标准输出写数据。本节不针对这种情况做特别说明，但对于纯 Python 代码来说应该是能正常工作的（如果需要从这种 C 扩展模块中捕获 I/O，可以打开一个临时文件，然后让标准输出临时重定向到那个文件来完成。这中间涉及各种操作文件描述符的技巧）。

有关在字符串和 StringIO 对象中捕获 I/O 的更多信息可参见 5.6 节。

# 14.2 在单元测试中为对象打补丁

# 14.2.1 问题

我们正在编写单元测试，需要对选定的对象添加补丁，以此才能在测试中针对它们的

使用情况做断言处理（例如，对特定的调用参数做断言、访问选定的属性时做断言等）。

# 14.2.2 解决方案

unittest.mock.patch()函数可用来帮助解决这个问题。虽然不太常见，但 patch()函数用法较多，可当做装饰器、上下文管理器或者单独使用。例如，在下面的示例中我们把它当做装饰器来用：

```python
from unittestmock import patch   
import example   
@patch('example.func')   
def test1(x, mock_func): example.func(x) #Uses patched example.func mock_func.assert Called_with(x) 
```

也可以把它当做上下文管理器来用：

```python
with patch('example.func') as mock_func: example.func(x) #Uses patched example.func mock_func.assert Called_with(x) 
```

最后但同样重要的是，也可以用它来手动打补丁：

```txt
p = patch('example.func')
mock_func = p.start()
example.func(x)
mock_func.assert Called_with(x)
p.stop() 
```

如果有必要的话，可以将装饰器和上下文管理器堆叠起来对多个对象打补丁。示例如下：

```python
@patch('example.func1')   
@patch('example.func2')   
@patch('example.func3')   
def test1(mock1, mock2, mock3):   
...   
def test2(): with patch('example.batch1') as mock1, \ patch('example.batch2') as mock2, \ patch('example.batch3') as mock3: 
```

# 14.2.3 讨论

patch()接受一个已有对象的完全限定名称并将其替换为一个新值。在装饰器函数或者上

下文管理器结束执行后会将对象恢复为原始值。默认情况下，对象会被替换为MagicMock 实例。示例如下：

>>>x=42   
>>>with patch('__main_.x'): . print(x)   
<MagicMock name $\equiv$ 'x' id $\equiv$ 4314230032'> >>>x   
42   
>>>

但是，实际上可以将对象替换为任何你希望的值，只要将值作为 patch()的第二个参数传入即可：

```python
>>>x   
42   
>>>with patch('main.x'，'patched_value'): . print(x)   
patched_value   
>> x   
42   
>>>
```

MagicMock 实例一般被当作替换值来用，旨在模仿可调用对象和实例。它们会记录使用信息而且允许创建断言。示例如下：

```python
>>> from unittest Mock import MagicMock
>>> m = MagicMock.return_value = 10)
>>> m(1, 2, debug=True)
10
>>> m.assert Called_with(1, 2, debug=True)
>>> m.assert Called_with(1, 2)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File ".../unittest/mock.py", line 726, in assertCalled_with raise AssertionError(msg)
AssertionError: Expected call: mock(1, 2)
Actual call: mock(1, 2, debug=True)
>>> 
>>> m_upper.return_value = 'HELLO'
>>> m_upper('hello')
'HELLO'
>>> assert m_upperCalled 
```

```txt
>>> m.split.return_value = ['hello', 'world']
>>> m.split('hello world')
['hello', 'world']
>>> m.split.assert Called_with('hello world')
>>> 
>>> m['blah']
<MagicMock name='mock._getitem_' id='4314412048'>
>>> m._getitem_.called
True
>>> m._getitem_.assertCalled_with('blah')
>>> 
```

一般来说，这类操作都是在单元测试中进行的。例如，假设有如下的函数：

# example.py   
from urllib.request import urlopen   
import csv   
def dowprices(): u $=$ urlopen('http://finance.yahoo.com/d/quotes.csv?s $\coloneqq$ @^DJI&f $\coloneqq$ sl1') lines $=$ (line.decode('utf-8') for line in u) rows $=$ (row for row in csv-readerlines) if len(row) $= = 2$ prices $=$ {name:float(price) for name, price in rows} return prices

通常，这个函数会使用 urlopen()从 Web 上抓取一些数据然后进行解析。要对这个函数做单元测试，我们可能会想自己创建一份更加可预测的数据集，然后将其作为函数的测试数据。下面的示例采用了前文讨论过的补丁技术：

import unittest   
from unittestmock import patch   
import io   
import example   
sample_data $=$ io BytesIO(b''\n"IBM",91.1\r\n"AA",13.25\r\n"MSFT",27.72\r\n\r'   
class Tests(unittest.TestCase): @patch('example.urlopen'，return_value $\equiv$ sample_data) def test_dowprices(self，mock.urlopen)： p $=$ example.dowprices() self.assertTrue(mock.urlopencalled)

```txt
self.assertEqual(p,{ 'IBM':91.1, 'AA':13.25, 'MSFT'：27.72})
```

```python
if _name_ == 'main':  
    unittest.main() 
```

在这个示例中，example 模块中的 urlopen()函数被替换成了一个 mock 对象，返回的BytesIO()中就包含着作为替代的样例数据。

关于这个测试，一个重要但微妙的地方在于我们是对 example.urlopen 打补丁而不是针对 urllib.request.urlopen。在打补丁时，使用的名称必须和被测代码中使用的名称保持一致。由于示例代码使用的是 from urllib.request import urlopen，因此实际上由函数dowprices()调用的 urlopen()其实是位于 example 模块中的。

本节仅仅只是小小体验了一下 unittest.mock 模块的功能。要使用更加高级的功能和特性，官方文档（http://docs.python.org/3/library/unittest.mock）是必读的资料。

# 14.3 在单元测试中检测异常情况

# 14.3.1 问题

我们想编写一个能够快速检测异常的单元测试。

# 14.3.2 解决方案

检测异常可使用 assertRaise()方法。例如，如果想检查某个函数是否引发了 ValueError 异常，可以使用下面的代码完成：

```python
import unittest   
#A simple function to illustrate   
def parse_int(s): return int(s)   
class TestConversion(unittest.TestCase): def test.bad_int(self): self.assertRaises(ValueError, parse_int,'N/A') 
```

如果需要以某种方式检查异常的值，那就需要用到另一种不同的方法。示例如下：

```python
import errno   
class TestIO(unittest.TestCase): 
```

```python
def test_file_not_found(self):
    try:
        f = open('/file/not/found')
    except IOException as e:
        self assertEqual(e.errno, errno.ENOENT)
    else:
        self失败('IOError not raised') 
```

# 14.3.3 讨论

assertRaise()方法提供了一种简便的方式来测试是否有异常出现。编写测试代码时，一个常见的误区就是自己手工尝试用 try 和 except 来处理异常。比如：

```python
class TestConversion(unittest.TestCase):
    def test.bad_int(self):
        try:
            r = parse_int('N/A')
        except ValueError as e:
            self.assertEqual(type(e), ValueError) 
```

这种方法的问题在于容易忘记边界情况，比如当根本没有异常产生时。为了应对这点，需要为这种情况添加一个检查，示例如下：

```python
class TestConversion(unittest.TestCase):
    def test.bad_int(self):
        try:
            r = parse_int('N/A')
        except ValueError as e:
            self.assertEqual(type(e), ValueError)
        else:
            self失败('ValueError not raised') 
```

assertRaises()方法则替我们处理了所有这些细节，所以应该尽量多使用它。

assertRaises()的局限性在于对于所产生的异常对象的值，并不提供测试方法。要实现这一目的，必须像上面的示例那样手动进行测试。在这两种极端情况之间，可能会考虑使用 assertRaisesRegex()方法。该方法允许我们在测试异常的同时还可以针对异常的字符串表示进行正则表达式匹配。示例如下：

```python
class TestConversion(unittest.TestCase):
    def test.bad_int(self):
        self.assertRaisesRegex(ValueError, 'invalid literal ', parse_int, 'N/A') 
```

关于 assertRaises()和 assertRaisesRegex()还有一个鲜为人知的事实，即，它们也可以当做上下文管理器来使用：

```python
class TestConversion(unittest.TestCase):
    def test.bad_int(self):
        with self.assertRaisesRegex(ValueError, 'invalid literal *'): r = parse_int('N/A') 
```

如果我们的测试除了要执行一个可调用对象之外还涉及多个步骤，那么上下文管理器的形式就很有用了。

# 14.4 将测试结果作为日志记录到文件中

# 14.4.1 问题

我们想把单元测试的结果写入到文件中，而不是打印到标准输出上。

# 14.4.2 解决方案

运行单元测试的一种非常常用的技术就是在测试文件底部包含下列代码：

import unittest   
class MyTest(unittest.TestCase): ... if_name $= =$ 'main': unittest.main()

这么做会让测试文件变为可执行文件，而且会把测试结果打印到标准输出上。如果想对输出做重定向，需要将原来的main()展开，然后编写自己的main()函数。示例如下：

import sys   
def main(out $\equiv$ sys.stderr,verbosity $= 2$ . loader $=$ unittest.TestLoader() suite $=$ loader.loadTestsFromModule(sys/modules[_name_]) unittest.TextTestRunner(out,verbosity $\equiv$ verbosity).runsuite)   
if_name $\equiv$ __main__: with open('testing.out'，'w') as f: main(f)

# 14.4.3 讨论

本节中有趣的地方不在于将测试结果重定向到文件中，而是当这么做的时候暴露了unittest模块内部值得注意的一些工作原理。

从基本的层次来说，unittest 模块首先会组装一个测试套件。这个测试套件中包含了各

种定义的测试方法。一旦套件装配完成，它所包含的测试就开始执行。

单元测试的这两个部分是彼此相分离的。解决方案中创建的 unittest.TestLoader 实例是用来组装测试套件的。而 loadTestsFromModule()是 TestLoader 的几个实例方法之一，用来收集测试。在这种情况下，它会为 TestCase 类扫描模块，并从模块中提取出测试方法。如果想获得更细粒度的控制，可以用 loadTestsFromTestCase()方法（本节未给出）从继承自 TestCase的子类中提取测试方法。

TextTestRunner 类是一个测试运行类的例子。该类的主要目的就是运行包含在测试套件中的测试。这个类也是 unittest.main()函数所使用的测试运行类。但是，这里我们还对它做了一点底层配置，包括设置输出文件和输出信息的详细程度。

尽管本节中只包含了几行代码，但对于读者今后应该如何定制化 unittest框架带来了一些启示。要定制化测试套件的装配过程，可以利用 TestLoader 类的各种操作来完成。要定制化测试的执行，可以创建自定义的测试运行类，以此模拟出 TextTestRunner类的功能。这些主题都超出了本节可以涵盖的范围。但是，unittest 模块的文档中对这些底层的协议有着详尽的说明。

# 14.5 跳过测试，或者预计测试结果为失败

# 14.5.1 问题

我们想在自己的单元测试中跳过某些测试，或者选择几个测试将它们标记为预测会失败。

# 14.5.2 解决方案

unittest 模块中有一些装饰器可作用于所选的测试方法上，以此控制它们的处理行为。示例如下：

import unittest   
import os   
import platform   
class Tests(unittestTestCase): def test_0(self): self.assertTrue(True) @unittestulates('skipped test') def test_1(self): self失败(!) @unittest.If(os.name $= =$ 'posit','Not supported on Unix') def test_2(self):

import winreg   
@unittest.skipUnless platform.system() $= =$ 'Darwin'，'Mac specific test') def test_3(self): self.assertTrue(True) @unittest(expectedFailure def test_4(self): self.assertEqual(2+2,5)   
if_name $= =$ 'main': unittest.main()

如果在一台 Mac 电脑上运行上述代码，将得到如下输出：

```batch
bash % python3 testsample.py -v  
test_0 (__main__Tests) ... ok  
test_1 (__main__Tests) ... skipped 'skipped test'  
test_2 (__main__Tests) ... skipped 'Not supported on Unix'  
test_3 (__main__Tests) ... ok  
test_4 (__main__Tests) ... expected failure  
Ran 5 tests in 0.002s  
OK (skipped=2, expected failures=1) 
```

# 14.5.3 讨论

装饰器 skip()可用来跳过某个根本就不想运行的测试。skipIf()和 skipUnless()在编写那些只针对特定平台或 Python 版本甚至其他依赖的测试时非常有用。对于已知会失败的测试项，而又不想让测试框架生成更多报告信息，那么可以使用装饰器@expectedFailure对其进行标注。

用来跳过检查的装饰器同样可以作用到整个测试类上。示例如下：

```txt
@unittest.skipUnless platform.system() == 'Darwin', 'Mac specific tests')  
class DarwinTests(unittest.TestCase): 
```

# 14.6 处理多个异常

# 14.6.1 问题

我们有一段代码可以抛出几个不同的异常，而我们需要负责处理所有可能会发生的异常。要求处理的时候无需创建重复代码或者冗长的代码段。

# 14.6.2 解决方案

如果能够用一个单独的代码块处理所有不同的异常，可以将它们归组到一个元组中。示例如下：

```python
try: client_obj.get_url(url) except (URLError, ValueError, SocketTimeout): client_obj.remove_url(url) 
```

在上面的代码中，如果列出的这些异常中有任何一个出现，则会调用 remove_url()方法。另一方面，如果需要对其中某个异常采取不同的处理办法，可以将其放入单独的 except子句中去：

```txt
try: client_obj.get_url(url)   
except (URLError, ValueError): client_obj.remove_url(url)   
except SocketTimeout: client_obj.handle_url_timeout(url) 
```

有许多异常都会被归组为继承体系。对于这样的异常，可以通过指定一个基类来捕获所有的异常。例如，与其像这样编写代码：

```txt
try: f = open(filename) except (FileNotFoundError, PermissionError): 
```

不如像这样重写 except 语句：

```python
try: f = open(filename) except OSError: 
```

这么做是可行的，因为 OSError 是基类，FileNotFoundError 和 PermissionError 异常都是它的子类。

# 14.6.3 讨论

值得一提的是，可以在抛出的异常上使用关键字 as，尽管这并非是特定于处理多个异常的技术。

```txt
try: f = open(filename) except OSError as e: 
```

```python
if e.errno == errno.ENOENT:  
    logger.error('File not found')  
elif e.errno == errno.EACCES:  
    logger.error('Permission denied')  
else:  
    logger.error('Unexpected error: %d', e.errno) 
```

在上面的例子中，变量 e 保存着异常 OSError 的实例。如果之后需要检查这个异常，比如要根据额外的状态码来进行处理，那这就很有用了。

需要注意的是，except 子句是按照列出的顺序进行检查的，而第一个匹配成功的子句将得到执行。虽然这么做会有些病态，但是可以轻易地创建出多个 except 子句都可能匹配的情况。示例如下：

```python
>>> f = open('missing')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
FileNotFoundError: [Errno 2] No such file or directory: 'missing'
>>> try:
    ...     f = open('missing')
    ... except OSError:
        print('It failed')
    ... except FileNotFoundError:
        print('File not found')
    ...
It failed
>>> 
```

这里的 except FileNotFoundError 子句不会执行，因为 OSError 更加一般化，它也能匹配 FileNotFoundError 异常，而且它是首先列出的，因此会先匹配执行。

给大家一个调试的小技巧，如果你不能完全确定某个特定异常的类层次结构，可以通过检查异常的__mro__属性来快速查阅。示例如下：

```txt
>>> FileNotFoundError.__mro__(<class 'FileNotFoundError'>, <class 'OSError'>, <class 'Exception'>, <class 'BaseException'>, <class 'object'>)  
>>> 
```

以上列出的这些类中，只要排在 BaseException 之前的都可以用在 except 语句中。

# 14.7 捕获所有的异常

# 14.7.1 问题

我们想编写代码来捕获所有的异常。

# 14.7.2 解决方案

要捕获所有的异常，可以为 Exception 类编写一个异常处理程序，示例如下：

```txt
try:  
...  
except Exception as e:  
...  
log('Reason:', e) # Important! 
```

除了 SystemExit、KeyboardInterrupt 和 GeneratorExit 之外，上面的代码能够捕获所有的异常。如果也想捕获这些异常的话，只要把 Exception 修改为 BaseException 即可。

# 14.7.3 讨论

有时候当程序员没法记住某个复杂操作中可能产生的所有异常时，捕获所有的异常就成了他们唯一的支柱。同样，如果你不够小心的话，这也是写出无法调试的代码的绝佳方式。

正因为如此，如果选择捕获所有的异常，那么针对异常产生的实际原因做日志记录或报告就绝对是至关重要的了（例如，产生日志文件，或者将出错信息打印到屏幕上等）。如果不这么做，那么某个时刻你的大脑很可能会乱成一锅粥。考虑下面这个示例：

```python
def parse_int(s):
    try:
        n = int(v)
    except Exception:
        print("Couldn't parse") 
```

如果试着调用这个函数，它的行为是这样的：

```txt
>>> parse_int('n/a')
Couldn't parse
>>> parse_int('42')
Couldn't parse
>>> 
```

此时，你可能会抓着脑袋想为什么它不能工作呢？现在假设函数被改写为如下形式：

```python
def parse_int(s):
    try:
        n = int(v)
    except Exception as e:
        print("Couldn't parse")
        print('Reason: ', e) 
```

在这种情况下，会得到如下形式的输出，能够清楚表明上述代码中出现了一个编程错误：

```txt
>>> parse_int('42')
Couldn't parse
Reason: global name 'v' is not defined
>>> 
```

所有事情都是平等的，在处理异常的时候最好还是尽可能使用精确的异常类。但是，如果必须捕获所有的异常，那就要确保提供高质量的诊断信息，或者将异常传播出去，这样就不会丢失异常产生的原因。

# 14.8 创建自定义的异常

# 14.8.1 问题

我们正在构建一个应用，希望对底层的异常进行包装从而打造出自定义的异常类。这种自定义的异常在应用程序的上下文环境中可以包含更多的含义。

# 14.8.2 解决方案

创建新的异常是非常简单的——只要将它们定义成继承自 Exception 的类即可（也可以继承自其他已有的异常类型，如果这么做更有道理的话）。例如，如果正在编写网络编程相关的代码，则可能会像这样定义一些自定义的异常：

```python
class NetworkError(Exception):
    pass
class HostnameError(NetworkError):
    pass
class TimeoutErroretworkError):
    pass
class ProtocolError NETWORKError):
    pass 
```

用户能够以普通的方式来使用这些异常，示例如下：

try: msg $\equiv$ s.recv() except TimeoutError as e: .. except ProtocolError as e:

# 14.8.3 讨论

自定义的异常类应该总是继承自内建的 Exception 类，或者继承自一些本地定义的基类，而这个基类本身又是继承自 Exception 的。虽然所有的异常也都继承自 BaseException，但不应该将它作为基类来产生新的异常。BaseException 是预留给系统退出异常的，比如 KeyboardInterrupt 或者 SystemExit，以及其他应该通知应用程序退出的异常。因此，捕获这些异常并不适用于它们本来的用途。假设遵循这个约定，从 BaseException继承而来的自定义异常将无法捕获，也不能通知应用程序关闭！

在自己的应用中使用自定义的异常，这使得任何需要阅读源代码的人能够更好地理解程序的行为。有一种设计上的考虑是通过继承机制将自定义的异常归类到一起。在复杂的应用中，引入更高层的基类将不同的异常类归组到一起是很有意义的。这给了用户捕获细粒度错误的能力，比如：

```txt
try: s.send(msg) except ProtocolError: 
```

但同样也能够捕获粗粒度范围内的错误，比如：

```javascript
try: s.send(msg) except NetworkError: 
```

如果打算定义一个新的异常并且改写 Exception 的__init__()方法，请确保总是用所有传递过来的参数调用 Exception.__init__()。示例如下：

```python
class CustomError(Exception):
    def __init__(self, message, status):
        super().__init__(message, status)
        self.message = message
        self.status = status 
```

这可能看起来有点古怪，但是 Exception 的默认行为就是接受所有传递过来的参数并将它们以元组的形式保存到.args 属性中。Python 中的其他组件以及各种各样的库都期望所有的异常都有一个.args 属性，因此如果跳过了这一步，那么就会发现新创建的异常在特定上下文环境中表现出不正确的行为。为了说明对.args 的使用，考虑下面的交互式会话，这里用到了内建的 RuntimeError 异常，注意在 raise 语句中可以使用多少个参数：

```txt
>>> try:  
... raise RuntimeError('It failed')  
... except RuntimeError as e: 
```

print(e.args)   
(It failed')   
>> try: raiseRuntime('Itfailed'，42,'spam') 1 exceptRuntimeas e: print(e.args)   
( $\mathrm{I}$ itfailed'，42，'spam')   
>>

要得到关于创建自定义异常的更多内容，请查阅 Python 文档（http://docs. python.org/3/tutorial/errors.html）。

# 14.9 通过引发异常来响应另一个异常

# 14.9.1 问题

我们想引发一个异常作为捕获另一个异常时的响应，但是希望在 traceback 回溯中同时包含这两个异常的有关信息。

# 14.9.2 解决方案

要将异常串联起来，可以用raise from语句来替代普通的 raise。这么做能够提供这两个异常的有关信息。示例如下：

```txt
>>> def example():
    ...     try:
        ...         int('N/A')
    ...     except ValueError as e:
        ...         raise ValueError('A parsing error occurred') from e...
>>> 
example()
Traceback (most recent call last):
    File "<stdin>", line 3, in example
ValueError: invalid literal for int() with base 10: 'N/A'
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "<stdin>", line 5, in example
RuntimeError: A parsing error occurred
>>> 
```

在 traceback 回溯中可以发现这两个异常都被捕获了。要捕获这样的异常，可以使用普通的 except 语句。但是，可以通过查看异常对象的__cause__属性来跟踪所希望的异常链。示例如下：

```python
try: example() except RuntimeError as e: print("It didn't work:"，e) if e._cause_： print('Cause:'，e._cause_) 
```

当在 except 语句块中引发另一个异常时，此时会产生异常链的隐式形式。示例如下：

```txt
>>> def example2():  
... try:  
... int('N/A')  
... except ValueError as e:  
... print("Couldn't parse:", err)  
...  
>>> example2()  
Traceback (most recent call last):  
File "<stdin>", line 3, in example2  
ValueError: invalid literal for int() with base 10: 'N/A'  
During handling of the above exception, another exception occurred:  
Traceback (most recent call last):  
File "<stdin>", line 1, in <module>  
File "<stdin>", line 5, in example2  
NameError: global name 'err' is not defined  
>>> 
```

在这个例子中可以得到这两个异常的相关信息，但是这与第一个例子有所不同。在这种情况下，NameError 异常是由于编程错误而产生的，并非是对解析错误的直接响应。因此在这种情况下，异常对象的__cause__属性并没有被设置。相反，会把__context属性设置为前一个异常（即，ValueError）。

如果出于某种原因想阻止异常链的产生，可以使用 raise from None 来完成：

```txt
>>> def example3():  
... try:  
... int('N/A')  
... except ValueError:  
... raise ValueError('A parsing error occurred') from None... 
```

```txt
>>>example3()   
Traceback (most recent call last): File "<stdin>",line 1,in <module> File "<stdin>",line 5,in example3   
RuntimeError:A parsing error occurred   
>>> 
```

# 14.9.3 讨论

在设计代码的时候，对于在 except块中使用 raise语句的情况，大家应该特别小心。大部分情况下，这种raise语句都应该改为raise from。也就是说，我们应该采用下面这种风格：

```txt
try: except SomeException as e: raise DifferentException() from e 
```

这么做的原因在于我们需要显式将异常产生的原因串联起来。也就是说，DifferentException 是直接响应 SomeException。这两个异常间的关系会在 traceback 回溯中显式给出。

如果采用下面这种风格，还是可以得到异常链。但是通常这么做不能明确表达出异常链是程序员有意为之，还是由于无法预见的编程错误而产生的：

```txt
try: except SomeException: raise DifferentException() 
```

当使用 raise from 语句时，就需明确表达出你希望引发第二个异常的意图。

最好不要像最后那个例子中那样抑制异常信息。尽管这么做产生的 traceback 回溯会比较短小，但同时也丢弃了对调试而言很有用的信息。任何事情没有绝对之分，通常最好还是尽可能多地保留调试信息为好。

# 14.10 重新抛出上一个异常

# 14.10.1 问题

我们在 except 块中捕获了一个异常，现在想将它重新抛出。

# 14.10.2 解决方案

只需要单独使用 raise 语句即可。示例如下：

```txt
>>> def example():
...
...
try:
...
int('N/A')
except ValueError:
...
print("Didn't work")
raise
...
example()
Didn't work
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "<stdin>", line 3, in example
ValueError: invalid literal for int() with base 10: 'N/A'
>>> 
```

# 14.10.3 讨论

这个问题通常出现在需要对某个异常做出响应（比如记录日志、完成清理工作等），但之后希望将异常再传播出去时。一个非常常见的用途就是用在捕获所有异常的处理中：

```txt
try:  
...  
except Exception as e:  
# Process exception information in some way  
...  
# Propagate the exception  
raise 
```

# 14.11 发出告警信息

# 14.11.1 问题

我们想让自己的程序能够发出告警信息（例如，对废弃的功能或者使用上的问题进行告警提示）。

# 14.11.2 解决方案

要让程序产生告警信息，可以使用 warnings.warn()函数。示例如下：

```python
import warnings
def func(x, y, logfile=None, debug=False): 
```

```javascript
if logfile is not None: warnings.warn('logfile argument deprecated',DeprecationWarning) 
```

warn()函数的参数是一条告警信息附带一个告警类别，通常类别为 UserWarning、DeprecationWarning 、 SyntaxWarning 、 RuntimeWarning 、 ResourceWarning 或 者FutureWarning 中的一种。

对告警信息的处理取决于如何执行解释器以及其他的相关配置。例如，如果用-W all选项来运行 Python 解释器，则会得到如下的输出：

```txt
bash % python3 -W all example.py
example.py:5: DeprecationWarning: logfile argument is deprecated
    warnings.warn('logfile argument is deprecated', DeprecationWarning) 
```

一般来说，告警信息只会发送到标准错误输出上。如果想把告警转换为异常，可以使用-W error 选项：

```txt
bash % python3 -W error example.py  
Traceback (most recent call last):  
    File "example.py", line 10, in <module>  
    func(2, 3, logfile='log.txt')  
    File "example.py", line 5, in func  
    warnings.warn(' logfile argument is deprecated', DeprecationWarning)  
    DeprecationWarning: logfile argument is deprecated  
    bash % 
```

# 14.11.3 讨论

为了维护软件以及帮助用户更好地使用软件，产生告警信息常常是一项有用的技术。这么做可以让那些没必要上升到异常层面的问题以告警信息的形式表达出来。比如说，如果打算修改某个库或者框架的行为，可以针对打算修改的部分启用告警信息，同时在一段时间内仍然提供向后兼容性。也可以提醒用户有关用法方面的问题。

在内建的 warning 库中还有另一个关于告警应用的示例。下面的例子用来说明当未关闭文件对象就打算将其销毁时所产生的告警信息：

```txt
>>> import warnings
>>> warnings.simplefilter('always')
>>> f = open('/etc/passwd')
>>> del f
__main__:1: ResourceWarning: unclosed file <io.TextIOWrapper name='/etc/passwd'
mode='r' encoding='UTF-8']
>>> 
```

默认情况下，并非所有的告警信息都会显示出来。-W选项能够控制告警信息的输出。

-W all 将会输出所有的告警信息，而-W ignore 选项会忽略所有的告警，-W error 会将告警转换为异常。另外一种替代方案就是使用 warnings.simplefilter()函数来控制输出，上面的示例采用的正是这个方法。参数“always”使得所有的告警信息都会显示，而“ignore”则表示忽略所有的告警，“error”则会把告警转换为异常。

对于简单的情况，这就是所有需要了解的有关产生告警信息的知识。warnings 模块提供了许多与信息过滤以及处理告警信息相关的高级配置选项。更多信息可参阅 Python 文档（http://docs.python.org/3/library/warnings.html）。

# 14.12 对基本的程序崩溃问题进行调试

# 14.12.1 问题

我们的程序崩溃了，我们希望通过一些简单的策略来调试它。

# 14.12.2 解决方案

如果程序由于产生异常而崩溃了，可以通过 python3 -i someprogram.py 的方式来运行程序。这么做可以简单地查看产生问题的原因。一旦程序终止，-i 选项就会开启一个交互式 shell，这里就可以对程序运行的环境做一番探究了。例如，假设有如下的代码：

```txt
sample.py   
def func(n): return n + 10   
func('Hello') 
```

通过 python3 -i 来运行程序会产生下列输出：

```txt
bash % python3 -i sample.py   
Traceback (most recent call last): File "sample.py", line 6, in <module> func('Hello') File "sample.py", line 4, in func return n + 10   
TypeError: Can't convert 'int' object to str implicitly. >>> func(10)   
20   
>>> 
```

如果这么做还看不出什么明显的问题，那么在程序崩溃之后还可以加载 Python 调试器

来助阵。示例如下：

```txt
>>> import pdb
>>> pdb.pm()
> sample.py(4)func()
-> return n + 10
(Pdb) w
sample.py(6)<module>
-> func('Hello')
> sample.py(4)func()
-> return n + 10
(Pdb) print n
'Hello'
(Pdb) q
>>> 
```

如果我们的代码深埋在一个难以获取交互式 shell的环境中（比如在服务器中），通常可以捕获错误并自己生成 traceback 回溯。示例如下：

```python
importtraceback   
importsys   
try: func(arg)   
except: print('\*\*\*\*ANERROROCCCurred\*\*\*\*) traceback.print exc(file=sys.stderr) 
```

如果程序并没有崩溃只是产生错误的结果，又或者我们只是想了解程序究竟是如何工作的，那么在代码中感兴趣的位置上插入一些print()调用是完全合理的。但是，如果真的打算这么做，这里有一些我们可能会感兴趣的相关技术。首先，traceback. print_stack()函数会在程序调用它的地方立刻打印出调用栈的信息。示例如下：

```python
>>> def sample(n):
    ... if n > 0:
        ... sample(n-1)
    ... else:
        ... traceback.print_stack(file=sys.stderr)
    ...
>>> sample(5)
File "<stdin>", line 1, in <module>
File "<stdin>", line 3, in sample
File "<stdin>", line 3, in sample
File "<stdin>", line 3, in sample
File "<stdin>", line 3, in sample 
```

```txt
File "<stdin>", line 5, in sample >>> 
```

作为替代方案，也可以在程序的任意位置通过调用 pdb.set_trace()来手动加载调试器：

```python
import pdb   
def func(arg): .pdb.set_trace()
```

这对于研究大型程序的内部原理、了解程序的控制流或者函数参数都是非常有用的技术。例如，一旦启动了调试器，就可以通过 print来观察变量，或者输入命令比如w 来获取栈回溯信息。

# 14.12.3 讨论

不要把调试弄的过于复杂。简单的错误常常可以通过阅读程序的 traceback 回溯来解决（实际错误通常是 traceback 的最后一行）。如果你正处于开发过程中，而你只是想获得一些诊断信息，那么在代码中插入一些 print()函数也能很好的完成任务（只是稍后要记得将这些打印语句去掉）。

调试器的常见用途就是对已经崩溃的函数中的变量进行检查。知道如何在程序崩溃之后进入调试器是一项有用的技能。

如果要研究一个特别复杂的程序，其底层的控制流并不明显，那么插入像pdb.set_trace()这样的语句也是十分有帮助的。从本质上说，程序会一直运行，直到遇到 set_trace()调用为止，此时会立刻进入调试器。这之后就可以好好利用调试器的功能了。

如果使用IDE 来做Python 开发，一般来说IDE 都会提供自己的调试接口。调试接口要么是构建在 pdb 之上的，要么就取代了 pdb。可以参考你的 IDE 手册以获得更多的信息。

# 14.13 对程序做性能分析以及计时统计

# 14.13.1 问题

我们想知道程序在运行时把时间都花在了哪些地方，并且想对运行时间做计时统计。

# 14.13.2 解决方案

如果只是想简单地对整个程序做计时统计，通常使用 UNIX 下的 time 命令就足够了。示例如下：

```batch
bash % time python3 someprogram.py  
real 0m13.937s  
user 0m12.162s  
sys 0m0.098s  
bash % 
```

再来看看另一个极端。如果想针对程序的行为产生一份详细的报告，那么可以使用cProfile 模块：

```txt
bash % python3 -m cProfile someprogram.py
859647 function calls in 16.016 CPU seconds
Ordered by: standard name
ncalls tottime percall cumtime percall filename:lineno(function)
263169 0.080 0.000 0.080 0.000 someprogram.py:16(frange)
513 0.001 0.000 0.002 0.000 someprogram.py:30(generate_mandel)
262656 0.194 0.000 15.295 0.000 someprogram.py:32(<genexpr>
1 0.036 0.036 16.077 16.077 someprogram.py:4(<module>
262144 15.021 0.000 15.021 0.000 someprogram.py:4(in_mandelbrot)
1 0.000 0.000 0.000 0.000 os.py:746(urandom)
1 0.000 0.000 0.000 0.000 png.py:1056(_readable)
1 0.000 0.000 0.000 0.000 png.py:1073(Reader)
1 0.227 0.227 0.438 0.438 png.py:163(<module>
512 0.010 0.000 0.010 0.000 png.py:200(group)
...
bash % 
```

对代码做性能分析，更常见的情况则处于上述两个极端情况之间。比如，我们可能已经知道了代码把大部分运行时间都花在某几个函数上了。要对函数进行性能分析，使用装饰器就能办到。示例如下：

# timethis.py   
import time   
from functools import wraps   
def timethis(func): @wraps(func) def wrapper(\*args, \*\*kwargs): start $=$ time.perfcounter() r $=$ func(\*args,\*\* kwargs) end $=$ time.perfcounter() print('{}.{}：{}.format(func._module_,func._name_,end - start))

```lua
return r return wrapper 
```

要使用这个装饰器，只要简单地将其放在函数定义之前，就能得到对应函数的计时信息了。示例如下：

```txt
>>> @timethis  
... def countdown(n):  
    while n > 0:  
        n -= 1  
    >>> countdown(10000000)  
__main__.countdown: 0.803001880645752  
>>> 
```

要对语句块进行计时统计，可以定义一个上下文管理器来实现。示例如下：

from contextlib import contextmanager   
@contextmanager   
def timeblock.label): start $=$ time.perfcounter() try: yield finally: end $=$ time.perfcounter() print('{}：{}'.format.label，end-start))

下面的例子演示了这个上下文管理器是如何工作的：

```txt
>>> with timeblock('counting'):  
... n = 10000000  
... while n > 0:  
... n -= 1  
...  
counting : 1.5551159381866455  
>>> 
```

如果要对短小的代码片段做性能统计，timeit模块会很有帮助。示例如下：

```txt
>>> from timeit import timeit
>>> timeit('math.sqrt(2)', 'import math')
0.1432319980012835
>>> timeit('sqrt(2)', 'from math import sqrt')
0.10836604500218527
>>> 
```

timeit会执行第一个参数中指定的语句一百万次，然后计算时间。第二个参数是一个配置字符串，在运行测试之前会先执行以设定好环境。如果需要修改迭代的次数，只需要提供一个 number 参数即可。示例如下：

```txt
>>> timeit('math.sqrt(2)', 'import math', number=1000000)
1.434852126003534
>>> timeit('sqrt(2)', 'from math import sqrt', number=1000000)
1.0270336690009572
>>> 
```

# 14.13.3 讨论

请注意，在进行性能统计时，任何得到的结果都是近似值。解决方案中使用的函数time.perf_counter()能够提供给定平台上精度最高的计时器。但是，它计算的仍然是墙上时间（wall-clock time），这会受到许多不同因素的影响，例如机器当前的负载。

如果相对于墙上时间，我们更感兴趣的是进程时间，那么可以使用 time.process_time()来替代。示例如下：

from functools import wraps   
def timethis func): @wraps(func) def wrapper $\text{串串}$ ， \*\*kwarges): start $=$ time(process_time() r $=$ func $\text{串串}$ ， \*\*kwarges) end $=$ time(process_time(） print('{}.{}：{}').format(func._module_,func._name_,end - start)) return r return wrapper

最后但同样重要的是，如果打算进行详细的计时统计分析，请确保先阅读 time、timeit以及其他相关模块的文档。这样才能理解不同系统平台之间的重要差异以及其他一些缺陷。

本书 13.13 节也介绍了一个相关的主题，即创建一个秒表定时器。

# 14.14 让你的程序运行得更快

# 14.14.1 问题

我们的程序运行得太慢了，我们想让它提速，但不使用那些极端的解决方案，比如 C扩展或 JIT 编译器。

# 14.14.2 解决方案

尽管关于优化的第一原则也许是“不优化”，但第二原则几乎肯定是“不要优化那些不重要的部分”。基于这两个原则，如果我们的程序运行的很慢，应该采用 14.13 节中讨论的方法开始对代码进行性能分析。

多半时候我们都会发现程序把大量的时间花在了几个“热点”（hotspot）上，比如处理数据时的内层循环。一旦确认了这些“热点”，就可以使用以下各小节中介绍的技术让程序运行得更快。

# 使用函数

有很多程序员开始使用 Python 时都用它来编写一些简单的脚本。当编写脚本时，很容易陷入只管编写代码而不重视程序结构的怪圈。例如：

# somescript.py

import sys

import csv

with open(sys.argv[1]) as f:

for row in csv.reader(f):

$\#$ Some kind of processing

一个鲜为人知的事实是，像上面这样定义在全局范围内的代码运行起来比定义在函数中的代码要慢。速度的差异与局部变量和全局变量的实现机制有关（涉及局部变量的操作要更快）。因此，如果想让程序运行得更快，只要将脚本中的语句放入一个函数中即可：

# somescript.py

import sys

import csv

def main(filename):

with open(filename) as f:

for row in csv.reader(f):

# Some kind of processing

main(sys.argv[1])

运行速度的差异与所执行的处理有很大关系，但根据我们的经验，提升 $1 5 \% \sim 3 0 \%$ 的情况并非罕见。

# 有选择性的消除属性访问

每次使用句点操作符（.）来访问属性时都会带来开销。在底层，这会触发调用特殊方法，比如__getattribute__()和__getattr__()，而调用这些方法常常会导致做字典查询操作。

通常可以通过 from module import name 的导入形式以及选择性地使用绑定方法（boundmethod）来避免出现属性查询操作。为了说明清楚，考虑下面的代码片段：

import math   
def compute_roots(nums): result $=$ [] for n in nums: result.append(math.sqrt(n)) return result   
#Test   
nums $=$ range(1000000)   
for n in range(100): r $=$ compute_roots(nums)

当在我们的机器上测试时，这个程序运行了大约 40 秒。现在将 compute_roots()函数修改为如下形式：

from math import sqrt   
def compute_root(s(nums): result $= []$ result.append $\equiv$ result.append for n in nums: result.append(square(n)) return result

这个版本的运行时间大约是 29 秒。两个版本间的唯一区别就在属性访问上，第二个版本消除了对属性的访问。与其使用 math.sqrt()，现在代码可直接使用 sqrt()。此外，result.append()方法现在被放置在一个局部变量 result_append 中，然后再在内层循环中重复使用它。

但是，必须要强调的是，只有在频繁执行的代码中做这些修改才有意义，比如在循环中。因此，这种优化技术适用的场景需要经过仔细挑选。

# 理解变量所处的位置

前面已经说过了，访问局部变量比全局变量要更快。对于需要频繁访问的名称，想提高运行速度，可以通过让这些名称尽可能成为局部变量来达成。例如，考虑下面这个

修改过的 compute_roots()函数：

import math   
def compute_roots(nums): sqrt $=$ math.sqrt result $= []$ result_add $\equiv$ result.append for n in nums: result_add(sqrt(n)) return result

在这个版本中，sqrt 方法已经从 math 模块中提取出来并放置在一个局部变量中。如果运行这份代码，现在的运行时间大约是 25 秒（比上一个版本的 29 秒又有所提升）。这次提升就是因为查找局部变量 sqrt 比在全局范围内查找 sqrt 要更快。

当使用类的时候，局部参数同样能起到提速的效果。一般来说，查找像 self.name 这样的值会比访问一个局部变量要慢很多。在内层循环中将需要经常访问的属性移到局部变量中来会很划算。示例如下：

Slower   
class SomeClass: def method(self): for x in s: op(self.value) #Faster   
class SomeClass: def method(self): value $=$ self.value for x in s: op(value)

# 避免不必要的抽象

任何时候当使用额外的处理层比如装饰器（decorator）、属性（property）或者描述符（descriptor）来包装代码时，代码的运行速度就会变慢。作为示例，参考下面这个类：

```python
class A: def __init__(self, x, y): self.x = x self.y = y 
```

```txt
@property   
def y(self): return self._y   
@y setter   
def y(self, value): self._y = value 
```

现在做一个简单的计时测试：

```diff
>>> from timeit import timeit
>>> a = A(1,2)
>>> timeit('a.x', 'from __main__ import a')
0.07817923510447145
>>> timeit('a.y', 'from __main__ import a')
0.35766440676525235
>>> 
```

可以看到，访问 property 属性 y 比访问普通的属性 $\mathbf { X }$ 慢了不止一点，而是大约慢了 4.5倍。如果这种差异对你而言很重要，你应该问问自己是否真的有必要将 y 定义为 property属性。如果不是，那么去掉 property 重新用普通的属性来替代即可。不能仅仅因为在其他的编程语言中使用 getter/setter 函数非常普遍，就错误地把这种编程风格应用到Python 上来。

# 使用内建的容器

内建的数据类型比如字符串、元组、列表、集合以及字典都是用 C 语言实现的，速度非常快。如果倾向于构建自己的数据结构作为替代（例如，链表、平衡二叉树等），想在速度上和内建的数据结构相抗衡即使并非不可能也绝对会相当困难。因此，通常最好还是直接使用内建的数据结构。

# 避免产生不必要的数据结构或者拷贝动作

有时候程序员会在不必要的情况下忘乎所以地创建一些不必要的数据结构。例如，有的人可能会编写出如下的代码：

```toml
values = [x for x in sequence]  
squares = [x* x for x in values] 
```

也许这里的想法是先将一些值收集到一个列表中，然后再对列表进行操作，比如列表推导。但是，这里的第一个列表完全是没有必要的。只要把代码写成这样即可：

```txt
squares = [x*x for x in sequence] 
```

与此相关的是，那些对 Python 中共享值的行为过于偏执的程序员，他们编写的代码需

要好好检查一番。过度使用像 copy.deepcopy()这样的函数就是一个信号，这表示代码的编写者不完全理解或者说信赖 Python 的内存模型。在这样的代码中消除那些不必要的拷贝应该是安全的。

# 14.14.3 讨论

在进行优化之前，首先分析一下正在使用的算法通常都是很值得的。把算法的复杂度切换为 O(nlgn)所带来的性能提升绝对比费力调整一个 $\mathrm { O } ( \mathrm { n } ^ { \ast \ast } 2 )$ 的实现要高得多。

如果仍然决定必须优化，那么就从大的方向考虑。一般来说，我们不会针对程序的每个部分都去优化，因为这样的修改会使得代码难以阅读和理解。相反，只针对已知的性能瓶颈做修改，比如内层循环。

我们需要特别留意微优化（micro-optimization）所带来的结果。比如，考虑下面这两种创建字典的方法：

```python
a = {
    'name': 'AAPL',
    'shares': 100,
    'price': 534.22
} 
```

后一种方法的好处在于不需要输入那么多字符（不需要将键名括起来）。但是，如果对上述两个代码片段做一个性能测试对比，就会发现使用 dict()的版本要慢 3 倍！有了这种认识之后，我们可能会倾向于将自己的代码扫描一遍，把每个用到 dict()的地方都用更加冗长的方法替换掉。但是，聪明的程序员只会把精力集中在程序中实际会产生性能影响的地方，比如内层循环。而其他地方的速度差异根本就是无关紧要的。

另一方面，如果我们对性能提升的要求已经远远超出了本节所讨论的这几种简单技术，那就需要考虑使用基于即时编译（just-in-time compilation）技术的工具了。例如，PyPy项目（http://pypy.org）就是对 Python 解释器的重新实现，可以分析你的程序并针对频繁执行的部分生成原始机器码。有时候能使 Python 程序的运行速度快上一个数量级，常常能接近（甚至超越）C代码的执行速度。不幸的是，在写作本书时 PyPy还没能完全支持 Python 3。因此，这是未来需要关注的问题。此外也可以考虑 Numba 项目（http://numba.pydata.org）。Numba 是一个动态编译器，我们可以选择需要优化的 Python函数，然后用装饰器来装饰。这些函数就会通过 LLVM（http://llvm.org）编译成原始的机器码。这种方法同样能够获得显著的性能提升。但是和 PyPy 一样，Numba 对 Python3 的支持应该还只能看做是试验阶段。

最后但同样重要的是，请牢记John Ousterhout（Tcl/Tk语言发明者）的名言：最好的性能提升就是从不能工作转变为可以工作。（The best performance improvement is the transitionfrom the nonworking to the working state.）在确实需要优化之前别担心这个问题。确保让程序能够正常工作总是比让它运行的更快要更加重要（至少在最初阶段是如此）。

# C 语言扩展

本章将讨论从 Python 中访问 C 代码的问题。许多 Python 的内建库都是用 C 语言编写的，能够访问 C 代码对于让 Python 同现有的库进行交互是十分重要的一环。此外，如果我们面临着将扩展代码从 Python 2 移植到 Python 3 中，那么这也是需要重点学习的部分。

尽管 Python提供了广泛的 C语言编程 API，但实际上有着多种不同的方法来应对 C代码。与其针对每个可能的工具和技术都给出详尽的参考，我们采用的方法是把重点放在小段的 C 代码上，用有代表性的示例来展示如何同 C 代码交互。目的是提供一系列的编程模板，有经验的程序员可以展开后供自己使用。

以下就是我们在后续大部分章节中需要打交道的 C代码：

/\*sample.c\*/_method   
#include <math.h>   
/\*Compute the greatest common divisor \*/   
int gcd(int x,int y){ int $\mathrm{g} = \mathrm{y}$ while $(x > 0)$ { $\mathrm{g} = \mathrm{x};$ $\mathrm{x} = \mathrm{y}\% \mathrm{x};$ $\mathrm{y} = \mathrm{g};$ } return g;   
1   
/\*Test if (x0,y0) is in the Mandelbrot set or not \*/ int in_mandel(double x0,double y0,int n){ double $x = 0,y = 0$ ,xtemp;

while $(n > 0)$ { xtemp $=$ x\*x-y\*y+x0; y $= 2^{*}\mathrm{x}^{*}\mathrm{y} + \mathrm{y}0$ . x=xtemp; n $= 1$ if $\left(\mathbf{x}^{\star}\mathbf{x} + \mathbf{y}^{\star}\mathbf{y} > 4\right)$ return 0; } return 1;   
\*/ Divide two numbers \*/ int divide(int a, int b, int *remainder) { int quot $=$ a / b; \*remainder $=$ a % b; return quot;   
\*/ Average values in an array \*/ double avg(double \*a, int n) { int i; double total $= 0.0$ . for $(i = 0;i <   n;i++)$ { total += a[i]; } return total / n;   
\*/ A C data structure \*/ typedef struct Point { double x,y; } Point;   
\*/ Function involving a C data structure \*/ double distance (Point \*p1, Point \*p2) { return hypot(p1->x - p2->x, p1->y - p2->y);

这份代码包含了大量 C 编程中用到的不同特性。首先，有一些简单的函数如 gcd()和is_mandel()。而 divide()则是 C 函数中返回多个值的一个例子，其中一个值是通过指针参数返回的。avg()函数遍历了 C 数组并做了数据转换。Point 和 distance()函数涉及了 C结构体。

后面所有的小节都假设前面这些 C 代码保存在名为 sample.c 的文件中，声明可以在sample.h 中找到，而且代码已经被编译为 libsample 库，可以将其链接到其他的 C 代码

中。编译和链接的具体细节在不同的系统之间有所区别，但这不是我们主要关注的问题。我们假设如果你正在同 C代码打交道，则你已经了解了这些知识。

# 15.1 利用 ctypes 来访问 C 代码

# 15.1.1 问题

我们有一些 C 函数已经被编译为共享库或者 DLL 了。我们想从纯 Python 代码中直接调用这些函数，而不必额外编写 C代码或者使用第三方的扩展工具。

# 15.1.2 解决方案

对于用 C 语言编写的小程序，使用 Python 标准库中的 ctypes 模块来访问通常是非常容易的。要使用 ctypes，必须首先确保想要访问的 C 代码已经被编译为与 Python解释器相兼容（即，采用同样的体系结构、字长、编译器等）的共享库了。对于本小节来说，假设已经创建了共享库 libsample.so，其中包含了本章介绍中所示的那些代码。我们进一步假设文件 libsample.so 与接下来展示的 sample.py 放置在同一个目录中了。

要访问这个共享库，需要创建一个 Python 模块来包装它，示例如下：

# sample.py   
import ctypes   
import os   
# Try to locate the .so file in the same directory as this file _file $=$ 'libsample.so' path $=$ os.path.join $(\ast$ (os.path.split(_file_）[-1] $^+$ (_file,))) _mod $=$ ctypes.cdl.LoadLibrary(_path)   
# int gcd(int, int)   
gcd $=$ _mod.gcd   
gcd.argtypes $=$ (ctypes.c_int, ctypes.c_int)   
gcd/restype $=$ ctypes.c_int   
# int in_mandel(double, double, int)   
in_mandel $=$ _mod.in_mandel   
in_mandel.argtypes $=$ (ctypes.c_double, ctypes.c-double, ctypes.c_int)   
in_mandel/restype $=$ ctypes.c_int   
# int divide(int, int, int *)   
_divide $=$ _mod(divide   
_divide.argtypes $=$ (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))

divide(restype $=$ ctypes.c_int

```python
def divide(x, y):
    rem = types.c_int()
    quot = _divide(x, y, rem)
    return quot, rem.value 
```

```txt
void avg(double *, int n)  
# Define a special type for the 'double*' argument 
```

class DoubleArrayType: def fromparam(self, param): typename $=$ type(param)._name__ if hasattr(self,'from_' + typename): return getattr(self,'from_' + typename)(param) elif isinstance(param, ctypes-array): return param else: raise TypeError("Can't convert %s" % typename)

```python
# Cast from array.array objects
def from_array(self, param):
    if param.typecode != 'd':
        raise TypeError('must be an array of doubles')
    ptr, _ = param.buffer_info()
    return ctypes.cast(ctr, ctypes.POINTER(ctypes.c-double)) 
```

```python
# Cast from lists/tuples
def from_list(self, param):
    val = ((types.c_double) * len(param)) (*param)
    return val
from_tuple = from_list 
```

```python
# Cast from a numpy array
def from ndarray(self, param):
    return paramypes.data_as(ctypes.POINTER(ctypes.c_double)) 
```

DoubleArray $=$ DoubleArrayType() _avg $=$ _mod(avg _avg.argtypes $=$ (DoubleArray, ctypes.c_int) _avg/restype $=$ ctypes.c-double

```python
def avg(values):
    return _avg(values, len(values)) 
```

```python
# struct Point {
class Point(ctypesSTRUCTure):
    _fields_ = ['x', ctypes.c-double),
        ('y', ctypes.c-double)]
# double distance(Map *, Point[])
distance = _mod_distance
distance.argtypes = (ctypes.POINTER(Map), ctypes.POINTER(Map))
distance/restype = ctypes.c_double 
```

如果一切顺利，现在应该可以加载这个模块并使用相应的 C 函数了。例如：

```txt
>>> import sample
>>> sample.gcd(35, 42)
7
>>> sample.in_mandel(0, 0, 500)
1
>>> sample.in_mandel(2.0, 1.0, 500)
0
>>> sampledivide(42, 8)
(5, 2)
>>> sample(avg([1, 2, 3])
2.0
>>> p1 = sample.Point(1, 2)
>>> p2 = sample.Point(4, 5)
>>> sampledistance(p1, p2)
4.242640687119285
>>> 
```

# 15.1.3 讨论

本节中有几个地方需要进行讨论。第一个问题是关于将 C和 Python 代码打包在一起。如果要用 ctypes 来访问自己编译的 C 代码，得确保把共享库放在 sample.py 模块可以找得到的地方。一种可能是将得到的.so 文件与所支撑的 Python 代码放在同一个目录中。这正是本节给出的解决方案中首先完成的——sample.py 查询__file__变量，看看自己被安装到何处，然后在同样的目录下构建一个路径指向 libsample.so 文件。

如果要把 C 库安装到别处，那么必须相应地调整路径。如果 C 库已经作为标准库安装到你的机器上了，那么可以使用 ctypes.util.find_library()函数。示例如下：

```python
>>> from ctypes.util import find_library
>>> find_library('m')
'/usr/lib/libm.dylib'
>>> find_library('pthread')
'/usr/lib/libpthread.dylib'
>>> find_library('sample') 
```

```txt
'/usr/local/lib/libsample.so'  
>>> 
```

再次申明，如果 ctypes 无法找到 C 库则不能继续工作。因此，需要花几分钟时间考虑一下该如何安装库。

一旦知道了 C 库的位置，可以使用 ctypes.cdll.LoadLibrary()来加载。在解决方案中，_path是指向共享库的完整路径，而下列语句则用来加载 C 库：

```python
_mod = ctypes.cdl1.LoadLibrary(_path) 
```

一旦成功加载了 C 库，我们需要编写代码来提取特定的符号并为其附上类型签名。这正是由如下代码完成的：

```txt
int in_mandel(double, double, int)  
in_mandel = _mod.in_mandel  
in_mandel.argtypes = (ctypes.c-double, ctypes.c-double, ctypes.c_int)  
in_mandel(restype = ctypes.c_int 
```

在这份代码中，.argtypes 属性是一个元组，其中包含了函数的输入参数，而.restype 表示返回类型。ctypes 中定义了许多类型对象（例如 c_double、c_int、c_short、c_float等），它们用来代表常见的 C 数据类型。如果想要 Python 传递正确的参数类型并对数据做正确的转换，那么给值附上类型签名就是至关重要的了（如果不这么做，不仅代码不会正常工作，而且还会使得整个解释器进程崩溃）。

使用 ctypes 时，一个多少有些棘手的地方在于原始的 C 代码中可能会用到一些惯用法，而它们在概念上不能清晰地映射到 Python 中。divide()函数就是个很好的例子，因为它是通过其中一个参数来返回值的。尽管这在 C中是非常常见的技术，但放在Python 中往往就不清楚应该如何工作了。例如，我们不能直接像这样做：

```txt
>>> divide = _mod(divide
>>> divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int)
>>> x = 0
>>> divide(10, 3, x)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ctypes.ArgError: argument 3: <class 'TypeError'>: expected LP_c_int
instance instead of int
>>> 
```

就算这么做行的通，也会违反 Python 中整数是不可变对象的事实，可能会导致整个解释器进程卡死在黑洞中。对于涉及指针的参数，通常必须构建一个兼容的 ctypes 对象，然后像下面这样传入：

```rust
>>> x = ctypes.c_int()
>>> divide(10, 3, x) 
```

```txt
3  
>>> x.value  
1  
>>> 
```

这里我们创建了一个 ctypes.c_int 对象，并把它作为指针对象传递给函数。与普通的Python 整数不同，c_int对象是可变的。可以根据需要通过.value属性来获取或修改值。

对于那些 C 调用约定（calling convention）属于非 Pythonic（Pythonic 是俚语，表示按照 Python 的方式来优雅的工作）的情况，通常都需要编写一个小型的包装函数来处理。在解决方案中，这个包装函数使得 divide()函数用一个元组来返回两个结果值：

```python
int divide(int, int, int *)
divide = _mod/mod
divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))
divide(restype = ctypes.c_int
def divide(x, y):
    rem = ctypes.c_int()
    quot = _divide(x, y, rem)
    return quot, rem.value 
```

avg()函数则带来了全新的挑战。底层的 C 代码期望接收一个指针以及长度来代表一个数组。但是从 Python 的角度来看，我们必须考虑下列问题：什么是数组？它是列表还是元组？亦或是 array 模块中的 array 对象？是 numpy 中的数组吗？还是以上都有可能呢？在实践中，一个 Python“数组”可能有着许多不同的形式，而且也许我们会想支持这多种可能。

类 DoubleArrayType 展示了如何处理这种情况。在这个类中我们定义了方法from_param()。这个方法的任务就是接受一个单独的参数并将其范围缩小为一个兼容的ctypes 对象（在本例中就是指向 ctypes.c_double 的指针）。在 from_param()中，我们可以自由地做任何想做的事。在我们的解决方案中，参数的类型名被提取出来并发送给更加具体的方法。例如，如果传递的是列表，则类型名就是 list，调用的就是 from_list()方法。

对于列表和元组，from_list()方法会执行转换到 ctypes 数组对象的操作。这看起来有点古怪，但下面是将列表转换为 ctypes 数组的交互式例子：

```txt
>>> nums = [1, 2, 3]  
>>> a = (ctypes.c_double * len(numbers)) (*numbers)  
>>> a  
<__main__.c_double_array_3 object at 0x10069cd40>  
>>> a[0]  
1.0 
```

```txt
>>> a[1]  
2.0  
>>> a[2]  
3.0  
>>> 
```

对于 array 对象，from_array()方法会提取底层的内存指针并将其转换为一个 ctypes 指针对象。示例如下：

```python
>>> import array
>>> a = array.array('d', [1,2,3])
>>> a
array('d', [1.0, 2.0, 3.0])
>>> ptr_ = a.buffer_info()
>>> ptr
4298687200
>>> ctypes.cast_ptr, ctypes.POINTER(ctypes.c-double))
<__main__.LP_c_double object at 0x10069cd40>
>>> 
```

from_ndarray()则对 numpy 数组做了处理。

通过定义 DoubleArrayType 类并在 avg()的类型签名中使用，可以看到，函数可接受多种不同形式的数组输入：

```diff
>>> import sample
>>> sample AVG([1,2,3])
2.0
>>> sample AVG((1,2,3))
2.0
>>> import array
>>> sample AVG(array.array('d',[1,2,3]))
2.0
>>> import numpy
>>> sample AVG(numpy.array([1.0,2.0,3.0]))
2.0
>>> 
```

本节的最后部分是展示如何同简单的 C 结构体打交道。对于结构体来说，只用定义一个类，并在其中包含适当的字段和类型，示例如下：

```python
class Point(ctypesSTRUCTURE):
    _fields_ = ['x', ctypes.c-double), ('y', ctypes.c-double)] 
```

一旦定义完成，就可以在类型签名中使用它，也可以在需要实例化结构体对象的代码

中使用。示例如下：

```txt
>>> p1 = sample.Point(1,2)  
>>> p2 = sample.Point(4,5)  
>>> p1.x  
1.0  
>>> p1.y  
2.0  
>>> sampledistance(p1,p2)  
4.242640687119285  
>>> 
```

最后再多说几句：如果所有你想做的只是在 Python 中访问几个 C 函数，那么 ctypes 是很有用的库。但是，如果打算访问一个庞大的 C 库，就应该看看其他的方法，比如 Swig（见 15.9 节）或者 Cython（见 15.10 节）。

大型库的主要问题在于由于 ctypes 并不是全自动化处理的，我们将不得不花费大量时间来编写所有的类型签名，就像示例中的那样。根据库的复杂程度，我们可能也不得不编写大量的小型包装函数和支撑类（类似于 DoubleArrayType）。此外，除非完全理解了所有 C接口的底层细节，包括内存管理和错误处理，否则很容易会让 Python 由于段错误、非法访问或其他类似的错误而产生灾难性的崩溃。

作为 ctypes 之外的选择，读者可以去看看 CFFI（http://cffi.readthedocs.org/en/latest）。CFFI提供了很多相同的功能，但使用的是 C的语法，而且支持更多高级的 C代码。在写作本节时，相对来说 CFFI 依然是一个很新的项目，但对它的使用已经得到了极大的增长。甚至有一些关于在今后的 Python版本中将其纳入到 Python标准库中的讨论。因此，CFFI 绝对是值得去留意的项目。

# 15.2 编写简单的 C 语言扩展模块

# 15.2.1 问题

我们想不依赖任何其他工具直接用Python的扩展API编写一个简单的C语言扩展模块。

# 15.2.2 解决方案

对于简单的 C 代码，手工创建一个扩展模块是很简单直接的。作为第一步，可能要确保自己的 C代码有一个合适的头文件。比如：

```txt
/* sample.h */
#include <math.h> 
```

```c
extern int gcd(int, int);   
extern int in_mandel(double x0, double y0, int n);   
extern int divide(int a, int b, int \*remainder);   
extern double avg(double \*a, int n);   
typedef struct Point { double x,y;   
} Point;   
extern double distance(Point \*p1, Point \*p2); 
```

通常情况下这个头文件会对应于一个单独编译好的库。带着这个假设，下面是一个 C语言扩展模块的样例，用来说明编写扩展函数的基础：

include"Python.h" #include "sample.h" /\*int gcd(int,int）\*/ static PyObject \*py_gcd(PyObject \*self，PyObject \*args){ intx,y,result; if(!PyArg_ParseTuple(args,"ii",&x,&y)) { return NULL; } result $=$ gcd(x,y); return Py_BuildValue("i",result);   
} /*int in_mandel(double,double,int）\*/ static PyObject \*py_in_mandel(PyObject \*self，PyObject \*args){ double x0,y0; int n; int result; if(!PyArg_ParseTuple(args,"ddi",&x0,&y0,&n)) { return NULL; } result $=$ in_mandel(x0,y0,n); return Py_BuildValue("i",result);   
} /*int divide(int,int,int\*)\*/ static PyObject \*py Divide(PyObject \*self，PyObject \*args){ int a,b,quotient,remainder; if(!PyArg_ParseTuple(args,"ii",&a,&b)) { return NULL;

}   
quotient $=$ divide(a,b, &remainder);   
return Py_BuildValue(" (ii) ", quotient, remainder);   
}   
/\*Module method table \*/   
staticPyMethodDef SampleMethods[] $=$ { {"gcd",py_gcd,METH.VarARGS,"Greatest common divisor"}, {"in_mandel",py_in_mandel,METH.VarARGS,"Mandelbrot test"},{ "divide",py_divide,METH.VarARGS,"Integer division"},{ NULL,NULL,O,NULL}   
};   
/\*Module structure \*/   
static struct PyModuleDef samplemodule $=$ { PyModuleDef_HEAD_INIT, "sample", /* name of module */ "A sample module",/* Doc string (may be NULL) \*/ -1, /\* Size of per-interpreter state or -1 \*/ SampleMethods /\* Method table \*/   
};   
/\*Module initialization function \*/   
PyMODINIT FUNC   
PyInit_sample(void){ return PyModule_Create(&samplemodule);   
}

为了构建扩展模块，需要创建一个 setup.py 文件，看起来是这样的：

setup.py   
from distutils.core import setup, Extension   
setup(name $=$ 'sample',   
ext Modules $\equiv$ [   
Extension('sample',   
['pysample.c'],   
include_dirs $= \mathrm{[}^{\prime \prime}$ /some/dir'],   
define Macros $= \mathrm{[}^{\prime \prime}$ FOO'1')],   
undef Macros $= \mathrm{[}^{\prime \prime}$ BAR'],   
library_dirs $= \mathrm{[}^{\prime \prime}$ /usr/local/lib'],   
libraries $= \mathrm{[}^{\prime \prime}$ sample']   
]

现在，要构建出目标库，只需要用 python3 buildlib.py build_ext --inplace。示例如下：

```batch
bash % python3 setup.py build_ext --inplace  
running build_ext  
building 'sample' extension  
gcc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes  
-I/usr/local/include/python3.3m -c pysample.c  
-o build/temp.macsx-10.6-x86_64-3.3/pysample.o  
gcc -bundle -undefined dynamic.lookup  
build/temp.macsx-10.6-x86_64-3.3/pysample.o  
-L/usr/local/lib -lsample -o sample.so  
bash % 
```

如上所示，这样就创建了一个名为 sample.so的共享库。编译结束后，应该就可以开始将其当做一个 Python 模块来导入了：

```txt
>>> import sample
>>> sample.gcd(35, 42)
7
>>> sample.in_mandel(0, 0, 500)
1
>>> sample.in_mandel(2.0, 1.0, 500)
0
>>> sampledivide(42, 8)
(5, 2)
>>> 
```

如果打算在 Windows 上尝试这些步骤，可能需要花点时间设置构建环境，以便正确生成扩展模块。Python 的二进制发行版通常是用微软的 Visual Studio 来构建的。要让扩展模块正常工作，我们可能也要用相同或兼容的工具来编译。具体请参见 Python 的有关文档（http://docs.python.org/3/extending/windows.html）。

# 15.2.3 讨论

在尝试手工编写任何类型的扩展前，查阅Python文档中的“扩展和内嵌Python解释器”（Extending and Embedding the Python Interpreter）一节是至关重要的。Python 的 C 语言扩展 API 很庞大，在这里重复所有的 API 是不现实的。但是，我们可以就最重要的部分在此讨论。

首先，在扩展模块中编写的函数通常都有着如下的共同原型：

```txt
static PyObject \*py_func(PyObject \*self, PyObject \*args) { 
```

PyObject 是一个 C 数据类型，表示任意的 Python 对象。从很高的层次来看，一个扩展

函数就是一个 C 函数，它接受一组 Python 对象（在 PyObject *args 中）并返回一个新的 Python 对象作为结果。对于简单的扩展函数来说，函数的 self 参数是用不到的，但是当想在 C 中定义新的类或对象类型时就会派上用场了（例如，如果扩展函数是类的一个方法，那么 self 就会用来表示对象实例）。

PyArg_ParseTuple()函数用来将值从 Python 转换为 C 语言中的表示。作为输入，它接受一个格式化字符串用来表示所需的值类型，例如“i”表示整数，而“d”表示 double型浮点数。此外，它还接受 C 变量的地址作为参数，用来放置转换后的结果。PyArg_ParseTuple()会对参数的数量和类型做许多检查。如果在格式化字符串中发现有任何不匹配的项，则会产生一个异常并返回 NULL。通过对参数的检查以及返回 NULL，在调用端就会产生适当的异常了。

函数 Py_BuildValue()用来从 C数据类型创建出对应的 Python 对象。它也接受一个格式化代码用来表示所需的类型。在扩展函数中，它用来将结果返回给 Python。Py_BuildValue()的一个特性是它可以构建类型更加复杂的对象，比如元组和字典。在针对 py_divide()的代码中，我们已经展示了一个返回元组的例子。但是，下面还有一些更多的示例：

```lua
return Py_BuildValue("i", 34); // Return an integer  
return Py_BuildValue("d", 3.4); // Return a double  
return Py_BuildValue("s", "Hello"); // Null-terminated UTF-8 string  
return Py_BuildValue(" (ii)", 3, 4); // Tuple (3, 4) 
```

在任何扩展模块代码的底部，我们都会找到一个像示例中的 SampleMethods 这样的函数表。这张表列出了C函数、在 Python 中所用的名称以及文档字符串。所有的模块都需要指定一个这样的表，它会在模块初始化时用到。

最后，函数PyInit_sample()是模块的初始化函数，当模块首次导入时会调用执行。它的主要工作就是把模块对象注册到解释器中。

作为最后的说明，必须要强调的是，关于用 C 函数来扩展 Python，还有相当多的内容没有在这里给出（事实上，Python 的 C API 中包含了超过 500 个函数）。你应该把本节当做入门的踏脚石。要完成更多功能，可以从函数 PyArg_ParseTuple()和 Py_BuildValue()的文档开始，然后从那儿开始扩展。

# 15.3 编写一个可操作数组的扩展函数

# 15.3.1 问题

我们想编写一个 C 扩展函数来操作数组型数据，数组可能会通过 array 模块或 NumPy这样的库来创建。但是，我们想让自己的函数变得通用，而不必具体于任何一个创建

数组的库。

# 15.3.2 解决方案

要以可移植的方式来接收和处理数组，应该编写使用了 Buffer Protocol（http://docs.python.org/3/c-api/buffer.html）的代码。下面是一个手写的 C 扩展函数示例，它接受数组数据并调用本章介绍部分给出的 avg(double *buf, int len)函数：

/* Call double avg(double *, int) */   
static PyObject \*py_avg(PyObject \*self, PyObject \*args) { PyObject \*bufobj; Py_buffer view; double result; /* Get the passed Python object */ if (!PyArg_ParseTuple(args, "O", &bufobj)) { return NULL; } /* Attempt to extract buffer information from it */ if (PyObject_GetBuffer(bufobj, &view, PyBUF_ANY_CONTIGUOUS | PyBUF_FORMAT) $= = -1$ ）{ return NULL; } if (view.ndim != 1) { PyErr_SetString(PyExc_TYPEError, "Expected a 1-dimensional array"); PyBuffer_Release(&view); return NULL; } /* Check the type of items in the array */ if (strcmp_view.format,"d") != 0) { PyErr_SetString(PyExc_TYPEError, "Expected an array of doubles"); PyBuffer_Release(&view); return NULL; } /* Pass the raw buffer and size to the C function */ result = avg.view(buf, view.shape[0]); /* Indicate we're done working with the buffer */ PyBuffer_Release(&view); return Py_BuildValue("d", result);

下面的示例展示了这个扩展函数是如何工作的：

```txt
>>> import array
>>> avg(array.array('d', [1, 2, 3])
2.0
>>> import numpy
>>> avg(numpy.array([1.0, 2.0, 3.0])
2.0
>>> avg([1, 2, 3])
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: 'list' does not support the buffer interface
>>> avg(b'Hello')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: Expected an array of doubles
>>> a = numpy.array([[1., 2., 3.], [4., 5., 6.]])
>>> avg(a[:, 2])
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ValueError: ndarray is not contiguous
>>> sample_avg(a)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: Expected a 1-dimensional array
>>> sample_avg(a[0])
2.0
>>> 
```

# 15.3.3 讨论

将数组对象传递给 C 函数可能是在编写扩展函数中最常遇到的情况之一了。从图像处理到科学计算领域，有大量的 Python 应用程序都依赖于对数组的高效处理。通过编写可以接受并操作数组的代码，就可以编写自定义的代码来很好地应用到这些应用之上，而不是鼓捣出某种自定义的解决方案只能用在自己的代码中。

示例代码的核心就在 PyBuffer_GetBuffer()函数上。任意给定一个 Python 对象，该函数会尝试获取有关对象底层内存表示的相关信息。如果无法做到这点——大部分普通的Python 对象都属于这种情况，则会产生一个异常并返回-1。传给 PyBuffer_GetBuffer()的特殊标记进一步提供了所请求的内存缓冲区的类型。例如，PyBUF_ANY_CONTIGUOUS 表示请求的是一段连续的内存。

针对数组、字节串以及其他类似的对象，结构体 Py_buffer 中会保存有关底层内存的信息。这包括一个指向内存块的指针、总内存大小、数组中每个元素的大小、格式以及其它细节。下面是这个结构体的定义：

```c
typedef struct bufferinfo {
    void *buf; /* Pointer to buffer memory */ 
```

```c
PyObject *obj; /* Python object that is the owner */ Py_ssize_t len; /* Total size in bytes */ Py_ssize_t itemsize; /* Size in bytes of a single item */ int readonlyly; /* Read-only access flag */ int ndim; /* Number of dimensions */ char *format; /* struct code of a single item */ Py_ssize_t *shape; /* Array containing dimensions */ Py_ssize_t *strides; /* Array containing strides */ Py_ssize_t *suboffsets; /* Array containing suboffsets */ } Py_buffer; 
```

在本节中，我们只考虑接收一个内存连续的 double 型浮点数数组。要检查数组元素是否是 double 型的，可以检查 format 属性的格式化字符串是否为“d”。这个格式化字符串也是标准库中 struct模块用来编码二进制值时所用的。一般来说，format可以是任意一种同 struct模块相兼容的格式化字符串。如果数组中包含 C结构体，那么这个格式化字符串也可能会包含多个类型代码。

一旦我们验证了底层缓冲区的信息，我们只需简单地将其传给 C 函数（示例中为 avg()函数），则它会被当做一个普通的C数组来对待。这么做的实践意义在于不用考虑数组是什么类型，也不必考虑它是由什么库创建出来的。这就是为什么我们的函数既可以同 array模块也可以同 numpy库创建出的数组一起工作的原因。

在返回最终结果前，底层的缓冲区必须通过 PyBuffer_Release()来释放。我们需要通过这个步骤来恰当地管理对象的引用计数。

再次申明，本节只展示了一小段接收数组的代码。如果要同数组打交道，则可能会遇到多维数组、不同的数据类型以及更多需要学习的技术。确保去查看官方文档（http://docs.python.org/3/c-api/buffer.html）以获得更多的细节。

如果需要编写许多涉及数组处理的 C扩展函数，可能会发现以 Cython来实现这些代码会更容易些。具体请参见 15.11 节。

# 15.4 在 C 扩展模块中管理不透明指针

# 15.4.1 问题

我们有一个扩展模块需要处理指向 C 结构体的指针，但是不想把结构体的任何内部细节暴露给 Python。

# 15.4.2 解决方案

不透明数据结构很容易通过将它们包装进一个 capsule 对象中来处理。考虑下面的代码

片段：

```c
typedef struct Point { double x,y;   
} Point;   
extern double distance (Point \*p1, Point \*p2); 
```

这里是一个扩展代码的示例，其中使用了 capsule 对象来对 Point 结构体和 distance()函数进行包装：

```c
/* Destructor function for points */
static void del_Point(PyObject *obj) {
    free(PyCapsule_GetPointer(obj, "Point"));
}
/* Utility functions */
static Point *PyPoint_AsPoint(PyObject *obj) {
    return (Point *) PyCapsule_GetPointer(obj, "Point");
}
static PyObject *PyPoint_FromPoint(Point *p, int must_free) {
    return PyCapsule_New(p, "Point", must_free ? del_Point : NULL);
}
/* Create a new Point object */
static PyObject *py_Point(PyObject *self, PyObject *args) {
    Point *p;
    double x, y;
    if (!PyArg_ParseTuple(args, "dd", &x, &y)) {
        return NULL;
    }
    p = (Point *) malloc(sizeof(Point));
    p->x = x;
    p->y = y;
    return PyPoint_FromPoint(p, 1);
}
static PyObject *py_distance(PyObject *self, PyObject *args) {
    Point *p1, *p2;
    PyObject *py_p1, *py_p2;
    double result;
    if (!PyArg_ParseTuple(args, "OO", &py_p1, &py_p2)) {
        return NULL;
    }
}; 
```

```txt
if (!p1 = PyPoint_AsPoint(py_p1)) { return NULL; } if (!p2 = PyPoint_AsPoint(py_p2)) { return NULL; } result = distance(p1, p2); return Py_BuildValue("d", result); } 
```

下面让我们从 Python 中来使用这些函数：

```haskell
>>> import sample
>>> p1 = sample.Point(2,3)
>>> p2 = sample.Point(4,5)
>>> p1
<caption object "Point" at 0x1004ea330>
>>> p2
<caption object "Point" at 0x1005d1db0>
>>> sampledistance(p1,p2)
2.8284271247461903
>>> 
```

# 15.4.3 讨论

capsule 对象和 C 语言中的 void 指针很相似。capsule 对象内部持有一个泛型指针以及一个标识名称。可以通过 PyCapsule_New()函数来轻松创建出 capsule 对象。此外，可以在 capsule 对象上关联一个可选的析构函数，当 capsule 对象被垃圾收集机制回收时可用来释放底层的内存空间。

要提取出包含在 capsule 对象中的指针，可以通过函数 PyCapsule_GetPointer()来完成，只要指定名称即可。如果提供的名称与 capsule 对象不匹配或者出现了其他的错误，那么会产生一个异常并返回 NULL。

本节中我们编写了一对功能函数 PyPoint_FromPoint()和 PyPoint_AsPoint()，用来处理从capsule 对象中创建和回退 Point 实例。在所有的扩展函数中，我们都会使用这一对函数而不是直接同 capsule 对象打交道。这个设计决策使得将来对包装 Point 对象的修改会变得更容易些。例如，如果稍后决定使用其他的机制而不是 capsule 对象的话，只需要修改这两个函数即可。

在使用 capsule 对象时，一个比较棘手的地方在于需要考虑垃圾收集和内存管理。函数PyPoint_FromPoint()接受一个 must_free 参数，表示当 capsule 对象被销毁时，底层的 Point结构体所占用的内存是否也要被回收。当遇到这样的C代码时，对象的归属（ownership）问题会很难处理（例如，也许 Point 结构体嵌入到了另一个更大的数据结构中，而那个

结构体是单独管理的）。与其把宝都押在垃圾收集上，这个额外的参数使得控制权重新回到程序员手上。应该要注意的是，已经在 capsule对象上关联的析构函数也可以通过使用 PyCapsule_SetDestructor()函数来修改。

当面对某些涉及结构体的C代码时，capsules 是一种明智的解决方案。比如说，有时候我们并不在乎把结构体的细节暴露出来，或者会将其转换为一个全功能的扩展类型。有了 capsule，我们可以为结构体加上一个轻量级的包装层，这样可以轻松将其传递给其他的扩展函数。

# 15.5 在扩展模块中定义并导出 C API

# 15.5.1 问题

我们有一个 C 扩展模块在内部定义了各种有用的函数，现在想将它们导出作为公有的 CAPI 在别处使用。我们想把这些函数用在其他的扩展模块中，但是不知道该如何将它们链接在一起，而用 C编译器/链接器来做似乎又显得过于复杂（或者根本不可能做到）。

# 15.5.2 解决方案

本节把重点放在处理Point对象的代码上，代码在15.4节中已给出。如果回顾一下，这里的 C代码中包含了一些实用的函数，比如：

```c
/\*Destructor function for points \*/   
static void del_Point(PyObject \*obj) { free(PyCapsule_GetPointer(obj, "Point"));   
}   
\*/ Utility functions \*/   
static Point \*PyPoint_AsPoint(PyObject \*obj) { return (Point \*) PyCapsule_GetPointer(obj, "Point");   
}   
static PyObject \*PyPoint_FromPoint (Point \*p, int must_free) { return PyCapsule_New(p, "Point", must_free ? del_Point : NULL); 
```

现在的问题就是如何将函数 PyPoint_AsPoint()和 PyPoint_FromPoint()作为 API 导出，让其他的扩展模块可以使用和链接（例如，如果有其他的扩展模块也想使用包装过的 Point对象）。

要解决这个问题，首先为示例扩展模块引入一个全新的头文件 pysample.h。将下列代码输入到这个头文件中：

/\* pysample.h\*/   
#include"Python.h"   
#include"sample.h"   
#ifndef cplusplus   
extern"C" {   
endif   
/\*Public API Table \*/   
typedef struct{ Point \*(\*aspoint)(PyObject \*); PyObject \*(\*frompoint)(Point \*,int); }_PointAPIMethods;   
#	define PYSAMPLE_MODULE   
/\*Method table in external module \*/   
static _PointAPIMethods \*_point_api $= 0$ .   
\*/Import the API table from sample \*/   
static int import_sample(void){ _point_api $=$ (_PointAPIMethods \*) PyCapsule_Import("sample._point_api",0); return(_point_api！ $= \mathrm{NULL})$ ?1:0;   
}   
/\* Macros to implement the programming interface \*/   
#define PyPoint_Aspoint(obj) (_point_api->aspoint)(obj)   
#define PyPoint_FromPoint(obj) (_point_api->frompoint)(obj)   
#endif   
#ifdef_cplusplus   
}   
#endif

这里最重要的特性就是函数指针表_PointAPIMethods。它会在导出模块中进行初始化，这样在导入模块中就可以找到它。

修改原来的扩展模块，增加这个函数指针表并像下面这样进行导出：

/\* pysample.c \*/   
#include"Python.h"   
#define PYSAMPLEMODULE   
#include "pysample.h" $\ldots$ /\*Destructor function for points \*/ static void del_Point(PyObject \*obj) {

printf("Deleting point\n"); free(PyCapsule_GetPointer(obj, "Point"));   
}   
/\* Utility functions \*/ static Point \*PyPoint_Aspoint(PyObject \*obj) { return (Point \*) PyCapsule_GetPointer(obj, "Point");   
}   
static PyObject \*PyPoint_FromPoint (Point \*p, int free) { return PyCapsule_New(p, "Point", free ? del_Point : NULL);   
}   
static _PointAPIMethods _point_api = { PyPoint_Aspoint, PyPoint_FromPoint };   
...   
/\*Moduleinitializationfunction\*/ PyMODINIT FUNC   
PyInit_sample(void) { PyObject \*m; PyObject \*py_point_api; m $=$ PyModule_Create(&samplemodule); if (m $= =$ NULL) return NULL; /\*Add the Point C API functions \*/ py_point_api $=$ PyCapsule_New((void \*) &_point_api, "sample._point_api", NULL); if (py_point_api) { PyModule_AddObject(m, " _point_api", py_point_api); } return m;   
}

最后，下面这个示例是一个新的扩展模块，它会加载并使用这些 API 函数：

```javascript
/\*ptexample.c\*/   
/\*Include the header associated with the other module \*/ #include"pysample.h"   
/\*An extension function that uses the exported API \*/ static PyObject \*print_point(PyObject \*self,PyObject \*args) { 
```

PyObject *obj;   
Point \*p;   
if (!PyArg_ParseTuple(args, "O", &obj)) { return NULL; } /* Note: This is defined in a different module */ p = PyPoint_Aspoint(obj); if (!p) { return NULL; } printf("%f %f\n", p->x, p->y); return Py_BuildValue("");   
}   
static PyMethodDef PtExampleMethods[] = { {"print_point", print_point, METH_varARGS, "output a point"},{ NULL, NULL, 0, NULL} };   
static struct PyModuleDef ptexamplemodule $=$ { PyModuleDef_HEAD_INIT, "ptexample", /* name of module */ "A module that imports an API", /* Doc string (may be NULL) */ -1, /* Size of per-interpreter state or -1 */ PtExampleMethods /* Method table */ };   
/* Module initialization function */ PyMODINIT FUNC   
PyInit.ptexample(void) { PyObject \*m; m $=$ PyModule_Create(&ptexamplemodule); if (m $= =$ NULL) return NULL; /* Import sample, loading its API functions */ if (!import_sample()) { return NULL; } return m;

当编译这个新的模块时，我们甚至不必操心去链接任何库或者其他模块中的代码。只

用创建一个简单的 setup.py 文件即可：

# setup.py   
from distutils.core import setup, Extension   
setup(name $\equiv$ 'ptexample', ext Modules $=$ [ Extension('ptexample', ['ptexample.c'], includedirs $= []$ # May need pysample.h directory 1

如果一切顺利，就会发现我们的新扩展模块可以完美地同定义在其他模块中的 C API一起工作了：

```diff
>>> import sample
>>> p1 = sample.Point(2,3)
>>> p1
<caption object "Point *" at 0x1004ea330>
>>> import pxample
>>> pxample.print_point(p1)
2.000000 3.000000
>>> 
```

# 15.5.3 讨论

本节所讨论的技术依赖于一个事实，即，capsule 对象可以持有一个指针，该指针可指向任何所希望的对象。在这种情况下，定义 capsule 对象的模块会去填充函数指针结构体，创建一个 capsule 对象并让它指向这个函数指针表，最后将 capsule 对象保存在模块级属性中（即，sample._point_api）。

当导入模块后，其他的模块就可以通过编程的方式来获取这个属性并提取出底层的指针。实际上，Python 提供了实用函数 PyCapsule_Import()，它可以为我们完成所有的步骤。我们只用给它提供一个属性名（例如 sample._point_api），它就会找到 capsule 对象并提取出指针。

这里用到了一些 C 编程技巧使得导出的函数在其他模块中看起来也并无不同之处。在文件 pysample.h 中，指针_point_api 用来指向在导出模块中初始化的函数指针表。用import_sample()函数来执行导入 capsule 对象以及初始化指针_point_api 的任务。在使用模块中的其他函数之前，必须先调用 import_sample()。通常这会在模块初始化的时候完成调用。最后，还定义了一组 C 预处理器宏以透明的方式通过函数指针来引用 API 函数。用户只需要使用原来的函数名即可，并不需要知道底层是通过这些宏经过额外的一层间接关系来引用函数的。

最后，为什么要用这项技术来将模块链接在一起还有另一个重要的原因——这样做更加简单而且保证了模块间层次清晰、耦合度低。如果不想使用本节展示的技术，也可以利用共享库和动态加载器的功能来做交叉链接。比如，把所有公用的 API 函数放在共享库中，并确保所有的扩展模块都来链接这个共享库。是的，这么做可行，但是在大型系统中这么操作会非常繁琐。本质上，本节已经揭示了所有的魔法，允许模块通过 Python 的普通导入机制以及极少数的 capsule调用实现对其他模块的链接。至于模块的编译问题，只需要担心头文件而不是共享库的实现细节。

更多有关为扩展模块提供 C API 的信息可以在 Python 文档（http://docs.python.org/3/extending/extending.html）中找到。

# 15.6 从 C 中调用 Python

# 15.6.1 问题

我们想以安全的方式从 C中执行一个 Python 的可调用对象，并将结果返回到C中。比方说，也许你正在编写 C代码，希望把一个 Python 函数当做回调来使用。

# 15.6.2 解决方案

在 C中调用Python 基本上是简单明了的事，但是有几个地方需要用到一些技巧。下面的 C 代码作为一个示例展示了如何安全的从 C 中调用 Python：

include <Python.h>   
/\* Execute func(x,y) in the Python interpreter. The arguments and return result of the function must be Python floats \*/   
double call_func(PyObject \*func, double x, double y) { PyObject \*args; PyObject \*kwargs; PyObject \*result $= 0$ double retval; /\*Make sure we own the GIL \*/ PyGILState_STATE state $=$ PyGILState_Ensure(); /\* Verify that func is a proper callable \*/ if(!PyCallable_Check(func)){ fprintf(stderr,"call_func:expected a callable\n"); goto fail;

} /\*Build arguments \*/ args $=$ Py_BuildValue("dd"，x，y); kwargs $=$ NULL; /\*Call the function \*/ result $=$ PyObject_Call(func，args,kkwargs); Py_DECBCF(args); Py_XDECBCF(kwargs); /\*Check for Python exceptions (if any) \*/ if(PyErr_Occurred()){ PyErr Print(); goto fail; } /\* Verify the result is a float object \*/ if(!PyFloat_Uncheck(result)){ fprintf(stderr,"call_func: callable didn't return a float\n"); goto fail; } /\*Create the return value \*/ retval $=$ PyFloat_ADouble(result); Py_DECBCF(result); /\*Restore previous GIL state and return \*/ PyGILState_Release(state); return retval;   
fail: Py_XDECBCF(result); PyGILState_Release(state); abort(）; //Change to something more appropriate

要使用这个函数，需要将一个已存在的 Python 可调用对象的引用传递进来。有许多种方法可以实现，比如把一个可调用对象传递到一个扩展模块中，或者直接编写 C 代码从已有的模块中提取出相应的符号。

下面这个简单的例子展示了从一个嵌入的 Python 解释器中调用函数：

```txt
include <Python.h> /\*Definition of call_func() same as above \*/ 
```

$\star$ Load a symbol from a module \*/   
PyObject \*import_name(const char \*modname, const char \*symbol) { PyObject \*u_name,\*module; u_name $=$ PyUnicode_FromString(modname); module $=$ PyImport_Import(u_name); Py_DECREF(u_name); return PyObject_GetAttrString/module，symbol);   
}   
/\* Simple embedding example \*/   
int main() { PyObject \*pow_func; double x; Py初始化(); /\*Getareference to the math.pow function \*/ pow_func $=$ import_name("math","pow"); /\*Call it using our call_func() code \*/ for $(x = 0.0;x <   10.0;x + = 0.1)$ { printf("%0.2f %0.2f\n",x，call_func(pow_func,x,2.0)); } /\*Done\*/ Py_DECREF(pow_func); Py_Final(){ return 0;

要构建这个最新的示例，需要编译上述 C 代码并同 Python 解释器链接。这里有一个Makefile 告诉我们如何去做（可能需要在自己的机器上做些调整）

```makefile
all:: cc -g embed.c -I/usr/local/include/python3.3m \ -L/usr/local/lib/python3.3/config-3.3m -lpython3.3m 
```

编译代码并运行得到的可执行文件，应该会产生类似这样的输出：

```txt
0.00 0.00  
0.10 0.01  
0.20 0.04  
0.30 0.09  
0.40 0.16  
... 
```

下面的示例稍有不同，一个扩展函数接收一个 Python 可调用对象以及一些参数，并将

它们传递给 call_func()用于测试：

/\*Extension function for testing the C-Python callback \*/   
PyObject \*py_call_func(PyObject \*self, PyObject \*args) { PyObject \*func; double x,y,result; if(!PyArg_ParseTuple(args,"Odd",&func,&x,&y)) { return NULL; } result $=$ call_func(func,x,y); return Py_BuildValue("d",result);   
}

使用这个扩展函数，可以像下面这样测试其功能：

```txt
>>> import sample
>>> def add(x,y):
    ...
        return x+y
...
>>> sample.call_func(add,3,4)
7.0
>>> 
```

# 15.6.3 讨论

如果要从 C中调用Python，需要记住的最重要的事情就是此时 C会获得程序的控制权。也就是说，创建参数、调用 Python 函数、检查是否有异常、检查类型、获取返回值等责任都落在了 C的身上。

首先，很重要的一点是我们得有一个 Python 对象，用来代表打算去调用的那个可调用对象。这可以是函数、类、方法、内建方法或者任何实现了__call__()操作的对象。要验证对象是否是可调用的，可以使用下列代码片段中给出的 PyCallable_Check()函数：

```txt
double call_func(PyObject *func, double x, double y) {
    ...
    /* Verify that func is a proper callable */
    if (!PyCallable_check(func)) {
        printf(stderr, "call_func: expected a callable\n");
        goto fail;
    }
} 
```

顺便说一句，我们需要仔细学习如何在 C 代码中处理错误。一般来说，我们没法直接抛出一个 Python 异常。相反，错误需要按照 C语言的方式来处理。在给出的解决方案中，我们使用 goto 来将控制流转移到一个错误处理块中，并在那里调用 abort()函数。

这会导致整个程序退出，但是在现实环境中可能需要做些更加优雅的处理（例如返回一个状态码）。请记住，此时是 C代码在接管控制流，因此抛出异常是无法同C兼容的。错误处理必须由构建到程序中的组件来完成。

调用一个函数相对来说就简单直接多了——只需要调用 PyObject_Call()，提供给它可调用对象、参数元组以及一个可选的关键字参数字典即可。要构建参数元组或者字典，可以使用 Py_BuildValue()，示例如下：

```c
double call_func(PyObject *func, double x, double y) {
    PyObject *args;
    PyObject *kwargs;
    ...
    /* Build arguments */
    args = Py_BuildValue("&", x, y);
    kwargs = NULL;
    /* Call the function */
    result = PyObject_call(func, args, kwargs);
    Py_DECREF(args);
    Py_XDECREF(k kwargs);
    ...
} 
```

如上述代码所示，如果没有关键字参数，那么可以传NULL。在完成函数调用后，需要确保使用 Py_DECREF()或者 Py_XDECREF()来清理参数。后者可安全地接受 NULL 指针（会忽略掉），这也是为什么我们用它来清理可选的关键字参数。

在调用了 Python 函数后，必须检查是否有异常出现。PyErr_Occurred()函数可以用来完成这个任务。比较棘手的地方在于知道如何去响应异常。因为我们工作在 C 语言的环境中，缺少 Python 所拥有的异常机制。因此，需要设定错误状态码，对错误做日志记录，或者去做一些明智的处理。在我们的解决方案中，由于没有更加简单的替代方案，因此直接调用了 abort()（此外，C 语言的拥护者也会更欣赏这种直接让程序崩溃的方案）。

$\dots$ /\* Check for Python exceptions (if any) \*/ if (PyErr_Occurred()) { PyErr(Print(); goto fail; }   
fail: PyGILState_Release(state); abort();

从调用的 Python 函数的返回值中提取出信息，一般来说需要涉及某种类型检查和提取值 的 过 程 。 要 做 到 这 点 ， 可 能 必 须 用 到 Python concrete 对 象 层 （ https://docs.python.org/3/c-api/concrete.html）中的函数。在解决方案中，我们使用了 PyFloat_Check()和 Py_Float_AsDouble()函数来检查并提取出 Python 浮点数。

关于从 C 中调用 Python，最后一个棘手的部分在于管理 Python 的全局解释器锁（GIL）。每当从C中访问 Python时，需要保证对 GIL做合适的获取和释放动作。否则，就会有Python 解释器破坏了数据或者崩溃的风险。调用 PyGILState_Ensure()和 PyGILState_Release()可确保正确地完成这些步骤：

```c
double call_func(PyObject *func, double x, double y) {
    ...
    double retval;
    /* Make sure we own the GIL */
    PyGILState_STATE state = PyGILState_Encsure();
    ...
    /* Code that uses Python C API functions */
    ...
    /* Restore previous GIL state and return */
    PyGILState_Release(state);
    return retval;
}
fail:
    PyGILState_Release(state);
    abort();
} 
```

调用 PyGILState_Ensure()成功后，将总是保证调用线程对 Python 解释器享有独占访问权。甚至当调用的 C代码正在运行着另一个对Python 解释器来说未知的线程时也是如此。此时，C 代码可自由使用任何想调用的 Python C-API 函数了。当成功返回时，使用 PyGILState_Release()来将解释器恢复到它原来的状态。

要重点提到的是，每个 PyGILState_Ensure()调用必须跟着一个匹配的 PyGILStateRelease()调用——甚至在出现错误的情况下也必须如此。在解决方案中，我们使用的goto 语句可能看起来是种糟糕的设计，但是实际上我们利用它来将控制流跳转到一个公共的退出语句块中，在那里执行这个必要的步骤。可以把 fail:标签后的代码想象成Python 中的 finally:语句块，它们的作用和目的是一样的。

如果我们编写的 C 代码使用了所有这些约定，包括管理 GIL、检查异常以及对错误的彻底检查，将会发现我们能够以可靠的方式从 C 中调用 Python 解释器——即使在使用了高级编程技术（比如多线程）的复杂程序中也是如此。

# 15.7 在 C 扩展模块中释放 GIL

# 15.7.1 问题

我们希望自己的 C 扩展代码能够同其他的线程一起在 Python 解释器中并发运行。要做到这点，需要释放并重新获取全局解释器锁（GIL）。

# 15.7.2 解决方案

在 C 扩展代码中，可以通过插入下列宏来释放并重新获取 GIL：

```txt
include"Python.h"   
PyObject \*pyfunc(PyObject \*self，PyObject \*args）{ Py_BEGIN_OPEN Threads // Threaded C code．Must not use Python API functions Py_END_OPEN Threads return result; 
```

# 15.7.3 讨论

GIL 只能在一种情况下被安全的释放，即，如果可以保证在 C 代码中不执行任何 PythonC API 函数。典型的例子就是在计算密集型代码中对 C 数组执行计算时（例如在 numpy这样的扩展模块中）或者在执行阻塞式I/O 操作的代码中（例如在文件描述符上执行读取或写入操作时）。

因为 GIL 的释放，其他 Python 线程就允许在解释器中执行了。宏 Py_END_ALLOW_THREADS 会阻塞执行，直到调用线程重新获取到 GIL 为止。

# 15.8 混合使用 C 和 Python 环境中的线程

# 15.8.1 问题

我们的程序中混合了 C、Python 和线程，但是其中有些线程是在 C 环境中创建的，不受 Python 解释器的控制。此外，还有一些特定的线程使用了 Python C API 中的函数。

# 15.8.2 解决方案

如果打算将 C、Python 和线程混合在一起使用，需要确保以恰当的方式初始化和管理Python 的全局解释器锁（GIL）。要做到这点，可以在C代码中包含如下的代码，并确保在创建任何线程之前先调用它：

```cpp
include <Python.h>   
if(!PyEval Threads Initialized()）{ PyEval_InitThreads();   
} 
```

对于任何涉及 Python 对象或者 Python C API 的 C 代码，首先需要保证以合适的方式获取和释放 GIL。这可以通过 PyGILState_Ensure()和 PyGILState_Release()来做到，示例如下：

$\text{一} ^ { \text{一} }$ Make sure we own the GIL \*/ PyGILState_STATE state $=$ PyGILState_Ensure(); $\text{一} ^ { \text{一} }$ Use functions in the interpreter \*/ $\text{一} ^ { \text{一} }$ Restore previous GIL state and return \*/ PyGILState_Release(state);

每一个 PyGILState_Ensure()调用都必须有一个配对的 PyGILState_Release()。

# 15.8.3 讨论

在涉及 C 和 Python 的高级应用中，让许多事情同时运行的情况并非不常见——很可能会涉及 C 代码、Python 代码、C 线程以及 Python 线程。只要保证 Python 解释器经过恰当的初始化且涉及解释器相关的C代码能够管理好GIL，那么就可以正常工作。

请注意 PyGILState_Ensure()并不会立刻抢占或中断解释器。如果其他代码正在执行中，这个函数将阻塞直到其他代码决定释放 GIL 为止。在内部，Python 解释器会周期性地切换线程，因此即使另一个线程正在执行，调用方最终依然会得到运行（尽管可能先要等待一会儿）。

# 15.9 用 Swig 来包装 C 代码

# 15.9.1 问题

我们想将已有的 C 代码作为 C 扩展模块来访问。我们想通过 Swig（http://www.swig.org）

来实现这一目标。

# 15.9.2 解决方案

Swig 可以解析 C头文件并自动创建出扩展代码来。要使用这个工具，首先需要有一个C头文件。例如，下面这个头文件就可用于我们的示例代码：

```c
/* sample.h */
#include <math.h>
extern int gcd(int, int);
extern int in_mandel(double x0, double y0, int n);
extern int divide(int a, int b, int *remainder);
extern double avg(double *a, int n);
typedef struct Point {
    double x, y;
} Point;
extern double distance(Point *p1, Point *p2); 
```

一旦有了头文件，下一步就是编写一个 Swig“接口”文件。根据约定，这些接口文件都以.i作为后缀，看起来类似于这样：

```c
// sample.i - Swig interface  
%module sample  
%{  
#include "sample.h"  
%}  
/* Customizations */  
%extend Point {  
/* Constructor for Point objects */  
Point(double x, double y) {  
    Point *p = (Point *) malloc(sizeof(Point));  
    p->x = x;  
    p->y = y;  
    return p;  
};  
};  
/* Map int *remainder as an output argument */  
%include typedefs.i  
%apply int *OUTPUT { int * remainder };  
/* Map the argument pattern (double *a, int n) to arrays */  
%typedef (in) (double *a, int n) (Py_buffer view) { 
```

```c
viewobj \(\equiv\) NULL; if (PyObject_GetBuffer(\$input, &view, PyBUF_ANY_CONTIGUOUS | PyBUF_FORMAT) \(= = -1\) { SWIG_fail; } if (strcmp_view.format,"d") != 0) { PyErr_SetString(PyExc_TYPEError, "Expected an array of doubles"); SWIG_fail; } \\(1 = (double *) view(buf; \\)2 = view.len / sizeof(double);   
}   
%typemap(freearg) (double *a, int n) { if (view\$argnum obj) { PyBuffer_Release(&view\$argnum); }   
}   
/\*C declarations to be included in the extension module \*/   
extern int gcd(int, int);   
extern int in_mandel(double x0,double y0,int n);   
extern int divide(int a,int b,int \*remainder);   
extern double avg(double \)*a$, int n);   
typedef struct Point { double x,y; } Point;   
extern double distance(Point \*p1, Point \*p2); 
```

一旦编写好了这个接口文件，Swig 就可以作为命令行工具在终端中调用了：

```batch
bash % swig -python -py3 sample.i  
bash % 
```

Swig 会产生两个文件：sample wrap.c 和 sample.py。后者是用户用来导入的。samplewrap.c 文件是 C 程序源代码，需要将其编译到一个支撑模块_sample 中。这和普通的扩展模块所采用的技术一样。例如，要像这样创建一个 setup.py 文件：

setup.py   
from distutils.core import setup, Extension   
setup(name $=$ 'sample' py Modules $\coloneqq$ ['sample.py'], ext Modules $=$ [ Extension'_sample',

['sampleWrap.c'], include_dirs $=$ [], define/macros $= \left[\right]$ undef/macros $= \left[\right]$ , library_dirs $= \left[\right]$ libraries $= [\text{'}$ sample'] 1

要编译和测试，只要针对 setup.py 文件运行 python3 即可：

```batch
bash % python3 setup.py build_ext --inplace  
running build_ext  
building 'sample' extension  
gcc -fno-strict-aliasing -DNDEBUGUG -g -fwrapv -O3 -Wall -Wstrict-prototypes  
-I/usr/local/include/python3.3m -c sampleWrap.c  
-o build/temp.macsx-10.6-x86_64-3.3/sampleWrap.o  
sampleWrap.c: In function 'SWIG_IitializeModule':  
sampleWrap.c:3589: warning: statement with no effect  
gcc -bundle -undefined dynamic.lookup build/temp.macsx-10.6-x86_64-3.3/sample.o  
build/temp.macsx-10.6-x86_64-3.3/sampleWrap.o -o_sample.so -lsample  
bash % 
```

如果一切顺利，会发现现在能够直接使用得到的 C 扩展模块了，示例如下：

```diff
>>> import sample
>>> sample.gcd(42,8)
2
>>> sampledivide(42,8)
[5,2]
>>> p1 = sample.Point(2,3)
>>> p2 = sample.Point(4,5)
>>> sampledistance(p1,p2)
2.8284271247461903
>>> p1.x
2.0
>>> p1.y
3.0
>>> import array
>>> a = array.array('d',[1,2,3])
>>> sample AVG(a)
2.0
>>> 
```

# 15.9.3 讨论

Swig 是用来构建扩展模块的最古老的工具之一，时间要追溯到 Python 1.4 时期。但是，

目前的版本已经开始对 Python 3 提供支持了。Swig 的主要用途是使用 Python 作为高层控制语言来访问已有的大型 C 代码库。例如，用户的 C 代码中可能包含了上千个函数以及各式各样的数据结构，而用户希望通过 Python 来访问它们。大部分生成包装函数的过程都可以用 Swig 来自动化进行。

所有的 Swig 接口都有着如下的简短开头：

```lisp
%module sample  
%{  
#include "sample.h"  
%} 
```

这仅仅只是声明了扩展模块的名称，而且指定了必须要包含在内才能让所有组件通过编译的 C 头文件（包在 $\%$ {和 $\%$ }之间的代码会直接粘贴到输出的代码文件中，因此为了能编译通过，要将所有需要包含的头文件和其他的定义都放置在这里）。

Swig 接口文件的底部是一些想包含在扩展模块中的C声明式。这些通常可以直接从头文件中拷贝过来。在我们的示例中，我们是直接从头文件中粘贴过来的：

```lisp
%module sample   
%{   
#include"sample.h"   
%}   
...   
extern int gcd(int, int);   
extern int in_mandel(double x0,double y0,int n);   
extern int divide(int a,int b,int \*remainder);   
extern double avg(double \*a,int n);   
typedef struct Point { double x,y;   
} Point;   
external double distance(Point \*p1,Point \*p2); 
```

要重点强调的是这些声明式就是在告诉 Swig 你希望包含在 Python 模块中的内容。对声明列表做适当的修改和编辑是很常见的。比如，如果不想包含某些特定的声明式，可以从声明列表中将它们移除。

使用 Swig 最复杂的部分在于它可以对 C 代码做各种各样的定制化处理。这是个庞大的主题不可能在这里涵盖所有细节，但是本节中也展示了几个这样的定制化处理。

第一个定制化处理涉及%extend 指令，它允许将方法关联到已有的结构体和类定义中。在示例中，我们使用这个技术给 Point结构体添加了一个构造函数。这个定制化处理使

得像这样使用结构体成为可能：

```prolog
>>> p1 = sample.Point(2,3)  
>>> 
```

如果忽略这一步，那么 Point对象就需要以更加复杂的方式来创建了：

```txt
>>> # Usage if %extend Point is omitted  
>>> p1 = sample.Point()  
>>> p1.x = 2.0  
>>> p1.y = 3 
```

第二个定制化处理涉及包含 typemaps.i 库，以及对%apply 指令的使用。%apply 指令告诉 Swig 参数签名 int *remainder 应该被看做是一个输出值。这实际上是一个模式匹配规则。在后面所有的声明式中，只要遇到了 int *remainder，那么就把它当做输出处理。这个定制化处理使得 divide()函数可以返回两个值：

```txt
>>> sampledivide(42,8) [5,2]   
>>> 
```

最后一个定制化处理涉及对%typemap 指令的使用，这也许是本节所展示的最高级的功能了。typemap 就是一种规则，可作用于输入中特定的参数模式上。在本节中，我们已经编写了一个 typemap 来匹配形式为(double *a, int n)这样的参数模式。在 typemap 内部是一段 C 代码，用来告诉 Swig 如何去把一个 Python 对象转换为相关的 C 参数。本节给出的代码中使用了 Python 中的 buffer 协议，用来对任何看起来像是 double 数组的输入参数做匹配（即，NumPy 数组、由 array 模块创建的数组等）。对数组的操作可参见15.3 节。

在 typemap 代码中，类似像 $\$ 1$ 和 $\$ 2$ 这样的替换符用来代表变量，这些变量保存着在typemap 模式中经过转换的 C 参数的值（例如， $\$ 1$ 会映射为 double $\ast _ { \mathrm { a } }$ ，而 $\$ 2$ 会映射为 int n）。$input 表示一个 PyObject *参数，它作为输入参数。 $\$ 1$ argument 则表示参数的个数。

编写和理解 typemap 常常成为程序员使用 Swig 的最大障碍。不仅因为这种代码相当隐晦，而且需要我们同时对 Python C API以及 Swig 与它们交互的方式的复杂细节有着很好的理解。Swig 的文档中有更多的示例和详细的信息。

然而，如果有许多 C 代码需要以扩展模块的方式暴露给 Python，则 Swig 可以成为一件非常得力的工具。需要牢记的关键点就是 Swig 基本上就是一个用来处理 C 语言声明的编译器，但是还有着强大的模式匹配以及定制化组件，能让我们对特定的声明和类型的处理方式做出改变。更多信息可在 Swig 的网站（http://www.swig.org）以及特定于Python 的文档（http://www.swig.org/Doc2.0/Python.html）中找到。

# 15.10 用 Cython 来包装 C 代码

# 15.10.1 问题

我们想用 Cython 来创建一个 Python 扩展模块，用来包装一个已有的 C 库。

# 15.10.2 解决方案

从某种程度上来看，用 Cython 创建一个扩展模块和手动编写扩展模块有些相似。它们都需要我们创建一组包装函数。但是与前几节不同的是，我们不必再用 C 来完成这些事情了——现在使用的代码看起来非常像 Python。

提前说明，假设本章介绍部分给出的示例代码已经被编译为 C 库，名称为 libsample。我们首先创建一个名为 csample.pxd 的文件，它看起来是这样的：

```python
# csample.pxd
# 
# Declarations of "external" C functions and structures
cdef extern from "sample.h": 
    int gcd(int, int) 
    hint in_mandel(double, double, int) 
    int divide(int, int, int *) 
    double avg(double *, int) nogil
    typedef struct Point: 
        double x 
        double y 
    double distance(Point *, Point*) 
```

这个文件在 Cython 中的目的和作用就相当于一个 C头文件。文件中最开始的声明 cdefextern from "sample.h"声明了所需的 C 头文件。后面跟着的声明都是取自那个 C 头文件中。这个文件的名称是 csample.pxd，不是 sample.pxd——这一点很重要。

接下来，创建一个名为 sample.pyx 的文件。这个文件将定义包装函数，作为 Python 解释器到 csample.pxd 文件中定义的底层 C 代码之间的桥梁：

```python
sample.pyx   
# Import the low-level C declarations   
cimport csample   
# Import some functionality from Python and the C stdlib 
```

from cpython.pycapsule cimport * from libc.stdlib cimport malloc, free   
```txt
Wrappers def gcd(unsigned int x, unsigned int y): return csample.gcd(x, y) 
```

```txt
def in_mandel(x, y, unsigned int n): return csample.in_mandel(x, y, n) 
```

```python
def divide(x, y):
    cdef int rem
        quot = csample.divide(x, y, &rem)
        return quot, rem 
```

```txt
def avg(double[:] a):  
    cdef:  
        int sz  
        double result 
```

```python
sz = a.size  
with nogil:  
    result = csample(avg(&a[0], sz)  
return result 
```

```txt
Destructor for cleaning up Point objects  
cdef del_Point(object obj):  
    pt = <csample.Point *> PyCapsule_GetPointer(obj, "Point")  
    free(<void *> pt) 
```

Create a Point object and return as a capsule   
def Point(double x,double y): cdef csample.Point \*p p $=$ <csample.Point $\ast >$ malloc(sizeof(csample.Point)) if $\mathbb{P} = =$ NULL: raise MemoryError("No memory to make a Point") p.x $=$ x p.y $=$ y return PyCapsule_New(<void $\ast >$ p,"Point",<PyCapsule_Destructor>del_Point)

```python
def distance(p1, p2):
    pt1 = <csample.Point <> PyCapsule_GetPointer(p1, "Point")
    pt2 = <csample.Point <> PyCapsule_GetPointer(p2, "Point")
    return csample(distance(pt1, pt2) 
```

有关这个文件的各种细节将在讨论部分做进一步的说明。最后，要构建出扩展模块，需要创建一个 setup.py文件，看起来是这样的：

from distutils.core import setup   
from distutilsextension import Extension   
from Cython.Distutils import build_ext   
ext Modules $=$ [ Extension('sample', ['sample.pyx'], libraries $\coloneqq$ ['sample'], librarydirs $\coloneqq$ ['.'])]   
setup( name $=$ 'Sample extension module', cmdclass $=$ {'build_ext': build_ext}, ext Modules $=$ ext Modules

要构建出最后的结果用于实验，在终端中键入如下命令：

```batch
bash % python3 setup.py build_ext --inplace  
running build_ext  
cythoning sample.pyx to sample.c  
building 'sample' extension  
gcc -fno-strict-aliasing -DNDEBUGUG -g -fwrapv -O3 -Wall -Wstrict-prototypes  
-I/usr/local/include/python3.3m -c sample.c  
-o build/temp.macsx-10.6-x86_64-3.3/sample.o  
gcc -bundle -undefined dynamic.lookup build/temp.macsx-10.6-x86_64-3.3/sample.o  
-L. -lsample -o sample.so  
bash % 
```

如果一切顺利，现在应该有一个名为 sample.so 的扩展模块了，可以像下列示例中那样使用：

```python
>>> import sample
>>> sample.gcd(42, 10)
2
>>> sample.in_mandel(1, 1, 400)
False
>>> sample.in_mandel(0, 0, 400)
True
>>> sampledivide(42, 10)
(4, 2)
>>> import array
>>> a = array.array('d', [1, 2, 3])
>>> sample AVG(a) 
```

```txt
2.0  
>>> p1 = sample.Point(2,3)  
>>> p2 = sample.Point(4,5)  
>>> p1  
<caption object "Point" at 0x1005d1e70>  
>>> p2  
<caption object "Point" at 0x1005d1ea0>  
>>> sampledistance(p1,p2)  
2.8284271247461903  
>>> 
```

# 15.10.3 讨论

本节包含了一些在前面章节中讨论过的高级特性，这包括操作数组、包装不透明指针以及释放 GIL。这些部分在本节中会依次进行讨论，但我们先回顾一下前面的章节。

从高层来看，Cython 是用来模仿 C 的。.pxd 文件仅仅只是包含了 C 中的声明（和 C 中的. $\mathbf { \nabla } _ { . } h$ 文件类似），而.pyx 文件包含了实现（类似于 $^ { . c }$ 文件）。Cython 中的 cimport 语句用来从.pxd 文件中导入声明。这和使用普通的 Python import 语句有所不同，在 Python 中这会加载一个正常的 Python 模块。

尽管.pxd 文件包含了声明，但它们并不是用来自动产生扩展代码的。因此，我们仍然必须编写简单的包装函数。例如，尽管 csample.pxd 文件声明了函数 int gcd(int, int)，我们仍然需要在 sample.pyx 中编写一个包装函数。示例如下：

```python
cimport csample
def gcd(unsigned int x, unsigned int y):
    return csample gcd(x, y) 
```

对于简单的函数，要做的事情并不会太多。Cython 会产生包装代码对参数和返回值做适当的转换。关联到参数上的 C 数据类型是可选的。但是，如果包含了它们，不用花任何代价就能获得额外的错误检查。例如，如果有人用负数做参数来调用这个函数，那么就会产生一个异常：

```txt
>>> sample.gcd(-10,2)
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "sample.pyx", line 7, in sample.gcd (sample.c:1284)
    def gcd(unsigned int x, unsigned int y):
        OverflowError: can't convert negative value to unsigned int
>>> 
```

如果想给包装函数添加额外的检查机制，只要再增加包装的代码即可。示例如下：

```python
def gcd(unsigned int x, unsigned int y):
    if x <= 0:
        raise ValueError("x must be > 0")
    if y <= 0:
        raise ValueError("y must be > 0")
    return csample gcd(x, y) 
```

在 csample.pxd 文件中声明的 in_mandel()有一个有趣但不太明显的地方。在那个文件中，函数被声明为返回一个 bint 而不是 int。这使得函数会创建一个合适的布尔值而不是一个简单的整型值。因此，返回值 0 会被映射为 False，而 1 会对应 True。

在 Cython 包装函数内，除了可以使用所有普通的 Python 对象外，还可以选择声明 C数据类型。divide()的包装函数就展示了这样一个用法，也演示了如何处理指针参数。

```python
def divide(x,y):
    cdef int rem
        quot = csample.divide(x,y,&rem)
        return quot, rem 
```

这里，变量 rem 被显式声明为 C 语言中的 int 变量。当传递给下面的 divide()函数时，&rem 就表示指向 rem 的指针，这和 C 语言中的表达方式一致。

avg()函数的代码则说明了 Cython 中一些更加高级的功能。首先，声明式 def avg(double[:]a)将 avg()声明为接受一个一维 double 值的内存视图（memoryview）。令人惊讶的地方在于这使得 avg 函数将接受任意一种兼容的数组对象，包括那些由 numpy 库所创建的数组也是如此。示例如下：

```python
>>> import array
>>> a = array.array('d', [1, 2, 3])
>>> import numpy
>>> b = numpy.array([1., 2., 3.])
>>> import sample
>>> sample_avg(a)
2.0
>>> sample_avg(b)
2.0 
```

在包装函数中，a.size 和&a[0]分别表示数组元素的个数以及指向数组的指针。语法<double $^ * >$ &a[0]可以让我们在必要的时候对指针类型进行转换。需要保证 C 中的 avg()函数接受的是类型正确的指针。下一节中会介绍一些有关 Cython 中内存视图的高级用法。

除了同常规数组打交道外，avg()的示例也展示了如何同全局解释器锁（GIL）打交道。语句 with nogil:声明了一个代码块，表示执行时不持有 GIL。在语句块内部，使用任何

普通的Python对象都是非法的——只有声明为cdef的对象和函数可以使用。除此之外，外部函数必须显式声明为可以不持有 GIL 执行。因此，在 csample.pxd 文件中函数 avg()被声明为 double avg(double *, int) nogil。

对结构体Point的处理则带来了特殊的挑战。如前文所示，本节使用capsule对象将Point对象当做不透明指针来处理，这部分内容在 15.4 节中已有说明。但是，要做到这点，底层的 Cython 代码就要更加复杂一点。首先，下面这些导入语句是用来从 C 库和 PythonC API 中引入函数定义：

```python
from cpython.pycapsule cimport \*   
from libc.stdlib cimport malloc, free 
```

函数 del_Point()和 Point()使用导入的函数来创建一个 capsule 对象用来包装 Point *指针。声明式 cdef del_Point()把 del_Point()声明为一个只能从 Cython 而不是 Python 中访问的函数。因而，这个函数对外部是不可见的——相反，它作为回调函数来清理由 capsule对象占用的内存空间。对 PyCapsule_New()和 PyCapsule_GetPointer()的调用是直接从Python C API 中得来的，使用方法一样。

distance()函数会从由 Point()中创建出的 capsule 对象里提取出指针。这里值得注意的是我们不必担心异常处理的问题。如果传递了一个不正确的对象，PyCapsule_GetPointer()会产生一个异常，但是 Cython 会去查找异常并将其传播到 distance()外部。

在这个解决方案中，对 Point 结构体的处理缺点在于这完全是不透明的。我们无法查看或访问任何属性。下面还有一个替代方案，就是定义一个扩展类型，示例如下：

```python
# sample.pyx
cimport csample
from libc.stdlib cimport malloc, free
...
cdef class Point:
    cdef csample.Point *_c_point
    def _cinit__(self, double x, double y):
        self._c_point = <cSample.Point *} malloc(sizeof(csample.Point))
        self._c_point.x = x
        self._c_point.y = y
    def _dealloc__(self):
        free(self._c_point)
    property x:
        def _get__(self):
            return self._c_point.x
        def _set__(self, value): 
```

```python
self._c_point.x = value
property y:
    def __get__(self):
        return self._c_point.y
    def __set__(self, value):
        self._c_point.y = value
def distance(Point p1, Point p2):
    return csample_distance(p1._c_point, p2._c_point) 
```

这里，cdef class Point 将 Point 声明为一个扩展类型。类变量 cdef csample.Point *_c_point用来保存指向底层 C 代码中 Point 结构体的指针。__cinit__()和__dealloc__()方法使用malloc()和 free()调用来创建和销毁底层的 C 结构体。property x 和 property y 声明让代码可以对底层的结构体属性进行 get 和 set 操作。distance()的包装函数也被适当地修改为接受 Point扩展类型实例作为参数，但是会传递底层的指针给 C函数。

做了这样的修改，会发现现在用来操作 Point 对象的代码会更加自然些：

```txt
>>> import sample
>>> p1 = sample.Point(2,3)
>>> p2 = sample.Point(4,5)
>>> p1
<sample.Point object at 0x100447288>
>>> p2
<sample.Point object at 0x1004472a0>
>>> p1.x
2.0
>>> p1.y
3.0
>>> sampledistance(p1,p2)
2.8284271247461903
>>> 
```

本节介绍了许多 Cython 的核心功能，我们可以将它们应用到情况更加复杂的包装处理上。可以肯定的是，要想实现更多功能，一定要去看看官方文档（http://docs.cython.org）。

接下来的几节同样会介绍一些额外的 Cython 功能。

# 15.11 用 Cython 来高效操作数组

# 15.11.1 问题

我们想编写一些用来处理数组的高性能函数，把它们作用在类似 NumPy 这样的库创建的

数组上。我们听说像 Cython 这样的工具会让这个过程变得简单，但是不确定该如何去做。

# 15.11.2 解决方案

作为示例，下面的代码展示了一个用来对一维 double 数组元素进行修改的函数：

sample.pyx (Cython)   
cimport cython   
@cython.boundscheck(False)   
@cython wrapsround(False)   
cpdef clip(double[:] a, double min, double max, double[:] out): 1 Clip the values in a to be between min and max. Result in out 1 if min $>$ max: raise ValueError("min must be $\leq$ max") if a.shape[0] != out.shape[0]: raise ValueError("input and output arrays must be the same size") for i in range(a.shape[0]): if a[i] $<$ min: out[i] $=$ min elif a[i] $>$ max: out[i] $=$ max else: out[i] $=$ a[i]

要编译并构建这个扩展函数，需要一个下面这样的 setup.py 文件（使用命令 python3setup.py build_ext --inplace 来构建）：

from distutils.core import setup   
from distutilsextension import Extension   
from Cython.Distutils import build_ext   
ext Modules $=$ [ Extension('sample', ['sample.pyx'])   
]   
setup( name $=$ 'Sample app', cmdclass $=$ {'build_ext': build_ext}, ext Modules $=$ ext Modules

就会发现得到的函数可以对数组元素进行修改，而且可以用于许多不同类型的数组对象。例如：

```txt
>>> # array module example  
>>> import sample  
>>> import array  
>>> a = array.array('d', [1, -3, 4, 7, 2, 0])  
>>> a  
array('d', [1.0, -3.0, 4.0, 7.0, 2.0, 0.0])  
>>> sampleclip(a, 1, 4, a)  
>>> a  
array('d', [1.0, 1.0, 4.0, 4.0, 2.0, 1.0])  
>>> # numpy example  
>>> import numpy  
>>> b = numpy.random.uniform(-10, 10, size=1000000)  
>>> b  
array([-9.55546017, 7.45599334, 0.69248932, ..., 0.69583148, -3.86290931, 2.37266888])  
>>> c = numpy.zeros_like(b)  
>>> c  
array([0., 0., 0., ..., 0., 0., 0.])  
>>> sample clip(b, -5, 5, c)  
>>> c  
array([-5., 5., 0.69248932, ..., 0.69583148, -3.86290931, 2.37266888])  
>>> min(c)  
-5.0  
>>> max(c)  
5.0  
>>> 
```

同样，我们也会发现得到的代码运行起来很快。在下面的交互式会话中，我们用这个实现同 numpy库中已有的 clip()函数进行了一场针锋相对的比拼：

```txt
>>> timeit('numpyclip(b,-5,5,c)', 'from __main__ import b,c,numpy', number=1000)
8.093049556000551
>>> timeit('sample clip(b,-5,5,c)', 'from __main__ import b,c,sample', 
...             number=1000)
3.760528204000366
>>> 
```

可以看到，我们的实现要快上许多——考虑到 NumPy 的版本其核心是用 C 语言编写的，得出这样的结果真是有趣。

# 15.11.3 讨论

本节利用了 Cython 中的类型化内存视图（typed memoryview），它极大地简化了操作于数组的代码。语句 cpdef clip()将 clip()同时声明为 C 和 Python 函数。在 Cython 中这么做是很有用的，因为这意味着该函数在其他 Cython 函数中调用起来要更有效率（例如，如果想从另一个不同的 Cython 函数中调用 clip()）。

类型化参数 double[:] a 以及 double[:] out 将这些参数声明为一维 double 数组。作为输入，它们可以访问任何实现了内存视图接口的数组对象（有关内存视图接口，可参见 PEP3118）。这包括 NumPy 以及内建的 array 库中的数组。

如果编写的代码产生的结果同样是数组，我们应该遵循示例代码中所展示的惯例，即，让一个参数成为输出参数。这就把创建输出数组的责任留给了调用者，而实现者则不必了解太多有关数组是什么类型的具体细节（这里只假设数组已经准备就位，只需要做一些基本的检查，例如确保它们的大小是兼容的）。在 NumPy 这样的库中，使用numpy.zeros()或者 numpy.zeros_like()来创建输出数组相对来说是很容易的。或者，要创建未初始化的数组，可以使用 numpy.empty()或者 numpy.empty_like()。如果打算用结果来覆盖数组内容，这么做会稍微更快些。

在函数的实现中，只需要利用索引（例如 a[i]、out[i]等）编写简单直接的代码来处理数组即可。Cython 会采取措施确保产生高效的代码。

位于 clip()定义之前的那两个装饰器是用来做性能优化的可选项。@cython.boundscheck(False)会消除所有的数组边界检查，如果已经知道索引不会越界，那么就可以使用它。$@$ cython.wraparound(False)在包装整个数组时会消除针对下标为负数时的处理。包含了这些装饰器能让代码的运行速度显著提高（对于这个示例，我们测试的结果是会快大约 2.5 倍）。

每当同数组打交道时，对底层的算法做仔细的研究和试验同样也能获得巨大的速度提升。例如，考虑下面这个 clip()函数的变种，这里使用了条件表达式：

```txt
@cython.boundscheck(False)   
@cython wrapsround(False)   
cpdef clip(double[:] a, double min, double max, double[:] out): if min > max: raise ValueError("min must be <= max") if a.shape[0] != out.shape[0]: raise ValueError("input and output arrays must be the same size") for i in range(a.shape[0]): out[i] = (a[i] if a[i] < max else max) if a[i] > min else min 
```

测试的时候，这个版本的代码运行速度又要快 $50 \%$ 多（在 timeit()测试中，成绩是 2.44秒对比之前的 3.76 秒）。

此时，我们可能想知道为什么这份代码在执行效率上会力压手写的C语言版本。例如，也许我们编写了如下的 C 函数，然后利用前面几节中谈到的技术手工编写了一个 C 语言扩展版本：

void clip(double \*a,int n,double min,double max,double \*out){ double x; for(；n $> = 0$ ；n--，a++，out++){ $\mathrm{x} = \mathrm{*a};$ \*out $=$ x $>$ max?max:(x<min?min:x); }   
1

这里并没有给出扩展代码。但是经过测试后，我们发现手工打造的 C 扩展代码却比由Cython 创建出的扩展函数要慢 $10 \%$ 。底线就是，Cython 生成的代码运行速度比想象的还要快很多。

解决方案中给出的代码还可以做几个扩展。针对特定类型的数组操作，释放 GIL 使得多个线程能够并行运行是很有意义的。为了做到这点，修改代码使其包含 with nogil:语句：

```txt
@cython.boundscheck(False)   
@cython wrapsround(False)   
cpdef clip(double[:] a, double min, double max, double[:] out): if min > max: raise ValueError("min must be <= max") if a.shape[0] != out.shape[0]: raise ValueError("input and output arrays must be the same size") with nogil: for i in range(a.shape[0]): out[i] = (a[i] if a[i] < max else max) if a[i] > min else min 
```

如果想编写一个能操作二维数组的版本，下面是一种解决方案：

@cython.boundscheck(False)   
@cython wrapsround(False)   
cpdef clip2d(double[:,] a,double min,double max,double[:,] out): ifmin $>$ max: raise ValueError("min must be $\text{一} =$ max") for n in range(a.ndim): ifa.shape[n] $! =$ out.shape[n]: raiseTypeError("a and out have different shapes") for i in range(a.shape[0]): for j in range(a.shape[1]): if $a[i,j] <   \min$ .. out[i,j] $= \mathrm{min}$

elif $a[i,j] > \max$ .. out[i,j] = max else: out[i,j] = a[i,j]

希望读者看到这里时不会感到迷惑。本节所有给出的代码都不会特定于任何一种数组库（例如 NumPy）。这使得代码有着非常好的灵活性。但是同样值得注意的是，一旦牵涉到多维数组、步进、偏移以及其他的因素，那么对数组的处理就会变得更加复杂。这些主题超出了本节的范围，但是更多信息可以在 PEP 3118（http://www.python.org/dev/peps/pep-3118）中找到。Cython 文档中关于类型化内存视图（http://docs.cython.org/src/userguide/memoryviews.html）的章节也同样值得阅读。

# 15.12 把函数指针转换为可调用对象

# 15.12.1 问题

我们已经获取了某个C函数的内存地址，但是想将其转换成一个Python的可调用对象，这样我们可以把它当做扩展函数使用。

# 15.12.2 解决方案

ctypes 模块可用来创建包装任意内存地址的可调用对象。下面的示例展示了如何获取一个 C 函数的底层原始地址，以及如何将其转换成一个可调用对象：

```txt
>>> import types
>>> lib = types.cdl.LoadLibrary(None)
>>> # Get the address of sin() from the C math library
>>> addr = types.cast.lib.sin, types.c_void_p).value
>>> addr
140735505915760
>>> # Turn the address into a callable function
>>> functype = types.CFUNCTYPE(otypes.c_double, types.c_double)
>>> func = functype(addr)
>>> func
<CFunctionType object at 0x1006816d0>
>>> # Call the resulting function
>>> func(2)
0.9092974268256817
>>> func(0)
0.0
>>> 
```

# 15.12.3 讨论

要创建一个可调用对象，必须首先创建一个 CFUNCTYPE 实例。CFUNCTYPE()的第一个参数是返回类型，接下来的参数就是原始函数的参数类型。一旦定义了函数类型，就用它来包装一个整数内存地址以此创建出一个可调用对象。得到的结果对象就可以通过 ctypes 像任何普通函数一样进行访问了。

本节讨论的内容可能看起来相当隐晦也非常底层。然而，对于程序和库来说，利用像LLVM 库中使用的即时编译（just in-time compilation）这类高级代码生成技术也变得越来越普遍了。

例如，下面这样一个简单的示例使用 llvmpy 扩展（http://www.llvmpy.org）来创建一个小的汇编函数，获取一个指向该函数的指针，然后将其转换为一个 Python 可调用对象：

```python
>>> from llvm.core import Module, Function, Type, Builder
>>> mod = Module.new('example')
>>> f = Function.new(mod, Type.function(Type,double(), [\text{Type(double(), Type,double()}, False), 'foo'])
>>> block = f.append_basic_block('entry')
>>> builder = Builder.new(block)
>>> x2 = builder.fmul(f.args[0], f.args[0])
>>> y2 = builder.fmul(f.args[1], f.args[1])
>>> r = builder.fadd(x2, y2)
>>> builder(ret(r))
<llvm.coreinstruction object at 0x10078e990>
>>> from llvm.ee import ExecutionEngine
>>> engine = ExecutionEngine.new(mod)
>>> ptr = engine.get_pointer_to_function(f)
>>> ptr
4325863440
>>> foo = ctypes.CFUNCTYPE(ctypes.c-double, ctypes.c-double, ctypes.c-double) (ptr)
>>> # Call the resulting function
>>> foo(2, 3)
13.0
>>> foo(4, 5)
41.0
>>> foo(1, 2)
5.0
>>> 
```

不用多说，如果在这个层次上出任何差错的话将导致 Python 解释器“死于非命”。请记住，我们正在直接同机器的内存地址和原生机器码打交道，而不是 Python 函数。

# 15.13 把以 NULL 结尾的字符串传给 C 库

# 15.13.1 问题

我们正在编写的扩展模块需要把以 NULL 结尾的字符串传给 C 库。但是，我们并不能完全确定如何通过 Python 的 Unicode 字符串实现来做到这点。

# 15.13.2 解决方案

许多 C 库中包含的函数都把以 NULL 结尾的字符串类型声明为 char *。考虑如下的 C函数，我们将用它作为说明和测试的例子：

void print_chars(char \*s) { while \*s){ printf("%2x", (unsigned char) \*s); $s + +$ 1 printf("\n");   
}

这个函数只是简单地打印出每个字符的十六进制表示，这样可以方便地对传入的字符串进行调试。例如：

```txt
print chars("Hello"); // Outputs: 48 65 6c 6c 6f 
```

要从 Python 中调用这样一个 C 函数，有好几种选择。第一，可以使用转换代码“y”限制函数 PyArg_ParseTuple()只操作字节，示例如下：

```c
static PyObject \*py_print chars(PyObject \*self, PyObject \*args) { char \*s; if(!PyArg_ParseTuple(args，"y"，&s)) { return NULL; } print chars(s); Py_RETURN_NONE; 
```

得到的结果函数可以像下面这样进行操作。仔细观察嵌入了 NULL 字节的字节流是如何处理的，以及传入 Unicode 字符串时会被拒绝执行：

```txt
>>> print_chars(b'Hello World')  
48 65 6c 6c 6f 20 57 6f 72 6c 64  
>>> print_chars(b'Hello\x00World')  
Traceback (most recent call last): 
```

```txt
File "<stdin>", line 1, in <module>  
TypeError: must be bytes without null bytes, not bytes  
>>> print_chars('Hello World')  
Traceback (most recent call last):  
File "<stdin>", line 1, in <module>  
TypeError: 'str' does not support the buffer interface  
>>> 
```

如果想传入 Unicode 字符串，可以使用格式化代码“s”，示例如下：

```c
static PyObject \*py_print chars(PyObject \*self, PyObject \*args) { char \*s; if(!PyArg_ParseTuple(args，"s"，&s)) { return NULL; } print chars(s); Py_RETURN_NONE; 
```

当使用上面的函数时，会自动将所有的字符串转换为以 NULL 结尾且以 UTF-8 编码的形式。示例如下：

```txt
>>> print_chars('Hello World')
48 65 6c 6c 6f 20 57 6f 72 6c 64
>>> print_chars('Spicy Jalape\u00f1o') # Note: UTF-8 encoding
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> print_chars('Hello\x00World')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: must be str without null characters, not str
>>> print_chars(b'Hello World')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
TypeError: must be str, not bytes
>>> 
```

如果基于某些原因，我们需要直接同 PyObject *打交道而不能使用 PyArg_ParseTuple()，下面的代码示例告诉我们如何从字节流和字符串对象中检查并提取出一个合适的 char *引用：

/\* Some Python Object (obtained somehow) \*/   
PyObject \*obj;   
\*/ Conversion from bytes \*/   
{ char \*s; s $=$ PyBytesAsString(o); if (!s){ return NULL; /\*TypeError already raised \*/

}   
print chars(s);   
}   
/\* Conversion to UTF-8 bytes from a string \*/   
{ PyObject \*bytes; char \*s; if (!PyUnicode_Check(obj)) { PyErr_SetString(PyExc_TypeError，"Expected string"); return NULL; } bytes $=$ PyUnicode_AsUTF8String(obj); s $=$ PyBytes_Astring(bytes); print chars(s); Py_DECREF(bytes);

上面这两种转换都可保证接受以 NULL 结尾的数据，但是它们没有检查是否在字符串中间插入了NULL字节的情况。因此，如果这对我们而言很重要的话，那就需要自己做检查了。

# 15.13.3 讨论

如果可能的话，应该避免编写出依赖以NULL结尾的字符串的代码，因为 Python 对字符串并没有这样的要求。如果可能的话，处理字符串时使用指针并配合使用表示其大小的参数几乎总是更好的选择。然而，有时候我们不得不去处理遗留的 C 代码，那样的话就别无选择了。

尽管上述技术很容易使用，但在 PyArg_ParseTuple()中使用格式化代码“s”时会有一些内存方面的开销，而这是很容易被忽略的地方。当编写代码时，如果使用了上述转换规则，则会创建一个 UTF-8 编码的字符串，并且会永久将其关联到原始的字符串对象上。如果原始字符串中包含有非 ASCII 字符，那么就会使得字符串的大小增加，直到被垃圾收集机制处理为止。示例如下：

```txt
>>> import sys
>>> s = 'Spicy Jalape\u00f10'
>>> sys.getsizeof(s)
87
>>> print Characters(s) # Passing string
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> sys.getsizeof(s) # Notice increased size
103
>>> 
```

如果对这种内存使用的增加需要纳入考虑，应该使用 PyUnicode_AsUTF8String()函数来

重新编写 C 扩展代码，示例如下：

static PyObject \*py_print chars(PyObject \*self, PyObject \*args) { PyObject \*o, \*bytes; char \*s; if(!PyArg_ParseTuple(args，"U"，&o)) { return NULL; } bytes $=$ PyUnicode_AsUTF8String(o); s $=$ PyBytes_AsString(bytes); print chars(s); Py_DECBCF(bytes); Py_RETURN_NONE;   
}

按照上面这样修改，UTF-8 编码的字符串会根据需要进行创建，但是会在使用完毕之后丢弃。下面是修改过的版本产生的行为，可以看到字符串的大小并没有增加：

```diff
>>> import sys
>>> s = 'Spicy Jalape\u00f10'
>>> sys.getsizeof(s)
87
>>> print Characters(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> sys.getsizeof(s)
87
>>> 
```

如果打算把以 NULL 结尾的字符串传递给通过 ctypes 包装的函数，请注意 ctypes 只允许传入字节，而且不会检查传入的字节中是否有插入 NULL。示例如下：

```python
>>> import ctypes
>>> lib = ctypes.cdll.LoadLibrary("\\.libsample.so")
>>> print_chars = lib.print_chars
>>> print_chars.argtypes = (ctypes.c_char_p,)
>>> print_chars(b'Hello World')
48 65 6c 6c 6f 20 57 6f 72 6c 64
>>> print_chars(b'Hello\x00World')
48 65 6c 6c 6f
>>> print_chars('Hello World')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
ctypesArgumentError: argument 1: <class 'TypeError'>: wrong type
>>> 
```

如果想传入字符串而不是字节流，则需要手动先执行一次 UTF-8 编码。示例如下：

```prolog
>>> print_chars('Hello World'.encode('utf-8'))  
48 65 6c 6c 6f 20 57 6f 72 6c 64  
>>> 
```

对于其他的C扩展模块编写工具（例如Swig、Cython），是否要使用它们将字符串传递给 C 代码则需要做仔细的研究。

# 15.14 把 Unicode 字符串传递给 C 库

# 15.14.1 问题

我们正在编写的扩展模块需要把 Python字符串传递给一个 C语言编写的库函数，而这个库函数可能并不知道该如何恰当地处理 Unicode。

# 15.14.2 解决方案

这里需要考虑的问题比较多，但是主要问题在于已有的 C库并不能理解Python 原生的Unicode 字符串表示。因此，我们的挑战就是将 Python 字符串转换为让 C 库可以更容易理解的形式。

为了更好地说明这个问题，下面给出了两个 C 函数。出于调试和实验的目的，这两个函数操作字符串数据并产生输出。其中一个函数使用的是以 char *, int 形式给出的字节流，而另一个函数使用的是以 wchar_t *, int形式给出的宽字符。示例如下：

```c
void print_chars(char \*s, int len) { int n = 0; while (n < len) { printf("%2x ", (unsigned char) s[n]); n++; } printf("\n");   
}   
void print_wchars(wchar_t \*s, int len) { int n = 0; while (n < len) { printf("%x ", s[n]); n++; } printf("\n");   
} 
```

对于操作字节流的函数 print_chars()来说，需要将 Python 字符串转换成一个合适的字节编码，例如 UTF-8。示例如下：

```c
static PyObject \*py_print chars(PyObject \*self, PyObject \*args) { char \*s; Py_ssize_t len; if(!PyArg_ParseTuple(args，"s#",&s,&len)) { return NULL; } print chars(s，len); Py_RETURN_NONE;   
} 
```

对于可以处理机器上原生的 wchar_t类型的库函数来说，可以像这样编写扩展代码：

```c
static PyObject \*py_print_wchars(PyObject \*self, PyObject \*args) { wchar_t \*s; Py_ssize_t len; if(!PyArg_ParseTuple(args，"u#",&s,&len)) { return NULL; } print_wchars(s,len); Py_RETURN_NONE;   
} 
```

下面的交互式会话说明了这些函数是如何工作的：

>>>s $=$ 'Spicy Jalapeu00f1o'   
>>>print_chars(s)   
5370696379204a616c617065c3b16f   
>>>print_wchars(s)   
5370696379204a616c617065f16f   
>>>

请仔细观察基于字节流的函数 print_chars()是如何接受 UTF-8 编码的数据的，而print_wchars()是如何接受 Unicode 码点值的（Unicode code point value）。

# 15.14.3 讨论

在正式讨论前，应该先研究一下我们打算访问的 C 函数库有哪些本质特性。对于许多C库来说，传递字节流比传递字符串要显得更有意义些。要做到这点，可以使用下面的转换代码：

```c
static PyObject \*py_print chars(PyObject \*self, PyObject \*args) { char \*s; Py_ssize_t len; /\* accepts bytes,bytearray,or other byte-like object \*/ if(!PyArg_ParseTuple(args，"y#",&s,&len)) \{ return NULL; 
```

```c
} print_chars(s，len); Py_RETURN_NONE; } 
```

如果仍然决定传递字符串，我们需要了解到 Python 3 使用的字符串表示虽然适应性很强，但并不能全部直接映射到使用标准的 char *或 wchar_t *类型的 C 库中去，有关这方面的细节可参考 PEP 393（http://www.python.org/dev/peps/pep-0393）。因此，要把字符串数据传递给 C，那么几乎总是要做某种形式的转换才行。在 PyArg_ParseTuple()中使用格式化代码 s#和 u#可以安全地执行这样的转换。

可能存在的缺点就是这样的转换会导致原始字符串对象的大小永久性的增加。每当进行转换时，经过转换的数据会做一份拷贝关联到原始字符串对象上，这样稍后可以得到重用。可以通过下面的交互式会话来观察这个效果：

```txt
>>> import sys
>>> s = 'Spicy Jalape\u00f10'
>>> sys.getsizeof(s)
87
>>> print Characters(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> sys.getsizeof(s)
103
>>> print_Wchars(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 f1 6f
>>> sys.getsizeof(s)
163
>>> 
```

如果字符串数据总量很小，这种开销就无关紧要。但是如果需要在扩展模块中进行大量的文本处理，很可能就希望能够避免这种开销。下面对第一个扩展函数给出了另一种替代实现，这里就避免了这些内存开销：

static PyObject \*py_print chars(PyObject \*self, PyObject \*args) { PyObject \*obj, \*bytes; char \*s; Py_ssize_t len; if(!PyArg_ParseTuple(args，"U"，&obj)) { return NULL; } bytes $=$ PyUnicode_AsUTF8String(obj); PyBytes_AstringAndSize(bytes,&s,&len); print chars(s，len); Py_DECREF(bytes); Py_RETURN_NONE;   
}

要避免wchar_t处理的内存开销则更为棘手。在内部，Python 使用最有效的表示来存储字符串。例如，使用字节数组来存储仅包含 ASCII 的字符串，但是，如果字符串包含的字符范围是在 $\mathrm { U + 0 0 0 0 { \sim } U + F F F F }$ 之间，则需要使用两个字节来表示。由于不存在数据的单一表示，因此无法将内部数组转换为 wchar_t *，更别指望它会奏效。相反，必须创建一个 wchar_t 数组，然后将文本复制进来。PyArg_ParseTuple( )的“u#”格式代码会执行该操作，但是会以牺牲效率为代价（它将生成的副本附加到字符串对象）。

如果想避免这一长期的内存开销，唯一的选择是将 Unicode 数据复制到一个临时的数组，将其传递给 C库函数，然后释放该数组。下面是一个可能的实现：

```c
static PyObject \*py_print_wchars(PyObject \*self, PyObject \*args) { PyObject \*obj; wchar_t \*s; Py_ssize_t len; if(!PyArg_ParseTuple(args，"U"，&obj)) { return NULL; } if((s=PyUnicode_AsiWideCharString(obj,&len))==NULL){ return NULL; } print_wchars(s,len); PyMem_Free(s); Py_RETURN_NONE; } 
```

在这个实现中，PyUnicode_AsWideCharString()针对 wchar_t 字符创建了一个临时的缓冲区，并将数据拷贝到这里。这个缓冲区会在传递给 C 之后释放掉。在写作本小节时，关于这个行为似乎有一个 bug，可以在 Python 的问题页面（bugs.python.org/issue16254）中找到相关的描述。

如果出于某些原因知道 C库会以其他非 UTF-8 编码的形式来处理数据，我们可以强制Python 执行适当的转换，这可以通过下面的扩展函数来完成：

static PyObject \*py_print chars(PyObject \*self, PyObject \*args) { char $^{\ast}\mathrm{s} = 0$ int len; if(!PyArg_ParseTuple(args，"es#","encoding-name"，&s,&len)) { return NULL; } print chars(s，len); PyMem_Free(s); Py_RETURN_NONE;   
1

最后但同样重要的是，如果想直接同 Unicode 字符串中的字符打交道，下面给出的示例说明了其中的底层访问机制：

static PyObject \*py_print_wchars(PyObject \*self, PyObject \*args) { PyObject \*obj; int n, len; int kind; void \*data; if (!PyArg_ParseTuple(args, "U", &obj)) { return NULL; } if (PyUnicode_READY(obj) < 0) { return NULL; } len = PyUnicode_GET_LENGTH(obj); kind $=$ PyUnicode_KIND(obj); data $=$ PyUnicode_DATA(obj); for $(n = 0$ ; $n <   \mathrm{len}$ . $n + +$ ）{ Py_UCS4 ch $=$ PyUnicode_READ(kind, data, n); printf("%x ", ch); } printf("\n"); Py_RETURN_NONE;   
}

在上述代码中，宏 PyUnicode_KIND()和 PyUnicode_DATA()是同 Unicode 的宽度可变存储相关的，在 PEP 393中可找到相关描述。变量kind 对底层的存储信息（8位、16位或 32 位）进行编码，而变量 data 则指向缓冲区。事实上，只要把它们传递给 PyUnicode_READ()宏即可，不需要再做任何处理。

最后再多说几句，当从 Python 中把 Unicode 字符串传递给 C时，应该尽可能选择简单的方案。如果在 UTF-8 编码和宽字符中选择，那就选 UTF-8。提供对UTF-8的支持似乎更加普遍，麻烦也会少些，解释器对 UTF-8 的支持也更好。最后，请务必查看处理Unicode 的相关文档（https://docs.python.org/3/c-api/Unicode.html）。

# 15.15 把 C 字符串转换到 Python 中

# 15.15.1 问题

我们想把 C 字符串转换为 Python 字节流或者字符串对象。

# 15.15.2 解决方案

对于以char *, int形式表示的C字符串，我们必须决定是否要将它们以原始字节串或者Unicode 字符串的形式来表示。字节对象可以通过 Py_BuildValue()来构建，示例如下：

```c
char *s; /* Pointer to C string data */
int len; /* Length of data */
/* Make a bytes object */
PyObject *obj = PyBuildValue("y#", s, len); 
```

如果想构建一个 Unicode 字符串，而且知道 s 指向的数据是以 UTF-8 来编码的，则可以使用如下的代码来完成：

```txt
PyObject *obj = Py_BuildValue("s#", s, len); 
```

如果 s 指向的数据是以其他已知的格式来编码的，可以通过 PyUnicode_Decode()来创建字符串对象：

PyObject \*obj $=$ PyUnicode_Decode(s, len, "encoding", "errors");   
/\* Examples /\* obj $=$ PyUnicode_Decode(s, len, "latin-1", "strict"); obj $=$ PyUnicode_Decode(s, len, "ascii", "ignore");

如果刚好有一个以wchar_t *, len 表示的宽字符串，这里就有一些选择。首先，可以像这样使用 Py_BuildValue()：

```c
wchar_t *w; /* Wide character string */
int len; /* Length */
PyObject *obj = Py_BuildValue("u#", w, len); 
```

其次，可以直接使用 PyUnicode_FromWideChar()：

```txt
PyObject *obj = PyUnicode_FromWideChar(w, len); 
```

对于宽字符串来说，不会对字符数据做任何解释——这里会假设把原始的 Unicode 码点直接转换到 Python 中。

# 15.15.3 讨论

把字符串从 C 转换到 Python 所遵循的准则同 I/O 一样。即，C 中的数据必须依据某个编码规则显式将其解码为字符串。常见的编码包括 ASCII、Latin-1 以及 UTF-8。如果不能百分之百地确定编码格式或者数据本身是二进制的，那么最好的选择就是将字符串编码为字节流。

当创建对象时，Python 总是会拷贝我们提供的字符串数据。如果有必要的话，之后释放 C 字符串的任务就落在我们的身上。此外，为了获得更好的可靠性，在创建字符串时应该同时使用指针和表示字符串大小的参数，而不是依赖于以 NULL 结尾的数据。

# 15.16 同编码方式不确定的 C 字符串打交道

# 15.16.1 问题

我们需要在C和Python之间来回转换字符串，但是字符串在C中的编码方式是不确定的或者说是未知的。例如，假设 C 中的数据应该是按照 UTF-8 来编码的，但这个约定并没有得到强制执行。我们想编写代码能够以优雅的方式处理有问题的数据，在处理的过程中不会导致 Python 崩溃，也不会破坏字符串数据。

# 15.16.2 解决方案

下面的 C 函数以及数据可用来说明这个问题的本质：

/\* Some dubious string data (malformed UTF-8) \*/ const char \*sdata $=$ "Spicy Jalape\xc3\xb1o\xae"; int slen $= 16$ \*/ Output character data \*/   
void print_chars(char \*s, int len) { int n $= 0$ while $(n <   \mathrm{len})$ { printf("%2x"，(unsigned char)s[n]); n++; } printf("\n");

在上述代码中，字符串 sdata 将UTF-8 和格式不正确的数据混合在了一起。然而，如果用户在 C 中调用 print_chars(sdata, slen)，却能够正常工作。

现在假设我们想将 sdata 的内容转换为 Python 字符串。进一步假设我们想稍后再把这个Python 字符串通过扩展模块传回给 print_chars()函数。下面给出的做法可以保证就算有编码问题存在，也能够完全保留原始数据不变。

```c
/\*Return the C string back to Python \*/   
static PyObject \*py_retstr(PyObject \*self,PyObject \*args){ if(!PyArg_ParseTuple(args，"")){ return NULL; } return PyUnicode_Decode(sdata,slen，"utf-8"，"surrogateescape"); 1 /\*Wrapper for the print chars() function \*/ static PyObject \*pyprint chars(PyObject \*self,PyObject \*args) { 
```

PyObject \*obj, \*bytes;   
char $^*\mathrm{s} = 0$ .   
Py_ssize_t len;   
if (!PyArg_ParseTuple(args, "U", &obj)) { return NULL;   
1   
if ((bytes $=$ PyUnicode_AsEncodedString(obj,"utf-8","surrogatescape")) $= =$ NULL){ return NULL;   
PyBytes_AsStringAndSize(bytes,&s,&len);   
print_chars(s,len);   
Py_DECBCF(bytes);   
Py_RETURN_NONE;

如果在 Python 中尝试调用这些函数，结果是这样的：

>>>s $\equiv$ retstr()   
>>>s   
'Spicy Jalapeño\udcae'   
>>>print_chars(s)   
5370696379204a616c617065c3b16fae   
>>>

仔细观察就会发现有问题的字符串在编码为 Python 字符串时没有出现错误，而且当将其传回到 C 中时会转换回字节串，而且编码方式与原始的 C 字符串一模一样。

# 15.16.3 讨论

本节解决了在扩展模块中处理字符串时微妙而又潜在的恼人问题。即，C 字符串在扩展模块中可能不会遵循严格的Unicode编码/解码规则，而这正是Python本来所期望的。因此，有可能会出现将有问题的 C数据传入到 Python中的情况。这方面有一个好的例子，那就是像文件名这种与系统底层调用相关的C字符串。例如，如果系统调用返回了一个有问题的字符串给Python解释器，而解释器不能正确地进行解码，此时会发生什么呢？

一般来说，Unicode 方面的错误通常会通过指定某种错误方案（error policy）来处理，例如严格（strict）、忽略（ignore）、替换（replace）或者其他类似的方案。但是，这些方案的缺点在于它们会不可挽回地破坏原始字符串的内容。例如，如果示例中有问题的数据采用以上这些方案进行解码，最终会得到这样的结果：

```txt
>>> raw = b'Spicy Jalape\xc3\xb1o\xae'  
>>> raw.decode('utf-8', 'ignore')  
'Spicy Jalapeño' 
```

```txt
>>> raw.decode('utf-8','replace') 'Spicy Jalapeño?'   
>>> 
```

而代理转义（surrogateescape）错误处理方案会接受所有不可解码的字节，并将它们转换为代理对的低半部（low-half）（\udcXX，这里 XX 是原始字节值）。例如：

```html
>>> raw.decode('utf-8','surrogatescape') 'Spicy Jalapeño\udcae' >>> 
```

像\udcae 这种独立的低位代理字符从来不会出现在有效的 Unicode 字符中。因此，这个字符串从技术上来说是个非法的表示。事实上，如果试着将它传给函数并执行输出，就会得到编码错误：

```txt
>>>s = raw.decode('utf-8', 'surrogatescape')   
>>>print(s)   
Traceback (most recent call last): File "<stdin>", line 1, in <module>   
UnicodeEncodeError:'utf-8' codec can't encode character '\udcae' in position 14: surrogates not allowed   
>>> 
```

但是，使用代理转义的主要原因在于这么做能够让有问题的字符串从 C 传到 Python，然后再传回到 C 中时不破坏任何数据。当字符串再次使用 surrogateescape 进行编码时，代理字符将转换回它们原来的字节。示例如下：

```txt
>>>s   
'Spicy Jalapeño\udcae'   
>>s.encode('utf-8','surrogateescape')   
b'Spicy Jalape\xc3\xb1o\xae'   
>>> 
```

一般来说，最好尽可能避免使用代理编码——如果使用了适当的编码方式，我们的代码将更加可靠。但是，有时候我们没法控制数据的编码，而且也不能随意忽略或者替换有问题的数据，因为其他函数可能会用到它们。本节讲解了应该如何应对这些情况。

最后再说一句，许多与系统相关的 Python 函数，尤其是那些与文件名、环境变量以及命令行选项相关的函数都使用了代理编码。例如，如果某个目录中包含有不可解码的文件名，而我们针对这个目录使用像 os.listdir()这样的函数，那么它就会以代理转义的方式返回字符串。相关内容在 5.15 节中也有涉及。

PEP 383（http://www.python.org/dev/peps/pep-0383）中有更多关于本节提到的问题的相关信息，对代理转义的错误处理机制也有说明。

# 15.17 把文件名传给 C 扩展模块

# 15.17.1 问题

我们需要将文件名传给 C 扩展函数，但是需要确保文件名已经根据系统所期望的文件名编码方式进行了编码。

# 15.17.2 解决方案

要编写一个扩展函数用来接收文件名，可以使用下面的代码来完成：

```txt
static PyObject \*py_get檔案(PyObject \*self, PyObject \*args) { PyObject \*bytes; char \*filename; Py_ssize_t len; if(!PyArg_ParseTuple(args,"O&",PyUnicode_FSCONverter,&bytes)) \{ return NULL; } PyBytes_AsStringAndSize(bytes,&filename,&len); /* Use filename*/ ... /\* Cleanup and return \*/ Py_DECREF(bytes) Py_RETURN_NONE;   
1 
```

如果已经有了一个 PyObject *并希望将其转换成文件名，可以使用下面的代码完成：

```c
PyObject *obj; /* Object with the filename */ PyObject *bytes;  
char *filename;  
Py_ssize_t len;  
bytes = PyUnicode_EncodeFSDefault(obj);  
PyBytes_AstringAndSize(bytes, &filename, &len); /* Use filename */  
...  
/* Cleanup */  
Py_DECBCF(bytes); 
```

如果需要将文件名再返回给 Python，可以使用下面的代码：

```txt
/\*Turn a filename into a Python object \*/ 
```

```c
char *filename; /* Already set */  
int filename_len; /* Already set */  
PyObject *obj = PyUnicode_DecodeFSDefaultAndSize(filename, filename_len); 
```

# 15.17.3 讨论

以可移植的方式来处理文件名是一个棘手的问题，最好把这个问题留给Python来解决。如果我们把本节提到的技术用在自己的扩展代码中，那么文件名的处理方式就和Python 中处理文件名的方式保持一致了。这包括对字节的编码/解码、处理有问题的字符、代理转义以及其他的复杂情况。

# 15.18 把打开的文件传给 C 扩展模块

# 15.18.1 问题

在 Python 中有一个打开的文件对象，我们需要将它传递给 C扩展代码，在扩展模块中使用这个文件。

# 15.18.2 解决方案

要把一个文件转换为一个整数表示的文件描述符，可以使用 PyObject_AsFileDescriptor()来完成。示例如下：

```c
PyObject *fobj; /* File object (already obtained somehow) */
int fd = PyObject_AsFileDescriptor(fobj);
if (fd < 0) {
return NULL;
} 
```

得到的文件描述符是通过在 fobj 上调用 fileno()方法来获取的。因此，任何以这种方式暴露出描述符的对象都应该能正常工作（即，文件、套接字等）。

一旦有了文件描述符，就可以将它传递给各种同文件打交道的底层 C 函数了。

如果需要将一个整数表示的文件描述符转换回 Python 对象，可以使用 PyFile_FromFd()，示例如下：

```c
int fd; /* Existing file descriptor (already open) */  
PyObject *fobj = PyFile_FromFd(fd, "filename", "r", -1, NULL, NULL, NULL, 1); 
```

函数 PyFile_FromFd()的参数正好对应着内建的 open()函数。这里的 NULL 表示采用默认的 encoding、errors 和 newline 设定。

# 15.18.3 讨论

如果要将文件对象从 Python 中传递给 C，这里有几个棘手的问题需要考虑。首先，Python是通过io 模块实现自己的I/O 缓冲处理的。在将任何类型的文件描述符传递给C 之前，应该首先在对应的文件对象上刷新 I/O 缓冲的。否则，在文件流上可能会出现数据乱序的情况。

其次，需要特别注意文件的归属（ownership）和由谁负责关闭文件的问题。如果把文件描述符传递给C，但仍然要在Python中使用这个文件的话，需要确保在C代码中不会意外地关闭了文件。同样，如果把文件描述符转换成了Python文件对象，需要明确由谁来负责关闭文件。PyFile_FromFd()的最后一个参数设为 1 就表示应该由 Python 来关闭这个文件。

如果要创建另一种不同类型的文件对象，比如在 C 标准 I/O 库中使用 fdopen()创建 FILE*对象，需要特别小心。这么做会引入两种完全不同的I/O缓冲层到I/O栈中（一个来自于Python 的 io 模块，另一个来自于 C 的 stdio）。在 C 中，像 fclose()这样的操作也会在无意中关闭将来要在 Python 中使用的文件。如果要选择的话，应该让扩展代码同底层的文件描述符相兼容，而不是采用<stdio.h>中提供的高层抽象（比如 FILE *）。

# 15.19 在 C 中读取文件型对象

# 15.19.1 问题

我们想编写C扩展代码使其能够从任意的 Python 文件型对象中读取数据（例如，普通的文件、StringIO 对象等）。

# 15.19.2 解决方案

要在文件型对象上读取数据，需要重复调用对象的 read()方法并采取适当的步骤将数据进行解码。

下面给出了一个 C 扩展函数示例，它只是读取出文件型对象上的所有数据并打印到标准输出上，这样可以看到结果：

define CHUNK_SIZE 8192   
/\* Consume a "file-like" object and write bytes to stdout \*/   
static PyObject \*py_consume_file(PyObject \*self, PyObject \*args) { PyObject \*obj; PyObject \*read_meth; PyObject \*result $=$ NULL; PyObject \*read_args; if(!PyArg_ParseTuple(args,"O",&obj)) { return NULL;

}   
/\*Get the read method of the passed object \*/   
if ((read_meth $=$ PyObject_GetAttrString(obj, "read")) $= =$ NULL){ return NULL;   
}   
/\*Build the argument list to read() \*/   
read_args $=$ Py_BuildValue("i"),CHUNK_SIZE);   
while (1){ PyObject \*data; PyObject \*enc_data; char \*buf; Py_ssize_t len; /\*Call read(）\*/ if((data $=$ PyObject_Call(read_meth,read_args,NULL)) $= =$ NULL){ goto final; } /\*Check for EOF \*/ if(PySequence_length(data) $= = 0$ { Py_DECBCF(data); break; } /\*Encode Unicode as Bytes for C\*/ if((enc_data $\equiv$ PyUnicode_AsEncodedString(data,"utf-8","strict")) $= =$ NULL){ Py_DECBCF(data); goto final; } /\*Extract underlying buffer data \*/ PyBytes_AsStringAndSize(enc_data,&buf,&len); /\*Write to stdout (replace with something more useful) \*/ write(1，buf，len); /\*Cleanup \*/ Py_DECBCF(enc_data); Py_DECBCF(data);   
} result $=$ Py_BuildValue("");   
final: /\*Cleanup \*/ Py_DECBCF(read_meth); Py_DECBCF(read_args);

```lua
return result; 
```

要测试上述代码，可以先创建一个文件型对象比如 StringIO 实例，然后将其传入：

```txt
>>> import io
>>> f = io.StringIO('Hello\nWorld\n')
>>> import sample
>>> sampleconsume_file(f)
Hello
World
>>> 
```

# 15.19.3 讨论

与普通的系统文件不同，一个文件型对象并不一定是围绕着底层的文件描述符来构建的。因此，不能用 C 库中的普通函数来访问。相反，我们需要用 Python 的 C API 来操纵文件型对象，这和在 Python 中工作时很像。

在解决方案中，read()方法是从所传入的对象中提取出来的。接着会创建一个参数列表，然后重复传给 PyObject_Call()来调用方法。要检测文件结尾（EOF），我们使用PySequence_Length()来检查返回的结果长度是否为 0。

对于所有的 I/O 操作，我们需要关注底层的编码以及字节和 Unicode 之间的区别。本节给出的解决方案展示了如何以文本模式读取文件，并将得到的结果解码为可以在C程序中使用的字节编码形式。如果想以二进制模式读取文件，只需要做很少的修改即可。示例如下：

/\*Call read() \*/ if((data $=$ PyObject(Call(read_meth,read_args，NULL)) $= =$ NULL){ goto final; } /\* Check for EOF \*/ if(PySequence_length(data) $= = 0$ ）{ Py_DECREF(data); break; } if(!PyBytes_Uheck(data)){ Py_DECREF(data); PyErr_SetString(PyExc_IOError，"File must be in binary mode"); goto final; } /\*Extract underlying buffer data\*/ PyBytes_AstringAndSize(data,&buf,&len);

本节中最棘手的地方在于内存管理。当使用 PyObject *变量时，需要仔细管理引用计数并且当不再使用时需要进行清理。示例中出现的 Py_DECREF()调用正是应对于此。

本节以一种通用的方式来编写，这样可以把技术应用到其他的文件操作上，比如说写文件。例如，要写入数据，只要从文件型对象中获取 write()方法，将数据转换为合适的 Python 对象（字节或者 Unicode），然后调用方法将数据写入文件即可。

最后，尽管文件型对象通常还提供了其他的方法（比如 readline()、read_into()），但为了获得最大的可移植性，最好还是专注于基本的 read()和 write()方法。对于 C 扩展代码来说，尽量保持简单通常才是行之有效的方案。

# 15.20 从 C 中访问可迭代对象

# 15.20.1 问题

我们想编写 C 扩展代码，用来访问任意的 Python 可迭代对象，比如列表、元组、文件或者生成器。

# 15.20.2 解决方案

下面这个简单的 C 扩展函数展示了如何去访问一个可迭代对象中的元素：

static PyObject \*py_consume iterable(PyObject \*self, PyObject \*args) { PyObject \*obj; PyObject \*iter; PyObject \*item; if(!PyArg_ParseTuple(args，"O",&obj)) { return NULL; } if((iter $=$ PyObject_GetIter(obj)) $= =$ NULL){ return NULL; } while((item $=$ PyIter_Next(iterator）！ $= =$ NULL）{ /\*Use item\*/ ... Py_DECREF(item); } Py_DECREF(iterator); return Py_BuildValue("");   
1

# 15.20.3 讨论

本节给出的代码概念上可以映射为 Python 中类似的代码。PyObject_GetIter()调用和

Python 中的 iter()一样，都是用来获取一个迭代器。PyIter_Next()函数调用迭代器上的next方法，返回迭代器中的下一个元素，如果没有更多元素可返回，则返回NULL。请确保自己对内存管理多加小心——获取迭代器元素以及迭代器对象本身都需要调用Py_DECREF()来避免内存泄露。

# 15.21 排查段错误

# 15.21.1 问题

Python 解释器由于段错误、总线错误、非法访问或者其他的致命错误而发生崩溃。我们希望有一个 Python traceback 能够告诉我们出现错误时，程序运行到了哪一步。

# 15.21.2 解决方案

faulthandler 模块可帮我们解决这个问题。在程序中包含如下代码：

```python
import faulthandler  
faulthandler enable() 
```

此外，也可以在运行 Python 时带上选项-Xfaulthandler：

```batch
bash % python3 -Xfauthandler program.py 
```

最后但同样重要的是，我们可以设定环境变量 PYTHONFAULTHANDLER。

开启 faulthandler 功能后，C 扩展模块中出现的致命错误将在 Python 的 traceback 中打印出来。示例如下：

```txt
Fatal Python error: Segmentation fault  
Current thread 0x00007fff71106cc0:  
File "example.py", line 6 in foo  
File "example.py", line 10 in bar  
File "example.py", line 14 in spam  
File "example.py", line 19 in <module>  
Segmentation fault 
```

尽管这还是没有告诉我们错误究竟出现在 C 代码中的什么地方，但至少能够告诉我们错误是如何传到 Python 中来的。

# 15.21.3 讨论

当出现错误时，faulthandler模块会告诉我们 Python 代码的调用栈回溯。最起码，这会告诉我们调用的最顶层的扩展函数是什么。再结合 pdb 或者其他的 Python 调试器，就

能追踪调查导致出现这个错误的 Python 代码执行流程。

faulthandler 模块无法告诉我们任何来自于 C 代码中的错误。正因为如此，我们需要使用一个传统的 C 调试器，比如说 gdb。但是，从 faulthandler 的栈回溯中得到的信息应该可以更好地指引排错的方向。

应该要指出的是，某些在 C 中出现的特定类型的错误可能不太容易得到恢复。比如，如果某个C扩展模块破坏了栈或者程序的堆，这会使得faulthandler模块无法正常工作，所以此时得不到任何有用的输出（除了程序崩溃之外）。显然，每个人遇到的情况会有所不同。

# 补充阅读

现在有大量的图书和在线资源可供我们学习和实践 Python 编程。但是，如果和本书一样，你关注的重点是如何使用 Python 3，那么要找到一些可靠的资源则有些困难。因为市面上大量的图书都是为早期的 Python 版本而编写的。

在这份附录中，我们提供了一些材料的链接，它们对于学习 Python 3 编程以及理解本书中的内容会特别有用。这绝不是一份详尽无遗的资源列表，所以你绝对应该查查看这些资源是否有了新的名称或者发布了更新的版本。

# A.1 在线资源

# http://docs.python.org

如果你需要深入了解语言的细节以及探究各个模块，那么不必多说，Python 自带的在线文档绝对是极佳的资源。只是在查阅的时候需要确保你看的是 Python 3 的文档，不是之前的老版本。

# http://www.python.org/dev/peps

如果想理解为 Python 语言添加新特性的动机以及一些微妙的实现细节，那么 PEPs（Python Enhancement Proposals）绝对是珍贵的参考资源。尤其是对于一些更加高级的语言特性更是如此。在写作本书时，我们发现 PEP 往往比官方文档还要有帮助。

# http://pyvideo.org

这里有大量的视频演讲以及教程，素材都是取自最近一次的 PyCon 大会、用户组会议等。对于学习现代 Python 开发来说是非常优秀的资源。在许多视频中都会有 Python 的

核心开发者现身说法，讲解将会添加到 Python 3 中的新特性。

# http://code.activestate.com/recipes/langs/python

很长一段时间以来，在 ActiveState的Python 版块上可找到数以千计的针对特定编程问题的解决方案。在写作本书时，已经包含有大约 300 条特定于 Python 3 的秘籍。你会发现其中的许多秘籍要么对本书中已经涵盖的主题进行了扩展，要么缩小范围，专注于更加具体的任务。因此，它是学习 Python 3 时的好伴侣。

# http://stackoverflow.com/questions/tagged/python

Stack Overflow 上目前有超过 175000 个问题被标记为与 Python 相关（而这其中又有大约 5000 个问题是特定于 Python 3 的）。尽管每个问题与回答的质量有所区别，但仍然可以找到许多优秀的素材。

# A.2 学习 Python 的图书

下面这些图书提供了对 Python 编程的入门介绍，且重点放在了 Python 3 上。

• Learning Python，第四版，作者 Mark Lutz， O’Reilly&Associates 出版（2009）。  
• The Quick Python Book，第二版，作者 Vernon Ceder，Manning 出版（2010）。  
Python Programming for the Absolute Beginner，第三版， 作者 Michael Dawson，Course Technology PTR 出版（2010）。  
Beginning Python: From Novice to Professional，第二版，作者 Magnus Lie Hetland，Apress 出版（2008）。  
Programming in Python 3，第二版，作者 Mark Summerfield，Addison-Wesley 出版（2010）。

# A.3 高级图书

下面这些图书则涵盖了更加高级的技术，其中也包括了 Python 3 方面的主题。

• Programming Python，第四版，作者 Mark Lutz， O’Reilly & Associates 出版（2010）。  
• Python Essential Reference，第四版，作者 David Beazley，Addison-Wesley 出版（2009）。  
Core Python Applications Programming，第三版，作者 Wesley Chun，Prentice Hall 出版（2012）。  
• The Python Standard Library by Example，作者 Doug Hellmann， Addison-Wesley 出

版（2011）。

Python 3 Object Oriented Programming，作者 Dusty Phillips，Packt Publishing 出版（2010）。  
Porting to Python 3，作者 Lennart Regebro，CreateSpace 出版（2011），http://python3porting.com。

# 关于作者

David Beazley是一位居住在芝加哥的独立软件开发者以及图书作者。他主要的工作在于编程工具，提供定制化的软件开发服务，以及为软件开发者、科学家和工程师教授编程实践课程。他最为人熟知的工作在于 Python 编程语言，他已为此创建了好几个开源的软件包（例如 Swig 和 PLY），并且是备受赞誉的图书 Python Essential Reference 的作者。他也对 C、 $\mathrm { C } { + } { + }$ 以及汇编语言下的系统编程有着丰富的经验。

Brain K. Jones 是普林斯顿大学计算机系的一位系统管理员。

# 封面说明

本面封面上的动物是一只跳鼠（跳兔），也被称为春兔。跳兔根本就不是野兔，而只是啮齿目跳兔科中唯一的成员。它们不是有袋类动物，但隐约有些袋鼠的样子，有着短小的前腿，强有力的后腿，适合于跳跃。除此之外还有一根长长的、强壮有力且毛发浓密（但不容易抓住）的尾巴，用来掌握平衡以及坐下来的时候作为支撑物。它们能长到 $1 4 \sim 1 8$ 英寸，尾巴几乎和身体一样长，体重大约可达8 磅。跳兔有着油腻且富有光泽的褐色或金色表皮，柔软的皮毛，它的腹部是白色的。它们的头部有着和身体不成比例的大小，耳朵则很长（耳朵底部有一块可翻动的皮肤，这样当它们在打洞时可以避免让沙子进入耳朵里），眼睛是深棕色的。

跳兔全年都可以交配，孕期在 $7 8 \sim 8 2$ 天。雌性跳兔一般每窝只会产一只小跳兔（小跳兔会一直呆在它的妈妈身边，直到大约7周大为止），但每年会有3个或4 个窝。刚生下的小跳兔是有牙齿的，而且毛发齐全，眼睛是闭上的，而耳朵是打开着的。

跳兔是陆生生物，非常适应于挖掘地洞。它们白天喜欢呆在自己构筑的洞穴和地道所交织成的网络中。跳兔是夜间活动生物，主要食草，可以吃鳞茎植物、根、谷物，偶尔也吃昆虫。当它们在觅食时会移动四肢，但每一次水平跳跃能移动 $1 0 \sim 2 5$ 英尺，遇到危险时能够快速逃离。虽然常能在野外看见跳兔集体觅食，但它们并不会形成一个有组织的社会化单位，通常都是独自筑窝或者成对繁殖。跳兔在圈养下可以存活15 年。可以在扎伊尔、肯尼亚以及南非的干燥沙漠或者半干旱地区见到跳兔的身影，它们也是南非最受喜爱的重要食物来源。

# 欢迎来到异步社区！

# 异步社区的来历

异步社区 (www.epubit.com.cn) 是人民邮电出版社旗下 IT 专业图书旗舰社区，于 2015 年 8 月上线运营。

异步社区依托于人民邮电出版社 20 余年的 IT专业优质出版资源和编辑策划团队，打造传统出版与电子出版和自出版结合、纸质书与电子书结合、传统印刷与 POD 按需印刷结合的出版平台，提供最新技术资讯，为作者和读者打造交流互动的平台。

# 社区里都有什么？

![](images/2baa64c4ddfc165cac93159bb937db8b179f4d9f8ced0158e743946919eb0764.jpg)

# 购买图书

我们出版的图书涵盖主流IT技术，在编程语言、Web技术、数据科学等领域有众多经典畅销图书。社区现已上线图书 1000 余种，电子书 400 多种，部分新书实现纸书、电子书同步出版。我们还会定期发布新书书讯。

# 下载资源

社区内提供随书附赠的资源，如书中的案例或程序源代码。

另外，社区还提供了大量的免费电子书，只要注册成为社区用户就可以免费下载。

# 与作译者互动

很多图书的作译者已经入驻社区，您可以关注他们，咨询技术问题；可以阅读不断更新的技术文章，听作译者和编辑畅聊好书背后有趣的故事；还可以参与社区的作者访谈栏目，向您关注的作者提出采访题目。

# 灵活优惠的购书

您可以方便地下单购买纸质图书或电子图书，纸质图书直接从人民邮电出版社书库发货，电子书提供多种阅读格式。

对于重磅新书，社区提供预售和新书首发服务，用户可以第一时间买到心仪的新书。

用户帐户中的积分可以用于购书优惠。100积分 $^ { = 1 }$ 元，购买图书时，在 使用积分里填入可使用的积分数值，即可扣减相应金额。

# 特 别 优 惠

购买本书的读者专享异步社区购书优惠券。

使用方法：注册成为社区用户，在下单购书时输入 57AWG ，然后点击ì使用优惠码”，即可享受电子书8折优惠（本优惠券只可使用一次）。

# 纸电图书组合购买

社区独家提供纸质图书和电子书组合购买方式，价格优惠，一次购买，多种阅读选择。

# 社区里还可以做什么？

# 提交勘误

![](images/699351bfd0f5cd29acb36a3eb0aa29ed0cc1bf4e179f7a6dc0c96f45cf8b5986.jpg)

您可以在图书页面下方提交勘误，每条勘误被确认后可以获得100 积分。热心勘误的读者还有机会参与书稿的审校和翻译工作。

# 写作

社区提供基于 Markdown 的写作环境，喜欢写作的您可以在此一试身手，在社区里分享您的技术心得和读书体会，更可以体验自出版的乐趣，轻松实现出版的梦想。

如果成为社区认证作译者，还可以享受异步社区提供的作者专享特色服务。

# 会议活动早知道

您可以掌握IT 圈的技术会议资讯，更有机会免费获赠大会门票。

# 加入异步

扫描任意二维码都能找到我们：

![](images/48d6f84f578ea1c18c6c66248ddac8095c397811bdc9bd2f5a0c7f1bddb097ba.jpg)  
异步社区

![](images/27486c6effcd2b9673eeed14c72e66d032c93e6e3e18a895f664e949b25deeba.jpg)  
微信服务号

![](images/6ba0bb5dca3b68faff3766b631ca53bcdbdb246532a20adb9158d4100e4052c6.jpg)  
微信订阅号

![](images/388c114443936fc5a728e9d6a03f3711b1be8179169f7fe67e3127b67ffef70b.jpg)  
官方微博

![](images/fb2502e848c115667b04997195282818f62562d896cfb5dee599979a027395b0.jpg)  
QQ 群：368449889

社区网址：www.epubit.com.cn

官方微信：异步社区

官方微博： $@$ 人邮异步社区， $@$ 人民邮电出版社- 信息技术分社

投稿 & 咨询：contact@epubit.com.cn

